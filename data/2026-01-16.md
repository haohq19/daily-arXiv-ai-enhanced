<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: 提出VERHallu基准评估视频大语言模型的事件关系幻觉，包括因果、时序和子事件关系，并提出关键帧传播策略缓解该问题


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注视频中事件、物体和场景的存在性幻觉，而忽视了事件关系幻觉。视频大语言模型在密集事件关系推理上存在困难，常常依赖先验知识而非帧级线索。

Method: 1) 提出VERHallu基准，包含关系分类、问答和反事实问答三种任务；2) 提出关键帧传播策略，在中间层重新分配帧级注意力以增强多事件理解

Result: 当前最先进的视频大语言模型在密集事件关系推理上表现不佳，虽然对关键事件有较强的定位能力，但常常忽略周围子事件。关键帧传播策略能有效缓解事件关系幻觉且不影响推理速度。

Conclusion: 事件关系幻觉是视频大语言模型的重要问题，VERHallu基准提供了全面评估工具，关键帧传播策略为缓解该问题提供了有效解决方案。

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [2] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 首个合成水下事件相机光流基准数据集，通过物理光线追踪渲染水下RGBD序列生成真实事件数据流，包含密集真值光流、深度和相机运动。


<details>
  <summary>Details</summary>
Motivation: 水下成像面临波长相关光衰减、悬浮颗粒散射、浑浊模糊和非均匀照明等挑战，传统相机难以获取地面真值运动。事件相机虽有微秒分辨率和动态范围优势，但缺乏真实水下光学与准确光流配对的数据库限制了其在水下环境的研究进展。

Method: 使用基于物理的光线追踪渲染水下RGBD序列，通过现代视频到事件转换管道生成真实事件数据流，包含密集真值光流、深度和相机运动。

Result: 建立了首个合成水下事件相机光流基准数据集，对最先进的基于学习和基于模型的光流预测方法进行了基准测试，分析了水下光传输对事件形成和运动估计精度的影响。

Conclusion: 该数据集为未来水下事件感知算法的开发和评估建立了新基准，代码和数据集已公开。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [3] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: LaViT框架通过对齐潜在视觉思维而非静态嵌入，解决多模态推理中的感知鸿沟问题，显著提升视觉基础能力


<details>
  <summary>Details</summary>
Motivation: 当前多模态潜在推理依赖外部监督，忽视了内在视觉注意力动态。研究发现蒸馏过程中存在感知鸿沟：学生模型模仿教师文本输出时关注完全不同的视觉区域，依赖语言先验而非基础感知

Method: 提出LaViT框架，通过让学生模型自回归重构教师的视觉语义和注意力轨迹来对齐潜在视觉思维，采用课程感官门控机制防止捷径学习

Result: LaViT显著增强视觉基础能力，在复杂推理任务上提升高达+16.9%，使紧凑的3B模型超越更大的开源变体和GPT-4o等专有模型

Conclusion: 通过对齐潜在视觉思维而非静态嵌入，LaViT有效解决了多模态推理中的感知鸿沟问题，为更可靠的视觉基础推理提供了新方法

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [4] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 论文提出视频异常推理(VAR)新任务，将视频异常分析从描述性理解提升到结构化多阶段推理，并构建包含8641个视频的大规模数据集和基于MLLM的Vad-R1-Plus模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频异常检测与理解(VAD&U)领域主要局限于异常定位或事后描述，缺乏显式推理过程、风险意识和决策导向解释，需要将异常分析提升到结构化推理层面。

Method: 1) 定义VAR任务，要求模型在回答异常相关问题前进行渐进式推理；2) 构建包含8641个视频、5万+样本的数据集，基于PerCoAct-CoT结构化标注；3) 提出异常感知组相对策略优化增强弱监督下推理可靠性；4) 开发端到端MLLM模型Vad-R1-Plus，支持自适应分层推理和风险感知决策。

Result: 提出的基准和方法有效提升了MLLM在VAR任务上的推理能力，在开源和专有基线模型上都取得了优越性能，验证了结构化推理框架的有效性。

Conclusion: VAR任务将视频异常分析从描述性理解推进到结构化多阶段推理，通过PerCoAct-CoT标注框架和Vad-R1-Plus模型实现了视觉感知、因果解释和风险感知决策的整合，为视频异常理解提供了新范式。

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [5] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: Alterbute是一种基于扩散模型的图像编辑方法，专注于修改物体的内在属性（颜色、纹理、材质、形状），同时保持物体身份识别特征和场景上下文不变。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖无监督先验但难以保持物体身份，要么使用过于严格的监督限制了内在属性的有效变化。需要一种既能保持物体身份又能实现有意义内在属性编辑的方法。

Method: 1) 使用宽松的训练目标，允许模型根据身份参考图像、描述目标内在属性的文本提示、背景图像和物体掩码来改变内在和外在属性；推理时通过重用原始背景和掩码限制外在变化。2) 引入视觉命名实体(VNEs)作为细粒度视觉身份类别，通过视觉语言模型从大规模公共图像数据集中自动提取VNE标签和内在属性描述。

Result: Alterbute在保持物体身份的同时进行内在属性编辑方面优于现有方法。

Conclusion: Alterbute通过结合宽松训练目标、推理时约束和视觉命名实体，实现了既能保持物体身份又能有效编辑内在属性的图像编辑方法。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 机器学习价值链存在结构性不平等：数据生成者获得极少价值，大部分价值流向聚合者，威胁AI可持续发展，需要建立公平数据价值交换框架。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习价值链存在经济数据处理不平等问题，数据生成者在整个数据循环中（从输入到模型权重再到合成输出）虽然贡献技术信号，但被剥夺了经济权益。这种不平等不仅影响经济福利，还威胁到维持当前学习算法的反馈循环的可持续性。

Method: 分析了73个公开数据交易，识别出三个结构性缺陷：缺失溯源、不对称议价能力和非动态定价。沿着机器学习价值链追踪这些问题，并提出公平数据价值交换（EDVEX）框架，旨在建立一个使所有参与者受益的最小市场。

Result: 研究发现大多数价值流向聚合者，记录显示创作者版税几乎为零，交易条款普遍不透明。这些发现证实了机器学习价值链的结构性不可持续性。

Conclusion: 需要建立公平的数据价值交换机制来解决机器学习价值链中的结构性不平等问题。提出了EDVEX框架作为解决方案，并概述了研究社区可以做出具体贡献的研究方向，以促进更可持续和公平的数据经济。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [7] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出一种混合Transformer架构，用连续深度的神经ODE块替换离散中间层，通过学习的控制信号实现推理时对生成属性的控制。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏对生成过程的细粒度控制。需要一种能够在推理时灵活控制生成属性（如情感、风格等）的机制，同时保持模型效率。

Method: 设计混合Transformer架构，将离散中间层替换为连续深度的神经ODE块。使用学习到的向量场F_θ(H, τ, u)，其中u是通过显式拼接注入的低维控制信号。深度作为连续变量，通过ODE求解器处理。

Result: 1) 梯度流稳定，无梯度爆炸/消失；2) 语义控制达到98%/88%的正/负情感控制准确率；3) 连续插值验证，固定与自适应求解器轨迹差异仅0.068%；4) 效率与标准离散基线相当。控制信号将向量场划分为具有不同曲率特征的动态机制。

Conclusion: 具有学习控制信号的连续深度动力学为可操控语言生成提供了可行且高效的机制。伴随方法实现O(1)内存训练，自适应ODE求解器揭示了学习动态中的几何结构。

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [8] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 该论文研究了点击率预测中XGBoost模型的时间聚合特征，在Avazu数据集上比较了时间感知目标编码与多种时间窗口设计，发现滑动窗口表现最佳，事件计数窗口有微小改进。


<details>
  <summary>Details</summary>
Motivation: 研究在点击率预测中如何有效利用时间聚合特征，特别是在严格的时序分割和无前瞻特征约束下，比较不同时间窗口设计对模型性能的影响。

Method: 使用Avazu点击率预测数据集，采用严格的时序分割和无前瞻特征约束。比较了强时间感知目标编码基线模型与添加实体历史时间聚合特征的模型，测试了多种窗口设计（滑动窗口、事件计数窗口、间隔窗口、分桶窗口）。

Result: 滑动窗口相比单独使用目标编码，ROC AUC提高了约0.0066-0.0082，PR AUC提高了约0.0084-0.0094。在时间聚合设计网格中，只有事件计数窗口能持续改进滑动窗口性能，但增益很小。间隔窗口和分桶窗口表现不如简单滑动窗口。

Conclusion: 建议将滑动窗口作为实践中的默认选择，当边际ROC AUC增益重要时，可考虑添加事件计数窗口。间隔窗口和分桶窗口在该数据集和协议下表现不佳。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [9] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 提出图正则化PCA（GR-PCA），通过图拉普拉斯正则化处理非各向同性噪声数据，抑制高频噪声同时保留低频信号，提升主成分的结构保真度。


<details>
  <summary>Details</summary>
Motivation: 传统PCA假设噪声各向同性（球形协方差），但实际高维数据中变量间存在依赖关系，噪声通常不是独立同分布的。需要一种能考虑特征间依赖结构的降维方法。

Method: 引入图正则化PCA（GR-PCA），通过图拉普拉斯正则化将主成分载荷偏向图的低频傅里叶模式。学习稀疏精度图，利用图拉普拉斯惩罚抑制高频信号，保留图一致的低频信号。

Result: 在多种图拓扑、信噪比和稀疏度下测试，GR-PCA能集中方差到目标支撑集，产生更低图拉普拉斯能量的载荷，保持竞争性的样本外重建精度。当存在高频信号时，能防止过拟合，提高结构保真度。

Conclusion: GR-PCA提供了一种实用、可扩展的结构感知降维方法，在保持预测性能的同时提升结构保真度。当高频信号与图相关时优势明显，而PCA在信号近似旋转不变时仍具竞争力。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 论文提出了一个分析AI生存风险的框架，基于两个前提构建了人类存续的不同情景分类，并评估了不同情景面临的挑战和应对策略，最终给出了AI毁灭人类概率的粗略估计。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成生存风险的争论日益激烈。本文旨在建立一个系统性的框架来分析AI的生存风险，为这一重要议题提供结构化的思考方式。

Method: 基于两个核心前提构建分析框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。通过这两个前提构建了四种人类存续的情景分类，每种情景对应其中一个前提失败的情况。

Result: 建立了四种人类存续情景：1）科学障碍阻止AI变得极其强大；2）人类禁止AI研究；3）极其强大的AI因其目标而不毁灭人类；4）人类能够可靠检测并禁用有毁灭目标的AI系统。分析了每种情景面临的挑战，并基于此给出了P(doom)的粗略概率估计。

Conclusion: 不同的存续情景面临不同的挑战，需要不同的应对策略。通过这一分类框架，可以更系统地评估AI的生存风险，并为制定相应的风险缓解策略提供理论基础。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [11] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种针对计算机使用代理（CUAs）的单次规划方法，通过可信规划器在执行前生成完整的条件分支执行图，为防范指令注入攻击提供可证明的控制流完整性保证。


<details>
  <summary>Details</summary>
Motivation: AI代理易受提示注入攻击，现有唯一稳健防御是架构隔离，但计算机使用代理需要持续观察UI状态来确定每个动作，这与安全所需的隔离要求相冲突。

Method: 引入单次规划方法：可信规划器在观察任何潜在恶意内容之前生成完整的条件分支执行图，提供可证明的控制流完整性保证，同时需要额外措施防止分支导向攻击。

Result: 在OSWorld上评估，保留了前沿模型高达57%的性能，同时将较小开源模型的性能提升高达19%，证明严格的安全性和实用性可以在CUAs中共存。

Conclusion: 通过单次规划方法解决了计算机使用代理中安全隔离与功能需求之间的根本矛盾，虽然成功防止指令注入，但仍需防范分支导向攻击，实现了安全与效用的平衡。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [12] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出了一种针对多步推理任务的定向路由方法，仅在关键步骤（可能导致解决方案崩溃的步骤）使用大模型，而让小模型处理常规步骤，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 多步推理任务（如数学问题解决）容易发生级联失败，当前LLM路由方法将整个查询分配给一个模型，将所有推理步骤视为同等重要，缺乏对关键步骤的针对性处理。

Method: TRIM在步骤级别进行操作：使用过程奖励模型识别错误步骤，基于步骤级别的不确定性和预算约束做出路由决策。开发了多种路由策略，从简单的阈值策略到考虑长期准确性-成本权衡的策略。

Result: 在MATH-500上，最简单的阈值策略比先前路由方法提高5倍成本效率；更高级的策略使用80%更少的大模型token就能达到强大昂贵模型的性能。在AIME等更难基准上，TRIM实现高达6倍的成本效率提升。

Conclusion: 步骤级别的难度代表了推理的基本特征，定向步骤级别干预可以根本性地改变推理效率，将昂贵调用限制在那些需要更强模型防止级联错误的关键步骤上。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [13] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个用于HRV解释的临床推理框架，通过八步可追溯推理步骤和Z-score优先级层次结构，解决LLM在HRV分析中的生理幻觉问题，实现透明的情感计算决策支持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在心率变异性解释中存在生理幻觉问题，包括呼吸性窦性心律失常污染、非线性指标的短数据不稳定性，以及忽视个体化基线而偏向群体标准。这些问题阻碍了LLM在HRV临床解释中的应用。

Method: 提出C-GRASP（临床基础推理的情感信号处理）框架，采用防护的RAG增强管道，将HRV解释分解为八个可追溯推理步骤。核心是Z-score优先级层次结构，强制优先考虑个体化基线偏移而非群体统计。通过自动RSA感知防护机制减轻频谱幻觉。

Result: 在DREAMER数据集的414个试验中，C-GRASP与高规模推理模型（如MedGemma3-thinking）结合，在4类情感分类中达到37.3%准确率，临床推理一致性得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平道路，通过可追溯推理解决LLM的"群体偏见"问题。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [14] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出一种基于解码过程潜在安全信号的早期检测方法，有效防御LLM越狱攻击，同时保持良性输入的低误拒率和响应质量


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过广泛的安全对齐，但现有防御机制（解码约束和后处理检测器）难以应对复杂越狱攻击，要么检测不鲁棒，要么过度降低模型效用

Method: 通过观察发现即使成功越狱时，模型在生成过程中仍会内部表现出潜在安全信号，但被流畅续写的驱动力覆盖。提出显式提取和利用这些潜在安全信号进行早期检测的方法

Result: 在多样越狱攻击实验中，该方法显著增强安全性，同时在良性输入上保持低误拒率并保持响应质量

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

TL;DR: BHyT是一种替代Pre-LN的归一化方法，通过tanh非线性与显式输入边界控制，解决了深度模型中的激活值增长问题，同时提高了训练效率和推理性能。


<details>
  <summary>Details</summary>
Motivation: Pre-LN虽然是大语言模型的标准选择，但存在效率低下（重复统计计算）和深度诅咒（隐藏状态幅度和方差随层数增长）的问题。现有高效的无归一化方法如DyT在深度模型中仍然脆弱。

Method: 提出Bounded Hyperbolic Tanh (BHyT)，结合tanh非线性与数据驱动的显式输入边界控制，将激活值限制在非饱和范围内。采用每块一次精确统计计算，并用轻量级方差近似替代第二次归一化。

Result: BHyT在预训练中表现出更好的稳定性和效率：平均训练速度提升15.8%，token生成吞吐量比RMSNorm平均提高4.2%，同时在语言理解和推理基准测试中匹配或超越Pre-LN的推理性能和鲁棒性。

Conclusion: BHyT通过联合解决稳定性和效率问题，为深度语言模型提供了一个有效的Pre-LN替代方案，具有理论稳定性保证和实际性能优势。

Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [Formal Safety Guarantees for Autonomous Vehicles using Barrier Certificates](https://arxiv.org/abs/2601.09740)
*Oumaima Barhoumi,Mohamed H Zaki,Sofiène Tahar*

Main category: cs.RO

TL;DR: 本文提出了一种结合障碍证书和可解释交通冲突指标的形式化验证安全框架，用于连接自动驾驶车辆，通过SMT求解器验证安全条件，并在真实高速公路数据上显著减少了不安全交互。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统虽然能感知复杂场景并做出实时决策，但数据驱动组件通常缺乏可解释性和严格的安全保证。在动态混合交通环境中，与人类驾驶车辆的交互引入了不确定性和安全挑战。

Method: 开发了结合障碍证书（BCs）和可解释交通冲突指标（特别是作为时空安全指标的碰撞时间TTC）的形式化验证安全框架。使用可满足性模理论（SMT）求解器验证安全条件，并采用自适应控制机制确保车辆实时遵守这些约束。

Result: 在真实世界高速公路数据集上的评估显示，不安全交互显著减少：TTC低于3秒阈值的事件减少了高达40%，某些车道中的冲突完全消除。该方法提供了可解释且可证明的安全保证。

Conclusion: 该方法为安全自动驾驶提供了一种实用且可扩展的策略，通过形式化验证框架结合了可解释的安全指标和严格的安全保证，能够有效应对混合交通环境中的不确定性挑战。

Abstract: Modern AI technologies enable autonomous vehicles to perceive complex scenes, predict human behavior, and make real-time driving decisions. However, these data-driven components often operate as black boxes, lacking interpretability and rigorous safety guarantees. Autonomous vehicles operate in dynamic, mixed-traffic environments where interactions with human-driven vehicles introduce uncertainty and safety challenges. This work develops a formally verified safety framework for Connected and Autonomous Vehicles (CAVs) that integrates Barrier Certificates (BCs) with interpretable traffic conflict metrics, specifically Time-to-Collision (TTC) as a spatio-temporal safety metric. Safety conditions are verified using Satisfiability Modulo Theories (SMT) solvers, and an adaptive control mechanism ensures vehicles comply with these constraints in real time. Evaluation on real-world highway datasets shows a significant reduction in unsafe interactions, with up to 40\% fewer events where TTC falls below a 3 seconds threshold, and complete elimination of conflicts in some lanes. This approach provides both interpretable and provable safety guarantees, demonstrating a practical and scalable strategy for safe autonomous driving.

</details>


### [17] [CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments](https://arxiv.org/abs/2601.10116)
*Xintong Zhang,Junfeng Chen,Yuxiao Zhu,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: CoCoPlan：一个统一框架，联合优化协作任务规划和团队间歇通信，在动态时空任务分布下实现高效多机器人协调


<details>
  <summary>Details</summary>
Motivation: 多机器人系统通过协调协作可大幅提升效率，但实践中难以实现全时通信，交互仅限于近距离交换。现有方法要么维持全时连接，要么依赖固定调度或成对协议，无法在有限通信条件下有效适应动态时空任务分布，导致协调效果不佳。

Method: 提出CoCoPlan统一框架，包含：1）分支定界架构联合编码任务分配和通信事件；2）自适应目标函数平衡任务效率与通信延迟；3）通信事件优化模块策略性确定何时、何地以及如何重建全局连接。

Result: 实验表明CoCoPlan优于现有方法：任务完成率提高22.4%，通信开销降低58.6%，可扩展性支持多达100个机器人在动态环境中运行。硬件实验包括复杂2D办公环境和3D大规模灾难响应场景。

Conclusion: CoCoPlan通过联合优化任务规划和间歇通信，有效解决了有限通信条件下多机器人系统的动态协调问题，显著提升了任务效率、通信效率和系统可扩展性。

Abstract: Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.

</details>


### [18] [The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation](https://arxiv.org/abs/2601.10268)
*Eszter Birtalan,Miklós Koller*

Main category: cs.RO

TL;DR: 研究通过仿真评估6种不同触觉传感器配置（密度和布局）对强化学习性能的影响，发现特定配置在两种实验设置中均表现最佳


<details>
  <summary>Details</summary>
Motivation: 触觉传感器对机器人手（包括假肢）的抓握稳定性至关重要，但目前设计中的传感器配置差异很大且占用大量空间，需要系统评估不同配置对学习性能的影响

Method: 使用双设置仿真系统评估6种不同密度和布局的触觉传感器配置，确保结果不依赖于特定物理模拟器、机器人手模型或机器学习算法

Result: 结果显示特定设置的效果以及跨6种传感器化仿真的通用效果，识别出一种配置在两种设置中始终表现最佳

Conclusion: 研究结果为未来机器人手（包括假肢）设计提供了指导，有助于优化触觉传感器配置以提高学习性能

Abstract: Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.

</details>
