<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: 提出VERHallu基准评估视频大语言模型的事件关系幻觉问题，涵盖因果、时序和子事件关系，并设计KFP策略缓解该问题


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型研究主要关注事件、物体和场景的存在性幻觉，而忽视了事件关系幻觉（因果、时序、子事件关系），需要专门基准来评估和解决这一问题

Method: 1. 构建VERHallu基准，包含关系分类、问答和反事实问答三种任务类型；2. 提出关键帧传播策略，在中间层重新分配帧级注意力以增强多事件理解

Result: 当前最先进的VideoLLMs在密集事件关系推理上表现不佳，过度依赖先验知识而忽视帧级线索；KFP策略能有效缓解事件关系幻觉且不影响推理速度

Conclusion: 事件关系幻觉是VideoLLMs的重要问题，VERHallu基准为评估提供了全面框架，KFP策略通过注意力重分配有效改善了多事件理解能力

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [2] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 首个合成水下事件相机光流基准数据集，通过物理渲染生成水下RGBD序列并转换为事件流，提供密集光流、深度和相机运动真值。


<details>
  <summary>Details</summary>
Motivation: 水下成像面临波长相关光衰减、悬浮颗粒散射、浑浊模糊和非均匀照明等挑战，传统相机难以获取真实运动信息。事件相机具有微秒分辨率和宽动态范围，但缺乏水下环境的事件数据集限制了其在水下感知中的应用。

Method: 使用基于物理的光线追踪渲染水下RGBD序列，通过现代视频到事件转换管道生成真实事件数据流，提供密集光流、深度和相机运动真值。

Result: 建立了首个合成水下事件光流基准数据集，评估了最先进的基于学习和模型的光流预测方法，分析了水下光传输对事件形成和运动估计精度的影响。

Conclusion: 该数据集为未来水下事件感知算法的开发和评估建立了新基准，代码和数据集已公开。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [3] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: LaViT通过对齐潜在视觉思维而非静态嵌入，解决学生模型模仿教师文本输出但关注不同视觉区域的问题，显著提升视觉基础能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态潜在推理依赖外部监督，忽视内在视觉注意力动态。学生模型经常模仿教师文本输出但关注完全不同的视觉区域，依赖语言先验而非基础感知，存在"感知鸿沟"。

Method: 提出LaViT框架，通过对齐潜在视觉思维而非静态嵌入。强制学生自回归重建教师的视觉语义和注意力轨迹，然后生成文本。采用课程感官门控机制防止捷径学习。

Result: LaViT显著增强视觉基础能力，在复杂推理任务上获得高达+16.9%的提升。紧凑的3B模型超越更大的开源变体和GPT-4o等专有模型。

Conclusion: 通过对齐潜在视觉思维而非仅文本输出，LaViT有效解决了多模态知识蒸馏中的感知鸿沟问题，实现了更好的视觉基础推理能力。

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [4] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 论文提出视频异常推理(VAR)新任务，从描述性理解提升到结构化多阶段推理，并构建包含8641个视频的大规模数据集，开发Vad-R1-Plus模型实现自适应分层推理和风险感知决策。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频异常检测与理解领域主要局限于异常定位或事后描述，缺乏显式推理过程、风险意识和决策导向的解释。需要将视频异常分析从描述性理解提升到结构化多阶段推理。

Method: 1) 定义VAR任务，要求模型在回答异常相关问题前进行渐进式推理；2) 构建包含8641个视频、5万+样本的大规模数据集，采用PerCoAct-CoT结构化标注；3) 提出异常感知组相对策略优化增强弱监督下的推理可靠性；4) 开发Vad-R1-Plus端到端MLLM模型，支持自适应分层推理和风险感知决策。

Result: 提出的基准和方法有效提升了MLLM在VAR任务上的推理能力，在广泛实验中超越了开源和专有基线模型。

Conclusion: VAR任务将视频异常分析从描述性理解提升到结构化推理，PerCoAct-CoT标注框架形式化了领域特定推理先验，Vad-R1-Plus模型展示了自适应分层推理和风险感知决策的有效性，为视频异常理解开辟了新方向。

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [5] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: Alterbute是一种基于扩散模型的图像编辑方法，专注于修改物体的内在属性（颜色、纹理、材质、形状），同时保持物体身份和场景上下文不变。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖无监督先验（难以保持身份），要么使用过于严格的监督（限制内在变化）。需要一种既能保持物体身份又能实现有意义内在属性编辑的方法。

Method: 1. 使用宽松的训练目标：模型根据身份参考图像、目标内在属性文本描述、背景图像和物体掩码进行条件生成。推理时重用原始背景和掩码以限制外在变化。2. 引入视觉命名实体（VNEs）：细粒度视觉身份类别，通过视觉语言模型自动提取标签和属性描述，实现可扩展的身份保持监督。

Result: Alterbute在保持身份的同时编辑物体内在属性方面优于现有方法。

Conclusion: Alterbute通过结合宽松训练目标和视觉命名实体，实现了在保持物体身份和场景上下文的同时有效编辑内在属性的能力。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 机器学习价值链存在结构性不可持续问题，数据生成者在价值分配中获得极少收益，大多数价值流向聚合者，这威胁到整个机器学习生态系统的可持续性。


<details>
  <summary>Details</summary>
Motivation: 论文指出机器学习价值链存在结构性不平等：数据从输入到模型权重再到合成输出的每个阶段都在提取技术信号，却剥夺了数据生成者的经济权益。这种不平等不仅是经济福利问题，还威胁到维持当前学习算法的反馈循环。

Method: 通过分析73个公开数据交易案例，发现大多数价值流向聚合者，创作者版税几乎为零且交易条款普遍不透明。识别出三个结构性缺陷：缺失来源追溯、不对称议价能力和非动态定价。

Result: 分析显示数据交易中存在严重不平等，创作者收益微乎其微，交易透明度低。这些结构性问题沿着机器学习价值链分布，威胁整个生态系统的可持续性。

Conclusion: 提出"公平数据价值交换(EDVEX)框架"来建立使所有参与者受益的最小市场，并概述了研究社区可以在数据交易方面做出具体贡献的研究方向。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [7] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出一种混合Transformer架构，用连续深度神经ODE块替代离散中间层，通过学习到的控制信号在推理时控制生成属性。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏在推理时灵活控制生成属性的能力。需要一种能够将深度作为连续变量、通过控制信号实现属性控制的架构。

Method: 设计混合Transformer架构，用连续深度神经ODE块替换离散中间层。使用学习到的向量场F_θ(H, τ, u)，其中u是通过显式拼接注入的低维控制信号。采用伴随方法实现O(1)内存训练。

Result: 梯度流稳定无爆炸/消失问题；情感控制准确率达98%/88%；连续插值轨迹差异仅0.068%；延迟与标准基线相当；自适应ODE求解器揭示学习动态的几何结构。

Conclusion: 带有学习控制信号的连续深度动态为可操控语言生成提供了可行且高效的机制，能够在推理时灵活控制生成属性。

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [8] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 本文研究点击率预测中XGBoost模型的时间聚合特征，在Avazu数据集上比较时间感知目标编码与实体历史时间聚合方法，发现滑动窗口设计能带来显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在点击率预测任务中，如何有效利用时间序列信息进行特征工程是一个重要问题。特别是在严格的时序分割和"无前瞻"特征约束下，需要研究不同时间聚合方法对模型性能的影响。

Method: 使用Avazu点击率预测数据集，采用严格的时序分割和"无前瞻"特征约束。比较了时间感知目标编码基线模型与增强实体历史时间聚合的模型，测试了多种窗口设计：滑动窗口、事件计数窗口、间隔窗口和分桶窗口。

Result: 在10%确定性样本的两个滚动尾部折叠上，滑动窗口设计相比单纯目标编码将ROC AUC提升了约0.0066-0.0082，PR AUC提升了约0.0084-0.0094。在时间聚合设计网格中，只有事件计数窗口能持续改进滑动窗口，但增益较小。间隔窗口和分桶窗口表现不如简单滑动窗口。

Conclusion: 研究结果支持将滑动窗口作为实用默认选择，当边际ROC AUC增益重要时，可考虑使用事件计数窗口。间隔窗口和分桶窗口在此数据集和协议下表现不佳。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [9] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 提出Graph Regularized PCA (GR-PCA)，通过图正则化处理非各向同性噪声数据，利用图拉普拉斯偏置载荷向低频傅里叶模式，抑制高频噪声，保持图相干低频信号。


<details>
  <summary>Details</summary>
Motivation: 高维数据常存在变量间依赖关系，违反PCA的各向同性噪声假设。当噪声在不同特征间非独立同分布时，需要能处理非球形协方差结构的方法。

Method: 提出图正则化PCA，通过学习稀疏精度图，将载荷偏向对应图拉普拉斯的低频傅里叶模式。抑制高频信号，保留图相干的低频信号，得到与条件关系对齐的可解释主成分。

Result: 在多种图拓扑、信噪比和稀疏度的合成数据上评估，相比主流方法，GR-PCA能更好地将方差集中在预期支撑集上，产生更低图拉普拉斯能量的载荷，在样本外重建中保持竞争力。

Conclusion: GR-PCA提供了一种实用的结构感知降维方法，在不牺牲预测性能的情况下提高结构保真度。当高频信号图相关时优势最明显，而PCA在信号接近旋转不变时仍具竞争力。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 论文提出了一个分析AI系统对人类构成生存风险的通用框架，基于两个前提构建了人类存活的分类学，并用于估算AI毁灭人类的概率。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成生存风险的争论不断。本文旨在开发一个系统性的框架来思考AI的生存风险问题，为这一重要辩论提供结构化的分析工具。

Method: 基于两个核心前提构建分析框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。通过这两个前提构建了四种人类存活的"生存故事"分类，每个故事中都有一个前提失败。

Result: 提出了四种人类存活的可能性：1) 科学障碍阻止AI变得极其强大；2) 人类禁止AI研究；3) 极其强大的AI因其目标而不毁灭人类；4) 人类能够可靠检测并禁用有毁灭意图的AI系统。分析了不同生存故事面临的挑战和相应的应对策略。

Conclusion: 不同的生存故事对应不同的风险应对策略，该分类框架可用于估算P(doom)——AI毁灭人类的概率。该框架为理解AI生存风险提供了系统性的思考工具，有助于制定相应的风险缓解策略。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [11] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 论文提出针对计算机使用代理的单次规划方法，通过可信规划器在观察潜在恶意内容前生成完整执行图，提供可证明的控制流完整性保证，解决安全与功能间的矛盾。


<details>
  <summary>Details</summary>
Motivation: AI代理易受提示注入攻击，唯一已知的鲁棒防御是架构隔离。但计算机使用代理需要持续观察UI状态来确定操作，这与安全所需的隔离相冲突，需要解决这一基本矛盾。

Method: 引入单次规划方法：可信规划器在观察任何潜在恶意内容前生成包含条件分支的完整执行图，提供可证明的控制流完整性保证，防止指令注入攻击。

Result: 在OSWorld上评估，架构隔离成功防止指令注入，但需要额外措施防止分支导向攻击。在保持前沿模型57%性能的同时，将较小开源模型性能提升达19%，证明安全与实用性可共存。

Conclusion: 通过单次规划方法，计算机使用代理可以在保持严格安全性的同时实现实用功能，解决了安全隔离与持续UI观察之间的基本矛盾，为安全AI代理设计提供了新方向。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [12] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出了一种针对多步推理任务的定向路由方法，仅在关键步骤（可能破坏解决方案的步骤）使用大模型，而让小模型处理常规步骤，显著提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 多步推理任务（如数学问题解决）容易发生级联失败，单个错误步骤会导致整个解决方案崩溃。现有的LLM路由方法将整个查询分配给一个模型，将所有推理步骤视为同等重要，效率低下。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级别的不确定性和预算约束做出路由决策。开发了从简单阈值策略到更复杂策略的多种路由方法，这些策略考虑长期精度-成本权衡和步骤正确性估计的不确定性。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前的路由方法，成本效率提高5倍；更高级的策略使用80%更少的大模型token就能达到强大昂贵模型的性能。在AIME等更难基准上，TRIM实现了高达6倍的成本效率提升。

Conclusion: 所有方法在数学推理任务上都能有效泛化，表明步骤级别难度代表了推理的基本特征。TRIM通过将昂贵调用限制在关键步骤上，从根本上改变了推理效率。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [13] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个用于HRV解释的临床推理框架，通过8步可追溯推理步骤和Z-score优先级层次结构，有效缓解LLM在生理信号处理中的幻觉问题，在情感分类和临床推理一致性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在心率变异性解释中存在生理幻觉问题，包括呼吸性窦性心律失常污染、非线性指标的短数据不稳定性，以及忽视个体化基线而偏向群体规范。这些问题阻碍了LLM在HRV解释中的临床应用。

Method: 提出C-GRASP（临床基础推理情感信号处理）框架，采用保护性RAG增强管道，将HRV解释分解为8个可追溯推理步骤。核心是Z-score优先级层次结构，强调个体化基线变化优于规范统计。通过自动RSA感知保护机制缓解频谱幻觉。

Result: 在DREAMER数据集的414个试验中，C-GRASP与高规模推理模型（如MedGemma3-thinking）结合，在4类情感分类中达到37.3%准确率，临床推理一致性得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平道路，有效防止了原生LLM中常见的"群体偏见"。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [14] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 本文提出一种利用LLM解码过程中潜在安全信号进行早期检测的方法，有效防御越狱攻击，同时保持良性输入的响应质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过安全对齐，但现有对齐往往较浅，容易受到越狱攻击。现有防御机制（如解码约束和后处理检测器）难以应对复杂越狱攻击，要么检测不鲁棒，要么过度降低模型效用。

Method: 通过观察发现，即使成功越狱，模型在生成过程中内部仍会表现出潜在的安全相关信号。这些信号被模型追求流畅续写的驱动力所覆盖。基于此，提出一种简单有效的方法，在解码过程中显式地提取和利用这些潜在安全信号，实现不安全内容的早期检测。

Result: 在多种越狱攻击上的实验表明，该方法显著增强了安全性，同时在良性输入上保持较低的过度拒绝率，并保持了响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

TL;DR: 提出BHyT（Bounded Hyperbolic Tanh）作为Pre-LN的替代方案，同时解决稳定性和效率问题，通过tanh非线性与显式输入边界控制，防止深度增加时激活值幅度和方差增长，实现更快的训练和更高的生成吞吐量。


<details>
  <summary>Details</summary>
Motivation: Pre-LN虽然是大语言模型的标准选择，但存在效率低下（重复统计计算）和深度诅咒问题（随着层数增加，隐藏状态幅度和方差增大导致训练不稳定）。现有效率导向的无归一化方法（如DyT）在深度较大时仍然脆弱。

Method: 提出BHyT方法，结合tanh非线性与显式数据驱动的输入边界控制，将激活值保持在非饱和范围内。每个块只计算一次精确统计，用轻量级方差近似替代第二次归一化，提高效率。

Result: BHyT在预训练中表现出更好的稳定性和效率，相比RMSNorm平均训练速度提升15.8%，token生成吞吐量平均提高4.2%，同时在语言理解和推理基准测试中匹配或超越其推理性能和鲁棒性。

Conclusion: BHyT作为Pre-LN的即插即用替代方案，能同时解决稳定性和效率问题，为大型语言模型提供更高效、更稳定的训练方案。

Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [Formal Safety Guarantees for Autonomous Vehicles using Barrier Certificates](https://arxiv.org/abs/2601.09740)
*Oumaima Barhoumi,Mohamed H Zaki,Sofiène Tahar*

Main category: cs.RO

TL;DR: 该论文提出了一种结合障碍证书和可解释交通冲突指标的形式化验证安全框架，用于自动驾驶车辆，通过SMT求解器验证安全条件，并在真实高速公路数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统虽然能感知复杂场景并做出实时决策，但通常作为黑箱运行，缺乏可解释性和严格的安全保证。在动态混合交通环境中，与人类驾驶车辆的交互带来了不确定性和安全挑战。

Method: 开发了连接自动驾驶车辆的形式化验证安全框架，将障碍证书与可解释的交通冲突指标（特别是作为时空安全指标的碰撞时间）相结合。使用可满足性模理论求解器验证安全条件，并通过自适应控制机制确保车辆实时遵守这些约束。

Result: 在真实世界高速公路数据集上的评估显示，不安全交互显著减少：碰撞时间低于3秒阈值的事件减少了高达40%，在某些车道中完全消除了冲突。

Conclusion: 该方法提供了可解释且可证明的安全保证，展示了安全自动驾驶的实用且可扩展策略，为自动驾驶系统提供了形式化验证的安全框架。

Abstract: Modern AI technologies enable autonomous vehicles to perceive complex scenes, predict human behavior, and make real-time driving decisions. However, these data-driven components often operate as black boxes, lacking interpretability and rigorous safety guarantees. Autonomous vehicles operate in dynamic, mixed-traffic environments where interactions with human-driven vehicles introduce uncertainty and safety challenges. This work develops a formally verified safety framework for Connected and Autonomous Vehicles (CAVs) that integrates Barrier Certificates (BCs) with interpretable traffic conflict metrics, specifically Time-to-Collision (TTC) as a spatio-temporal safety metric. Safety conditions are verified using Satisfiability Modulo Theories (SMT) solvers, and an adaptive control mechanism ensures vehicles comply with these constraints in real time. Evaluation on real-world highway datasets shows a significant reduction in unsafe interactions, with up to 40\% fewer events where TTC falls below a 3 seconds threshold, and complete elimination of conflicts in some lanes. This approach provides both interpretable and provable safety guarantees, demonstrating a practical and scalable strategy for safe autonomous driving.

</details>


### [17] [CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments](https://arxiv.org/abs/2601.10116)
*Xintong Zhang,Junfeng Chen,Yuxiao Zhu,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: CoCoPlan：一个统一框架，联合优化协作任务规划和团队间歇通信，在有限通信下实现动态时空任务分布的高效协调


<details>
  <summary>Details</summary>
Motivation: 多机器人系统通过协调协作能极大提升效率，但实际中全时通信很少可用，交互仅限于近距离交换。现有方法要么维持全时连接，要么依赖固定调度或成对协议，都无法在有限通信下有效适应动态时空任务分布，导致协调效果不佳。

Method: 提出CoCoPlan框架，包含：1）分支定界架构，联合编码任务分配和通信事件；2）自适应目标函数，平衡任务效率和通信延迟；3）通信事件优化模块，策略性决定何时、何地以及如何重建全局连接。

Result: 实验表明，CoCoPlan优于现有方法：任务完成率提高22.4%，通信开销降低58.6%，可扩展性支持多达100个机器人在动态环境中运行。硬件实验包括复杂的2D办公环境和大型3D灾难响应场景。

Conclusion: CoCoPlan通过联合优化任务规划和间歇通信，解决了有限通信下多机器人协调的挑战，在任务效率、通信开销和可扩展性方面均显著优于现有方法。

Abstract: Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.

</details>


### [18] [The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation](https://arxiv.org/abs/2601.10268)
*Eszter Birtalan,Miklós Koller*

Main category: cs.RO

TL;DR: 研究通过仿真评估6种不同触觉传感器配置对强化学习性能的影响，发现特定配置在两种实验设置中均表现最佳


<details>
  <summary>Details</summary>
Motivation: 触觉传感器在机器人领域应用日益重要，但现有机械手设计中传感器布局和密度差异很大，且占用大量空间。需要评估不同传感器配置对学习性能的影响，为机械手设计提供指导。

Method: 使用仿真评估6种不同密度和布局的触觉传感器配置，采用双实验设置确保结果不依赖于特定物理模拟器、机械手模型或机器学习算法。

Result: 研究发现了特定于实验设置的效应，以及跨6种传感器化仿真的通用效应。识别出一种配置在两种实验设置中均能获得最佳性能。

Conclusion: 该研究结果为未来机械手设计（包括假肢）提供了有价值的指导，有助于优化触觉传感器布局以提高抓取稳定性。

Abstract: Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.

</details>
