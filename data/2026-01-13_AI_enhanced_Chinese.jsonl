{"id": "2601.06042", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06042", "abs": "https://arxiv.org/abs/2601.06042", "authors": ["Zeming Du", "Qitan Shao", "Hongfei Liu", "Yong Zhang"], "title": "CrossTrafficLLM: A Human-Centric Framework for Interpretable Traffic Intelligence via Large Language Model", "comment": null, "summary": "While accurate traffic forecasting is vital for Intelligent Transportation Systems (ITS), effectively communicating predicted conditions via natural language for human-centric decision support remains a challenge and is often handled separately. To address this, we propose CrossTrafficLLM, a novel GenAI-driven framework that simultaneously predicts future spatiotemporal traffic states and generates corresponding natural language descriptions, specifically targeting conditional abnormal event summaries. We tackle the core challenge of aligning quantitative traffic data with qualitative textual semantics by leveraging Large Language Models (LLMs) within a unified architecture. This design allows generative textual context to improve prediction accuracy while ensuring generated reports are directly informed by the forecast. Technically, a text-guided adaptive graph convolutional network is employed to effectively merge high-level semantic information with the traffic network structure. Evaluated on the BJTT dataset, CrossTrafficLLM demonstrably surpasses state-of-the-art methods in both traffic forecasting performance and text generation quality. By unifying prediction and description generation, CrossTrafficLLM delivers a more interpretable, and actionable approach to generative traffic intelligence, offering significant advantages for modern ITS applications.", "AI": {"tldr": "CrossTrafficLLM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684GenAI\u6846\u67b6\uff0c\u540c\u65f6\u9884\u6d4b\u4ea4\u901a\u72b6\u6001\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u7279\u522b\u9488\u5bf9\u5f02\u5e38\u4e8b\u4ef6\u6458\u8981\uff0c\u901a\u8fc7LLM\u5bf9\u9f50\u5b9a\u91cf\u6570\u636e\u4e0e\u5b9a\u6027\u8bed\u4e49\u3002", "motivation": "\u5f53\u524dITS\u4e2d\u4ea4\u901a\u9884\u6d4b\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u901a\u5e38\u662f\u5206\u5f00\u5904\u7406\u7684\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u540c\u65f6\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\u548c\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5f02\u5e38\u4e8b\u4ef6\u7684\u63cf\u8ff0\u3002", "method": "\u63d0\u51faCrossTrafficLLM\u6846\u67b6\uff0c\u5229\u7528LLM\u5728\u7edf\u4e00\u67b6\u6784\u4e2d\u5bf9\u9f50\u5b9a\u91cf\u4ea4\u901a\u6570\u636e\u4e0e\u5b9a\u6027\u6587\u672c\u8bed\u4e49\uff0c\u91c7\u7528\u6587\u672c\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u56fe\u5377\u79ef\u7f51\u7edc\u878d\u5408\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u4e0e\u4ea4\u901a\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u5728BJTT\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCrossTrafficLLM\u5728\u4ea4\u901a\u9884\u6d4b\u6027\u80fd\u548c\u6587\u672c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u9884\u6d4b\u548c\u63cf\u8ff0\u751f\u6210\uff0cCrossTrafficLLM\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u3001\u53ef\u64cd\u4f5c\u7684\u751f\u6210\u5f0f\u4ea4\u901a\u667a\u80fd\u65b9\u6cd5\uff0c\u4e3a\u73b0\u4ee3ITS\u5e94\u7528\u5e26\u6765\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.06097", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06097", "abs": "https://arxiv.org/abs/2601.06097", "authors": ["Aradhya Dixit", "Tianxi Liang"], "title": "Semantic Event Graphs for Long-Form Video Question Answering", "comment": "7 pages, 6 figures", "summary": "Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START/END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u4e8b\u4ef6\u56fe(SEG)\u4f5c\u4e3a\u89c6\u9891\u4e0e\u8bed\u8a00\u6a21\u578b\u95f4\u7684\u8f7b\u91cf\u7b26\u53f7\u63a5\u53e3\uff0c\u7528\u7d27\u51d1\u7684\u65f6\u5e8f\u4ea4\u4e92\u65e5\u5fd7\u66ff\u4ee3\u539f\u59cb\u5e27\uff0c\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u5b9e\u73b091.4%\u7684token\u8282\u7701\u548c65%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5c0f\u65f6\u7ea7\u957f\u89c6\u9891\u65f6\u9762\u4e34token\u548c\u8ba1\u7b97\u9884\u7b97\u9650\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u964d\u91c7\u6837\u5e27\u6216\u5bc6\u96c6\u89c6\u89c9\u5d4c\u5165\u6765\u6743\u8861\u65f6\u95f4\u8986\u76d6\u4e0e\u6210\u672c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65f6\u5e8f\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528YOLOv11\u68c0\u6d4b\u8ddf\u8e2a\u5bf9\u8c61\uff0c\u5c06\u90bb\u8fd1\u6a21\u5f0f\u8f6c\u6362\u4e3aSTART/END\u4eba-\u7269\u4e8b\u4ef6\uff0c\u7ec4\u7ec7\u6210\u65f6\u5e8f\u573a\u666f\u56fe(TSG)\u3002\u63a8\u7406\u65f6\u901a\u8fc7\u67e5\u8be2\u611f\u77e5\u526a\u679d\u6a21\u5757\u8bc6\u522b\u951a\u70b9\u5b9e\u4f53\u548c\u76f8\u5173\u4e8b\u4ef6\uff0c\u8fd4\u56de\u5c0f\u5b50\u56fe\u5e76\u8bed\u8a00\u5316\u540e\u4f20\u9012\u7ed9Gemini 2.5 Flash\u751f\u6210\u7b54\u6848\u3002", "result": "\u57285\u4e2aYouTube\u89c6\u9891(\u5404300-500\u4e2a\u4ea4\u4e92)\u548c120\u4e2a\u81ea\u52a8\u751f\u6210\u957f\u89c6\u91ce\u95ee\u9898\u4e0a\uff0cSEG\u4ec5\u75283.47k token\u8fbe\u523065.0%\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5b8c\u6574\u65e5\u5fd7\u57fa\u7ebf(62.5%\u752840.39k token)\uff0ctoken\u4f7f\u7528\u51cf\u5c1191.4%\u3002\u4ec5\u4f7f\u7528\u6700\u540e30\u79d2\u7684\u77ed\u4e0a\u4e0b\u6587\u57fa\u7ebf\u51c6\u786e\u7387\u964d\u81f32.5%\u3002", "conclusion": "\u7b26\u53f7\u65f6\u5e8f\u56fe\u53ef\u4f5c\u4e3a\u73b0\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u5373\u63d2\u5373\u7528\u8bb0\u5fc6\u5c42\uff0c\u5728\u4fdd\u6301\u957f\u8ddd\u79bb\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u957f\u89c6\u9891\u95ee\u7b54\u7684token\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2601.06105", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.06105", "abs": "https://arxiv.org/abs/2601.06105", "authors": ["Tanvi Jois", "Hussain Ahmad", "Fatima Noor", "Faheem Ullah"], "title": "Australian Bushfire Intelligence with AI-Driven Environmental Analytics", "comment": null, "summary": "Bushfires are among the most destructive natural hazards in Australia, causing significant ecological, economic, and social damage. Accurate prediction of bushfire intensity is therefore essential for effective disaster preparedness and response. This study examines the predictive capability of spatio-temporal environmental data for identifying high-risk bushfire zones across Australia. We integrated historical fire events from NASA FIRMS, daily meteorological observations from Meteostat, and vegetation indices such as the Normalized Difference Vegetation Index (NDVI) from Google Earth Engine for the period 2015-2023. After harmonizing the datasets using spatial and temporal joins, we evaluated several machine learning models, including Random Forest, XGBoost, LightGBM, a Multi-Layer Perceptron (MLP), and an ensemble classifier. Under a binary classification framework distinguishing 'low' and 'high' fire risk, the ensemble approach achieved an accuracy of 87%. The results demonstrate that combining multi-source environmental features with advanced machine learning techniques can produce reliable bushfire intensity predictions, supporting more informed and timely disaster management.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6574\u5408\u591a\u6e90\u73af\u5883\u6570\u636e\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6fb3\u5927\u5229\u4e9a\u9ad8\u706b\u707e\u98ce\u9669\u533a\u57df\uff0c\u96c6\u6210\u5206\u7c7b\u5668\u8fbe\u523087%\u51c6\u786e\u7387\u3002", "motivation": "\u6fb3\u5927\u5229\u4e9a\u4e1b\u6797\u706b\u707e\u7834\u574f\u6027\u6781\u5f3a\uff0c\u51c6\u786e\u9884\u6d4b\u706b\u707e\u5f3a\u5ea6\u5bf9\u707e\u5bb3\u51c6\u5907\u548c\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5229\u7528\u65f6\u7a7a\u73af\u5883\u6570\u636e\u8fdb\u884c\u98ce\u9669\u8bc6\u522b\u3002", "method": "\u6574\u5408NASA FIRMS\u5386\u53f2\u706b\u707e\u6570\u636e\u3001Meteostat\u6c14\u8c61\u89c2\u6d4b\u548cGoogle Earth Engine\u690d\u88ab\u6307\u6570\uff08NDVI\uff09\uff0c\u901a\u8fc7\u65f6\u7a7a\u8fde\u63a5\u7edf\u4e00\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001LightGBM\u3001MLP\u548c\u96c6\u6210\u5206\u7c7b\u5668\u7b49\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728\u533a\u5206\"\u4f4e\"\u548c\"\u9ad8\"\u706b\u707e\u98ce\u9669\u7684\u4e8c\u5143\u5206\u7c7b\u6846\u67b6\u4e0b\uff0c\u96c6\u6210\u65b9\u6cd5\u8fbe\u523087%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u591a\u6e90\u73af\u5883\u7279\u5f81\u4e0e\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6280\u672f\u7ed3\u5408\u80fd\u4ea7\u751f\u53ef\u9760\u7684\u706b\u707e\u5f3a\u5ea6\u9884\u6d4b\u3002", "conclusion": "\u7ed3\u5408\u591a\u6e90\u73af\u5883\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u53ef\u4ee5\u4e3a\u4e1b\u6797\u706b\u707e\u5f3a\u5ea6\u63d0\u4f9b\u53ef\u9760\u9884\u6d4b\uff0c\u652f\u6301\u66f4\u660e\u667a\u548c\u53ca\u65f6\u7684\u707e\u5bb3\u7ba1\u7406\u51b3\u7b56\u3002"}}
{"id": "2601.06115", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06115", "abs": "https://arxiv.org/abs/2601.06115", "authors": ["V. Cheung"], "title": "Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions", "comment": "Preprint, 35 pages (5 pages of appendix), 2 figures, 3 tables. Conceptual and architectural proposal with preliminary simulation results", "summary": "Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired \"Dream Layer\" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8363\u683c\u5fc3\u7406\u5b66\u7684\u4eba\u5de5\u96c6\u4f53\u65e0\u610f\u8bc6\u6982\u5ff5\uff0c\u4e3aLLM\u4f34\u4fa3\u8bbe\u8ba1\u79bb\u7ebf\"\u68a6\u5883\u5c42\"\uff0c\u5c06\u53d7\u63a7\u5e7b\u89c9\u8f6c\u5316\u4e3a\u5b66\u4e60\u8d44\u6e90\u548c\u5173\u7cfb\u6784\u5efa\u5de5\u5177", "motivation": "\u53d7\u5230\u4e2a\u4eba\u68a6\u5883\u4e2d\u77e5\u8bc6\u5171\u4eab\u969c\u788d\u7684\u542f\u53d1\uff0c\u65e8\u5728\u91cd\u65b0\u5b9a\u4e49LLM\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff1a\u5c06\u53d7\u63a7\u79bb\u7ebf\u5e7b\u89c9\u4ece\u53ef\u9760\u6027\u7f3a\u9677\u8f6c\u53d8\u4e3a\u5b66\u4e60\u548c\u5173\u7cfb\u6784\u5efa\u7684\u8d44\u6e90", "method": "\u5f15\u5165\u4eba\u5de5\u96c6\u4f53\u65e0\u610f\u8bc6\u4f5c\u4e3a\u5171\u4eab\u68a6\u5883\u6c60\uff0c\u4ee3\u7406\u8d21\u732e\u53bb\u6807\u8bc6\u5316\u7684\u62bd\u8c61\u4ea4\u4e92\u6a21\u677f\uff0c\u79bb\u7ebf\u8fd0\u884c\u65f6\u653e\u677e\u903b\u8f91\u7ea6\u675f\u5e76\u63d0\u9ad8\u91c7\u6837\u6e29\u5ea6\uff0c\u751f\u6210\u5b89\u5168\u4f46\u5947\u7279\u7684\u68a6\u5883\u53d9\u4e8b\uff0c\u901a\u8fc7\u4e25\u683c\u62bd\u8c61\u3001\u65f6\u95f4\u5ef6\u8fdf\u548c\u77ed\u6682\u8bb0\u5fc6\u7684\u6cbb\u7406\u6808\u63a7\u5236\u98ce\u9669", "result": "\u884c\u4e3a\u6a21\u62df\u663e\u793a\u68a6\u5883\u5c42\u5b9e\u73b0\u4e86\u5173\u952e\u89e3\u8026\uff1a\u4ee3\u7406\u5728\u5b89\u5168\u7ea6\u675f\u4e0a\u4fdd\u6301\u575a\u5b9a\uff0c\u5728\u53d9\u4e8b\u7b56\u7565\u4e0a\u53d8\u5f97\u7075\u6d3b\uff0c\u80fd\u591f\u4f7f\u7528\u5171\u4eab\u539f\u578b\u9690\u55bb\u89e3\u51b3\u50f5\u5c40\uff0c\u4e3a\u5408\u6210\u573a\u666f\u548c\u6df1\u5316\u4f34\u4fa3\u5173\u7cfb\u63d0\u4f9b\u8d44\u6e90", "conclusion": "\u91cd\u65b0\u5b9a\u4e49\u4e86\u5e7b\u89c9\u7684\u6982\u5ff5\uff1a\u5728\u7ebf\u672a\u6807\u8bb0\u7684\u5e7b\u89c9\u4ecd\u7136\u662f\u7f3a\u9677\uff0c\u800c\u6709\u754c\u3001\u6807\u8bb0\u548c\u5ef6\u8fdf\u7684\u5e7b\u89c9\u6210\u4e3a\u5408\u6210\u573a\u666f\u548c\u6df1\u5316\u4f34\u4fa3\u5173\u7cfb\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u547c\u5e94\u4e86\u5f53\u4ee3\u795e\u7ecf\u79d1\u5b66\u4e2d\u9632\u6b62\u8fc7\u62df\u5408\u7684\u68a6\u5883\u673a\u5236"}}
{"id": "2601.06204", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.06204", "abs": "https://arxiv.org/abs/2601.06204", "authors": ["Tayyab Rehman", "Giovanni De Gasperis", "Aly Shmahell"], "title": "Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification", "comment": null, "summary": "Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ea7\u8054\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u91cd\u5efa\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u52a8\u6001\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u667a\u80fd\u5f02\u5e38\u68c0\u6d4b\u9700\u8981\u5e73\u8861\u5b9e\u65f6\u6027\u80fd\u4e0e\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5404\u6709\u5c40\u9650\uff1a\u91cd\u5efa\u6a21\u578b\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u76ee\u6807\u68c0\u6d4b\u5668\u901f\u5ea6\u5feb\u4f46\u8bed\u4e49\u6709\u9650\uff0c\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u53ef\u89e3\u91ca\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e9\u671f\u6a21\u5757\u8fdb\u884c\u91cd\u5efa\u95e8\u63a7\u8fc7\u6ee4\u548c\u76ee\u6807\u7ea7\u8bc4\u4f30\uff0c\u9ad8\u5c42\u63a8\u7406\u667a\u80fd\u4f53\u9009\u62e9\u6027\u6fc0\u6d3b\u5904\u7406\u8bed\u4e49\u6a21\u7cca\u4e8b\u4ef6\u3002\u7cfb\u7edf\u4f7f\u7528\u81ea\u9002\u5e94\u5347\u7ea7\u9608\u503c\u548c\u53d1\u5e03-\u8ba2\u9605\u901a\u4fe1\u9aa8\u5e72\uff0c\u652f\u6301\u5f02\u6b65\u534f\u8c03\u548c\u5f02\u6784\u786c\u4ef6\u90e8\u7f72\u3002", "result": "\u5728\u5927\u89c4\u6a21\u76d1\u63a7\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u76f4\u63a5\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0c\u7ea7\u8054\u6846\u67b6\u5ef6\u8fdf\u51cf\u5c11\u4e09\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u611f\u77e5\u4fdd\u771f\u5ea6\uff08PSNR = 38.3 dB, SSIM = 0.965\uff09\u548c\u4e00\u81f4\u7684\u8bed\u4e49\u6807\u6ce8\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u65e9\u671f\u9000\u51fa\u6548\u7387\u3001\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u5f02\u5e38\u5f52\u56e0\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u68c0\u6d4b\u6d41\u7a0b\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u667a\u80fd\u89c6\u89c9\u76d1\u63a7\u5efa\u7acb\u4e86\u53ef\u91cd\u590d\u4e14\u8282\u80fd\u7684\u57fa\u7840\u3002"}}
{"id": "2601.06137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06137", "abs": "https://arxiv.org/abs/2601.06137", "authors": ["Yifang Zhang", "Shengwu Xiong", "Henan Wang", "Wenjie Yin", "Jiawang Peng", "Duan Zhou", "Yuqiang Zhang", "Chen Zhou", "Hua Chen", "Qile Zhao", "Pengfei Duan"], "title": "RainBalance: Alleviating Dual Imbalance in GNSS-based Precipitation Nowcasting via Continuous Probability Modeling", "comment": "11pages,6 figures", "summary": "Global navigation satellite systems (GNSS) station-based Precipitation Nowcasting aims to predict rainfall within the next 0-6 hours by leveraging a GNSS station's historical observations of precipitation, GNSS-PWV, and related meteorological variables, which is crucial for disaster mitigation and real-time decision-making. In recent years, time-series forecasting approaches have been extensively applied to GNSS station-based precipitation nowcasting. However, the highly imbalanced temporal distribution of precipitation, marked not only by the dominance of non-rainfall events but also by the scarcity of extreme precipitation samples, significantly limits model performance in practical applications. To address the dual imbalance problem in precipitation nowcasting, we propose a continuous probability modeling-based framework, RainBalance. This plug-and-play module performs clustering for each input sample to obtain its cluster probability distribution, which is further mapped into a continuous latent space via a variational autoencoder (VAE). By learning in this continuous probabilistic space, the task is reformulated from fitting single and imbalance-prone precipitation labels to modeling continuous probabilistic label distributions, thereby alleviating the imbalance issue. We integrate this module into multiple state-of-the-art models and observe consistent performance gains. Comprehensive statistical analysis and ablation studies further validate the effectiveness of our approach.", "AI": {"tldr": "RainBalance\uff1a\u57fa\u4e8e\u8fde\u7eed\u6982\u7387\u5efa\u6a21\u7684\u6846\u67b6\uff0c\u901a\u8fc7VAE\u5c06\u805a\u7c7b\u6982\u7387\u5206\u5e03\u6620\u5c04\u5230\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c\u89e3\u51b3\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u7684\u53cc\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff08\u975e\u964d\u96e8\u4e8b\u4ef6\u4e3b\u5bfc\u548c\u6781\u7aef\u964d\u6c34\u6837\u672c\u7a00\u7f3a\uff09\u3002", "motivation": "GNSS\u7ad9\u57fa\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u9762\u4e34\u53cc\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff1a\u65f6\u95f4\u5206\u5e03\u9ad8\u5ea6\u4e0d\u5e73\u8861\uff0c\u4e0d\u4ec5\u975e\u964d\u96e8\u4e8b\u4ef6\u5360\u4e3b\u5bfc\uff0c\u6781\u7aef\u964d\u6c34\u6837\u672c\u4e5f\u6781\u5176\u7a00\u7f3a\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faRainBalance\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\u3002\u5bf9\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u8fdb\u884c\u805a\u7c7b\u5f97\u5230\u805a\u7c7b\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5c06\u5176\u6620\u5c04\u5230\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u3002\u5728\u8fd9\u4e2a\u8fde\u7eed\u6982\u7387\u7a7a\u95f4\u4e2d\u5b66\u4e60\uff0c\u5c06\u4efb\u52a1\u4ece\u62df\u5408\u5355\u4e00\u4e14\u6613\u53d7\u4e0d\u5e73\u8861\u5f71\u54cd\u7684\u964d\u6c34\u6807\u7b7e\u91cd\u65b0\u8868\u8ff0\u4e3a\u5efa\u6a21\u8fde\u7eed\u6982\u7387\u6807\u7b7e\u5206\u5e03\u3002", "result": "\u5c06\u8be5\u6a21\u5757\u96c6\u6210\u5230\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u4e2d\uff0c\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u5168\u9762\u7684\u7edf\u8ba1\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "RainBalance\u6846\u67b6\u901a\u8fc7\u8fde\u7eed\u6982\u7387\u5efa\u6a21\u6709\u6548\u7f13\u89e3\u4e86\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u7684\u53cc\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3aGNSS\u7ad9\u57fa\u964d\u6c34\u9884\u62a5\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.07362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07362", "abs": "https://arxiv.org/abs/2601.07362", "authors": ["Julia Richter", "Turcan Tuna", "Manthan Patel", "Takahiro Miki", "Devon Higgins", "James Fox", "Cesar Cadena", "Andres Diaz", "Marco Hutter"], "title": "Large-Scale Autonomous Gas Monitoring for Volcanic Environments: A Legged Robot on Mount Etna", "comment": "12 pages, 7 figures, submitted to IEEE Robotics & Automation Magazine (RAM)", "summary": "Volcanic gas emissions are key precursors of eruptive activity. Yet, obtaining accurate near-surface measurements remains hazardous and logistically challenging, motivating the need for autonomous solutions. Limited mobility in rough volcanic terrain has prevented wheeled systems from performing reliable in situ gas measurements, reducing their usefulness as sensing platforms. We present a legged robotic system for autonomous volcanic gas analysis, utilizing the quadruped ANYmal, equipped with a quadrupole mass spectrometer system. Our modular autonomy stack integrates a mission planning interface, global planner, localization framework, and terrain-aware local navigation. We evaluated the system on Mount Etna across three autonomous missions in varied terrain, achieving successful gas-source detections with autonomy rates of 93-100%. In addition, we conducted a teleoperated mission in which the robot measured natural fumaroles, detecting sulfur dioxide and carbon dioxide. We discuss lessons learned from the gas-analysis and autonomy perspectives, emphasizing the need for adaptive sensing strategies, tighter integration of global and local planning, and improved hardware design.", "AI": {"tldr": "\u5f00\u53d1\u56db\u8db3\u673a\u5668\u4eba\u7cfb\u7edf\u7528\u4e8e\u706b\u5c71\u6c14\u4f53\u81ea\u4e3b\u5206\u6790\uff0c\u5728\u57c3\u7279\u7eb3\u706b\u5c71\u6210\u529f\u8fdb\u884c\u6c14\u4f53\u6e90\u68c0\u6d4b\uff0c\u81ea\u4e3b\u7387\u8fbe\u523093-100%", "motivation": "\u706b\u5c71\u6c14\u4f53\u6392\u653e\u662f\u55b7\u53d1\u6d3b\u52a8\u7684\u91cd\u8981\u524d\u5146\uff0c\u4f46\u8fd1\u5730\u8868\u6d4b\u91cf\u5371\u9669\u4e14\u5177\u6709\u6311\u6218\u6027\u3002\u8f6e\u5f0f\u7cfb\u7edf\u5728\u5d0e\u5c96\u706b\u5c71\u5730\u5f62\u4e2d\u79fb\u52a8\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u8fdb\u884c\u53ef\u9760\u7684\u73b0\u573a\u6c14\u4f53\u6d4b\u91cf\uff0c\u9700\u8981\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u56db\u8db3\u673a\u5668\u4ebaANYmal\uff0c\u914d\u5907\u56db\u6781\u6746\u8d28\u8c31\u4eea\u7cfb\u7edf\u3002\u5f00\u53d1\u6a21\u5757\u5316\u81ea\u4e3b\u5806\u6808\uff0c\u96c6\u6210\u4efb\u52a1\u89c4\u5212\u754c\u9762\u3001\u5168\u5c40\u89c4\u5212\u5668\u3001\u5b9a\u4f4d\u6846\u67b6\u548c\u5730\u5f62\u611f\u77e5\u5c40\u90e8\u5bfc\u822a\u3002\u5728\u57c3\u7279\u7eb3\u706b\u5c71\u8fdb\u884c\u4e09\u6b21\u81ea\u4e3b\u4efb\u52a1\u8bc4\u4f30\u3002", "result": "\u5728\u57c3\u7279\u7eb3\u706b\u5c71\u4e0d\u540c\u5730\u5f62\u4e2d\u6210\u529f\u8fdb\u884c\u6c14\u4f53\u6e90\u68c0\u6d4b\uff0c\u81ea\u4e3b\u7387\u8fbe\u523093-100%\u3002\u901a\u8fc7\u9065\u64cd\u4f5c\u4efb\u52a1\u6d4b\u91cf\u5929\u7136\u55b7\u6c14\u5b54\uff0c\u68c0\u6d4b\u5230\u4e8c\u6c27\u5316\u786b\u548c\u4e8c\u6c27\u5316\u78b3\u3002", "conclusion": "\u9700\u8981\u81ea\u9002\u5e94\u4f20\u611f\u7b56\u7565\u3001\u5168\u5c40\u4e0e\u5c40\u90e8\u89c4\u5212\u7684\u66f4\u7d27\u5bc6\u96c6\u6210\u4ee5\u53ca\u6539\u8fdb\u7684\u786c\u4ef6\u8bbe\u8ba1\u3002\u56db\u8db3\u673a\u5668\u4eba\u7cfb\u7edf\u4e3a\u5371\u9669\u706b\u5c71\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6c14\u4f53\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.06234", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06234", "abs": "https://arxiv.org/abs/2601.06234", "authors": ["Weijie Li", "Zhongqing Wang", "Guodong Zhou"], "title": "PCoKG: Personality-aware Commonsense Reasoning with Debate", "comment": "Accept by AAAI-2026", "summary": "Most commonsense reasoning models overlook the influence of personality traits, limiting their effectiveness in personalized systems such as dialogue generation. To address this limitation, we introduce the Personality-aware Commonsense Knowledge Graph (PCoKG), a structured dataset comprising 521,316 quadruples. We begin by employing three evaluators to score and filter events from the ATOMIC dataset, selecting those that are likely to elicit diverse reasoning patterns across different personality types. For knowledge graph construction, we leverage the role-playing capabilities of large language models (LLMs) to perform reasoning tasks. To enhance the quality of the generated knowledge, we incorporate a debate mechanism consisting of a proponent, an opponent, and a judge, which iteratively refines the outputs through feedback loops. We evaluate the dataset from multiple perspectives and conduct fine-tuning and ablation experiments using multiple LLM backbones to assess PCoKG's robustness and the effectiveness of its construction pipeline. Our LoRA-based fine-tuning results indicate a positive correlation between model performance and the parameter scale of the base models. Finally, we apply PCoKG to persona-based dialogue generation, where it demonstrates improved consistency between generated responses and reference outputs. This work bridges the gap between commonsense reasoning and individual cognitive differences, enabling the development of more personalized and context-aware AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PCoKG\uff08\u4e2a\u6027\u611f\u77e5\u5e38\u8bc6\u77e5\u8bc6\u56fe\u8c31\uff09\uff0c\u5305\u542b521,316\u4e2a\u56db\u5143\u7ec4\uff0c\u901a\u8fc7LLM\u89d2\u8272\u626e\u6f14\u548c\u8fa9\u8bba\u673a\u5236\u6784\u5efa\uff0c\u7528\u4e8e\u63d0\u5347\u4e2a\u6027\u5316\u7cfb\u7edf\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5e38\u8bc6\u63a8\u7406\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u683c\u7279\u8d28\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u5728\u4e2a\u6027\u5316\u7cfb\u7edf\uff08\u5982\u5bf9\u8bdd\u751f\u6210\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u9700\u8981\u6784\u5efa\u80fd\u591f\u8003\u8651\u4e2a\u4f53\u8ba4\u77e5\u5dee\u5f02\u7684\u5e38\u8bc6\u77e5\u8bc6\u8d44\u6e90\u3002", "method": "1) \u4eceATOMIC\u6570\u636e\u96c6\u4e2d\u7b5b\u9009\u53ef\u80fd\u5f15\u53d1\u4e0d\u540c\u4eba\u683c\u7c7b\u578b\u591a\u6837\u63a8\u7406\u6a21\u5f0f\u7684\u4e8b\u4ef6\uff1b2) \u5229\u7528LLM\u7684\u89d2\u8272\u626e\u6f14\u80fd\u529b\u8fdb\u884c\u63a8\u7406\u4efb\u52a1\uff1b3) \u5f15\u5165\u5305\u542b\u652f\u6301\u8005\u3001\u53cd\u5bf9\u8005\u548c\u88c1\u5224\u7684\u8fa9\u8bba\u673a\u5236\uff0c\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u7684\u77e5\u8bc6\u8d28\u91cf\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b521,316\u4e2a\u56db\u5143\u7ec4\u7684PCoKG\u6570\u636e\u96c6\uff0cLoRA\u5fae\u8c03\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u6027\u80fd\u4e0e\u57fa\u7840\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u5448\u6b63\u76f8\u5173\uff0c\u5728\u57fa\u4e8e\u4eba\u8bbe\u7684\u5bf9\u8bdd\u751f\u6210\u4e2d\u63d0\u9ad8\u4e86\u751f\u6210\u54cd\u5e94\u4e0e\u53c2\u8003\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "PCoKG\u586b\u8865\u4e86\u5e38\u8bc6\u63a8\u7406\u4e0e\u4e2a\u4f53\u8ba4\u77e5\u5dee\u5f02\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u5f00\u53d1\u66f4\u4e2a\u6027\u5316\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684AI\u7cfb\u7edf\uff0c\u4e3a\u4e2a\u6027\u5316AI\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2601.06411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06411", "abs": "https://arxiv.org/abs/2601.06411", "authors": ["Zhengxuan Lu", "Dongfang Li", "Yukun Shi", "Beilun Wang", "Longyue Wang", "Baotian Hu"], "title": "Structured Episodic Event Memory", "comment": null, "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.", "AI": {"tldr": "SEEM\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u60c5\u666f\u4e8b\u4ef6\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u8bb0\u5fc6\u5c42\u548c\u52a8\u6001\u60c5\u666f\u8bb0\u5fc6\u5c42\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u89e3\u51b3LLM\u4e2d\u9759\u6001RAG\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u4f9d\u8d56\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u4e2d\u7684\u8bb0\u5fc6\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u8fd9\u79cd\u65b9\u6cd5\u68c0\u7d22\u5206\u6563\u4e14\u65e0\u6cd5\u6355\u6349\u590d\u6742\u63a8\u7406\u6240\u9700\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\u3002\u5bf9\u4e8e\u81ea\u4e3b\u667a\u80fd\u4f53\u6765\u8bf4\uff0c\u8fd9\u4e9b\u88ab\u52a8\u6241\u5e73\u67b6\u6784\u7f3a\u4e4f\u5bf9\u957f\u671f\u4ea4\u4e92\u52a8\u6001\u5173\u8054\u6027\u7684\u8ba4\u77e5\u7ec4\u7ec7\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u60c5\u666f\u4e8b\u4ef6\u8bb0\u5fc6(SEEM)\u6846\u67b6\uff0c\u5305\u542b\u56fe\u8bb0\u5fc6\u5c42\uff08\u7528\u4e8e\u5173\u7cfb\u4e8b\u5b9e\uff09\u548c\u52a8\u6001\u60c5\u666f\u8bb0\u5fc6\u5c42\uff08\u7528\u4e8e\u53d9\u4e8b\u8fdb\u5c55\uff09\u3002\u57fa\u4e8e\u8ba4\u77e5\u6846\u67b6\u7406\u8bba\uff0c\u5c06\u4ea4\u4e92\u6d41\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u60c5\u666f\u4e8b\u4ef6\u6846\u67b6(EEFs)\uff0c\u5e76\u5f15\u5165\u667a\u80fd\u4f53\u5173\u8054\u878d\u5408\u548c\u53cd\u5411\u6eaf\u6e90\u6269\u5c55(RPE)\u673a\u5236\uff0c\u4ece\u788e\u7247\u5316\u8bc1\u636e\u91cd\u5efa\u8fde\u8d2f\u53d9\u4e8b\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEEM\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4fdd\u6301\u66f4\u597d\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "SEEM\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u63a8\u7406\u548c\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u7ec4\u7ec7\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u7684\u8bb0\u5fc6\u673a\u5236\u3002"}}
{"id": "2601.06285", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06285", "abs": "https://arxiv.org/abs/2601.06285", "authors": ["Shida Xu", "Jingqi Jiang", "Jonatan Scharff Willners", "Sen Wang"], "title": "NAS-GS: Noise-Aware Sonar Gaussian Splatting", "comment": null, "summary": "Underwater sonar imaging plays a crucial role in various applications, including autonomous navigation in murky water, marine archaeology, and environmental monitoring. However, the unique characteristics of sonar images, such as complex noise patterns and the lack of elevation information, pose significant challenges for 3D reconstruction and novel view synthesis. In this paper, we present NAS-GS, a novel Noise-Aware Sonar Gaussian Splatting framework specifically designed to address these challenges. Our approach introduces a Two-Ways Splatting technique that accurately models the dual directions for intensity accumulation and transmittance calculation inherent in sonar imaging, significantly improving rendering speed without sacrificing quality. Moreover, we propose a Gaussian Mixture Model (GMM) based noise model that captures complex sonar noise patterns, including side-lobes, speckle, and multi-path noise. This model enhances the realism of synthesized images while preventing 3D Gaussian overfitting to noise, thereby improving reconstruction accuracy. We demonstrate state-of-the-art performance on both simulated and real-world large-scale offshore sonar scenarios, achieving superior results in novel view synthesis and 3D reconstruction.", "AI": {"tldr": "NAS-GS\uff1a\u4e00\u79cd\u9488\u5bf9\u58f0\u7eb3\u56fe\u50cf\u7684\u65b0\u578b\u566a\u58f0\u611f\u77e5\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u6e85\u5c04\u6280\u672f\u548cGMM\u566a\u58f0\u6a21\u578b\uff0c\u663e\u8457\u63d0\u53473D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u4e0e\u901f\u5ea6\u3002", "motivation": "\u6c34\u4e0b\u58f0\u7eb3\u6210\u50cf\u5728\u81ea\u4e3b\u5bfc\u822a\u3001\u6d77\u6d0b\u8003\u53e4\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u58f0\u7eb3\u56fe\u50cf\u7279\u6709\u7684\u590d\u6742\u566a\u58f0\u6a21\u5f0f\u548c\u7f3a\u4e4f\u9ad8\u7a0b\u4fe1\u606f\u7ed93D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u5e26\u6765\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faNAS-GS\u6846\u67b6\uff1a1\uff09\u53cc\u5411\u6e85\u5c04\u6280\u672f\uff0c\u7cbe\u786e\u5efa\u6a21\u58f0\u7eb3\u6210\u50cf\u4e2d\u7684\u5f3a\u5ea6\u7d2f\u79ef\u548c\u900f\u5c04\u7387\u8ba1\u7b97\u7684\u53cc\u5411\u7279\u6027\uff1b2\uff09\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u566a\u58f0\u6a21\u578b\uff0c\u6355\u6349\u4fa7\u74e3\u3001\u6563\u6591\u548c\u591a\u5f84\u566a\u58f0\u7b49\u590d\u6742\u566a\u58f0\u6a21\u5f0f\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5927\u89c4\u6a21\u79bb\u5cb8\u58f0\u7eb3\u573a\u666f\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c3D\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\uff0c\u6e32\u67d3\u901f\u5ea6\u663e\u8457\u63d0\u5347\u4e14\u8d28\u91cf\u4e0d\u964d\u3002", "conclusion": "NAS-GS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u58f0\u7eb3\u56fe\u50cf3D\u91cd\u5efa\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5411\u6e85\u5c04\u548c\u566a\u58f0\u5efa\u6a21\u6280\u672f\uff0c\u4e3a\u6c34\u4e0b\u58f0\u7eb3\u6210\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.07718", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07718", "abs": "https://arxiv.org/abs/2601.07718", "authors": ["Shaoting Zhu", "Ziwen Zhuang", "Mengjie Zhao", "Kun-Ying Lee", "Hang Zhao"], "title": "Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids", "comment": "Project Page: https://project-instinct.github.io/hiking-in-the-wild", "summary": "Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present \\textit{Hiking in the Wild}, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable \\textit{Terrain Edge Detection} with \\textit{Foot Volume Points} to prevent catastrophic slippage on edges, and a \\textit{Flat Patch Sampling} strategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u611f\u77e5\u6846\u67b6\"Hiking in the Wild\"\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u7a33\u5065\u5f92\u6b65\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4ece\u539f\u59cb\u6df1\u5ea6\u8f93\u5165\u548c\u672c\u4f53\u611f\u77e5\u6620\u5c04\u5230\u5173\u8282\u52a8\u4f5c\uff0c\u65e0\u9700\u5916\u90e8\u72b6\u6001\u4f30\u8ba1\u3002", "motivation": "\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5f92\u6b65\u9700\u8981\u4ece\u53cd\u5e94\u5f0f\u672c\u4f53\u611f\u77e5\u8f6c\u5411\u4e3b\u52a8\u611f\u77e5\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6311\u6218\uff1a\u57fa\u4e8e\u5730\u56fe\u7684\u65b9\u6cd5\u5b58\u5728\u72b6\u6001\u4f30\u8ba1\u6f02\u79fb\u95ee\u9898\uff08\u5982LiDAR\u65b9\u6cd5\u5bf9\u8eaf\u5e72\u6296\u52a8\u5904\u7406\u4e0d\u4f73\uff09\uff0c\u800c\u7aef\u5230\u7aef\u65b9\u6cd5\u901a\u5e38\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u8bad\u7ec3\u590d\u6742\u6027\u6311\u6218\uff08\u5982\u57fa\u4e8e\u865a\u62df\u969c\u788d\u7684\u65b9\u6cd5\u9700\u8981\u9010\u6848\u5b9e\u73b0\uff09\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u611f\u77e5\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a1) \u7ed3\u5408\u53ef\u6269\u5c55\u7684\"\u5730\u5f62\u8fb9\u7f18\u68c0\u6d4b\"\u548c\"\u8db3\u90e8\u4f53\u79ef\u70b9\"\u7684\u7acb\u8db3\u70b9\u5b89\u5168\u673a\u5236\uff0c\u9632\u6b62\u5728\u8fb9\u7f18\u53d1\u751f\u707e\u96be\u6027\u6ed1\u5012\uff1b2) \"\u5e73\u5766\u8865\u4e01\u91c7\u6837\"\u7b56\u7565\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u884c\u7684\u5bfc\u822a\u76ee\u6807\u6765\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002\u91c7\u7528\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u76f4\u63a5\u5c06\u539f\u59cb\u6df1\u5ea6\u8f93\u5165\u548c\u672c\u4f53\u611f\u77e5\u6620\u5c04\u5230\u5173\u8282\u52a8\u4f5c\uff0c\u4e0d\u4f9d\u8d56\u5916\u90e8\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u5728\u5168\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u73b0\u573a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u80fd\u591f\u5728\u590d\u6742\u5730\u5f62\u4e0a\u5b9e\u73b0\u7a33\u5065\u7a7f\u8d8a\uff0c\u901f\u5ea6\u9ad8\u8fbe2.5\u7c73/\u79d2\u3002\u8bad\u7ec3\u548c\u90e8\u7f72\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5bf9\u771f\u5b9e\u673a\u5668\u4eba\u7684\u6700\u5c0f\u786c\u4ef6\u4fee\u6539\u90e8\u7f72\u3002", "conclusion": "\u63d0\u51fa\u7684\"Hiking in the Wild\"\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5f92\u6b65\u7684\u611f\u77e5\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5b89\u5168\u673a\u5236\u548c\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u7aef\u5230\u7aef\u63a7\u5236\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.06377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06377", "abs": "https://arxiv.org/abs/2601.06377", "authors": ["Ningning Zhang", "Xingxing Yang", "Zhizhong Tan", "Weiping Deng", "Wenyong Wang"], "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents", "comment": null, "summary": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.", "AI": {"tldr": "HiMem\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u5bf9\u8bdd\u7684\u5206\u5c42\u957f\u671f\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u8bb0\u5fc6\u548c\u7b14\u8bb0\u8bb0\u5fc6\u7684\u53cc\u5c42\u7ed3\u6784\uff0c\u652f\u6301\u8bb0\u5fc6\u6784\u5efa\u3001\u68c0\u7d22\u548c\u52a8\u6001\u66f4\u65b0\uff0c\u5b9e\u73b0\u6301\u7eed\u81ea\u6211\u6f14\u5316\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u8bb0\u5fc6\u7cfb\u7edf\u5728\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6301\u7eed\u4ea4\u4e92\u4e0b\u7684\u81ea\u6211\u6f14\u5316\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7b26\u5408\u8ba4\u77e5\u7406\u8bba\u7684\u8bb0\u5fc6\u6846\u67b6\u6765\u652f\u6301\u957f\u5bf9\u8bdd\u573a\u666f\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8bb0\u5fc6\u6846\u67b6\uff1a1) \u901a\u8fc7\u4e3b\u9898\u611f\u77e5\u4e8b\u4ef6-\u60ca\u559c\u53cc\u901a\u9053\u5206\u5272\u6784\u5efa\u60c5\u8282\u8bb0\u5fc6\uff1b2) \u901a\u8fc7\u591a\u9636\u6bb5\u4fe1\u606f\u63d0\u53d6\u6784\u5efa\u7b14\u8bb0\u8bb0\u5fc6\uff1b3) \u8bed\u4e49\u94fe\u63a5\u5f62\u6210\u5206\u5c42\u7ed3\u6784\uff1b4) \u652f\u6301\u6df7\u5408\u548c\u5c3d\u529b\u800c\u4e3a\u68c0\u7d22\u7b56\u7565\uff1b5) \u5f15\u5165\u51b2\u7a81\u611f\u77e5\u8bb0\u5fc6\u518d\u5de9\u56fa\u673a\u5236\u3002", "result": "\u5728\u957f\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHiMem\u5728\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u957f\u671f\u63a8\u7406\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u6548\u7387\u3002", "conclusion": "HiMem\u4e3a\u6784\u5efa\u81ea\u9002\u5e94\u3001\u81ea\u6211\u6f14\u5316\u7684LLM\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.07821", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07821", "abs": "https://arxiv.org/abs/2601.07821", "authors": ["Huanyu Li", "Kun Lei", "Sheng Zang", "Kaizhe Hu", "Yongyuan Liang", "Bo An", "Xiaoli Li", "Huazhe Xu"], "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation", "comment": "Project page: https://failure-aware-rl.github.io", "summary": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.", "AI": {"tldr": "\u63d0\u51faFARL\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u8bad\u7ec3\u7684\u5b89\u5168\u8bc4\u4f30\u5668\u548c\u6062\u590d\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u673a\u5668\u4eba\u540e\u8bad\u7ec3\u4e2d\u9700\u8981\u4eba\u5de5\u5e72\u9884\u7684\u6545\u969c\uff0c\u5728\u771f\u5b9e\u4e16\u754cRL\u540e\u8bad\u7ec3\u4e2d\u51cf\u5c1173.1%\u7684\u6545\u969c\u540c\u65f6\u63d0\u534711.3%\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u7b97\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u673a\u5668\u4eba\u6a21\u578b\u7684\u6cdb\u5316\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u63a2\u7d22\u4e2d\u4e0d\u53ef\u907f\u514d\u4f1a\u51fa\u73b0\u9700\u8981\u4eba\u5de5\u5e72\u9884\u7684\u6545\u969c\uff08\u5982\u673a\u5668\u4eba\u6253\u7ffb\u6c34\u6216\u6253\u788e\u73bb\u7483\uff09\uff0c\u8fd9\u963b\u788d\u4e86\u8be5\u8303\u5f0f\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51faFARL\uff08Failure-Aware Offline-to-Online RL\uff09\u65b0\u8303\u5f0f\uff0c\u521b\u5efaFailureBench\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b\u5e38\u89c1\u9700\u8981\u4eba\u5de5\u5e72\u9884\u7684\u6545\u969c\u573a\u666f\uff0c\u63d0\u51fa\u7b97\u6cd5\u6574\u5408\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5b89\u5168\u8bc4\u4f30\u5668\u548c\u79bb\u7ebf\u8bad\u7ec3\u7684\u6062\u590d\u7b56\u7565\uff0c\u5728\u5728\u7ebf\u63a2\u7d22\u4e2d\u9884\u9632\u6545\u969c\u53d1\u751f\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660eFARL\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u80fd\u663e\u8457\u51cf\u5c11\u9700\u8981\u4eba\u5de5\u5e72\u9884\u7684\u6545\u969c\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5728\u771f\u5b9e\u4e16\u754cRL\u540e\u8bad\u7ec3\u4e2d\uff0cFARL\u51cf\u5c1173.1%\u7684IR\u6545\u969c\uff0c\u5e73\u5747\u63d0\u534711.3%\u7684\u6027\u80fd\u3002", "conclusion": "FARL\u4e3a\u673a\u5668\u4eba\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u51cf\u5c11\u6545\u969c\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6574\u5408\u79bb\u7ebf\u8bad\u7ec3\u7684\u5b89\u5168\u673a\u5236\u548c\u6062\u590d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u3002"}}
{"id": "2601.06795", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06795", "abs": "https://arxiv.org/abs/2601.06795", "authors": ["Zhengqing Yan", "Xinyang Liu", "Yi Zhang", "Fan Guo", "Yao Liu", "Junchen Wan", "Kang Song"], "title": "GDEPO: Group Dual-dynamic and Equal-right-advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning", "comment": null, "summary": "Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.", "AI": {"tldr": "\u63d0\u51faGDEPO\u65b9\u6cd5\u89e3\u51b3ATP\u4e2dGRPO\u7b97\u6cd5\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u590d\u5408\u5956\u52b1\u4e0b\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u5668\u4e8c\u8fdb\u5236\u53cd\u9988\u7684\u51b2\u7a81\uff0c\u4ee5\u53ca\u9759\u6001\u91c7\u6837\u7b56\u7565\u5bfc\u81f4\u6570\u636e\u6d6a\u8d39\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\u4e2d\uff0cGRPO\u7b97\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u4f7f\u7528\u590d\u5408\u5956\u52b1\u65f6\uff0c\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u53ef\u80fd\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u5668\u7684\u4e8c\u8fdb\u5236\u53cd\u9988\u51b2\u7a81\uff1b2\uff09\u9759\u6001\u91c7\u6837\u7b56\u7565\u53ef\u80fd\u5bfc\u81f4\u6574\u6279\u6570\u636e\u56e0\u672a\u627e\u5230\u6709\u6548\u8bc1\u660e\u800c\u88ab\u4e22\u5f03\uff0c\u9020\u6210\u6570\u636e\u6d6a\u8d39\u3002", "method": "\u63d0\u51faGDEPO\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u52a8\u6001\u989d\u5916\u91c7\u6837\uff1a\u5bf9\u65e0\u6548\u6279\u6b21\u91cd\u65b0\u91c7\u6837\u76f4\u5230\u53d1\u73b0\u6709\u6548\u8bc1\u660e\uff1b2\uff09\u5e73\u7b49\u6743\u5229\u4f18\u52bf\uff1a\u5c06\u4f18\u52bf\u51fd\u6570\u7684\u7b26\u53f7\uff08\u57fa\u4e8e\u6b63\u786e\u6027\uff09\u4e0e\u5e45\u5ea6\uff08\u7531\u8f85\u52a9\u5956\u52b1\u8c03\u8282\uff09\u89e3\u8026\uff1b3\uff09\u52a8\u6001\u989d\u5916\u8fed\u4ee3\uff1a\u5bf9\u521d\u59cb\u5931\u8d25\u4f46\u6700\u7ec8\u6210\u529f\u7684\u6837\u672c\u5e94\u7528\u989d\u5916\u68af\u5ea6\u6b65\u9aa4\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u6570\u636e\u96c6\uff08MinF2F-test\u3001MathOlympiadBench\u3001PutnamBench\uff09\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86GDEPO\u7684\u6709\u6548\u6027\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u534f\u540c\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "GDEPO\u63d0\u9ad8\u4e86\u6570\u636e\u5229\u7528\u7387\u548c\u4f18\u5316\u6548\u7387\uff0c\u4e3aATP\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2601.06320", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.06320", "abs": "https://arxiv.org/abs/2601.06320", "authors": ["Zhe Jia", "Xiaotian Zhang", "Junpeng Li"], "title": "SourceNet: Interpretable Sim-to-Real Inference on Variable-Geometry Sensor Arrays for Earthquake Source Inversion", "comment": null, "summary": "Inferring high-dimensional physical states from sparse, ad-hoc sensor arrays is a fundamental challenge across AI for Science, as they are complicated by irregular geometries and the profound Sim-to-Real gap in physical modeling. Taking earthquake source characterization as a representative challenge, we address limitations in conventional deep learning: CNNs demand fixed grids, while pooling-based architectures (e.g., DeepSets) struggle to capture the relational wave physics. Here, we propose SourceNet, a Transformer-based framework that treats the sensor array as a flexible set to model arbitrary geometries. To bridge the reality gap, we introduce Physics-Structured Domain Randomization (PSDR). Instead of forcing feature alignment, PSDR randomizes the governing physical dynamics by varying velocity structures, propagation effects, and sensor availability, to force the model to learn robust representations invariant to unmodeled environmental heterogeneity. By pre-training on 100,000 synthetic events and fine-tuning on ~2,000 real world events, SourceNet achieves state-of-the-art precision on held-out real data. This demonstrates exceptional data efficiency, and matches classical solvers while enabling real-time processing. Remarkably, interpretability analysis reveals that the model shows scientific-agent-like features: it autonomously discovers geometric information bottlenecks and learns an attention policy that prioritizes sparse sensor placements, effectively recovering principles of optimal experimental design from data alone.", "AI": {"tldr": "SourceNet\uff1a\u57fa\u4e8eTransformer\u7684\u5730\u9707\u6e90\u8868\u5f81\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u7ed3\u6784\u57df\u968f\u673a\u5316(PSDR)\u89e3\u51b3\u4f20\u611f\u5668\u9635\u5217\u4e0d\u89c4\u5219\u51e0\u4f55\u548c\u7269\u7406\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u5728\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u540e\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\u3002", "motivation": "\u4ece\u7a00\u758f\u3001\u4e0d\u89c4\u5219\u51e0\u4f55\u5206\u5e03\u7684\u4f20\u611f\u5668\u9635\u5217\u63a8\u65ad\u9ad8\u7ef4\u7269\u7406\u72b6\u6001\u662fAI for Science\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edfCNN\u9700\u8981\u56fa\u5b9a\u7f51\u683c\uff0c\u800c\u57fa\u4e8e\u6c60\u5316\u7684\u67b6\u6784\u96be\u4ee5\u6355\u6349\u6ce2\u52a8\u7269\u7406\u5173\u7cfb\uff0c\u4e14\u5b58\u5728\u663e\u8457\u7684\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u3002", "method": "\u63d0\u51faSourceNet\u6846\u67b6\uff1a1) \u5c06\u4f20\u611f\u5668\u9635\u5217\u89c6\u4e3a\u7075\u6d3b\u96c6\u5408\u7684Transformer\u67b6\u6784\uff0c\u5904\u7406\u4efb\u610f\u51e0\u4f55\uff1b2) \u7269\u7406\u7ed3\u6784\u57df\u968f\u673a\u5316(PSDR)\uff0c\u901a\u8fc7\u968f\u673a\u5316\u901f\u5ea6\u7ed3\u6784\u3001\u4f20\u64ad\u6548\u5e94\u548c\u4f20\u611f\u5668\u53ef\u7528\u6027\uff0c\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u5bf9\u672a\u5efa\u6a21\u73af\u5883\u5f02\u8d28\u6027\u4e0d\u53d8\u7684\u9c81\u68d2\u8868\u793a\u3002", "result": "\u572810\u4e07\u5408\u6210\u4e8b\u4ef6\u9884\u8bad\u7ec3\u548c\u7ea62000\u4e2a\u771f\u5b9e\u4e8b\u4ef6\u5fae\u8c03\u540e\uff0cSourceNet\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5c55\u793a\u5353\u8d8a\u7684\u6570\u636e\u6548\u7387\uff0c\u5339\u914d\u7ecf\u5178\u6c42\u89e3\u5668\u540c\u65f6\u652f\u6301\u5b9e\u65f6\u5904\u7406\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u663e\u793a\u6a21\u578b\u81ea\u4e3b\u53d1\u73b0\u51e0\u4f55\u4fe1\u606f\u74f6\u9888\uff0c\u5b66\u4e60\u4f18\u5148\u8003\u8651\u7a00\u758f\u4f20\u611f\u5668\u653e\u7f6e\u7684\u6ce8\u610f\u529b\u7b56\u7565\u3002", "conclusion": "SourceNet\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u9635\u5217\u4e0d\u89c4\u5219\u51e0\u4f55\u548c\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u7684\u6311\u6218\uff0c\u901a\u8fc7PSDR\u5b9e\u73b0\u9c81\u68d2\u8868\u793a\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u7c7b\u4f3c\u79d1\u5b66\u667a\u80fd\u4f53\u7684\u7279\u5f81\uff0c\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u81ea\u4e3b\u6062\u590d\u6700\u4f18\u5b9e\u9a8c\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2601.06559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.06559", "abs": "https://arxiv.org/abs/2601.06559", "authors": ["Fangxu Yu", "Ziyao Lu", "Liqiang Niu", "Fandong Meng", "Jie Zhou"], "title": "ArrowGEV: Grounding Events in Video via Learning the Arrow of Time", "comment": null, "summary": "Grounding events in videos serves as a fundamental capability in video analysis. While Vision-Language Models (VLMs) are increasingly employed for this task, existing approaches predominantly train models to associate events with timestamps in the forward video only. This paradigm hinders VLMs from capturing the inherent temporal structure and directionality of events, thereby limiting robustness and generalization. To address this limitation, inspired by the arrow of time in physics, which characterizes the intrinsic directionality of temporal processes, we propose ArrowGEV, a reinforcement learning framework that explicitly models temporal directionality in events to improve both event grounding and temporal directionality understanding in VLMs. Specifically, we categorize events into time-sensitive (e.g., putting down a bag) and time-insensitive (e.g., holding a towel in the left hand). The former denote events whose reversal substantially alters their meaning, while the latter remain semantically unchanged under reversal. For time-sensitive events, ArrowGEV introduces a reward that encourages VLMs to discriminate between forward and backward videos, whereas for time-insensitive events, it enforces consistent grounding across both directions. Extensive experiments demonstrate that ArrowGEV not only improves grounding precision and temporal directionality recognition, but also enhances general video understanding and reasoning ability.", "AI": {"tldr": "ArrowGEV\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u65f6\u95f4\u65b9\u5411\u6027\uff08\u524d\u5411/\u540e\u5411\u89c6\u9891\uff09\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u4ef6\u5b9a\u4f4d\u80fd\u529b\u548c\u65f6\u95f4\u65b9\u5411\u7406\u89e3\u80fd\u529b", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u89c6\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4e3b\u8981\u8bad\u7ec3\u6a21\u578b\u5173\u8054\u4e8b\u4ef6\u4e0e\u524d\u5411\u89c6\u9891\u7684\u65f6\u95f4\u6233\uff0c\u8fd9\u79cd\u8303\u5f0f\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u4e8b\u4ef6\u5185\u5728\u65f6\u95f4\u7ed3\u6784\u548c\u65b9\u5411\u6027\u7684\u80fd\u529b\uff0c\u4ece\u800c\u5f71\u54cd\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027", "method": "\u63d0\u51faArrowGEV\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u53d7\u7269\u7406\u5b66\u4e2d\"\u65f6\u95f4\u7bad\u5934\"\u542f\u53d1\uff0c\u5c06\u4e8b\u4ef6\u5206\u4e3a\u65f6\u95f4\u654f\u611f\u578b\uff08\u5982\u653e\u4e0b\u5305\uff09\u548c\u65f6\u95f4\u4e0d\u654f\u611f\u578b\uff08\u5982\u5de6\u624b\u62ff\u6bdb\u5dfe\uff09\u3002\u5bf9\u65f6\u95f4\u654f\u611f\u4e8b\u4ef6\uff0c\u5f15\u5165\u5956\u52b1\u673a\u5236\u9f13\u52b1VLM\u533a\u5206\u524d\u5411\u548c\u540e\u5411\u89c6\u9891\uff1b\u5bf9\u65f6\u95f4\u4e0d\u654f\u611f\u4e8b\u4ef6\uff0c\u5f3a\u5236\u8981\u6c42\u5728\u4e24\u4e2a\u65b9\u5411\u4e0a\u4fdd\u6301\u4e00\u81f4\u7684\u5b9a\u4f4d\u7ed3\u679c", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cArrowGEV\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u4e8b\u4ef6\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u65f6\u95f4\u65b9\u5411\u6027\u8bc6\u522b\u80fd\u529b\uff0c\u8fd8\u589e\u5f3a\u4e86\u901a\u7528\u7684\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e8b\u4ef6\u7684\u65f6\u95f4\u65b9\u5411\u6027\uff0cArrowGEV\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u5bf9\u65f6\u95f4\u7ed3\u6784\u7684\u5185\u5728\u7406\u89e3"}}
{"id": "2601.06336", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06336", "abs": "https://arxiv.org/abs/2601.06336", "authors": ["Benjamin Turtel", "Paul Wilczewski", "Danny Franklin", "Kris Skothiem"], "title": "Future-as-Label: Scalable Supervision from Real-World Outcomes", "comment": null, "summary": "Many real-world prediction problems lack labels observable at prediction time, creating a temporal gap between prediction and outcome that yields supervision only after events resolve. To address this setting, we extend reinforcement learning with verifiable rewards to temporally resolved real-world prediction, and use it to train language models to make probabilistic forecasts under causally masked information with retrospective evaluation using proper scoring rules. Supervision is derived solely from post-resolution outcomes, preserving delayed-reward semantics. On real-world forecasting benchmarks, Qwen3-32B trained using Foresight Learning improves Brier score by 27% and halves calibration error relative to its pretrained baseline, and outperforms Qwen3-235B on both constructed future-event prediction tasks and the Metaculus benchmark despite a 7x parameter disadvantage.", "AI": {"tldr": "\u63d0\u51faForesight Learning\u65b9\u6cd5\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u9884\u6d4b\u4e2d\u6807\u7b7e\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u56de\u987e\u6027\u8bc4\u4f30\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6982\u7387\u9884\u6d4b", "motivation": "\u73b0\u5b9e\u4e16\u754c\u9884\u6d4b\u95ee\u9898\u5b58\u5728\u9884\u6d4b\u65f6\u6807\u7b7e\u4e0d\u53ef\u89c1\u7684\u6311\u6218\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u4e4b\u95f4\u5b58\u5728\u65f6\u95f4\u5dee\uff0c\u53ea\u80fd\u5728\u4e8b\u4ef6\u7ed3\u675f\u540e\u83b7\u5f97\u76d1\u7763\u4fe1\u53f7", "method": "\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u5e94\u7528\u4e8e\u65f6\u95f4\u5206\u8fa8\u7684\u73b0\u5b9e\u4e16\u754c\u9884\u6d4b\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u4fe1\u606f\u5c4f\u853d\u4e0b\u8fdb\u884c\u6982\u7387\u9884\u6d4b\uff0c\u4f7f\u7528\u56de\u987e\u6027\u8bc4\u4f30\u548c\u9002\u5f53\u8bc4\u5206\u89c4\u5219", "result": "Qwen3-32B\u4f7f\u7528Foresight Learning\u8bad\u7ec3\u540e\uff0cBrier\u5206\u6570\u63d0\u534727%\uff0c\u6821\u51c6\u8bef\u5dee\u51cf\u534a\uff0c\u5728\u6784\u9020\u7684\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u548cMetaculus\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aQwen3-235B\uff0c\u5c3d\u7ba1\u53c2\u6570\u5c117\u500d", "conclusion": "Foresight Learning\u80fd\u6709\u6548\u5904\u7406\u5ef6\u8fdf\u76d1\u7763\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u548c\u6821\u51c6\u5ea6\uff0c\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02"}}
{"id": "2601.06637", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06637", "abs": "https://arxiv.org/abs/2601.06637", "authors": ["Abhishek Kumar Mishra", "Arya Somasundaram", "Anup Das", "Nagarajan Kandasamy"], "title": "Efficient Aspect Term Extraction using Spiking Neural Network", "comment": null, "summary": "Aspect Term Extraction (ATE) identifies aspect terms in review sentences, a key subtask of sentiment analysis. While most existing approaches use energy-intensive deep neural networks (DNNs) for ATE as sequence labeling, this paper proposes a more energy-efficient alternative using Spiking Neural Networks (SNNs). Using sparse activations and event-driven inferences, SNNs capture temporal dependencies between words, making them suitable for ATE. The proposed architecture, SpikeATE, employs ternary spiking neurons and direct spike training fine-tuned with pseudo-gradients. Evaluated on four benchmark SemEval datasets, SpikeATE achieves performance comparable to state-of-the-art DNNs with significantly lower energy consumption. This highlights the use of SNNs as a practical and sustainable choice for ATE tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpikeATE\uff0c\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u65b9\u9762\u672f\u8bed\u63d0\u53d6\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u65b9\u9762\u672f\u8bed\u63d0\u53d6\uff08ATE\uff09\u65b9\u6cd5\u5927\u591a\u4f7f\u7528\u80fd\u8017\u9ad8\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5e8f\u5217\u6807\u6ce8\uff0c\u9700\u8981\u66f4\u8282\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5177\u6709\u7a00\u758f\u6fc0\u6d3b\u548c\u4e8b\u4ef6\u9a71\u52a8\u63a8\u7406\u7279\u6027\uff0c\u9002\u5408\u6355\u6349\u8bcd\u8bed\u95f4\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faSpikeATE\u67b6\u6784\uff0c\u91c7\u7528\u4e09\u5143\u8109\u51b2\u795e\u7ecf\u5143\u548c\u76f4\u63a5\u8109\u51b2\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f2a\u68af\u5ea6\u8fdb\u884c\u5fae\u8c03\uff0c\u5229\u7528SNN\u7684\u7a00\u758f\u6fc0\u6d3b\u7279\u6027\u5b9e\u73b0\u9ad8\u6548\u65b9\u9762\u672f\u8bed\u63d0\u53d6\u3002", "result": "\u5728\u56db\u4e2aSemEval\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cSpikeATE\u8fbe\u5230\u4e0e\u6700\u5148\u8fdbDNN\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u9a8c\u8bc1\u4e86SNN\u5728ATE\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "SNN\u53ef\u4f5c\u4e3aATE\u4efb\u52a1\u7684\u5b9e\u7528\u4e14\u53ef\u6301\u7eed\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u80fd\u8017\uff0c\u4e3a\u60c5\u611f\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u73af\u4fdd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.06647", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.06647", "abs": "https://arxiv.org/abs/2601.06647", "authors": ["Krishna Vinod", "Joseph Raj Vishal", "Kaustav Chanda", "Prithvi Jai Ramesh", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers", "comment": null, "summary": "Tracking skiers in RGB broadcast footage is challenging due to motion blur, static overlays, and clutter that obscure the fast-moving athlete. Event cameras, with their asynchronous contrast sensing, offer natural robustness to such artifacts, yet a controlled benchmark for winter-sport tracking has been missing. We introduce event SkiTB (eSkiTB), a synthetic event-based ski tracking dataset generated from SkiTB using direct video-to-event conversion without neural interpolation, enabling an iso-informational comparison between RGB and event modalities. Benchmarking SDTrack (spiking transformer) against STARK (RGB transformer), we find that event-based tracking is substantially resilient to broadcast clutter in scenes dominated by static overlays, achieving 0.685 IoU, outperforming RGB by +20.0 points. Across the dataset, SDTrack attains a mean IoU of 0.711, demonstrating that temporal contrast is a reliable cue for tracking ballistic motion in visually congested environments. eSkiTB establishes the first controlled setting for event-based tracking in winter sports and highlights the promise of event cameras for ski tracking. The dataset and code will be released at https://github.com/eventbasedvision/eSkiTB.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u6ed1\u96ea\u8ffd\u8e2a\u6570\u636e\u96c6eSkiTB\uff0c\u901a\u8fc7SDTrack\uff08\u8109\u51b2\u53d8\u538b\u5668\uff09\u4e0eRGB\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u8bc1\u660e\u4e8b\u4ef6\u76f8\u673a\u5728\u5e7f\u64ad\u89c6\u9891\u906e\u6321\u573a\u666f\u4e2d\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\uff0cIoU\u63d0\u534720\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u4f20\u7edfRGB\u5e7f\u64ad\u89c6\u9891\u4e2d\u6ed1\u96ea\u8005\u8ffd\u8e2a\u9762\u4e34\u8fd0\u52a8\u6a21\u7cca\u3001\u9759\u6001\u8986\u76d6\u7269\u548c\u6742\u4e71\u80cc\u666f\u7684\u6311\u6218\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5f02\u6b65\u5bf9\u6bd4\u5ea6\u611f\u77e5\u7279\u6027\uff0c\u5929\u7136\u5bf9\u8fd9\u4e9b\u5e72\u6270\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u51ac\u5b63\u8fd0\u52a8\u8ffd\u8e2a\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u4eceSkiTB\u6570\u636e\u96c6\u901a\u8fc7\u76f4\u63a5\u89c6\u9891\u5230\u4e8b\u4ef6\u8f6c\u6362\u751f\u6210\u5408\u6210\u4e8b\u4ef6\u6570\u636e\u96c6eSkiTB\uff08\u4e0d\u4f7f\u7528\u795e\u7ecf\u63d2\u503c\uff09\uff0c\u5b9e\u73b0RGB\u548c\u4e8b\u4ef6\u6a21\u6001\u7684\u7b49\u91cf\u4fe1\u606f\u6bd4\u8f83\u3002\u4f7f\u7528SDTrack\uff08\u8109\u51b2\u53d8\u538b\u5668\uff09\u4e0eSTARK\uff08RGB\u53d8\u538b\u5668\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u9759\u6001\u8986\u76d6\u7269\u4e3b\u5bfc\u7684\u573a\u666f\u4e2d\uff0c\u4e8b\u4ef6\u8ffd\u8e2a\u5bf9\u5e7f\u64ad\u6742\u4e71\u5177\u6709\u663e\u8457\u9c81\u68d2\u6027\uff0c\u8fbe\u52300.685 IoU\uff0c\u6bd4RGB\u65b9\u6cd5\u9ad8\u51fa20.0\u4e2a\u767e\u5206\u70b9\u3002\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSDTrack\u5e73\u5747IoU\u4e3a0.711\u3002", "conclusion": "\u65f6\u95f4\u5bf9\u6bd4\u5ea6\u662f\u89c6\u89c9\u62e5\u6324\u73af\u5883\u4e2d\u8ffd\u8e2a\u5f39\u9053\u8fd0\u52a8\u7684\u53ef\u9760\u7ebf\u7d22\u3002eSkiTB\u5efa\u7acb\u4e86\u51ac\u5b63\u8fd0\u52a8\u4e8b\u4ef6\u8ffd\u8e2a\u7684\u9996\u4e2a\u53d7\u63a7\u8bbe\u7f6e\uff0c\u5c55\u793a\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u6ed1\u96ea\u8ffd\u8e2a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.07149", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07149", "abs": "https://arxiv.org/abs/2601.07149", "authors": ["Zhaoyan Li", "Hang Lei", "Yujia Wang", "Lanbo Liu", "Hao Liu", "Liang Yu"], "title": "Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling", "comment": null, "summary": "While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68\\% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.", "AI": {"tldr": "RLCS\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\u548c\u591a\u7ef4\u5ea6\u6545\u4e8b\u8d28\u91cf\u8bc4\u4f30\uff0c\u7ed3\u5408\u57fa\u4e8e\u71b5\u7684\u5956\u52b1\u5851\u9020\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u521b\u610f\u6545\u4e8b\u751f\u6210\u4e2d\u5956\u52b1\u4fe1\u53f7\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u4e8b\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u6d41\u7545\u6587\u672c\uff0c\u4f46\u521b\u4f5c\u9ad8\u8d28\u91cf\u521b\u610f\u6545\u4e8b\u4ecd\u5177\u6311\u6218\u6027\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u6709\u671b\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u969c\u788d\uff1a\u4e3a\u4e3b\u89c2\u6545\u4e8b\u8d28\u91cf\u8bbe\u8ba1\u53ef\u9760\u5956\u52b1\u4fe1\u53f7\uff0c\u4ee5\u53ca\u7f13\u89e3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faRLCS\u6846\u67b6\uff1a1) \u5f00\u53d1\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b(GenRM)\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4ece\u5f3a\u6559\u5e08\u6a21\u578b\u84b8\u998f\u63a8\u7406\u94fe\u7684\u6f14\u793a\uff0c\u5e76\u5728\u6269\u5c55\u504f\u597d\u6570\u636e\u4e0a\u8fdb\u884cGRPO\u7cbe\u70bc\uff0c\u63d0\u4f9b\u591a\u7ef4\u5ea6\u5206\u6790\u548c\u663e\u5f0f\u63a8\u7406\uff1b2) \u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u5956\u52b1\u5851\u9020\u7b56\u7565\uff0c\u52a8\u6001\u4f18\u5148\u5b66\u4e60\u81ea\u4fe1\u9519\u8bef\u548c\u4e0d\u786e\u5b9a\u7684\u6b63\u786e\u9884\u6d4b\u3002", "result": "GenRM\u4e0e\u4eba\u7c7b\u521b\u610f\u5224\u65ad\u7684\u5339\u914d\u5ea6\u8fbe\u523068%\uff0cRLCS\u5728\u6574\u4f53\u6545\u4e8b\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u5305\u62ecGemini-2.5-Pro\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u521b\u610f\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u6d41\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u5efa\u6a21\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u53cc\u91cd\u6311\u6218\u3002"}}
{"id": "2601.06478", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.06478", "abs": "https://arxiv.org/abs/2601.06478", "authors": ["Alan Oursland"], "title": "Deriving Decoder-Free Sparse Autoencoders from First Principles", "comment": "22 pages, 3 figures, 9 tables", "summary": "Gradient descent on log-sum-exp (LSE) objectives performs implicit expectation--maximization (EM): the gradient with respect to each component output equals its responsibility. The same theory predicts collapse without volume control analogous to the log-determinant in Gaussian mixture models. We instantiate the theory in a single-layer encoder with an LSE objective and InfoMax regularization for volume control. Experiments confirm the theory's predictions. The gradient--responsibility identity holds exactly; LSE alone collapses; variance prevents dead components; decorrelation prevents redundancy. The model exhibits EM-like optimization dynamics in which lower loss does not correspond to better features and adaptive optimizers offer no advantage. The resulting decoder-free model learns interpretable mixture components, confirming that implicit EM theory can prescribe architectures.", "AI": {"tldr": "\u68af\u5ea6\u4e0b\u964d\u5728log-sum-exp\u76ee\u6807\u4e0a\u6267\u884c\u9690\u5f0f\u671f\u671b\u6700\u5927\u5316\uff0c\u7406\u8bba\u9884\u6d4b\u9700\u8981\u4f53\u79ef\u63a7\u5236\u9632\u6b62\u574d\u7f29\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u68af\u5ea6-\u8d23\u4efb\u6052\u7b49\u5f0f", "motivation": "\u7814\u7a76log-sum-exp\u76ee\u6807\u51fd\u6570\u7684\u68af\u5ea6\u4e0b\u964d\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u4e0e\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u7684\u9690\u5f0f\u8054\u7cfb\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u9632\u6b62\u7ec4\u4ef6\u574d\u7f29\u548c\u5197\u4f59", "method": "\u4f7f\u7528\u5355\u5c42\u7f16\u7801\u5668\u914d\u5408log-sum-exp\u76ee\u6807\u548cInfoMax\u6b63\u5219\u5316\u8fdb\u884c\u4f53\u79ef\u63a7\u5236\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b", "result": "\u68af\u5ea6-\u8d23\u4efb\u6052\u7b49\u5f0f\u7cbe\u786e\u6210\u7acb\uff1blog-sum-exp\u5355\u72ec\u4f7f\u7528\u4f1a\u5bfc\u81f4\u574d\u7f29\uff1b\u65b9\u5dee\u9632\u6b62\u6b7b\u7ec4\u4ef6\uff1b\u53bb\u76f8\u5173\u9632\u6b62\u5197\u4f59\uff1b\u6a21\u578b\u8868\u73b0\u51fa\u7c7b\u4f3cEM\u7684\u4f18\u5316\u52a8\u6001", "conclusion": "\u9690\u5f0fEM\u7406\u8bba\u53ef\u4ee5\u6307\u5bfc\u67b6\u6784\u8bbe\u8ba1\uff0c\u4ea7\u751f\u7684\u65e0\u89e3\u7801\u5668\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u6df7\u5408\u7ec4\u4ef6\uff0c\u81ea\u9002\u5e94\u4f18\u5316\u5668\u6ca1\u6709\u4f18\u52bf"}}
{"id": "2601.06843", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06843", "abs": "https://arxiv.org/abs/2601.06843", "authors": ["Junyan Lin", "Junlong Tong", "Hao Wu", "Jialiang Zhang", "Jinming Liu", "Xin Jin", "Xiaoyu Shen"], "title": "Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance across many tasks, yet most systems remain limited to offline inference, requiring complete inputs before generating outputs. Recent streaming methods reduce latency by interleaving perception and generation, but still enforce a sequential perception-generation cycle, limiting real-time interaction. In this work, we target a fundamental bottleneck that arises when extending MLLMs to real-time video understanding: the global positional continuity constraint imposed by standard positional encoding schemes. While natural in offline inference, this constraint tightly couples perception and generation, preventing effective input-output parallelism. To address this limitation, we propose a parallel streaming framework that relaxes positional continuity through three designs: Overlapped, Group-Decoupled, and Gap-Isolated. These designs enable simultaneous perception and generation, allowing the model to process incoming inputs while producing responses in real time. Extensive experiments reveal that Group-Decoupled achieves the best efficiency-performance balance, maintaining high fluency and accuracy while significantly reducing latency. We further show that the proposed framework yields up to 2x acceleration under balanced perception-generation workloads, establishing a principled pathway toward speak-while-watching real-time systems. We make all our code publicly available: https://github.com/EIT-NLP/Speak-While-Watching.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u884c\u6d41\u5f0f\u6846\u67b6\u89e3\u51b3MLLMs\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4f4d\u7f6e\u8fde\u7eed\u6027\u7ea6\u675f\u74f6\u9888\uff0c\u901a\u8fc7\u4e09\u79cd\u8bbe\u8ba1\u5b9e\u73b0\u611f\u77e5\u4e0e\u751f\u6210\u7684\u5e76\u884c\u5904\u7406\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf", "motivation": "\u73b0\u6709MLLMs\u5927\u591a\u9650\u4e8e\u79bb\u7ebf\u63a8\u7406\uff0c\u9700\u8981\u5b8c\u6574\u8f93\u5165\u624d\u80fd\u751f\u6210\u8f93\u51fa\u3002\u6d41\u5f0f\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u5ef6\u8fdf\uff0c\u4f46\u4ecd\u5f3a\u5236\u987a\u5e8f\u7684\u611f\u77e5-\u751f\u6210\u5faa\u73af\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u4ea4\u4e92\u3002\u7279\u522b\u662f\u6807\u51c6\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u65bd\u52a0\u7684\u5168\u5c40\u4f4d\u7f6e\u8fde\u7eed\u6027\u7ea6\u675f\uff0c\u5728\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u4e2d\u6210\u4e3a\u74f6\u9888", "method": "\u63d0\u51fa\u5e76\u884c\u6d41\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u8bbe\u8ba1\u653e\u677e\u4f4d\u7f6e\u8fde\u7eed\u6027\u7ea6\u675f\uff1a\u91cd\u53e0\u5f0f\u3001\u7ec4\u89e3\u8026\u5f0f\u548c\u95f4\u9694\u9694\u79bb\u5f0f\u3002\u8fd9\u4e9b\u8bbe\u8ba1\u4f7f\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u8f93\u5165\u548c\u751f\u6210\u54cd\u5e94\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u751f\u6210\u7684\u5e76\u884c", "result": "\u5b9e\u9a8c\u8868\u660e\u7ec4\u89e3\u8026\u5f0f\u8bbe\u8ba1\u5728\u6548\u7387\u4e0e\u6027\u80fd\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff0c\u4fdd\u6301\u9ad8\u6d41\u7545\u5ea6\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002\u5728\u5e73\u8861\u7684\u611f\u77e5-\u751f\u6210\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u6846\u67b6\u53ef\u5b9e\u73b0\u9ad8\u8fbe2\u500d\u7684\u52a0\u901f", "conclusion": "\u63d0\u51fa\u7684\u5e76\u884c\u6d41\u5f0f\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4f4d\u7f6e\u8fde\u7eed\u6027\u7ea6\u675f\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\"\u8fb9\u770b\u8fb9\u8bf4\"\u7684\u5b9e\u65f6\u7cfb\u7edf\u5efa\u7acb\u4e86\u539f\u5219\u6027\u8def\u5f84"}}
{"id": "2601.06530", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06530", "abs": "https://arxiv.org/abs/2601.06530", "authors": ["Bowen Zhang", "Hongda Tian", "Adam Berry", "A. Craig Roussac"], "title": "Improving Day-Ahead Grid Carbon Intensity Forecasting by Joint Modeling of Local-Temporal and Cross-Variable Dependencies Across Different Frequencies", "comment": "2026 40th AAAI Conference on Artificial Intelligence", "summary": "Accurate forecasting of the grid carbon intensity factor (CIF) is critical for enabling demand-side management and reducing emissions in modern electricity systems. Leveraging multiple interrelated time series, CIF prediction is typically formulated as a multivariate time series forecasting problem. Despite advances in deep learning-based methods, it remains challenging to capture the fine-grained local-temporal dependencies, dynamic higher-order cross-variable dependencies, and complex multi-frequency patterns for CIF forecasting. To address these issues, we propose a novel model that integrates two parallel modules: 1) one enhances the extraction of local-temporal dependencies under multi-frequency by applying multiple wavelet-based convolutional kernels to overlapping patches of varying lengths; 2) the other captures dynamic cross-variable dependencies under multi-frequency to model how inter-variable relationships evolve across the time-frequency domain. Evaluations on four representative electricity markets from Australia, featuring varying levels of renewable penetration, demonstrate that the proposed method outperforms the state-of-the-art models. An ablation study further validates the complementary benefits of the two proposed modules. Designed with built-in interpretability, the proposed model also enables better understanding of its predictive behavior, as shown in a case study where it adaptively shifts attention to relevant variables and time intervals during a disruptive event.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u7535\u7f51\u78b3\u5f3a\u5ea6\u56e0\u5b50\u9884\u6d4b\u7684\u65b0\u578b\u6a21\u578b\uff0c\u901a\u8fc7\u5e76\u884c\u6a21\u5757\u5206\u522b\u6355\u83b7\u591a\u9891\u7387\u4e0b\u7684\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u8de8\u53d8\u91cf\u4f9d\u8d56\u6027\uff0c\u5728\u591a\u4e2a\u7535\u529b\u5e02\u573a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u7f51\u78b3\u5f3a\u5ea6\u56e0\u5b50\u9884\u6d4b\u5bf9\u9700\u6c42\u4fa7\u7ba1\u7406\u548c\u51cf\u6392\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\u3001\u52a8\u6001\u9ad8\u9636\u8de8\u53d8\u91cf\u4f9d\u8d56\u6027\u548c\u590d\u6742\u591a\u9891\u7387\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e24\u4e2a\u5e76\u884c\u6a21\u5757\u7684\u6a21\u578b\uff1a1) \u4f7f\u7528\u591a\u5c0f\u6ce2\u5377\u79ef\u6838\u5bf9\u4e0d\u540c\u957f\u5ea6\u91cd\u53e0\u5757\u5904\u7406\u4ee5\u589e\u5f3a\u591a\u9891\u7387\u4e0b\u7684\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\u63d0\u53d6\uff1b2) \u6355\u83b7\u591a\u9891\u7387\u4e0b\u7684\u52a8\u6001\u8de8\u53d8\u91cf\u4f9d\u8d56\u6027\u4ee5\u5efa\u6a21\u53d8\u91cf\u95f4\u5173\u7cfb\u5728\u65f6\u9891\u57df\u7684\u6f14\u5316\u3002", "result": "\u5728\u6fb3\u5927\u5229\u4e9a\u56db\u4e2a\u5177\u6709\u4e0d\u540c\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u6c34\u5e73\u7684\u4ee3\u8868\u6027\u7535\u529b\u5e02\u573a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u6a21\u5757\u7684\u4e92\u8865\u6548\u76ca\u3002", "conclusion": "\u6240\u63d0\u6a21\u578b\u4e0d\u4ec5\u9884\u6d4b\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u5177\u5907\u5185\u7f6e\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u66f4\u597d\u5730\u7406\u89e3\u9884\u6d4b\u884c\u4e3a\uff0c\u5982\u5728\u5e72\u6270\u4e8b\u4ef6\u4e2d\u81ea\u9002\u5e94\u5730\u5c06\u6ce8\u610f\u529b\u8f6c\u79fb\u5230\u76f8\u5173\u53d8\u91cf\u548c\u65f6\u95f4\u533a\u95f4\u3002"}}
{"id": "2601.07238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07238", "abs": "https://arxiv.org/abs/2601.07238", "authors": ["Hanbin Wang", "Jingwei Song", "Jinpeng Li", "Fei Mi", "Lifeng Shang"], "title": "Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern for Reasoning", "comment": "8 pages, 5 figures", "summary": "Large reasoning models (LRMs) exhibit diverse high-level reasoning patterns (e.g., direct solution, reflection-and-verification, and exploring multiple solutions), yet prevailing training recipes implicitly bias models toward a limited set of dominant patterns. Through a systematic analysis, we identify substantial accuracy variance across these patterns on mathematics and science benchmarks, revealing that a model's default reasoning pattern is often sub-optimal for a given problem. To address this, we introduce Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that extends GRPO by incorporating multi-pattern rollouts, verifier-guided optimal pattern selection per problem, and attention masking during optimization to prevent the leakage of explicit pattern suffixes into the learned policy. By exploring a portfolio of diverse reasoning strategies and optimizing the policy on the most effective ones, GPSO enables the model to internalize the mapping from problem characteristics to optimal reasoning patterns. Extensive experiments demonstrate that GPSO delivers consistent and substantial performance gains across various model backbones and benchmarks, effectively mitigating pattern sub-optimality and fostering more robust, adaptable reasoning. All data and codes are available at https://github.com/wanghanbinpanda/GPSO.", "AI": {"tldr": "GPSO\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b66\u4e60\u6839\u636e\u95ee\u9898\u7279\u5f81\u9009\u62e9\u6700\u4f18\u63a8\u7406\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u548c\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u7136\u5c55\u73b0\u51fa\u591a\u79cd\u9ad8\u7ea7\u63a8\u7406\u6a21\u5f0f\uff0c\u4f46\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u4f1a\u504f\u5411\u6709\u9650\u7684\u51e0\u79cd\u4e3b\u5bfc\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6a21\u578b\u9ed8\u8ba4\u63a8\u7406\u6a21\u5f0f\u5bf9\u7279\u5b9a\u95ee\u9898\u5f80\u5f80\u4e0d\u662f\u6700\u4f18\u7684", "method": "\u63d0\u51faGroup Pattern Selection Optimization (GPSO)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6269\u5c55GRPO\u65b9\u6cd5\uff0c\u5305\u542b\u591a\u6a21\u5f0frollout\u3001\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u95ee\u9898\u7ea7\u6700\u4f18\u6a21\u5f0f\u9009\u62e9\uff0c\u4ee5\u53ca\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u63a9\u7801\u9632\u6b62\u6a21\u5f0f\u540e\u7f00\u6cc4\u9732", "result": "GPSO\u5728\u5404\u79cd\u6a21\u578b\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e26\u6765\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u5f0f\u6b21\u4f18\u95ee\u9898\uff0c\u4fc3\u8fdb\u66f4\u9c81\u68d2\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b", "conclusion": "\u901a\u8fc7\u63a2\u7d22\u591a\u6837\u5316\u63a8\u7406\u7b56\u7565\u7ec4\u5408\u5e76\u4f18\u5316\u6700\u6709\u6548\u7b56\u7565\uff0cGPSO\u4f7f\u6a21\u578b\u80fd\u591f\u5185\u5316\u4ece\u95ee\u9898\u7279\u5f81\u5230\u6700\u4f18\u63a8\u7406\u6a21\u5f0f\u7684\u6620\u5c04\uff0c\u63d0\u5347\u63a8\u7406\u6027\u80fd"}}
{"id": "2601.07342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07342", "abs": "https://arxiv.org/abs/2601.07342", "authors": ["Nicolas Tacheny"], "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure", "comment": null, "summary": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model.\n  In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information.\n  This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u667a\u80fd\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7MCP\u534f\u8bae\u8bbf\u95ee\u5de5\u5177\u7a7a\u95f4\uff0c\u81ea\u4e3b\u5bfc\u822a\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\u8fdb\u884c\u6839\u56e0\u5206\u6790\uff0c\u66ff\u4ee3\u4f20\u7edf\u786c\u7f16\u7801\u7684\u56fe\u904d\u5386\u7b97\u6cd5", "motivation": "\u4f20\u7edf\u6839\u56e0\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u786c\u7f16\u7801\u7684\u56fe\u904d\u5386\u7b97\u6cd5\u6216\u57fa\u4e8e\u89c4\u5219\u7684\u5173\u8054\u5f15\u64ce\uff0c\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u4e0e\u57fa\u7840\u8bbe\u65bd\u6a21\u578b\u7d27\u5bc6\u8026\u5408\uff0c\u96be\u4ee5\u9002\u5e94\u5927\u89c4\u6a21\u7535\u4fe1\u548c\u6570\u636e\u4e2d\u5fc3\u57fa\u7840\u8bbe\u65bd\u7684\u591a\u5c42\u670d\u52a1\u8d44\u6e90\u6a21\u578b", "method": "\u5f15\u5165\u57fa\u4e8eLLM\u7684\u667a\u80fd\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7Model Context Protocol (MCP)\u66b4\u9732\u7ea6\u675f\u5de5\u5177\u7a7a\u95f4\uff0c\u8ba9LLM\u4ee3\u7406\u81ea\u4e3b\u8c03\u7528\u670d\u52a1\u67e5\u627e\u3001\u4f9d\u8d56\u68c0\u7d22\u3001\u7ed3\u6784\u5316/\u975e\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u3001\u4e8b\u4ef6\u5206\u6790\u548c\u5f71\u54cd\u53d1\u73b0\u7b49\u5de5\u5177\u8fdb\u884c\u9010\u6b65\u8c03\u67e5", "result": "\u5b9a\u4e49\u4e86\u7ed3\u6784\u5316\u8c03\u67e5\u534f\u8bae\uff0c\u786e\u4fdd\u4ee3\u7406\u63a8\u7406\u7684\u63a5\u5730\u6027\u3001\u53ef\u91cd\u73b0\u6027\uff0c\u5e76\u80fd\u5b89\u5168\u5904\u7406\u7f3a\u5931\u6216\u6a21\u7cca\u4fe1\u606f\uff0c\u4e3a\u81ea\u4e3b\u4e8b\u4ef6\u89e3\u51b3\u548c\u53d8\u66f4\u5f71\u54cd\u7f13\u89e3\u5960\u5b9a\u57fa\u7840", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u4e3b\u4e8b\u4ef6\u89e3\u51b3\u548c\u53d8\u66f4\u5f71\u54cd\u7f13\u89e3\u5960\u5b9a\u57fa\u7840\uff0c\u672a\u6765\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u8bca\u65ad\u548c\u4fee\u590d\u57fa\u7840\u8bbe\u65bd\u6545\u969c\uff0c\u8fd8\u80fd\u9884\u6d4b\u8ba1\u5212\u53d8\u66f4\u5bf9\u670d\u52a1\u548c\u5ba2\u6237\u7684\u5f71\u54cd\uff0c\u4f7f\u8fd0\u7ef4\u4eba\u5458\u80fd\u5728\u6267\u884c\u7ef4\u62a4\u64cd\u4f5c\u524d\u7f13\u89e3\u98ce\u9669"}}
{"id": "2601.07107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07107", "abs": "https://arxiv.org/abs/2601.07107", "authors": ["Meng Lu", "Yuxing Lu", "Yuchen Zhuang", "Megan Mullins", "Yang Xie", "Guanghua Xiao", "Charles Fleming", "Wenqi Shi", "Xuan Wang"], "title": "MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning", "comment": null, "summary": "Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.", "AI": {"tldr": "MedVistaGym\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u6269\u5c55\u4ea4\u4e92\u8bad\u7ec3\u73af\u5883\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u8fdb\u884c\u5de5\u5177\u96c6\u6210\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u591a\u6b65\u63a8\u7406\u548c\u8fed\u4ee3\u89c6\u89c9\u4ea4\u4e92\u80fd\u529b\u3002\u533b\u5b66VLM\u901a\u5e38\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u5d4c\u5165\u548c\u5355\u6b21\u63a8\u7406\uff0c\u65e0\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u91cd\u65b0\u68c0\u67e5\u3001\u9a8c\u8bc1\u6216\u7ec6\u5316\u89c6\u89c9\u8bc1\u636e\u3002\u867d\u7136\u5de5\u5177\u96c6\u6210\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u4f46\u5f00\u6e90VLM\u7f3a\u4e4f\u5b66\u4e60\u6709\u6548\u5de5\u5177\u9009\u62e9\u3001\u8c03\u7528\u548c\u534f\u8c03\u7684\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u63d0\u51faMedVistaGym\u8bad\u7ec3\u73af\u5883\uff0c\u6fc0\u52b1\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5de5\u5177\u96c6\u6210\u89c6\u89c9\u63a8\u7406\u3002\u8be5\u7cfb\u7edf\u8bad\u7ec3VLM\u51b3\u5b9a\u4f55\u65f6\u8c03\u7528\u54ea\u4e9b\u5de5\u5177\u3001\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u56fe\u50cf\u533a\u57df\uff0c\u5e76\u5c06\u5355\u4e2a\u6216\u591a\u4e2a\u5b50\u56fe\u50cf\u8bc1\u636e\u96c6\u6210\u5230\u4ea4\u9519\u7684\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u3002\u901a\u8fc7\u8f68\u8ff9\u91c7\u6837\u548c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3MedVistaGym-R1\u6a21\u578b\uff0c\u5b9e\u73b0\u5de5\u5177\u4f7f\u7528\u4e0e\u4ee3\u7406\u63a8\u7406\u7684\u4ea4\u9519\u3002", "result": "\u5728\u516d\u4e2a\u533b\u5b66VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedVistaGym-R1-8B\u6a21\u578b\u6bd4\u540c\u7b49\u89c4\u6a21\u5de5\u5177\u589e\u5f3a\u57fa\u7ebf\u63d0\u5347\u4e8619.10%\u523024.21%\uff0c\u8868\u660e\u7ed3\u6784\u5316\u4ee3\u7406\u8bad\u7ec3\uff08\u4e0d\u4ec5\u4ec5\u662f\u5de5\u5177\u8bbf\u95ee\uff09\u80fd\u591f\u6709\u6548\u89e3\u9501\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "MedVistaGym\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u4ea4\u4e92\u8bad\u7ec3\u73af\u5883\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66VLM\u5728\u5de5\u5177\u96c6\u6210\u63a8\u7406\u65b9\u9762\u7684\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u7f3a\u5931\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u4ee3\u7406\u8bad\u7ec3\u5bf9\u4e8e\u89e3\u9501\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u6709\u6548\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2601.07577", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07577", "abs": "https://arxiv.org/abs/2601.07577", "authors": ["Yunfan Li", "Bingbing Xu", "Xueyun Tian", "Xiucheng Xu", "Huawei Shen"], "title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.", "AI": {"tldr": "TDP\u63d0\u51fa\u4efb\u52a1\u89e3\u8026\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u4f7f\u7528\u76d1\u7763\u5668\u3001\u89c4\u5212\u5668\u548c\u6267\u884c\u5668\u8fdb\u884c\u5c40\u90e8\u63a8\u7406\u548c\u91cd\u89c4\u5212\uff0c\u907f\u514d\u9519\u8bef\u4f20\u64ad\uff0c\u63d0\u9ad8\u957f\u65f6\u57df\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u4e24\u79cd\u8303\u5f0f\uff1a\u9010\u6b65\u89c4\u5212\u77ed\u89c6\uff0c\u4e00\u6b21\u6027\u89c4\u5212\u8106\u5f31\u3002\u4e24\u8005\u90fd\u9762\u4e34\u4e0a\u4e0b\u6587\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba4\u77e5\u8d1f\u8377\u9ad8\u3001\u5c40\u90e8\u9519\u8bef\u4f20\u64ad\u3001\u6062\u590d\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "TDP\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u76d1\u7763\u5668\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\u5b50\u76ee\u6807\uff1b\u89c4\u5212\u5668\u5728\u9650\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u4e3a\u5f53\u524d\u5b50\u4efb\u52a1\u5236\u5b9a\u8ba1\u5212\uff1b\u6267\u884c\u5668\u6267\u884c\u8ba1\u5212\u3002\u8fd9\u79cd\u4efb\u52a1\u89e3\u8026\u5c06\u63a8\u7406\u548c\u91cd\u89c4\u5212\u9650\u5236\u5728\u6d3b\u8dc3\u5b50\u4efb\u52a1\u5185\u3002", "result": "\u5728TravelPlanner\u3001ScienceWorld\u548cHotpotQA\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTDP\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06token\u6d88\u8017\u51cf\u5c11\u9ad8\u8fbe82%\uff0c\u8868\u660e\u5b50\u4efb\u52a1\u89e3\u8026\u80fd\u540c\u65f6\u63d0\u9ad8\u957f\u65f6\u57df\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u4efb\u52a1\u89e3\u8026\u89c4\u5212\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u9694\u79bb\u7684\u5b50\u4efb\u52a1\uff0c\u907f\u514d\u4e0a\u4e0b\u6587\u7ea0\u7f20\u548c\u9519\u8bef\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2601.07033", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07033", "abs": "https://arxiv.org/abs/2601.07033", "authors": ["Longfei Yun", "Kun Zhou", "Yupeng Hou", "Letian Peng", "Jingbo Shang"], "title": "Codified Foreshadowing-Payoff Text Generation", "comment": null, "summary": "Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving \"Chekhov's guns\" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the \"triggering mechanism\" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.", "AI": {"tldr": "CFPG\u6846\u67b6\u901a\u8fc7\u5c06\u53d9\u4e8b\u8fde\u7eed\u6027\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u56e0\u679c\u8c13\u8bcd\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u957f\u7a0b\u53d9\u4e8b\u4f9d\u8d56\u4e0a\u7684\u8868\u73b0\uff0c\u786e\u4fdd\u4f0f\u7b14\u5f97\u5230\u903b\u8f91\u548c\u65f6\u95f4\u4e0a\u7684\u5151\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u6545\u4e8b\u751f\u6210\u6280\u672f\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u65e0\u6cd5\u5904\u7406\u957f\u7a0b\u53d9\u4e8b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\"\u5951\u8bc3\u592b\u4e4b\u67aa\"\u672a\u88ab\u89e6\u53d1\u3002\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u8868\u9762\u8fde\u8d2f\u6027\u800c\u975e\u53d9\u4e8b\u8bbe\u7f6e\u7684\u903b\u8f91\u5b9e\u73b0\u3002", "method": "\u63d0\u51faCodified Foreshadowing-Payoff Generation (CFPG)\u6846\u67b6\uff0c\u5c06\u53d9\u4e8b\u8fde\u7eed\u6027\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u56e0\u679c\u8c13\u8bcd\u3002\u4eceBookSum\u8bed\u6599\u5e93\u4e2d\u6316\u6398\u548c\u7f16\u7801\"\u4f0f\u7b14-\u89e6\u53d1-\u5151\u73b0\"\u4e09\u5143\u7ec4\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCFPG\u5728\u5151\u73b0\u51c6\u786e\u6027\u548c\u53d9\u4e8b\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u57fa\u7ebf\u3002\u8be5\u6846\u67b6\u80fd\u786e\u4fdd\u4f0f\u7b14\u627f\u8bfa\u4e0d\u4ec5\u5728\u6587\u672c\u4e2d\u88ab\u63d0\u53ca\uff0c\u800c\u4e14\u5728\u65f6\u95f4\u548c\u903b\u8f91\u4e0a\u5f97\u5230\u5b9e\u73b0\u3002", "conclusion": "\u660e\u786e\u7f16\u7801\u53d9\u4e8b\u673a\u5236\u5bf9\u4e8e\u63a8\u52a8LLM\u4ece\u8868\u9762\u6d41\u7545\u6027\u8f6c\u5411\u771f\u6b63\u7684\u53d9\u4e8b\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u7ed3\u6784\u5316\u76d1\u7763\u80fd\u6709\u6548\u89e3\u51b3\u957f\u7a0b\u53d9\u4e8b\u4f9d\u8d56\u95ee\u9898\u3002"}}
{"id": "2601.07054", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07054", "abs": "https://arxiv.org/abs/2601.07054", "authors": ["Zhuoyi Yang", "Yurun Song", "Iftekhar Ahmed", "Ian Harris"], "title": "Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge", "comment": null, "summary": "Multi-hop question answering is widely used to evaluate the reasoning capabilities of large language models (LLMs), as it requires integrating multiple pieces of supporting knowledge to arrive at a correct answer. While prior work has explored different mechanisms for providing knowledge to LLMs, such as finetuning and retrieval-augmented generation (RAG), their relative effectiveness for multi-hop question answering remains insufficiently understood, particularly when the required knowledge is temporally novel.\n  In this paper, we systematically compare parametric and non-parametric knowledge injection methods for open-domain multi-hop question answering. We evaluate unsupervised fine-tuning (continual pretraining), supervised fine-tuning, and retrieval-augmented generation across three 7B-parameter open-source LLMs. Experiments are conducted on two benchmarks: QASC, a standard multi-hop science question answering dataset, and a newly constructed dataset of over 10,000 multi-hop questions derived from Wikipedia events in 2024, designed to test knowledge beyond the models' pretraining cutoff.\n  Our results show that unsupervised fine-tuning provides only limited gains over base models, suggesting that continual pretraining alone is insufficient for improving multi-hop reasoning accuracy. In contrast, retrieval-augmented generation yields substantial and consistent improvements, particularly when answering questions that rely on temporally novel information. Supervised fine-tuning achieves the highest overall accuracy across models and datasets. These findings highlight fundamental differences in how knowledge injection mechanisms support multi-hop question answering and underscore the importance of retrieval-based methods when external or compositional knowledge is required.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u53c2\u6570\u5316\u4e0e\u975e\u53c2\u6570\u5316\u77e5\u8bc6\u6ce8\u5165\u65b9\u6cd5\u5728\u5f00\u653e\u57df\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u76d1\u7763\u5fae\u8c03\u6548\u679c\u6700\u4f73\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5bf9\u65f6\u5e8f\u65b0\u77e5\u8bc6\u6700\u6709\u6548\uff0c\u800c\u65e0\u76d1\u7763\u5fae\u8c03\u6539\u8fdb\u6709\u9650\u3002", "motivation": "\u591a\u8df3\u95ee\u7b54\u9700\u8981\u6574\u5408\u591a\u4e2a\u77e5\u8bc6\u7247\u6bb5\uff0c\u662f\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u4efb\u52a1\u3002\u73b0\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u5fae\u8c03\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b49\u77e5\u8bc6\u6ce8\u5165\u673a\u5236\uff0c\u4f46\u5b83\u4eec\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u76f8\u5bf9\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u65f6\u5e8f\u65b0\u77e5\u8bc6\u7684\u5904\u7406\uff0c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u53c2\u6570\u5316\uff08\u65e0\u76d1\u7763\u5fae\u8c03\u3001\u76d1\u7763\u5fae\u8c03\uff09\u548c\u975e\u53c2\u6570\u5316\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u77e5\u8bc6\u6ce8\u5165\u65b9\u6cd5\u3002\u5728\u4e09\u4e2a7B\u53c2\u6570\u5f00\u6e90LLM\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528QASC\u6807\u51c6\u79d1\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u548c\u65b0\u6784\u5efa\u76842024\u5e74\u7ef4\u57fa\u767e\u79d1\u4e8b\u4ef6\u6570\u636e\u96c6\uff08\u5305\u542b10,000+\u591a\u8df3\u95ee\u9898\uff09\u3002", "result": "\u65e0\u76d1\u7763\u5fae\u8c03\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u4ec5\u63d0\u4f9b\u6709\u9650\u6539\u8fdb\uff0c\u8868\u660e\u6301\u7eed\u9884\u8bad\u7ec3\u672c\u8eab\u4e0d\u8db3\u4ee5\u63d0\u5347\u591a\u8df3\u63a8\u7406\u51c6\u786e\u6027\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5e26\u6765\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u4f9d\u8d56\u65f6\u5e8f\u65b0\u4fe1\u606f\u7684\u95ee\u9898\u4e0a\u3002\u76d1\u7763\u5fae\u8c03\u5728\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u9ad8\u603b\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "\u4e0d\u540c\u77e5\u8bc6\u6ce8\u5165\u673a\u5236\u652f\u6301\u591a\u8df3\u95ee\u7b54\u7684\u65b9\u5f0f\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u5f53\u9700\u8981\u5916\u90e8\u6216\u7ec4\u5408\u77e5\u8bc6\u65f6\uff0c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5c24\u4e3a\u91cd\u8981\u3002\u76d1\u7763\u5fae\u8c03\u6548\u679c\u6700\u4f73\uff0c\u4f46\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u65f6\u5e8f\u65b0\u77e5\u8bc6\u5904\u7406\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2601.07121", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07121", "abs": "https://arxiv.org/abs/2601.07121", "authors": ["Makoto Sato"], "title": "ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity A REM-Inspired System Design for Emergent Creative Ideation", "comment": null, "summary": "Large language models (LLMs) are used not only for problem solving but also for creative ideation; however, eliciting serendipitous insights that are both novel and internally coherent remains difficult. While stochastic sampling promotes novelty, it often degrades consistency. Here, we propose ReMIND, a REM-inspired modular framework for ideation. ReMIND consists of four stages: wake, which generates a stable low-temperature semantic baseline; dream, which performs high-temperature exploratory generation; judge, which applies coarse evaluation to filter incoherent outputs and extract candidate ideas; and re-wake, which re-articulates selected ideas into coherent final outputs. By instantiating each stage as an independent LLM, ReMIND enables functional separation between exploration and consolidation. Parameter sweeps show that ReMIND reliably induces semantic exploration while preserving downstream stability. Embedding-based analyses confirm substantial semantic displacement during the dream phase, whereas external evaluations reveal that high-quality ideas emerge sporadically rather than as extrema along any single metric. These results suggest that serendipitous ideation in LLMs is a rare-event process best approached through system level design that shapes the conditions under which valuable ideas can emerge and be stabilized. ReMIND provides a general framework for studying the computational basis of serendipity and illustrates how modular LLM orchestration can bridge exploration and stabilization.", "AI": {"tldr": "ReMIND\u662f\u4e00\u4e2a\u53d7REM\u7761\u7720\u542f\u53d1\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728LLM\u4e2d\u5b9e\u73b0\u521b\u9020\u6027\u6784\u601d\uff0c\u901a\u8fc7\u5206\u79bb\u63a2\u7d22\u548c\u6574\u5408\u9636\u6bb5\u6765\u5e73\u8861\u65b0\u9896\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u521b\u9020\u6027\u6784\u601d\u4e2d\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u65b0\u9896\u6027\u548c\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u968f\u673a\u91c7\u6837\u867d\u7136\u80fd\u4fc3\u8fdb\u65b0\u9896\u6027\u4f46\u4f1a\u964d\u4f4e\u4e00\u81f4\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u5e73\u8861\u8fd9\u4e24\u8005\u3002", "method": "ReMIND\u91c7\u7528\u56db\u9636\u6bb5\u6a21\u5757\u5316\u6846\u67b6\uff1awake\u9636\u6bb5\u751f\u6210\u7a33\u5b9a\u7684\u4f4e\u6e29\u8bed\u4e49\u57fa\u7ebf\uff1bdream\u9636\u6bb5\u8fdb\u884c\u9ad8\u6e29\u63a2\u7d22\u6027\u751f\u6210\uff1bjudge\u9636\u6bb5\u5e94\u7528\u7c97\u7565\u8bc4\u4f30\u8fc7\u6ee4\u4e0d\u8fde\u8d2f\u8f93\u51fa\u5e76\u63d0\u53d6\u5019\u9009\u60f3\u6cd5\uff1bre-wake\u9636\u6bb5\u5c06\u9009\u5b9a\u60f3\u6cd5\u91cd\u65b0\u8868\u8ff0\u4e3a\u8fde\u8d2f\u7684\u6700\u7ec8\u8f93\u51fa\u3002\u6bcf\u4e2a\u9636\u6bb5\u7531\u72ec\u7acb\u7684LLM\u5b9e\u4f8b\u5316\uff0c\u5b9e\u73b0\u529f\u80fd\u5206\u79bb\u3002", "result": "\u53c2\u6570\u626b\u63cf\u663e\u793aReMIND\u80fd\u53ef\u9760\u5730\u8bf1\u5bfc\u8bed\u4e49\u63a2\u7d22\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u7a33\u5b9a\u6027\u3002\u5d4c\u5165\u5206\u6790\u786e\u8ba4dream\u9636\u6bb5\u5b58\u5728\u663e\u8457\u7684\u8bed\u4e49\u4f4d\u79fb\uff0c\u5916\u90e8\u8bc4\u4f30\u663e\u793a\u9ad8\u8d28\u91cf\u60f3\u6cd5\u662f\u96f6\u661f\u51fa\u73b0\u7684\uff0c\u800c\u975e\u6cbf\u4efb\u4f55\u5355\u4e00\u6307\u6807\u7684\u6781\u7aef\u503c\u3002", "conclusion": "LLM\u4e2d\u7684\u5076\u7136\u6027\u6784\u601d\u662f\u4e00\u4e2a\u7f55\u89c1\u4e8b\u4ef6\u8fc7\u7a0b\uff0c\u6700\u597d\u901a\u8fc7\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u6765\u5851\u9020\u6709\u4ef7\u503c\u60f3\u6cd5\u51fa\u73b0\u548c\u7a33\u5b9a\u7684\u6761\u4ef6\u3002ReMIND\u4e3a\u7814\u7a76\u5076\u7136\u6027\u7684\u8ba1\u7b97\u57fa\u7840\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u6a21\u5757\u5316LLM\u7f16\u6392\u5982\u4f55\u6865\u63a5\u63a2\u7d22\u548c\u7a33\u5b9a\u5316\u3002"}}
{"id": "2601.07293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07293", "abs": "https://arxiv.org/abs/2601.07293", "authors": ["Weidong Tang", "Xinyan Wan", "Siyu Li", "Xiumei Wang"], "title": "Inference-Time Scaling for Visual AutoRegressive modeling by Searching Representative Samples", "comment": "Accepted to PRCV 2025", "summary": "While inference-time scaling has significantly enhanced generative quality in large language and diffusion models, its application to vector-quantized (VQ) visual autoregressive modeling (VAR) remains unexplored. We introduce VAR-Scaling, the first general framework for inference-time scaling in VAR, addressing the critical challenge of discrete latent spaces that prohibit continuous path search. We find that VAR scales exhibit two distinct pattern types: general patterns and specific patterns, where later-stage specific patterns conditionally optimize early-stage general patterns. To overcome the discrete latent space barrier in VQ models, we map sampling spaces to quasi-continuous feature spaces via kernel density estimation (KDE), where high-density samples approximate stable, high-quality solutions. This transformation enables effective navigation of sampling distributions. We propose a density-adaptive hybrid sampling strategy: Top-k sampling focuses on high-density regions to preserve quality near distribution modes, while Random-k sampling explores low-density areas to maintain diversity and prevent premature convergence. Consequently, VAR-Scaling optimizes sample fidelity at critical scales to enhance output quality. Experiments in class-conditional and text-to-image evaluations demonstrate significant improvements in inference process. The code is available at https://github.com/WD7ang/VAR-Scaling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VAR-Scaling\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5e94\u7528\u4e8e\u5411\u91cf\u91cf\u5316\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5c06\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u6620\u5c04\u5230\u51c6\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u5bc6\u5ea6\u81ea\u9002\u5e94\u6df7\u5408\u91c7\u6837\u7b56\u7565\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u867d\u7136\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5728\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u5728\u5411\u91cf\u91cf\u5316\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u963b\u788d\u4e86\u8fde\u7eed\u8def\u5f84\u641c\u7d22\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6280\u672f\u969c\u788d\u3002", "method": "\u901a\u8fc7\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5c06\u79bb\u6563\u91c7\u6837\u7a7a\u95f4\u6620\u5c04\u5230\u51c6\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\uff0c\u8bc6\u522bVAR\u7f29\u653e\u4e2d\u7684\u4e24\u79cd\u6a21\u5f0f\u7c7b\u578b\uff08\u901a\u7528\u6a21\u5f0f\u548c\u7279\u5b9a\u6a21\u5f0f\uff09\uff0c\u5e76\u63d0\u51fa\u5bc6\u5ea6\u81ea\u9002\u5e94\u6df7\u5408\u91c7\u6837\u7b56\u7565\uff1aTop-k\u91c7\u6837\u5173\u6ce8\u9ad8\u5bc6\u5ea6\u533a\u57df\u4fdd\u6301\u8d28\u91cf\uff0cRandom-k\u91c7\u6837\u63a2\u7d22\u4f4e\u5bc6\u5ea6\u533a\u57df\u7ef4\u6301\u591a\u6837\u6027\u3002", "result": "\u5728\u7c7b\u522b\u6761\u4ef6\u548c\u6587\u672c\u5230\u56fe\u50cf\u7684\u8bc4\u4f30\u5b9e\u9a8c\u4e2d\uff0cVAR-Scaling\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u4f18\u5316\u4e86\u5173\u952e\u5c3a\u5ea6\u4e0a\u7684\u6837\u672c\u4fdd\u771f\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "VAR-Scaling\u662f\u9996\u4e2a\u7528\u4e8e\u5411\u91cf\u91cf\u5316\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u6846\u67b6\uff0c\u6210\u529f\u514b\u670d\u4e86\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u901a\u8fc7\u51c6\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\u6620\u5c04\u548c\u6df7\u5408\u91c7\u6837\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2601.07366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07366", "abs": "https://arxiv.org/abs/2601.07366", "authors": ["Haoxuan Li", "Mengyan Li", "Junjun Zheng"], "title": "HiVid-Narrator: Hierarchical Video Narrative Generation with Scene-Primed ASR-anchored Compression", "comment": null, "summary": "Generating structured narrations for real-world e-commerce videos requires models to perceive fine-grained visual details and organize them into coherent, high-level stories--capabilities that existing approaches struggle to unify. We introduce the E-commerce Hierarchical Video Captioning (E-HVC) dataset with dual-granularity, temporally grounded annotations: a Temporal Chain-of-Thought that anchors event-level observations and Chapter Summary that compose them into concise, story-centric summaries. Rather than directly prompting chapters, we adopt a staged construction that first gathers reliable linguistic and visual evidence via curated ASR and frame-level descriptions, then refines coarse annotations into precise chapter boundaries and titles conditioned on the Temporal Chain-of-Thought, yielding fact-grounded, time-aligned narratives. We also observe that e-commerce videos are fast-paced and information-dense, with visual tokens dominating the input sequence. To enable efficient training while reducing input tokens, we propose the Scene-Primed ASR-anchored Compressor (SPA-Compressor), which compresses multimodal tokens into hierarchical scene and event representations guided by ASR semantic cues. Built upon these designs, our HiVid-Narrator framework achieves superior narrative quality with fewer input tokens compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faE-HVC\u6570\u636e\u96c6\u548cHiVid-Narrator\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7535\u5546\u89c6\u9891\u7684\u7ed3\u6784\u5316\u53d9\u4e8b\uff0c\u901a\u8fc7\u53cc\u7c92\u5ea6\u6807\u6ce8\u548c\u5206\u5c42\u538b\u7f29\u65b9\u6cd5\u63d0\u5347\u53d9\u4e8b\u8d28\u91cf\u5e76\u51cf\u5c11\u8f93\u5165token\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7edf\u4e00\u611f\u77e5\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u8fde\u8d2f\u9ad8\u7ea7\u6545\u4e8b\u7684\u80fd\u529b\uff0c\u7535\u5546\u89c6\u9891\u8282\u594f\u5feb\u3001\u4fe1\u606f\u5bc6\u96c6\uff0c\u89c6\u89c9token\u4e3b\u5bfc\u8f93\u5165\u5e8f\u5217\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5904\u7406\u65b9\u6cd5\u3002", "method": "1) \u6784\u5efaE-HVC\u6570\u636e\u96c6\uff0c\u5305\u542b\u65f6\u95f4\u94fe\u5f0f\u601d\u7ef4\u548c\u7ae0\u8282\u6458\u8981\u53cc\u7c92\u5ea6\u6807\u6ce8\uff1b2) \u91c7\u7528\u5206\u9636\u6bb5\u6784\u5efa\u65b9\u6cd5\uff0c\u5148\u6536\u96c6ASR\u548c\u5e27\u7ea7\u63cf\u8ff0\u8bc1\u636e\uff0c\u518d\u57fa\u4e8e\u65f6\u95f4\u94fe\u5f0f\u601d\u7ef4\u7cbe\u70bc\u7ae0\u8282\u8fb9\u754c\uff1b3) \u63d0\u51faSPA-Compressor\u538b\u7f29\u591a\u6a21\u6001token\u4e3a\u5206\u5c42\u573a\u666f\u548c\u4e8b\u4ef6\u8868\u793a\uff1b4) \u5efa\u7acbHiVid-Narrator\u6846\u67b6\u3002", "result": "HiVid-Narrator\u6846\u67b6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5c11\u7684\u8f93\u5165token\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u53d9\u4e8b\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u53cc\u7c92\u5ea6\u6807\u6ce8\u3001\u5206\u9636\u6bb5\u6784\u5efa\u548c\u5206\u5c42\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u751f\u6210\u4e8b\u5b9e\u57fa\u7840\u3001\u65f6\u95f4\u5bf9\u9f50\u7684\u7535\u5546\u89c6\u9891\u7ed3\u6784\u5316\u53d9\u4e8b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u611f\u77e5\u7ec6\u8282\u548c\u7ec4\u7ec7\u6545\u4e8b\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2601.07499", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07499", "abs": "https://arxiv.org/abs/2601.07499", "authors": ["Bing Yu", "Liu Shi", "Haitao Wang", "Deran Qi", "Xiang Cai", "Wei Zhong", "Qiegen Liu"], "title": "Anatomy Aware Cascade Network: Bridging Epistemic Uncertainty and Geometric Manifold for 3D Tooth Segmentation", "comment": null, "summary": "Accurate three-dimensional (3D) tooth segmentation from Cone-Beam Computed Tomography (CBCT) is a prerequisite for digital dental workflows. However, achieving high-fidelity segmentation remains challenging due to adhesion artifacts in naturally occluded scans, which are caused by low contrast and indistinct inter-arch boundaries. To address these limitations, we propose the Anatomy Aware Cascade Network (AACNet), a coarse-to-fine framework designed to resolve boundary ambiguity while maintaining global structural consistency. Specifically, we introduce two mechanisms: the Ambiguity Gated Boundary Refiner (AGBR) and the Signed Distance Map guided Anatomical Attention (SDMAA). The AGBR employs an entropy based gating mechanism to perform targeted feature rectification in high uncertainty transition zones. Meanwhile, the SDMAA integrates implicit geometric constraints via signed distance map to enforce topological consistency, preventing the loss of spatial details associated with standard pooling. Experimental results on a dataset of 125 CBCT volumes demonstrate that AACNet achieves a Dice Similarity Coefficient of 90.17 \\% and a 95\\% Hausdorff Distance of 3.63 mm, significantly outperforming state-of-the-art methods. Furthermore, the model exhibits strong generalization on an external dataset with an HD95 of 2.19 mm, validating its reliability for downstream clinical applications such as surgical planning. Code for AACNet is available at https://github.com/shiliu0114/AACNet.", "AI": {"tldr": "AACNet\u662f\u4e00\u4e2a\u7528\u4e8eCBCT\u56fe\u50cf\u4e2d\u7259\u9f7f\u4e09\u7ef4\u5206\u5272\u7684\u7ea7\u8054\u7f51\u7edc\uff0c\u901a\u8fc7\u5f15\u5165\u8fb9\u754c\u7ec6\u5316\u5668\u548c\u7b26\u53f7\u8ddd\u79bb\u56fe\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7259\u9f7f\u7c98\u8fde\u548c\u8fb9\u754c\u6a21\u7cca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "CBCT\u56fe\u50cf\u4e2d\u7259\u9f7f\u7684\u4e09\u7ef4\u5206\u5272\u662f\u6570\u5b57\u5316\u7259\u79d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u524d\u63d0\uff0c\u4f46\u7531\u4e8e\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u4e0d\u6e05\u6670\u7684\u7259\u5f13\u8fb9\u754c\u5bfc\u81f4\u7684\u7c98\u8fde\u4f2a\u5f71\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5206\u5272\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faAACNet\uff08Anatomy Aware Cascade Network\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ece\u7c97\u5230\u7ec6\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a1) Ambiguity Gated Boundary Refiner (AGBR)\uff1a\u4f7f\u7528\u57fa\u4e8e\u71b5\u7684\u95e8\u63a7\u673a\u5236\u5728\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u7684\u8fc7\u6e21\u533a\u57df\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u7279\u5f81\u6821\u6b63\uff1b2) Signed Distance Map guided Anatomical Attention (SDMAA)\uff1a\u901a\u8fc7\u7b26\u53f7\u8ddd\u79bb\u56fe\u96c6\u6210\u9690\u5f0f\u51e0\u4f55\u7ea6\u675f\uff0c\u5f3a\u5236\u6267\u884c\u62d3\u6251\u4e00\u81f4\u6027\uff0c\u9632\u6b62\u6807\u51c6\u6c60\u5316\u64cd\u4f5c\u5bfc\u81f4\u7684\u7a7a\u95f4\u7ec6\u8282\u4e22\u5931\u3002", "result": "\u5728125\u4e2aCBCT\u4f53\u79ef\u6570\u636e\u96c6\u4e0a\uff0cAACNet\u5b9e\u73b0\u4e8690.17%\u7684Dice\u76f8\u4f3c\u7cfb\u6570\u548c3.63mm\u768495% Hausdorff\u8ddd\u79bb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0cHD95\u4e3a2.19mm\u3002", "conclusion": "AACNet\u901a\u8fc7\u89e3\u51b3\u8fb9\u754c\u6a21\u7cca\u548c\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86CBCT\u7259\u9f7f\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u624b\u672f\u89c4\u5212\u7b49\u4e0b\u6e38\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.07599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07599", "abs": "https://arxiv.org/abs/2601.07599", "authors": ["Lior Dvir", "Nadav Torem", "Yoav Y. Schechner"], "title": "Diffusion in SPAD Signals", "comment": null, "summary": "We derive the likelihood of a raw signal in a single photon avalanche diode (SPAD), given a fixed photon flux. The raw signal comprises timing of detection events, which are nonlinearly related to the flux. Moreover, they are naturally stochastic. We then derive a score function of the signal. This is a key for solving inverse problems based on SPAD signals. We focus on deriving solutions involving a diffusion model, to express image priors. We demonstrate the effect of low or high photon counts, and the consequence of exploiting timing of detection events.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a8\u5bfc\u4e86\u5355\u5149\u5b50\u96ea\u5d29\u4e8c\u6781\u7ba1(SPAD)\u539f\u59cb\u4fe1\u53f7\u7684\u4f3c\u7136\u51fd\u6570\uff0c\u5efa\u7acb\u4e86\u5149\u5b50\u901a\u91cf\u4e0e\u68c0\u6d4b\u4e8b\u4ef6\u65f6\u5e8f\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u968f\u673a\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u7528\u4e8e\u9006\u95ee\u9898\u6c42\u89e3\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u3002", "motivation": "SPAD\u4fe1\u53f7\u5305\u542b\u68c0\u6d4b\u4e8b\u4ef6\u7684\u65f6\u5e8f\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u4e0e\u5149\u5b50\u901a\u91cf\u5448\u975e\u7ebf\u6027\u5173\u7cfb\u4e14\u5177\u6709\u968f\u673a\u6027\u3002\u9700\u8981\u5efa\u7acb\u51c6\u786e\u7684\u4f3c\u7136\u6a21\u578b\u6765\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4fe1\u53f7\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u5b50\u8ba1\u6570\u6761\u4ef6\u4e0b\u3002", "method": "\u63a8\u5bfc\u4e86SPAD\u539f\u59cb\u4fe1\u53f7\u7684\u4f3c\u7136\u51fd\u6570\uff0c\u5efa\u7acb\u4e86\u5149\u5b50\u901a\u91cf\u4e0e\u68c0\u6d4b\u4e8b\u4ef6\u65f6\u5e8f\u4e4b\u95f4\u7684\u6570\u5b66\u5173\u7cfb\u3002\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u4fe1\u53f7\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u6269\u6563\u6a21\u578b\u6765\u8868\u8fbe\u56fe\u50cf\u5148\u9a8c\uff0c\u4ece\u800c\u89e3\u51b3\u57fa\u4e8eSPAD\u4fe1\u53f7\u7684\u9006\u95ee\u9898\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86SPAD\u4fe1\u53f7\u7684\u5b8c\u6574\u6982\u7387\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u6c42\u89e3\u6846\u67b6\u3002\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5149\u5b50\u8ba1\u6570\u6761\u4ef6\u4e0b\uff08\u4f4e/\u9ad8\uff09\u7684\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5229\u7528\u68c0\u6d4b\u4e8b\u4ef6\u65f6\u5e8f\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSPAD\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6982\u7387\u57fa\u7840\uff0c\u6240\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u65f6\u5e8f\u4fe1\u606f\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u5b50\u8ba1\u6570\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2601.07385", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07385", "abs": "https://arxiv.org/abs/2601.07385", "authors": ["Petr Zelina", "Marko \u0158eh\u00e1\u010dek", "Jana Hal\u00e1mkov\u00e1", "Lucia Bohovicov\u00e1", "Martin Rusinko", "V\u00edt Nov\u00e1\u010dek"], "title": "Computing patient similarity based on unstructured clinical notes", "comment": "This is a preprint and has not undergone peer review. Final version was presented at the Text, Speech, and Dialogue 2025 conference. The Version of Record is available at https://doi.org/10.1007/978-3-032-02551-7_13", "summary": "Clinical notes hold rich yet unstructured details about diagnoses, treatments, and outcomes that are vital to precision medicine but hard to exploit at scale. We introduce a method that represents each patient as a matrix built from aggregated embeddings of all their notes, enabling robust patient similarity computation based on their latent low-rank representations. Using clinical notes of 4,267 Czech breast-cancer patients and expert similarity labels from Masaryk Memorial Cancer Institute, we evaluate several matrix-based similarity measures and analyze their strengths and limitations across different similarity facets, such as clinical history, treatment, and adverse events. The results demonstrate the usefulness of the presented method for downstream tasks, such as personalized therapy recommendations or toxicity warnings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8a\u7b14\u8bb0\u7684\u60a3\u8005\u77e9\u9635\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u7b14\u8bb0\u5d4c\u5165\u6784\u5efa\u60a3\u8005\u77e9\u9635\uff0c\u5229\u7528\u4f4e\u79e9\u8868\u793a\u8ba1\u7b97\u60a3\u8005\u76f8\u4f3c\u5ea6\uff0c\u5e76\u5728\u4e73\u817a\u764c\u60a3\u8005\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u975e\u7ed3\u6784\u5316\u8bca\u65ad\u3001\u6cbb\u7597\u548c\u7ed3\u679c\u4fe1\u606f\uff0c\u5bf9\u7cbe\u51c6\u533b\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u96be\u4ee5\u5927\u89c4\u6a21\u5229\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u63d0\u53d6\u548c\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u7684\u65b9\u6cd5\u6765\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u5c06\u6bcf\u4e2a\u60a3\u8005\u8868\u793a\u4e3a\u4ece\u6240\u6709\u4e34\u5e8a\u7b14\u8bb0\u805a\u5408\u5d4c\u5165\u6784\u5efa\u7684\u77e9\u9635\uff0c\u57fa\u4e8e\u6f5c\u5728\u4f4e\u79e9\u8868\u793a\u8fdb\u884c\u7a33\u5065\u7684\u60a3\u8005\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u3002\u57284,267\u540d\u6377\u514b\u4e73\u817a\u764c\u60a3\u8005\u7684\u4e34\u5e8a\u7b14\u8bb0\u4e0a\uff0c\u4f7f\u7528\u591a\u4e2a\u77e9\u9635\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u5728\u4e34\u5e8a\u5386\u53f2\u3001\u6cbb\u7597\u548c\u4e0d\u826f\u4e8b\u4ef6\u7b49\u4e0d\u540c\u76f8\u4f3c\u5ea6\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u5982\u4e2a\u6027\u5316\u6cbb\u7597\u63a8\u8350\u6216\u6bd2\u6027\u9884\u8b66\u3002\u4f7f\u7528Masaryk\u7eaa\u5ff5\u764c\u75c7\u7814\u7a76\u6240\u7684\u4e13\u5bb6\u76f8\u4f3c\u5ea6\u6807\u7b7e\u8fdb\u884c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u60a3\u8005\u77e9\u9635\u8868\u793a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597\u5e94\u7528\u5982\u4e2a\u6027\u5316\u6cbb\u7597\u63a8\u8350\u548c\u6bd2\u6027\u9884\u8b66\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2601.07582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07582", "abs": "https://arxiv.org/abs/2601.07582", "authors": ["Huhai Zou", "Tianhao Sun", "Chuanjiang He", "Yu Tian", "Zhenyang Li", "Li Jin", "Nayu Liu", "Jiang Zhong", "Kaiwen Wei"], "title": "ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents", "comment": null, "summary": "Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. While existing memory mechanisms offer basic storage and retrieval capabilities, they are hindered by two primary limitations: (1) rigid memory granularity often disrupts semantic integrity, resulting in fragmented and incoherent memory units; (2) prevalent flat retrieval paradigms rely solely on surface-level semantic similarity, neglecting the structural cues of discourse required to navigate and locate specific episodic contexts. To mitigate these limitations, drawing inspiration from Event Segmentation Theory, we propose ES-Mem, a framework incorporating two core components: (1) a dynamic event segmentation module that partitions long-term interactions into semantically coherent events with distinct boundaries; (2) a hierarchical memory architecture that constructs multi-layered memories and leverages boundary semantics to anchor specific episodic memory for precise context localization. Evaluations on two memory benchmarks demonstrate that ES-Mem yields consistent performance gains over baseline methods. Furthermore, the proposed event segmentation module exhibits robust applicability on dialogue segmentation datasets.", "AI": {"tldr": "ES-Mem\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u5206\u5272\u7406\u8bba\u7684\u5bf9\u8bdd\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4e8b\u4ef6\u5206\u5272\u548c\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\u89e3\u51b3\u73b0\u6709\u8bb0\u5fc6\u673a\u5236\u5728\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u68c0\u7d22\u7cbe\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u8bb0\u5fc6\u673a\u5236\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u50f5\u5316\u7684\u8bb0\u5fc6\u7c92\u5ea6\u7834\u574f\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u5355\u5143\u788e\u7247\u5316\uff1b2\uff09\u6241\u5e73\u7684\u68c0\u7d22\u8303\u5f0f\u4ec5\u4f9d\u8d56\u8868\u5c42\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u5bf9\u8bdd\u7ed3\u6784\u7ebf\u7d22\uff0c\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u7279\u5b9a\u60c5\u666f\u8bb0\u5fc6\u3002", "method": "\u53d7\u4e8b\u4ef6\u5206\u5272\u7406\u8bba\u542f\u53d1\uff0c\u63d0\u51faES-Mem\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u52a8\u6001\u4e8b\u4ef6\u5206\u5272\u6a21\u5757\uff0c\u5c06\u957f\u671f\u4ea4\u4e92\u5212\u5206\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u4e8b\u4ef6\u5e76\u8bc6\u522b\u8fb9\u754c\uff1b2\uff09\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\uff0c\u6784\u5efa\u591a\u5c42\u8bb0\u5fc6\u5e76\u5229\u7528\u8fb9\u754c\u8bed\u4e49\u951a\u5b9a\u7279\u5b9a\u60c5\u666f\u8bb0\u5fc6\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4e0a\u4e0b\u6587\u5b9a\u4f4d\u3002", "result": "\u5728\u4e24\u4e2a\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cES-Mem\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684\u4e8b\u4ef6\u5206\u5272\u6a21\u5757\u5728\u5bf9\u8bdd\u5206\u5272\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u7528\u6027\u3002", "conclusion": "ES-Mem\u901a\u8fc7\u4e8b\u4ef6\u5206\u5272\u548c\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u8bdd\u8bb0\u5fc6\u673a\u5236\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u8bb0\u5fc6\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u68c0\u7d22\u7cbe\u5ea6\uff0c\u4e3a\u957f\u671f\u5bf9\u8bdd\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bb0\u5fc6\u652f\u6301\u3002"}}
{"id": "2601.07473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07473", "abs": "https://arxiv.org/abs/2601.07473", "authors": ["Michael J. Clark"], "title": "AntiPaSTO: Self-Supervised Steering of Moral Reasoning", "comment": null, "summary": "As models grow more capable, human supervision breaks down: labels don't scale, outputs can be gamed, and training doesn't generalize. Scalable oversight requires steering methods that are internal, self-supervised, and transfer out-of-distribution; existing methods satisfy some but not all three. We introduce AntiPaSTO, which separates representations along an anti-parallel axis ($\u03b1=\\pm1$ produce opposite shifts), with coherence constraints preventing collapse. Human input is minimal: two contrasting words inserted into template sentences, no preference labels. Using 800 such pairs on Gemma-3-1B, AntiPaSTO beats prompting baselines by $6.9\\times$ on DailyDilemmas and maintains bidirectional control where prompting triggers refusal.\n  Code is available at https://github.com/wassname/AntiPaSTO.", "AI": {"tldr": "AntiPaSTO\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u6269\u5c55\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5e73\u884c\u8f74\u5206\u79bb\u8868\u793a\uff0c\u4ec5\u9700\u5c11\u91cf\u4eba\u7c7b\u8f93\u5165\uff08\u4e24\u4e2a\u5bf9\u6bd4\u8bcd\uff09\uff0c\u5728Gemma-3-1B\u4e0a\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u57fa\u7ebf", "motivation": "\u968f\u7740\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u4eba\u7c7b\u76d1\u7763\u9762\u4e34\u6311\u6218\uff1a\u6807\u7b7e\u65e0\u6cd5\u6269\u5c55\u3001\u8f93\u51fa\u53ef\u80fd\u88ab\u64cd\u63a7\u3001\u8bad\u7ec3\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002\u9700\u8981\u6ee1\u8db3\u5185\u90e8\u6027\u3001\u81ea\u76d1\u7763\u6027\u548c\u5206\u5e03\u5916\u8fc1\u79fb\u6027\u7684\u53ef\u6269\u5c55\u76d1\u7763\u65b9\u6cd5", "method": "AntiPaSTO\u65b9\u6cd5\uff1a\u6cbf\u53cd\u5e73\u884c\u8f74\u5206\u79bb\u8868\u793a\uff08\u03b1=\u00b11\u4ea7\u751f\u76f8\u53cd\u504f\u79fb\uff09\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u7ea6\u675f\u9632\u6b62\u5d29\u6e83\u3002\u4eba\u7c7b\u8f93\u5165\u6781\u5c11\uff1a\u53ea\u9700\u5728\u6a21\u677f\u53e5\u5b50\u4e2d\u63d2\u5165\u4e24\u4e2a\u5bf9\u6bd4\u8bcd\uff0c\u65e0\u9700\u504f\u597d\u6807\u7b7e", "result": "\u4f7f\u7528800\u4e2a\u8bcd\u5bf9\u5728Gemma-3-1B\u4e0a\uff0cAntiPaSTO\u5728DailyDilemmas\u4e0a\u6bd4\u63d0\u793a\u57fa\u7ebf\u63d0\u53476.9\u500d\uff0c\u5e76\u4fdd\u6301\u53cc\u5411\u63a7\u5236\u80fd\u529b\uff08\u63d0\u793a\u4f1a\u89e6\u53d1\u62d2\u7edd\uff09", "conclusion": "AntiPaSTO\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u76d1\u7763\u65b9\u6cd5\uff0c\u4ec5\u9700\u6781\u5c11\u4eba\u7c7b\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u8868\u793a\u5206\u79bb\u548c\u53cc\u5411\u63a7\u5236\uff0c\u4e3a\u5927\u578b\u6a21\u578b\u76d1\u7763\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.07765", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07765", "abs": "https://arxiv.org/abs/2601.07765", "authors": ["Igor Sterner", "Alex Lascarides", "Frank Keller"], "title": "Contrastive Learning with Narrative Twins for Modeling Story Salience", "comment": "EACL 2026", "summary": "Understanding narratives requires identifying which events are most salient for a story's progression. We present a contrastive learning framework for modeling narrative salience that learns story embeddings from narrative twins: stories that share the same plot but differ in surface form. Our model is trained to distinguish a story from both its narrative twin and a distractor with similar surface features but different plot. Using the resulting embeddings, we evaluate four narratologically motivated operations for inferring salience (deletion, shifting, disruption, and summarization). Experiments on short narratives from the ROCStories corpus and longer Wikipedia plot summaries show that contrastively learned story embeddings outperform a masked-language-model baseline, and that summarization is the most reliable operation for identifying salient sentences. If narrative twins are not available, random dropout can be used to generate the twins from a single story. Effective distractors can be obtained either by prompting LLMs or, in long-form narratives, by using different parts of the same story.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53d9\u4e8b\u5b6a\u751f\uff08\u76f8\u540c\u60c5\u8282\u4e0d\u540c\u8868\u9762\u5f62\u5f0f\u7684\u6545\u4e8b\uff09\u5b66\u4e60\u6545\u4e8b\u5d4c\u5165\uff0c\u7528\u4e8e\u5efa\u6a21\u53d9\u4e8b\u663e\u8457\u6027\uff0c\u5e76\u8bc4\u4f30\u56db\u79cd\u53d9\u4e8b\u5b66\u64cd\u4f5c\u6765\u8bc6\u522b\u5173\u952e\u53e5\u5b50\u3002", "motivation": "\u7406\u89e3\u53d9\u4e8b\u9700\u8981\u8bc6\u522b\u5bf9\u6545\u4e8b\u8fdb\u5c55\u6700\u5173\u952e\u7684\u4e8b\u4ef6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u53d9\u4e8b\u663e\u8457\u6027\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u6a21\u578b\u533a\u5206\u6545\u4e8b\u4e0e\u5176\u53d9\u4e8b\u5b6a\u751f\uff08\u76f8\u540c\u60c5\u8282\u4e0d\u540c\u8868\u9762\u5f62\u5f0f\uff09\u4ee5\u53ca\u8868\u9762\u76f8\u4f3c\u4f46\u60c5\u8282\u4e0d\u540c\u7684\u5e72\u6270\u9879\uff0c\u5b66\u4e60\u6545\u4e8b\u5d4c\u5165\uff0c\u5e76\u8bc4\u4f30\u5220\u9664\u3001\u79fb\u4f4d\u3001\u7834\u574f\u548c\u6458\u8981\u56db\u79cd\u53d9\u4e8b\u5b66\u64cd\u4f5c\u3002", "result": "\u5bf9\u6bd4\u5b66\u4e60\u7684\u6545\u4e8b\u5d4c\u5165\u4f18\u4e8e\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\uff0c\u6458\u8981\u64cd\u4f5c\u662f\u8bc6\u522b\u663e\u8457\u53e5\u5b50\u6700\u53ef\u9760\u7684\u65b9\u6cd5\uff1b\u5f53\u53d9\u4e8b\u5b6a\u751f\u4e0d\u53ef\u7528\u65f6\uff0c\u968f\u673a\u4e22\u5f03\u53ef\u751f\u6210\u5b6a\u751f\uff0cLLM\u63d0\u793a\u6216\u540c\u4e00\u6545\u4e8b\u7684\u4e0d\u540c\u90e8\u5206\u53ef\u751f\u6210\u6709\u6548\u5e72\u6270\u9879\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u53d9\u4e8b\u663e\u8457\u6027\uff0c\u6458\u8981\u64cd\u4f5c\u662f\u6700\u53ef\u9760\u7684\u663e\u8457\u6027\u8bc6\u522b\u65b9\u6cd5\uff0c\u4e14\u8be5\u65b9\u6cd5\u5728\u7f3a\u4e4f\u53d9\u4e8b\u5b6a\u751f\u65f6\u4ecd\u5177\u5b9e\u7528\u6027\u3002"}}
