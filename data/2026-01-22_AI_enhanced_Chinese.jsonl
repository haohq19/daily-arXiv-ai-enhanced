{"id": "2601.14550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14550", "abs": "https://arxiv.org/abs/2601.14550", "authors": ["Tailai Cheng", "Kejia Chen", "Lingyun Chen", "Liding Zhang", "Yue Zhang", "Yao Ling", "Mahdi Hamad", "Zhenshan Bing", "Fan Wu", "Karan Sharma", "Alois Knoll"], "title": "TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks", "comment": null, "summary": "Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.", "AI": {"tldr": "TacUMI\uff1a\u96c6\u6210\u591a\u6a21\u6001\u4f20\u611f\u5668\u7684\u624b\u6301\u6f14\u793a\u7cfb\u7edf\uff0c\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u6f14\u793a\u6570\u636e\u91c7\u96c6\u4e0e\u5206\u5272", "motivation": "\u590d\u6742\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u4efb\u52a1\u5206\u89e3\uff0c\u4ec5\u4f9d\u8d56\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u4fe1\u606f\u96be\u4ee5\u63ed\u793a\u4e8b\u4ef6\u8f6c\u6362\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u548c\u9c81\u68d2\u5206\u5272\u65b9\u6cd5", "method": "\u57fa\u4e8eUMI\u624b\u6301\u6f14\u793a\u8bbe\u5907\uff0c\u96c6\u6210ViTac\u4f20\u611f\u5668\u3001\u529b\u626d\u77e9\u4f20\u611f\u5668\u548c\u59ff\u6001\u8ddf\u8e2a\u5668\uff0c\u6784\u5efa\u7d27\u51d1\u7684\u673a\u5668\u4eba\u517c\u5bb9\u5939\u722a\u8bbe\u8ba1\uff1b\u63d0\u51fa\u591a\u6a21\u6001\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528\u65f6\u5e8f\u6a21\u578b\u68c0\u6d4b\u8bed\u4e49\u6709\u610f\u4e49\u7684\u4e8b\u4ef6\u8fb9\u754c", "result": "\u5728\u7535\u7f06\u5b89\u88c5\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u5206\u5272\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u591a\u6a21\u6001\u96c6\u6210\u5e26\u6765\u663e\u8457\u6539\u8fdb", "conclusion": "TacUMI\u4e3a\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u591a\u6a21\u6001\u6f14\u793a\u6570\u636e\u53ef\u6269\u5c55\u91c7\u96c6\u548c\u5206\u5272\u5efa\u7acb\u4e86\u5b9e\u7528\u57fa\u7840"}}
{"id": "2601.14330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14330", "abs": "https://arxiv.org/abs/2601.14330", "authors": ["Mengyu Sun", "Ziyuan Yang", "Andrew Beng Jin Teoh", "Junxu Liu", "Haibo Hu", "Yi Zhang"], "title": "LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models", "comment": null, "summary": "Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLURE\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u6f5c\u5728\u7a7a\u95f4\u548c\u5f15\u5bfc\u91c7\u6837\u8f68\u8ff9\u6765\u91cd\u65b0\u5524\u9192\u88ab\u64e6\u9664\u7684\u6982\u5ff5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u7684\u6f0f\u6d1e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5b58\u5728\u6f0f\u6d1e\uff0c\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u4ecd\u80fd\u88ab\u91cd\u65b0\u5524\u9192\u3002\u73b0\u6709\u91cd\u65b0\u5524\u9192\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u7ea7\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u751f\u6210\u56e0\u7d20\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u5e95\u5c42\u52a8\u6001\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u5c06\u751f\u6210\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9690\u51fd\u6570\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51faLURE\u65b9\u6cd5\uff1a1\uff09\u8bed\u4e49\u91cd\u65b0\u7ed1\u5b9a\u673a\u5236\u91cd\u6784\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u68af\u5ea6\u573a\u6b63\u4ea4\u5316\u9632\u6b62\u591a\u6982\u5ff5\u573a\u666f\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\uff1b3\uff09\u6f5c\u5728\u8bed\u4e49\u8bc6\u522b\u5f15\u5bfc\u91c7\u6837\u786e\u4fdd\u91cd\u65b0\u5524\u9192\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLURE\u80fd\u591f\u5728\u591a\u79cd\u64e6\u9664\u4efb\u52a1\u548c\u65b9\u6cd5\u4e2d\uff0c\u540c\u65f6\u9ad8\u8d28\u91cf\u5730\u91cd\u65b0\u5524\u9192\u591a\u4e2a\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u3002", "conclusion": "\u901a\u8fc7\u5168\u9762\u5206\u6790\u751f\u6210\u8fc7\u7a0b\u7684\u591a\u4e2a\u56e0\u7d20\uff0c\u63d0\u51fa\u7684LURE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u7684\u6f0f\u6d1e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u9ad8\u8d28\u91cf\u7684\u6982\u5ff5\u91cd\u65b0\u5524\u9192\u3002"}}
{"id": "2601.14628", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14628", "abs": "https://arxiv.org/abs/2601.14628", "authors": ["Weiyu Guo", "He Zhang", "Pengteng Li", "Tiefu Cai", "Ziyang Chen", "Yandong Guo", "Xiao He", "Yongkui Yang", "Ying Sun", "Hui Xiong"], "title": "A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control", "comment": null, "summary": "Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.", "AI": {"tldr": "NeuroVLA\uff1a\u9996\u4e2a\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u7684\u795e\u7ecf\u5f62\u6001\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u6a21\u4eff\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7ed3\u6784\uff0c\u5b9e\u73b0\u52a8\u6001\u7a33\u5b9a\u6027\u3001\u53cd\u5c04\u54cd\u5e94\u548c\u65f6\u95f4\u8bb0\u5fc6\u80fd\u529b", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7b56\u7565\u96be\u4ee5\u590d\u5236\u751f\u7269\u8fd0\u52a8\u7684\u52a8\u6001\u7a33\u5b9a\u6027\u3001\u53cd\u5c04\u54cd\u5e94\u548c\u65f6\u95f4\u8bb0\u5fc6\u80fd\u529b\uff0c\u800c\u751f\u7269\u7cfb\u7edf\u80fd\u4ece\u7a00\u758f\u7ecf\u9a8c\u4e2d\u5feb\u901f\u5b66\u4e60\u6280\u80fd", "method": "\u91c7\u7528\u7cfb\u7edf\u7ea7\u4eff\u751f\u8bbe\u8ba1\uff1a\u9ad8\u5c42\u6a21\u578b\u89c4\u5212\u76ee\u6807\uff0c\u81ea\u9002\u5e94\u5c0f\u8111\u6a21\u5757\u4f7f\u7528\u9ad8\u9891\u4f20\u611f\u5668\u53cd\u9988\u7a33\u5b9a\u8fd0\u52a8\uff0c\u4eff\u751f\u810a\u9ad3\u5c42\u6267\u884c\u5feb\u901f\u52a8\u4f5c\u751f\u6210", "result": "\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u4e0a\u4ec5\u6d88\u80170.4w\u80fd\u91cf\uff0c\u6d88\u9664\u673a\u68b0\u81c2\u6296\u52a8\uff0c\u5c55\u73b0\u65f6\u95f4\u8bb0\u5fc6\u80fd\u529b\uff0c\u5b89\u5168\u53cd\u5c04\u54cd\u5e94\u65f6\u95f4\u5c0f\u4e8e20\u6beb\u79d2", "conclusion": "NeuroVLA\u6846\u67b6\u6210\u529f\u6a21\u4eff\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7ed3\u6784\uff0c\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u751f\u7269\u8fd0\u52a8\u7279\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u4eff\u751f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.14304", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14304", "abs": "https://arxiv.org/abs/2601.14304", "authors": ["Juncheng Wang", "Zhe Hu", "Chao Xu", "Siyue Ren", "Yuxiang Feng", "Yang Liu", "Baigui Sun", "Shujun Wang"], "title": "Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding", "comment": "Accepted at EACL 2026", "summary": "Autoregressive (AR) models excel at generating temporally coherent audio by producing tokens sequentially, yet they often falter in faithfully following complex textual prompts, especially those describing complex sound events. We uncover a surprising capability in AR audio generators: their early prefix tokens implicitly encode global semantic attributes of the final output, such as event count and sound-object category, revealing a form of implicit planning. Building on this insight, we propose Plan-Critic, a lightweight auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict final instruction-following quality from partial generations. At inference time, Plan-Critic enables guided exploration: it evaluates candidate prefixes early, prunes low-fidelity trajectories, and reallocates computation to high-potential planning seeds. Our Plan-Critic-guided sampling achieves up to a 10-point improvement in CLAP score over the AR baseline-establishing a new state of the art in AR text-to-audio generation-while maintaining computational parity with standard best-of-N decoding. This work bridges the gap between causal generation and global semantic alignment, demonstrating that even strictly autoregressive models can plan ahead.", "AI": {"tldr": "AR\u97f3\u9891\u751f\u6210\u5668\u65e9\u671f\u524d\u7f00token\u9690\u542b\u7f16\u7801\u5168\u5c40\u8bed\u4e49\u5c5e\u6027\uff0c\u4f5c\u8005\u63d0\u51faPlan-Critic\u6a21\u578b\u901a\u8fc7\u65e9\u671f\u8bc4\u4f30\u5019\u9009\u524d\u7f00\u6765\u5f15\u5bfc\u751f\u6210\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6307\u4ee4\u8ddf\u968f\u80fd\u529b", "motivation": "\u81ea\u56de\u5f52\u97f3\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u65f6\u5e8f\u8fde\u8d2f\u7684\u97f3\u9891\uff0c\u4f46\u5728\u9075\u5faa\u590d\u6742\u6587\u672c\u63d0\u793a\uff08\u7279\u522b\u662f\u63cf\u8ff0\u590d\u6742\u58f0\u97f3\u4e8b\u4ef6\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u4f5c\u8005\u53d1\u73b0AR\u6a21\u578b\u7684\u65e9\u671f\u524d\u7f00token\u9690\u542b\u7f16\u7801\u4e86\u6700\u7ec8\u8f93\u51fa\u7684\u5168\u5c40\u8bed\u4e49\u5c5e\u6027\uff0c\u8fd9\u63ed\u793a\u4e86\u4e00\u79cd\u9690\u5f0f\u89c4\u5212\u80fd\u529b", "method": "\u63d0\u51faPlan-Critic\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6a21\u578b\uff0c\u4f7f\u7528GAE\u542f\u53d1\u7684\u76ee\u6807\u51fd\u6570\u8bad\u7ec3\uff0c\u4ece\u90e8\u5206\u751f\u6210\u4e2d\u9884\u6d4b\u6700\u7ec8\u6307\u4ee4\u8ddf\u968f\u8d28\u91cf\u3002\u5728\u63a8\u7406\u65f6\uff0cPlan-Critic\u8bc4\u4f30\u5019\u9009\u524d\u7f00\uff0c\u526a\u679d\u4f4e\u8d28\u91cf\u8f68\u8ff9\uff0c\u5c06\u8ba1\u7b97\u8d44\u6e90\u91cd\u65b0\u5206\u914d\u7ed9\u9ad8\u6f5c\u529b\u89c4\u5212\u79cd\u5b50", "result": "Plan-Critic\u5f15\u5bfc\u7684\u91c7\u6837\u5728CLAP\u5206\u6570\u4e0a\u6bd4AR\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe10\u5206\uff0c\u5728AR\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6best-of-N\u89e3\u7801\u76f8\u540c\u7684\u8ba1\u7b97\u590d\u6742\u5ea6", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f25\u5408\u4e86\u56e0\u679c\u751f\u6210\u4e0e\u5168\u5c40\u8bed\u4e49\u5bf9\u9f50\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8bc1\u660e\u5373\u4f7f\u662f\u4e25\u683c\u7684\u81ea\u56de\u5f52\u6a21\u578b\u4e5f\u80fd\u8fdb\u884c\u524d\u77bb\u6027\u89c4\u5212\u3002Plan-Critic\u901a\u8fc7\u65e9\u671f\u8bc4\u4f30\u5019\u9009\u524d\u7f00\u5b9e\u73b0\u4e86\u5f15\u5bfc\u5f0f\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86AR\u97f3\u9891\u751f\u6210\u5668\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b"}}
{"id": "2601.14530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14530", "abs": "https://arxiv.org/abs/2601.14530", "authors": ["Xiaoyan Kui", "Zijie Fan", "Zexin Ji", "Qinsong Li", "Hao Xu", "Weixin Si", "Haodong Xu", "Beiji Zou"], "title": "PAS-Mamba: Phase-Amplitude-Spatial State Space Model for MRI Reconstruction", "comment": null, "summary": "Joint feature modeling in both the spatial and frequency domains has become a mainstream approach in MRI reconstruction. However, existing methods generally treat the frequency domain as a whole, neglecting the differences in the information carried by its internal components. According to Fourier transform theory, phase and amplitude represent different types of information in the image. Our spectrum swapping experiments show that magnitude mainly reflects pixel-level intensity, while phase predominantly governs image structure. To prevent interference between phase and magnitude feature learning caused by unified frequency-domain modeling, we propose the Phase-Amplitude-Spatial State Space Model (PAS-Mamba) for MRI Reconstruction, a framework that decouples phase and magnitude modeling in the frequency domain and combines it with image-domain features for better reconstruction. In the image domain, LocalMamba preserves spatial locality to sharpen fine anatomical details. In frequency domain, we disentangle amplitude and phase into two specialized branches to avoid representational coupling. To respect the concentric geometry of frequency information, we propose Circular Frequency Domain Scanning (CFDS) to serialize features from low to high frequencies. Finally, a Dual-Domain Complementary Fusion Module (DDCFM) adaptively fuses amplitude phase representations and enables bidirectional exchange between frequency and image domains, delivering superior reconstruction. Extensive experiments on the IXI and fastMRI knee datasets show that PAS-Mamba consistently outperforms state of the art reconstruction methods.", "AI": {"tldr": "PAS-Mamba\uff1a\u4e00\u79cd\u7528\u4e8eMRI\u91cd\u5efa\u7684\u76f8\u4f4d-\u5e45\u5ea6-\u7a7a\u95f4\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u9891\u57df\u89e3\u8026\u76f8\u4f4d\u548c\u5e45\u5ea6\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf\u57df\u7279\u5f81\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709MRI\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u5c06\u9891\u57df\u89c6\u4e3a\u6574\u4f53\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u5176\u5185\u90e8\u76f8\u4f4d\u548c\u5e45\u5ea6\u5206\u91cf\u643a\u5e26\u7684\u4e0d\u540c\u4fe1\u606f\u7c7b\u578b\u3002\u76f8\u4f4d\u4e3b\u8981\u63a7\u5236\u56fe\u50cf\u7ed3\u6784\uff0c\u5e45\u5ea6\u53cd\u6620\u50cf\u7d20\u7ea7\u5f3a\u5ea6\uff0c\u7edf\u4e00\u5efa\u6a21\u4f1a\u5bfc\u81f4\u7279\u5f81\u5b66\u4e60\u5e72\u6270\u3002", "method": "\u63d0\u51faPAS-Mamba\u6846\u67b6\uff1a1\uff09\u56fe\u50cf\u57df\u4f7f\u7528LocalMamba\u4fdd\u6301\u7a7a\u95f4\u5c40\u90e8\u6027\uff1b2\uff09\u9891\u57df\u5c06\u5e45\u5ea6\u548c\u76f8\u4f4d\u89e3\u8026\u4e3a\u4e24\u4e2a\u4e13\u95e8\u5206\u652f\uff1b3\uff09\u63d0\u51fa\u5706\u5f62\u9891\u57df\u626b\u63cf\uff08CFDS\uff09\u6309\u9891\u7387\u987a\u5e8f\u5e8f\u5217\u5316\u7279\u5f81\uff1b4\uff09\u53cc\u57df\u4e92\u8865\u878d\u5408\u6a21\u5757\uff08DDCFM\uff09\u81ea\u9002\u5e94\u878d\u5408\u5e45\u5ea6\u76f8\u4f4d\u8868\u793a\u5e76\u5b9e\u73b0\u9891\u57df\u4e0e\u56fe\u50cf\u57df\u53cc\u5411\u4ea4\u6362\u3002", "result": "\u5728IXI\u548cfastMRI\u819d\u5173\u8282\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPAS-Mamba\u5728MRI\u91cd\u5efa\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u9891\u57df\u76f8\u4f4d\u548c\u5e45\u5ea6\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf\u57df\u7279\u5f81\uff0cPAS-Mamba\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u9891\u57df\u4fe1\u606f\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684MRI\u91cd\u5efa\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u5206\u79bb\u5904\u7406\u76f8\u4f4d\u548c\u5e45\u5ea6\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.14473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14473", "abs": "https://arxiv.org/abs/2601.14473", "authors": ["Danny Butvinik", "Nana Boateng", "Achi Hackmon"], "title": "Adaptive KDE for Real-Time Thresholding: Prioritized Queues for Financial Crime Investigation", "comment": null, "summary": "We study the problem of converting a stream of risk scores into one or more review queues under explicit intake constraints[cite: 6]. Instead of top-$K$ or manually tuned cutoffs, we fit an online adaptive kernel density to the score stream, transform the density into a tail-mass curve to meet capacity, and ``snap'' the resulting cut to a persistent density valley detected across bandwidths[cite: 7]. The procedure is label-free, supports multi-queue routing, and operates in real time with sliding windows or exponential forgetting[cite: 8]. On synthetic, drifting, multimodal streams, the method achieves competitive capacity adherence while reducing threshold jitter[cite: 9]. Updates cost $O(G)$ per event with constant memory per activity", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5c06\u98ce\u9669\u8bc4\u5206\u6d41\u8f6c\u6362\u4e3a\u591a\u4e2a\u5ba1\u6838\u961f\u5217\uff0c\u65e0\u9700\u6807\u7b7e\u4e14\u80fd\u5b9e\u65f6\u8fd0\u884c\uff0c\u901a\u8fc7\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u5c3e\u8d28\u91cf\u66f2\u7ebf\u6ee1\u8db3\u5bb9\u91cf\u7ea6\u675f\uff0c\u51cf\u5c11\u9608\u503c\u6296\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982top-K\u6216\u624b\u52a8\u8c03\u6574\u9608\u503c\uff09\u5728\u5c06\u98ce\u9669\u8bc4\u5206\u6d41\u8f6c\u6362\u4e3a\u5ba1\u6838\u961f\u5217\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u6807\u7b7e\u65e0\u5173\u3001\u80fd\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3001\u652f\u6301\u591a\u961f\u5217\u8def\u7531\u4e14\u6ee1\u8db3\u660e\u786e\u5bb9\u91cf\u7ea6\u675f\u7684\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5bf9\u8bc4\u5206\u6d41\u62df\u5408\u5728\u7ebf\u81ea\u9002\u5e94\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff0c\u5c06\u5bc6\u5ea6\u8f6c\u6362\u4e3a\u5c3e\u8d28\u91cf\u66f2\u7ebf\u4ee5\u6ee1\u8db3\u5bb9\u91cf\u7ea6\u675f\uff0c\u5e76\u5c06\u7ed3\u679c\"\u6355\u6349\"\u5230\u8de8\u5e26\u5bbd\u68c0\u6d4b\u5230\u7684\u6301\u4e45\u5bc6\u5ea6\u8c37\u503c\uff0c\u652f\u6301\u6ed1\u52a8\u7a97\u53e3\u6216\u6307\u6570\u9057\u5fd8\u7684\u5b9e\u65f6\u64cd\u4f5c\u3002", "result": "\u5728\u5408\u6210\u3001\u6f02\u79fb\u3001\u591a\u6a21\u6001\u6d41\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5bb9\u91cf\u9075\u5b88\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u9608\u503c\u6296\u52a8\uff0c\u6bcf\u6b21\u4e8b\u4ef6\u66f4\u65b0\u6210\u672c\u4e3aO(G)\uff0c\u6bcf\u4e2a\u6d3b\u52a8\u5360\u7528\u6052\u5b9a\u5185\u5b58\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6807\u7b7e\u65e0\u5173\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5c06\u98ce\u9669\u8bc4\u5206\u6d41\u8f6c\u6362\u4e3a\u5ba1\u6838\u961f\u5217\uff0c\u5728\u6ee1\u8db3\u5bb9\u91cf\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u9608\u503c\u51b3\u7b56\u3002"}}
{"id": "2601.14594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14594", "abs": "https://arxiv.org/abs/2601.14594", "authors": ["Lianying Chao", "Linfeng Yin", "Peiyu Ren", "Yifan Jiang", "Qiaoyu Ren", "Dingcheng Shan", "Jing-cheng Pang", "Sijie Wu", "Xubin Li", "Kai Zhang"], "title": "LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning", "comment": null, "summary": "Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53ef\u5b66\u4e60\u5e27\u9009\u62e9\u5668(LFS)\uff0c\u901a\u8fc7\u9009\u62e9\u65f6\u95f4\u591a\u6837\u4e14\u4e8b\u4ef6\u76f8\u5173\u7684\u5e27\u6765\u6539\u8fdb\u89c6\u9891\u63cf\u8ff0\u751f\u6210\uff0c\u5e76\u5f15\u5165\u65b0\u57fa\u51c6ICH-CC\u6765\u8bc4\u4f30\u4eba\u7c7b\u4e00\u81f4\u6027\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u4f7f\u7528\u5747\u5300\u91c7\u6837\u5e27\uff0c\u4f46\u5ffd\u7565\u4e86\u89c6\u9891\u4e8b\u4ef6\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u6355\u6349\u91cd\u8981\u4e8b\u4ef6\u3002\u540c\u65f6\u73b0\u6709\u57fa\u51c6\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u63d0\u51faLFS\u6846\u67b6\uff1a1) \u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u91cd\u8981\u6027\u4ee5\u5e73\u8861\u65f6\u95f4\u591a\u6837\u6027\u548c\u4e8b\u4ef6\u76f8\u5173\u6027\uff1b2) \u91c7\u7528\u5206\u5c42\u7b56\u7565\u786e\u4fdd\u65f6\u95f4\u8986\u76d6\u5e76\u907f\u514d\u805a\u7c7b\uff1b3) \u5229\u7528\u51bb\u7ed3\u89c6\u9891LLM\u7684caption\u53cd\u9988\u6765\u5b66\u4e60\u76f4\u63a5\u4f18\u5316\u4e0b\u6e38\u63cf\u8ff0\u8d28\u91cf\u7684\u5e27\u9009\u62e9\u3002", "result": "LFS\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u793e\u533a\u57fa\u51c6\u548cICH-CC\u4e0a\u4e00\u81f4\u6539\u8fdb\u8be6\u7ec6\u89c6\u9891\u63cf\u8ff0\uff0c\u5728VDC\u4e0a\u63d0\u5347\u8fbe2.0%\uff0c\u5728ICH-CC\u4e0a\u63d0\u5347\u8d854%\u3002\u589e\u5f3a\u7684\u63cf\u8ff0\u8fd8\u63d0\u9ad8\u4e86\u89c6\u9891\u95ee\u7b54\u6027\u80fd\u3002", "conclusion": "LFS\u4e3a\u8be6\u7ec6\u89c6\u9891\u63cf\u8ff0\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u667a\u80fd\u5e27\u9009\u62e9\u663e\u8457\u63d0\u5347\u63cf\u8ff0\u8d28\u91cf\uff0c\u540c\u65f6\u65b0\u57fa\u51c6ICH-CC\u66f4\u597d\u5730\u53cd\u6620\u4e86\u4eba\u7c7b\u4e00\u81f4\u6027\u7406\u89e3\u3002"}}
{"id": "2601.15018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15018", "abs": "https://arxiv.org/abs/2601.15018", "authors": ["Leon Tolksdorf", "Arturo Tejada", "Jonas Bauernfeind", "Christian Birkner", "Nathan van de Wouw"], "title": "Risk Estimation for Automated Driving", "comment": "10 pages, 5 figures", "summary": "Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u78b0\u649e\u6982\u7387\u4f30\u8ba1\u4e0e\u78b0\u649e\u4e25\u91cd\u6027\u7684\u901a\u7528\u98ce\u9669\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u5b89\u5168\u8bc4\u4f30", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u9700\u8981\u51c6\u786e\u7684\u98ce\u9669\u4f30\u8ba1\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7ecf\u9a8c\u6a21\u578b\uff0c\u8981\u4e48\u91c7\u7528\u4e25\u91cd\u8fd1\u4f3c\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u51c6\u786e\u6027", "method": "\u7ed3\u5408\u6700\u65b0\u7684\u78b0\u649e\u6982\u7387\u4f30\u8ba1\u6280\u672f\u4e0e\u78b0\u649e\u4e25\u91cd\u6027\u6982\u5ff5\uff0c\u5f00\u53d1\u901a\u7528\u98ce\u9669\u4f30\u8ba1\u65b9\u6cd5\uff0c\u53ef\u4e3a\u4e0d\u540c\u78b0\u649e\u7c7b\u578b\u5206\u914d\u4e2a\u4f53\u5316\u4e25\u91cd\u6027\u51fd\u6570", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u5e94\u7528\uff0c\u5e76\u4e3a\u9ad8\u65af\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u793a\u4f8b\u5b9e\u73b0\u4ee3\u7801", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u901a\u7528\u7684\u98ce\u9669\u4f30\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u548c\u6f5c\u5728\u78b0\u649e\u4e25\u91cd\u6027\u4fdd\u6301\u5b89\u5168\u8ddd\u79bb"}}
{"id": "2601.14519", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14519", "abs": "https://arxiv.org/abs/2601.14519", "authors": ["Giulio Rossolini"], "title": "How Worst-Case Are Adversarial Attacks? Linking Adversarial and Statistical Robustness", "comment": null, "summary": "Adversarial attacks are widely used to evaluate model robustness, yet their validity as proxies for robustness to random perturbations remains debated. We ask whether an adversarial perturbation provides a representative estimate of robustness under random noise of the same magnitude, or instead reflects an atypical worst-case event. To this end, we introduce a probabilistic metric that quantifies noisy risk with respect to directionally biased perturbation distributions, parameterized by a concentration factor $\u03ba$ that interpolates between isotropic noise and adversarial direction. Using this framework, we study the limits of adversarial perturbations as estimators of noisy risk by proposing an attack strategy designed to operate in regimes statistically closer to uniform noise. Experiments on ImageNet and CIFAR-10 systematically benchmark widely used attacks, highlighting when adversarial success meaningfully reflects noisy risk and when it fails, thereby informing their use in safety-oriented evaluation.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u6982\u7387\u6846\u67b6\u8bc4\u4f30\u5bf9\u6297\u653b\u51fb\u80fd\u5426\u4ee3\u8868\u968f\u673a\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5bf9\u6297\u6270\u52a8\u5e76\u975e\u603b\u662f\u968f\u673a\u566a\u58f0\u9c81\u68d2\u6027\u7684\u6709\u6548\u4ee3\u7406", "motivation": "\u5bf9\u6297\u653b\u51fb\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u4f5c\u4e3a\u968f\u673a\u6270\u52a8\u9c81\u68d2\u6027\u4ee3\u7406\u7684\u6709\u6548\u6027\u5b58\u5728\u4e89\u8bae\u3002\u9700\u8981\u63a2\u7a76\u5bf9\u6297\u6270\u52a8\u662f\u5426\u80fd\u4ee3\u8868\u76f8\u540c\u5e45\u5ea6\u968f\u673a\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u662f\u4ec5\u53cd\u6620\u6700\u574f\u60c5\u51b5\u7684\u5f02\u5e38\u4e8b\u4ef6", "method": "\u5f15\u5165\u6982\u7387\u5ea6\u91cf\u91cf\u5316\u65b9\u5411\u6027\u504f\u7f6e\u6270\u52a8\u5206\u5e03\u4e0b\u7684\u566a\u58f0\u98ce\u9669\uff0c\u53c2\u6570\u03ba\u5728\u5747\u5300\u566a\u58f0\u548c\u5bf9\u6297\u65b9\u5411\u4e4b\u95f4\u63d2\u503c\u3002\u63d0\u51fa\u5728\u7edf\u8ba1\u4e0a\u66f4\u63a5\u8fd1\u5747\u5300\u566a\u58f0\u7684\u5bf9\u6297\u653b\u51fb\u7b56\u7565\uff0c\u5728ImageNet\u548cCIFAR-10\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u5e38\u7528\u653b\u51fb\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5bf9\u6297\u653b\u51fb\u6210\u529f\u4f55\u65f6\u6709\u610f\u4e49\u5730\u53cd\u6620\u566a\u58f0\u98ce\u9669\uff0c\u4f55\u65f6\u4f1a\u5931\u8d25\uff0c\u4e3a\u5b89\u5168\u5bfc\u5411\u7684\u8bc4\u4f30\u63d0\u4f9b\u6307\u5bfc", "conclusion": "\u5bf9\u6297\u6270\u52a8\u5e76\u975e\u603b\u662f\u968f\u673a\u566a\u58f0\u9c81\u68d2\u6027\u7684\u6709\u6548\u4f30\u8ba1\u5668\uff0c\u9700\u8981\u66f4\u8c28\u614e\u5730\u4f7f\u7528\u5bf9\u6297\u653b\u51fb\u8fdb\u884c\u5b89\u5168\u8bc4\u4f30\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u5728\u7edf\u8ba1\u76f8\u5173\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027"}}
{"id": "2601.15164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15164", "abs": "https://arxiv.org/abs/2601.15164", "authors": ["Yaru Liu", "Ao-bo Wang", "Nanyang Ye"], "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks", "comment": null, "summary": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.", "AI": {"tldr": "V-CAGE\u662f\u4e00\u4e2a\u95ed\u73af\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7269\u7406\u5408\u7406\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u5927\u89c4\u6a21\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5b9e\u4f8b\u5316\u3001\u5206\u5c42\u6307\u4ee4\u5206\u89e3\u548cVLM\u9a8c\u8bc1\u5faa\u73af\u6765\u89e3\u51b3\u5408\u6210\u6570\u636e\u4e2d\u7684\u7269\u7406\u4e0d\u73b0\u5b9e\u548c\u8bed\u4e49\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u751f\u6210\u7684\u573a\u666f\u7269\u7406\u4e0a\u4e0d\u5408\u7406\uff1b\u8bed\u8a00\u9a71\u52a8\u7a0b\u5e8f\u7ecf\u5e38\"\u6210\u529f\"\u4f46\u4e0d\u6ee1\u8db3\u4efb\u52a1\u8bed\u4e49\uff1b\u9ad8\u7ea7\u6307\u4ee4\u9700\u8981\u63a5\u5730\u5230\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u3002\u8fd9\u4e9b\u9650\u5236\u4e86\u4ece\u5408\u6210\u6570\u636e\u5b66\u4e60\u957f\u65f6\u7a0b\u5177\u8eab\u884c\u4e3a\u7684\u6548\u679c\u3002", "method": "1. \u4e0a\u4e0b\u6587\u611f\u77e5\u5b9e\u4f8b\u5316\u673a\u5236\uff1a\u901a\u8fc7\u52a8\u6001\u7ef4\u62a4\u7981\u6b62\u7a7a\u95f4\u533a\u57df\u5730\u56fe\uff0c\u9632\u6b62\u7269\u4f53\u7a7f\u63d2\uff0c\u786e\u4fdd\u5728\u6742\u4e71\u73af\u5883\u4e2d\u53ef\u8fbe\u3001\u65e0\u51b2\u7a81\u7684\u914d\u7f6e\u30022. \u5206\u5c42\u6307\u4ee4\u5206\u89e3\u6a21\u5757\uff1a\u5c06\u9ad8\u7ea7\u76ee\u6807\u5206\u89e3\u4e3a\u7ec4\u5408\u52a8\u4f5c\u57fa\u5143\uff0c\u4fc3\u8fdb\u8fde\u8d2f\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u30023. VLM\u9a8c\u8bc1\u5faa\u73af\uff1a\u4f5c\u4e3a\u89c6\u89c9\u6279\u8bc4\u8005\uff0c\u5728\u6bcf\u4e2a\u5b50\u4efb\u52a1\u540e\u6267\u884c\u4e25\u683c\u62d2\u7edd\u91c7\u6837\uff0c\u8fc7\u6ee4\"\u9759\u9ed8\u5931\u8d25\"\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cV-CAGE\u751f\u6210\u7684\u6570\u200b\u200b\u636e\u96c6\u5177\u6709\u4f18\u8d8a\u7684\u7269\u7406\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u4e0e\u975e\u9a8c\u8bc1\u57fa\u7ebf\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u7b56\u7565\u7684\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "V-CAGE\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u4e00\u81f4\u6027\u5f3a\u5236\u3001\u5206\u5c42\u89c4\u5212\u548c\u8bed\u4e49\u9a8c\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u5177\u8eab\u64cd\u4f5c\u6570\u636e\u96c6\u521b\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.15075", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15075", "abs": "https://arxiv.org/abs/2601.15075", "authors": ["Chen Qian", "Peng Wang", "Dongrui Liu", "Junyao Yang", "Dadi Guo", "Ling Tang", "Jilin Mei", "Qihan Ren", "Shuai Shao", "Yong Liu", "Jie Fu", "Jing Shao", "Xia Hu"], "title": "The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution", "comment": null, "summary": "Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \\textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \\textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \\textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \\textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u667a\u80fd\u4f53\u5f52\u56e0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u9a71\u52a8\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5185\u5728\u56e0\u7d20\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5931\u8d25\u5f52\u56e0", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u5728\u5ba2\u670d\u3001\u7f51\u9875\u5bfc\u822a\u3001\u8f6f\u4ef6\u5de5\u7a0b\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u7406\u89e3\u667a\u80fd\u4f53\u4e3a\u4f55\u91c7\u53d6\u7279\u5b9a\u884c\u52a8\u5bf9\u4e8e\u95ee\u8d23\u548c\u6cbb\u7406\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5931\u8d25\u5f52\u56e0\uff0c\u4e0d\u8db3\u4ee5\u89e3\u91ca\u667a\u80fd\u4f53\u884c\u4e3a\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6846\u67b6\uff1a\u5728\u7ec4\u4ef6\u7ea7\u522b\u4f7f\u7528\u65f6\u5e8f\u4f3c\u7136\u52a8\u6001\u8bc6\u522b\u5173\u952e\u4ea4\u4e92\u6b65\u9aa4\uff1b\u5728\u53e5\u5b50\u7ea7\u522b\u4f7f\u7528\u57fa\u4e8e\u6270\u52a8\u7684\u5206\u6790\u6765\u7cbe\u786e\u5b9a\u4f4d\u7279\u5b9a\u6587\u672c\u8bc1\u636e", "result": "\u5728\u5305\u62ec\u6807\u51c6\u5de5\u5177\u4f7f\u7528\u548c\u5185\u5b58\u8bf1\u5bfc\u504f\u5dee\u7b49\u53ef\u9760\u6027\u98ce\u9669\u5728\u5185\u7684\u591a\u6837\u5316\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u53ef\u9760\u5730\u8bc6\u522b\u9a71\u52a8\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5173\u952e\u5386\u53f2\u4e8b\u4ef6\u548c\u53e5\u5b50", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u95ee\u8d23\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u80fd\u591f\u8bc6\u522b\u9a71\u52a8\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5185\u5728\u56e0\u7d20\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5931\u8d25\u5f52\u56e0"}}
{"id": "2601.14570", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14570", "abs": "https://arxiv.org/abs/2601.14570", "authors": ["Xiaojie Yang", "Dizhi Huang", "Hangli Ge", "Masahiro Sano", "Takeaki Ohdake", "Kazuma Hatano", "Noboru Koshizuka"], "title": "Place with Intention: An Empirical Attendance Predictive Study of Expo 2025 Osaka, Kansai, Japan", "comment": "Accepted by Special Session 10 of SMD II: Synergizing Mobility Data for Human Life Evolution in Real Spaces at IEEE Big Data 2025", "summary": "Accurate forecasting of daily attendance is vital for managing transportation, crowd flows, and services at large-scale international events such as Expo 2025 Osaka, Kansai, Japan. However, existing approaches often rely on multi-source external data (such as weather, traffic, and social media) to improve accuracy, which can lead to unreliable results when historical data are insufficient. To address these challenges, we propose a Transformer-based framework that leverages reservation dynamics, i.e., ticket bookings and subsequent updates within a time window, as a proxy for visitors' attendance intentions, under the assumption that such intentions are eventually reflected in reservation patterns. This design avoids the complexity of multi-source integration while still capturing external influences like weather and promotions implicitly embedded in reservation dynamics. We construct a dataset combining entrance records and reservation dynamics and evaluate the model under both single-channel (total attendance) and two-channel (separated by East and West gates) settings. Results show that separately modeling East and West gates consistently improves accuracy, particularly for short- and medium-term horizons. Ablation studies further confirm the importance of the encoder-decoder structure, inverse-style embedding, and adaptive fusion module. Overall, our findings indicate that reservation dynamics offer a practical and informative foundation for attendance forecasting in large-scale international events.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u7ea6\u52a8\u6001\uff08\u95e8\u7968\u9884\u8ba2\u53ca\u66f4\u65b0\uff09\u4f5c\u4e3a\u53c2\u89c2\u8005\u51fa\u5e2d\u610f\u56fe\u7684\u4ee3\u7406\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u56fd\u9645\u6d3b\u52a8\uff08\u5982\u5927\u962a\u4e16\u535a\u4f1a\uff09\u7684\u51fa\u5e2d\u9884\u6d4b\uff0c\u907f\u514d\u591a\u6e90\u5916\u90e8\u6570\u636e\u4f9d\u8d56\u3002", "motivation": "\u5927\u89c4\u6a21\u56fd\u9645\u6d3b\u52a8\uff08\u59822025\u5927\u962a\u4e16\u535a\u4f1a\uff09\u7684\u51c6\u786e\u51fa\u5e2d\u9884\u6d4b\u5bf9\u4ea4\u901a\u3001\u4eba\u6d41\u548c\u670d\u52a1\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u6e90\u5916\u90e8\u6570\u636e\uff08\u5929\u6c14\u3001\u4ea4\u901a\u3001\u793e\u4ea4\u5a92\u4f53\uff09\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5386\u53f2\u6570\u636e\u4e0d\u8db3\u65f6\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u9760\u7ed3\u679c\u3002", "method": "\u63d0\u51faTransformer\u6846\u67b6\uff0c\u5229\u7528\u9884\u7ea6\u52a8\u6001\uff08\u95e8\u7968\u9884\u8ba2\u53ca\u65f6\u95f4\u7a97\u53e3\u5185\u7684\u66f4\u65b0\uff09\u4f5c\u4e3a\u53c2\u89c2\u8005\u51fa\u5e2d\u610f\u56fe\u7684\u4ee3\u7406\u3002\u6784\u5efa\u5305\u542b\u5165\u573a\u8bb0\u5f55\u548c\u9884\u7ea6\u52a8\u6001\u7684\u6570\u636e\u96c6\uff0c\u5728\u5355\u901a\u9053\uff08\u603b\u51fa\u5e2d\uff09\u548c\u53cc\u901a\u9053\uff08\u4e1c\u3001\u897f\u95e8\u5206\u5f00\uff09\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u6a21\u578b\u3002\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u3001\u9006\u98ce\u683c\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5206\u522b\u5efa\u6a21\u4e1c\u3001\u897f\u95e8\u80fd\u6301\u7eed\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5bf9\u77ed\u671f\u548c\u4e2d\u671f\u9884\u6d4b\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u3001\u9006\u98ce\u683c\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u9884\u7ea6\u52a8\u6001\u4e3a\u5927\u89c4\u6a21\u56fd\u9645\u6d3b\u52a8\u7684\u51fa\u5e2d\u9884\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u57fa\u7840\uff0c\u907f\u514d\u4e86\u591a\u6e90\u6570\u636e\u6574\u5408\u7684\u590d\u6742\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u9884\u7ea6\u6a21\u5f0f\u9690\u5f0f\u6355\u6349\u4e86\u5929\u6c14\u3001\u4fc3\u9500\u7b49\u5916\u90e8\u5f71\u54cd\u3002"}}
{"id": "2601.14914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14914", "abs": "https://arxiv.org/abs/2601.14914", "authors": ["Tianxiang Fei", "Cheng Chen", "Yue Pan", "Mao Zheng", "Mingyang Song"], "title": "CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents", "comment": null, "summary": "Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.", "AI": {"tldr": "CodeDelegator\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u4e13\u4e1a\u5316\u5c06\u89c4\u5212\u4e0e\u5b9e\u73b0\u5206\u79bb\uff0c\u4f7f\u7528\u6301\u4e45\u89c4\u5212\u8005\u548c\u4e34\u65f6\u7f16\u7801\u8005\uff0c\u5f15\u5165EPSS\u72b6\u6001\u5206\u79bb\u673a\u5236\u9632\u6b62\u4e0a\u4e0b\u6587\u6c61\u67d3", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u8be6\u7ec6\u5b9e\u73b0\uff0c\u4f46\u5355\u4e00\u667a\u80fd\u4f53\u540c\u65f6\u5904\u7406\u4e24\u8005\u4f1a\u5bfc\u81f4\u8c03\u8bd5\u75d5\u8ff9\u548c\u4e2d\u95f4\u5931\u8d25\u6c61\u67d3\u4e0a\u4e0b\u6587\uff0c\u635f\u5bb3\u957f\u671f\u6027\u80fd", "method": "\u63d0\u51faCodeDelegator\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u6301\u4e45Delegator\u8d1f\u8d23\u6218\u7565\u76d1\u7763\u3001\u4efb\u52a1\u5206\u89e3\u3001\u89c4\u8303\u7f16\u5199\u548c\u8fdb\u5ea6\u76d1\u63a7\uff1b2) \u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5b9e\u4f8b\u5316\u65b0\u7684Coder\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u5e72\u51c0\u7684\u4e0a\u4e0b\u6587\uff1b3) \u5f15\u5165EPSS\u673a\u5236\u9694\u79bb\u6bcf\u4e2aCoder\u7684\u6267\u884c\u72b6\u6001\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86CodeDelegator\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u5c06\u89c4\u5212\u4e0e\u5b9e\u73b0\u5206\u79bb\uff0cCodeDelegator\u89e3\u51b3\u4e86\u5355\u4e00\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u6c61\u67d3\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u4efb\u52a1\u7684\u6027\u80fd"}}
{"id": "2601.14590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14590", "abs": "https://arxiv.org/abs/2601.14590", "authors": ["Shovito Barua Soumma", "Asiful Arefeen", "Stephanie M. Carpenter", "Melanie Hingle", "Hassan Ghasemzadeh"], "title": "Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation", "comment": null, "summary": "Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.\n  Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFEs\uff09\u5728\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u7684LLMs\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u3001\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u53cd\u4e8b\u5b9e\uff0c\u53ef\u7528\u4e8e\u5e72\u9884\u8bbe\u8ba1\u548c\u6570\u636e\u589e\u5f3a\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\u80fd\u63d0\u4f9b\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89e3\u91ca\u6027\uff0c\u8bc6\u522b\u6539\u53d8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6240\u9700\u7684\u6700\u5c0f\u3001\u53ef\u64cd\u4f5c\u7684\u6539\u53d8\u3002\u7136\u800c\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u751f\u6210\u7684CFEs\u5728\u4e34\u5e8a\u573a\u666f\u4e2d\u53ef\u80fd\u7f3a\u4e4f\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u9700\u8981\u63a2\u7d22LLMs\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u53cd\u4e8b\u5b9e\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u591a\u79cdLLMs\uff08GPT-4\u96f6\u6837\u672c/\u5c11\u6837\u672c\u3001BioMistral-7B\u3001LLaMA-3.1-8B\uff09\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u914d\u7f6e\u4e0b\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002\u57fa\u4e8e\u591a\u6a21\u6001AI-READI\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u4ece\u5e72\u9884\u8d28\u91cf\u3001\u7279\u5f81\u591a\u6837\u6027\u548c\u589e\u5f3a\u6548\u679c\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30CFs\u3002\u4e0eDiCE\u3001CFNOW\u3001NICE\u7b49\u4f18\u5316\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5fae\u8c03\u540e\u7684LLMs\uff08\u7279\u522b\u662fLLaMA-3.1-8B\uff09\u80fd\u4ea7\u751f\u9ad8\u53ef\u4fe1\u5ea6\uff08\u8fbe99%\uff09\u3001\u5f3a\u6709\u6548\u6027\uff08\u8fbe0.99\uff09\u4e14\u5177\u6709\u73b0\u5b9e\u53ef\u4fee\u6539\u7279\u5f81\u8c03\u6574\u7684\u53cd\u4e8b\u5b9e\u3002\u5728\u6807\u7b7e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0cLLM\u751f\u6210\u7684CFs\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u80fd\u663e\u8457\u6062\u590d\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u5728\u4e09\u79cd\u7a00\u7f3a\u573a\u666f\u4e0b\u5e73\u5747F1\u6062\u590d\u7387\u8fbe20%\u3002\u76f8\u6bd4\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\uff0cLLMs\u80fd\u751f\u6210\u66f4\u4e34\u5e8a\u53ef\u64cd\u4f5c\u3001\u8bed\u4e49\u66f4\u8fde\u8d2f\u7684\u53cd\u4e8b\u5b9e\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5728\u53ef\u89e3\u91ca\u5e72\u9884\u8bbe\u8ba1\u548c\u4f20\u611f\u5668\u6570\u5b57\u5065\u5eb7\u9886\u57df\u7684\u6570\u636e\u9ad8\u6548\u6a21\u578b\u8bad\u7ec3\u65b9\u9762\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002SenseCF\u6846\u67b6\u901a\u8fc7\u5fae\u8c03LLM\u751f\u6210\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e76\u8865\u5145\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u7684\u5c11\u6570\u7c7b\uff0c\u80fd\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u3001\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14738", "abs": "https://arxiv.org/abs/2601.14738", "authors": ["Liqin Wang", "Qianyue Hu", "Wei Lu", "Xiangyang Luo"], "title": "Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption", "comment": null, "summary": "The rapid evolution of diffusion models has democratized face swapping but also raises concerns about privacy and identity security. Existing proactive defenses, often adapted from image editing attacks, prove ineffective in this context. We attribute this failure to an oversight of the structural resilience and the unique static conditional guidance mechanism inherent in face swapping systems. To address this, we propose VoidFace, a systemic defense method that views face swapping as a coupled identity pathway. By injecting perturbations at critical bottlenecks, VoidFace induces cascading disruption throughout the pipeline. Specifically, we first introduce localization disruption and identity erasure to degrade physical regression and semantic embeddings, thereby impairing the accurate modeling of the source face. We then intervene in the generative domain by decoupling attention mechanisms to sever identity injection, and corrupting intermediate diffusion features to prevent the reconstruction of source identity. To ensure visual imperceptibility, we perform adversarial search in the latent manifold, guided by a perceptual adaptive strategy to balance attack potency with image quality. Extensive experiments show that VoidFace outperforms existing defenses across various diffusion-based swapping models, while producing adversarial faces with superior visual quality.", "AI": {"tldr": "VoidFace\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u4eba\u8138\u4ea4\u6362\u653b\u51fb\u7684\u7cfb\u7edf\u6027\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u6270\u52a8\u7834\u574f\u8eab\u4efd\u4fe1\u606f\u901a\u8def\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u9632\u5fa1\u591a\u79cd\u4eba\u8138\u4ea4\u6362\u6a21\u578b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u4eba\u8138\u4ea4\u6362\u6280\u672f\u666e\u53ca\uff0c\u4f46\u5f15\u53d1\u4e86\u9690\u79c1\u548c\u8eab\u4efd\u5b89\u5168\u95ee\u9898\u3002\u73b0\u6709\u7684\u4e3b\u52a8\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u662f\u4ece\u56fe\u50cf\u7f16\u8f91\u653b\u51fb\u4e2d\u6539\u7f16\u800c\u6765\uff0c\u5728\u4eba\u8138\u4ea4\u6362\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ffd\u89c6\u4e86\u4eba\u8138\u4ea4\u6362\u7cfb\u7edf\u7684\u7ed3\u6784\u97e7\u6027\u548c\u72ec\u7279\u7684\u9759\u6001\u6761\u4ef6\u5f15\u5bfc\u673a\u5236\u3002", "method": "VoidFace\u5c06\u4eba\u8138\u4ea4\u6362\u89c6\u4e3a\u8026\u5408\u7684\u8eab\u4efd\u901a\u8def\uff0c\u901a\u8fc7\u5728\u5173\u952e\u74f6\u9888\u5904\u6ce8\u5165\u6270\u52a8\u5f15\u53d1\u7ea7\u8054\u7834\u574f\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u5b9a\u4f4d\u7834\u574f\u548c\u8eab\u4efd\u64e6\u9664\u4ee5\u964d\u4f4e\u7269\u7406\u56de\u5f52\u548c\u8bed\u4e49\u5d4c\u5165\uff1b2\uff09\u901a\u8fc7\u89e3\u8026\u6ce8\u610f\u529b\u673a\u5236\u5207\u65ad\u8eab\u4efd\u6ce8\u5165\uff1b3\uff09\u7834\u574f\u4e2d\u95f4\u6269\u6563\u7279\u5f81\u9632\u6b62\u6e90\u8eab\u4efd\u91cd\u5efa\u3002\u4e3a\u4e86\u4fdd\u6301\u89c6\u89c9\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u5728\u6f5c\u5728\u6d41\u5f62\u4e2d\u8fdb\u884c\u5bf9\u6297\u641c\u7d22\uff0c\u91c7\u7528\u611f\u77e5\u81ea\u9002\u5e94\u7b56\u7565\u5e73\u8861\u653b\u51fb\u6548\u679c\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVoidFace\u5728\u5404\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u4eba\u8138\u4ea4\u6362\u6a21\u578b\u4e2d\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u540c\u65f6\u751f\u6210\u7684\u5bf9\u6297\u4eba\u8138\u5177\u6709\u66f4\u4f18\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "VoidFace\u901a\u8fc7\u7cfb\u7edf\u6027\u7834\u574f\u4eba\u8138\u4ea4\u6362\u7684\u8eab\u4efd\u901a\u8def\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u89c6\u89c9\u8d28\u91cf\u9ad8\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u4eba\u8138\u4ea4\u6362\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2601.15129", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15129", "abs": "https://arxiv.org/abs/2601.15129", "authors": ["Yishu Wei", "Adam E. Flanders", "Errol Colak", "John Mongan", "Luciano M Prevedello", "Po-Hao Chen", "Henrique Min Ho Lee", "Gilberto Szarf", "Hamilton Shoji", "Jason Sho", "Katherine Andriole", "Tessa Cook", "Lisa C. Adams", "Linda C. Chu", "Maggie Chung", "Geraldine Brusca-Augello", "Djeven P. Deva", "Navneet Singh", "Felipe Sanchez Tijmes", "Jeffrey B. Alpert", "Elsie T. Nguyen", "Drew A. Torigian", "Kate Hanneman", "Lauren K Groner", "Alexander Phan", "Ali Islam", "Matias F. Callejas", "Gustavo Borges da Silva Teles", "Faisal Jamal", "Maryam Vazirabad", "Ali Tejani", "Hari Trivedi", "Paulo Kuriki", "Rajesh Bhayana", "Elana T. Benishay", "Yi Lin", "Yifan Peng", "George Shih"], "title": "RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)", "comment": null, "summary": "Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. A total of 13,735 deidentified chest radiographs and their corresponding reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked \"Agree all\", \"Agree mostly\" or \"Disagree\" to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected \"Agree All\" for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released radiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available https://imaging.rsna.org, with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b200\u4e2a\u80f8\u90e8X\u5149\u7247\u300112\u4e2a\u6807\u6ce8\u6807\u7b7e\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u91c7\u7528AI\u8f85\u52a9\u4e13\u5bb6\u6807\u6ce8\u6d41\u7a0b\uff0c\u65e8\u5728\u4e3a\u4e34\u5e8a\u6709\u7528\u7684\u591a\u6a21\u6001LLM\u5de5\u5177\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6", "motivation": "\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u8003\u8bd5\u4e2d\u8868\u73b0\u4e0e\u653e\u5c04\u79d1\u4f4f\u9662\u533b\u5e08\u76f8\u5f53\uff0c\u4f46\u5f00\u53d1\u4e34\u5e8a\u6709\u7528\u7684\u5de5\u5177\u9700\u8981\u7531\u9886\u57df\u4e13\u5bb6\u7b56\u5212\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\u96c6", "method": "\u4f7f\u7528GPT-4o\u4ece\u62a5\u544a\u4e2d\u63d0\u53d6\u5f02\u5e38\u53d1\u73b0\uff0c\u901a\u8fc7\u672c\u5730LLM\u6620\u5c04\u523012\u4e2a\u57fa\u51c6\u6807\u7b7e\uff1b\u91c7\u7528AI\u8f85\u52a9\u6807\u6ce8\u6d41\u7a0b\uff0c\u8ba917\u540d\u653e\u5c04\u79d1\u4e13\u5bb6\u8bc4\u4f30AI\u5efa\u8bae\u7684\u6807\u7b7e\uff0c\u6bcf\u4e2aX\u5149\u7247\u75313\u540d\u4e13\u5bb6\u8bc4\u5ba1", "result": "\u521b\u5efa\u4e86\u5305\u542b200\u4e2a\u80f8\u90e8X\u5149\u7247\u300112\u4e2a\u57fa\u51c6\u6807\u7b7e\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5176\u4e2d381\u4e2aX\u5149\u7247\u83b7\u5f97\u81f3\u5c112\u540d\u4e13\u5bb6\"\u5b8c\u5168\u540c\u610f\"AI\u6807\u7b7e\uff1b\u5206\u4e3a100\u4e2a\u516c\u5f00\u6837\u672c\u548c100\u4e2a\u4fdd\u7559\u6837\u672c\u7528\u4e8e\u72ec\u7acb\u8bc4\u4f30", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9ad8\u8d28\u91cf\u80f8\u90e8X\u5149\u57fa\u51c6\u6570\u636e\u96c6\u548cAI\u8f85\u52a9\u6807\u6ce8\u6d41\u7a0b\uff0c\u652f\u6301\u653e\u5c04\u79d1\u533b\u751f\u9ad8\u6548\u6807\u6ce8\uff0c\u4e3a\u591a\u6a21\u6001LLM\u5de5\u5177\u7684\u4e34\u5e8a\u8bc4\u4f30\u63d0\u4f9b\u53ef\u9760\u6807\u51c6"}}
{"id": "2601.14799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14799", "abs": "https://arxiv.org/abs/2601.14799", "authors": ["Qihua Liang", "Liang Chen", "Yaozong Zheng", "Jian Nong", "Zhiyi Mo", "Bineng Zhong"], "title": "UBATrack: Spatio-Temporal State Space Model for General Multi-Modal Tracking", "comment": null, "summary": "Multi-modal object tracking has attracted considerable attention by integrating multiple complementary inputs (e.g., thermal, depth, and event data) to achieve outstanding performance. Although current general-purpose multi-modal trackers primarily unify various modal tracking tasks (i.e., RGB-Thermal infrared, RGB-Depth or RGB-Event tracking) through prompt learning, they still overlook the effective capture of spatio-temporal cues. In this work, we introduce a novel multi-modal tracking framework based on a mamba-style state space model, termed UBATrack. Our UBATrack comprises two simple yet effective modules: a Spatio-temporal Mamba Adapter (STMA) and a Dynamic Multi-modal Feature Mixer. The former leverages Mamba's long-sequence modeling capability to jointly model cross-modal dependencies and spatio-temporal visual cues in an adapter-tuning manner. The latter further enhances multi-modal representation capacity across multiple feature dimensions to improve tracking robustness. In this way, UBATrack eliminates the need for costly full-parameter fine-tuning, thereby improving the training efficiency of multi-modal tracking algorithms. Experiments show that UBATrack outperforms state-of-the-art methods on RGB-T, RGB-D, and RGB-E tracking benchmarks, achieving outstanding results on the LasHeR, RGBT234, RGBT210, DepthTrack, VOT-RGBD22, and VisEvent datasets.", "AI": {"tldr": "\u63d0\u51faUBATrack\uff0c\u4e00\u79cd\u57fa\u4e8eMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u591a\u6a21\u6001\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7aMamba\u9002\u914d\u5668\u548c\u52a8\u6001\u591a\u6a21\u6001\u7279\u5f81\u6df7\u5408\u5668\u6709\u6548\u6355\u6349\u65f6\u7a7a\u7ebf\u7d22\uff0c\u65e0\u9700\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u5728\u591a\u4e2aRGB-T\u3001RGB-D\u3001RGB-E\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u591a\u6a21\u6001\u8ddf\u8e2a\u5668\u4e3b\u8981\u901a\u8fc7\u63d0\u793a\u5b66\u4e60\u7edf\u4e00\u5404\u79cd\u6a21\u6001\u8ddf\u8e2a\u4efb\u52a1\uff0c\u4f46\u5ffd\u89c6\u4e86\u65f6\u7a7a\u7ebf\u7d22\u7684\u6709\u6548\u6355\u6349\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5efa\u6a21\u8de8\u6a21\u6001\u4f9d\u8d56\u548c\u65f6\u7a7a\u89c6\u89c9\u7ebf\u7d22\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUBATrack\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1) \u65f6\u7a7aMamba\u9002\u914d\u5668(STMA)\uff0c\u5229\u7528Mamba\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u4ee5\u9002\u914d\u5668\u8c03\u4f18\u65b9\u5f0f\u8054\u5408\u5efa\u6a21\u8de8\u6a21\u6001\u4f9d\u8d56\u548c\u65f6\u7a7a\u89c6\u89c9\u7ebf\u7d22\uff1b2) \u52a8\u6001\u591a\u6a21\u6001\u7279\u5f81\u6df7\u5408\u5668\uff0c\u8de8\u591a\u4e2a\u7279\u5f81\u7ef4\u5ea6\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u80fd\u529b\u4ee5\u63d0\u9ad8\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002", "result": "\u5728RGB-T\u3001RGB-D\u3001RGB-E\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u5728LasHeR\u3001RGBT234\u3001RGBT210\u3001DepthTrack\u3001VOT-RGBD22\u548cVisEvent\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "UBATrack\u901a\u8fc7Mamba\u6a21\u578b\u6709\u6548\u6355\u6349\u65f6\u7a7a\u7ebf\u7d22\uff0c\u65e0\u9700\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u8ddf\u8e2a\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.14917", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14917", "abs": "https://arxiv.org/abs/2601.14917", "authors": ["Giorgia Rigamonti", "Mirko Paolo Barbato", "Davide Marelli", "Paolo Napoletano"], "title": "Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models", "comment": null, "summary": "Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u8840\u7cd6\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u60a3\u8005\u7279\u5b9a\u6570\u636e\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u901a\u7528\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5904\u7406\u4e2a\u4f53\u5dee\u5f02\uff0c\u663e\u8457\u6539\u5584\u4e0d\u826f\u4e8b\u4ef6\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u8840\u7cd6\u76d1\u6d4b\u8bbe\u5907\u548c\u79fb\u52a8\u5065\u5eb7\u5e94\u7528\u7684\u666e\u53ca\uff0c\u51c6\u786e\u7684\u8840\u7cd6\u9884\u6d4b\u5bf9\u4e8e\u589e\u5f3a\u81ea\u52a8\u5316\u80f0\u5c9b\u7d20\u8f93\u9001\u548c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u901a\u7528\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u5904\u7406\u4e2a\u4f53\u5dee\u5f02\uff0c\u9700\u8981\u4e2a\u6027\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u60a3\u8005\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u4e2a\u6027\u5316\u5efa\u6a21\u3002\u6bd4\u8f83\u7559\u4e00\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc1\u4e0e\u5fae\u8c03\u7b56\u7565\uff0c\u8bc4\u4f30\u5176\u5bf9\u60a3\u8005\u7279\u5b9a\u52a8\u6001\u7684\u5efa\u6a21\u80fd\u529b\u3002\u5b9e\u9a8c\u5bf9\u6bd4\u591a\u6a21\u6001\u60a3\u8005\u7279\u5b9a\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684\u4ec5\u4f7f\u7528\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u6709\u6548\u4e2a\u6027\u5316\u6240\u9700\u7684\u6700\u5c0f\u6570\u636e\u91cf\u3002", "result": "\u4e2a\u6027\u5316\u6a21\u578b\u663e\u8457\u6539\u5584\u4e0d\u826f\u4e8b\u4ef6\u9884\u6d4b\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u53ca\u65f6\u7684\u5e72\u9884\u3002\u591a\u6a21\u6001\u60a3\u8005\u7279\u5b9a\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edfCGM-only\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u4e86\u6709\u6548\u4e2a\u6027\u5316\u6240\u9700\u7684\u6700\u5c0f\u8bad\u7ec3\u6570\u636e\u91cf\uff0c\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u6536\u96c6\u53d7\u9650\u7684\u60c5\u51b5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "conclusion": "\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u8840\u7cd6\u9884\u6d4b\u6a21\u578b\u5728\u4e0b\u4e00\u4ee3\u7cd6\u5c3f\u75c5\u7ba1\u7406\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u53ef\u7a7f\u6234\u548c\u79fb\u52a8\u5065\u5eb7\u5e73\u53f0\u4e2d\uff0c\u80fd\u591f\u589e\u5f3a\u9762\u5411\u6d88\u8d39\u8005\u7684\u7cd6\u5c3f\u75c5\u62a4\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14971", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14971", "abs": "https://arxiv.org/abs/2601.14971", "authors": ["Liping Chen", "Mujie Liu", "Haytham Fayek"], "title": "Fine-Grained Traceability for Transparent ML Pipelines", "comment": "Accepted at The Web Conference (WWW) 2026", "summary": "Modern machine learning systems are increasingly realised as multistage pipelines, yet existing transparency mechanisms typically operate at a model level: they describe what a system is and why it behaves as it does, but not how individual data samples are operationally recorded, tracked, and verified as they traverse the pipeline. This absence of verifiable, sample-level traceability leaves practitioners and users unable to determine whether a specific sample was used, when it was processed, or whether the corresponding records remain intact over time. We introduce FG-Trac, a model-agnostic framework that establishes verifiable, fine-grained sample-level traceability throughout machine learning pipelines. FG-Trac defines an explicit mechanism for capturing and verifying sample lifecycle events across preprocessing and training, computes contribution scores explicitly grounded in training checkpoints, and anchors these traces to tamper-evident cryptographic commitments. The framework integrates without modifying model architectures or training objectives, reconstructing complete and auditable data-usage histories with practical computational overhead. Experiments on a canonical convolutional neural network and a multimodal graph learning pipeline demonstrate that FG-Trac preserves predictive performance while enabling machine learning systems to furnish verifiable evidence of how individual samples were used and propagated during model execution.", "AI": {"tldr": "FG-Trac\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u7ec6\u7c92\u5ea6\u6837\u672c\u7ea7\u8ffd\u6eaf\u80fd\u529b\uff0c\u901a\u8fc7\u52a0\u5bc6\u627f\u8bfa\u786e\u4fdd\u6570\u636e\u4f7f\u7528\u5386\u53f2\u7684\u5b8c\u6574\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u591a\u4e3a\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u4f46\u73b0\u6709\u900f\u660e\u5ea6\u673a\u5236\u901a\u5e38\u53ea\u5728\u6a21\u578b\u5c42\u9762\u8fd0\u4f5c\uff0c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u6837\u672c\u7ea7\u8ffd\u6eaf\u80fd\u529b\u3002\u8fd9\u4f7f\u5f97\u5b9e\u8df5\u8005\u548c\u7528\u6237\u65e0\u6cd5\u786e\u5b9a\u7279\u5b9a\u6837\u672c\u662f\u5426\u88ab\u4f7f\u7528\u3001\u4f55\u65f6\u88ab\u5904\u7406\uff0c\u4ee5\u53ca\u76f8\u5e94\u8bb0\u5f55\u662f\u5426\u968f\u65f6\u95f4\u4fdd\u6301\u5b8c\u6574\u3002", "method": "FG-Trac\u5b9a\u4e49\u4e86\u4e00\u4e2a\u660e\u786e\u7684\u673a\u5236\u6765\u6355\u83b7\u548c\u9a8c\u8bc1\u9884\u5904\u7406\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6837\u672c\u751f\u547d\u5468\u671f\u4e8b\u4ef6\uff0c\u57fa\u4e8e\u8bad\u7ec3\u68c0\u67e5\u70b9\u8ba1\u7b97\u8d21\u732e\u5206\u6570\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8ffd\u6eaf\u8bb0\u5f55\u951a\u5b9a\u5230\u9632\u7be1\u6539\u7684\u52a0\u5bc6\u627f\u8bfa\u4e2d\u3002\u8be5\u6846\u67b6\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u8bad\u7ec3\u76ee\u6807\u5373\u53ef\u96c6\u6210\u3002", "result": "\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u6a21\u6001\u56fe\u5b66\u4e60\u6d41\u6c34\u7ebf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFG-Trac\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4f7f\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u5173\u4e8e\u5355\u4e2a\u6837\u672c\u5728\u6a21\u578b\u6267\u884c\u8fc7\u7a0b\u4e2d\u5982\u4f55\u88ab\u4f7f\u7528\u548c\u4f20\u64ad\u7684\u53ef\u9a8c\u8bc1\u8bc1\u636e\u3002", "conclusion": "FG-Trac\u4e3a\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u5efa\u7acb\u4e86\u53ef\u9a8c\u8bc1\u7684\u7ec6\u7c92\u5ea6\u6837\u672c\u7ea7\u8ffd\u6eaf\u80fd\u529b\uff0c\u901a\u8fc7\u52a0\u5bc6\u627f\u8bfa\u786e\u4fdd\u6570\u636e\u4f7f\u7528\u5386\u53f2\u7684\u5b8c\u6574\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2601.15061", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15061", "abs": "https://arxiv.org/abs/2601.15061", "authors": ["Qiwei Ma", "Jun Zhang"], "title": "Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD", "comment": null, "summary": "Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u8bef\u5dee\u53cd\u9988\u968f\u673a\u68af\u5ea6\u4e0b\u964d(EFSGD)\u65b9\u6cd5\uff0c\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u548c\u53ef\u7528\u6027\u7684\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u63a9\u7801\u6280\u672f\uff08\u5982\u533f\u540d\u5316\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u4e2d\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u9884\u671f\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u6548\u7528\u3002\u5408\u6210\u6570\u636e\u5728\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6837\u672c\u548c\u9632\u6b62\u771f\u5b9e\u6570\u636e\u4fe1\u606f\u6cc4\u9732\u65b9\u9762\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9690\u79c1\u548c\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u91cd\u590d\u6743\u8861\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u5dee\u5206\u9690\u79c1\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u8bef\u5dee\u53cd\u9988\u968f\u673a\u68af\u5ea6\u4e0b\u964d(EFSGD)\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u91cd\u6784\u635f\u5931\u548c\u566a\u58f0\u6ce8\u5165\u673a\u5236\u3002", "result": "\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u751f\u6210\u6bd4\u76f8\u5173\u5de5\u4f5c\u66f4\u9ad8\u8d28\u91cf\u548c\u53ef\u7528\u6027\u7684\u56fe\u50cf\u3002\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08MNIST\u3001Fashion-MNIST\u3001CelebA\uff09\u4e0a\u51e0\u4e4e\u6240\u6709\u6307\u6807\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7070\u5ea6\u548cRGB\u56fe\u50cf\u4e0a\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u4e2d\u9690\u79c1\u4e0e\u6548\u7528\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2601.15281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15281", "abs": "https://arxiv.org/abs/2601.15281", "authors": ["Ying Yang", "Zhengyao Lv", "Tianlin Pan", "Haofan Wang", "Binxin Yang", "Hubery Yin", "Chen Li", "Ziwei Liu", "Chenyang Si"], "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation", "comment": "17 pages, 21 figures,", "summary": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.", "AI": {"tldr": "\u63d0\u51faStableWorld\u65b9\u6cd5\u89e3\u51b3\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u5e27\u6dd8\u6c70\u673a\u5236\u8fc7\u6ee4\u9000\u5316\u5e27\uff0c\u9632\u6b62\u7d2f\u79ef\u6f02\u79fb", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u957f\u65f6\u95f4\u4ea4\u4e92\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u65f6\u95f4\u9000\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u7a7a\u95f4\u6f02\u79fb\u548c\u573a\u666f\u5d29\u6e83\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u4ea4\u4e92\u751f\u6210", "method": "\u63d0\u51faStableWorld\u65b9\u6cd5\uff0c\u91c7\u7528\u52a8\u6001\u5e27\u6dd8\u6c70\u673a\u5236\uff0c\u6301\u7eed\u8fc7\u6ee4\u9000\u5316\u5e27\u540c\u65f6\u4fdd\u7559\u51e0\u4f55\u4e00\u81f4\u7684\u5e27\uff0c\u4ece\u6e90\u5934\u4e0a\u9632\u6b62\u7d2f\u79ef\u6f02\u79fb", "result": "\u5728\u591a\u4e2a\u4ea4\u4e92\u5f0f\u89c6\u9891\u6a21\u578b\uff08Matrix-Game\u3001Open-Oasis\u3001Hunyuan-GameCraft\uff09\u4e0a\u9a8c\u8bc1\u4e86StableWorld\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "StableWorld\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u53ef\u5e94\u7528\u4e8e\u4e0d\u540c\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u4e3a\u89e3\u51b3\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
