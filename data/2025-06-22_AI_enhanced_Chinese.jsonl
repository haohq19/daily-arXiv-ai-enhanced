{"id": "2506.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14900", "abs": "https://arxiv.org/abs/2506.14900", "authors": ["Imane Guellil", "Salom\u00e9 Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "comment": "Accepted and will be published at ACL2025 (main conference)", "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u8001\u5e74\u60a3\u8005\u51fa\u9662\u6458\u8981\u4e2d\u4e0d\u826f\u4e8b\u4ef6\uff08AE\uff09\u63d0\u53d6\u7684\u8bed\u6599\u5e93\uff0c\u586b\u8865\u4e86\u4e34\u5e8aNLP\u8d44\u6e90\u4e2d\u8001\u5e74\u4eba\u7fa4\u7684\u4e0d\u8db3\u3002\u6570\u636e\u96c6\u5305\u542b14\u79cd\u4e34\u5e8a\u663e\u8457AE\u53ca\u4e0a\u4e0b\u6587\u5c5e\u6027\uff0c\u652f\u6301\u4e0d\u8fde\u7eed\u548c\u91cd\u53e0\u5b9e\u4f53\u6807\u6ce8\u3002\u5b9e\u9a8c\u663e\u793a\uff0cBERT\u7b49\u6a21\u578b\u5728\u7c97\u7c92\u5ea6\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff08F1=0.943\uff09\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff08F1=0.675\uff09\u548c\u7f55\u89c1\u4e8b\u4ef6\u4e0a\u8868\u73b0\u8f83\u5dee\u3002\u6570\u636e\u96c6\u901a\u8fc7DataLoch\u63d0\u4f9b\uff0c\u53ef\u4f5c\u4e3aAE\u63d0\u53d6\u65b9\u6cd5\u7684\u57fa\u51c6\u3002", "motivation": "\u8001\u5e74\u60a3\u8005\u5728\u4e34\u5e8aNLP\u8d44\u6e90\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u4e0d\u8fde\u7eed\u548c\u91cd\u53e0\u5b9e\u4f53\u7684\u5904\u7406\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u8001\u5e74\u60a3\u8005\u7684AE\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b14\u79cdAE\u53ca\u5176\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u652f\u6301\u4e0d\u8fde\u7eed\u548c\u91cd\u53e0\u5b9e\u4f53\u3002\u4f7f\u7528FlairNLP\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982BERT\uff09\u5728\u4e09\u79cd\u6807\u6ce8\u7c92\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "result": "BERT\u5728\u7c97\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff08F1=0.943\uff09\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff08F1=0.675\uff09\u548c\u7f55\u89c1\u4e8b\u4ef6\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u8868\u660e\u590d\u6742\u4e34\u5e8a\u8bed\u8a00\u548c\u7f55\u89c1\u4e8b\u4ef6\u7684\u68c0\u6d4b\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u7c97\u7c92\u5ea6\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u7ec6\u7c92\u5ea6AE\u63d0\u53d6\u548c\u7f55\u89c1\u4e8b\u4ef6\u68c0\u6d4b\u4ecd\u9700\u6539\u8fdb\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u652f\u6301\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u3002"}}
{"id": "2506.15030", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15030", "abs": "https://arxiv.org/abs/2506.15030", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "comment": "22 pages, 2 figures, 5 tables", "summary": "Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528NLP\u6280\u672f\u4ece\u6267\u6cd5\u548c\u6cd5\u533b\u53d9\u8ff0\u4e2d\u8bc6\u522b\u793e\u4f1a\u9694\u79bb\u548c\u5b64\u72ec\u611f\uff0c\u5f00\u53d1\u4e86\u9ad8\u8d28\u91cf\u5206\u7c7b\u5668\uff0c\u5e76\u5206\u6790\u4e86\u81ea\u6740\u6848\u4f8b\u4e2d\u7684\u76f8\u5173\u56e0\u7d20\u3002", "motivation": "\u793e\u4f1a\u9694\u79bb\u548c\u5b64\u72ec\u611f\u8fd1\u5e74\u589e\u52a0\uff0c\u4e0e\u81ea\u6740\u7387\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u672a\u88ab\u7f8e\u56fd\u56fd\u5bb6\u66b4\u529b\u6b7b\u4ea1\u62a5\u544a\u7cfb\u7edf\u8bb0\u5f55\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u8bc6\u522b\u3002", "method": "\u4f7f\u7528\u4e3b\u9898\u5efa\u6a21\u751f\u6210\u8bcd\u5178\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5206\u67902002\u81f32020\u5e7430\u4e07\u4f8b\u81ea\u6740\u6848\u4f8b\u3002", "result": "\u5f00\u53d1\u51fa\u9ad8\u8d28\u91cf\u5206\u7c7b\u5668\uff08F1\u5e73\u57470.86\uff0c\u51c6\u786e\u73870.82\uff09\uff0c\u8bc6\u522b\u51fa1,198\u4f8b\u6162\u6027\u793e\u4f1a\u9694\u79bb\u6848\u4f8b\uff0c\u7537\u6027\u3001\u540c\u6027\u604b\u6216\u79bb\u5a5a\u8005\u98ce\u9669\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6539\u8fdb\u7f8e\u56fd\u793e\u4f1a\u9694\u79bb\u548c\u5b64\u72ec\u611f\u7684\u76d1\u6d4b\u4e0e\u9884\u9632\u3002"}}
{"id": "2506.14835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14835", "abs": "https://arxiv.org/abs/2506.14835", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "comment": null, "summary": "Precisely localizing 3D objects from a single image constitutes a central\nchallenge in monocular 3D detection. While DETR-like architectures offer a\npowerful paradigm, their direct application in this domain encounters inherent\nlimitations, preventing optimal performance. Our work addresses these\nchallenges by introducing MonoVQD, a novel framework designed to fundamentally\nadvance DETR-based monocular 3D detection. We propose three main contributions.\nFirst, we propose the Mask Separated Self-Attention mechanism that enables the\nintegration of the denoising process into a DETR architecture. This improves\nthe stability of Hungarian matching to achieve a consistent optimization\nobjective. Second, we present the Variational Query Denoising technique to\naddress the gradient vanishing problem of conventional denoising methods, which\nseverely restricts the efficiency of the denoising process. This explicitly\nintroduces stochastic properties to mitigate this fundamental limitation and\nunlock substantial performance gains. Finally, we introduce a sophisticated\nself-distillation strategy, leveraging insights from later decoder layers to\nsynergistically improve query quality in earlier layers, thereby amplifying the\niterative refinement process. Rigorous experimentation demonstrates that\nMonoVQD achieves superior performance on the challenging KITTI monocular\nbenchmark. Highlighting its broad applicability, MonoVQD's core components\nseamlessly integrate into other architectures, delivering significant\nperformance gains even in multi-view 3D detection scenarios on the nuScenes\ndataset and underscoring its robust generalization capabilities.", "AI": {"tldr": "MonoVQD\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdbDETR\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5305\u62ec\u7a33\u5b9a\u6027\u3001\u68af\u5ea6\u6d88\u5931\u548c\u67e5\u8be2\u8d28\u91cf\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5355\u76ee3D\u68c0\u6d4b\u4e2d\uff0cDETR\u67b6\u6784\u7684\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002MonoVQD\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u63d0\u51faMask Separated Self-Attention\u673a\u5236\uff0c\u5c06\u53bb\u566a\u8fc7\u7a0b\u96c6\u6210\u5230DETR\u4e2d\uff1b2. \u5f15\u5165Variational Query Denoising\u6280\u672f\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff1b3. \u91c7\u7528\u81ea\u84b8\u998f\u7b56\u7565\u4f18\u5316\u67e5\u8be2\u8d28\u91cf\u3002", "result": "\u5728KITTI\u5355\u76ee\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5176\u6838\u5fc3\u7ec4\u4ef6\u5728\u591a\u89c6\u56fe3D\u68c0\u6d4b\uff08nuScenes\u6570\u636e\u96c6\uff09\u4e2d\u4e5f\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "MonoVQD\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.15150", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15150", "abs": "https://arxiv.org/abs/2506.15150", "authors": ["Yuanlong Ji", "Xingbang Yang", "Ruoqi Zhao", "Qihan Ye", "Quan Zheng", "Yubo Fan"], "title": "Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation", "comment": null, "summary": "Gait phase estimation based on inertial measurement unit (IMU) signals\nfacilitates precise adaptation of exoskeletons to individual gait variations.\nHowever, challenges remain in achieving high accuracy and robustness,\nparticularly during periods of terrain changes. To address this, we develop a\ngait phase estimation neural network based on implicit modeling of human\nlocomotion, which combines temporal convolution for feature extraction with\ntransformer layers for multi-channel information fusion. A channel-wise masked\nreconstruction pre-training strategy is proposed, which first treats gait phase\nstate vectors and IMU signals as joint observations of human locomotion, thus\nenhancing model generalization. Experimental results demonstrate that the\nproposed method outperforms existing baseline approaches, achieving a gait\nphase RMSE of $2.729 \\pm 1.071%$ and phase rate MAE of $0.037 \\pm 0.016%$ under\nstable terrain conditions with a look-back window of 2 seconds, and a phase\nRMSE of $3.215 \\pm 1.303%$ and rate MAE of $0.050 \\pm 0.023%$ under terrain\ntransitions. Hardware validation on a hip exoskeleton further confirms that the\nalgorithm can reliably identify gait cycles and key events, adapting to various\ncontinuous motion scenarios. This research paves the way for more intelligent\nand adaptive exoskeleton systems, enabling safer and more efficient human-robot\ninteraction across diverse real-world environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u5efa\u6a21\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u65f6\u95f4\u5377\u79ef\u548cTransformer\u5c42\uff0c\u901a\u8fc7\u901a\u9053\u63a9\u7801\u91cd\u5efa\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u7a33\u5b9a\u548c\u5730\u5f62\u53d8\u5316\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eIMU\u4fe1\u53f7\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u5728\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5730\u5f62\u53d8\u5316\u65f6\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u5377\u79ef\u548cTransformer\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u901a\u9053\u63a9\u7801\u91cd\u5efa\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u6b65\u6001\u76f8\u4f4d\u72b6\u6001\u5411\u91cf\u548cIMU\u4fe1\u53f7\u4f5c\u4e3a\u8054\u5408\u89c2\u6d4b\u3002", "result": "\u5728\u7a33\u5b9a\u5730\u5f62\u4e0b\u6b65\u6001\u76f8\u4f4dRMSE\u4e3a2.729\u00b11.071%\uff0c\u76f8\u4f4d\u7387MAE\u4e3a0.037\u00b10.016%\uff1b\u5730\u5f62\u53d8\u5316\u65f6RMSE\u4e3a3.215\u00b11.303%\uff0cMAE\u4e3a0.050\u00b10.023%\u3002\u786c\u4ef6\u9a8c\u8bc1\u8868\u660e\u7b97\u6cd5\u80fd\u53ef\u9760\u8bc6\u522b\u6b65\u6001\u5468\u671f\u548c\u5173\u952e\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u667a\u80fd\u548c\u81ea\u9002\u5e94\u7684\u5916\u9aa8\u9abc\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.14830", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.14830", "abs": "https://arxiv.org/abs/2506.14830", "authors": ["Zhizhao Wen", "Ruoxin Zhang", "Chao Wang"], "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model", "comment": "Source code available; Accepted by 2025 6th International Conference\n  on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures", "summary": "Aiming at the critical role of SSD health state prediction in data\nreliability assurance, this study proposes a hybrid BiGRU-MHA model that\nincorporates a multi-head attention mechanism to enhance the accuracy and\nstability of storage device health classification. The model innovatively\nintegrates temporal feature extraction and key information focusing\ncapabilities. Specifically, it leverages the bidirectional timing modeling\nadvantages of the BiGRU network to capture both forward and backward\ndependencies of SSD degradation features. Simultaneously, the multi-head\nattention mechanism dynamically assigns feature weights, improving the model's\nsensitivity to critical health indicators. Experimental results show that the\nproposed model achieves classification accuracies of 92.70% on the training set\nand 92.44% on the test set, with a minimal performance gap of only 0.26%,\ndemonstrating excellent generalization ability. Further analysis using the\nreceiver operating characteristic (ROC) curve shows an area under the curve\n(AUC) of 0.94 on the test set, confirming the model's robust binary\nclassification performance. This work not only presents a new technical\napproach for SSD health prediction but also addresses the generalization\nbottleneck of traditional models, offering a verifiable method with practical\nvalue for preventive maintenance of industrial-grade storage systems. The\nresults show the model can significantly reduce data loss risks by providing\nearly failure warnings and help optimize maintenance costs, supporting\nintelligent decision-making in building reliable storage systems for cloud\ncomputing data centers and edge storage environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BiGRU\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df7\u5408\u6a21\u578b\uff08BiGRU-MHA\uff09\uff0c\u7528\u4e8eSSD\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "SSD\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\u5bf9\u6570\u636e\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u6a21\u578b\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528BiGRU\u7f51\u7edc\u7684\u53cc\u5411\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\u6355\u6349SSD\u9000\u5316\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u5206\u914d\u7279\u5f81\u6743\u91cd\u3002", "result": "\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a92.70%\u548c92.44%\uff0cAUC\u4e3a0.94\uff0c\u6cdb\u5316\u80fd\u529b\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aSSD\u5065\u5eb7\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u65b9\u6848\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u6570\u636e\u4e22\u5931\u98ce\u9669\u5e76\u4f18\u5316\u7ef4\u62a4\u6210\u672c\u3002"}}
{"id": "2506.14911", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.14911", "abs": "https://arxiv.org/abs/2506.14911", "authors": ["Ganyu Wang", "Boyu Wang", "Bin Gu", "Charles Ling"], "title": "Event-Driven Online Vertical Federated Learning", "comment": "Published as a conference paper at ICLR 2025", "summary": "Online learning is more adaptable to real-world scenarios in Vertical\nFederated Learning (VFL) compared to offline learning. However, integrating\nonline learning into VFL presents challenges due to the unique nature of VFL,\nwhere clients possess non-intersecting feature sets for the same sample. In\nreal-world scenarios, the clients may not receive data streaming for the\ndisjoint features for the same entity synchronously. Instead, the data are\ntypically generated by an \\emph{event} relevant to only a subset of clients. We\nare the first to identify these challenges in online VFL, which have been\noverlooked by previous research. To address these challenges, we proposed an\nevent-driven online VFL framework. In this framework, only a subset of clients\nwere activated during each event, while the remaining clients passively\ncollaborated in the learning process. Furthermore, we incorporated\n\\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by\nonline learning problems with non-convex models within a non-stationary\nenvironment. We conducted a comprehensive regret analysis of our proposed\nframework, specifically examining the DLR under non-convex conditions with\nevent-driven online VFL. Extensive experiments demonstrated that our proposed\nframework was more stable than the existing online VFL framework under\nnon-stationary data conditions while also significantly reducing communication\nand computation costs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebf\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6b65\u548c\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5c40\u90e8\u9057\u61be\uff08DLR\uff09\u4f18\u5316\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u5728VFL\u4e2d\u66f4\u5177\u9002\u5e94\u6027\uff0c\u4f46\u6570\u636e\u5f02\u6b65\u6027\u548c\u975e\u5e73\u7a33\u6027\u5e26\u6765\u4e86\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\uff0c\u4ec5\u6fc0\u6d3b\u90e8\u5206\u5ba2\u6237\u7aef\uff0c\u5f15\u5165\u52a8\u6001\u5c40\u90e8\u9057\u61be\uff08DLR\uff09\u5904\u7406\u975e\u51f8\u548c\u975e\u5e73\u7a33\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u975e\u5e73\u7a33\u6570\u636e\u4e0b\u66f4\u7a33\u5b9a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u7ebfVFL\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2506.15201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15201", "abs": "https://arxiv.org/abs/2506.15201", "authors": ["Xuelin Shen", "Jiayin Xu", "Kangsheng Yin", "Wenhan Yang"], "title": "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models", "comment": "11 pages, 6 figures, publised to ICML 2025", "summary": "The improved semantic understanding of vision-language pretrained (VLP)\nmodels has made it increasingly difficult to protect publicly posted images\nfrom being exploited by search engines and other similar tools. In this\ncontext, this paper seeks to protect users' privacy by implementing defenses at\nthe image compression stage to prevent exploitation. Specifically, we propose a\nflexible coding method, termed Privacy-Shielded Image Compression (PSIC), that\ncan produce bitstreams with multiple decoding options. By default, the\nbitstream is decoded to preserve satisfactory perceptual quality while\npreventing interpretation by VLP models. Our method also retains the original\nimage compression functionality. With a customizable input condition, the\nproposed scheme can reconstruct the image that preserves its full semantic\ninformation. A Conditional Latent Trigger Generation (CLTG) module is proposed\nto produce bias information based on customizable conditions to guide the\ndecoding process into different reconstructed versions, and an\nUncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed\nto leverage the soft labels inferred from the target VLP model's uncertainty on\nthe training data. This paper further incorporates an adaptive multi-objective\noptimization strategy to obtain improved encrypting performance and perceptual\nquality simultaneously within a unified training process. The proposed scheme\nis plug-and-play and can be seamlessly integrated into most existing Learned\nImage Compression (LIC) models. Extensive experiments across multiple\ndownstream tasks have demonstrated the effectiveness of our design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPSIC\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89e3\u7801\u9009\u9879\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u7559\u56fe\u50cf\u538b\u7f29\u529f\u80fd\uff0c\u5e76\u5229\u7528CLTG\u548cUAEO\u6a21\u5757\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u8bed\u4e49\u7406\u89e3\u7684\u63d0\u5347\uff0c\u516c\u5f00\u56fe\u7247\u6613\u88ab\u641c\u7d22\u5f15\u64ce\u7b49\u5de5\u5177\u5229\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "method": "\u63d0\u51faPSIC\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u89e3\u7801\u9009\u9879\uff1b\u8bbe\u8ba1CLTG\u6a21\u5757\u751f\u6210\u504f\u7f6e\u4fe1\u606f\uff0cUAEO\u4f18\u5316\u51fd\u6570\u5229\u7528\u76ee\u6807\u6a21\u578b\u7684\u8f6f\u6807\u7b7e\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u591a\u76ee\u6807\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePSIC\u80fd\u6709\u6548\u4fdd\u62a4\u9690\u79c1\u5e76\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\uff0c\u4e14\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709LIC\u6a21\u578b\u4e2d\u3002", "conclusion": "PSIC\u662f\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u56fe\u50cf\u538b\u7f29\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2506.15285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15285", "abs": "https://arxiv.org/abs/2506.15285", "authors": ["Mattia Nardon", "Stefano Messelodi", "Antonio Granata", "Fabio Poiesi", "Alberto Danese", "Davide Boscaini"], "title": "AI-driven visual monitoring of industrial assembly tasks", "comment": null, "summary": "Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT", "AI": {"tldr": "ViMAT\u662f\u4e00\u79cd\u65e0\u9700\u6807\u8bb0\u6216\u56fa\u5b9a\u5de5\u4f5c\u73af\u5883\u7684AI\u9a71\u52a8\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u89c6\u9891\u6d41\u548c\u63a8\u7406\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u76d1\u63a7\u3002", "motivation": "\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\u7684\u89c6\u89c9\u76d1\u63a7\u5bf9\u9632\u6b62\u8bbe\u5907\u635f\u574f\u548c\u4fdd\u969c\u5de5\u4eba\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9700\u8981\u56fa\u5b9a\u5de5\u4f5c\u73af\u5883\u6216\u89c6\u89c9\u6807\u8bb0\u3002", "method": "ViMAT\u7ed3\u5408\u611f\u77e5\u6a21\u5757\uff08\u4ece\u591a\u89c6\u89d2\u89c6\u9891\u6d41\u63d0\u53d6\u89c6\u89c9\u4fe1\u606f\uff09\u548c\u63a8\u7406\u6a21\u5757\uff08\u57fa\u4e8e\u89c2\u5bdf\u7684\u88c5\u914d\u72b6\u6001\u548c\u4efb\u52a1\u77e5\u8bc6\u63a8\u65ad\u6700\u53ef\u80fd\u7684\u52a8\u4f5c\uff09\u3002", "result": "\u5728LEGO\u7ec4\u4ef6\u66f4\u6362\u548c\u6db2\u538b\u6a21\u5177\u91cd\u7ec4\u4efb\u52a1\u4e2d\uff0cViMAT\u5728\u90e8\u5206\u548c\u4e0d\u786e\u5b9a\u89c6\u89c9\u89c2\u5bdf\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ViMAT\u5728\u65e0\u9700\u6807\u8bb0\u6216\u56fa\u5b9a\u73af\u5883\u7684\u6761\u4ef6\u4e0b\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\u7684\u5b9e\u65f6\u89c6\u89c9\u76d1\u63a7\u3002"}}
{"id": "2506.15681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15681", "abs": "https://arxiv.org/abs/2506.15681", "authors": ["Byung-Kwan Lee", "Ryo Hachiuma", "Yong Man Ro", "Yu-Chiang Frank Wang", "Yueh-Hua Wu"], "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models", "comment": "Project page: https://byungkwanlee.github.io/GenRecal-page/", "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.", "AI": {"tldr": "GenRecal\u662f\u4e00\u79cd\u901a\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u548c\u9002\u5e94\u5b9e\u73b0\u5f02\u6784VLM\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784VLM\u67b6\u6784\u591a\u6837\u6027\u5bfc\u81f4\u7684\u84b8\u998f\u56f0\u96be\uff0c\u4f7f\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u63d0\u51faGeneration after Recalibration (GenRecal)\u6846\u67b6\uff0c\u5305\u542bRecalibrator\u6a21\u5757\uff0c\u5bf9\u9f50\u5f02\u6784VLM\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8d85\u8d8a\u5f00\u6e90\u548c\u95ed\u6e90VLM\u3002", "conclusion": "GenRecal\u4e3a\u5f02\u6784VLM\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
