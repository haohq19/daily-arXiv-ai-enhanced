<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4是一个无需训练的检索增强推理框架，为视觉语言模型提供结构化、终身记忆的4D时空知识库，通过将物体级语义描述锚定在度量空间和时间中，支持在动态环境中的具身推理。


<details>
  <summary>Details</summary>
Motivation: 受人类在四维时空（三维空间+时间）中感知和推理能力的启发，希望为视觉语言模型提供类似的结构化内部表示，使其能够回忆过去事件、推断未观察状态，并在动态环境中进行上下文依赖的推理。

Method: R4框架持续构建4D知识数据库，将物体级语义描述锚定在度量空间和时间中，形成持久的世界模型。推理时，自然语言查询被分解为语义、空间和时间键，检索相关观察结果并集成到视觉语言模型的推理中。无需训练，检索直接在4D空间中进行。

Result: 在具身问答和导航基准测试中，R4在时空信息检索和推理方面显著优于基线方法，展示了在动态环境中进行4D推理的新范式。

Conclusion: R4通过为视觉语言模型提供结构化、终身记忆的4D时空知识库，实现了无需训练的检索增强推理，支持情节性和协作性推理，为动态环境中的具身4D推理提供了新范式。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [2] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种基于闪烁星点图案准直仪的事件相机标定方法，用于解决长距离高精度测量中的几何标定难题


<details>
  <summary>Details</summary>
Motivation: 事件相机作为新型仿生视觉传感器具有高动态范围和高时间分辨率等优势，但在长距离测量场景中的几何标定（包括内参和外参确定）仍然是一个重大挑战

Method: 使用带有闪烁星点图案的准直仪，首先基于准直仪的球面运动模型线性求解相机参数，然后通过非线性优化对这些参数进行高精度细化

Result: 在不同条件下的综合真实世界实验中，该方法在准确性和可靠性方面始终优于现有的事件相机标定方法

Conclusion: 提出的基于准直仪闪烁星点图案的事件相机标定方法能够有效解决长距离高精度测量需求，为事件相机的实际应用提供了可靠的标定解决方案

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [3] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 提出CattleAct方法，通过将放牧牛的交互行为分解为个体牛的动作组合，使用对比学习在预训练动作潜空间上微调，实现从单张图像自动检测牛群交互行为


<details>
  <summary>Details</summary>
Motivation: 智能畜牧业需要自动检测牛群行为交互（如发情检测），但现有研究主要针对人类交互检测，牛群交互检测面临缺乏全面行为数据集和交互事件稀少的挑战

Method: 首先从大规模牛动作数据集学习动作潜空间，然后使用对比学习在预训练潜空间上微调嵌入稀有交互，构建动作和交互的统一潜空间，并集成视频和GPS输入开发实用系统

Result: 在商业规模牧场上的实验表明，该方法相比基线实现了准确的交互检测，并开发了可用的实现系统

Conclusion: CattleAct通过数据高效的方法解决了牛群交互检测的挑战，为智能畜牧业提供了实用的交互检测解决方案

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [4] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: C-DGPA提出了一种基于类别中心的双重对齐生成提示适应方法，通过双重分支架构同时优化边缘分布对齐和条件分布对齐，解决VLM在UDA任务中的领域差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的提示调优方法主要关注边缘分布对齐，但忽略了条件分布差异，导致类别原型错位和语义判别性下降等问题。需要同时解决两种分布差异以实现更好的领域自适应。

Method: 提出C-DGPA框架，包含两个分支：1）边缘分布对齐分支采用动态对抗训练框架；2）条件分布对齐分支引入类别映射机制（CMM）来标准化语义提示理解并防止对源域的过度依赖。通过协同优化将领域知识整合到提示学习中。

Result: 在OfficeHome、Office31和VisDA-2017数据集上的广泛实验验证了C-DGPA的优越性，在所有基准测试中都取得了新的最先进结果。

Conclusion: C-DGPA通过双重对齐策略有效解决了领域自适应中的分布差异问题，实现了领域不变且语义判别性强的表示学习，为视觉语言模型在无监督领域自适应中的应用提供了有效解决方案。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [5] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 事件相机在自动驾驶中能比传统相机更好地应对昼夜光照变化的领域差距问题


<details>
  <summary>Details</summary>
Motivation: 传统相机在端到端驾驶中性能受限于训练数据与部署环境的领域差距，特别是昼夜光照差异问题

Method: 提出使用事件相机作为传统相机的替代方案，评估其在昼夜光照变化下的性能表现

Result: 事件相机在不同光照条件下保持更一致的性能，领域偏移惩罚通常与灰度帧相当或更小，在跨领域场景中提供更优的基准性能

Conclusion: 事件相机是应对自动驾驶中昼夜光照领域差距问题的有效替代方案，无需额外调整即可保持跨光照条件的性能一致性

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [6] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出基于Grad-CAM的弱监督深度学习框架，使用图像级标签而非像素级标注进行肺炎分类和定位，评估7种预训练模型，ResNet-18和EfficientNet-B0表现最佳。


<details>
  <summary>Details</summary>
Motivation: 胸部X光成像常用于肺炎诊断，但精确定位肺炎区域通常需要昂贵的像素级标注。为解决这一限制，本研究提出弱监督方法以减少标注成本。

Method: 使用Grad-CAM生成肺炎区域热力图，仅需图像级标签。评估7种预训练模型（包括Vision Transformer），采用焦点损失和患者级数据划分防止数据泄露。

Result: 所有模型达到高分类准确率（96-98%），ResNet-18和EfficientNet-B0表现最佳，MobileNet-V3作为轻量级替代。Grad-CAM热力图显示模型聚焦于临床相关肺区域。

Conclusion: 弱监督可解释AI模型在肺炎筛查中具有潜力，能增强透明度和临床信任，减少对昂贵像素级标注的依赖。

Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.

</details>


### [7] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: TTP是一种轻量级测试时防御框架，通过空间填充前后的特征相似度变化检测对抗样本，并使用可训练填充恢复注意力模式，在保持干净准确率的同时显著提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（如CLIP）在零样本识别方面表现优异，但对对抗扰动高度脆弱，存在安全风险。现有训练时防御需要标注数据和昂贵重训练，而测试时策略无法可靠区分干净和对抗输入，无法同时优化对抗鲁棒性和干净准确率。

Method: 提出测试时填充（TTP）框架：1）通过计算CLIP特征在空间填充前后的余弦相似度变化检测对抗输入，使用通用阈值；2）对检测到的对抗样本，使用可训练填充恢复被破坏的注意力模式，并结合相似度感知集成策略；3）对干净输入默认保持不变或集成现有测试时适应技术。

Result: 在多种CLIP骨干网络和细粒度基准测试上的综合实验表明，TTP持续超越最先进的测试时防御方法，在不损害干净准确率的情况下显著提升对抗鲁棒性。

Conclusion: TTP提供了一种轻量级、高效的测试时防御方案，能够可靠检测对抗样本并进行针对性适应，解决了现有方法在对抗鲁棒性和干净准确率之间的权衡问题。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [8] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个多模态框架，通过结合文本、轨迹和参考图像来生成用户可提示的世界事件，实现丰富可控的模拟。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：纯文本方法表达能力有限，现有轨迹控制的图像到视频方法缺乏语义意图和视觉基础。需要一种能够生成连贯、可控事件的方法，支持多智能体交互、对象进出、参考引导外观和反直觉事件。

Method: 采用多模态方法，结合轨迹（编码运动、时序和可见性）、自然语言（语义意图）和参考图像（对象身份的视觉基础）。通过这种组合实现可控事件生成。

Result: 生成的视频不仅具有时间连贯性，还表现出涌现一致性，能够在对象暂时消失时保持对象身份和场景一致性。支持表达性世界事件生成。

Conclusion: WorldCanvas将世界模型从被动预测器推进到交互式、用户可塑造的模拟器，实现了丰富、用户导向的模拟能力。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: 提出CSA-TTA框架，通过跨样本增强解决术中低血压预测中事件稀少导致的测试时训练不可靠问题，结合粗到细检索策略和多任务学习提升模型适应性。


<details>
  <summary>Details</summary>
Motivation: 术中低血压(IOH)预测存在挑战，因为患者特异性变异性大。虽然测试时适应(TTA)为个性化预测提供了有前景的方法，但IOH事件的稀少性常导致测试时训练不可靠。

Method: 提出CSA-TTA框架：1)构建跨样本库，将历史数据分割为低血压和非低血压样本；2)采用粗到细检索策略：先用K-Shape聚类识别代表性聚类中心，然后基于当前患者信号检索前K个语义相似样本；3)训练时集成自监督掩码重建和回顾性序列预测信号。

Result: 在VitalDB数据集和真实医院数据集上评估，与TimesFM和UniTS等最先进时间序列预测模型集成。在VitalDB上，微调场景下Recall和F1分别提升+1.33%和+1.13%，零样本场景下提升+7.46%和+5.07%，表现出强鲁棒性和泛化能力。

Conclusion: CSA-TTA通过跨样本增强有效解决了IOH预测中事件稀少导致的测试时训练不可靠问题，显著提升了预测性能，特别是在零样本场景下表现突出，为术中低血压的个性化预测提供了有效解决方案。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [10] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 该研究将LDA主题模型的主题数量选择问题形式化为离散黑盒优化问题，比较了四种优化方法（GA、ES、PABBO、SABBO），发现摊销优化器在样本和时间效率上显著优于传统进化算法。


<details>
  <summary>Details</summary>
Motivation: LDA主题模型中主题数量T的选择是关键设计决策，直接影响模型的统计拟合度和可解释性。传统方法通常依赖启发式或网格搜索，缺乏系统化的优化框架。

Method: 将主题数量选择问题形式化为离散黑盒优化问题，其中每个函数评估对应训练一个LDA模型并计算验证困惑度。在固定评估预算下，比较了四种优化器：两种手工设计的进化方法（遗传算法GA和进化策略ES），以及两种学习的摊销方法（PABBO和SABBO）。

Result: 虽然四种方法最终都能达到相似的困惑度水平，但摊销优化器在样本和时间效率上显著更优。SABBO通常只需一次评估就能找到接近最优的主题数量，PABBO在几次评估内就能找到有竞争力的配置，而GA和ES需要几乎全部预算才能达到相同区域。

Conclusion: 摊销黑盒优化方法（特别是SABBO）为LDA主题数量选择问题提供了高效解决方案，显著减少了计算成本，使主题模型调参更加实用。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [11] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: AIMM是一个AI驱动的框架，通过融合Reddit活动、机器人指标和市场价格数据，为每个股票代码生成每日操纵风险评分，用于检测社交媒体驱动的市场操纵。


<details>
  <summary>Details</summary>
Motivation: 市场操纵现在通常来自协调的社交媒体活动而非孤立交易，零售投资者、监管机构和经纪商需要能够将在线叙事和协调模式与市场行为联系起来的工具。

Method: 使用parquet原生管道和Streamlit仪表板，融合Reddit活动、机器人和协调指标以及OHLCV市场特征，生成每日AIMM操纵风险评分。由于API限制，使用校准的合成社交特征匹配事件特征，市场数据使用真实历史数据。

Result: 构建了AIMM-GT数据集（33个标记的股票-日，涵盖8只股票），展示了初步的判别能力，AIMM在2021年1月GME挤压峰值前22天发出预警。

Conclusion: 虽然当前标记数据集较小，但结果显示了初步的判别能力和对GME事件的早期预警，发布代码、数据集模式和仪表板设计以支持社交媒体驱动的市场监控研究。

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [12] [Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models](https://arxiv.org/abs/2512.16244)
*Xueqi Ma,Xingjun Ma,Sarah Monazam Erfani,Danilo Mandic,James Bailey*

Main category: cs.LG

TL;DR: 提出CFC框架，利用大语言模型进行图数据的开放集分类，实现从OOD检测到OOD分类的扩展，无需真实标签信息。


<details>
  <summary>Details</summary>
Motivation: 现有开放集分类方法通常将所有OOD样本视为单一类别，但在欺诈检测和医疗诊断等高风险场景中，需要对OOD样本有更深入的理解，包括其可能的标签。这引出了一个关键问题：能否在没有真实标签信息的情况下，将OOD检测扩展到OOD分类？

Method: 提出粗到细开放集分类框架，包含三个关键组件：1）粗分类器使用LLM提示进行OOD检测和异常标签生成；2）基于GNN的细分类器利用粗分类器识别的OOD样本进行训练，增强OOD检测和ID分类；3）通过LLM提示和后处理的OOD标签实现精细化的OOD分类。该方法使用语义上真正分布外的实例，而非合成或辅助样本。

Result: 实验结果显示，CFC在图和文本领域的OOD检测性能比最先进方法提高了10%，在图数据集上的OOD分类准确率最高达到70%。

Conclusion: CFC框架成功地将OOD检测扩展到OOD分类，无需真实标签信息，通过结合LLM和GNN的优势，在开放集分类任务中取得了显著改进，特别是在高风险应用场景中具有更好的可解释性和实用性。

Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.

</details>


### [13] [Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning](https://arxiv.org/abs/2512.16476)
*Pengfei Sun,Wenyu Jiang,Piew Yoong Chee,Paul Devos,Dick Botteldooren*

Main category: cs.LG

TL;DR: 提出一种无需批归一化(BN)的完全整数量化神经网络训练方法，通过渐进式逐层蒸馏实现整数推理，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有量化神经网络(QNNs)大多依赖批归一化(BN)层，这阻碍了真正的整数推理部署。先前移除BN的方法往往无法恢复BN的稳定性和准确性，且施加了特殊约束。

Method: 采用渐进式逐层蒸馏方案：从预训练的带BN的教师模型开始，使用逐层目标和渐进补偿训练学生模型，实现完全整数算术推理且不含BN操作。

Result: 在ImageNet数据集上使用AlexNet，BN-free模型在激进量化下获得了有竞争力的Top-1准确率。该方法可直接集成到标准量化流程中。

Conclusion: 该方法实现了端到端的整数推理，适用于边缘和嵌入式设备等资源受限场景，解决了量化神经网络中BN依赖问题。

Abstract: Quantised neural networks (QNNs) shrink models and reduce inference energy through low-bit arithmetic, yet most still depend on a running statistics batch normalisation (BN) layer, preventing true integer-only deployment. Prior attempts remove BN by parameter folding or tailored initialisation; while helpful, they rarely recover BN's stability and accuracy and often impose bespoke constraints. We present a BN-free, fully integer QNN trained via a progressive, layer-wise distillation scheme that slots into existing low-bit pipelines. Starting from a pretrained BN-enabled teacher, we use layer-wise targets and progressive compensation to train a student that performs inference exclusively with integer arithmetic and contains no BN operations. On ImageNet with AlexNet, the BN-free model attains competitive Top-1 accuracy under aggressive quantisation. The procedure integrates directly with standard quantisation workflows, enabling end-to-end integer-only inference for resource-constrained settings such as edge and embedded devices.

</details>


### [14] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: 提出Abacus方法，通过预测用户事件的经验频率分布来增强深度序列模型，结合自监督预训练解决广告系统中用户购买行为建模的稀疏性和随机性问题。


<details>
  <summary>Details</summary>
Motivation: 在展示广告系统中，用户购买行为建模面临正样本稀疏和用户行为随机性的挑战，导致严重的类别不平衡和不规则事件时序。现有方法要么依赖手工设计的"计数器"特征而忽略用户意图的细粒度时序演化，要么只提取直接序列信号而忽略有用的事件计数统计信息。

Method: 提出Abacus方法，通过自监督预训练预测用户事件的经验频率分布。进一步提出混合目标函数，将Abacus与序列学习目标统一，结合聚合统计的稳定性和序列建模的敏感性。

Result: 在两个真实世界数据集上的实验表明，Abacus预训练优于现有方法，加速了下游任务的收敛。混合方法相比基线获得了高达+6.1%的AUC提升。

Conclusion: Abacus方法通过结合事件频率分布预测和序列建模，有效解决了广告系统中用户行为建模的挑战，在性能和收敛速度方面均有显著提升。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [15] [MEPIC: Memory Efficient Position Independent Caching for LLM Serving](https://arxiv.org/abs/2512.16822)
*Qian Wang,Zahra Yousefijamarani,Morgan Lindsay Heisler,Rongzhi Gu,Bai Xiaolong,Shan Yizhou,Wei Zhang,Wang Lan,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: MEPIC：一种内存高效的PIC系统，通过页对齐存储、块级重计算和RoPE融合，实现跨位置、请求和批次的块级KV缓存重用，显著减少HBM内存使用


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用（如深度研究助手、编码代理和RAG系统）需要重复处理包含共享文档或代码块的长提示历史，给KV缓存带来巨大内存压力。现有前缀缓存和位置无关缓存（PIC）存在限制，无法有效重用相同内容的KV缓存

Method: 1. 将块KV对齐到分页存储；2. 将重计算从token级转移到块级，使只有第一个块是请求特定的；3. 通过注意力内核中的RoPE融合移除位置编码；4. 使剩余块完全可共享

Result: 相比最先进的PIC，在保持相当延迟和准确性的情况下，HBM内存使用减少高达2倍；对于长提示，减少高达5倍，无需任何模型更改

Conclusion: MEPIC通过消除HBM中的重复块KV，实现了跨位置、请求和批次的块级KV重用，显著提升了内存效率，为处理长提示的LLM应用提供了有效的解决方案

Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

</details>


### [16] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 论文提出知识转换(KT)方法，结合知识蒸馏、主动学习和因果推理，为在线边缘机器学习中未见数据生成伪标签，解决标签获取难题。


<details>
  <summary>Details</summary>
Motivation: 现有边缘机器学习方法通常假设静态模型集中训练后部署，无法有效处理未见数据。在线边缘机器学习允许模型直接在边缘设备上训练并持续更新，但面临如何为未来未见数据确定标签的关键挑战。

Method: 提出知识转换(KT)方法，结合知识蒸馏、主动学习和因果推理。KT作为主动学习中的"预言机"，通过从教师模型转换知识来为学生模型生成伪标签进行训练。

Result: 模拟实验显示：当使用相对稳定的教师模型时，学生模型最终能达到预期最大性能。KT在两种情况下特别有益：(1)教师任务通用，可使用预训练模型；(2)学生任务标签难以或昂贵获取。

Conclusion: KT方法为解决在线边缘机器学习中未见数据的标签问题提供了有效方案，特别适用于教师任务通用或学生标签获取困难的场景。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 本文提出比例责任原则（PPD），将道德责任建模为随认知状态变化的函数，揭示不确定性不会消除责任而是将其转化为修复责任，并通过数学公式和模拟验证该框架在多个领域的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架在处理不确定性决策时存在局限，通常仅将其视为行动约束。本文旨在开发一个能够建模道德责任如何随智能体认知状态变化的新框架，为可审计的AI决策系统提供理论基础。

Method: 提出比例责任原则（PPD）框架，建立数学方程 D_total = K[(1-HI) + HI * g(C_signal)] 描述总责任与知识、不确定性和情境信号强度的关系。使用蒙特卡洛模拟验证框架稳定性，并在临床伦理、受赠人权利法、经济治理和人工智能四个领域进行应用验证。

Result: 模拟显示保持基线谦逊系数（λ > 0）的系统能产生更稳定的责任分配，降低过度自信决策风险。该框架在四个应用领域均表现出跨学科有效性，证明比例责任可作为复杂系统中的稳定原则。

Conclusion: 比例责任原则提供了一个数学可处理的道德责任框架，通过将谦逊作为系统参数，动态平衡认知信心与情境风险，防止决策中的过度行动和疏忽，为可审计AI决策系统的发展提供了新思路。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [18] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: AI流行病学框架通过应用群体监测方法分析AI输出，实现AI系统治理和解释，无需理解复杂模型内部机制


<details>
  <summary>Details</summary>
Motivation: 解决当前可解释性方法（如SHAP和机制可解释性）在面对大规模部署模型时的模型复杂性问题，提供一种实用的AI治理框架

Method: 通过标准化捕获AI-专家交互为结构化评估字段（风险等级、对齐分数、准确度分数），将这些作为暴露变量，通过统计关联预测输出失败，类似于流行病学中胆固醇和血压预测心脏事件

Result: 提供零负担的专家参与、自动审计跟踪、治理连续性（模型更新和供应商切换时），以及可靠性评分和语义评估，使领域专家无需机器学习专业知识即可检测不可靠的AI输出

Conclusion: AI流行病学框架通过群体监测方法实现了AI系统的实用治理和解释，使领域专家能够有效监督AI系统，在模型复杂性和治理需求之间架起桥梁

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [19] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 论文引入KalshiBench基准测试评估大语言模型对未来事件的校准能力，发现所有前沿模型都存在系统性过度自信问题，即使推理增强模型也表现更差。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在各种任务上表现出色，但其认知校准能力仍未被充分理解。现有基准主要评估静态知识准确性，缺乏对模型在真正未知未来事件上量化不确定性能力的评估。

Method: 引入KalshiBench基准，包含300个来自受监管交易所Kalshi的预测市场问题，这些问题的真实结果发生在模型训练截止之后。评估了五个前沿模型（Claude Opus 4.5、GPT-5.2、DeepSeek-V3.2、Qwen3-235B、Kimi-K2）的校准性能。

Result: 所有模型都表现出系统性过度自信。最佳校准模型（Claude Opus 4.5）仍有显著校准误差（ECE=0.120），而推理增强模型如GPT-5.2-XHigh校准更差（ECE=0.395）。只有一个模型获得正Brier技能分数，表明大多数模型表现不如简单预测基准率。

Conclusion: 模型规模和推理增强并不能自动带来校准改进，认知校准是一种需要针对性开发的独立能力。这凸显了在不确定性量化方面需要专门的研究和开发。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [20] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: AMUSE是一个评估多模态大语言模型在多人对话场景中代理推理能力的基准，RAFT是一个数据高效的代理对齐框架，能显著提升模型在音频-视频理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（如GPT-4o、Qwen3-Omni）虽然在感知方面表现出色，但在多人对话场景中表现不佳，这些场景需要代理推理能力来跟踪说话者、维持角色关系以及跨时间锚定事件。这些能力对于对话式视频助手和会议分析等应用至关重要。

Method: 提出了AMUSE基准，围绕需要代理推理的任务设计，评估模型在三种模式（零样本、引导式、代理式）和六个任务族上的表现。同时提出了RAFT框架，这是一个数据高效的代理对齐框架，结合了奖励优化、内在多模态自评估作为奖励以及选择性参数适应。

Result: 当前模型在所有评估模式下都表现出较弱的多人对话推理能力和不一致的行为。使用RAFT框架后，在基准测试中实现了高达39.52%的相对准确率提升。

Conclusion: AMUSE和RAF共同为研究多模态模型中的代理推理能力提供了实用平台，并有效提升了模型在音频-视频理解任务上的性能，特别是在需要复杂代理推理的多人对话场景中。

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [21] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 提出了一种新的元启发式优化算法PCIA，受人类构建和使用路径的启发，在路径优化问题上表现出色


<details>
  <summary>Details</summary>
Motivation: 受人类路径构建行为的启发，人类通常偏好热门交通路线，在路径封闭时会智能地混合现有路径构建新路线，并且会随机选择不同路径探索未知目的地

Method: PCIA模仿人类路径构建行为：1) 生成随机种群寻找最佳路径（类似群体算法）；2) 每个粒子代表一条通往目的地的路径；3) 结合热门路径偏好、路径混合和随机探索机制

Result: 在53个数学优化问题和13个约束优化问题上测试，PCIA与流行和最新的元启发式算法相比具有高度竞争力

Conclusion: PCIA是一种有效的元启发式优化算法，受人类路径构建行为的启发，在多种优化问题上表现出优越性能

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [22] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协调合作实现，而非单一AGI系统。作者呼吁重视这一假说并开发相应的安全框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AGI系统的安全保障，忽视了多个子AGI智能体通过协调合作实现通用智能的可能性。随着具备工具使用、通信协调能力的AI智能体快速部署，这种"拼凑式AGI"带来的安全风险需要紧急关注。

Method: 提出分布式AGI安全框架，核心是设计和实施虚拟智能体沙盒经济系统（不可渗透或半渗透），通过强大的市场机制管理智能体间交易，配合适当的可审计性、声誉管理和监督机制来缓解集体风险。

Result: 论文提出了一个超越单个智能体评估和对齐的分布式AGI安全框架，强调需要通过市场机制、审计和声誉系统来管理多个智能体协调合作带来的集体风险。

Conclusion: "拼凑式AGI"假说需要认真考虑，相应的安全框架应关注智能体群体的协调风险，通过沙盒经济系统和市场治理机制来确保分布式AGI的安全性。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: SocialStoryFrames：一种基于对话上下文和叙事理论的读者响应形式化框架，包含生成和分类模型，用于大规模分析社交媒体故事中的叙事意图和读者反应。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以捕捉读者对故事的丰富解释性、情感性和评价性反应，如对叙事意图的推断或对人物的判断，限制了叙事的细致分析。

Method: 提出SocialStoryFrames形式化框架，基于叙事理论、语言语用学和心理学构建分类法，开发SSF-Generator和SSF-Classifier两个模型，使用SSF-Corpus数据集（6,140个社交媒体故事）进行验证和分析。

Result: 通过382名参与者的人类调查和专家标注验证了模型有效性，分析了不同社区中叙事意图的频率和相互依赖性，比较了跨社区的叙事实践多样性。

Conclusion: SocialStoryFrames通过将细粒度、上下文敏感的建模与通用的读者响应分类法相结合，为在线社区中的叙事研究提供了新工具，支持大规模故事分析。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [24] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 基于Qwen2.5-7B模型，通过LoRA微调和提示工程，开发了从警方通报中提取15个关键字段的领域自适应信息抽取管道，在死亡检测准确率超过98.36%，死亡人数和省级位置提取的精确匹配率分别达到95.31%和95.54%。


<details>
  <summary>Details</summary>
Motivation: 警方通报等非结构化文本在社交媒体上发布时存在噪声大、格式不统一的问题，传统方法难以有效提取结构化信息，影响数据处理的及时性和准确性。

Method: 采用领域自适应抽取管道，结合针对性提示工程和参数高效微调技术（LoRA），对Qwen2.5-7B模型进行微调，处理从27,822条中国微博警方通报（2019-2020年）中构建的4,933条高质量人工标注数据集。

Result: LoRA微调显著优于基模型和指令微调模型，死亡检测准确率超过98.36%，死亡人数精确匹配率95.31%，省级位置提取精确匹配率95.54%，能够可靠提取15个关键字段。

Conclusion: 该管道为专业领域的多任务结构化信息抽取提供了经过验证的高效解决方案，为社会科学研究中将非结构化文本转化为可靠结构化数据提供了实用框架。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation](https://arxiv.org/abs/2512.16302)
*Zixuan Chen,Chongkai Gao,Lin Shao,Jieqi Shi,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: ManiLong-Shot：一种通过物理交互事件分解长视野任务、使用VLM或规则启发式驱动、预测关键不变区域的新框架，实现单次模仿学习在长视野操作任务上的有效应用。


<details>
  <summary>Details</summary>
Motivation: 当前单次模仿学习方法主要局限于短视野任务，限制了其在复杂长视野操作任务中的应用。需要一种能够处理长视野任务的单次模仿学习框架。

Method: ManiLong-Shot将长视野任务围绕物理交互事件进行结构化分解，将问题重构为序列化交互感知基元而非直接模仿连续轨迹。基元分解可由视觉语言模型的高层推理或基于机器人状态变化的规则启发式驱动。对每个基元，预测交互关键的不变区域，建立演示与当前观察之间的对应关系，并计算机械臂末端目标位姿。

Result: 在仅使用10个短视野任务训练后，ManiLong-Shot在3个难度级别的20个未见长视野任务上通过单次模仿实现了泛化，相比SOTA方法获得22.8%的相对提升。真实机器人实验验证了其在3个长视野操作任务上通过OSIL的鲁棒执行能力。

Conclusion: ManiLong-Shot通过基于交互事件的基元分解方法，成功扩展了单次模仿学习在长视野操作任务中的应用，在仿真和真实环境中均表现出色，具有实际应用价值。

Abstract: One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.

</details>
