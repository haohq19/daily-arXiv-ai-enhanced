<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos](https://arxiv.org/abs/2508.15903)
*Kaining Li,Shuwei He,Zihan Xu*

Main category: cs.CV

TL;DR: VT-LVLM-AR框架通过视频到事件映射器和LVLM动作推理模块，在长视频动作识别中实现了最先进的性能表现


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型在长视频动作识别中计算开销大、难以捕捉长程时序依赖和语义理解有限的问题，探索LVLM在连续视频流细粒度动作识别中的应用

Method: 使用Video-to-Event Mapper将原始视频转换为紧凑的视觉事件序列，然后通过参数高效的Prompt Tuning适配冻结的LLaVA-1.5模型进行动作分类

Result: 在NTU RGB+D和NTU RGB+D 120数据集上达到最先进性能（NTU RGB+D X-Sub准确率94.1%），消融研究验证了各组件的重要性

Conclusion: 该工作展示了通过有效的视频到语言转换和高效模型适配，利用LVLM实现鲁棒且可解释的视频动作理解的巨大潜力

Abstract: Human action recognition in long-term videos, characterized by complex
backgrounds and subtle action differences, poses significant challenges for
traditional deep learning models due to computational overhead, difficulty in
capturing long-range temporal dependencies, and limited semantic understanding.
While Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
have shown remarkable capabilities in multi-modal understanding and reasoning,
their direct application to continuous video streams for fine-grained action
recognition remains an open problem. This paper introduces VT-LVLM-AR
(Video-Temporal Large Vision-Language Model Adapter for Action Recognition), a
novel framework designed to bridge this gap. VT-LVLM-AR comprises a
Video-to-Event Mapper (VTEM) that efficiently transforms raw video into
compact, semantically rich, and temporally coherent "visual event sequences"
through lightweight spatio-temporal feature extraction, adaptive temporal
pooling, and conceptual quantization with an event coherence bias. These visual
event sequences are then fed into an LVLM-based Action Reasoning module,
specifically a frozen LLaVA-1.5 model, adapted using parameter-efficient Prompt
Tuning (P-Tuning v2) for action classification. Comprehensive evaluations on
the NTU RGB+D and NTU RGB+D 120 datasets demonstrate that VT-LVLM-AR
consistently achieves state-of-the-art performance, surpassing existing methods
(e.g., 94.1% accuracy on NTU RGB+D X-Sub). Ablation studies confirm the
critical contributions of VTEM's components and the efficacy of Prompt Tuning,
while human evaluations underscore the interpretability of our visual event
representations. This work highlights the immense potential of leveraging LVLMs
for robust and interpretable video action understanding through effective
video-to-language translation and efficient model adaptation.

</details>


### [2] [Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection](https://arxiv.org/abs/2508.16157)
*Pi-Wei Chen,Jerry Chun-Wei Lin,Wei-Han Chen,Jia Ji,Zih-Ching Chen,Feng-Hao Yeh,Chao-Chun Chen*

Main category: cs.CV

TL;DR: APT是一种无需先验知识的少样本异常检测框架，通过自适应提示调优和语义对齐来克服传统基于提示方法的局限性


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型异常检测方法依赖于人工设计的提示词且缺乏异常样本，导致在特定上下文异常理解方面存在显著差距

Method: 使用噪声扰动生成的自异常样本来训练可学习提示词，并提出了自优化元提示引导方案(SMGS)来防止对合成噪声的过拟合

Result: 在多个基准数据集上实现了最先进的性能，无需先验知识进行提示词设计

Conclusion: APT为现实世界的异常检测提供了一个强大且通用的解决方案，不仅推进了像素级异常检测，还建立了无需先验知识的稳健框架

Abstract: Pre-trained Vision-Language Models (VLMs) have recently shown promise in
detecting anomalies. However, previous approaches are fundamentally limited by
their reliance on human-designed prompts and the lack of accessible anomaly
samples, leading to significant gaps in context-specific anomaly understanding.
In this paper, we propose \textbf{A}daptive \textbf{P}rompt \textbf{T}uning
with semantic alignment for anomaly detection (APT), a groundbreaking prior
knowledge-free, few-shot framework and overcomes the limitations of traditional
prompt-based approaches. APT uses self-generated anomaly samples with noise
perturbations to train learnable prompts that capture context-dependent
anomalies in different scenarios. To prevent overfitting to synthetic noise, we
propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively
aligns the prompts with general anomaly semantics while incorporating diverse
synthetic anomaly. Our system not only advances pixel-wise anomaly detection,
but also achieves state-of-the-art performance on multiple benchmark datasets
without requiring prior knowledge for prompt crafting, establishing a robust
and versatile solution for real-world anomaly detection.

</details>


### [3] [RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution](https://arxiv.org/abs/2508.16158)
*Haodong He,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu,Gui-Song Xia*

Main category: cs.CV

TL;DR: 本文提出RAGSR方法，通过区域注意力机制提升多对象场景下的超分辨率效果，解决了现有方法在细粒度区域细节生成上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型组合文本到图像渗透模型的方法在单图超分辨率中表现优异，但在多对象场景下生成清晰准确的区域细节时仍面临挑战，主要因为缺乏细粒度区域描述和模型对复杂提示的理解能力不足。

Method: 提出区域注意导向超分辨率(RAGSR)方法：1)定位图像中的物体区域并为每个区域分配细粒度标题，形成区域-文本对作为T2I模型的文本先验知识；2)采用新的区域导向注意机制，确保每个区域-文本对在注意过程中得到适当考虑，同时防止不相关区域间的不应有交互。

Result: 在标准数据集上的实验结果显示，该方法在生成感知真实的视觉细节时表现出优异性能，同时保持了上下文一致性，超过了现有方法。

Conclusion: RAGSR通过明确提取局部细粒度信息并通过区域注意机制进行有效编码，能够同时提升细节和整体视觉协调性，有效克服了传统SISR技术的局限性。

Abstract: The rich textual information of large vision-language models (VLMs) combined
with the powerful generative prior of pre-trained text-to-image (T2I) diffusion
models has achieved impressive performance in single-image super-resolution
(SISR). However, existing methods still face significant challenges in
generating clear and accurate regional details, particularly in scenarios
involving multiple objects. This challenge primarily stems from a lack of
fine-grained regional descriptions and the models' insufficient ability to
capture complex prompts. To address these limitations, we propose a Regional
Attention Guided Super-Resolution (RAGSR) method that explicitly extracts
localized fine-grained information and effectively encodes it through a novel
regional attention mechanism, enabling both enhanced detail and overall
visually coherent SR results. Specifically, RAGSR localizes object regions in
an image and assigns fine-grained caption to each region, which are formatted
as region-text pairs as textual priors for T2I models. A regional guided
attention is then leveraged to ensure that each region-text pair is properly
considered in the attention process while preventing unwanted interactions
between unrelated region-text pairs. By leveraging this attention mechanism,
our approach offers finer control over the integration of text and image
information, thereby effectively overcoming limitations faced by traditional
SISR techniques. Experimental results on benchmark datasets demonstrate that
our approach exhibits superior performance in generating perceptually authentic
visual details while maintaining contextual consistency compared to existing
approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net是一个新颖的多模态情感分析深度学习框架，通过渐进式门控融合、自适应门控仲裁和参数高效微调策略，实现了高效且可解释的多模态情感分析，在MOSI数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态情感分析中深度、上下文相关的特征融合问题，同时保持模型的参数效率和可解释性，需要开发一种能够动态平衡不同模态信息并防止噪声干扰的融合框架。

Method: 提出了渐进式层内融合范式（Cross-Attention机制）、自适应门控仲裁机制（动态控制器）和混合参数高效微调策略（LoRA+Post-Fusion Adapters），集成到分层编码器架构中。

Result: 在MOSI数据集上取得MAE 0.691和F1-Score 86.9%的SOTA性能，仅使用3.09M可训练参数，展现了优异的性能与计算效率平衡。

Conclusion: PGF-Net通过创新的融合机制和参数高效设计，成功实现了深度、动态且可解释的多模态情感分析，为资源受限场景提供了有效的解决方案。

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [5] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 提出了解决平衡传播(EP)在深层网络中梯度消失问题的新框架，通过引入中间误差信号和知识蒸馏，首次实现了深层架构的训练，在CIFAR数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 平衡传播(EP)作为生物启发的局部学习规则，虽然计算效率高且适合神经形态架构，但先前研究仅限于浅层架构，深层网络存在梯度消失问题，导致能量最小化和梯度计算收敛困难

Method: 提出新颖的EP框架，整合中间误差信号来增强信息流和神经元动力学收敛，首次将知识蒸馏和局部误差信号集成到EP中

Result: 在CIFAR-10和CIFAR-100数据集上实现了最先进的性能，展示了在深度VGG架构上的可扩展性

Conclusion: 这项工作显著推进了EP的可扩展性，为其在现实世界系统中的应用铺平了道路

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [6] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM-VAE的无监督在线学习框架，用于检测供水管网中的管道堵塞（集体异常）和背景泄漏（概念漂移），在非平稳条件下实现鲁棒检测和自适应。


<details>
  <summary>Details</summary>
Motivation: 供水管网对公共福祉和经济稳定至关重要，但面临管道堵塞和背景泄漏等挑战，且存在数据非平稳性和标记数据有限等操作约束。

Method: 结合长短期记忆变分自编码器（LSTM-VAE）与双重漂移检测机制，采用轻量级、内存高效的设计实现实时边缘级监控。

Result: 在两个现实供水管网上的实验表明，该方法在检测异常和适应循环漂移方面 consistently 优于强基线方法。

Conclusion: 该方法在动态供水管网环境中进行无监督事件检测方面表现出有效性，特别适用于非平稳条件下的故障检测。

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [7] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin Müller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: 提出了Doctor方法，通过双重检查机制改进离线强化学习中的目标回报对齐问题，实现更精确的策略性能控制


<details>
  <summary>Details</summary>
Motivation: 现有RvS方法如Decision Transformer在目标回报对齐方面存在不足，特别是在数据集内插值和外推时难以可靠地实现指定回报

Method: Doctor方法采用双重检查机制，通过目标对齐来监督Transformer，确保实际回报与指定目标回报精确匹配

Result: 在动态治疗机制基准测试EpiCare上，该方法有效调节治疗策略的激进程度，平衡治疗效果与不良事件风险

Conclusion: Doctor方法显著提升了离线强化学习中目标回报对齐的准确性和灵活性，为实际应用提供了更精确的性能控制能力

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: GDP是一个多模态基础模型，通过CNN-Transformer编码器处理结构化EHR时间序列数据，并与非结构化临床笔记融合，在临床预测和叙事生成任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHRs)包含丰富的多模态临床数据，但现有方法通常将数值数据序列化为文本，会丢失时间和定量细节信息。需要开发能够原生处理多模态EHR数据的基础模型。

Method: 提出Generative Deep Patient (GDP)模型：1) 使用CNN-Transformer编码器原生编码结构化EHR时间序列；2) 通过跨模态注意力与无结构EHR融合；3) 基于LLaMA的解码器；4) 两阶段训练：生成式预训练（临床叙事生成+掩码特征预测+下一时间步预测）和多任务微调。

Result: 在MIMIC-IV数据集上：心衰预测AUROC=0.923，2型糖尿病AUROC=0.817，30天再入院AUROC=0.627；叙事生成ROUGE-L=0.135，BERTScore-F1=0.545；人工评估显示在忠实性、流畅性和临床实用性方面得分最高。

Conclusion: 单一多模态基础模型能够同时预测临床可操作事件并生成高质量临床叙事，GDP的灵活架构可扩展到其他模态，有望减少医院文档工作量而不牺牲准确性。

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [9] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: 提出了MSEF框架，通过多层可引导嵌入融合技术，让大语言模型能够在所有深度直接访问时间序列模式，解决了现有方法中时间序列信息在深层逐渐消失的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间序列信息主要集成在输入层浅层，导致时间序列表示在深层逐渐消失，造成文本嵌入和时间序列表示之间的无效适配

Method: 利用现成的时间序列基础模型提取语义丰富的嵌入，通过层特定的引导向量与LLM中间文本表示进行融合，持续优化时间序列和文本模态的对齐

Result: 在7个基准测试中相比基线方法平均MSE降低31.8%，表现出显著的性能提升

Conclusion: MSEF框架有效解决了时间序列信息在深层消失的问题，实现了高效的小样本学习能力，为LLM在时间序列预测中的应用提供了新思路

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions](https://arxiv.org/abs/2508.15801)
*Seyedali Mohammadi,Manas Paldhe,Amit Chhabra*

Main category: cs.CL

TL;DR: LingVarBench是一个合成数据生成管道，通过自动验证生成包含电话对话特征的合成对话数据，并利用DSPy的SIMBA优化器自动合成提取提示，显著提高了从真实客户通话记录中提取结构化信息的准确性。


<details>
  <summary>Details</summary>
Motivation: 电话通话记录标注成本极高（约2美元/分钟），且现有提取方法在处理包含不流利、打断和说话人重叠的对话语音时效果不佳。隐私法规和手动标注成本阻碍了大规模电话通话分析。

Method: 1. 使用LLM生成多个用例的现实结构化字段值；2. 递归提示模型将这些值转换为包含典型电话通话特征的自然对话语句；3. 通过测试单独的基于LLM的提取器是否能恢复原始结构化信息来验证每个合成语句；4. 使用DSPy的SIMBA优化器从验证过的合成记录中自动合成提取提示。

Result: 优化后的提示在真实客户记录中达到：数字字段95%准确率（零样本为88-89%）、姓名90%准确率（零样本为47-79%）、日期超过80%准确率（零样本为72-77%）。合成到真实的迁移表明从生成数据中学到的对话模式能有效泛化到包含背景噪声和领域特定术语的真实电话通话。

Conclusion: LingVarBench为从合成对话数据中进行结构化提取提供了首个系统性基准，证明自动提示优化克服了商业环境中大规模电话通话分析的成本和隐私障碍。

Abstract: Phone call transcript labeling is prohibitively expensive (approximately 2
USD per minute) due to privacy regulations, consent requirements, and manual
annotation costs requiring 3 hours of expert time per hour of audio. Existing
extraction methods fail on conversational speech containing disfluencies,
interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data
generation pipeline that addresses these constraints through automated
validation. First, we prompt an LLM to generate realistic structured field
values across multiple use cases. Second, we recursively prompt the model to
transform these values into thousands of natural conversational utterances
containing typical phone call characteristics. Third, we validate each
synthetic utterance by testing whether a separate LLM-based extractor can
recover the original structured information. We employ DSPy's SIMBA optimizer
to automatically synthesize extraction prompts from validated synthetic
transcripts, eliminating manual prompt engineering. Our optimized prompts
achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent
zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for
dates (vs. 72-77 percent) on real customer transcripts, demonstrating
substantial gains over zero-shot prompting. The synthetic-to-real transfer
demonstrates that conversational patterns learned from generated data
generalize effectively to authentic phone calls containing background noise and
domain-specific terminology. LingVarBench provides the first systematic
benchmark for structured extraction from synthetic conversational data,
demonstrating that automated prompt optimization overcomes cost and privacy
barriers preventing large-scale phone call analysis in commercial settings.

</details>


### [11] [MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr](https://arxiv.org/abs/2508.15853)
*Xuwen Yang*

Main category: cs.CL

TL;DR: 提出MGSC框架，通过同时正则化宏观句子语义和微观词元对齐来增强ASR模型的内部一致性，显著减少噪声环境下的语义错误


<details>
  <summary>Details</summary>
Motivation: 端到端ASR模型在噪声环境中容易产生灾难性语义错误，现有直接映射目标只惩罚最终输出错误，缺乏对内部计算过程的约束

Method: 引入多粒度软一致性(MGSC)框架，同时正则化宏观句子语义和微观词元对齐，发现两种粒度一致性联合优化的协同效应

Result: 在公开数据集上，MGSC在不同噪声条件下平均字符错误率相对降低8.7%，主要防止了严重的语义改变错误

Conclusion: 强制内部一致性是构建更鲁棒和可信AI的关键步骤，多粒度一致性优化具有强大的协同效应

Abstract: End-to-end ASR models, despite their success on benchmarks, often pro-duce
catastrophic semantic errors in noisy environments. We attribute this fragility
to the prevailing 'direct mapping' objective, which solely penalizes final
output errors while leaving the model's internal computational pro-cess
unconstrained. To address this, we introduce the Multi-Granularity Soft
Consistency (MGSC) framework, a model-agnostic, plug-and-play module that
enforces internal self-consistency by simultaneously regulariz-ing macro-level
sentence semantics and micro-level token alignment. Cru-cially, our work is the
first to uncover a powerful synergy between these two consistency
granularities: their joint optimization yields robustness gains that
significantly surpass the sum of their individual contributions. On a public
dataset, MGSC reduces the average Character Error Rate by a relative 8.7%
across diverse noise conditions, primarily by preventing se-vere
meaning-altering mistakes. Our work demonstrates that enforcing in-ternal
consistency is a crucial step towards building more robust and trust-worthy AI.

</details>


### [12] [LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining](https://arxiv.org/abs/2508.16270)
*Vira Pyrih,Adrian Rebmann,Han van der Aa*

Main category: cs.CL

TL;DR: 本文研究了指令微调在语义感知过程挖掘中的潜力，发现其对不同任务的影响存在差异：在过程发现和预测任务上性能显著提升，但在异常检测任务上的效果因模型而异


<details>
  <summary>Details</summary>
Motivation: 传统过程挖掘主要基于频率分析，而语义感知过程挖掘关注过程应有的行为期望。虽然大型语言模型(LLMs)能处理语义感知任务，但任务特定的微调计算成本高且缺乏泛化能力

Method: 采用指令微调方法，将LLM暴露于不同过程挖掘任务（如异常检测和下一活动预测）的提示-答案对，使其更熟悉过程挖掘领域，从而提升在未见任务（如过程发现）上的性能

Result: 指令微调对过程发现和预测任务的性能有显著提升，但在异常检测任务上的效果因模型选择而有所不同，表明指令微调任务的选择对最终结果至关重要

Conclusion: 指令微调是提升LLM在语义感知过程挖掘中泛化能力的有效方法，但需要仔细选择用于微调的任务组合以获得最佳性能

Abstract: Process mining is increasingly using textual information associated with
events to tackle tasks such as anomaly detection and process discovery. Such
semantics-aware process mining focuses on what behavior should be possible in a
process (i.e., expectations), thus providing an important complement to
traditional, frequency-based techniques that focus on recorded behavior (i.e.,
reality). Large Language Models (LLMs) provide a powerful means for tackling
semantics-aware tasks. However, the best performance is so far achieved through
task-specific fine-tuning, which is computationally intensive and results in
models that can only handle one specific task. To overcome this lack of
generalization, we use this paper to investigate the potential of
instruction-tuning for semantics-aware process mining. The idea of
instruction-tuning here is to expose an LLM to prompt-answer pairs for
different tasks, e.g., anomaly detection and next-activity prediction, making
it more familiar with process mining, thus allowing it to also perform better
at unseen tasks, such as process discovery. Our findings demonstrate a varied
impact of instruction-tuning: while performance considerably improved on
process discovery and prediction tasks, it varies across models on anomaly
detection tasks, highlighting that the selection of tasks for
instruction-tuning is critical to achieving desired outcomes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [13] [Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems](https://arxiv.org/abs/2508.16574)
*Yizhi Wang,Degang Xu,Yongfang Xie,Shuzhong Tan,Xianan Zhou,Peng Chen*

Main category: cs.RO

TL;DR: 提出了一种用于四轮独立转向驱动系统的分层决策框架，结合深度强化学习和模糊逻辑控制，实现高效稳定的自主导航


<details>
  <summary>Details</summary>
Motivation: 解决4WISD系统在复杂环境中导航时既要保证任务性能又要满足物理可行性的挑战，避免纯DRL方法可能产生的机械应变和车轮打滑问题

Method: 分层框架：高层使用深度强化学习生成全局运动指令，低层使用模糊逻辑控制器强制执行运动学约束

Result: 仿真实验显示该方法优于传统导航方法，训练效率更高、稳定性更好，减少了异常行为；真实环境验证证实了在动态工业场景中的安全有效导航能力

Conclusion: 该工作为4WISD移动机器人在复杂真实场景中的部署提供了可扩展且可靠的解决方案

Abstract: This paper presents a hierarchical decision-making framework for autonomous
navigation in four-wheel independent steering and driving (4WISD) systems. The
proposed approach integrates deep reinforcement learning (DRL) for high-level
navigation with fuzzy logic for low-level control to ensure both task
performance and physical feasibility. The DRL agent generates global motion
commands, while the fuzzy logic controller enforces kinematic constraints to
prevent mechanical strain and wheel slippage. Simulation experiments
demonstrate that the proposed framework outperforms traditional navigation
methods, offering enhanced training efficiency and stability and mitigating
erratic behaviors compared to purely DRL-based solutions. Real-world
validations further confirm the framework's ability to navigate safely and
effectively in dynamic industrial settings. Overall, this work provides a
scalable and reliable solution for deploying 4WISD mobile robots in complex,
real-world scenarios.

</details>
