<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 提出基于传感器物理的NeRF框架，从单曝光模糊LDR图像和事件数据合成锐利HDR新视角，通过两个映射场对齐渲染值与传感器记录值。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用事件数据解决LDR模糊图像的HDR新视角合成问题，但忽略了相机输出与物理世界辐射之间的传感器物理不匹配，导致HDR和去模糊效果不佳。

Method: 提出统一传感器物理基础的NeRF框架：1) 用NeRF直接表示HDR域中的实际场景辐射；2) 引入像素级RGB映射场对齐渲染值与传感器记录的LDR像素值；3) 设计事件映射场桥接物理场景动态与事件传感器输出；4) 两个映射场与NeRF网络联合优化。

Result: 在收集和公开数据集上的实验表明，该方法能够从单曝光模糊LDR图像和对应事件数据实现最先进的去模糊HDR新视角合成结果。

Conclusion: 通过传感器物理建模和两个映射场的联合优化，成功解决了LDR模糊图像到HDR锐利3D表示的转换问题，实现了高质量的HDR新视角合成。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [2] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream：基于事件感知的视频流理解框架，通过检测语义连贯的事件边界来减少冗余帧处理，提升长视频实时理解能力


<details>
  <summary>Details</summary>
Motivation: 现有视频流理解系统存在两个主要问题：1）固定间隔解码会产生重复输出；2）缓存剪枝会丢弃关键时序信息。这导致多模态大语言模型在处理长视频流时存在冗余帧处理和快速遗忘过去上下文的问题。

Method: 提出事件感知框架，将连续视频表示为离散的语义连贯事件序列。系统通过整合运动、语义和预测线索来检测有意义的状态转换，仅在事件边界处触发语言生成。每个事件嵌入被整合到持久记忆库中，支持长时程推理。

Result: 在OVOBench-Realtime上比VideoLLM-Online-8B基线提升10.4分；使用通用LLaMA-3-8B文本骨干就能达到接近Flash-VStream-7B的性能；在2小时Ego4D流上保持约70%的GPT-5胜率。

Conclusion: Event-VStream通过事件感知的框架有效解决了长视频流理解中的冗余处理和遗忘问题，在保持低延迟的同时实现了竞争性的性能表现。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice是一个混合序列预测系统，通过二元置信度门控有条件地激活学习到的行为结构，在推荐系统、科学时间序列和金融市场中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决在序列预测中如何智能地利用学习到的行为模式，同时避免在不确定或分布偏移时错误激活的问题，需要一种能够管理认知不确定性的架构原则。

Method: 系统将行为窗口聚类为行为原型，使用二元置信度门控机制：当置信度超过阈值时激活基于原型的评分，否则回退到基线预测。在MovieLens、LIGO和金融市场数据上使用LSTM和Transformer骨干网络进行验证。

Result: 在MovieLens上使用LSTM时，Lattice比LSTM基线在HR@10上提升31.9%，比SASRec和BERT4Rec分别提升109.4%和218.6%。在LIGO和金融数据上，系统能正确拒绝原型激活以防止分布偏移时的错误激活。在Transformer骨干网络上，Lattice保持中性（0.0%改进，无性能下降）。

Conclusion: 双向验证表明：在模式适用时激活，不适用时拒绝，冗余时优雅推迟，这支持置信度门控作为管理安全关键应用中认知不确定性的有前景的架构原则。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [4] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther是一个PyTorch兼容的RandNLA库，通过高效的随机化算法压缩深度学习模型，显著减少内存使用（BERT上可达75%），同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型训练受GPU内存和计算限制，RandNLA技术虽然有效但缺乏统一的生产级库，阻碍了这些方法的广泛应用。

Method: 开发Panther库，整合成熟的RandNLA算法，提供优化的C++/CUDA后端(pawX)，实现线性层、2D卷积、多头注意力等标准组件的随机化替代方案。

Result: 仅需几行代码替换PyTorch标准层，在BERT上实现高达75%的内存节省，同时保持可比较的损失性能。

Conclusion: Panther成功展示了RandNLA技术的有效性，提供了一个易于采用的高性能框架，有助于缓解深度学习模型训练中的内存和计算约束。

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [5] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++ 通过引入两种漂移检测机制（基于熵和KL散度）和自适应重置策略，改进了持续测试时适应方法，在长时域分布变化下保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法（如Tent、EATA）在短期分布变化下表现良好，但在快速变化或极长时域（如CCC基准测试中的750万样本流）下容易失效，需要更鲁棒的适应机制。

Method: 提出RDumb++，扩展RDumb方法，引入两种漂移检测机制：1）基于熵的漂移评分，2）KL散度漂移评分，结合自适应重置策略，在检测到有害累积适应时及时恢复模型。

Result: 在CCC-medium基准测试（三种速度、三种随机种子，共9次运行，每次包含100万样本）中，RDumb++相比RDumb获得约3%的绝对准确率提升，并在整个流中保持稳定适应。

Conclusion: 漂移感知的重置机制对于防止模型崩溃和实现可靠的长期CTTA至关重要，RDumb++通过智能漂移检测和自适应恢复策略，在持续变化环境中保持了鲁棒性能。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [6] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 提出了首个从部分观测数据学习神经算子的系统框架，解决了实际应用中空间数据不完整的问题，在多种PDE任务上实现了显著的误差降低。


<details>
  <summary>Details</summary>
Motivation: 现实科学应用中经常遇到观测数据不完整的问题，而现有神经算子方法假设完全观测的空间输入，严重限制了在实际应用中的适用性。

Method: 提出了Latent Autoregressive Neural Operator (LARNO)，包含两个核心组件：掩码预测训练策略（通过策略性掩码观测区域创建人工监督）和物理感知潜在传播器（在潜在空间通过边界优先自回归生成重建解）。

Result: 在POBench-PDE基准测试中，LARNO在补丁式缺失率低于50%的情况下，实现了18-69%的相对L2误差降低，包括真实世界气候预测，并能处理高达75%的缺失率。

Conclusion: 该框架有效解决了部分观测的核心挑战，在一定程度上弥合了理想化研究设置与现实世界科学计算复杂性之间的差距。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [7] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 提出一种后训练统计准则和结构退火方法，通过移除未支持的参数依赖来揭示稳定、独立的子结构，实现无需修改模型功能的结构化并行推理。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的推理通常基于稠密参数矩阵，导致推理成本和系统复杂度随模型规模不可持续地增长。这种限制并非源于模型容量不足，而是由于将后训练推理系统视为整体算子而忽略了学习过程中形成的内部结构。

Method: 1. 观察到大型模型中的梯度更新事件高度局部化和选择性，许多参数依赖在训练后与其初始化分布在统计上无法区分；2. 提出后训练统计准则和结构退火程序，移除未支持的依赖关系，揭示稳定、独立的子结构。

Result: 建立了后训练、模型无关的推理系统结构视图，实现了结构化并行推理，无需修改模型功能或接口。

Conclusion: 后训练推理系统本质上是结构非均匀且可分解的，通过统计分析和结构优化可以显著降低推理成本，同时保持模型功能不变。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [8] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 该论文将可解释AI（XAI）应用于工业信息物理系统（CPS）中的机器学习模型，通过SHAP值分析时间序列数据分解组件对预测的影响，发现训练数据上下文信息不足，并通过增加数据窗口大小来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统（CPS）在安全和经济效益方面都是敏感基础设施，其可靠性至关重要。机器学习（特别是深度学习）越来越多地集成到工业CPS中，但ML模型的固有复杂性导致其操作不透明。需要严格评估以防止模型在未来未见数据上表现出意外行为。

Method: 应用可解释AI（XAI）来揭示模型推理过程，使用SHAP值分析时间序列数据分解组件对模型预测的影响。通过XAI发现训练数据上下文信息不足的问题，并基于此发现增加数据实例的窗口大小。

Result: 通过XAI分析发现了模型训练中上下文信息不足的证据。基于XAI的发现，通过增加数据窗口大小，成功提高了模型性能。

Conclusion: 可解释AI（XAI）能够有效揭示工业CPS中ML模型的推理过程，识别训练数据的局限性，并通过基于XAI发现的改进措施（如增加数据窗口大小）来提升模型预测性能。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出双过程代理不确定性量化框架，将语言化不确定性转化为主动控制信号，解决AI代理在长程推理中的"幻觉螺旋"问题


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两难：不确定性量化方法只能被动诊断风险而不解决问题，而自我反思机制则存在持续或漫无目的的修正问题。需要解决早期认知错误不可逆传播的"幻觉螺旋"问题

Method: 提出双过程代理不确定性量化框架，包含两个互补机制：系统1（不确定性感知记忆）隐式传播语言化置信度和语义解释；系统2（不确定性感知反思）利用这些解释作为理性线索，仅在必要时触发有针对性的推理时解析

Result: 在闭环基准测试和开放式深度研究任务上的广泛实验表明，这种无需训练的方法实现了优越的性能和轨迹级校准

Conclusion: 该原则性框架代表了向可靠代理迈出的重要一步，能够动态平衡高效执行和深度思考

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [10] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC是一种从零开始学习的离线策略actor-critic方法，仅需单条专家轨迹，通过sigmoid有界熵项防止负熵驱动的OOD动作优化，减少Q函数振荡，在真实世界机器人任务中实现低成本RL部署。


<details>
  <summary>Details</summary>
Motivation: 真实世界强化学习面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据或大规模预训练，缺乏低成本、小数据需求的实用解决方案。

Method: 提出SigEnt-SAC离线策略actor-critic方法，核心设计是sigmoid有界熵项，防止负熵驱动的分布外动作优化，减少Q函数振荡，仅需单条专家轨迹从零开始学习。

Result: 在D4RL基准测试中显著缓解Q函数振荡，比现有方法更快达到100%成功率。在四个真实世界机器人任务中，仅需少量真实交互就能从原始图像和稀疏奖励中学习成功策略。

Conclusion: SigEnt-SAC为真实世界RL部署提供了低成本、实用的途径，仅需最小数据需求就能在真实机器人任务中有效学习。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [11] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 提出结合视觉语言模型与外部知识检索的方法，以增强对气候虚假信息的检测能力，特别是针对训练数据中未包含的最新事件和更新内容。


<details>
  <summary>Details</summary>
Motivation: 气候虚假信息在数字世界中日益严重，特别是社交媒体上广泛传播的误导性图像和视频。现有的视觉语言模型仅依赖训练时的知识，无法处理最新事件或更新，限制了检测虚假信息的能力。

Method: 结合视觉语言模型与外部知识检索系统，通过检索最新信息（如反向图像搜索结果、在线事实核查、可信专家内容）来评估图像及其声明的准确性。

Result: 该方法提高了模型处理真实世界气候虚假信息的能力，能够更准确地判断图像和声明的真实性（准确、误导、虚假或无法验证）。

Conclusion: 通过整合外部知识，该系统能够克服视觉语言模型的知识时效性限制，更好地支持保护公众科学理解，应对快速变化的信息环境中的气候虚假信息挑战。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: 论文评估RAG架构在公共卫生政策问答中减少LLM幻觉的效果，发现高级RAG配置比基础RAG和普通LLM在准确性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: LLM在公共卫生政策领域应用时会产生幻觉（看似合理但事实错误的断言），这在信息完整性至关重要的高风险环境中构成重大障碍，需要找到有效方法来减少这种风险。

Method: 比较三种架构：普通LLM、基础RAG和高级RAG（使用交叉编码器重排序）。使用Mistral-7B-Instruct-v0.2模型和all-MiniLM-L6-v2嵌入模型处理CDC政策文件，评估两种分块策略（递归字符分割和基于标记的语义分割）对系统准确性的影响。

Result: 基础RAG在忠实度（0.621）上比普通LLM（0.347）有显著提升，高级RAG配置达到最高的忠实度平均值0.797。两阶段检索机制对领域特定政策问答的精度至关重要，但文档分割的结构限制仍然是多步推理任务的主要瓶颈。

Conclusion: RAG架构特别是高级配置能有效减少LLM在公共卫生政策问答中的幻觉，但需要进一步优化文档分割策略以支持复杂的多步推理任务。

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [13] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 提出Trajectory Replay via Concept-Basis Reconstruction框架，通过概念指纹对齐和概念原子重构，将拒绝干预从捐赠模型转移到目标模型，证明安全对齐具有语义普遍性。


<details>
  <summary>Details</summary>
Motivation: 对齐LLM中的拒绝行为通常被视为模型特定的，但作者假设它源于跨模型共享的通用低维语义电路。为了验证这一假设，需要开发能够跨不同架构和训练机制转移拒绝干预的方法。

Method: 引入Trajectory Replay via Concept-Basis Reconstruction框架：1) 通过概念指纹对齐层；2) 使用共享的"概念原子"重构拒绝方向；3) 将捐赠模型的消融轨迹映射到目标模型的语义空间；4) 引入weight-SVD稳定性保护，将干预投影到低方差权重子空间以避免能力损害。

Result: 在8个模型对（包括GPT-OSS-20B和GLM-4）上的评估表明，转移的"配方"能够一致地减弱拒绝行为，同时保持模型性能，为安全对齐的语义普遍性提供了有力证据。

Conclusion: 拒绝行为源于跨模型共享的通用低维语义电路，安全对齐具有语义普遍性。提出的框架能够有效转移拒绝干预，为理解和操作LLM的安全机制提供了新方法。

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>
