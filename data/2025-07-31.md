<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SmartCLIP: Modular Vision-language Alignment with Identification Guarantees](https://arxiv.org/abs/2507.22264)
*Shaoan Xie,Lingjing Kong,Yujia Zheng,Yu Yao,Zeyu Tang,Eric P. Xing,Guangyi Chen,Kun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法SmartCLIP，解决了CLIP模型在图像-文本对齐中的信息不对齐和表示纠缠问题，通过模块化方式实现细粒度对齐。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在图像-文本对齐中存在信息不对齐和表示纠缠问题，限制了其在某些下游任务中的泛化能力。

Method: 提出理论条件，确保跨模态语义信息的完整保留和视觉表示的细粒度解耦，并引入SmartCLIP方法进行模块化对齐。

Result: SmartCLIP在多种任务中表现优异，验证了其处理信息不对齐的能力和支持的理论。

Conclusion: SmartCLIP通过细粒度对齐和解耦表示，显著提升了CLIP模型的性能，为多模态学习提供了新思路。

Abstract: Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning}
has emerged as a pivotal model in computer vision and multimodal learning,
achieving state-of-the-art performance at aligning visual and textual
representations through contrastive learning. However, CLIP struggles with
potential information misalignment in many image-text datasets and suffers from
entangled representation. On the one hand, short captions for a single image in
datasets like MSCOCO may describe disjoint regions in the image, leaving the
model uncertain about which visual features to retain or disregard. On the
other hand, directly aligning long captions with images can lead to the
retention of entangled details, preventing the model from learning
disentangled, atomic concepts -- ultimately limiting its generalization on
certain downstream tasks involving short prompts.
  In this paper, we establish theoretical conditions that enable flexible
alignment between textual and visual representations across varying levels of
granularity. Specifically, our framework ensures that a model can not only
\emph{preserve} cross-modal semantic information in its entirety but also
\emph{disentangle} visual representations to capture fine-grained textual
concepts. Building on this foundation, we introduce \ours, a novel approach
that identifies and aligns the most relevant visual and textual representations
in a modular manner. Superior performance across various tasks demonstrates its
capability to handle information misalignment and supports our identification
theory. The code is available at https://github.com/Mid-Push/SmartCLIP.

</details>


### [2] [Gems: Group Emotion Profiling Through Multimodal Situational Understanding](https://arxiv.org/abs/2507.22393)
*Anubhav Kataria,Surbhi Madan,Shreya Ghosh,Tom Gedeon,Abhinav Dhall*

Main category: cs.CV

TL;DR: 论文提出GEMS框架，通过多模态Swin-Transformer和S3Attention架构，预测细粒度个体情绪到粗粒度群体和事件级情绪，并在VGAF-GEMS基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解个体、群体和事件级情绪及其上下文信息对分析多人社交场景至关重要。现有研究多关注原子交互或群体级情绪，缺乏细粒度分析。

Method: 采用多模态Swin-Transformer和S3Attention架构，结合场景、群体成员和上下文信息，联合预测离散和连续情绪（如效价和唤醒度）。

Result: 在VGAF-GEMS基准测试中，GEMS框架在定量和定性比较中优于现有方法，验证了其有效性。

Conclusion: GEMS框架为情绪理解研究提供了新方向，代码和数据已开源。

Abstract: Understanding individual, group and event level emotions along with
contextual information is crucial for analyzing a multi-person social
situation. To achieve this, we frame emotion comprehension as the task of
predicting fine-grained individual emotion to coarse grained group and event
level emotion. We introduce GEMS that leverages a multimodal swin-transformer
and S3Attention based architecture, which processes an input scene, group
members, and context information to generate joint predictions. Existing
multi-person emotion related benchmarks mainly focus on atomic interactions
primarily based on emotion perception over time and group level. To this end,
we extend and propose VGAF-GEMS to provide more fine grained and holistic
analysis on top of existing group level annotation of VGAF dataset. GEMS aims
to predict basic discrete and continuous emotions (including valence and
arousal) as well as individual, group and event level perceived emotions. Our
benchmarking effort links individual, group and situational emotional responses
holistically. The quantitative and qualitative comparisons with adapted
state-of-the-art models demonstrate the effectiveness of GEMS framework on
VGAF-GEMS benchmarking. We believe that it will pave the way of further
research. The code and data is available at:
https://github.com/katariaak579/GEMS

</details>


### [3] [UAVScenes: A Multi-Modal Dataset for UAVs](https://arxiv.org/abs/2507.22412)
*Sijie Wang,Siqi Li,Yawei Zhang,Shangshu Yu,Shenghai Yuan,Rui She,Quanjiang Guo,JinXuan Zheng,Ong Kang Howe,Leonrich Chandra,Shrivarshann Srijeyan,Aditya Sivadas,Toshan Aggarwal,Heyuan Liu,Hongming Zhang,Chujie Chen,Junyu Jiang,Lihua Xie,Wee Peng Tay*

Main category: cs.CV

TL;DR: UAVScenes是一个多模态无人机数据集，旨在解决现有数据集在高级场景理解任务中的局限性，提供帧级标注和6-DoF位姿。


<details>
  <summary>Details</summary>
Motivation: 现有无人机数据集多偏向定位和3D重建任务，缺乏帧级标注，限制了高级场景理解任务的应用。

Method: 基于MARS-LVIG数据集，通过手动标注图像和LiDAR点云的语义信息，并添加6-DoF位姿。

Result: UAVScenes支持多种任务，如分割、深度估计、定位、场景识别和新视角合成。

Conclusion: UAVScenes填补了多模态无人机感知的空白，为高级场景理解任务提供了基准数据集。

Abstract: Multi-modal perception is essential for unmanned aerial vehicle (UAV)
operations, as it enables a comprehensive understanding of the UAVs'
surrounding environment. However, most existing multi-modal UAV datasets are
primarily biased toward localization and 3D reconstruction tasks, or only
support map-level semantic segmentation due to the lack of frame-wise
annotations for both camera images and LiDAR point clouds. This limitation
prevents them from being used for high-level scene understanding tasks. To
address this gap and advance multi-modal UAV perception, we introduce
UAVScenes, a large-scale dataset designed to benchmark various tasks across
both 2D and 3D modalities. Our benchmark dataset is built upon the
well-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only
for simultaneous localization and mapping (SLAM). We enhance this dataset by
providing manually labeled semantic annotations for both frame-wise images and
LiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.
These additions enable a wide range of UAV perception tasks, including
segmentation, depth estimation, 6-DoF localization, place recognition, and
novel view synthesis (NVS). Our dataset is available at
https://github.com/sijieaaa/UAVScenes

</details>


### [4] [From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras](https://arxiv.org/abs/2507.22438)
*Youngho Kim,Hoonhee Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 该论文提出了一种利用事件相机进行域适应的方法，以解决人体姿态估计在运动模糊条件下的性能下降问题。通过事件增强和师生框架，无需目标域标注即可实现鲁棒的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 运动模糊和低光条件导致人体姿态估计性能下降，现有数据集假设稳定条件，模型在模糊环境中表现不佳。

Method: 使用事件相机生成运动感知模糊图像，通过师生框架和互不确定性掩码迭代优化伪标签。

Result: 实验表明，该方法优于传统域适应方法，无需目标域标注即可在运动模糊条件下实现鲁棒姿态估计。

Conclusion: 事件相机为真实世界运动模糊环境中的域适应提供了可扩展且有效的解决方案。

Abstract: Human pose estimation is critical for applications such as rehabilitation,
sports analytics, and AR/VR systems. However, rapid motion and low-light
conditions often introduce motion blur, significantly degrading pose estimation
due to the domain gap between sharp and blurred images. Most datasets assume
stable conditions, making models trained on sharp images struggle in blurred
environments. To address this, we introduce a novel domain adaptation approach
that leverages event cameras, which capture high temporal resolution motion
data and are inherently robust to motion blur. Using event-based augmentation,
we generate motion-aware blurred images, effectively bridging the domain gap
between sharp and blurred domains without requiring paired annotations.
Additionally, we develop a student-teacher framework that iteratively refines
pseudo-labels, leveraging mutual uncertainty masking to eliminate incorrect
labels and enable more effective learning. Experimental results demonstrate
that our approach outperforms conventional domain-adaptive human pose
estimation methods, achieving robust pose estimation under motion blur without
requiring annotations in the target domain. Our findings highlight the
potential of event cameras as a scalable and effective solution for domain
adaptation in real-world motion blur environments. Our project codes are
available at https://github.com/kmax2001/EvSharp2Blur.

</details>


### [5] [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/abs/2507.22459)
*Jaeha Kim,Junghun Oh,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 论文提出EDTR方法，利用扩散先验恢复任务相关细节，提升多复杂退化场景下的任务性能和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有任务驱动图像恢复方法在多复杂退化场景下难以恢复任务相关细节的问题。

Method: 提出EDTR方法，通过从像素误差预恢复的低质量图像中生成并添加轻微噪声，直接利用扩散先验恢复任务相关细节，同时减少去噪步骤以避免冗余信息。

Result: EDTR显著提升了多复杂退化场景下的任务性能和视觉质量。

Conclusion: EDTR有效利用扩散先验，为任务驱动图像恢复提供了新思路。

Abstract: Task-driven image restoration (TDIR) has recently emerged to address
performance drops in high-level vision tasks caused by low-quality (LQ) inputs.
Previous TDIR methods struggle to handle practical scenarios in which images
are degraded by multiple complex factors, leaving minimal clues for
restoration. This motivates us to leverage the diffusion prior, one of the most
powerful natural image priors. However, while the diffusion prior can help
generate visually plausible results, using it to restore task-relevant details
remains challenging, even when combined with recent TDIR methods. To address
this, we propose EDTR, which effectively harnesses the power of diffusion prior
to restore task-relevant details. Specifically, we propose directly leveraging
useful clues from LQ images in the diffusion process by generating from
pixel-error-based pre-restored LQ images with mild noise added. Moreover, we
employ a small number of denoising steps to prevent the generation of redundant
details that dilute crucial task-related information. We demonstrate that our
method effectively utilizes diffusion prior for TDIR, significantly enhancing
task performance and visual quality across diverse tasks with multiple complex
degradations.

</details>


### [6] [A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks](https://arxiv.org/abs/2507.22733)
*Hang Su,Yunlong Feng,Daniel Gehrig,Panfeng Jiang,Ling Gao,Xavier Lagorce,Laurent Kneip*

Main category: cs.CV

TL;DR: 提出了一种统一的方法，用于从任意时间戳的2D点对应关系中估计结构和线性运动，适用于多种相机类型。


<details>
  <summary>Details</summary>
Motivation: 传统算法（如5点或8点算法）仅适用于同步视图，而滚动快门相机和事件相机等异步数据场景需要新的解决方案。

Method: 基于一阶动力学和恒定速度运动模型，推导出线性点入射关系，高效恢复线性速度和3D点。

Result: 在模拟和真实数据上验证了有效性，相比现有方法在所有模态下均有改进。

Conclusion: 该方法为异步数据的高效结构和运动估计提供了新途径。

Abstract: Structure and continuous motion estimation from point correspondences is a
fundamental problem in computer vision that has been powered by well-known
algorithms such as the familiar 5-point or 8-point algorithm. However, despite
their acclaim, these algorithms are limited to processing point correspondences
originating from a pair of views each one representing an instantaneous capture
of the scene. Yet, in the case of rolling shutter cameras, or more recently,
event cameras, this synchronization breaks down. In this work, we present a
unified approach for structure and linear motion estimation from 2D point
correspondences with arbitrary timestamps, from an arbitrary set of views. By
formulating the problem in terms of first-order dynamics and leveraging a
constant velocity motion model, we derive a novel, linear point incidence
relation allowing for the efficient recovery of both linear velocity and 3D
points with predictable degeneracies and solution multiplicities. Owing to its
general formulation, it can handle correspondences from a wide range of sensing
modalities such as global shutter, rolling shutter, and event cameras, and can
even combine correspondences from different collocated sensors. We validate the
effectiveness of our solver on both simulated and real-world data, where we
show consistent improvement across all modalities when compared to recent
approaches. We believe our work opens the door to efficient structure and
motion estimation from asynchronous data. Code can be found at
https://github.com/suhang99/AsyncTrack-Motion-Solver.

</details>


### [7] [CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models](https://arxiv.org/abs/2507.22828)
*Kedong Xiu,Saiqian Zhang*

Main category: cs.CV

TL;DR: CapRecover是一种跨模态反演框架，直接从中间特征恢复高级语义内容（如标签或描述），无需图像重建，解决了视觉语言模型中的语义信息泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）在分割DNN配置中的部署增加，中间特征的语义信息泄漏带来隐私风险。现有方法重建的图像模糊且语义模糊，无法直接解决语义泄漏问题。

Method: 提出CapRecover框架，直接从中间特征恢复语义内容（如标签或描述），并通过实验验证其性能。此外，提出一种保护方法：在每层中间特征添加随机噪声并在下一层去除。

Result: CapRecover在CIFAR-10上达到92.71%的Top-1标签准确率，在COCO2017上生成流畅描述（ROUGE-L得分0.52）。保护方法有效防止语义泄漏且无需额外训练成本。

Conclusion: CapRecover能高效恢复语义内容，同时提出的保护方法简单有效，为隐私保护提供了实用解决方案。

Abstract: As Vision-Language Models (VLMs) are increasingly deployed in split-DNN
configurations--with visual encoders (e.g., ResNet, ViT) operating on user
devices and sending intermediate features to the cloud--there is a growing
privacy risk from semantic information leakage. Existing approaches to
reconstructing images from these intermediate features often result in blurry,
semantically ambiguous images. To directly address semantic leakage, we propose
CapRecover, a cross-modality inversion framework that recovers high-level
semantic content, such as labels or captions, directly from intermediate
features without image reconstruction.
  We evaluate CapRecover on multiple datasets and victim models, demonstrating
strong performance in semantic recovery. Specifically, CapRecover achieves up
to 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from
ResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis
further reveals that deeper convolutional layers encode significantly more
semantic information compared to shallow layers. To mitigate semantic leakage,
we introduce a simple yet effective protection method: adding random noise to
intermediate features at each layer and removing the noise in the next layer.
Experimental results show that this approach prevents semantic leakage without
additional training costs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Comparing Cluster-Based Cross-Validation Strategies for Machine Learning Model Evaluation](https://arxiv.org/abs/2507.22299)
*Afonso Martini Spezia,Mariana Recamonde-Mendoza*

Main category: cs.LG

TL;DR: 论文研究了基于聚类的交叉验证策略，提出了一种结合Mini Batch K-Means和类分层的新技术，并在20个数据集上进行了实验比较。结果表明，该技术在平衡数据集上表现优异，但在不平衡数据集上传统分层交叉验证更优。


<details>
  <summary>Details</summary>
Motivation: 交叉验证在机器学习中至关重要，但传统方法可能导致数据子集无法充分代表原始数据的多样性，从而产生偏差。本文旨在探索基于聚类的交叉验证策略，以提高模型评估的鲁棒性。

Method: 提出了一种结合Mini Batch K-Means和类分层的新交叉验证技术，并在20个数据集（平衡和不平衡）上进行了实验，比较了不同策略的偏差、方差和计算成本。

Result: 在平衡数据集上，新技术的偏差和方差表现优于其他方法，但未显著降低计算成本；在不平衡数据集上，传统分层交叉验证表现更优。聚类算法比较中，无单一算法显著优于其他。

Conclusion: 本文通过深入研究基于聚类的数据分割技术，改进了模型评估策略，同时验证了传统分层交叉验证的有效性，尤其是在不平衡数据集中。研究为提升模型评估的鲁棒性和可靠性提供了新视角。

Abstract: Cross-validation plays a fundamental role in Machine Learning, enabling
robust evaluation of model performance and preventing overestimation on
training and validation data. However, one of its drawbacks is the potential to
create data subsets (folds) that do not adequately represent the diversity of
the original dataset, which can lead to biased performance estimates. The
objective of this work is to deepen the investigation of cluster-based
cross-validation strategies by analyzing the performance of different
clustering algorithms through experimental comparison. Additionally, a new
cross-validation technique that combines Mini Batch K-Means with class
stratification is proposed. Experiments were conducted on 20 datasets (both
balanced and imbalanced) using four supervised learning algorithms, comparing
cross-validation strategies in terms of bias, variance, and computational cost.
The technique that uses Mini Batch K-Means with class stratification
outperformed others in terms of bias and variance on balanced datasets, though
it did not significantly reduce computational cost. On imbalanced datasets,
traditional stratified cross-validation consistently performed better, showing
lower bias, variance, and computational cost, making it a safe choice for
performance evaluation in scenarios with class imbalance. In the comparison of
different clustering algorithms, no single algorithm consistently stood out as
superior. Overall, this work contributes to improving predictive model
evaluation strategies by providing a deeper understanding of the potential of
cluster-based data splitting techniques and reaffirming the effectiveness of
well-established strategies like stratified cross-validation. Moreover, it
highlights perspectives for increasing the robustness and reliability of model
evaluations, especially in datasets with clustering characteristics.

</details>


### [9] [Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities](https://arxiv.org/abs/2307.10803)
*Hanchen Yang,Wengen Li,Shuyu Wang,Hui Li,Jihong Guan,Shuigeng Zhou,Jiannong Cao*

Main category: cs.LG

TL;DR: 本文综述了时空海洋数据挖掘（STDM）的研究现状，包括数据集、数据质量增强技术、任务分类及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时空海洋数据的复杂性和独特性（如区域多样性和高稀疏性）使得STDM模型的设计和训练具有挑战性，但目前缺乏全面综述，阻碍了计算机科学家和海洋科学家的研究与应用。

Method: 首先回顾常用ST海洋数据集及其特性，探讨数据质量增强技术，将现有STDM研究分为预测、事件检测、模式挖掘和异常检测四类任务，并详细分析相关技术。

Result: 提供了STDM在海洋科学中的全面综述，帮助跨领域科学家理解基本概念、关键技术和开放挑战。

Conclusion: 本文填补了STDM在海洋科学领域的综述空白，为未来研究提供了方向。

Abstract: With the rapid amassing of spatial-temporal (ST) ocean data, many
spatial-temporal data mining (STDM) studies have been conducted to address
various oceanic issues, including climate forecasting and disaster warning.
Compared with typical ST data (e.g., traffic data), ST ocean data is more
complicated but with unique characteristics, e.g., diverse regionality and high
sparsity. These characteristics make it difficult to design and train STDM
models on ST ocean data. To the best of our knowledge, a comprehensive survey
of existing studies remains missing in the literature, which hinders not only
computer scientists from identifying the research issues in ocean data mining
but also ocean scientists to apply advanced STDM techniques. In this paper, we
provide a comprehensive survey of existing STDM studies for ocean science.
Concretely, we first review the widely-used ST ocean datasets and highlight
their unique characteristics. Then, typical ST ocean data quality enhancement
techniques are explored. Next, we classify existing STDM studies in ocean
science into four types of tasks, i.e., prediction, event detection, pattern
mining, and anomaly detection, and elaborate on the techniques for these tasks.
Finally, promising research opportunities are discussed. This survey can help
scientists from both computer science and ocean science better understand the
fundamental concepts, key techniques, and open challenges of STDM for ocean
science.

</details>


### [10] [LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning](https://arxiv.org/abs/2507.22499)
*Xiang Li,Qianli Shen,Haonan Wang,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: 提出了一种基于损失重加权的方法（LoReUn），动态调整遗忘数据的权重，显著提升机器遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法对所有遗忘数据赋予相同权重，难以有效处理难遗忘数据。

Method: 利用数据损失反映遗忘难度，提出动态重加权策略LoReUn。

Result: 在图像分类和生成任务中显著缩小与精确遗忘的差距，有效减少有害内容生成。

Conclusion: LoReUn是一种简单高效的插拔式方法，显著提升机器遗忘效果。

Abstract: Recent generative models face significant risks of producing harmful content,
which has underscored the importance of machine unlearning (MU) as a critical
technique for eliminating the influence of undesired data. However, existing MU
methods typically assign the same weight to all data to be forgotten, which
makes it difficult to effectively forget certain data that is harder to unlearn
than others. In this paper, we empirically demonstrate that the loss of data
itself can implicitly reflect its varying difficulty. Building on this insight,
we introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective
plug-and-play strategy that dynamically reweights data during the unlearning
process with minimal additional computational overhead. Our approach
significantly reduces the gap between existing MU methods and exact unlearning
in both image classification and generation tasks, effectively enhancing the
prevention of harmful content generation in text-to-image diffusion models.

</details>


### [11] [HGCN(O): A Self-Tuning GCN HyperModel Toolkit for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2507.22524)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGCN(O)是一个基于GCN的自适应工具包，用于事件序列预测，包含四种GCN架构，优化预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决事件序列预测中数据不平衡和传统方法性能不足的问题。

Method: 整合多种图表示和节点/图级属性，通过边权重引入时间依赖性。

Result: GCNConv模型在不平衡数据上表现优异，所有模型在平衡数据上表现一致，HGCN(O)优于传统方法。

Conclusion: HGCN(O)在事件序列预测中表现出色，适用于PBPM等应用。

Abstract: We propose HGCN(O), a self-tuning toolkit using Graph Convolutional Network
(GCN) models for event sequence prediction. Featuring four GCN architectures
(O-GCN, T-GCN, TP-GCN, TE-GCN) across the GCNConv and GraphConv layers, our
toolkit integrates multiple graph representations of event sequences with
different choices of node- and graph-level attributes and in temporal
dependencies via edge weights, optimising prediction accuracy and stability for
balanced and unbalanced datasets. Extensive experiments show that GCNConv
models excel on unbalanced data, while all models perform consistently on
balanced data. Experiments also confirm the superior performance of HGCN(O)
over traditional approaches. Applications include Predictive Business Process
Monitoring (PBPM), which predicts future events or states of a business process
based on event logs.

</details>


### [12] [Quantifying surprise in clinical care: Detecting highly informative events in electronic health records with foundation models](https://arxiv.org/abs/2507.22798)
*Michael C. Burkhart,Bashar Ramadan,Luke Solo,William F. Parker,Brett K. Beaulieu-Jones*

Main category: cs.LG

TL;DR: 提出一种基于基础模型的方法，用于识别电子健康记录中的高信息量标记和事件，并验证其预测患者结果的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法可能忽略异常事件，而新方法通过全面考虑患者住院数据，能更准确地识别这些事件。

Method: 利用基础模型分析患者住院数据的上下文，识别高信息量事件，并验证其预测能力。

Result: 模型识别的事件对预测患者结果有显著意义，且部分低信息量事件可安全忽略。

Conclusion: 该方法不仅能提高预测准确性，还能帮助解释预后模型的预测结果。

Abstract: We present a foundation model-derived method to identify highly informative
tokens and events in electronic health records. Our approach considers incoming
data in the entire context of a patient's hospitalization and so can flag
anomalous events that rule-based approaches would consider within a normal
range. We demonstrate that the events our model flags are significant for
predicting downstream patient outcomes and that a fraction of events identified
as carrying little information can safely be dropped. Additionally, we show how
informativeness can help interpret the predictions of prognostic models trained
on foundation model-derived representations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [Question Generation for Assessing Early Literacy Reading Comprehension](https://arxiv.org/abs/2507.22410)
*Xiaocheng Yang,Sumuk Shashidhar,Dilek Hakkani-Tur*

Main category: cs.CL

TL;DR: 提出了一种为K-2英语学习者生成阅读理解问题的新方法，确保覆盖材料并适应学习者水平，评估了多种语言模型在FairytaleQA数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 评估阅读理解对阅读学习至关重要，需要一种能适应学习者水平并全面覆盖材料的问题生成方法。

Method: 提出了一种生成多样化、难度可调的问题的方法，确保覆盖材料并适应学习者水平，使用FairytaleQA数据集评估语言模型。

Result: 方法能生成多样化问题并适应学习者水平，在FairytaleQA数据集上表现良好。

Conclusion: 该方法有望成为自主AI英语教学工具的重要组成部分。

Abstract: Assessment of reading comprehension through content-based interactions plays
an important role in the reading acquisition process. In this paper, we propose
a novel approach for generating comprehension questions geared to K-2 English
learners. Our method ensures complete coverage of the underlying material and
adaptation to the learner's specific proficiencies, and can generate a large
diversity of question types at various difficulty levels to ensure a thorough
evaluation. We evaluate the performance of various language models in this
framework using the FairytaleQA dataset as the source material. Eventually, the
proposed approach has the potential to become an important part of autonomous
AI-driven English instructors.

</details>


### [14] [AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini](https://arxiv.org/abs/2507.22445)
*Jill Walker Rettberg,Hermann Wigers*

Main category: cs.CL

TL;DR: 研究发现，基于英美文本训练的语言模型生成的故事虽包含表面文化符号，但叙事结构单一，缺乏文化深度和多样性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否能生成具有文化相关性的故事，揭示AI生成叙事的潜在偏见。

Method: 使用GPT-4o-mini为236个国家各生成50个故事，分析叙事结构和内容。

Result: 故事叙事结构高度同质化，强调传统与稳定，缺乏冲突和多样性。

Conclusion: AI生成叙事的结构同质化是一种新型偏见，需在文化对齐和AI研究中引起重视。

Abstract: Can a language model trained largely on Anglo-American texts generate stories
that are culturally relevant to other nationalities? To find out, we generated
11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a
1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although
the stories do include surface-level national symbols and themes, they
overwhelmingly conform to a single narrative plot structure across countries: a
protagonist lives in or returns home to a small town and resolves a minor
conflict by reconnecting with tradition and organising community events.
Real-world conflicts are sanitised, romance is almost absent, and narrative
tension is downplayed in favour of nostalgia and reconciliation. The result is
a narrative homogenisation: an AI-generated synthetic imaginary that
prioritises stability above change and tradition above growth. We argue that
the structural homogeneity of AI-generated narratives constitutes a distinct
form of AI bias, a narrative standardisation that should be acknowledged
alongside the more familiar representational bias. These findings are relevant
to literary studies, narratology, critical AI studies, NLP research, and
efforts to improve the cultural alignment of generative AI.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [15] [Bayesian Optimization applied for accelerated Virtual Validation of the Autonomous Driving Function](https://arxiv.org/abs/2507.22769)
*Satyesh Shanker Awasthi,Mohammed Irshadh Ismaaeel Sathyamangalam Imran,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: 该论文提出了一种基于贝叶斯优化（BO）的框架，用于加速自动驾驶功能（ADF）验证中的关键场景发现。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆（AV）的验证主要依赖仿真，但全面探索参数空间耗时且计算成本高，因此需要更高效的方法。

Method: 采用贝叶斯优化框架，以模型预测控制器（MPC）为基础的运动规划器为例，验证其有效性。

Result: 该框架能以远低于暴力实验设计（DoE）方法的仿真次数识别危险场景（如偏离道路事件），并在高维参数空间中展现可扩展性。

Conclusion: 该框架显著提高了验证效率，并能识别运动规划器操作设计域（ODD）中的多个关键区域。

Abstract: Rigorous Verification and Validation (V&V) of Autonomous Driving Functions
(ADFs) is paramount for ensuring the safety and public acceptance of Autonomous
Vehicles (AVs). Current validation relies heavily on simulation to achieve
sufficient test coverage within the Operational Design Domain (ODD) of a
vehicle, but exhaustively exploring the vast parameter space of possible
scenarios is computationally expensive and time-consuming. This work introduces
a framework based on Bayesian Optimization (BO) to accelerate the discovery of
critical scenarios. We demonstrate the effectiveness of the framework on an
Model Predictive Controller (MPC)-based motion planner, showing that it
identifies hazardous situations, such as off-road events, using orders of
magnitude fewer simulations than brute-force Design of Experiments (DoE)
methods. Furthermore, this study investigates the scalability of the framework
in higher-dimensional parameter spaces and its ability to identify multiple,
distinct critical regions within the ODD of the motion planner used as the case
study .

</details>
