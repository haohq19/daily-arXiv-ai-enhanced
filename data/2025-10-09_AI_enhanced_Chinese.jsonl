{"id": "2510.06267", "categories": ["cs.LG", "cs.AI", "I.2.6; H.2.8; J.3"], "pdf": "https://arxiv.org/pdf/2510.06267", "abs": "https://arxiv.org/abs/2510.06267", "authors": ["Khartik Uppalapati", "Shakeel Abdulkareem", "Bora Yimenicioglu"], "title": "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases", "comment": "6 pages, 2 figures, 2 tables. Submitted to IEEE International\n  Conference on Data Science and Advanced Analytics (DSAA)", "summary": "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion\nframework that generates realistic yet privacy-preserving synthetic\nelectronic-health-record (EHR) trajectories for ultra-rare diseases.\nRareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human\nPhenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA\nAdverse Event Reporting System (FAERS) into a heterogeneous knowledge graph\ncomprising approximately 8 M typed edges. Meta-path scores extracted from this\n8-million-edge KG modulate the per-token noise schedule in the forward\nstochastic differential equation, steering generation toward biologically\nplausible lab-medication-adverse-event co-occurrences while retaining\nscore-based diffusion model stability. The reverse denoiser then produces\ntimestamped sequences of lab-code, medication-code, and adverse-event-flag\ntriples that contain no protected health information. On simulated\nultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean\nDiscrepancy by 40 percent relative to an unguided diffusion baseline and by\ngreater than 60 percent versus GAN counterparts, without sacrificing downstream\npredictive utility. A black-box membership-inference evaluation using the\nDOMIAS attacker yields AUROC approximately 0.53, well below the 0.55\nsafe-release threshold and substantially better than the approximately 0.61\nplus or minus 0.03 observed for non-KG baselines, demonstrating strong\nresistance to re-identification. These results suggest that integrating\nbiomedical knowledge graphs directly into diffusion noise schedules can\nsimultaneously enhance fidelity and privacy, enabling safer data sharing for\nrare-disease research.", "AI": {"tldr": "RareGraph-Synth\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u771f\u5b9e\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u8d85\u7f55\u89c1\u75be\u75c5\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u8d44\u6e90\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u751f\u6210\u751f\u7269\u53ef\u4fe1\u7684\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u4e3a\u8d85\u7f55\u89c1\u75be\u75c5\u7814\u7a76\u751f\u6210\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u751f\u6210\u6570\u636e\u7684\u751f\u7269\u53ef\u4fe1\u5ea6\u3002", "method": "\u6574\u5408\u4e94\u4e2a\u516c\u5171\u8d44\u6e90\u6784\u5efa\u5f02\u8d28\u77e5\u8bc6\u56fe\u8c31\uff08\u7ea6800\u4e07\u6761\u8fb9\uff09\uff0c\u4f7f\u7528\u5143\u8def\u5f84\u5206\u6570\u8c03\u8282\u6269\u6563\u6a21\u578b\u7684\u524d\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u4e2d\u7684\u566a\u58f0\u8c03\u5ea6\uff0c\u5f15\u5bfc\u751f\u6210\u751f\u7269\u53ef\u4fe1\u7684\u5b9e\u9a8c\u5ba4-\u836f\u7269-\u4e0d\u826f\u4e8b\u4ef6\u5171\u73b0\u6a21\u5f0f\u3002", "result": "\u5728\u6a21\u62df\u8d85\u7f55\u89c1\u75be\u75c5\u961f\u5217\u4e2d\uff0c\u76f8\u6bd4\u65e0\u5f15\u5bfc\u6269\u6563\u57fa\u7ebf\u964d\u4f4e\u5206\u7c7b\u6700\u5927\u5747\u503c\u5dee\u5f0240%\uff0c\u76f8\u6bd4GAN\u65b9\u6cd5\u964d\u4f4e60%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u9884\u6d4b\u6548\u7528\u3002\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\u8bc4\u4f30\u663e\u793aAUROC\u7ea60.53\uff0c\u8fdc\u4f4e\u4e8e\u5b89\u5168\u53d1\u5e03\u9608\u503c0.55\u3002", "conclusion": "\u5c06\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u76f4\u63a5\u6574\u5408\u5230\u6269\u6563\u566a\u58f0\u8c03\u5ea6\u4e2d\u53ef\u4ee5\u540c\u65f6\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3a\u7f55\u89c1\u75be\u75c5\u7814\u7a76\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u6570\u636e\u5171\u4eab\u3002"}}
{"id": "2510.06492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06492", "abs": "https://arxiv.org/abs/2510.06492", "authors": ["Matthew Kim", "Kensuke Nakamura", "Andrea Bajcsy"], "title": "What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?", "comment": "8 tables 6 figures", "summary": "Safe control techniques, such as Hamilton-Jacobi reachability, provide\nprincipled methods for synthesizing safety-preserving robot policies but\ntypically assume hand-designed state spaces and full observability. Recent work\nhas relaxed these assumptions via latent-space safe control, where state\nrepresentations and dynamics are learned jointly through world models that\nreconstruct future high-dimensional observations (e.g., RGB images) from\ncurrent observations and actions. This enables safety constraints that are\ndifficult to specify analytically (e.g., spilling) to be framed as\nclassification problems in latent space, allowing controllers to operate\ndirectly from raw observations. However, these methods assume that\nsafety-critical features are observable in the learned latent state. We ask:\nwhen are latent state spaces sufficient for safe control? To study this, we\nexamine temperature-based failures, comparable to overheating in cooking or\nmanufacturing tasks, and find that RGB-only observations can produce myopic\nsafety behaviors, e.g., avoiding seeing failure states rather than preventing\nfailure itself. To predict such behaviors, we introduce a mutual\ninformation-based measure that identifies when observations fail to capture\nsafety-relevant features. Finally, we propose a multimodal-supervised training\nstrategy that shapes the latent state with additional sensory inputs during\ntraining, but requires no extra modalities at deployment, and validate our\napproach in simulation and on hardware with a Franka Research 3 manipulator\npreventing a pot of wax from overheating.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\uff0c\u53d1\u73b0\u4ec5\u4f7f\u7528RGB\u89c2\u6d4b\u53ef\u80fd\u5bfc\u81f4\u77ed\u89c6\u7684\u5b89\u5168\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u5ea6\u91cf\u65b9\u6cd5\u6765\u8bc6\u522b\u89c2\u6d4b\u662f\u5426\u6355\u83b7\u5b89\u5168\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u7a7a\u95f4\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u5047\u8bbe\u5b89\u5168\u5173\u952e\u7279\u5f81\u53ef\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u72b6\u6001\u4e2d\u89c2\u5bdf\u5230\uff0c\u4f46\u5b9e\u9645\u4e2dRGB\u89c2\u6d4b\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5b89\u5168\u76f8\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u77ed\u89c6\u7684\u5b89\u5168\u884c\u4e3a\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u5ea6\u91cf\u65b9\u6cd5\u8bc6\u522b\u89c2\u6d4b\u7f3a\u9677\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u989d\u5916\u4f20\u611f\u5668\u8f93\u5165\u5851\u9020\u6f5c\u5728\u72b6\u6001\uff0c\u4f46\u90e8\u7f72\u65f6\u65e0\u9700\u989d\u5916\u6a21\u6001\u3002", "result": "\u5728\u4eff\u771f\u548cFranka Research 3\u673a\u68b0\u81c2\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u6210\u529f\u9632\u6b62\u8721\u9505\u8fc7\u70ed\u3002", "conclusion": "\u591a\u6a21\u6001\u76d1\u7763\u8bad\u7ec3\u80fd\u591f\u6539\u5584\u6f5c\u5728\u72b6\u6001\u5bf9\u5b89\u5168\u76f8\u5173\u7279\u5f81\u7684\u8868\u793a\uff0c\u63d0\u9ad8\u5b89\u5168\u63a7\u5236\u6027\u80fd\uff0c\u800c\u65e0\u9700\u5728\u90e8\u7f72\u65f6\u589e\u52a0\u989d\u5916\u4f20\u611f\u5668\u3002"}}
{"id": "2510.06231", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06231", "abs": "https://arxiv.org/abs/2510.06231", "authors": ["Mingzhe Zheng", "Dingjie Song", "Guanyu Zhou", "Jun You", "Jiahao Zhan", "Xuran Ma", "Xinyuan Song", "Ser-Nam Lim", "Qifeng Chen", "Harry Yang"], "title": "CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation", "comment": "24 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating highly structured texts. However, while exhibiting a high degree of\nstructural organization, movie scripts demand an additional layer of nuanced\nstorytelling and emotional depth-the 'soul' of compelling cinema-that LLMs\noften fail to capture. To investigate this deficiency, we first curated\nCML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup\nLanguage (CML), where 'content' consists of segments from esteemed,\nhigh-quality movie scripts and 'summary' is a concise description of the\ncontent. Through an in-depth analysis of the intrinsic multi-shot continuity\nand narrative structures within these authentic scripts, we identified three\npivotal dimensions for quality assessment: Dialogue Coherence (DC), Character\nConsistency (CC), and Plot Reasonableness (PR). Informed by these findings, we\npropose the CML-Bench, featuring quantitative metrics across these dimensions.\nCML-Bench effectively assigns high scores to well-crafted, human-written\nscripts while concurrently pinpointing the weaknesses in screenplays generated\nby LLMs. To further validate our benchmark, we introduce CML-Instruction, a\nprompting strategy with detailed instructions on character dialogue and event\nlogic, to guide LLMs to generate more structured and cinematically sound\nscripts. Extensive experiments validate the effectiveness of our benchmark and\ndemonstrate that LLMs guided by CML-Instruction generate higher-quality\nscreenplays, with results aligned with human preferences.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CML-Bench\u57fa\u51c6\u548cCML-Instruction\u63d0\u793a\u7b56\u7565\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u5728\u751f\u6210\u7535\u5f71\u5267\u672c\u65f6\u7684\u8d28\u91cf\uff0c\u91cd\u70b9\u5173\u6ce8\u5bf9\u8bdd\u8fde\u8d2f\u6027\u3001\u89d2\u8272\u4e00\u81f4\u6027\u548c\u60c5\u8282\u5408\u7406\u6027\u3002", "motivation": "\u867d\u7136LLMs\u80fd\u591f\u751f\u6210\u7ed3\u6784\u826f\u597d\u7684\u6587\u672c\uff0c\u4f46\u5728\u521b\u4f5c\u9700\u8981\u7ec6\u817b\u53d9\u4e8b\u548c\u60c5\u611f\u6df1\u5ea6\u7684\u7535\u5f71\u5267\u672c\u65f6\uff0c\u5f80\u5f80\u7f3a\u4e4f\u7535\u5f71\u7684\u7075\u9b42\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e86CML\u6570\u636e\u96c6\uff0c\u5206\u6790\u771f\u5b9e\u5267\u672c\u7684\u5185\u5728\u8fde\u7eed\u6027\u7ed3\u6784\uff0c\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\uff1b\u7136\u540e\u5f00\u53d1CML-Bench\u91cf\u5316\u6307\u6807\u548cCML-Instruction\u63d0\u793a\u7b56\u7565\u3002", "result": "CML-Bench\u80fd\u6709\u6548\u533a\u5206\u4eba\u7c7b\u521b\u4f5c\u548cLLMs\u751f\u6210\u7684\u5267\u672c\u8d28\u91cf\uff0c\u4f7f\u7528CML-Instruction\u6307\u5bfc\u7684LLMs\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5267\u672c\uff0c\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u63d0\u793a\u7b56\u7565\u80fd\u6709\u6548\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u5728\u7535\u5f71\u5267\u672c\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.06566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06566", "abs": "https://arxiv.org/abs/2510.06566", "authors": ["Vincent Lam", "Robin Chhabra"], "title": "Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning", "comment": null, "summary": "The objective of this study is to develop a model-free workspace trajectory\nplanner for space manipulators using a Twin Delayed Deep Deterministic Policy\nGradient (TD3) agent to enable safe and reliable debris capture. A local\ncontrol strategy with singularity avoidance and manipulability enhancement is\nemployed to ensure stable execution. The manipulator must simultaneously track\na capture point on a non-cooperative target, avoid self-collisions, and prevent\nunintended contact with the target. To address these challenges, we propose a\ncurriculum-based multi-critic network where one critic emphasizes accurate\ntracking and the other enforces collision avoidance. A prioritized experience\nreplay buffer is also used to accelerate convergence and improve policy\nrobustness. The framework is evaluated on a simulated seven-degree-of-freedom\nKUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating\nsafe and adaptive trajectory generation for debris removal missions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTD3\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u6a21\u578b\u7a7a\u95f4\u673a\u68b0\u81c2\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u591a\u6279\u8bc4\u5668\u7f51\u7edc\u548c\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u592a\u7a7a\u788e\u7247\u6355\u83b7\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u975e\u5408\u4f5c\u76ee\u6807\u4e0a\u5b89\u5168\u6355\u83b7\u788e\u7247\u7684\u592a\u7a7a\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u8f68\u8ff9\u8ddf\u8e2a\u3001\u81ea\u78b0\u649e\u907f\u514d\u548c\u4e0e\u76ee\u6807\u610f\u5916\u63a5\u89e6\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528TD3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u63a7\u5236\u7b56\u7565\u8fdb\u884c\u5947\u5f02\u6027\u907f\u514d\u548c\u53ef\u64cd\u4f5c\u6027\u589e\u5f3a\uff0c\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u591a\u6279\u8bc4\u5668\u7f51\u7edc\uff08\u4e00\u4e2a\u5173\u6ce8\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u4e00\u4e2a\u5173\u6ce8\u78b0\u649e\u907f\u514d\uff09\uff0c\u5e76\u4f7f\u7528\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u52a0\u901f\u6536\u655b\u3002", "result": "\u5728Matlab/Simulink\u4e2d\u6a21\u62df\u7684\u4e03\u81ea\u7531\u5ea6KUKA LBR iiwa\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\uff0c\u80fd\u591f\u751f\u6210\u5b89\u5168\u81ea\u9002\u5e94\u7684\u8f68\u8ff9\u7528\u4e8e\u788e\u7247\u6e05\u9664\u4efb\u52a1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u4e3a\u592a\u7a7a\u788e\u7247\u6e05\u9664\u4efb\u52a1\u751f\u6210\u5b89\u5168\u53ef\u9760\u7684\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u81ea\u9002\u5e94\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2510.06293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06293", "abs": "https://arxiv.org/abs/2510.06293", "authors": ["Cristian Meo", "Varun Sarathchandran", "Avijit Majhi", "Shao Hung", "Carlo Saccardi", "Ruben Imhoff", "Roberto Deidda", "Remko Uijlenhoet", "Justin Dauwels"], "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression", "comment": null, "summary": "Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines.", "AI": {"tldr": "BlockGPT\u662f\u4e00\u79cd\u7528\u4e8e\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u7684\u751f\u6210\u5f0f\u81ea\u56de\u5f52Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u6279\u91cf\u6807\u8bb0\u5316\u65b9\u6cd5\u9884\u6d4b\u5b8c\u6574\u4e8c\u7ef4\u573a\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u65b9\u6cd5\u5b58\u5728\u7684\u5f52\u7eb3\u504f\u5dee\u95ee\u9898\u3001\u63a8\u7406\u901f\u5ea6\u6162\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u5b9e\u65f6\u5e94\u7528\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faBlockGPT\u6a21\u578b\uff0c\u91c7\u7528\u6279\u91cf\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9884\u6d4b\u5b8c\u6574\u4e8c\u7ef4\u573a\uff0c\u901a\u8fc7\u5e27\u5185\u81ea\u6ce8\u610f\u529b\u548c\u5e27\u95f4\u56e0\u679c\u6ce8\u610f\u529b\u5206\u89e3\u65f6\u7a7a\u5173\u7cfb\u3002", "result": "\u5728KNMI\u548cSEVIR\u4e24\u4e2a\u964d\u6c34\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cBlockGPT\u5728\u51c6\u786e\u6027\u3001\u4e8b\u4ef6\u5b9a\u4f4d\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u53ef\u6bd4\u57fa\u51c6\u5feb\u8fbe31\u500d\u3002", "conclusion": "BlockGPT\u4e3a\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.06254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06254", "abs": "https://arxiv.org/abs/2510.06254", "authors": ["Xiaochen Zhao", "Chengting Yu", "Kairong Yu", "Lei Liu", "Aili Wang"], "title": "Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training", "comment": null, "summary": "Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on\nneuromorphic hardware due to their sparse activation patterns. However,\nconventional training methods based on surrogate gradients and Backpropagation\nThrough Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in\nperformance, but also incur significant computational and memory overheads that\ngrow linearly with the temporal dimension. To enable high-performance SNN\ntraining under limited computational resources, we propose an enhanced\nself-distillation framework, jointly optimized with rate-based backpropagation.\nSpecifically, the firing rates of intermediate SNN layers are projected onto\nlightweight ANN branches, and high-quality knowledge generated by the model\nitself is used to optimize substructures through the ANN pathways. Unlike\ntraditional self-distillation paradigms, we observe that low-quality\nself-generated knowledge may hinder convergence. To address this, we decouple\nthe teacher signal into reliable and unreliable components, ensuring that only\nreliable knowledge is used to guide the optimization of the model. Extensive\nexperiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that\nour method reduces training complexity while achieving high-performance SNN\ntraining. Our code is available at\nhttps://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u901f\u7387\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u901a\u8fc7\u5c06SNN\u4e2d\u95f4\u5c42\u7684\u53d1\u653e\u7387\u6295\u5f71\u5230\u8f7b\u91cf\u7ea7ANN\u5206\u652f\u4e0a\uff0c\u5229\u7528\u9ad8\u8d28\u91cf\u81ea\u751f\u6210\u77e5\u8bc6\u4f18\u5316\u5b50\u7ed3\u6784\uff0c\u51cf\u5c11\u8bad\u7ec3\u590d\u6742\u5ea6\u5e76\u5b9e\u73b0\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edfSNN\u8bad\u7ec3\u65b9\u6cd5\u57fa\u4e8e\u66ff\u4ee3\u68af\u5ea6\u548cBPTT\uff0c\u4e0d\u4ec5\u6027\u80fd\u843d\u540e\u4e8eANN\uff0c\u8fd8\u4ea7\u751f\u968f\u65f6\u95f4\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002\u4e3a\u4e86\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\uff0c\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u589e\u5f3a\u7684\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u5c06SNN\u4e2d\u95f4\u5c42\u53d1\u653e\u7387\u6295\u5f71\u5230\u8f7b\u91cf\u7ea7ANN\u5206\u652f\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u81ea\u751f\u6210\u77e5\u8bc6\u901a\u8fc7ANN\u8def\u5f84\u4f18\u5316\u5b50\u7ed3\u6784\u3002\u901a\u8fc7\u5c06\u6559\u5e08\u4fe1\u53f7\u89e3\u8026\u4e3a\u53ef\u9760\u548c\u4e0d\u53ef\u9760\u7ec4\u4ef6\uff0c\u786e\u4fdd\u4ec5\u4f7f\u7528\u53ef\u9760\u77e5\u8bc6\u6307\u5bfc\u6a21\u578b\u4f18\u5316\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001CIFAR10-DVS\u548cImageNet\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u8bad\u7ec3\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u589e\u5f3a\u81ea\u84b8\u998f\u6846\u67b6\u80fd\u591f\u6709\u6548\u964d\u4f4eSNN\u8bad\u7ec3\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684SNN\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06260", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06260", "abs": "https://arxiv.org/abs/2510.06260", "authors": ["Sher Khan", "Raz Muhammad", "Adil Hussain", "Muhammad Sajjad", "Muhammad Rashid"], "title": "Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis", "comment": null, "summary": "Cutaneous malignancies demand early detection for favorable outcomes, yet\ncurrent diagnostics suffer from inter-observer variability and access\ndisparities. While AI shows promise, existing dermatological systems are\nlimited by homogeneous architectures, dataset biases across skin tones, and\nfragmented approaches that treat natural language processing as separate\npost-hoc explanations rather than integral to clinical decision-making. We\nintroduce a unified framework that fundamentally reimagines AI integration for\ndermatological diagnostics through two synergistic innovations. First, a\npurposefully heterogeneous ensemble of architecturally diverse convolutional\nneural networks provides complementary diagnostic perspectives, with an\nintrinsic uncertainty mechanism flagging discordant cases for specialist review\n-- mimicking clinical best practices. Second, we embed large language model\ncapabilities directly into the diagnostic workflow, transforming classification\noutputs into clinically meaningful assessments that simultaneously fulfill\nmedical documentation requirements and deliver patient-centered education. This\nseamless integration generates structured reports featuring precise lesion\ncharacterization, accessible diagnostic reasoning, and actionable monitoring\nguidance -- empowering patients to recognize early warning signs between\nvisits. By addressing both diagnostic reliability and communication barriers\nwithin a single cohesive system, our approach bridges the critical\ntranslational gap that has prevented previous AI implementations from achieving\nclinical impact. The framework represents a significant advancement toward\ndeployable dermatological AI that enhances diagnostic precision while actively\nsupporting the continuum of care from initial detection through patient\neducation, ultimately improving early intervention rates for skin lesions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u76ae\u80a4\u75c5AI\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u548c\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff0c\u63d0\u9ad8\u8bca\u65ad\u53ef\u9760\u6027\u5e76\u6539\u5584\u533b\u60a3\u6c9f\u901a\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u76ae\u80a4\u75c5\u8bca\u65ad\u4e2d\u5b58\u5728\u7684\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u3001\u8bbf\u95ee\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709AI\u7cfb\u7edf\u7684\u67b6\u6784\u540c\u8d28\u6027\u3001\u6570\u636e\u96c6\u504f\u89c1\u548c\u788e\u7247\u5316\u65b9\u6cd5\u7b49\u5c40\u9650\u6027\u3002", "method": "1. \u4f7f\u7528\u67b6\u6784\u591a\u6837\u5316\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5f02\u6784\u96c6\u6210\uff0c\u63d0\u4f9b\u4e92\u8865\u8bca\u65ad\u89c6\u89d2\uff0c\u5185\u7f6e\u4e0d\u786e\u5b9a\u6027\u673a\u5236\u6807\u8bb0\u4e0d\u4e00\u81f4\u75c5\u4f8b\uff1b2. \u5c06\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u76f4\u63a5\u5d4c\u5165\u8bca\u65ad\u5de5\u4f5c\u6d41\uff0c\u5c06\u5206\u7c7b\u8f93\u51fa\u8f6c\u5316\u4e3a\u4e34\u5e8a\u8bc4\u4f30\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\u7684\u7cfb\u7edf\uff0c\u5305\u542b\u7cbe\u786e\u75c5\u53d8\u7279\u5f81\u63cf\u8ff0\u3001\u53ef\u7406\u89e3\u7684\u8bca\u65ad\u63a8\u7406\u548c\u53ef\u64cd\u4f5c\u7684\u76d1\u6d4b\u6307\u5bfc\uff0c\u540c\u65f6\u6ee1\u8db3\u533b\u7597\u6587\u6863\u8981\u6c42\u548c\u60a3\u8005\u6559\u80b2\u9700\u6c42\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u540c\u65f6\u89e3\u51b3\u8bca\u65ad\u53ef\u9760\u6027\u548c\u6c9f\u901a\u969c\u788d\uff0c\u5f25\u5408\u4e86\u963b\u788d\u5148\u524dAI\u7cfb\u7edf\u5b9e\u73b0\u4e34\u5e8a\u5f71\u54cd\u7684\u5173\u952e\u8f6c\u5316\u5dee\u8ddd\uff0c\u4ee3\u8868\u4e86\u53ef\u90e8\u7f72\u76ae\u80a4\u75c5AI\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.06273", "categories": ["cs.CV", "astro-ph.IM", "cs.LG", "gr-qc"], "pdf": "https://arxiv.org/pdf/2510.06273", "abs": "https://arxiv.org/abs/2510.06273", "authors": ["Divyansh Srivastava", "Andrzej Niedzielski"], "title": "Vision Transformer for Transient Noise Classification", "comment": "9 pages, 4 figures", "summary": "Transient noise (glitches) in LIGO data hinders the detection of\ngravitational waves (GW). The Gravity Spy project has categorized these noise\nevents into various classes. With the O3 run, there is the inclusion of two\nadditional noise classes and thus a need to train new models for effective\nclassification. We aim to classify glitches in LIGO data into 22 existing\nclasses from the first run plus 2 additional noise classes from O3a using the\nVision Transformer (ViT) model. We train a pre-trained Vision Transformer\n(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset\nwith the additional two classes from the LIGO O3a run. We achieve a\nclassification efficiency of 92.26%, demonstrating the potential of Vision\nTransformer to improve the accuracy of gravitational wave detection by\neffectively distinguishing transient noise.\n  Key words: gravitational waves --vision transformer --machine learning", "AI": {"tldr": "\u4f7f\u7528Vision Transformer\u6a21\u578b\u5bf9LIGO\u6570\u636e\u4e2d\u7684\u77ac\u6001\u566a\u58f0\u8fdb\u884c\u5206\u7c7b\uff0c\u5c0622\u4e2a\u73b0\u6709\u7c7b\u522b\u548cO3a\u8fd0\u884c\u4e2d\u76842\u4e2a\u65b0\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\uff0c\u8fbe\u523092.26%\u7684\u5206\u7c7b\u6548\u7387\u3002", "motivation": "LIGO\u6570\u636e\u4e2d\u7684\u77ac\u6001\u566a\u58f0\uff08glitches\uff09\u963b\u788d\u4e86\u5f15\u529b\u6ce2\u7684\u68c0\u6d4b\uff0c\u968f\u7740O3\u8fd0\u884c\u7684\u8fdb\u884c\uff0c\u9700\u8981\u8bad\u7ec3\u65b0\u6a21\u578b\u6765\u6709\u6548\u5206\u7c7b\u65b0\u589e\u7684\u566a\u58f0\u7c7b\u522b\u3002", "method": "\u5728\u5305\u542bGravity Spy\u6570\u636e\u96c6\u548cLIGO O3a\u8fd0\u884c\u4e2d\u4e24\u4e2a\u65b0\u589e\u7c7b\u522b\u7684\u7ec4\u5408\u6570\u636e\u96c6\u4e0a\uff0c\u8bad\u7ec3\u9884\u8bad\u7ec3\u7684Vision Transformer\uff08ViT-B/32\uff09\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e8692.26%\u7684\u5206\u7c7b\u6548\u7387\uff0c\u8868\u660eVision Transformer\u5728\u6709\u6548\u533a\u5206\u77ac\u6001\u566a\u58f0\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "Vision Transformer\u6a21\u578b\u80fd\u591f\u63d0\u9ad8\u5f15\u529b\u6ce2\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u6709\u6548\u533a\u5206\u77ac\u6001\u566a\u58f0\u6765\u6539\u5584\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.06383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06383", "abs": "https://arxiv.org/abs/2510.06383", "authors": ["Pierre Lison", "Mark Anderson"], "title": "Protecting De-identified Documents from Search-based Linkage Attacks", "comment": null, "summary": "While de-identification models can help conceal the identity of the\nindividual(s) mentioned in a document, they fail to address linkage risks,\ndefined as the potential to map the de-identified text back to its source. One\nstraightforward way to perform such linkages is to extract phrases from the\nde-identified document and then check their presence in the original dataset.\nThis paper presents a method to counter search-based linkage attacks while\npreserving the semantic integrity of the text. The method proceeds in two\nsteps. We first construct an inverted index of the N-grams occurring in the\ndocument collection, making it possible to efficiently determine which N-grams\nappear in less than $k$ documents (either alone or in combination with other\nN-grams). An LLM-based rewriter is then iteratively queried to reformulate\nthose spans until linkage is no longer possible. Experimental results on a\ncollection of court cases show that the method is able to effectively prevent\nsearch-based linkages while remaining faithful to the original content.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u6b62\u57fa\u4e8e\u641c\u7d22\u7684\u94fe\u63a5\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6587\u6863\u4e2d\u7f55\u89c1\u7684N-gram\u5e76\u4f7f\u7528LLM\u91cd\u5199\u5668\u8fed\u4ee3\u91cd\u6784\u8fd9\u4e9b\u7247\u6bb5\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\u540c\u65f6\u9632\u6b62\u5c06\u53bb\u6807\u8bc6\u5316\u6587\u672c\u6620\u5c04\u56de\u539f\u59cb\u6570\u636e\u6e90\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u6807\u8bc6\u5316\u6a21\u578b\u867d\u7136\u80fd\u9690\u85cf\u6587\u6863\u4e2d\u4e2a\u4eba\u7684\u8eab\u4efd\u4fe1\u606f\uff0c\u4f46\u65e0\u6cd5\u5e94\u5bf9\u94fe\u63a5\u98ce\u9669\uff0c\u5373\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u63d0\u53d6\u53bb\u6807\u8bc6\u5316\u6587\u6863\u4e2d\u7684\u77ed\u8bed\u5e76\u5c06\u5176\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u8fdb\u884c\u5339\u914d\u6765\u91cd\u65b0\u8bc6\u522b\u6e90\u6587\u6863\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u6b65\uff1a\u9996\u5148\u6784\u5efa\u6587\u6863\u96c6\u5408\u4e2dN-gram\u7684\u5012\u6392\u7d22\u5f15\uff0c\u8bc6\u522b\u51fa\u73b0\u5728\u5c11\u4e8ek\u4e2a\u6587\u6863\u4e2d\u7684\u7f55\u89c1N-gram\uff1b\u7136\u540e\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u91cd\u5199\u5668\u8fed\u4ee3\u91cd\u6784\u8fd9\u4e9b\u7247\u6bb5\uff0c\u76f4\u5230\u65e0\u6cd5\u8fdb\u884c\u94fe\u63a5\u4e3a\u6b62\u3002", "result": "\u5728\u6cd5\u9662\u6848\u4f8b\u96c6\u5408\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9632\u6b62\u57fa\u4e8e\u641c\u7d22\u7684\u94fe\u63a5\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u539f\u59cb\u5185\u5bb9\u7684\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u6587\u672c\u7684\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u53bb\u6807\u8bc6\u5316\u4e2d\u7684\u94fe\u63a5\u98ce\u9669\u95ee\u9898\u3002"}}
{"id": "2510.06440", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06440", "abs": "https://arxiv.org/abs/2510.06440", "authors": ["Carly Sutter", "Kara J. Sulia", "Nick P. Bassill", "Christopher D. Wirz", "Christopher D. Thorncroft", "Jay C. Rothenberger", "Vanessa Przybylo", "Mariana G. Cains", "Jacob Radford", "David Aaron Evans"], "title": "Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data", "comment": null, "summary": "The New York State Department of Transportation (NYSDOT) has a network of\nroadside traffic cameras that are used by both the NYSDOT and the public to\nobserve road conditions. The NYSDOT evaluates road conditions by driving on\nroads and observing live cameras, tasks which are labor-intensive but necessary\nfor making critical operational decisions during winter weather events.\nHowever, machine learning models can provide additional support for the NYSDOT\nby automatically classifying current road conditions across the state. In this\nstudy, convolutional neural networks and random forests are trained on camera\nimages and weather data to predict road surface conditions. Models are trained\non a hand-labeled dataset of ~22,000 camera images, each classified by human\nlabelers into one of six road surface conditions: severe snow, snow, wet, dry,\npoor visibility, or obstructed. Model generalizability is prioritized to meet\nthe operational needs of the NYSDOT decision makers, and the weather-related\nroad surface condition model in this study achieves an accuracy of 81.5% on\ncompletely unseen cameras.", "AI": {"tldr": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u968f\u673a\u68ee\u6797\u7ed3\u5408\u6444\u50cf\u5934\u56fe\u50cf\u548c\u5929\u6c14\u6570\u636e\uff0c\u81ea\u52a8\u5206\u7c7b\u9053\u8def\u8868\u9762\u72b6\u51b5\uff0c\u4e3a\u7ebd\u7ea6\u5dde\u4ea4\u901a\u90e8\u95e8\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u7ebd\u7ea6\u5dde\u4ea4\u901a\u90e8\u95e8\u76ee\u524d\u901a\u8fc7\u4eba\u5de5\u9a7e\u9a76\u89c2\u5bdf\u548c\u5b9e\u65f6\u6444\u50cf\u5934\u8bc4\u4f30\u9053\u8def\u72b6\u51b5\uff0c\u8fd9\u4e9b\u4efb\u52a1\u52b3\u52a8\u5bc6\u96c6\u4f46\u53c8\u662f\u51ac\u5b63\u5929\u6c14\u4e8b\u4ef6\u4e2d\u505a\u51fa\u5173\u952e\u8fd0\u8425\u51b3\u7b56\u6240\u5fc5\u9700\u7684\u3002\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u63d0\u4f9b\u989d\u5916\u652f\u6301\u3002", "method": "\u5728\u7ea622,000\u5f20\u4eba\u5de5\u6807\u8bb0\u7684\u9053\u8def\u6444\u50cf\u5934\u56fe\u50cf\u4e0a\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u5c06\u56fe\u50cf\u5206\u7c7b\u4e3a\u516d\u79cd\u9053\u8def\u8868\u9762\u72b6\u51b5\uff1a\u4e25\u91cd\u79ef\u96ea\u3001\u79ef\u96ea\u3001\u6f6e\u6e7f\u3001\u5e72\u71e5\u3001\u80fd\u89c1\u5ea6\u5dee\u6216\u88ab\u906e\u6321\u3002", "result": "\u6a21\u578b\u5728\u5b8c\u5168\u672a\u89c1\u8fc7\u7684\u6444\u50cf\u5934\u4e0a\u5b9e\u73b0\u4e8681.5%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u5148\u8003\u8651\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u6ee1\u8db3NYSDOT\u51b3\u7b56\u8005\u7684\u8fd0\u8425\u9700\u6c42\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5206\u7c7b\u9053\u8def\u8868\u9762\u72b6\u51b5\uff0c\u4e3a\u4ea4\u901a\u90e8\u95e8\u7684\u51ac\u5b63\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2510.06391", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06391", "abs": "https://arxiv.org/abs/2510.06391", "authors": ["Elle"], "title": "Reward Model Perspectives: Whose Opinions Do Reward Models Reward?", "comment": "Published at EMNLP 2025 under the full author name \"Elle\"", "summary": "Reward models (RMs) are central to the alignment of language models (LMs). An\nRM often serves as a proxy for human preferences to guide downstream LM\nbehavior. However, our understanding of RM behavior is limited. Our work (i)\nformalizes a framework for measuring the alignment of opinions captured by RMs,\n(ii) investigates the extent to which RMs demonstrate sociodemographic biases,\nand (iii) explores the effects of prompting to steer rewards towards the\npreferences of a target group. We study the subjective and diverse perspectives\non controversial topics, which allows us to quantify RM perspectives in terms\nof their opinions, attitudes, and values. We show that RMs are poorly aligned\nwith several demographic groups and can systematically reward harmful\nstereotypes, and steering alone is not enough to overcome these limitations.\nOur findings underscore the need for more careful consideration of RM behavior\nin model alignment during preference learning to prevent the propagation of\nunwanted social biases in the language technologies that we use.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5956\u52b1\u6a21\u578b(RMs)\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0RMs\u4e0e\u591a\u4e2a\u7fa4\u4f53\u504f\u597d\u4e0d\u4e00\u81f4\uff0c\u4f1a\u7cfb\u7edf\u6027\u5956\u52b1\u6709\u5bb3\u523b\u677f\u5370\u8c61\uff0c\u4e14\u63d0\u793a\u5f15\u5bfc\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u7406\u89e3\u5956\u52b1\u6a21\u578b\u7684\u884c\u4e3a\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5728\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u6765\u6539\u5584\u6a21\u578b\u4e0e\u76ee\u6807\u7fa4\u4f53\u504f\u597d\u7684\u5bf9\u9f50\u3002", "method": "\u5efa\u7acb\u8861\u91cf\u5956\u52b1\u6a21\u578b\u89c2\u70b9\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u7814\u7a76RMs\u7684\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u7a0b\u5ea6\uff0c\u63a2\u7d22\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u5c06\u5956\u52b1\u5bfc\u5411\u76ee\u6807\u7fa4\u4f53\u504f\u597d\u7684\u6548\u679c\u3002", "result": "RMs\u4e0e\u591a\u4e2a\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u7684\u504f\u597d\u5bf9\u9f50\u5ea6\u5dee\uff0c\u4f1a\u7cfb\u7edf\u6027\u5956\u52b1\u6709\u5bb3\u523b\u677f\u5370\u8c61\uff0c\u4ec5\u9760\u63d0\u793a\u5f15\u5bfc\u65e0\u6cd5\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "conclusion": "\u5728\u504f\u597d\u5b66\u4e60\u4e2d\u9700\u8981\u66f4\u4ed4\u7ec6\u5730\u8003\u8651\u5956\u52b1\u6a21\u578b\u884c\u4e3a\uff0c\u4ee5\u9632\u6b62\u5728\u8bed\u8a00\u6280\u672f\u4e2d\u4f20\u64ad\u4e0d\u9700\u8981\u7684\u793e\u4f1a\u504f\u89c1\u3002"}}
{"id": "2510.07038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07038", "abs": "https://arxiv.org/abs/2510.07038", "authors": ["Wenxun Wu", "Yuanyang Li", "Guhan Chen", "Linyue Wang", "Hongyang Chen"], "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.", "AI": {"tldr": "\u63d0\u51faTAPO\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u591a\u8df3\u63a8\u7406\u4e0e\u81ea\u9002\u5e94\u5de5\u5177\u8c03\u7528\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u548c\u6570\u5b66\u8ba1\u7b97\u7684\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u6700\u65b0\u77e5\u8bc6\u6216\u8ba1\u7b97\u5de5\u5177\uff08\u5982\u8ba1\u7b97\u5668\u548c\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u7684\u590d\u6742\u7b97\u672f\u8fd0\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u52a8\u6001\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff08DAPO\uff09\u7684\u6539\u8fdb\u7248\u672c\uff0c\u4e13\u95e8\u9488\u5bf9\u5de5\u5177\u8c03\u7528\u573a\u666f\u8fdb\u884c\u9002\u914d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u52a8\u6001\u4ea4\u7ec7\u590d\u6742\u63a8\u7406\u4e0e\u6309\u9700\u5de5\u5177\u4f7f\u7528\uff08\u5305\u62ec\u641c\u7d22API\u548cPython\u89e3\u91ca\u5668\uff09\u3002", "result": "\u5728Qwen2.5-3B\u548cQwen2.5-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u548c\u6570\u5b66\u8ba1\u7b97\u7684\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u5229\u7528\u5de5\u5177\uff0c\u540c\u65f6\u9632\u6b62\u56e0\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u5bfc\u81f4\u7684\u8fc7\u5ea6\u8c03\u7528\u3002", "conclusion": "\u7ed3\u5408\u9ad8\u7ea7\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.06512", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06512", "abs": "https://arxiv.org/abs/2510.06512", "authors": ["Avishree Khare", "Hideki Okamoto", "Bardh Hoxha", "Georgios Fainekos", "Rajeev Alur"], "title": "LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval", "comment": null, "summary": "Neural models such as YOLO and HuBERT can be used to detect local properties\nsuch as objects (\"car\") and emotions (\"angry\") in individual frames of videos\nand audio clips respectively. The likelihood of these detections is indicated\nby scores in [0, 1]. Lifting these scores to temporal properties over sequences\ncan be useful for several downstream applications such as query matching (e.g.,\n\"does the speaker eventually sound happy in this audio clip?\"), and ranked\nretrieval (e.g., \"retrieve top 5 videos with a 10 second scene where a car is\ndetected until a pedestrian is detected\"). In this work, we formalize this\nproblem of assigning Scores for TempOral Properties (STOPs) over sequences,\ngiven potentially noisy score predictors for local properties. We then propose\na scoring function called LogSTOP that can efficiently compute these scores for\ntemporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,\nwith YOLO and HuBERT, outperforms Large Vision / Audio Language Models and\nother Temporal Logic-based baselines by at least 16% on query matching with\ntemporal properties over objects-in-videos and emotions-in-speech respectively.\nSimilarly, on ranked retrieval with temporal properties over objects and\nactions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a\n19% and 16% increase in mean average precision and recall over zero-shot\ntext-to-video retrieval baselines respectively.", "AI": {"tldr": "\u63d0\u51faLogSTOP\u8bc4\u5206\u51fd\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u65f6\u95f4\u5c5e\u6027\u5728\u5e8f\u5217\u4e0a\u7684\u5f97\u5206\uff0c\u57fa\u4e8e\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff0c\u5728\u89c6\u9891\u548c\u97f3\u9891\u5206\u6790\u4e2d\u4f18\u4e8e\u5927\u578b\u89c6\u89c9/\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5c06YOLO\u3001HuBERT\u7b49\u795e\u7ecf\u6a21\u578b\u68c0\u6d4b\u7684\u5c40\u90e8\u5c5e\u6027\u5f97\u5206\u63d0\u5347\u5230\u65f6\u95f4\u5c5e\u6027\u5c42\u9762\uff0c\u652f\u6301\u4e0b\u6e38\u5e94\u7528\u5982\u67e5\u8be2\u5339\u914d\u548c\u6392\u5e8f\u68c0\u7d22\u3002", "method": "\u63d0\u51faLogSTOP\u8bc4\u5206\u51fd\u6570\uff0c\u80fd\u9ad8\u6548\u8ba1\u7b97\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u8868\u793a\u7684\u65f6\u95f4\u5c5e\u6027\u5f97\u5206\uff0c\u5904\u7406\u53ef\u80fd\u542b\u6709\u566a\u58f0\u7684\u5c40\u90e8\u5c5e\u6027\u9884\u6d4b\u5668\u3002", "result": "\u5728\u89c6\u9891\u5bf9\u8c61\u548c\u8bed\u97f3\u60c5\u611f\u7684\u65f6\u95f4\u5c5e\u6027\u67e5\u8be2\u5339\u914d\u4e2d\uff0cLogSTOP\u6bd4\u5927\u578b\u89c6\u89c9/\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u548c\u5176\u4ed6\u65f6\u5e8f\u903b\u8f91\u57fa\u7ebf\u81f3\u5c11\u63d0\u534716%\uff1b\u5728\u89c6\u9891\u5bf9\u8c61\u548c\u52a8\u4f5c\u7684\u6392\u5e8f\u68c0\u7d22\u4e2d\uff0c\u6bd4\u96f6\u6837\u672c\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u57fa\u7ebf\u5728\u5e73\u5747\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u5206\u522b\u63d0\u5347\u81f3\u5c1119%\u548c16%\u3002", "conclusion": "LogSTOP\u80fd\u6709\u6548\u63d0\u5347\u65f6\u95f4\u5c5e\u6027\u5728\u5e8f\u5217\u4e0a\u7684\u8bc4\u5206\u6027\u80fd\uff0c\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.07161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07161", "abs": "https://arxiv.org/abs/2510.07161", "authors": ["Ali Norouzifar", "Humam Kourani", "Marcus Dees", "Wil van der Aalst"], "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models", "comment": "This paper is currently under review for publication in a journal", "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6d41\u7a0b\u53d1\u73b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u9886\u57df\u4e13\u5bb6\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u58f0\u660e\u6027\u89c4\u5219\uff0c\u6307\u5bfc\u6d41\u7a0b\u6a21\u578b\u53d1\u73b0\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u4ec5\u57fa\u4e8e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u6d41\u7a0b\u53d1\u73b0\u6a21\u578b\u53ef\u80fd\u4e0d\u51c6\u786e\uff0c\u56e0\u4e3a\u4e8b\u4ef6\u65e5\u5fd7\u901a\u5e38\u4e0d\u5b8c\u6574\u6216\u5305\u542b\u566a\u58f0\uff0c\u4e14\u5ffd\u7565\u4e86\u9886\u57df\u77e5\u8bc6\u8fd9\u4e00\u91cd\u8981\u8865\u5145\u8d44\u6e90\u3002", "method": "\u4f7f\u7528LLM\u4ece\u9886\u57df\u4e13\u5bb6\u7684\u6587\u672c\u63cf\u8ff0\u4e2d\u63d0\u53d6\u58f0\u660e\u6027\u89c4\u5219\uff0c\u6307\u5bfcIMr\u53d1\u73b0\u7b97\u6cd5\u9012\u5f52\u6784\u5efa\u6d41\u7a0b\u6a21\u578b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u65e5\u5fd7\u548c\u63d0\u53d6\u7684\u89c4\u5219\u907f\u514d\u4e0e\u9886\u57df\u77e5\u8bc6\u51b2\u7a81\u7684\u95ee\u9898\u7ed3\u6784\u3002", "result": "\u5f00\u53d1\u4e86\u5b8c\u5168\u5b9e\u73b0\u7684\u5de5\u5177\u652f\u6301\u8be5\u5de5\u4f5c\u6d41\uff0c\u5bf9\u591a\u4e2aLLM\u548c\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5305\u62ec\u57fa\u4e8e\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u6848\u4f8b\u7814\u7a76\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u63d0\u9ad8\u4e86\u6d41\u7a0b\u53d1\u73b0\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u9886\u57df\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5176\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2510.05336", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05336", "abs": "https://arxiv.org/abs/2510.05336", "authors": ["Yongan Yu", "Xianda Du", "Qingchen Hu", "Jiahao Liang", "Jingwei Ni", "Dan Qiang", "Kaiyu Huang", "Grant McKenzie", "Renee Sieber", "Fengran Mo"], "title": "WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives", "comment": null, "summary": "Historical archives on weather events are collections of enduring primary\nsource records that offer rich, untapped narratives of how societies have\nexperienced and responded to extreme weather events. These qualitative accounts\nprovide insights into societal vulnerability and resilience that are largely\nabsent from meteorological records, making them valuable for climate scientists\nto understand societal responses. However, their vast scale, noisy digitized\nquality, and archaic language make it difficult to transform them into\nstructured knowledge for climate research. To address this challenge, we\nintroduce WeatherArchive-Bench, the first benchmark for evaluating\nretrieval-augmented generation (RAG) systems on historical weather archives.\nWeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which\nmeasures a system's ability to locate historically relevant passages from over\none million archival news segments, and WeatherArchive-Assessment, which\nevaluates whether Large Language Models (LLMs) can classify societal\nvulnerability and resilience indicators from extreme weather narratives.\nExtensive experiments across sparse, dense, and re-ranking retrievers, as well\nas a diverse set of LLMs, reveal that dense retrievers often fail on historical\nterminology, while LLMs frequently misinterpret vulnerability and resilience\nconcepts. These findings highlight key limitations in reasoning about complex\nsocietal indicators and provide insights for designing more robust\nclimate-focused RAG systems from archival contexts. The constructed dataset and\nevaluation framework are publicly available at\nhttps://anonymous.4open.science/r/WeatherArchive-Bench/.", "AI": {"tldr": "\u63d0\u51fa\u4e86WeatherArchive-Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5386\u53f2\u5929\u6c14\u6863\u6848\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u5305\u542b\u68c0\u7d22\u548c\u8bc4\u4f30\u4e24\u4e2a\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5386\u53f2\u672f\u8bed\u548c\u793e\u4f1a\u6307\u6807\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5386\u53f2\u5929\u6c14\u6863\u6848\u5305\u542b\u4e30\u5bcc\u7684\u793e\u4f1a\u8106\u5f31\u6027\u548c\u97e7\u6027\u4fe1\u606f\uff0c\u4f46\u7531\u4e8e\u89c4\u6a21\u5e9e\u5927\u3001\u6570\u5b57\u5316\u8d28\u91cf\u5dee\u548c\u8bed\u8a00\u53e4\u65e7\uff0c\u96be\u4ee5\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u7528\u4e8e\u6c14\u5019\u7814\u7a76\u3002", "method": "\u6784\u5efaWeatherArchive-Bench\u57fa\u51c6\uff0c\u5305\u542bWeatherArchive-Retrieval\uff08\u4ece\u767e\u4e07\u6863\u6848\u65b0\u95fb\u6bb5\u843d\u4e2d\u68c0\u7d22\u76f8\u5173\u6bb5\u843d\uff09\u548cWeatherArchive-Assessment\uff08\u8bc4\u4f30LLM\u5bf9\u793e\u4f1a\u8106\u5f31\u6027\u548c\u97e7\u6027\u6307\u6807\u7684\u5206\u7c7b\u80fd\u529b\uff09\u4e24\u4e2a\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u7a20\u5bc6\u68c0\u7d22\u5668\u5728\u5386\u53f2\u672f\u8bed\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cLLM\u7ecf\u5e38\u8bef\u89e3\u8106\u5f31\u6027\u548c\u97e7\u6027\u6982\u5ff5\uff0c\u63ed\u793a\u4e86\u5728\u590d\u6742\u793e\u4f1a\u6307\u6807\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u7a33\u5065\u7684\u6c14\u5019\u5bfc\u5411RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6784\u5efa\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.06762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06762", "abs": "https://arxiv.org/abs/2510.06762", "authors": ["Shivam Padmani", "Akshay Joshi"], "title": "Function regression using the forward forward training and inferring paradigm", "comment": "Keywords: Neural Networks, Forward Forward training, Function\n  Regression, Physical Neural Networks, Analog Computing", "summary": "Function regression/approximation is a fundamental application of machine\nlearning. Neural networks (NNs) can be easily trained for function regression\nusing a sufficient number of neurons and epochs. The forward-forward learning\nalgorithm is a novel approach for training neural networks without\nbackpropagation, and is well suited for implementation in neuromorphic\ncomputing and physical analogs for neural networks. To the best of the authors'\nknowledge, the Forward Forward paradigm of training and inferencing NNs is\ncurrently only restricted to classification tasks. This paper introduces a new\nmethodology for approximating functions (function regression) using the\nForward-Forward algorithm. Furthermore, the paper evaluates the developed\nmethodology on univariate and multivariate functions, and provides preliminary\nstudies of extending the proposed Forward-Forward regression to Kolmogorov\nArnold Networks, and Deep Physical Neural Networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Forward-Forward\u7b97\u6cd5\u8fdb\u884c\u51fd\u6570\u56de\u5f52\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u51fd\u6570\u4e0a\u7684\u6027\u80fd\uff0c\u8fd8\u521d\u6b65\u7814\u7a76\u4e86\u5c06\u8be5\u65b9\u6cd5\u6269\u5c55\u5230Kolmogorov Arnold\u7f51\u7edc\u548c\u6df1\u5ea6\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u51fd\u6570\u56de\u5f52\u662f\u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u5e94\u7528\uff0c\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u5927\u91cf\u795e\u7ecf\u5143\u548c\u8bad\u7ec3\u5468\u671f\u3002Forward-Forward\u7b97\u6cd5\u662f\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u65b0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f46\u76ee\u524d\u4ec5\u9650\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u9700\u8981\u5c06\u5176\u6269\u5c55\u5230\u51fd\u6570\u56de\u5f52\u9886\u57df\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eForward-Forward\u7b97\u6cd5\u7684\u51fd\u6570\u56de\u5f52\u65b9\u6cd5\uff0c\u5728\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u51fd\u6570\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63a2\u7d22\u4e86\u8be5\u65b9\u6cd5\u5728Kolmogorov Arnold\u7f51\u7edc\u548c\u6df1\u5ea6\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6269\u5c55\u5e94\u7528\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4f7f\u7528Forward-Forward\u7b97\u6cd5\u8fdb\u884c\u51fd\u6570\u56de\u5f52\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u51fd\u6570\u8fd1\u4f3c\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Forward-Forward\u7b97\u6cd5\u53ef\u4ee5\u6210\u529f\u5e94\u7528\u4e8e\u51fd\u6570\u56de\u5f52\u4efb\u52a1\uff0c\u4e3a\u5728\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u548c\u7269\u7406\u795e\u7ecf\u7f51\u7edc\u4e2d\u5b9e\u73b0\u51fd\u6570\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.06820", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06820", "abs": "https://arxiv.org/abs/2510.06820", "authors": ["Mitchell Keren Taraday", "Shahaf Wagner", "Chaim Baskin"], "title": "Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking", "comment": "preprint", "summary": "Multimodal retrieval still leans on embedding-based models like CLIP for fast\nvector search over pre-computed image embeddings. Yet, unlike text retrieval,\nwhere joint-encoder rerankers are standard, comparable vision--language\nrerankers are largely absent. We find that seminal joint encoders such as BLIP\nare severely bottlenecked by an expensive visual feature-extraction stage,\npreventing practical deployment at scale. Motivated by this bottleneck, we\nintroduce EDJE, an Efficient Discriminative Joint Encoder that precomputes\nvision tokens offline and compresses them via a lightweight attention-based\nadapter, so online inference runs only a compact joint encoder over a small set\nof visual tokens plus the text. EDJE preserves strong retrieval performance\nwhile drastically reducing storage and online compute, enabling high-throughput\ninference. Specifically, EDJE processes 50k image--text pairs/second while\nrequiring 49kB of disk storage per image, matching prior art on Flickr\n(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints\nwill be made publicly available shortly.", "AI": {"tldr": "EDJE\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5224\u522b\u6027\u8054\u5408\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u89c6\u89c9token\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u9002\u914d\u5668\u538b\u7f29\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u68c0\u7d22\u4e3b\u8981\u4f9d\u8d56CLIP\u7b49\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u5411\u91cf\u641c\u7d22\uff0c\u4f46\u7f3a\u4e4f\u7c7b\u4f3c\u6587\u672c\u68c0\u7d22\u4e2d\u7684\u8054\u5408\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u5668\u3002\u73b0\u6709\u8054\u5408\u7f16\u7801\u5668\u5982BLIP\u7531\u4e8e\u6602\u8d35\u7684\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\u800c\u5b58\u5728\u74f6\u9888\uff0c\u65e0\u6cd5\u5728\u5b9e\u9645\u89c4\u6a21\u90e8\u7f72\u3002", "method": "EDJE\u9884\u8ba1\u7b97\u89c6\u89c9token\u79bb\u7ebf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u9002\u914d\u5668\u538b\u7f29\uff0c\u5728\u7ebf\u63a8\u7406\u65f6\u4ec5\u8fd0\u884c\u7d27\u51d1\u7684\u8054\u5408\u7f16\u7801\u5668\u5904\u7406\u5c11\u91cf\u89c6\u89c9token\u548c\u6587\u672c\u3002", "result": "EDJE\u5904\u7406\u901f\u5ea6\u8fbe50k\u56fe\u50cf-\u6587\u672c\u5bf9/\u79d2\uff0c\u6bcf\u5f20\u56fe\u50cf\u4ec5\u970049kB\u78c1\u76d8\u5b58\u50a8\uff0c\u5728Flickr\uff08\u96f6\u6837\u672c\uff09\u548cCOCO\uff08\u5fae\u8c03\uff09\u68c0\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "EDJE\u5728\u4fdd\u6301\u5f3a\u5927\u68c0\u7d22\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u5728\u7ebf\u8ba1\u7b97\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\u63a8\u7406\uff0c\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06827", "abs": "https://arxiv.org/abs/2510.06827", "authors": ["Jaeseok Jeong", "Junho Kim", "Gayoung Lee", "Yunjey Choi", "Youngjung Uh"], "title": "StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance", "comment": "Accepted to ICCV 2025; CVPRW AI4CC 2024 (Best Paper + Oral)", "summary": "In the domain of text-to-image generation, diffusion models have emerged as\npowerful tools. Recently, studies on visual prompting, where images are used as\nprompts, have enabled more precise control over style and content. However,\nexisting methods often suffer from content leakage, where undesired elements of\nthe visual style prompt are transferred along with the intended style. To\naddress this issue, we 1) extend classifier-free guidance (CFG) to utilize\nswapping self-attention and propose 2) negative visual query guidance (NVQG) to\nreduce the transfer of unwanted contents. NVQG employs negative score by\nintentionally simulating content leakage scenarios that swap queries instead of\nkey and values of self-attention layers from visual style prompts. This simple\nyet effective method significantly reduces content leakage. Furthermore, we\nprovide careful solutions for using a real image as visual style prompts.\nThrough extensive evaluation across various styles and text prompts, our method\ndemonstrates superiority over existing approaches, reflecting the style of the\nreferences, and ensuring that resulting images match the text prompts. Our code\nis available \\href{https://github.com/naver-ai/StyleKeeper}{here}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8d1f\u89c6\u89c9\u67e5\u8be2\u6307\u5bfc\uff08NVQG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u5e76\u5f15\u5165\u8d1f\u5206\u6570\u6765\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u63d0\u793a\u65b9\u6cd5\u5728\u63a7\u5236\u98ce\u683c\u548c\u5185\u5bb9\u65f6\u7ecf\u5e38\u51fa\u73b0\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\uff0c\u5373\u89c6\u89c9\u98ce\u683c\u63d0\u793a\u4e2d\u4e0d\u5e0c\u671b\u7684\u5143\u7d20\u88ab\u610f\u5916\u8f6c\u79fb\u5230\u751f\u6210\u56fe\u50cf\u4e2d\u3002", "method": "1\uff09\u6269\u5c55\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u4ee5\u5229\u7528\u4ea4\u6362\u81ea\u6ce8\u610f\u529b\uff1b2\uff09\u63d0\u51fa\u8d1f\u89c6\u89c9\u67e5\u8be2\u6307\u5bfc\uff08NVQG\uff09\uff0c\u901a\u8fc7\u6545\u610f\u6a21\u62df\u5185\u5bb9\u6cc4\u6f0f\u573a\u666f\u6765\u51cf\u5c11\u4e0d\u9700\u8981\u5185\u5bb9\u7684\u4f20\u8f93\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u98ce\u683c\u548c\u6587\u672c\u63d0\u793a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u53cd\u6620\u53c2\u8003\u56fe\u50cf\u7684\u98ce\u683c\uff0c\u540c\u65f6\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u5339\u914d\u3002", "conclusion": "NVQG\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\uff0c\u4e3a\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u4f5c\u4e3a\u89c6\u89c9\u98ce\u683c\u63d0\u793a\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06829", "abs": "https://arxiv.org/abs/2510.06829", "authors": ["Mikihiro Ikura", "Arren Glover", "Masayoshi Mizuno", "Chiara Bartolozzi"], "title": "Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera", "comment": "12 pages, 13 figures, 6 tables, ICCV Workshop NeVi2025", "summary": "Line segment extraction is effective for capturing geometric features of\nhuman-made environments. Event-based cameras, which asynchronously respond to\ncontrast changes along edges, enable efficient extraction by reducing redundant\ndata. However, recent methods often rely on additional frame cameras or\nstruggle with high event rates. This research addresses real-time line segment\ndetection and tracking using only a modern, high-resolution (i.e., high event\nrate) event-based camera. Our lattice-allocated pipeline consists of (i)\nvelocity-invariant event representation, (ii) line segment detection based on a\nfitting score, (iii) and line segment tracking by perturbating endpoints.\nEvaluation using ad-hoc recorded dataset and public datasets demonstrates\nreal-time performance and higher accuracy compared to state-of-the-art\nevent-only and event-frame hybrid baselines, enabling fully stand-alone event\ncamera operation in real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u4e8b\u4ef6\u76f8\u673a\u5b9e\u65f6\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7ebf\u6bb5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u901f\u5ea6\u4e0d\u53d8\u4e8b\u4ef6\u8868\u793a\u3001\u57fa\u4e8e\u62df\u5408\u5f97\u5206\u7684\u7ebf\u6bb5\u68c0\u6d4b\u548c\u7aef\u70b9\u6270\u52a8\u7684\u7ebf\u6bb5\u8ddf\u8e2a\uff0c\u5728\u5b9e\u65f6\u6027\u548c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u80fd\u6709\u6548\u6355\u6349\u4eba\u9020\u73af\u5883\u7684\u51e0\u4f55\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u989d\u5916\u7684\u5e27\u76f8\u673a\uff0c\u8981\u4e48\u96be\u4ee5\u5904\u7406\u9ad8\u4e8b\u4ef6\u7387\u3002\u672c\u7814\u7a76\u65e8\u5728\u4ec5\u4f7f\u7528\u73b0\u4ee3\u9ad8\u5206\u8fa8\u7387\u4e8b\u4ef6\u76f8\u673a\u5b9e\u73b0\u5b9e\u65f6\u7ebf\u6bb5\u68c0\u6d4b\u548c\u8ddf\u8e2a\u3002", "method": "\u91c7\u7528\u683c\u70b9\u5206\u914d\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\uff1a(i)\u901f\u5ea6\u4e0d\u53d8\u4e8b\u4ef6\u8868\u793a\uff0c(ii)\u57fa\u4e8e\u62df\u5408\u5f97\u5206\u7684\u7ebf\u6bb5\u68c0\u6d4b\uff0c(iii)\u901a\u8fc7\u7aef\u70b9\u6270\u52a8\u8fdb\u884c\u7ebf\u6bb5\u8ddf\u8e2a\u3002", "result": "\u5728\u4e13\u95e8\u8bb0\u5f55\u7684\u6570\u636e\u96c6\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u65f6\u6027\u80fd\uff0c\u4e14\u6bd4\u6700\u5148\u8fdb\u7684\u4e8b\u4ef6\u4e13\u7528\u548c\u4e8b\u4ef6-\u5e27\u6df7\u5408\u57fa\u7ebf\u65b9\u6cd5\u7cbe\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u5168\u72ec\u7acb\u7684\u4e8b\u4ef6\u76f8\u673a\u64cd\u4f5c\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u65f6\u8fd0\u884c\u3002"}}
{"id": "2510.06855", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.06855", "abs": "https://arxiv.org/abs/2510.06855", "authors": ["Hyungrok Jung", "Daneul Kim", "Seunggyun Lim", "Jeany Son", "Jonghyun Choi"], "title": "Online Generic Event Boundary Detection", "comment": "ICCV 2025", "summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos\nthrough the lens of human perception. However, current GEBD methods require\nprocessing complete video frames to make predictions, unlike humans processing\ndata online and in real-time. To bridge this gap, we introduce a new task,\nOnline Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries\nof generic events immediately in streaming videos. This task faces unique\nchallenges of identifying subtle, taxonomy-free event changes in real-time,\nwithout the access to future frames. To tackle these challenges, we propose a\nnovel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)\nwhich explains how humans segment ongoing activity into events by leveraging\nthe discrepancies between predicted and actual information. Our framework\nconsists of two key components: the Consistent Event Anticipator (CEA), and the\nOnline Boundary Discriminator (OBD). Specifically, the CEA generates a\nprediction of the future frame reflecting current event dynamics based solely\non prior frames. Then, the OBD measures the prediction error and adaptively\nadjusts the threshold using statistical tests on past errors to capture\ndiverse, subtle event transitions. Experimental results demonstrate that\nEstimator outperforms all baselines adapted from recent online video\nunderstanding models and achieves performance comparable to prior offline-GEBD\nmethods on the Kinetics-GEBD and TAPOS datasets.", "AI": {"tldr": "\u63d0\u51fa\u5728\u7ebf\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u4efb\u52a1(On-GEBD)\uff0c\u5f00\u53d1Estimator\u6846\u67b6\u5b9e\u65f6\u68c0\u6d4b\u6d41\u5a92\u4f53\u89c6\u9891\u4e2d\u7684\u4e8b\u4ef6\u8fb9\u754c\uff0c\u65e0\u9700\u672a\u6765\u5e27\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709GEBD\u65b9\u6cd5\u9700\u8981\u5904\u7406\u5b8c\u6574\u89c6\u9891\u5e27\uff0c\u800c\u4eba\u7c7b\u80fd\u591f\u5728\u7ebf\u5b9e\u65f6\u5904\u7406\u6570\u636e\u3002\u4e3a\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5f15\u5165\u5728\u7ebf\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u4e8b\u4ef6\u5206\u5272\u7406\u8bba\uff0c\u63d0\u51faEstimator\u6846\u67b6\uff0c\u5305\u542b\u4e00\u81f4\u6027\u4e8b\u4ef6\u9884\u6d4b\u5668(CEA)\u548c\u5728\u7ebf\u8fb9\u754c\u5224\u522b\u5668(OBD)\u3002CEA\u57fa\u4e8e\u5386\u53f2\u5e27\u9884\u6d4b\u672a\u6765\u5e27\uff0cOBD\u6d4b\u91cf\u9884\u6d4b\u8bef\u5dee\u5e76\u901a\u8fc7\u7edf\u8ba1\u6d4b\u8bd5\u81ea\u9002\u5e94\u8c03\u6574\u9608\u503c\u3002", "result": "\u5728Kinetics-GEBD\u548cTAPOS\u6570\u636e\u96c6\u4e0a\uff0cEstimator\u4f18\u4e8e\u6240\u6709\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u4e0e\u79bb\u7ebfGEBD\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684On-GEBD\u4efb\u52a1\u548cEstimator\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.06831", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.06831", "abs": "https://arxiv.org/abs/2510.06831", "authors": ["Syed Shazaib Shah", "Daoliang Tan"], "title": "Early wind turbine alarm prediction based on machine learning: AlarmForecasting", "comment": "International Journal of Electrical Power and Energy Systems", "summary": "Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and\nforms the backbone for advancedpredictive monitoring systems. Traditionally,\nresearch cohorts have been confined to utilizing alarm data solelyas a\ndiagnostic tool, merely indicative of unhealthy status. However, this study\naims to offer a transformativeleap towards preempting alarms, preventing alarms\nfrom triggering altogether, and consequently avertingimpending failures. Our\nproposed Alarm Forecasting and Classification (AFC) framework is designed on\ntwosuccessive modules: first, the regression module based on long short-term\nmemory (LSTM) for time-series alarmforecasting, and thereafter, the\nclassification module to implement alarm tagging on the forecasted alarm.\nThisway, the entire alarm taxonomy can be forecasted reliably rather than a few\nspecific alarms. 14 Senvion MM82turbines with an operational period of 5 years\nare used as a case study; the results demonstrated 82%, 52%,and 41% accurate\nforecasts for 10, 20, and 30 min alarm forecasts, respectively. The results\nsubstantiateanticipating and averting alarms, which is significant in curbing\nalarm frequency and enhancing operationalefficiency through proactive\nintervention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8b66\u62a5\u9884\u6d4b\u548c\u5206\u7c7b\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u524d\u9884\u6d4b\u98ce\u529b\u6da1\u8f6e\u673a\u8b66\u62a5\uff0c\u9632\u6b62\u8b66\u62a5\u89e6\u53d1\u548c\u6545\u969c\u53d1\u751f\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u4ec5\u5c06\u8b66\u62a5\u6570\u636e\u7528\u4f5c\u8bca\u65ad\u5de5\u5177\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u8b66\u62a5\u9884\u9632\uff0c\u5728\u8b66\u62a5\u89e6\u53d1\u524d\u8fdb\u884c\u5e72\u9884\uff0c\u907f\u514d\u6545\u969c\u53d1\u751f\u3002", "method": "\u57fa\u4e8eLSTM\u7684\u56de\u5f52\u6a21\u5757\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u8b66\u62a5\u9884\u6d4b\uff0c\u7136\u540e\u4f7f\u7528\u5206\u7c7b\u6a21\u5757\u5bf9\u9884\u6d4b\u7684\u8b66\u62a5\u8fdb\u884c\u6807\u8bb0\uff0c\u5b9e\u73b0\u5b8c\u6574\u8b66\u62a5\u5206\u7c7b\u4f53\u7cfb\u7684\u9884\u6d4b\u3002", "result": "\u572814\u53f0Senvion MM82\u6da1\u8f6e\u673a5\u5e74\u8fd0\u884c\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c10\u300120\u300130\u5206\u949f\u8b66\u62a5\u9884\u6d4b\u51c6\u786e\u7387\u5206\u522b\u4e3a82%\u300152%\u548c41%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u9884\u6d4b\u548c\u907f\u514d\u8b66\u62a5\uff0c\u663e\u8457\u964d\u4f4e\u8b66\u62a5\u9891\u7387\uff0c\u901a\u8fc7\u4e3b\u52a8\u5e72\u9884\u63d0\u9ad8\u8fd0\u884c\u6548\u7387\u3002"}}
{"id": "2510.06917", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06917", "abs": "https://arxiv.org/abs/2510.06917", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Shujie Liu", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models", "comment": "Work in progress", "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/", "AI": {"tldr": "SHANKS\u662f\u4e00\u4e2a\u63a8\u7406\u6846\u67b6\uff0c\u8ba9\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u7528\u6237\u8bf4\u8bdd\u65f6\u5c31\u80fd\u751f\u6210\u65e0\u58f0\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4f4e\u5ef6\u8fdf\u7684\u8bed\u97f3\u4ea4\u4e92\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u53ea\u5728\u7528\u6237\u8bf4\u5b8c\u540e\u624d\u5f00\u59cb\u601d\u8003\u548c\u884c\u52a8\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff0c\u4e0d\u9002\u5408\u9700\u8981\u5b9e\u65f6\u4ea4\u4e92\u7684\u8bed\u97f3\u5bf9\u8bdd\u573a\u666f\u3002", "method": "\u5c06\u8f93\u5165\u8bed\u97f3\u6d41\u5f0f\u5206\u5757\u5904\u7406\uff0c\u6bcf\u6536\u5230\u4e00\u4e2a\u8bed\u97f3\u5757\u5c31\u57fa\u4e8e\u4e4b\u524d\u6240\u6709\u8bed\u97f3\u548c\u63a8\u7406\u751f\u6210\u65e0\u58f0\u63a8\u7406\uff0c\u540c\u65f6\u51b3\u5b9a\u662f\u5426\u6253\u65ad\u7528\u6237\u6216\u8c03\u7528\u5de5\u5177\u3002", "result": "\u5728\u6570\u5b66\u95ee\u9898\u573a\u666f\u4e2d\uff0c\u6253\u65ad\u51c6\u786e\u7387\u6bd4\u65e0\u601d\u8003\u57fa\u7ebf\u9ad837.1%\uff1b\u5728\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\uff0c56.9%\u7684\u5de5\u5177\u8c03\u7528\u80fd\u5728\u7528\u6237\u8bf4\u5b8c\u524d\u5b8c\u6210\u3002", "conclusion": "SHANKS\u5b9e\u73b0\u4e86\u6a21\u578b\u5728\u6574\u4e2a\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u6301\u7eed\u601d\u8003\uff0c\u800c\u4e0d\u4ec5\u662f\u5728\u7528\u6237\u56de\u5408\u7ed3\u675f\u540e\u601d\u8003\uff0c\u63a8\u52a8\u4e86\u5b9e\u65f6\u8bed\u97f3\u4ea4\u4e92\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.06910", "categories": ["cs.LG", "I.2; I.5"], "pdf": "https://arxiv.org/pdf/2510.06910", "abs": "https://arxiv.org/abs/2510.06910", "authors": ["Iago Xabier V\u00e1zquez", "Javier Sedano", "Muhammad Afzal", "\u00c1ngel Miguel Garc\u00eda-Vico"], "title": "Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series", "comment": "53 pages, 16 figures, preprint submitted to a journal for review", "summary": "Anomaly detection is a key task across domains such as industry, healthcare,\nand cybersecurity. Many real-world anomaly detection problems involve analyzing\nmultiple features over time, making time series analysis a natural approach for\nsuch problems. While deep learning models have achieved strong performance in\nthis field, their trend to exhibit high energy consumption limits their\ndeployment in resource-constrained environments such as IoT devices, edge\ncomputing platforms, and wearables. To address this challenge, this paper\nintroduces the \\textit{Vacuum Spiker algorithm}, a novel Spiking Neural\nNetwork-based method for anomaly detection in time series. It incorporates a\nnew detection criterion that relies on global changes in neural activity rather\nthan reconstruction or prediction error. It is trained using Spike\nTime-Dependent Plasticity in a novel way, intended to induce changes in neural\nactivity when anomalies occur. A new efficient encoding scheme is also\nproposed, which discretizes the input space into non-overlapping intervals,\nassigning each to a single neuron. This strategy encodes information with a\nsingle spike per time step, improving energy efficiency compared to\nconventional encoding methods. Experimental results on publicly available\ndatasets show that the proposed algorithm achieves competitive performance\nwhile significantly reducing energy consumption, compared to a wide set of deep\nlearning and machine learning baselines. Furthermore, its practical utility is\nvalidated in a real-world case study, where the model successfully identifies\npower curtailment events in a solar inverter. These results highlight its\npotential for sustainable and efficient anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u578b\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5Vacuum Spiker\u7b97\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u795e\u7ecf\u6d3b\u52a8\u53d8\u5316\u8fdb\u884c\u68c0\u6d4b\uff0c\u76f8\u6bd4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u80fd\u8017\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982IoT\u8bbe\u5907\u3001\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\uff09\u4e2d\u7684\u90e8\u7f72\u9650\u5236\u3002", "method": "\u91c7\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u5f15\u5165\u65b0\u7684\u68c0\u6d4b\u6807\u51c6\uff08\u57fa\u4e8e\u5168\u5c40\u795e\u7ecf\u6d3b\u52a8\u53d8\u5316\u800c\u975e\u91cd\u6784\u6216\u9884\u6d4b\u8bef\u5dee\uff09\uff0c\u4f7f\u7528\u8109\u51b2\u65f6\u95f4\u4f9d\u8d56\u53ef\u5851\u6027\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7f16\u7801\u65b9\u6848\u5c06\u8f93\u5165\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u975e\u91cd\u53e0\u533a\u95f4\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff1b\u5728\u592a\u9633\u80fd\u9006\u53d8\u5668\u529f\u7387\u524a\u51cf\u4e8b\u4ef6\u68c0\u6d4b\u7684\u5b9e\u9645\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6301\u7eed\u548c\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u6f5c\u529b\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2510.07035", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07035", "abs": "https://arxiv.org/abs/2510.07035", "authors": ["Tengwei Song", "Min Wu", "Yuan Fang"], "title": "Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration", "comment": "CIKM 2025", "summary": "Molecular representation learning plays a crucial role in advancing\napplications such as drug discovery and material design. Existing work\nleverages 2D and 3D modalities of molecular information for pre-training,\naiming to capture comprehensive structural and geometric insights. However,\nthese methods require paired 2D and 3D molecular data to train the model\neffectively and prevent it from collapsing into a single modality, posing\nlimitations in scenarios where a certain modality is unavailable or\ncomputationally expensive to generate. To overcome this limitation, we propose\nFlexMol, a flexible molecule pre-training framework that learns unified\nmolecular representations while supporting single-modality input. Specifically,\ninspired by the unified structure in vision-language models, our approach\nemploys separate models for 2D and 3D molecular data, leverages parameter\nsharing to improve computational efficiency, and utilizes a decoder to generate\nfeatures for the missing modality. This enables a multistage continuous\nlearning process where both modalities contribute collaboratively during\ntraining, while ensuring robustness when only one modality is available during\ninference. Extensive experiments demonstrate that FlexMol achieves superior\nperformance across a wide range of molecular property prediction tasks, and we\nalso empirically demonstrate its effectiveness with incomplete data. Our code\nand data are available at https://github.com/tewiSong/FlexMol.", "AI": {"tldr": "FlexMol\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u5206\u5b50\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u5355\u6a21\u6001\u8f93\u5165\uff0c\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u548c\u89e3\u7801\u5668\u751f\u6210\u7f3a\u5931\u6a21\u6001\u7279\u5f81\uff0c\u5728\u591a\u79cd\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u914d\u5bf9\u76842D\u548c3D\u5206\u5b50\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5728\u67d0\u4e9b\u6a21\u6001\u4e0d\u53ef\u7528\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u65f6\u5b58\u5728\u9650\u5236\u3002", "method": "\u91c7\u7528\u5206\u79bb\u76842D\u548c3D\u6a21\u578b\uff0c\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u7528\u89e3\u7801\u5668\u751f\u6210\u7f3a\u5931\u6a21\u6001\u7279\u5f81\uff0c\u5b9e\u73b0\u591a\u9636\u6bb5\u8fde\u7eed\u5b66\u4e60\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u5b8c\u6574\u6570\u636e\u4e0b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "FlexMol\u80fd\u591f\u5b66\u4e60\u7edf\u4e00\u7684\u5206\u5b50\u8868\u793a\uff0c\u652f\u6301\u5355\u6a21\u6001\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\u3002"}}
