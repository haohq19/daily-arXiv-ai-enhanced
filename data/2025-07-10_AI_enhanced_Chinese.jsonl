{"id": "2507.06564", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06564", "abs": "https://arxiv.org/abs/2507.06564", "authors": ["Tianshun Li", "Tianyi Huai", "Zhen Li", "Yichun Gao", "Haoang Li", "Xinhu Zheng"], "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments", "comment": "8 pages, 9 figures, has been accepted by IROS 2025", "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.", "AI": {"tldr": "SkyVLN\u6846\u67b6\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u5728\u52a8\u60013D\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0cSkyVLN\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548cNMPC\uff0c\u63d0\u9ad8\u5bfc\u822a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "SkyVLN\u4f7f\u7528\u591a\u6a21\u6001\u5bfc\u822a\u4ee3\u7406\uff0c\u5305\u62ec\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u8bed\u8a00\u5316\u5668\u548c\u5386\u53f2\u8def\u5f84\u8bb0\u5fc6\u673a\u5236\uff0c\u7ed3\u5408NMPC\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u907f\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSkyVLN\u5728\u65b0\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "conclusion": "SkyVLN\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06415", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06415", "abs": "https://arxiv.org/abs/2507.06415", "authors": ["Zeming Chen", "Angelika Romanou", "Gail Weiss", "Antoine Bosselut"], "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning", "comment": "10 pages, 7 figures", "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.", "AI": {"tldr": "PERK\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u5b66\u4e60\u5c06\u957f\u4e0a\u4e0b\u6587\u7f16\u7801\u5230\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u4fe1\u606f\u68c0\u7d22\u7684\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u5143\u5b66\u4e60\u65b9\u6cd5\u7684\u9ad8\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5d4c\u5957\u4f18\u5316\u5faa\u73af\uff1a\u5185\u5faa\u73af\u5c06\u4e0a\u4e0b\u6587\u7f16\u7801\u5230\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\uff0c\u5916\u5faa\u73af\u5b66\u4e60\u5982\u4f55\u5229\u7528\u9002\u914d\u5668\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cPERK\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe90%\uff08\u5c0f\u6a21\u578b\uff09\u548c27%\uff08\u5927\u6a21\u578b\uff09\u3002", "conclusion": "PERK\u5728\u63a8\u7406\u65f6\u66f4\u9ad8\u6548\u4e14\u9c81\u68d2\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u590d\u6742\u63a8\u7406\u548c\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u3002"}}
{"id": "2507.06574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06574", "abs": "https://arxiv.org/abs/2507.06574", "authors": ["Thomas Touma", "Ersin Da\u015f", "Erica Tevere", "Martin Feather", "Ksenia Kolcio", "Maurice Prather", "Alberto Candela", "Ashish Goel", "Erik Kramer", "Hari Nayar", "Lorraine Fesq", "Joel W. Burdick"], "title": "AI Space Cortex: An Experimental System for Future Era Space Exploration", "comment": null, "summary": "Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)\neffort contributes to NASA's Concepts for Ocean worlds Life Detection\nTechnology (COLDTech) program, which explores science platform technologies for\nocean worlds such as Europa and Enceladus. Ocean world missions pose\nsignificant operational challenges. These include long communication lags,\nlimited power, and lifetime limitations caused by radiation damage and hostile\nconditions. Given these operational limitations, onboard autonomy will be vital\nfor future Ocean world missions. Besides the management of nominal lander\noperations, onboard autonomy must react appropriately in the event of\nanomalies. Traditional spacecraft rely on a transition into 'safe-mode' in\nwhich non-essential components and subsystems are powered off to preserve\nsafety and maintain communication with Earth. For a severely time-limited Ocean\nworld mission, resolutions to these anomalies that can be executed without\nEarth-in-the-loop communication and associated delays are paramount for\ncompletion of the mission objectives and science goals. To address these\nchallenges, the REASIMO effort aims to demonstrate a robust level of\nAI-assisted autonomy for such missions, including the ability to detect and\nrecover from anomalies, and to perform missions based on pre-trained behaviors\nrather than hard-coded, predetermined logic like all prior space missions. We\ndeveloped an AI-assisted, personality-driven, intelligent framework for control\nof an Ocean world mission by combining a mix of advanced technologies. To\ndemonstrate the capabilities of the framework, we perform tests of autonomous\nsampling operations on a lander-manipulator testbed at the NASA Jet Propulsion\nLaboratory, approximating possible surface conditions such a mission might\nencounter.", "AI": {"tldr": "REASIMO\u9879\u76ee\u65e8\u5728\u4e3aNASA\u7684COLDTech\u8ba1\u5212\u5f00\u53d1AI\u8f85\u52a9\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u51b0\u536b\u661f\u4efb\u52a1\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u548c\u6076\u52a3\u73af\u5883\u6311\u6218\u3002", "motivation": "\u51b0\u536b\u661f\u4efb\u52a1\uff08\u5982\u6b27\u7f57\u5df4\u548c\u6069\u514b\u62c9\u591a\u65af\uff09\u9762\u4e34\u901a\u4fe1\u5ef6\u8fdf\u3001\u80fd\u6e90\u9650\u5236\u548c\u8f90\u5c04\u7b49\u6311\u6218\uff0c\u9700\u8981\u81ea\u4e3b\u7cfb\u7edf\u4ee5\u5e94\u5bf9\u5f02\u5e38\u5e76\u5b8c\u6210\u4efb\u52a1\u76ee\u6807\u3002", "method": "\u7ed3\u5408AI\u6280\u672f\u548c\u9884\u8bad\u7ec3\u884c\u4e3a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u6062\u590d\u5f02\u5e38\uff0c\u5e76\u5728NASA\u55b7\u6c14\u63a8\u8fdb\u5b9e\u9a8c\u5ba4\u7684\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u81ea\u4e3b\u91c7\u6837\u64cd\u4f5c\u6d4b\u8bd5\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86AI\u8f85\u52a9\u81ea\u4e3b\u7cfb\u7edf\u5728\u6a21\u62df\u51b0\u536b\u661f\u8868\u9762\u6761\u4ef6\u4e0b\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "REASIMO\u6846\u67b6\u4e3a\u672a\u6765\u51b0\u536b\u661f\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u81ea\u4e3b\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5730\u7403\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u4efb\u52a1\u3002"}}
{"id": "2507.06450", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06450", "abs": "https://arxiv.org/abs/2507.06450", "authors": ["Xin Su", "Sungduk Yu", "Phillip Howard", "Steven Bethard"], "title": "A Semantic Parsing Framework for End-to-End Time Normalization", "comment": null, "summary": "Time normalization is the task of converting natural language temporal\nexpressions into machine-readable representations. It underpins many downstream\napplications in information retrieval, question answering, and clinical\ndecision-making. Traditional systems based on the ISO-TimeML schema limit\nexpressivity and struggle with complex constructs such as compositional,\nevent-relative, and multi-span time expressions. In this work, we introduce a\nnovel formulation of time normalization as a code generation task grounded in\nthe SCATE framework, which defines temporal semantics through symbolic and\ncompositional operators. We implement a fully executable SCATE Python library\nand demonstrate that large language models (LLMs) can generate executable SCATE\ncode. Leveraging this capability, we develop an automatic data augmentation\npipeline using LLMs to synthesize large-scale annotated data with code-level\nvalidation. Our experiments show that small, locally deployable models trained\non this augmented data can achieve strong performance, outperforming even their\nLLM parents and enabling practical, accurate, and interpretable time\nnormalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSCATE\u6846\u67b6\u7684\u65f6\u95f4\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u5b9e\u73b0\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eISO-TimeML\u7684\u7cfb\u7edf\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u65f6\u95f4\u8868\u8fbe\u5f0f\uff0c\u5982\u7ec4\u5408\u5f0f\u3001\u4e8b\u4ef6\u76f8\u5173\u548c\u591a\u8de8\u5ea6\u8868\u8fbe\u3002", "method": "\u5c06\u65f6\u95f4\u5f52\u4e00\u5316\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8eSCATE\u6846\u67b6\u7684\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5f00\u53d1\u4e86\u53ef\u6267\u884c\u7684SCATE Python\u5e93\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u3002\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u5408\u6210\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684\u5c0f\u578b\u672c\u5730\u90e8\u7f72\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5f52\u4e00\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65f6\u95f4\u5f52\u4e00\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u5b9e\u7528\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.06459", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06459", "abs": "https://arxiv.org/abs/2507.06459", "authors": ["Riadul Islam", "Joey Mul\u00e9", "Dhandeep Challagundla", "Shahmir Rizvi", "Sean Carson"], "title": "EA: An Event Autoencoder for High-Speed Vision Sensing", "comment": null, "summary": "High-speed vision sensing is essential for real-time perception in\napplications such as robotics, autonomous vehicles, and industrial automation.\nTraditional frame-based vision systems suffer from motion blur, high latency,\nand redundant data processing, limiting their performance in dynamic\nenvironments. Event cameras, which capture asynchronous brightness changes at\nthe pixel level, offer a promising alternative but pose challenges in object\ndetection due to sparse and noisy event streams. To address this, we propose an\nevent autoencoder architecture that efficiently compresses and reconstructs\nevent data while preserving critical spatial and temporal features. The\nproposed model employs convolutional encoding and incorporates adaptive\nthreshold selection and a lightweight classifier to enhance recognition\naccuracy while reducing computational complexity. Experimental results on the\nexisting Smart Event Face Dataset (SEFD) demonstrate that our approach achieves\ncomparable accuracy to the YOLO-v4 model while utilizing up to $35.5\\times$\nfewer parameters. Implementations on embedded platforms, including Raspberry Pi\n4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8\nFPS. The proposed classifier exhibits up to 87.84x better FPS than the\nstate-of-the-art and significantly improves event-based vision performance,\nmaking it ideal for low-power, high-speed applications in real-time edge\ncomputing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u538b\u7f29\u548c\u91cd\u5efa\u4e8b\u4ef6\u6570\u636e\uff0c\u63d0\u5347\u5b9e\u65f6\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u89c6\u89c9\u7cfb\u7edf\u5728\u9ad8\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u9ad8\u5ef6\u8fdf\u548c\u6570\u636e\u5197\u4f59\u95ee\u9898\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u867d\u80fd\u5f02\u6b65\u6355\u6349\u4eae\u5ea6\u53d8\u5316\uff0c\u4f46\u7a00\u758f\u548c\u566a\u58f0\u4e8b\u4ef6\u6d41\u5bf9\u76ee\u6807\u68c0\u6d4b\u6784\u6210\u6311\u6218\u3002", "method": "\u91c7\u7528\u5377\u79ef\u7f16\u7801\u7684\u4e8b\u4ef6\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u9608\u503c\u9009\u62e9\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002", "result": "\u5728SEFD\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u7cbe\u5ea6\u4e0eYOLO-v4\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u51cf\u5c1135.5\u500d\uff1b\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u5b9e\u73b08\u81f344.8 FPS\u7684\u9ad8\u5e27\u7387\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f87.84\u500d\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u89c6\u89c9\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u3001\u9ad8\u901f\u7684\u5b9e\u65f6\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u3002"}}
{"id": "2507.06750", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06750", "abs": "https://arxiv.org/abs/2507.06750", "authors": ["Tohid Kargar Tasooji", "Ramviyas Parasuraman"], "title": "Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments", "comment": "Accepted to IROS 2025 Conference", "summary": "In multi-robot systems (MRS), cooperative localization is a crucial task for\nenhancing system robustness and scalability, especially in GPS-denied or\ncommunication-limited environments. However, adversarial attacks, such as\nsensor manipulation, and communication jamming, pose significant challenges to\nthe performance of traditional localization methods. In this paper, we propose\na novel distributed fault-tolerant cooperative localization framework to\nenhance resilience against sensor and communication disruptions in adversarial\nenvironments. We introduce an adaptive event-triggered communication strategy\nthat dynamically adjusts communication thresholds based on real-time sensing\nand communication quality. This strategy ensures optimal performance even in\nthe presence of sensor degradation or communication failure. Furthermore, we\nconduct a rigorous analysis of the convergence and stability properties of the\nproposed algorithm, demonstrating its resilience against bounded adversarial\nzones and maintaining accurate state estimation. Robotarium-based experiment\nresults show that our proposed algorithm significantly outperforms traditional\nmethods in terms of localization accuracy and communication efficiency,\nparticularly in adversarial settings. Our approach offers improved scalability,\nreliability, and fault tolerance for MRS, making it suitable for large-scale\ndeployments in real-world, challenging environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5bb9\u9519\u534f\u540c\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e8b\u4ef6\u89e6\u53d1\u901a\u4fe1\u7b56\u7565\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u6216\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u5b9a\u4f4d\u65b9\u6cd5\u6613\u53d7\u4f20\u611f\u5668\u64cd\u7eb5\u548c\u901a\u4fe1\u5e72\u6270\u7b49\u5bf9\u6297\u653b\u51fb\u5f71\u54cd\uff0c\u9700\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u4e8b\u4ef6\u89e6\u53d1\u901a\u4fe1\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u901a\u4fe1\u9608\u503c\uff0c\u5e76\u7ed3\u5408\u5b9e\u65f6\u611f\u77e5\u548c\u901a\u4fe1\u8d28\u91cf\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7b97\u6cd5\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u53ef\u9760\u6027\u548c\u5bb9\u9519\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.06526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06526", "abs": "https://arxiv.org/abs/2507.06526", "authors": ["Chaoshuo Zhang", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Qian Wang", "Chao Shen"], "title": "Concept Unlearning by Modeling Key Steps of Diffusion Process", "comment": null, "summary": "Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,\nwhich generate highly realistic images based on textual input, have been widely\nused. However, their misuse poses serious security risks. While existing\nconcept unlearning methods aim to mitigate these risks, they struggle to\nbalance unlearning effectiveness with generative retainability.To overcome this\nlimitation, we innovatively propose the Key Step Concept Unlearning (KSCU)\nmethod, which ingeniously capitalizes on the unique stepwise sampling\ncharacteristic inherent in diffusion models during the image generation\nprocess. Unlike conventional approaches that treat all denoising steps equally,\nKSCU strategically focuses on pivotal steps with the most influence over the\nfinal outcome by dividing key steps for different concept unlearning tasks and\nfine-tuning the model only at those steps. This targeted approach reduces the\nnumber of parameter updates needed for effective unlearning, while maximizing\nthe retention of the model's generative capabilities.Through extensive\nbenchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs\nfrom generating undesirable images while better retaining the model's\ngenerative capabilities.Our code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKSCU\u7684\u65b0\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u6982\u5ff5\u9057\u5fd8\uff0c\u4ee5\u5e73\u8861\u9057\u5fd8\u6548\u679c\u4e0e\u751f\u6210\u80fd\u529b\u7684\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u5728\u5e73\u8861\u9057\u5fd8\u6548\u679c\u4e0e\u751f\u6210\u80fd\u529b\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KSCU\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u9010\u6b65\u91c7\u6837\u7279\u6027\uff0c\u4e13\u6ce8\u4e8e\u5bf9\u6700\u7ec8\u7ed3\u679c\u5f71\u54cd\u6700\u5927\u7684\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u5fae\u8c03\uff0c\u51cf\u5c11\u53c2\u6570\u66f4\u65b0\u6b21\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKSCU\u80fd\u6709\u6548\u9632\u6b62\u751f\u6210\u4e0d\u826f\u56fe\u50cf\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u7559\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002", "conclusion": "KSCU\u65b9\u6cd5\u5728\u6982\u5ff5\u9057\u5fd8\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u4f7f\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06658", "abs": "https://arxiv.org/abs/2507.06658", "authors": ["Gennadii Iakovlev"], "title": "Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models", "comment": null, "summary": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u68c0\u6d4b\u653f\u6cbb\u4eba\u7269\u4e92\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8861\u91cf\u7cbe\u82f1\u6781\u5316\uff0c\u5e76\u5206\u6790\u4e86\u82f1\u56fd\u3001\u5308\u7259\u5229\u548c\u610f\u5927\u5229\u7684\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u653f\u6cbb\u4eba\u7269\u5728\u8bae\u4f1a\u6f14\u8bb2\u4e2d\u7684\u4e92\u52a8\u548c\u60c5\u611f\u8868\u8fbe\uff0c\u91cf\u5316\u7cbe\u82f1\u6781\u5316\u73b0\u8c61\u3002", "method": "\u5229\u7528\u4eba\u5de5\u667a\u80fd\u68c0\u6d4b\u6f14\u8bb2\u4e2d\u7684\u53d1\u8a00\u8005\u548c\u88ab\u63d0\u53ca\u8005\uff0c\u8bc4\u4f30\u60c5\u611f\u6e29\u5ea6\uff0c\u6784\u5efa\u7cbe\u82f1\u6781\u5316\u6307\u6570\u3002", "result": "\u751f\u6210\u7684\u6307\u6570\u5bf9\u9009\u4e3e\u6d3b\u52a8\u3001\u653f\u515a\u5371\u673a\u7b49\u4e8b\u4ef6\u53cd\u5e94\u826f\u597d\uff0c\u53ef\u7528\u4e8e\u6b27\u76df\u8303\u56f4\u5185\u7684\u957f\u671f\u7814\u7a76\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cbe\u82f1\u6781\u5316\u7684\u91cf\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u8de8\u56fd\u6bd4\u8f83\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002"}}
{"id": "2507.06722", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06722", "abs": "https://arxiv.org/abs/2507.06722", "authors": ["Sunwoo Kim", "Haneul Yoo", "Alice Oh"], "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics", "comment": "Accepted to Actionable Interpretability Workshop - ICML 2025", "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u9884\u6d4b\u65f6\uff0c\u5176\u9690\u85cf\u72b6\u6001\u7684\u52a8\u6001\u53d8\u5316\u57fa\u672c\u4e00\u81f4\uff0c\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u5e76\u672a\u663e\u8457\u5f71\u54cd\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u63a2\u7d22LLMs\u5982\u4f55\u5185\u90e8\u8868\u793a\u548c\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u9632\u6b62\u5e7b\u89c9\u3002", "method": "\u4f7f\u7528Tuned Lens\uff08Logit Lens\u7684\u53d8\u4f53\uff09\u5206\u679011\u4e2a\u6570\u636e\u96c6\u548c5\u4e2a\u6a21\u578b\u7684\u5c42\u95f4\u6982\u7387\u8f68\u8ff9\uff0c\u6bd4\u8f83\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u9884\u6d4b\u7684\u52a8\u6001\u3002", "result": "\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u9884\u6d4b\u7684\u8f68\u8ff9\u4e00\u81f4\uff0c\u5747\u5728\u76f8\u4f3c\u5c42\u51fa\u73b0\u7f6e\u4fe1\u5ea6\u7a81\u7136\u589e\u52a0\u7684\u73b0\u8c61\u3002\u4f46\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u53ef\u80fd\u5b66\u4f1a\u4ee5\u4e0d\u540c\u65b9\u5f0f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u7b80\u5355\u65b9\u6cd5\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u7814\u7a76\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.06631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06631", "abs": "https://arxiv.org/abs/2507.06631", "authors": ["Enda D. V. Bigarella"], "title": "Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator", "comment": null, "summary": "This document reports on a method for detecting and preventing overfitting on\ndata regressions, herein applied to mesh-like data structures. The mesh\nstructure allows for the straightforward computation of the Laplace-operator\nsecond-order derivatives in a finite-difference fashion for noiseless data.\nDerivatives of the training data are computed on the original training mesh to\nserve as a true label of the entropy of the training data. Derivatives of the\ntrained data are computed on a staggered mesh to identify oscillations in the\ninterior of the original training mesh cells. The loss of the Laplace-operator\nderivatives is used for hyperparameter optimisation, achieving a reduction of\nunwanted oscillation through the minimisation of the entropy of the trained\nmodel. In this setup, testing does not require the splitting of points from the\ntraining data, and training is thus directly performed on all available\ntraining points. The Laplace operator applied to the trained data on a\nstaggered mesh serves as a surrogate testing metric based on diffusion\nproperties.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u6570\u636e\u7ed3\u6784\u7684\u8fc7\u62df\u5408\u68c0\u6d4b\u4e0e\u9884\u9632\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u4e8c\u9636\u5bfc\u6570\u8ba1\u7b97\u71b5\u635f\u5931\uff0c\u4f18\u5316\u8d85\u53c2\u6570\u4ee5\u51cf\u5c11\u632f\u8361\u3002", "motivation": "\u89e3\u51b3\u56de\u5f52\u4efb\u52a1\u4e2d\u6570\u636e\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7f51\u683c\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u907f\u514d\u8bad\u7ec3\u6570\u636e\u7684\u632f\u8361\u548c\u566a\u58f0\u5e72\u6270\u3002", "method": "\u5229\u7528\u539f\u59cb\u8bad\u7ec3\u7f51\u683c\u8ba1\u7b97\u5bfc\u6570\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u5728\u4ea4\u9519\u7f51\u683c\u4e0a\u8ba1\u7b97\u8bad\u7ec3\u6570\u636e\u7684\u5bfc\u6570\u4ee5\u68c0\u6d4b\u632f\u8361\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u635f\u5931\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u6700\u5c0f\u5316\u71b5\u635f\u5931\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8bad\u7ec3\u6a21\u578b\u7684\u632f\u8361\uff0c\u65e0\u9700\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u62c6\u5206\u6d4b\u8bd5\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7f51\u683c\u6570\u636e\u56de\u5f52\u4e2d\u6709\u6548\u9884\u9632\u8fc7\u62df\u5408\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u4f5c\u4e3a\u66ff\u4ee3\u6d4b\u8bd5\u6307\u6807\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2507.06979", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06979", "abs": "https://arxiv.org/abs/2507.06979", "authors": ["Panagiotis Koromilas", "Efthymios Georgiou", "Giorgos Bouritsas", "Theodoros Giannakopoulos", "Mihalis A. Nicolaou", "Yannis Panagakis"], "title": "A Principled Framework for Multi-View Contrastive Learning", "comment": null, "summary": "Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning\n(SSL), typically relies on pairs of data views generated through augmentation.\nWhile multiple augmentations per instance (more than two) improve\ngeneralization in supervised learning, current CL methods handle additional\nviews suboptimally by simply aggregating different pairwise objectives. This\napproach suffers from four critical limitations: (L1) it utilizes multiple\noptimization terms per data point resulting to conflicting objectives, (L2) it\nfails to model all interactions across views and data points, (L3) it inherits\nfundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL\nlosses, and (L4) it prevents fully realizing the benefits of increased view\nmultiplicity observed in supervised settings. We address these limitations\nthrough two novel loss functions: MV-InfoNCE, which extends InfoNCE to\nincorporate all possible view interactions simultaneously in one term per data\npoint, and MV-DHEL, which decouples alignment from uniformity across views\nwhile scaling interaction complexity with view multiplicity. Both approaches\nare theoretically grounded - we prove they asymptotically optimize for\nalignment of all views and uniformity, providing principled extensions to\nmulti-view contrastive learning. Our empirical results on ImageNet1K and three\nother datasets demonstrate that our methods consistently outperform existing\nmulti-view approaches and effectively scale with increasing view multiplicity.\nWe also apply our objectives to multimodal data and show that, in contrast to\nother contrastive objectives, they can scale beyond just two modalities. Most\nsignificantly, ablation studies reveal that MV-DHEL with five or more views\neffectively mitigates dimensionality collapse by fully utilizing the embedding\nspace, thereby delivering multi-view benefits observed in supervised learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570MV-InfoNCE\u548cMV-DHEL\uff0c\u89e3\u51b3\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u56db\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u591a\u89c6\u56fe\u65f6\u5b58\u5728\u4f18\u5316\u51b2\u7a81\u3001\u89c6\u56fe\u4ea4\u4e92\u4e0d\u8db3\u3001\u5bf9\u9f50\u4e0e\u5747\u5300\u6027\u8026\u5408\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u591a\u89c6\u56fe\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faMV-InfoNCE\u548cMV-DHEL\u4e24\u79cd\u635f\u5931\u51fd\u6570\uff0c\u524d\u8005\u540c\u65f6\u5efa\u6a21\u6240\u6709\u89c6\u56fe\u4ea4\u4e92\uff0c\u540e\u8005\u89e3\u8026\u5bf9\u9f50\u4e0e\u5747\u5300\u6027\u5e76\u652f\u6301\u591a\u6a21\u6001\u6269\u5c55\u3002", "result": "\u5728ImageNet1K\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u6709\u6548\u5229\u7528\u591a\u89c6\u56fe\u548c\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u4e86\u7ef4\u5ea6\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2507.06996", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06996", "abs": "https://arxiv.org/abs/2507.06996", "authors": ["Eunbyeol Cho", "Jiyoun Kim", "Minjae Lee", "Sungjin Park", "Edward Choi"], "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing", "comment": null, "summary": "Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.", "AI": {"tldr": "RawMed\u662f\u4e00\u4e2a\u751f\u6210\u591a\u8868\u65f6\u95f4\u5e8f\u5217\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u7684\u6846\u67b6\uff0c\u9996\u6b21\u6a21\u62df\u539f\u59cbEHR\u7684\u590d\u6742\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u76d1\u7ba1\u9650\u5236\uff0c\u771f\u5b9eEHR\u6570\u636e\u96be\u4ee5\u5171\u4eab\uff0c\u9700\u8981\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u751f\u6210\u4e13\u5bb6\u9009\u62e9\u7684\u7279\u5f81\uff0c\u65e0\u6cd5\u6a21\u62df\u539f\u59cbEHR\u7684\u590d\u6742\u6027\u3002", "method": "RawMed\u91c7\u7528\u57fa\u4e8e\u6587\u672c\u7684\u8868\u793a\u548c\u538b\u7f29\u6280\u672f\uff0c\u6700\u5c0f\u5316\u9884\u5904\u7406\uff0c\u751f\u6210\u591a\u8868\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "result": "\u5728\u4e24\u4e2a\u5f00\u6e90EHR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cRawMed\u5728\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RawMed\u4e3a\u5408\u6210EHR\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u539f\u59cb\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
