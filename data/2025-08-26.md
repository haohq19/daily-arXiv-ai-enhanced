<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers](https://arxiv.org/abs/2508.16752)
*Marco N. Bochernitsan,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CV

TL;DR: 提出了一种使用帕累托最优前沿评估文本到图像模型公平性和效用的方法，通过超参数优化来平衡公平性和视觉质量


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型的公平性评估主要依赖定性判断或有限比较，缺乏可重复的评估方法，难以同时评估公平性和效用

Method: 使用帕累托最优前沿分析不同去偏方法的超参数配置，采用归一化香农熵评估公平性，ClipScore评估效用

Result: 评估了多个主流文本到图像模型，发现大多数默认超参数配置在公平性-效用空间中被支配，可以轻松找到更好的超参数

Conclusion: 该方法为文本到图像模型的公平性和效用评估提供了可重复的量化框架，有助于发现更优的超参数配置

Abstract: Achieving fairness in text-to-image generation demands mitigating social
biases without compromising visual fidelity, a challenge critical to
responsible AI. Current fairness evaluation procedures for text-to-image models
rely on qualitative judgment or narrow comparisons, which limit the capacity to
assess both fairness and utility in these models and prevent reproducible
assessment of debiasing methods. Existing approaches typically employ ad-hoc,
human-centered visual inspections that are both error-prone and difficult to
replicate. We propose a method for evaluating fairness and utility in
text-to-image models using Pareto-optimal frontiers across hyperparametrization
of debiasing methods. Our method allows for comparison between distinct
text-to-image models, outlining all configurations that optimize fairness for a
given utility and vice-versa. To illustrate our evaluation method, we use
Normalized Shannon Entropy and ClipScore for fairness and utility evaluation,
respectively. We assess fairness and utility in Stable Diffusion, Fair
Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that
most default hyperparameterizations of the text-to-image model are dominated
solutions in the fairness-utility space, and it is straightforward to find
better hyperparameters.

</details>


### [2] [Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark](https://arxiv.org/abs/2508.16859)
*Jinpeng Hu,Hongchang Shi,Chongyuan Dai,Zhuo Li,Peipei Song,Meng Wang*

Main category: cs.CV

TL;DR: 本文提出了一个多轮多模态情感理解与推理基准(MTMEUR)，包含1,451个真实场景视频和5,101个渐进式问题，并开发了一个多智能体框架来提升情感推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在心理学领域主要关注情感识别能力，而忽视了情感推理这一对提升人机交互自然性和有效性至关重要的潜力。

Method: 构建包含多轮对话的MTMEUR基准数据集，并提出多智能体框架，每个智能体专注于特定方面（如背景上下文、角色动态、事件细节）来增强系统推理能力。

Result: 在提出的基准上对现有MLLM和基于智能体的方法进行实验，发现大多数模型在此任务上面临显著挑战。

Conclusion: 该研究强调了情感推理在人机交互中的重要性，提出的基准和方法为提升多模态模型的情感理解能力提供了新的方向和评估标准。

Abstract: Multimodal large language models (MLLMs) have been widely applied across
various fields due to their powerful perceptual and reasoning capabilities. In
the realm of psychology, these models hold promise for a deeper understanding
of human emotions and behaviors. However, recent research primarily focuses on
enhancing their emotion recognition abilities, leaving the substantial
potential in emotion reasoning, which is crucial for improving the naturalness
and effectiveness of human-machine interactions. Therefore, in this paper, we
introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)
benchmark, which encompasses 1,451 video data from real-life scenarios, along
with 5,101 progressive questions. These questions cover various aspects,
including emotion recognition, potential causes of emotions, future action
prediction, etc. Besides, we propose a multi-agent framework, where each agent
specializes in a specific aspect, such as background context, character
dynamics, and event details, to improve the system's reasoning capabilities.
Furthermore, we conduct experiments with existing MLLMs and our agent-based
method on the proposed benchmark, revealing that most models face significant
challenges with this task.

</details>


### [3] [Structural Damage Detection Using AI Super Resolution and Visual Language Model](https://arxiv.org/abs/2508.17130)
*Catherine Hoier,Khandaker Mamun Ahmed*

Main category: cs.CV

TL;DR: 提出基于无人机影像、VRT视频超分辨率和Gemma3:27b视觉语言模型的灾害损害评估框架，在土耳其地震和摩尔龙卷风数据上达到84.5%分类准确率


<details>
  <summary>Details</summary>
Motivation: 传统灾害损害评估方法劳动密集、成本高且危险，在资源有限环境下难以快速响应，需要开发自动化、低成本解决方案

Method: 整合无人机影像、VRT视频超分辨率模型和27B参数的Gemma3视觉语言模型，构建端到端系统进行影像增强、结构损伤识别和四类损害程度分类

Result: 在2023土耳其地震和2013摩尔龙卷风数据集上验证，达到84.5%的分类准确率，能够提供高精度结果

Conclusion: 该框架为非技术用户提供可访问的初步分析能力，显著提升了灾害管理的响应速度和效率

Abstract: Natural disasters pose significant challenges to timely and accurate damage
assessment due to their sudden onset and the extensive areas they affect.
Traditional assessment methods are often labor-intensive, costly, and hazardous
to personnel, making them impractical for rapid response, especially in
resource-limited settings. This study proposes a novel, cost-effective
framework that leverages aerial drone footage, an advanced AI-based video
super-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a
27 billion parameter Visual Language Model (VLM). This integrated system is
designed to improve low-resolution disaster footage, identify structural
damage, and classify buildings into four damage categories, ranging from
no/slight damage to total destruction, along with associated risk levels. The
methodology was validated using pre- and post-event drone imagery from the 2023
Turkey earthquakes (courtesy of The Guardian) and satellite data from the 2013
Moore Tornado (xBD dataset). The framework achieved a classification accuracy
of 84.5%, demonstrating its ability to provide highly accurate results.
Furthermore, the system's accessibility allows non-technical users to perform
preliminary analyses, thereby improving the responsiveness and efficiency of
disaster management efforts.

</details>


### [4] [Multi-Level LVLM Guidance for Untrimmed Video Action Recognition](https://arxiv.org/abs/2508.17442)
*Liyang Peng,Sihan Zhu,Yunjie Guo*

Main category: cs.CV

TL;DR: ECVT是一种新颖的视频动作识别与定位架构，通过双分支设计和大型视觉语言模型的多粒度语义描述，有效解决了复杂未修剪视频中的细粒度动作识别和长期时序依赖问题，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂未修剪视频中难以捕捉细粒度动作、长期时序依赖和从低级视觉特征中提取高级语义信息，需要新的架构来弥补这一差距。

Method: 采用双分支设计：视频编码分支负责时空特征提取，跨模态引导分支利用大型视觉语言模型生成多粒度语义描述（全局事件提示和时序子事件提示），通过自适应门控、跨模态注意力和事件图模块进行多模态融合。

Result: 在ActivityNet v1.3数据集上达到40.5%的平均mAP，在THUMOS14数据集上达到67.1%的mAP@0.5，显著优于现有基准方法。

Conclusion: ECVT通过结合大型视觉语言模型的语义理解能力和创新的多模态融合机制，成功提升了视频时序结构和事件逻辑的理解能力，为复杂视频分析提供了有效解决方案。

Abstract: Action recognition and localization in complex, untrimmed videos remain a
formidable challenge in computer vision, largely due to the limitations of
existing methods in capturing fine-grained actions, long-term temporal
dependencies, and high-level semantic information from low-level visual
features. This paper introduces the Event-Contextualized Video Transformer
(ECVT), a novel architecture that leverages the advanced semantic understanding
capabilities of Large Vision-Language Models (LVLMs) to bridge this gap. ECVT
employs a dual-branch design, comprising a Video Encoding Branch for
spatio-temporal feature extraction and a Cross-Modal Guidance Branch. The
latter utilizes an LVLM to generate multi-granularity semantic descriptions,
including Global Event Prompting for macro-level narrative and Temporal
Sub-event Prompting for fine-grained action details. These multi-level textual
cues are integrated into the video encoder's learning process through
sophisticated mechanisms such as adaptive gating for high-level semantic
fusion, cross-modal attention for fine-grained feature refinement, and an event
graph module for temporal context calibration. Trained end-to-end with a
comprehensive loss function incorporating semantic consistency and temporal
calibration terms, ECVT significantly enhances the model's ability to
understand video temporal structures and event logic. Extensive experiments on
ActivityNet v1.3 and THUMOS14 datasets demonstrate that ECVT achieves
state-of-the-art performance, with an average mAP of 40.5% on ActivityNet v1.3
and mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.

</details>


### [5] [Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks](https://arxiv.org/abs/2508.17537)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出多项式近似方法解决相机平移和角速度估计问题，开发最小求解器并在合成和真实数据集上验证


<details>
  <summary>Details</summary>
Motivation: 解决滚动快门和事件相机中从异步点轨迹估计相机平移和角速度的问题，原始问题是非多项式的

Method: 提出多项式近似方法，对最小问题进行分类并确定代数度，开发低代数度的最小求解器

Result: 在合成和真实数据集上评估了开发的求解器，代码将公开

Conclusion: 该方法有效解决了相机运动估计问题，为滚动快门和事件相机应用提供了实用解决方案

Abstract: We address the problem of estimating both translational and angular velocity
of a camera from asynchronous point tracks, a formulation relevant to rolling
shutter and event cameras. Since the original problem is non-polynomial, we
propose a polynomial approximation, classify the resulting minimal problems,
and determine their algebraic degrees. Furthermore, we develop minimal solvers
for several problems with low degrees and evaluate them on synthetic and real
datasets. The code will be made publicly available.

</details>


### [6] [Designing Practical Models for Isolated Word Visual Speech Recognition](https://arxiv.org/abs/2508.17894)
*Iason Ioannis Panagos,Giorgos Sfikas,Christophoros Nikou*

Main category: cs.CV

TL;DR: 这篇论文研究视觉语音识别(VSR)系统的轻量级架构，通过采用效率高的图像分类模型和时序卷积网络核心，在保持识别性能的同时降低硬件资源需求。


<details>
  <summary>Details</summary>
Motivation: 虽然深度网络在VSR中表现出色，但计算成本高、硬件要求高，限制了在资源受限的实际场景中的应用。需要开发轻量级架构以促进VSR系统的更广泛部署。

Method: 采用标准的双网络设计范式：一个网络处理视觉特征提取，另一个进行序列分类。首先基于图像分类领域的高效模型进行基准测试，然后在时序卷积网络核心中采用轻量级块设计。

Result: 在最大的公共英语单词数据库上进行实验，开发出多个统一模型，这些模型资源需求低但识别性能强。实验结果证明了所开发模型的有效性和实用性。

Conclusion: 该研究成功开发了轻量级的结合VSR系统，在保持高识别性能的同时显著降低了硬件资源需求，为VSR系统在实际应用场景中的更广泛部署提供了可行性。代码和训练好的模型将公开提供。

Abstract: Visual speech recognition (VSR) systems decode spoken words from an input
sequence using only the video data. Practical applications of such systems
include medical assistance as well as human-machine interactions. A VSR system
is typically employed in a complementary role in cases where the audio is
corrupt or not available. In order to accurately predict the spoken words,
these architectures often rely on deep neural networks in order to extract
meaningful representations from the input sequence. While deep architectures
achieve impressive recognition performance, relying on such models incurs
significant computation costs which translates into increased resource demands
in terms of hardware requirements and results in limited applicability in
real-world scenarios where resources might be constrained. This factor prevents
wider adoption and deployment of speech recognition systems in more practical
applications. In this work, we aim to alleviate this issue by developing
architectures for VSR that have low hardware costs. Following the standard
two-network design paradigm, where one network handles visual feature
extraction and another one utilizes the extracted features to classify the
entire sequence, we develop lightweight end-to-end architectures by first
benchmarking efficient models from the image classification literature, and
then adopting lightweight block designs in a temporal convolution network
backbone. We create several unified models with low resource requirements but
strong recognition performance. Experiments on the largest public database for
English words demonstrate the effectiveness and practicality of our developed
models. Code and trained models will be made publicly available.

</details>


### [7] [Alternating Training-based Label Smoothing Enhances Prompt Generalization](https://arxiv.org/abs/2508.17846)
*Yang Chen,Yanbin Wei,Ke Jin,Yi Kong,James Kwok,Yu Zhang*

Main category: cs.CV

TL;DR: 提出ATLaS方法，通过交替训练使用one-hot标签和标签平滑生成的软标签来提升提示调优的泛化能力，并引入两种高效离线软标签（CSL和ISL）来提供类间或实例-类关系。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在零样本泛化方面表现出色，但提示调优的泛化能力有限。标签平滑(LS)作为有效正则化技术可以防止模型过拟合，但传统LS反而会削弱提示调优的泛化能力。

Method: 提出交替训练标签平滑(ATLaS)方法，交替使用标准one-hot标签和LS生成的软标签来监督提示调优。引入类间软标签(CSL)和实例级软标签(ISL)两种高效离线软标签。

Result: 大量实验表明，ATLaS方法结合CSL和ISL能持续提升提示调优的泛化性能，且与主流提示调优方法高度兼容。

Conclusion: ATLaS方法有效解决了传统标签平滑在提示调优中的局限性，通过交替训练机制和高效软标签设计显著提升了提示调优的泛化能力。

Abstract: Recent advances in pre-trained vision-language models have demonstrated
remarkable zero-shot generalization capabilities. To further enhance these
models' adaptability to various downstream tasks, prompt tuning has emerged as
a parameter-efficient fine-tuning method. However, despite its efficiency, the
generalization ability of prompt remains limited. In contrast, label smoothing
(LS) has been widely recognized as an effective regularization technique that
prevents models from becoming over-confident and improves their generalization.
This inspires us to explore the integration of LS with prompt tuning. However,
we have observed that the vanilla LS even weakens the generalization ability of
prompt tuning. To address this issue, we propose the Alternating Training-based
Label Smoothing (ATLaS) method, which alternately trains with standard one-hot
labels and soft labels generated by LS to supervise the prompt tuning.
Moreover, we introduce two types of efficient offline soft labels, including
Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide
inter-class or instance-class relationships for prompt tuning. The theoretical
properties of the proposed ATLaS method are analyzed. Extensive experiments
demonstrate that the proposed ATLaS method, combined with CSL and ISL,
consistently enhances the generalization performance of prompt tuning.
Moreover, the proposed ATLaS method exhibits high compatibility with prevalent
prompt tuning methods, enabling seamless integration into existing methods.

</details>


### [8] [Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving](https://arxiv.org/abs/2508.17975)
*Md Shahi Amran Hossain,Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.CV

TL;DR: 提出了一种混合计算机视觉架构，结合YOLOv8快速检测和五层CNN验证，通过合成图像训练提高在数据漂移环境中的目标检测鲁棒性，检测准确率提升超过90%。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全需求日益增长，但恶劣天气和低光照等条件导致数据漂移，降低模型性能并带来安全隐患。ISO 8800标准提供了AI风险管理框架，但需要更鲁棒的检测方法。

Method: 采用混合架构：YOLOv8用于快速目标检测，五层CNN用于验证。使用数千张道路环境合成图像进行训练，采用序列处理方式提高在漂移环境中的鲁棒性。

Result: 在漂移增强的道路图像测试中，检测准确率提高了90%以上，显著提升了在未见漂移环境中的性能表现。

Conclusion: 混合计算机视觉架构能有效应对数据漂移问题，为自动驾驶提供更好的道路安全保障，验证了混合模型在复杂环境中的优越性。

Abstract: The use of computer vision in automotive is a trending research in which
safety and security are a primary concern. In particular, for autonomous
driving, preventing road accidents requires highly accurate object detection
under diverse conditions. To address this issue, recently the International
Organization for Standardization (ISO) released the 8800 norm, providing
structured frameworks for managing associated AI relevant risks. However,
challenging scenarios such as adverse weather or low lighting often introduce
data drift, leading to degraded model performance and potential safety
violations. In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments. Our dual mode
framework utilized YOLO version 8 for swift detection and incorporated a
five-layer CNN for verification. The system functioned in sequence and improved
the detection accuracy by more than 90\% when tested with drift-augmented road
images. The focus was to demonstrate how such a hybrid model can provide better
road safety when working together in a hybrid structure.

</details>


### [9] [EventTracer: Fast Path Tracing-based Event Stream Rendering](https://arxiv.org/abs/2508.18071)
*Zhenyang Li,Xiaoyang Bai,Jinfan Lu,Pengfei Shen,Edmund Y. Lam,Yifan Peng*

Main category: cs.CV

TL;DR: EventTracer是一个基于路径追踪的高效事件流模拟器，通过低采样路径追踪和轻量级脉冲网络去噪，能够以4分钟/秒的速度生成720p高保真事件序列，显著提升了事件模拟的时空分辨率和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的事件流模拟方法通常使用无噪声RGB帧，渲染成本高且时间分辨率低（100-300 FPS），远低于真实事件数据的频率，需要一种高效且物理准确的事件模拟方法。

Method: 采用低采样路径追踪加速渲染过程，训练轻量级事件脉冲网络（使用双极性泄漏积分发放单元和双向地球移动距离损失）将去噪后的RGB视频转换为真实事件序列。

Result: EventTracer能以约4分钟/秒的速度生成720p视频，在时空建模准确性方面优于其他事件模拟器，在细节捕捉和与真实事件数据相似性方面表现更好。

Conclusion: EventTracer是创建大规模事件-RGB数据集的有前景工具，能够缩小事件视觉的模拟-现实差距，推动机器人、自动驾驶和VR/AR等应用场景的发展。

Abstract: Simulating event streams from 3D scenes has become a common practice in
event-based vision research, as it meets the demand for large-scale, high
temporal frequency data without setting up expensive hardware devices or
undertaking extensive data collections. Yet existing methods in this direction
typically work with noiseless RGB frames that are costly to render, and
therefore they can only achieve a temporal resolution equivalent to 100-300
FPS, far lower than that of real-world event data. In this work, we propose
EventTracer, a path tracing-based rendering pipeline that simulates
high-fidelity event sequences from complex 3D scenes in an efficient and
physics-aware manner. Specifically, we speed up the rendering process via low
sample-per-pixel (SPP) path tracing, and train a lightweight event spiking
network to denoise the resulting RGB videos into realistic event sequences. To
capture the physical properties of event streams, the network is equipped with
a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a
bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at
a speed of about 4 minutes per second of 720p video, and it inherits the merit
of accurate spatiotemporal modeling from its path tracing backbone. We show in
two downstream tasks that EventTracer captures better scene details and
demonstrates a greater similarity to real-world event data than other event
simulators, which establishes it as a promising tool for creating large-scale
event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based
vision, and boosting various application scenarios such as robotics, autonomous
driving, and VRAR.

</details>


### [10] [BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding](https://arxiv.org/abs/2508.18187)
*Xuan-Bac Nguyen,Thanh-Dat Truong,Pawan Sinha,Khoa Luu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为BRAIN的偏置缓解持续学习方法，通过去偏对比学习和基于角度的遗忘缓解技术，解决脑信号随时间衰减导致的表示偏移和性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 人类记忆衰减导致脑信号随时间变弱、不确定且缺乏视觉上下文，这种不一致性对视觉-脑理解模型造成挑战，表现为脑信号表示随记录会话发生偏移，产生累积偏置。

Method: 提出BRAIN偏置缓解持续学习方法：1）在持续学习设置中训练模型；2）引入去偏对比学习损失函数解决偏置问题；3）采用基于角度的遗忘缓解方法防止灾难性遗忘。

Result: 实验证明该方法在各种基准测试中达到最先进性能，超越了先前的和非持续学习方法。

Conclusion: 该研究成功解决了脑信号衰减带来的表示偏移问题，提出的BRAIN方法有效缓解了累积偏置和灾难性遗忘，为视觉-脑理解领域提供了有效的解决方案。

Abstract: Memory decay makes it harder for the human brain to recognize visual objects
and retain details. Consequently, recorded brain signals become weaker,
uncertain, and contain poor visual context over time. This paper presents one
of the first vision-learning approaches to address this problem. First, we
statistically and experimentally demonstrate the existence of inconsistency in
brain signals and its impact on the Vision-Brain Understanding (VBU) model. Our
findings show that brain signal representations shift over recording sessions,
leading to compounding bias, which poses challenges for model learning and
degrades performance. Then, we propose a new Bias-Mitigation Continual Learning
(BRAIN) approach to address these limitations. In this approach, the model is
trained in a continual learning setup and mitigates the growing bias from each
learning step. A new loss function named De-bias Contrastive Learning is also
introduced to address the bias problem. In addition, to prevent catastrophic
forgetting, where the model loses knowledge from previous sessions, the new
Angular-based Forgetting Mitigation approach is introduced to preserve learned
knowledge in the model. Finally, the empirical experiments demonstrate that our
approach achieves State-of-the-Art (SOTA) performance across various
benchmarks, surpassing prior and non-continual learning methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations](https://arxiv.org/abs/2508.16634)
*Zhendong Yang,Jie Wang,Liansong Zong,Xiaorong Liu,Quan Qian,Shiqian Chen*

Main category: cs.LG

TL;DR: 提出DGGN框架解决少样本类增量故障诊断问题，通过双粒度表示和边界感知策略有效缓解灾难性遗忘和过拟合


<details>
  <summary>Details</summary>
Motivation: 工业系统中需要持续学习新故障类别但样本稀缺，现有方法面临灾难性遗忘和过拟合的严重挑战

Method: 双粒度指导网络(DGGN)：细粒度流捕获类别特定特征，粗粒度流保留通用知识，通过跨注意力机制融合；边界感知样本优先策略；解耦平衡随机森林分类器

Result: 在TEP基准和真实MFF数据集上实验表明，DGGN相比现有FSC-FD方法具有更优的诊断性能和稳定性

Conclusion: DGGN框架通过双粒度表示学习和边界感知策略，有效解决了少样本类增量故障诊断中的关键挑战

Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to
continuously learn from new fault classes with only a few samples without
forgetting old ones, is critical for real-world industrial systems. However,
this challenging task severely amplifies the issues of catastrophic forgetting
of old knowledge and overfitting on scarce new data. To address these
challenges, this paper proposes a novel framework built upon Dual-Granularity
Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN
explicitly decouples feature learning into two parallel streams: 1) a
fine-grained representation stream, which utilizes a novel Multi-Order
Interaction Aggregation module to capture discriminative, class-specific
features from the limited new samples. 2) a coarse-grained representation
stream, designed to model and preserve general, class-agnostic knowledge shared
across all fault types. These two representations are dynamically fused by a
multi-semantic cross-attention mechanism, where the stable coarse-grained
knowledge guides the learning of fine-grained features, preventing overfitting
and alleviating feature conflicts. To further mitigate catastrophic forgetting,
we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a
decoupled Balanced Random Forest classifier is employed to counter the decision
boundary bias caused by data imbalance. Extensive experiments on the TEP
benchmark and a real-world MFF dataset demonstrate that our proposed DGGN
achieves superior diagnostic performance and stability compared to
state-of-the-art FSC-FD approaches. Our code is publicly available at
https://github.com/MentaY/DGGN

</details>


### [12] [FRAME : Comprehensive Risk Assessment Framework for Adversarial Machine Learning Threats](https://arxiv.org/abs/2508.17405)
*Avishag Shapira,Simon Shigol,Asaf Shabtai*

Main category: cs.LG

TL;DR: FRAME是一个全面的自动化框架，用于评估机器学习系统的对抗性机器学习风险，通过系统评估部署环境、攻击技术和实证研究三个维度来量化风险。


<details>
  <summary>Details</summary>
Motivation: 传统风险评估框架无法有效应对对抗性机器学习威胁，现有方法主要关注技术攻击鲁棒性，忽视了部署环境、系统依赖和攻击可行性等现实因素，且缺乏跨领域的通用解决方案。

Method: FRAME框架包含新颖的风险评估方法，通过系统评估三个关键维度：目标系统部署环境、多样化AML技术特征、以及先前研究的实证见解。采用可行性评分机制和基于LLM的系统特定评估定制，并开发了全面的结构化AML攻击数据集。

Result: 在六个不同的真实世界应用中验证，表现出卓越的准确性和与AML专家分析的高度一致性，能够为系统所有者提供可直接使用的可操作结果。

Conclusion: FRAME使组织能够优先处理AML风险，支持在真实环境中安全部署AI系统，填补了现有风险评估框架的空白。

Abstract: The widespread adoption of machine learning (ML) systems increased attention
to their security and emergence of adversarial machine learning (AML)
techniques that exploit fundamental vulnerabilities in ML systems, creating an
urgent need for comprehensive risk assessment for ML-based systems. While
traditional risk assessment frameworks evaluate conventional cybersecurity
risks, they lack ability to address unique challenges posed by AML threats.
Existing AML threat evaluation approaches focus primarily on technical attack
robustness, overlooking crucial real-world factors like deployment
environments, system dependencies, and attack feasibility. Attempts at
comprehensive AML risk assessment have been limited to domain-specific
solutions, preventing application across diverse systems. Addressing these
limitations, we present FRAME, the first comprehensive and automated framework
for assessing AML risks across diverse ML-based systems. FRAME includes a novel
risk assessment method that quantifies AML risks by systematically evaluating
three key dimensions: target system's deployment environment, characteristics
of diverse AML techniques, and empirical insights from prior research. FRAME
incorporates a feasibility scoring mechanism and LLM-based customization for
system-specific assessments. Additionally, we developed a comprehensive
structured dataset of AML attacks enabling context-aware risk assessment. From
an engineering application perspective, FRAME delivers actionable results
designed for direct use by system owners with only technical knowledge of their
systems, without expertise in AML. We validated it across six diverse
real-world applications. Our evaluation demonstrated exceptional accuracy and
strong alignment with analysis by AML experts. FRAME enables organizations to
prioritize AML risks, supporting secure AI deployment in real-world
environments.

</details>


### [13] [Convergence and Generalization of Anti-Regularization for Parametric Models](https://arxiv.org/abs/2508.17412)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: 提出反正则化(AR)方法，在小样本情况下通过符号反转的奖励项增加模型表达能力，并随样本量增长以幂律衰减减弱干预。包含稳定性保障机制，在回归和分类任务中减少欠拟合同时保持泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决小样本学习中的欠拟合问题，传统正则化方法可能过度约束模型表达能力，需要一种能在数据稀缺时增强表达性、数据充足时自动退出的方法。

Method: 在损失函数中添加符号反转的奖励项来增加模型表达性，采用幂律衰减机制随样本量增加减弱干预，设计包含投影算子和梯度裁剪的稳定性保障机制，支持线性平滑器和NTK机制的理论分析。

Result: 实验表明AR能有效减少欠拟合，同时保持泛化性能和改善校准性，消融研究确认衰减计划和稳定性保障对防止过拟合和数值不稳定的重要性。

Conclusion: AR方法简单易实现，能无缝集成到标准经验风险最小化流程中，在数据和资源受限环境下实现鲁棒学习，只在有益时干预并在不必要时自动退出。

Abstract: We propose Anti-regularization (AR), which adds a sign-reversed reward term
to the loss to intentionally increase model expressivity in the small-sample
regime, and then attenuates this intervention with a power-law decay as the
sample size grows. We formalize spectral safety and trust-region conditions,
and design a lightweight stability safeguard that combines a projection
operator with gradient clipping, ensuring stable intervention under stated
assumptions. Our analysis spans linear smoothers and the Neural Tangent Kernel
(NTK) regime, providing practical guidance on selecting the decay exponent by
balancing empirical risk against variance. Empirically, AR reduces underfitting
while preserving generalization and improving calibration in both regression
and classification. Ablation studies confirm that the decay schedule and the
stability safeguard are critical to preventing overfitting and numerical
instability. We further examine a degrees-of-freedom targeting schedule that
keeps per-sample complexity approximately constant. AR is simple to implement
and reproducible, integrating cleanly into standard empirical risk minimization
pipelines. It enables robust learning in data- and resource-constrained
settings by intervening only when beneficial and fading away when unnecessary.

</details>


### [14] [Modeling Irregular Astronomical Time Series with Neural Stochastic Delay Differential Equations](https://arxiv.org/abs/2508.17521)
*YongKyung Oh,Seungsu Kam,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: 提出基于神经随机延迟微分方程的新框架，用于处理不规则采样的天文时间序列数据，在分类和异常检测方面表现优异


<details>
  <summary>Details</summary>
Motivation: 大规模天文调查如LSST产生的不规则采样和不完整时间序列数据给分类和异常检测带来挑战，需要新的建模方法

Method: 结合随机建模和神经网络，采用延迟感知神经架构、SDDE数值求解器，以及从噪声稀疏序列中稳健学习的机制

Result: 在不规则采样的天文数据实验中显示出强大的分类准确性，能有效检测新的天体物理事件，即使在部分标签情况下

Conclusion: 神经随机延迟微分方程是在观测约束下进行时间序列分析的原理性和实用工具

Abstract: Astronomical time series from large-scale surveys like LSST are often
irregularly sampled and incomplete, posing challenges for classification and
anomaly detection. We introduce a new framework based on Neural Stochastic
Delay Differential Equations (Neural SDDEs) that combines stochastic modeling
with neural networks to capture delayed temporal dynamics and handle irregular
observations. Our approach integrates a delay-aware neural architecture, a
numerical solver for SDDEs, and mechanisms to robustly learn from noisy, sparse
sequences. Experiments on irregularly sampled astronomical data demonstrate
strong classification accuracy and effective detection of novel astrophysical
events, even with partial labels. This work highlights Neural SDDEs as a
principled and practical tool for time series analysis under observational
constraints.

</details>


### [15] [Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA](https://arxiv.org/abs/2508.17586)
*Daniel Frees,Aditri Bhagirath,Moritz Bolling*

Main category: cs.LG

TL;DR: 本文研究了LoRA和DoRA在小型语言模型minBERT上的应用效果，发现即使在小模型上梯度更新也具有低秩特性，通过优化配置和AMP技术可以显著提升训练效率而不损失性能。


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA和DoRA在大语言模型上表现出色，但其在小型语言模型上的效果尚未充分研究。本文旨在验证这些高效微调方法在小规模模型minBERT上的适用性和效果。

Method: 使用LoRA和DoRA方法对minBERT模型进行微调，结合自动混合精度(AMP)技术，测试不同秩的分解效果，并探索多种架构、自定义损失函数和超参数配置。

Result: 研究发现即使在小模型上，梯度更新也具有低秩特性，秩1分解几乎不会造成性能损失。通过优化配置显著提升了训练效率，最终训练出能够同时执行情感分析、复述检测和相似度评分的最优集成多任务模型。

Conclusion: LoRA和DoRA方法不仅适用于大语言模型，在小规模语言模型上同样有效，验证了语言模型梯度更新的低秩特性具有普遍性，为资源受限环境下的高效模型微调提供了实用方案。

Abstract: While Large Language Models (LLMs) have revolutionized artificial
intelligence, fine-tuning LLMs is extraordinarily computationally expensive,
preventing smaller businesses and research teams with limited GPU resources
from engaging with new research. Hu et al and Liu et al introduce Low-Rank
Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) as highly
efficient and performant solutions to the computational challenges of LLM
fine-tuning, demonstrating huge speedups and memory usage savings for models
such as GPT-3 and RoBERTa. We seek to expand upon the original LoRA and DoRA
papers by benchmarking efficiency and performance of LoRA and DoRA when applied
to a much smaller scale of language model: our case study here is the compact
minBERT model. Our findings reveal that optimal custom configurations of LoRA
and DoRA, coupled with Automatic Mixed Precision (AMP), significantly enhance
training efficiency without compromising performance. Furthermore, while the
parameterization of minBERT is significantly smaller than GPT-3, our results
validate the observation that gradient updates to language models are
inherently low-rank even in small model space, observing that rank 1
decompositions yield negligible performance deficits. Furthermore, aided by our
highly efficient minBERT implementation, we investigate numerous architectures,
custom loss functions, and hyperparameters to ultimately train an optimal
ensembled multitask minBERT model to simultaneously perform sentiment analysis,
paraphrase detection, and similarity scoring.

</details>


### [16] [Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks](https://arxiv.org/abs/2508.18052)
*Silvia Beddar-Wiesing,Alice Moallemy-Oureh*

Main category: cs.LG

TL;DR: 本文扩展了图神经网络理论，从离散动态图扩展到连续时间动态图，提出了连续时间动态1-WL测试和对应的CGNN架构，保持了区分能力和通用逼近保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统（如通信网络、金融交易网络、分子相互作用）是异步演化的，可能分裂成断开连接的组件，而现有GNN理论仅限于离散动态图序列。

Method: 引入连续时间动态1-WL测试，证明其与连续时间动态展开树的等价性，基于离散动态GNN架构设计连续时间动态GNN（CGNN），使用分段连续可微分时间函数处理异步断开图。

Result: 建立了连续时间动态图的等价理论框架，识别出具有区分能力和通用逼近保证的CGNN类别，提供了实用的设计指南。

Conclusion: 成功将GNN理论扩展到连续时间动态图领域，为处理现实世界中异步演化的图数据提供了理论基础和架构设计指导。

Abstract: Graph Neural Networks (GNNs) are known to match the distinguishing power of
the 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide with
the unfolding tree equivalence classes of graphs. Preserving this equivalence,
GNNs can universally approximate any target function on graphs in probability
up to any precision. However, these results are limited to attributed
discrete-dynamic graphs represented as sequences of connected graph snapshots.
Real-world systems, such as communication networks, financial transaction
networks, and molecular interactions, evolve asynchronously and may split into
disconnected components. In this paper, we extend the theory of attributed
discrete-dynamic graphs to attributed continuous-time dynamic graphs with
arbitrary connectivity. To this end, we introduce a continuous-time dynamic
1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,
and identify a class of continuous-time dynamic GNNs (CGNNs) based on
discrete-dynamic GNN architectures that retain both distinguishing power and
universal approximation guarantees. Our constructive proofs further yield
practical design guidelines, emphasizing a compact and expressive CGNN
architecture with piece-wise continuously differentiable temporal functions to
process asynchronous, disconnected graphs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [17] [Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities](https://arxiv.org/abs/2508.16788)
*Bhagesh Gaur,Karan Gupta,Aseem Srivastava,Manish Gupta,Md Shad Akhtar*

Main category: cs.CL

TL;DR: 这篇论文提出了一个新框架MH-COPILOT，通过识别缺失的支持属性并提示用户补充信息，来提高在线心理健康社区中的帖子回复率和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 在线心理健康社区中许多帖子因缺乏关键支持属性而无人回复，需要一种方法来识别这些缺口并提示用户补充信息以提高帖子质量和参与度。

Method: 研究人员首先构建了REDDME数据集（4,760个心理健康子红板帖子），标注了事件、影响和需求三个关键支持属性的范围和强度。提出CueTaxo层次分类法来控制问题生成，并开发了基于强化学习的MH-COPILOT系统，包含属性识别、强度分类、控制问题生成和验证器奖励模型。

Result: 在四个语言模型上的实验结果显示，该模型在属性引导和用户参与度方面取得了显著改善。人工评估进一步验证了模型在真实在线心理健康社区环境中的效果。

Conclusion: MH-COPILOT框架能够有效识别帖子中缺失的支持属性，并通过有针对性的提示来引导用户补充信息，从而提高帖子质量和社区参与度，为在线心理健康支持提供了有效的解决方案。

Abstract: Online Mental Health Communities (OMHCs) provide crucial peer and expert
support, yet many posts remain unanswered due to missing support attributes
that signal the need for help. We present a novel framework that identifies
these gaps and prompts users to enrich their posts, thereby improving
engagement. To support this, we introduce REDDME, a new dataset of 4,760 posts
from mental health subreddits annotated for the span and intensity of three key
support attributes: event what happened?, effect what did the user experience?,
and requirement what support they need?. Next, we devise a hierarchical
taxonomy, CueTaxo, of support attributes for controlled question generation.
Further, we propose MH-COPILOT, a reinforcement learning-based system that
integrates (a) contextual attribute-span identification, (b) support attribute
intensity classification, (c) controlled question generation via a hierarchical
taxonomy, and (d) a verifier for reward modeling. Our model dynamically
assesses posts for the presence/absence of support attributes, and generates
targeted prompts to elicit missing information. Empirical results across four
notable language models demonstrate significant improvements in attribute
elicitation and user engagement. A human evaluation further validates the
model's effectiveness in real-world OMHC settings.

</details>


### [18] [Weights-Rotated Preference Optimization for Large Language Models](https://arxiv.org/abs/2508.17637)
*Chenxu Yang,Ruipeng Jia,Mingyu Zheng,Naibin Gu,Zheng Lin,Siyuan Chen,Weichong Yin,Hua Wu,Weiping Wang*

Main category: cs.CL

TL;DR: 提出了RoPO算法来解决DPO中的奖励黑客问题，通过权重旋转和正交约束防止模型过度偏离参考模型，在保持少量可训练参数的同时显著提升性能


<details>
  <summary>Details</summary>
Motivation: DPO在大型语言模型对齐中存在奖励黑客问题，模型过度降低被拒绝补全的概率以获得高奖励，导致生成过长、缺乏多样性以及灾难性遗忘知识

Method: 提出权重旋转偏好优化(RoPO)算法，隐式约束输出层logits的KL散度，显式约束中间隐藏状态通过多粒度正交矩阵微调

Result: 在AlpacaEval 2上提升3.27分，在MT-Bench上以仅0.015%的可训练参数超越最佳基线6.2-7.5分

Conclusion: RoPO有效缓解了DPO的奖励黑客问题，保留了预训练和SFT阶段获得的知识和表达能力

Abstract: Despite the efficacy of Direct Preference Optimization (DPO) in aligning
Large Language Models (LLMs), reward hacking remains a pivotal challenge. This
issue emerges when LLMs excessively reduce the probability of rejected
completions to achieve high rewards, without genuinely meeting their intended
goals. As a result, this leads to overly lengthy generation lacking diversity,
as well as catastrophic forgetting of knowledge. We investigate the underlying
reason behind this issue, which is representation redundancy caused by neuron
collapse in the parameter space. Hence, we propose a novel Weights-Rotated
Preference Optimization (RoPO) algorithm, which implicitly constrains the
output layer logits with the KL divergence inherited from DPO and explicitly
constrains the intermediate hidden states by fine-tuning on a multi-granularity
orthogonal matrix. This design prevents the policy model from deviating too far
from the reference model, thereby retaining the knowledge and expressive
capabilities acquired during pre-training and SFT stages. Our RoPO achieves up
to a 3.27-point improvement on AlpacaEval 2, and surpasses the best baseline by
6.2 to 7.5 points on MT-Bench with merely 0.015% of the trainable parameters,
demonstrating its effectiveness in alleviating the reward hacking problem of
DPO.

</details>


### [19] [ISACL: Internal State Analyzer for Copyrighted Training Data Leakage](https://arxiv.org/abs/2508.17767)
*Guangwei Zhang,Qisheng Su,Jiateng Liu,Cheng Qian,Yanzhou Pan,Yanjie Fu,Denghui Zhang*

Main category: cs.CL

TL;DR: 该研究提出了一种通过分析LLM内部状态来主动检测版权数据泄露风险的方法，在文本生成前进行干预以防止敏感信息泄露。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练过程中可能无意中暴露受版权保护或专有数据，传统方法只能在内容生成后处理泄露问题，存在敏感信息暴露风险。

Method: 使用精选的版权材料数据集训练神经网络分类器，通过分析LLM内部状态在文本生成前检测潜在泄露风险，并与检索增强生成(RAG)系统集成实现早期干预。

Result: 研究结果表明，分析内部状态能有效降低版权数据泄露风险，提供可扩展的解决方案，在保持高质量文本生成的同时确保符合版权法规。

Conclusion: 该方法为AI工作流程提供了平滑集成的主动防护机制，增强了数据隐私和伦理标准，确保版权和许可要求的合规性。

Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,
especially when such data is used for training but not intended for
distribution. Traditional methods address these leaks only after content is
generated, which can lead to the exposure of sensitive information. This study
introduces a proactive approach: examining LLMs' internal states before text
generation to detect potential leaks. By using a curated dataset of copyrighted
materials, we trained a neural network classifier to identify risks, allowing
for early intervention by stopping the generation process or altering outputs
to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)
system, this framework ensures adherence to copyright and licensing
requirements while enhancing data privacy and ethical standards. Our results
show that analyzing internal states effectively mitigates the risk of
copyrighted data leakage, offering a scalable solution that fits smoothly into
AI workflows, ensuring compliance with copyright regulations while maintaining
high-quality text generation. The implementation is available on
GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach](https://arxiv.org/abs/2508.16807)
*Marco S. Tayar,Lucas K. de Oliveira,Juliano D. Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 这篇论文对比研究了PPO和SAC两种深度强化学习算法在GPS失效环境中的无人机导航性能，发现PPO在危险密集导航任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 人工检查限制性工业基础设施(如通风管道)危险且效率低，需要研究在GPS失效环境下的稳健无人机控制策略。

Method: 使用程序生成的管道环境在Genesis模拟环境中训练PPO和SAC算法，设计奖励函数引导无人机突破离线点并避免碰撞。

Result: PPO学习到了稳定策略，完成所有评估演练且产生平滑轨迹；SAC始练于次优行为，只能穿行初始段落后失败。

Conclusion: 在危险密集导航任务中，方程策略方法的训练稳定性覆盖了方程外算法的标称样本效率，程序生成的高保真模拟是开发稳健导航策略的有效测试平台。

Abstract: Inspecting confined industrial infrastructure, such as ventilation shafts, is
a hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)
offer a promising alternative, but GPS-denied environments require robust
control policies to prevent collisions. Deep Reinforcement Learning (DRL) has
emerged as a powerful framework for developing such policies, and this paper
provides a comparative study of two leading DRL algorithms for this task: the
on-policy Proximal Policy Optimization (PPO) and the off-policy Soft
Actor-Critic (SAC). The training was conducted with procedurally generated duct
environments in Genesis simulation environment. A reward function was designed
to guide a drone through a series of waypoints while applying a significant
penalty for collisions. PPO learned a stable policy that completed all
evaluation episodes without collision, producing smooth trajectories. By
contrast, SAC consistently converged to a suboptimal behavior that traversed
only the initial segments before failure. These results suggest that, in
hazard-dense navigation, the training stability of on-policy methods can
outweigh the nominal sample efficiency of off-policy algorithms. More broadly,
the study provides evidence that procedurally generated, high-fidelity
simulations are effective testbeds for developing and benchmarking robust
navigation policies.

</details>


### [21] [SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation](https://arxiv.org/abs/2508.17643)
*Krishna Vinod,Prithvi Jai Ramesh,Pavan Kumar B N,Bharatesh Chakravarthi*

Main category: cs.RO

TL;DR: 开发了一个开源ROS包，用于在Gazebo模拟器中从RGB相机生成事件流，并研究了基于事件的机器人策略在导航和操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级延迟、高动态范围和低功耗等优势，适合在运动模糊、遮挡和光照变化等挑战性条件下进行实时机器人感知，但主流机器人模拟器中缺乏合成事件视觉模拟，阻碍了事件驱动方法在机器人操作和导航任务中的评估。

Method: 提出了一个开源的、用户友好的v2e ROS包，用于Gazebo模拟，能够从RGB相机流无缝生成事件流。使用该包研究基于事件的机器人策略，通过行为克隆训练基于Transformer的策略，并在移动机器人目标跟随和机械臂目标检测与抓取两个代表性场景中与RGB基策略进行比较。

Result: 实验结果表明，事件引导的策略在各种操作条件下始终提供竞争优势，突显了事件驱动感知在改善实时机器人导航和操作方面的潜力。

Conclusion: 这项工作为事件相机更广泛地集成到机器人策略学习中奠定了基础，展示了事件驱动感知在机器人应用中的价值。

Abstract: Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/

</details>
