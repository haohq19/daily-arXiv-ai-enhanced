{"id": "2512.09969", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.09969", "abs": "https://arxiv.org/abs/2512.09969", "authors": ["Paul Hueber", "Luca Peres", "Florian Pitters", "Alejandro Gloriani", "Oliver Rhodes"], "title": "Neuromorphic Eye Tracking for Low-Latency Pupil Detection", "comment": "8 pages, 2 figures, conference", "summary": "Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u795e\u7ecf\u5f62\u6001\u773c\u52a8\u8ffd\u8e2a\u6a21\u578b\uff0c\u4f7f\u7528LIF\u5c42\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u66ff\u4ee3\u4f20\u7edfANN\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u529f\u8017\u3002", "motivation": "\u53ef\u7a7f\u6234\u7cfb\u7edf\u773c\u52a8\u8ffd\u8e2a\u9700\u8981\u4f4e\u5ef6\u8fdf\u548c\u6beb\u74e6\u7ea7\u529f\u8017\uff0c\u4f46\u4f20\u7edf\u57fa\u4e8e\u5e27\u7684\u7ba1\u9053\u5b58\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u6709\u9650\u7684\u95ee\u9898\u3002\u795e\u7ecf\u5f62\u6001\u4f20\u611f\u5668\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709SNN\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u4e13\u95e8\u5316\uff0c\u8981\u4e48\u6027\u80fd\u4e0d\u53ca\u73b0\u4ee3ANN\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u5f62\u6001\u7248\u672c\u7684\u9ad8\u6027\u80fd\u4e8b\u4ef6\u773c\u52a8\u8ffd\u8e2a\u6a21\u578b\uff0c\u7528\u8f7b\u91cf\u7ea7LIF\u5c42\u66ff\u6362\u5176\u5faa\u73af\u548c\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002", "result": "\u6a21\u578b\u83b7\u5f973.7-4.1px\u5e73\u5747\u8bef\u5dee\uff0c\u63a5\u8fd1\u4e13\u7528\u795e\u7ecf\u5f62\u6001\u7cfb\u7edfRetina\uff083.24px\uff09\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u76f8\u6bd4\u6700\u63a5\u8fd1\u7684ANN\u53d8\u4f53\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1120\u500d\uff0c\u7406\u8bba\u8ba1\u7b97\u91cf\u51cf\u5c11850\u500d\u3002\u8fd9\u4e9b\u9ad8\u6548\u53d8\u4f53\u9884\u8ba1\u57281kHz\u4e0b\u4ee53.9-4.9 mW\u529f\u8017\u548c3 ms\u5ef6\u8fdf\u8fd0\u884c\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u6027\u80fd\u4e8b\u4ef6\u773c\u52a8\u8ffd\u8e2a\u67b6\u6784\u53ef\u4ee5\u91cd\u65b0\u8bbe\u8ba1\u4e3aSNN\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u53ef\u7a7f\u6234\u90e8\u7f72\u6240\u9700\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u83b7\u5f97\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2512.10033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10033", "abs": "https://arxiv.org/abs/2512.10033", "authors": ["Sarwan Ali"], "title": "Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation", "comment": null, "summary": "Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $\u03ba=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.", "AI": {"tldr": "HB-SGE\u662f\u4e00\u79cd\u7ed3\u5408\u91cd\u7403\u52a8\u91cf\u548c\u9884\u6d4b\u68af\u5ea6\u5916\u63a8\u7684\u9c81\u68d2\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u75c5\u6001\u548c\u975e\u51f8\u95ee\u9898\u4e0a\u6bd4NAG\u548c\u6807\u51c6\u52a8\u91cf\u65b9\u6cd5\u66f4\u7a33\u5b9a\u3002", "motivation": "NAG\u7b49\u52a0\u901f\u68af\u5ea6\u65b9\u6cd5\u5728\u6761\u4ef6\u826f\u597d\u7684\u95ee\u9898\u4e0a\u6536\u655b\u5feb\uff0c\u4f46\u5728\u75c5\u6001\u6216\u975e\u51f8\u95ee\u9898\u4e0a\u7531\u4e8e\u52a8\u91cf\u79ef\u7d2f\u8fc7\u6fc0\u800c\u5bb9\u6613\u53d1\u6563\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u52a0\u901f\u6548\u679c\u53c8\u80fd\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7a33\u5b9a\u7684\u65b9\u6cd5\u3002", "method": "HB-SGE\u7ed3\u5408\u91cd\u7403\u52a8\u91cf\u548c\u9884\u6d4b\u68af\u5ea6\u5916\u63a8\uff0c\u4f7f\u7528\u5c40\u90e8\u6cf0\u52d2\u8fd1\u4f3c\u4f30\u8ba1\u672a\u6765\u68af\u5ea6\u65b9\u5411\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u7d2f\u79ef\u5386\u53f2\u68af\u5ea6\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u81ea\u9002\u5e94\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u75c5\u6001\u4e8c\u6b21\u95ee\u9898\uff08\u6761\u4ef6\u6570\u03ba=50\uff09\u4e0a\uff0cHB-SGE\u5728119\u6b21\u8fed\u4ee3\u6536\u655b\uff0c\u800cSGD\u548cNAG\u90fd\u53d1\u6563\u3002\u5728\u975e\u51f8Rosenbrock\u51fd\u6570\u4e0a\uff0cHB-SGE\u57282718\u6b21\u8fed\u4ee3\u6536\u655b\uff0c\u800c\u7ecf\u5178\u52a8\u91cf\u65b9\u6cd5\u572810\u6b65\u5185\u5c31\u53d1\u6563\u3002HB-SGE\u5728\u826f\u597d\u6761\u4ef6\u4e0b\u4e0d\u5982NAG\u5feb\uff0c\u4f46\u6bd4SGD\u5feb\u4e14\u66f4\u9c81\u68d2\u3002", "conclusion": "HB-SGE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u4f18\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u75c5\u6001\u548c\u975e\u51f8\u95ee\u9898\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301O(d)\u5185\u5b58\u5f00\u9500\u548c\u4e0e\u6807\u51c6\u52a8\u91cf\u76f8\u540c\u7684\u8d85\u53c2\u6570\u8bbe\u7f6e\u3002"}}
{"id": "2512.10117", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.10117", "abs": "https://arxiv.org/abs/2512.10117", "authors": ["Sangli Teng", "Hang Liu", "Jingyu Song", "Koushil Sreenath"], "title": "CHyLL: Learning Continuous Neural Representations of Hybrid Systems", "comment": null, "summary": "Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.", "AI": {"tldr": "CHyLL\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u6df7\u5408\u7cfb\u7edf\u8fde\u7eed\u795e\u7ecf\u8868\u793a\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u8f68\u8ff9\u5206\u5272\u3001\u4e8b\u4ef6\u51fd\u6570\u6216\u6a21\u5f0f\u5207\u6362\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u5b66\u4e60\u6bcf\u4e2a\u79bb\u6563\u6a21\u5f0f\u7684\u52a8\u6001\uff0c\u4f46\u9762\u4e34\u6a21\u5f0f\u5207\u6362\u548c\u6d41\u4e0d\u8fde\u7eed\u6027\u7684\u7ec4\u5408\u6311\u6218\uff0c\u96be\u4ee5\u51c6\u786e\u5b66\u4e60\u6df7\u5408\u7cfb\u7edf\u7684\u52a8\u6001\u3002", "method": "CHyLL\u5c06\u72b6\u6001\u7a7a\u95f4\u91cd\u65b0\u8868\u8ff0\u4e3a\u5206\u6bb5\u5149\u6ed1\u5546\u6d41\u5f62\uff0c\u5176\u4e2d\u6d41\u53d8\u5f97\u7a7a\u95f4\u8fde\u7eed\u3002\u57fa\u4e8e\u5fae\u5206\u62d3\u6251\u7684\u5d4c\u5165\u5b9a\u7406\uff0c\u5728\u66f4\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u540c\u65f6\u5b66\u4e60\u65e0\u5947\u70b9\u7684\u795e\u7ecf\u5d4c\u5165\u548c\u5176\u4e2d\u7684\u8fde\u7eed\u6d41\u3002", "result": "CHyLL\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6df7\u5408\u7cfb\u7edf\u7684\u6d41\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u6df7\u5408\u7cfb\u7edf\u7684\u62d3\u6251\u4e0d\u53d8\u91cf\u3002\u6210\u529f\u5e94\u7528\u4e8e\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002", "conclusion": "CHyLL\u901a\u8fc7\u5c06\u6df7\u5408\u7cfb\u7edf\u8868\u793a\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u6d41\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6a21\u5f0f\u5207\u6362\u548c\u4e0d\u8fde\u7eed\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a\u6df7\u5408\u7cfb\u7edf\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2512.10179", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.10179", "abs": "https://arxiv.org/abs/2512.10179", "authors": ["Abolfazl Shahrooei", "Luke Arthur", "Om Patel", "Derek Kamper"], "title": "Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography", "comment": "5 pages, 6 figures. Poster included as ancillary file (IEEE_NER2025_NeuromorphicEMG_poster.pdf). Presented at IEEE EMBS NER 2025, also at NC State College of Engineering Applied AI Symposium and NC State ECE Graduate Research Symposium (tied for Best Poster)", "summary": "High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.", "AI": {"tldr": "SNN\u4e0eTCN\u5728HD-sEMG\u4fe1\u53f7\u89e3\u7801\u6307\u5c16\u529b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6bd4\u8f83\uff0cTCN\u8868\u73b0\u66f4\u4f18\u4f46SNN\u6709\u6f5c\u529b\u901a\u8fc7\u4f18\u5316\u7f29\u5c0f\u5dee\u8ddd", "motivation": "\u9ad8\u5bc6\u5ea6\u8868\u9762\u808c\u7535\u4fe1\u53f7\uff08HD-sEMG\uff09\u4e3a\u975e\u4fb5\u5165\u5f0f\u795e\u7ecf\u63a5\u53e3\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46\u5c06\u795e\u7ecf\u6d3b\u52a8\u6620\u5c04\u5230\u7528\u6237\u8fd0\u52a8\u610f\u56fe\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4f5c\u4e3a\u795e\u7ecf\u5f62\u6001\u67b6\u6784\u4e0e\u65f6\u57df\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u5728\u89e3\u7801\u6307\u5c16\u529b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5355\u4e2a\u53c2\u4e0e\u8005\uff0810\u6b21\u8bd5\u9a8c\uff09\u7684\u524d\u81c2\u7535\u6781\u9635\u5217\u6536\u96c6\u6570\u636e\uff0c\u901a\u8fc7FastICA\u5206\u89e3\u83b7\u53d6\u8fd0\u52a8\u5355\u5143\uff08MU\uff09\u653e\u7535\u6d3b\u52a8\u3002\u91c7\u7528\u91cd\u53e0\u7a97\u53e3\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u7528\u7aef\u5230\u7aef\u7684\u56e0\u679c\u5377\u79ef\u67b6\u6784\u3002\u6bd4\u8f83SNN\u548cTCN\u5728\u89e3\u7801\u6307\u5c16\u529b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728\u4fdd\u7559\u8bd5\u9a8c\u4e2d\uff0cTCN\u8fbe\u5230\u4e864.44% MVC RMSE\uff08Pearson r = 0.974\uff09\uff0c\u800cSNN\u8fbe\u5230\u4e868.25% MVC\uff08r = 0.922\uff09\u3002TCN\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8eSNN\u3002", "conclusion": "\u867d\u7136TCN\u66f4\u51c6\u786e\uff0c\u4f46SNN\u4f5c\u4e3a\u4e00\u4e2a\u73b0\u5b9e\u7684\u795e\u7ecf\u5f62\u6001\u57fa\u7ebf\uff0c\u901a\u8fc7\u9002\u5ea6\u7684\u67b6\u6784\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u6709\u671b\u7f29\u5c0f\u4e0eTCN\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2512.10734", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10734", "abs": "https://arxiv.org/abs/2512.10734", "authors": ["Rebekka G\u00f6rge", "Sujan Sai Gannamaneni", "Tabea Naeven", "Hammam Abdelwahab", "H\u00e9ctor Allende-Cid", "Armin B. Cremers", "Lennard Helmer", "Michael Mock", "Anna Schmitz", "Songkai Xue", "Elif Yildirir", "Maximilian Poretschkin", "Stefan Wrobel"], "title": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation", "comment": null, "summary": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u504f\u89c1\u68c0\u6d4b\u4e0e\u7f13\u89e3\u6d41\u7a0b\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\u6765\u5904\u7406\u8868\u793a\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\uff0c\u901a\u8fc7\u4eba\u7c7b\u9a8c\u8bc1\u548c\u6a21\u578b\u5fae\u8c03\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u591a\u65b9\u9762\u504f\u89c1\uff0c\u5305\u62ec\u6709\u5bb3\u8bed\u8a00\u548c\u503e\u659c\u7684\u4eba\u53e3\u5206\u5e03\u3002\u6b27\u76dfAI\u6cd5\u6848\u7b49\u6cd5\u89c4\u8981\u6c42\u8bc6\u522b\u548c\u7f13\u89e3\u9488\u5bf9\u53d7\u4fdd\u62a4\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u8df5\u6307\u5bfc\u548c\u64cd\u4f5c\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u56db\u6b65\u6d41\u7a0b\uff1a1) \u4f7f\u7528LLM\u751f\u6210\u7b26\u5408\u8d28\u91cf\u6807\u51c6\u7684\u8bcd\u8868\u68c0\u6d4b\u76f8\u5173\u7fa4\u4f53\u6807\u7b7e\uff1b2) \u7528\u4eba\u53e3\u8868\u793a\u5206\u6570\u91cf\u5316\u8868\u793a\u504f\u89c1\uff1b3) \u7528\u793e\u4f1a\u8bed\u8a00\u5b66\u8fc7\u6ee4\u68c0\u6d4b\u548c\u7f13\u89e3\u523b\u677f\u5370\u8c61\uff1b4) \u901a\u8fc7\u8bed\u6cd5\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u8865\u507f\u8868\u793a\u504f\u89c1\u3002", "result": "\u5728\u6027\u522b\u3001\u5b97\u6559\u548c\u5e74\u9f84\u6848\u4f8b\u4e2d\uff0c\u6210\u529f\u51cf\u5c11\u4e86\u6587\u672c\u6570\u636e\u96c6\u4e2d\u7684\u8868\u793a\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u3002\u4f46\u4f7f\u7528\u53bb\u504f\u89c1\u6570\u636e\u5fae\u8c03\u7684LLM\u5728\u504f\u89c1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e76\u672a\u4e00\u81f4\u8868\u73b0\u66f4\u597d\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u7f3a\u9677\u3002", "conclusion": "\u867d\u7136\u6570\u636e\u53bb\u504f\u89c1\u65b9\u6cd5\u6709\u6548\uff0c\u4f46\u4ec5\u9760\u6570\u636e\u53bb\u504f\u89c1\u4e0d\u8db3\u4ee5\u786e\u4fdd\u6a21\u578b\u504f\u89c1\u51cf\u5c11\uff0c\u9700\u8981\u66f4\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u64cd\u4f5c\u6765\u89e3\u51b3\u5177\u4f53\u8868\u73b0\u7684\u6a21\u578b\u504f\u89c1\uff0c\u540c\u65f6\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2512.10741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10741", "abs": "https://arxiv.org/abs/2512.10741", "authors": ["Elroy Galbraith", "Chadwick Sutherland", "Donahue Morgan"], "title": "TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage", "comment": null, "summary": "Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.\n  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.\n  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.", "AI": {"tldr": "TRIDENT\u662f\u4e00\u4e2a\u4e09\u5c42\u8c03\u5ea6\u5458\u652f\u6301\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u5931\u8d25\u65f6\u5904\u7406\u52a0\u52d2\u6bd4\u53e3\u97f3\u82f1\u8bed\u7684\u7d27\u6025\u547c\u53eb\uff0c\u901a\u8fc7\u7ed3\u5408\u53e3\u97f3\u8c03\u4f18\u7684ASR\u3001\u672c\u5730\u5b9e\u4f53\u63d0\u53d6\u548c\u751f\u7269\u58f0\u5b66\u75db\u82e6\u68c0\u6d4b\uff0c\u4e3a\u8c03\u5ea6\u5458\u63d0\u4f9b\u8f6c\u5f55\u7f6e\u4fe1\u5ea6\u3001\u7ed3\u6784\u5316\u4e34\u5e8a\u5b9e\u4f53\u548c\u58f0\u97f3\u538b\u529b\u6307\u6807\u4e09\u4e2a\u4e92\u8865\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709\u7d27\u6025\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5bf9\u975e\u6807\u51c6\u82f1\u8bed\u53d8\u4f53\uff08\u7279\u522b\u662f\u52a0\u52d2\u6bd4\u53e3\u97f3\uff09\u5b58\u5728\u7cfb\u7edf\u6027\u6027\u80fd\u4e0b\u964d\uff0c\u5bfc\u81f4\u52a0\u52d2\u6bd4\u4eba\u53e3\u5728\u7d27\u6025\u670d\u52a1\u4e2d\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002\u9700\u8981\u786e\u4fdd\u52a0\u52d2\u6bd4\u53e3\u97f3\u80fd\u591f\u516c\u5e73\u83b7\u5f97\u56fd\u5bb6\u5206\u8bca\u534f\u8bae\u7684\u670d\u52a1\u3002", "method": "\u63d0\u51faTRIDENT\u4e09\u5c42\u67b6\u6784\uff1a1) \u52a0\u52d2\u6bd4\u53e3\u97f3\u8c03\u4f18\u7684ASR\u7cfb\u7edf\uff1b2) \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u672c\u5730\u5b9e\u4f53\u63d0\u53d6\uff1b3) \u751f\u7269\u58f0\u5b66\u75db\u82e6\u68c0\u6d4b\u3002\u7cfb\u7edf\u5c06\u4f4eASR\u7f6e\u4fe1\u5ea6\u89c6\u4e3a\u6709\u4ef7\u503c\u7684\u961f\u5217\u4f18\u5148\u7ea7\u4fe1\u53f7\uff0c\u7279\u522b\u662f\u4e0e\u5347\u9ad8\u7684\u58f0\u97f3\u538b\u529b\u6807\u8bb0\u7ed3\u5408\u65f6\u3002\u5b9e\u4f53\u63d0\u53d6\u5c42\u6355\u6349\u8bad\u7ec3\u6709\u7d20\u7684\u54cd\u5e94\u8005\u548c\u51b7\u9759\u65c1\u89c2\u8005\u53ef\u80fd\u62a5\u544a\u7684\u751f\u547d\u5a01\u80c1\u7d27\u6025\u60c5\u51b5\u3002", "result": "\u5efa\u7acb\u4e86\u53e3\u97f3\u5f39\u6027\u7d27\u6025AI\u6846\u67b6\uff0c\u786e\u4fdd\u52a0\u52d2\u6bd4\u53e3\u97f3\u80fd\u591f\u516c\u5e73\u83b7\u5f97\u56fd\u5bb6\u5206\u8bca\u534f\u8bae\uff08ESI\u5e38\u89c4\u64cd\u4f5c\u548cSTART\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u4ef6\uff09\u3002\u7cfb\u7edf\u8bbe\u8ba1\u8003\u8651\u4e86\u707e\u96be\u573a\u666f\u4e0b\u7684\u79bb\u7ebf\u64cd\u4f5c\u3002\u52a0\u52d2\u6bd4\u7d27\u6025\u547c\u53eb\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u662f\u672a\u6765\u5de5\u4f5c\u3002", "conclusion": "TRIDENT\u901a\u8fc7\u5c06\u4f4eASR\u7f6e\u4fe1\u5ea6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6709\u4ef7\u503c\u7684\u4f18\u5148\u7ea7\u4fe1\u53f7\uff0c\u7ed3\u5408\u5fc3\u7406\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u538b\u529b\u8bf1\u5bfc\u7684\u8bed\u7801\u8f6c\u6362\u7406\u8bba\uff0c\u4e3a\u53e3\u97f3\u5f39\u6027\u7d27\u6025AI\u5efa\u7acb\u4e86\u6846\u67b6\uff0c\u786e\u4fdd\u52a0\u52d2\u6bd4\u4eba\u53e3\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u83b7\u5f97\u516c\u5e73\u670d\u52a1\u3002"}}
{"id": "2512.10287", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.10287", "abs": "https://arxiv.org/abs/2512.10287", "authors": ["Apurba Sarker", "Reza T. Batley", "Darshan Sarojini", "Sourav Saha"], "title": "A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field", "comment": "24 pages, 15 figures", "summary": "Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.", "AI": {"tldr": "KHRONOS\u662f\u4e00\u79cd\u57fa\u4e8e\u6838\u7684\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u7a00\u758f\u9ad8\u4fdd\u771f\u6570\u636e\u548c\u4f4e\u4fdd\u771f\u4fe1\u606f\u6765\u9884\u6d4b\u7a7a\u6c14\u52a8\u529b\u5b66\u573a\uff0c\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u4f20\u7edf\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u66f4\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u63d0\u4f9b\u66f4\u5feb\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u7a7a\u6c14\u52a8\u529b\u5b66\u6a21\u62df\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5feb\u901f\u66ff\u4ee3\u6a21\u578b\u7528\u4e8e\u8bbe\u8ba1\u548c\u4f18\u5316\u5e94\u7528\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u7a00\u758f\u9ad8\u4fdd\u771f\u6570\u636e\u548c\u4f4e\u4fdd\u771f\u4fe1\u606f\u878d\u5408\u65b9\u9762\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faKHRONOS\u6a21\u578b\uff0c\u57fa\u4e8e\u53d8\u5206\u539f\u7406\u3001\u63d2\u503c\u7406\u8bba\u548c\u5f20\u91cf\u5206\u89e3\u6784\u5efa\u3002\u4f7f\u7528AirfRANS\u6570\u636e\u96c6\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u57fa\u51c6\uff0cNeuralFoil\u751f\u6210\u4f4e\u4fdd\u771f\u5bf9\u5e94\u6570\u636e\u3002\u4e0e\u591a\u5c42\u611f\u77e5\u673a(MLP)\u3001\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u548c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u8fdb\u884c\u6bd4\u8f83\u3002\u8003\u8651\u4e0d\u540c\u9ad8\u4fdd\u771f\u6570\u636e\u53ef\u7528\u6027\u6c34\u5e73(0%\u300110%\u300130%)\u548c\u590d\u6742\u51e0\u4f55\u53c2\u6570\u5316\uff0c\u9884\u6d4b\u7ffc\u578b\u8868\u9762\u538b\u529b\u7cfb\u6570\u5206\u5e03\u3002", "result": "\u6240\u6709\u6a21\u578b\u6700\u7ec8\u8fbe\u5230\u53ef\u6bd4\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46KHRONOS\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002KHRONOS\u9700\u8981\u6570\u91cf\u7ea7\u66f4\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u53ef\u6bd4\u7cbe\u5ea6\u4e0b\u63d0\u4f9b\u66f4\u5feb\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u9ad8\u4fdd\u771f\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "KHRONOS\u53ca\u5176\u7c7b\u4f3c\u67b6\u6784\u5728\u591a\u4fdd\u771f\u7a7a\u6c14\u52a8\u529b\u5b66\u573a\u9884\u6d4b\u4e2d\u80fd\u591f\u6709\u6548\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u7a7a\u6c14\u52a8\u529b\u5b66\u8bbe\u8ba1\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u66ff\u4ee3\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10327", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.10327", "abs": "https://arxiv.org/abs/2512.10327", "authors": ["Cai Xu", "Jinlong Liu", "Yilin Zhang", "Ziyu Guan", "Wei Zhao"], "title": "Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering", "comment": null, "summary": "Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.", "AI": {"tldr": "ISMVC\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u91cf\u7684\u9009\u62e9\u6027\u63d2\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u7f3a\u5931\u4f4d\u7f6e\u7684\u4fe1\u606f\u91cf\u6765\u51b3\u5b9a\u662f\u5426\u63d2\u8865\uff0c\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6df7\u5408\u9ad8\u65af\u5148\u9a8c\u5b66\u4e60\u805a\u7c7b\u53cb\u597d\u7684\u8868\u793a\uff0c\u5728\u975e\u5e73\u8861\u7f3a\u5931\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u89c6\u56fe\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff1a\u57fa\u4e8e\u63d2\u8865\u7684\u65b9\u6cd5\u53ef\u80fd\u5f15\u5165\u566a\u58f0\u548c\u504f\u5dee\uff0c\u800c\u514d\u63d2\u8865\u65b9\u6cd5\u5728\u4e25\u91cd\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7f3a\u4e4f\u8de8\u89c6\u56fe\u4e92\u8865\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u9009\u62e9\u6027\u63d2\u8865\u7b56\u7565\u6765\u5e73\u8861\u4fe1\u606f\u5229\u7528\u548c\u566a\u58f0\u63a7\u5236\u3002", "method": "ISMVC\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u89c6\u56fe\u5185\u76f8\u4f3c\u6027\u548c\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u8bc4\u4f30\u6bcf\u4e2a\u7f3a\u5931\u4f4d\u7f6e\u7684\u4fe1\u606f\u91cf\uff1b2\uff09\u53ea\u5728\u6709\u8db3\u591f\u652f\u6301\u65f6\u9009\u62e9\u6027\u63d2\u8865\uff1b3\uff09\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6df7\u5408\u9ad8\u65af\u5148\u9a8c\u5b66\u4e60\u805a\u7c7b\u53cb\u597d\u8868\u793a\uff1b4\uff09\u5728\u5206\u5e03\u5c42\u9762\u8fdb\u884c\u63d2\u8865\uff0c\u7a33\u5b9a\u540e\u9a8c\u5206\u5e03\u805a\u5408\u5e76\u5efa\u6a21\u63d2\u8865\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cISMVC\u5728\u66f4\u73b0\u5b9e\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u975e\u5e73\u8861\u7f3a\u5931\u573a\u666f\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u63d2\u8865\u548c\u514d\u63d2\u8865\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9a71\u52a8\u4e14\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u96c6\u6210\u5230\u73b0\u6709IMC\u6a21\u578b\u4e2d\u3002", "conclusion": "ISMVC\u901a\u8fc7\u9009\u62e9\u6027\u63d2\u8865\u7b56\u7565\u6709\u6548\u5e73\u8861\u4e86\u4fe1\u606f\u5229\u7528\u548c\u566a\u58f0\u63a7\u5236\uff0c\u5728\u5206\u5e03\u5c42\u9762\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u4e0d\u5b8c\u6574\u591a\u89c6\u56fe\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10355", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10355", "abs": "https://arxiv.org/abs/2512.10355", "authors": ["Hyunsung Kim", "Sangwoo Seo", "Hoyoung Choi", "Tom Boomstra", "Jinsung Yoon", "Chanyoung Park"], "title": "Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks", "comment": null, "summary": "Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.", "AI": {"tldr": "DEFCON\u662f\u4e00\u4e2a\u8bc4\u4f30\u8db3\u7403\u9632\u5b88\u8d21\u732e\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u91cf\u5316\u7403\u5458\u5728\u6bcf\u6b21\u8fdb\u653b\u60c5\u5883\u4e2d\u7684\u9632\u5b88\u4ef7\u503c\uff0c\u901a\u8fc7\u8ba1\u7b97\u5bf9\u624b\u9884\u671f\u63a7\u7403\u4ef7\u503c\u7684\u53d8\u5316\u6765\u5206\u914d\u9632\u5b88\u79ef\u5206\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6709\u7403\u52a8\u4f5c\uff08\u5982\u62e6\u622a\u3001\u62a2\u65ad\uff09\uff0c\u4f46\u6709\u6548\u9632\u5b88\u5f80\u5f80\u4f53\u73b0\u5728\u963b\u6b62\u5371\u9669\u673a\u4f1a\u53d1\u751f\u4e4b\u524d\uff0c\u5bfc\u81f4\u9632\u5b88\u7403\u5458\u7684\u771f\u5b9e\u5f71\u54cd\u672a\u88ab\u5145\u5206\u8861\u91cf\u3002", "method": "\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u4f30\u8ba1\u6bcf\u4e2a\u8fdb\u653b\u9009\u9879\u7684\u6210\u529f\u6982\u7387\u548c\u9884\u671f\u4ef7\u503c\uff0c\u4ee5\u53ca\u6bcf\u4e2a\u9632\u5b88\u7403\u5458\u7684\u9632\u5b88\u8d23\u4efb\uff0c\u8ba1\u7b97\u8fdb\u653b\u65b9\u5728\u6bcf\u6b21\u52a8\u4f5c\u524d\u540e\u7684\u9884\u671f\u63a7\u7403\u4ef7\u503c\u53d8\u5316\uff0c\u6839\u636e\u9632\u5b88\u7403\u5458\u662f\u5426\u51cf\u5c11\u6216\u589e\u52a0\u5bf9\u624b\u7684EPV\u6765\u5206\u914d\u6b63\u8d1f\u79ef\u5206\u3002", "result": "\u57282023-24\u8d5b\u5b63\u8bad\u7ec3\u30012024-25\u8d5b\u5b63\u8377\u7532\u8054\u8d5b\u6570\u636e\u4e0a\u8bc4\u4f30\uff0cDEFCON\u805a\u5408\u7684\u7403\u5458\u79ef\u5206\u4e0e\u5e02\u573a\u4f30\u503c\u5448\u73b0\u5f3a\u6b63\u76f8\u5173\u3002\u5c55\u793a\u4e86\u591a\u79cd\u5b9e\u9645\u5e94\u7528\uff1a\u6bd4\u8d5b\u4e2d\u9632\u5b88\u8d21\u732e\u65f6\u95f4\u7ebf\u3001\u7403\u573a\u533a\u57df\u7a7a\u95f4\u5206\u6790\u3001\u653b\u9632\u7403\u5458\u4e92\u52a8\u914d\u5bf9\u603b\u7ed3\u3002", "conclusion": "DEFCON\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u91cf\u5316\u9632\u5b88\u8d21\u732e\u7684\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u9632\u5b88\u7403\u5458\u7684\u771f\u5b9e\u5f71\u54cd\uff0c\u5e76\u5177\u6709\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.10352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.10352", "abs": "https://arxiv.org/abs/2512.10352", "authors": ["Keyi Chen", "Mingze Sun", "Zhenyu Liu", "Zhangquan Chen", "Ruqi Huang"], "title": "Topology-Agnostic Animal Motion Generation from Text Prompt", "comment": "10 pages, 7 figures.Conference submission", "summary": "Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.", "AI": {"tldr": "OmniZoo\uff1a\u9996\u4e2a\u5927\u89c4\u6a21\u5f02\u6784\u52a8\u7269\u8fd0\u52a8\u6570\u636e\u96c6\u4e0e\u901a\u7528\u81ea\u56de\u5f52\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u9aa8\u9abc\u62d3\u6251\u548c\u6587\u672c\u9a71\u52a8\u7684\u8fd0\u52a8\u751f\u6210", "motivation": "\u73b0\u6709\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u56fa\u5b9a\u9aa8\u9abc\u6a21\u677f\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540c\u6216\u6270\u52a8\u62d3\u6251\u7684\u9aa8\u9abc\u7ed3\u6784\u3002\u540c\u65f6\u7f3a\u4e4f\u5927\u89c4\u6a21\u5f02\u6784\u52a8\u7269\u8fd0\u52a8\u6570\u636e\u548c\u7edf\u4e00\u751f\u6210\u6846\u67b6\u6765\u8054\u5408\u5efa\u6a21\u4efb\u610f\u9aa8\u9abc\u62d3\u6251\u4e0e\u6587\u672c\u6761\u4ef6\u3002", "method": "\u63d0\u51faOmniZoo\u6570\u636e\u96c6\uff08140\u4e2a\u7269\u79cd\uff0c32,979\u4e2a\u5e8f\u5217\uff0c\u591a\u6a21\u6001\u6807\u6ce8\uff09\u548c\u901a\u7528\u81ea\u56de\u5f52\u8fd0\u52a8\u751f\u6210\u6846\u67b6\u3002\u6838\u5fc3\u662f\u62d3\u6251\u611f\u77e5\u9aa8\u9abc\u5d4c\u5165\u6a21\u5757\uff0c\u5c06\u4efb\u610f\u9aa8\u9abc\u7684\u51e0\u4f55\u548c\u7ed3\u6784\u5c5e\u6027\u7f16\u7801\u5230\u5171\u4eab\u6807\u8bb0\u7a7a\u95f4\uff0c\u4e0e\u6587\u672c\u8bed\u4e49\u65e0\u7f1d\u878d\u5408\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u6587\u672c\u63d0\u793a\u548c\u76ee\u6807\u9aa8\u9abc\u751f\u6210\u65f6\u95f4\u8fde\u8d2f\u3001\u7269\u7406\u5408\u7406\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\uff0c\u5e76\u652f\u6301\u8de8\u7269\u79cd\u8fd0\u52a8\u98ce\u683c\u8fc1\u79fb\u3002", "conclusion": "OmniZoo\u89e3\u51b3\u4e86\u5f53\u524d\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u7684\u6838\u5fc3\u9650\u5236\uff0c\u4e3a\u4efb\u610f\u9aa8\u9abc\u62d3\u6251\u7684\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u7edf\u4e00\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.10363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.10363", "abs": "https://arxiv.org/abs/2512.10363", "authors": ["Mingyu Jeon", "Jisoo Yang", "Sungjin Han", "Jinkwon Hwang", "Sunjae Yoon", "Jonghee Kim", "Junyeoung Kim"], "title": "Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos", "comment": null, "summary": "Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \\textbf{P}oint-\\textbf{to}-\\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\\% on R5@0.1 on MAD).", "AI": {"tldr": "P2S\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8de8\u5ea6\u751f\u6210\u548c\u67e5\u8be2\u5206\u89e3\u6280\u672f\uff0c\u5728\u957f\u89c6\u9891\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u65f6\u95f4\u5b9a\u4f4d\uff0c\u8d85\u8d8a\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u957f\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u95ee\u9898\uff1a\u6709\u76d1\u7763\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\u3001\u6cdb\u5316\u80fd\u529b\u5f31\uff1b\u96f6\u6837\u672c\u65b9\u6cd5\u5b58\u5728\u641c\u7d22\u9636\u6bb5\u5019\u9009\u7206\u70b8\u548c\u7cbe\u70bc\u9636\u6bb5\u9700\u8981\u9ad8\u6210\u672cVLM\u9a8c\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faP2S\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u81ea\u9002\u5e94\u8de8\u5ea6\u751f\u6210\u5668\u9632\u6b62\u641c\u7d22\u9636\u6bb5\u5019\u9009\u7206\u70b8\uff1b2\uff09\u67e5\u8be2\u5206\u89e3\u6280\u672f\u5728\u4e0d\u4f9d\u8d56\u9ad8\u6210\u672cVLM\u7684\u60c5\u51b5\u4e0b\u7cbe\u70bc\u5019\u9009\u3002", "result": "P2S\u5728MAD\u6570\u636e\u96c6\u4e0aR5@0.1\u6307\u6807\u6bd4\u6709\u76d1\u7763SOTA\u65b9\u6cd5\u63d0\u53473.7%\uff0c\u662f\u9996\u4e2a\u80fd\u5728\u5c0f\u65f6\u7ea7\u957f\u89c6\u9891\u4e2d\u8fdb\u884c\u65f6\u95f4\u5b9a\u4f4d\u7684\u96f6\u6837\u672c\u6846\u67b6\u3002", "conclusion": "P2S\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u4e2d\u7684\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u7cbe\u70bc\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u6709\u76d1\u7763\u65b9\u6cd5\u3002"}}
{"id": "2512.10659", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10659", "abs": "https://arxiv.org/abs/2512.10659", "authors": ["Tommaso Amico", "Pernille Matthews", "Lena Krieger", "Arthur Zimek", "Ira Assent"], "title": "DCFO Additional Material", "comment": null, "summary": "Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.", "AI": {"tldr": "\u63d0\u51faDCFO\u65b9\u6cd5\uff0c\u4e13\u95e8\u4e3aLOF\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u901a\u8fc7\u6570\u636e\u7a7a\u95f4\u5206\u533a\u5b9e\u73b0\u68af\u5ea6\u4f18\u5316\uff0c\u572850\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f02\u5e38\u68c0\u6d4b\u9700\u8981\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u53cd\u4e8b\u5b9e\u89e3\u91ca\u80fd\u8bf4\u660e\u5982\u4f55\u6539\u53d8\u6570\u636e\u70b9\u4f7f\u5176\u4e0d\u88ab\u5224\u4e3a\u5f02\u5e38\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u5f02\u5e38\u68c0\u6d4b\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4e14\u672a\u9488\u5bf9LOF\u7b49\u7ecf\u5178\u7b97\u6cd5\uff0c\u800cLOF\u867d\u5e7f\u6cdb\u5e94\u7528\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faDCFO\u65b9\u6cd5\uff0c\u4e13\u95e8\u4e3aLOF\u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002\u901a\u8fc7\u5c06\u6570\u636e\u7a7a\u95f4\u5212\u5206\u4e3aLOF\u884c\u4e3a\u5e73\u6ed1\u7684\u533a\u57df\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff0c\u751f\u6210\u6700\u5c0f\u6539\u53d8\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u572850\u4e2aOpenML\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cDCFO\u5728\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u90bb\u8fd1\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\uff0c\u59cb\u7ec8\u4f18\u4e8e\u57fa\u51c6\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "DCFO\u6210\u529f\u89e3\u51b3\u4e86LOF\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.01426", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01426", "abs": "https://arxiv.org/abs/2508.01426", "authors": ["Hang Ni", "Weijia Zhang", "Hao Liu"], "title": "UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting", "comment": "35 pages, 80 figures, submitted to ACM KDD 2026 conference", "summary": "Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.", "AI": {"tldr": "UniExtreme\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\u548c\u4e8b\u4ef6\u5148\u9a8c\u589e\u5f3a\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u6837\u5316\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5929\u6c14\u9884\u6d4b\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u4e00\u822c\u5929\u6c14\u6761\u4ef6\u6216\u7279\u5b9a\u7c7b\u578b\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u6837\u5316\u6781\u7aef\u4e8b\u4ef6\u7684\u5927\u6c14\u6a21\u5f0f\u3002\u6781\u7aef\u4e8b\u4ef6\u5177\u6709\u4e24\u4e2a\u5173\u952e\u7279\u5f81\uff1a\u4e0e\u6b63\u5e38\u5929\u6c14\u6a21\u5f0f\u7684\u5149\u8c31\u5dee\u5f02\uff0c\u4ee5\u53ca\u591a\u6837\u5316\u6781\u7aef\u4e8b\u4ef6\u7684\u5206\u5c42\u9a71\u52a8\u548c\u5730\u7406\u6df7\u5408\u3002", "method": "\u63d0\u51faUniExtreme\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684Beta\u5206\u5e03\u6ee4\u6ce2\u5668\u548c\u591a\u7c92\u5ea6\u5149\u8c31\u805a\u5408\u6355\u6349\u6b63\u5e38\u4e0e\u6781\u7aef\u5929\u6c14\u7684\u533a\u57df\u5149\u8c31\u5dee\u5f02\uff1b2) \u4e8b\u4ef6\u5148\u9a8c\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u7ea7\u8bb0\u5fc6\u878d\u5408\u7f51\u7edc\u878d\u5165\u533a\u57df\u7279\u5b9a\u7684\u6781\u7aef\u4e8b\u4ef6\u5148\u9a8c\uff0c\u89e3\u51b3\u6781\u7aef\u591a\u6837\u6027\u548c\u590d\u5408\u6781\u7aef\u6a21\u5f0f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniExtreme\u5728\u6781\u7aef\u5929\u6c14\u548c\u4e00\u822c\u5929\u6c14\u9884\u6d4b\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5728\u591a\u6837\u5316\u6781\u7aef\u573a\u666f\u4e2d\u7684\u5353\u8d8a\u9002\u5e94\u6027\u3002", "conclusion": "UniExtreme\u901a\u8fc7\u6574\u5408\u5149\u8c31\u5dee\u5f02\u5efa\u6a21\u548c\u4e8b\u4ef6\u5148\u9a8c\u589e\u5f3a\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u6837\u5316\u6781\u7aef\u4e8b\u4ef6\u7684\u590d\u6742\u6a21\u5f0f\u3002"}}
