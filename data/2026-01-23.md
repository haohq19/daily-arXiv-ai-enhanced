<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 提出一种基于传感器物理的NeRF框架，从单曝光模糊LDR图像和对应事件数据中实现锐利HDR新视角合成


<details>
  <summary>Details</summary>
Motivation: 现有方法使用事件数据解决低动态范围模糊图像的新视角合成问题，但忽略了相机输出与物理世界辐射之间的传感器物理不匹配，导致HDR和去模糊效果不佳

Method: 提出统一的传感器物理基础NeRF框架：1) 用NeRF直接表示HDR域中的实际场景辐射；2) 引入像素级RGB映射场对齐渲染像素值与传感器记录的LDR像素值；3) 设计事件映射场桥接物理场景动态与事件传感器输出；4) 联合优化两个映射场与NeRF网络

Result: 在收集和公开数据集上的实验表明，该方法能够实现最先进的去模糊HDR新视角合成结果

Conclusion: 通过传感器物理建模和联合优化框架，成功解决了从单曝光模糊LDR图像和事件数据中恢复锐利HDR 3D表示的问题

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [2] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream：一个事件感知的视频流理解框架，通过检测语义连贯的事件边界来触发语言生成，减少冗余处理并保持长期记忆


<details>
  <summary>Details</summary>
Motivation: 现有视频流理解系统存在两个主要问题：1）固定间隔解码导致重复输出；2）缓存剪枝丢弃关键时序信息。多模态大语言模型在处理长视频流时面临冗余帧处理和快速遗忘过去上下文的问题。

Method: 提出事件感知框架Event-VStream，将连续视频表示为离散的语义连贯事件序列。系统通过整合运动、语义和预测线索来检测有意义的状态转换，仅在事件边界触发语言生成。每个事件嵌入被整合到持久内存库中，支持长期推理同时保持低延迟。

Result: 在OVOBench-Realtime和长格式Ego4D评估中表现优异：相比VideoLLM-Online-8B基线提升10.4分；使用通用LLaMA-3-8B文本骨干即可接近Flash-VStream-7B性能；在2小时Ego4D流上保持约70%的GPT-5胜率。

Conclusion: Event-VStream通过事件感知的框架有效解决了长视频流理解中的冗余处理和记忆遗忘问题，在保持低延迟的同时实现了长期推理能力，为实时视频理解提供了新思路。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice是一个混合序列预测系统，通过二元置信度门控有条件地激活学习到的行为结构，在推荐系统、科学时间序列和金融市场中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决在安全关键应用中管理认知不确定性的问题，需要一种能够智能地激活或拒绝行为模式的结构化预测系统。

Method: 将行为窗口聚类为行为原型，使用二元置信度门控在置信度超过阈值时激活基于原型的评分，否则回退到基线预测。

Result: 在MovieLens上，Lattice比LSTM基线提升31.9%，比SASRec和BERT4Rec分别提升109.4%和218.6%；在LIGO和金融数据中能正确拒绝原型激活；在transformer骨干上保持中性（0.0%改进）。

Conclusion: 置信度门控是一种有前景的架构原则，能够在模式适用时激活、不适用时拒绝、冗余时优雅地推迟，有效管理安全关键应用中的认知不确定性。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [4] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther是一个PyTorch兼容的RandNLA库，通过高效实现随机化算法显著减少深度学习模型训练的内存占用（BERT上可达75%），同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型训练受GPU内存和计算限制，虽然RandNLA技术能压缩模型，但缺乏统一的生产级库阻碍了这些方法的广泛应用。

Method: 开发Panther库，整合成熟的RandNLA算法，提供高效的即插即用组件：草图线性层、2D卷积、多头注意力、随机矩阵分解。通过自定义C++/CUDA后端(pawX)优化CPU/GPU实现。

Result: 仅需几行代码将标准PyTorch线性层替换为Panther层，在BERT上实现高达75%的内存节省，同时保持可比较的损失性能。

Conclusion: Panther证明了RandNLA技术的有效性并降低了采用门槛，为深度学习社区提供了实用的内存优化工具。

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [5] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++通过引入两种漂移检测机制（基于熵和KL散度）和自适应重置策略，在持续测试时适应任务中显著提升了长期性能，特别是在CCC基准测试上取得了约3%的绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应方法（如Tent、EATA等）在短期分布漂移下表现良好，但在快速变化或极长时间跨度的测试分布下表现不佳。CCC基准测试包含750万个样本的连续流，其中腐败类型和严重程度不断变化，现有方法难以应对这种长期挑战。

Method: RDumb++是RDumb的扩展，引入了两种漂移检测机制：1）基于熵的漂移评分，2）基于KL散度的漂移评分。结合自适应重置策略，使模型能够检测到累积适应何时变得有害，并在预测崩溃发生前恢复。

Result: 在CCC-medium基准测试（三种速度、三种种子，共9次运行，每次包含100万个样本）上，RDumb++始终优于RDumb，获得约3%的绝对准确率提升，并在整个流中保持稳定的适应性能。消融实验进一步表明漂移感知重置对于防止崩溃和实现可靠的长期CTTA至关重要。

Conclusion: 漂移检测机制和自适应重置策略是解决长期持续测试时适应问题的关键。RDumb++通过检测有害的累积适应并在崩溃前恢复，在极长时间跨度的分布漂移下实现了稳定且可靠的性能。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [6] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 提出首个从部分观测数据学习神经算子的系统框架，通过掩码预测训练和物理感知潜在传播器解决部分观测问题，在多种PDE任务上实现18-69%误差降低。


<details>
  <summary>Details</summary>
Motivation: 现实科学应用中常遇到不完整的观测数据（传感器限制、地理约束、测量成本），而现有神经算子假设完全观测空间输入，严重限制了在实际应用中的适用性。

Method: 提出Latent Autoregressive Neural Operator (LARNO)，包含两个核心组件：1) 掩码预测训练策略，通过策略性掩码观测区域创建人工监督；2) 物理感知潜在传播器，在潜在空间中通过边界优先自回归生成重建解。

Result: 在POBench-PDE基准测试中，LARNO在缺失率低于50%的补丁式缺失情况下，所有基准上实现18-69%的相对L2误差降低，包括真实世界气候预测，能有效处理高达75%缺失率的实际场景。

Conclusion: 该框架首次系统解决了从部分观测学习神经算子的问题，通过创新的训练策略和潜在空间传播机制，在一定程度上弥合了理想化研究设置与现实世界科学计算复杂性之间的差距。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [7] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 提出一种后训练结构分析方法，通过统计准则和结构退火去除无用的参数依赖，实现大规模AI模型的结构化并行推理，无需修改模型功能或接口。


<details>
  <summary>Details</summary>
Motivation: 当前大规模AI模型的推理通常在密集参数矩阵上进行，导致推理成本和系统复杂度随模型规模不可持续地增长。这种限制并非源于模型容量不足，而是因为将后训练推理系统视为单一整体算子，忽略了学习过程中形成的内部结构。

Method: 研究发现大型模型中的梯度更新事件高度局部化和选择性，许多参数依赖在训练后与其初始化分布统计上无法区分。基于此观察，提出后训练统计准则和结构退火程序，移除无支持的依赖关系，揭示稳定、独立的子结构。

Result: 建立了后训练、模型无关的推理系统结构视图，实现了结构化并行推理，无需修改模型功能或接口。

Conclusion: 通过揭示大型AI模型中固有的可分解结构，提供了一种降低推理成本和系统复杂度的新方法，为大规模模型的高效部署开辟了新途径。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [8] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 将XAI应用于工业CPS中的时间序列预测模型，通过SHAP值分析数据分解组件对预测的影响，发现训练数据上下文信息不足，通过增加窗口大小提升模型性能


<details>
  <summary>Details</summary>
Motivation: 工业CPS对可靠性和安全性要求极高，但集成的深度学习模型存在不透明性问题，需要严格评估以防止在未见数据上出现意外行为

Method: 应用可解释AI（XAI）特别是SHAP值分析，研究时间序列数据分解组件对模型预测的影响，基于XAI发现增加数据实例的窗口大小

Result: XAI分析显示模型训练缺乏足够的上下文信息，通过增加窗口大小能够改善模型性能

Conclusion: XAI不仅能解释模型行为，还能指导改进模型设计，通过增加上下文信息可以提升工业CPS中ML模型的预测性能

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出双过程代理不确定性量化框架，将语言化不确定性转化为主动控制信号，解决AI代理在长程推理中的"幻觉螺旋"问题


<details>
  <summary>Details</summary>
Motivation: AI代理在长程推理中面临"幻觉螺旋"问题，早期认知错误会不可逆传播。现有方法存在局限性：不确定性量化方法只能被动诊断风险而不解决问题，自我反思机制则存在连续或漫无目的的修正问题

Method: 提出双过程代理不确定性量化框架，包含两个互补机制：系统1（不确定性感知记忆）隐式传播语言化置信度和语义解释以防止盲目决策；系统2（不确定性感知反思）利用这些解释作为理性线索，仅在必要时触发有针对性的推理时解析

Result: 在闭环基准测试和开放式深度研究任务上的广泛实验表明，这种无需训练的方法实现了卓越的性能和轨迹级校准

Conclusion: 该原则性框架代表了向可靠代理迈出的重要一步，能够动态平衡高效执行和深度思考

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [10] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC是一种从零开始学习的离策略actor-critic方法，仅需单条专家轨迹，通过sigmoid有界熵项防止负熵驱动的优化，减少Q函数振荡，在真实机器人任务中实现低成本强化学习部署。


<details>
  <summary>Details</summary>
Motivation: 真实世界强化学习面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据或大规模预训练，缺乏低成本、数据需求少的实用方案。

Method: 提出SigEnt-SAC方法，核心设计是sigmoid有界熵项，防止负熵驱动的优化导致分布外动作，减少Q函数振荡。仅需单条专家轨迹从零开始学习。

Result: 在D4RL基准测试中显著缓解Q函数振荡，比先前方法更快达到100%成功率。在四种真实机器人任务中，仅需少量真实世界交互就能学习成功策略。

Conclusion: SigEnt-SAC为真实世界强化学习部署提供了低成本、实用的途径，仅需最小数据需求就能在真实机器人任务中学习成功策略。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [11] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 该论文提出了一种结合视觉语言模型与外部知识检索的方法，以改进气候虚假信息的检测能力，特别是针对训练数据中未包含的最新事件和误导性图像。


<details>
  <summary>Details</summary>
Motivation: 气候虚假信息在数字世界中日益严重，特别是社交媒体上广泛传播的误导性图像和视频。这些虚假信息往往难以检测，可能延迟气候行动。现有的视觉语言模型仅依赖训练时的知识，无法处理最新事件或更新，存在局限性。

Method: 通过将视觉语言模型与外部知识检索相结合，系统可以获取最新的信息，包括反向图像搜索结果、在线事实核查和可信专家内容。这些外部知识帮助系统更好地评估图像及其声明的准确性，判断其是否为准确、误导、虚假或无法验证。

Result: 该方法提高了模型处理现实世界气候虚假信息的能力，能够更有效地识别误导性图像和视频。系统通过整合最新信息，增强了应对快速变化信息环境的能力。

Conclusion: 结合外部知识检索的视觉语言模型方法能够克服传统模型仅依赖训练知识的局限性，显著提升气候虚假信息的检测效果，有助于保护公众对科学的理解，应对快速变化的信息环境挑战。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: RAG架构能有效减少LLM在公共卫生政策问答中的幻觉，高级RAG配置比基础RAG和普通LLM在事实准确性上有显著提升


<details>
  <summary>Details</summary>
Motivation: LLM在公共卫生政策领域应用时容易产生幻觉，这在信息完整性要求高的高风险环境中构成严重障碍，需要寻找可靠方法来减少错误信息

Method: 使用Mistral-7B-Instruct-v0.2模型和all-MiniLM-L6-v2嵌入模型，比较普通LLM、基础RAG和采用交叉编码器重排序的高级RAG管道，测试两种分块策略（递归字符分块和基于令牌的语义分割）对CDC政策文档问答准确性的影响

Result: 基础RAG在忠实度上（0.621）比普通LLM基线（0.347）有显著提升，高级RAG配置达到最高的忠实度平均值0.797，但文档分割的结构限制仍然是多步推理任务的主要瓶颈

Conclusion: 两阶段检索机制对于实现领域特定政策问答所需的精度至关重要，高级RAG架构能有效减少LLM幻觉，但需要进一步优化文档分割策略以支持复杂推理任务

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [13] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 本文提出Trajectory Replay via Concept-Basis Reconstruction框架，通过概念指纹对齐和概念原子重构，将拒绝干预从捐赠模型转移到目标模型，证明拒绝行为源于跨模型的通用低维语义电路。


<details>
  <summary>Details</summary>
Motivation: 对齐LLM中的拒绝行为通常被视为模型特定的，但作者假设它源于跨模型共享的通用低维语义电路。为了验证这一假设，需要开发一种方法在不同架构和训练机制之间转移拒绝干预。

Method: 提出Trajectory Replay via Concept-Basis Reconstruction框架：1）通过概念指纹对齐不同模型的层；2）使用共享的"概念原子"重构拒绝方向；3）通过权重SVD稳定性保护防止能力退化，将干预投影到低方差权重子空间。

Result: 在8个模型对（包括GPT-OSS-20B和GLM-4）上的评估表明，转移的"配方"能持续减弱拒绝行为，同时保持模型性能，为安全对齐的语义通用性提供了有力证据。

Conclusion: 拒绝行为源于跨不同架构和训练机制的LLM共享的通用低维语义电路，这为理解安全对齐的语义基础提供了新视角，并展示了跨模型干预转移的可行性。

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>
