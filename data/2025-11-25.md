<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 21]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach](https://arxiv.org/abs/2511.17618)
*Ju-Young Oh*

Main category: cs.CV

TL;DR: FIQ框架通过生成描述性问答对来增强视频问答模型的推理能力，在SUTD-TrafficQA数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法主要依赖事件中心标注，缺乏对场景基础信息（如物体类别、空间配置、视觉属性）的理解，限制了模型的泛化和推理能力

Method: 提出FIQ框架：1）从视频中提取描述性信息生成问答对，丰富数据集；2）设计VQ-CAlign模块对齐问题嵌入与视觉特征，保留上下文线索

Result: 在SUTD-TrafficQA数据集上实验表明，FIQ超越了现有基线方法，达到最先进性能

Conclusion: 通过增强对视频内容的基础理解，FIQ能有效提升VQA模型的泛化能力和推理性能

Abstract: Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.

</details>


### [2] [MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation](https://arxiv.org/abs/2511.17668)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: 提出了MedPEFT-CL框架，通过双阶段架构解决医学视觉语言分割模型在新任务学习中的灾难性遗忘问题，使用参数高效微调方法实现持续学习。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言分割模型在适应新解剖结构时存在灾难性遗忘问题，需要完全重新训练，限制了临床部署。针对医学视觉语言任务的持续学习方法研究不足。

Method: 基于CLIPSeg的双阶段架构：自适应学习阶段使用语义相似性适配器分配和参数高效微调；知识巩固阶段采用双向Fisher记忆协调。包含语义驱动适配器分配机制、双模态LoRA适应和双向Fisher记忆协调。

Result: 在多个医学数据集上的广泛实验表明，该框架在最小参数开销下实现了优越的遗忘缓解和性能保持。

Conclusion: MedPEFT-CL框架有效解决了医学视觉语言场景中的持续学习问题，具有实际部署价值。

Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.

</details>


### [3] [Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization](https://arxiv.org/abs/2511.17918)
*Youngsik Yun,Dongjun Gu,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出了频率自适应锐度正则化（FASR）方法，通过改进3D高斯泼溅（3DGS）的训练目标，解决其在少样本场景下对新视角泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在大多数配置下表现出色，但在少样本场景中由于对稀疏观测的过拟合，缺乏对新视角的泛化能力。从机器学习角度重新审视3DGS优化，将新视角合成视为对未见视角的泛化问题。

Method: 提出FASR方法，通过反映图像的局部频率来设置正则化权重和邻域半径，在估计局部锐度时防止过度正则化导致高频细节丢失，同时避免对锐度的惩罚不足。

Result: 在各种配置的数据集上，该方法持续改进了广泛的基线模型，防止了新视角中的漂浮伪影，并重建了SAM方法容易过度平滑的精细细节。

Conclusion: FASR方法通过频率自适应锐度正则化，有效提升了3DGS在新视角合成任务中的泛化性能，解决了现有方法在少样本场景下的局限性。

Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.

</details>


### [4] [Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2511.17973)
*Hiroto Honda*

Main category: cs.CV

TL;DR: APR方法通过对抗性攻击扰动新任务图像，在线合成伪重放图像，无需存储历史样本，解决了EFCIL中的可塑性-稳定性困境。


<details>
  <summary>Details</summary>
Motivation: EFCIL面临在不存储历史图像的情况下学习新类别同时保留旧知识的挑战，主要困难在于可塑性-稳定性困境和灾难性遗忘问题。

Method: 使用对抗性攻击在新任务图像上生成伪重放图像，以增强的旧类均值原型为目标，通过知识蒸馏防止语义漂移，并校准协方差矩阵补偿语义漂移。

Result: 在标准EFCIL基准测试的冷启动设置中实现了最先进的性能。

Conclusion: APR方法有效调和了稳定性和可塑性，无需存储重放样本即可实现优异的类增量学习性能。

Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.

</details>


### [5] [Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation](https://arxiv.org/abs/2511.18037)
*Yunfan Lu,Nico Messikommer,Xiaogang Xu,Liming Chen,Yuhan Chen,Nikola Zubic,Davide Scaramuzza,Hui Xiong*

Main category: cs.CV

TL;DR: 提出了首个统一的统计成像噪声模型，联合描述APS和EVS像素的噪声行为，并开发了校准流程和HESIM模拟器，在多个成像任务中验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 事件帧混合传感器集成了APS和EVS，但复杂的电路架构引入了难以理解的噪声模式，目前缺乏对这些噪声的统一建模。

Method: 开发了基于统计的统一成像噪声模型，包含光子散粒噪声、暗电流噪声、固定模式噪声和量化噪声，建立了校准流程来估计噪声参数，并提出了HESIM模拟器。

Result: 在两个混合传感器上的实验验证了模型在多个成像任务（如视频帧插值和去模糊）中的有效性，展示了从模拟到真实数据的强迁移能力。

Conclusion: 该工作提供了首个联合描述APS和EVS噪声行为的统一模型，通过校准和模拟验证了其在实际应用中的有效性。

Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.

</details>


### [6] [A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)](https://arxiv.org/abs/2511.18063)
*Gabriela Fernandes*

Main category: cs.CV

TL;DR: 开发基于深度学习的虚拟病理助手，使用EfficientNet-B3模型区分宫颈腺原位癌与正常宫颈腺组织，在CAISHI数据集上达到73.23%准确率。


<details>
  <summary>Details</summary>
Motivation: 宫颈腺原位癌是重要的癌前病变，准确诊断具有挑战性，早期检测对预防进展为浸润性腺癌至关重要。

Method: 使用2240张专家标注的H&E图像，经过Macenko染色归一化和基于补丁的预处理，采用EfficientNet-B3 CNN模型，使用类别平衡采样和焦点损失函数解决数据不平衡问题。

Result: 最终模型总体准确率为0.7323，异常类F1分数0.75，正常类F1分数0.71。Grad-CAM热图显示与AIS形态一致的生物学可解释激活模式。

Conclusion: 证明了轻量级、可解释AI系统在宫颈腺病理学中的可行性，在筛查工作流程、教育和资源匮乏环境中具有应用潜力。

Abstract: Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.

</details>


### [7] [Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective](https://arxiv.org/abs/2511.18089)
*Wenjing Liu,Qin Ren,Wen Zhang,Yuewei Lin,Chenyu You*

Main category: cs.CV

TL;DR: 提出了Together-Then-Apart (TTA)框架，通过统一的最小-最大优化同时建模共享和模态特定表示，在保持模态特定结构的同时实现语义对齐，显著提升了多模态生存分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度强调跨模态对齐，导致表示崩溃和多样性减少，而保持模态特定结构与实现语义一致性同等重要。

Method: TTA框架包含Together阶段（通过共享原型对齐嵌入，使用不平衡最优传输目标自适应突出信息标记）和Apart阶段（通过模态锚点和对比正则化器最大化表示多样性，防止特征崩溃）。

Result: 在五个TCGA基准测试上的广泛实验表明，TTA始终优于最先进的方法。

Conclusion: 该框架为如何在多模态生存分析中联合实现对齐和独特性提供了新的理论视角，实现了鲁棒、可解释且具有生物学意义的结果。

Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.

</details>


### [8] [Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation](https://arxiv.org/abs/2511.18272)
*Richard J. Young*

Main category: cs.CV

TL;DR: 本文首次系统评估了在医疗文档OCR中使用视觉token掩码作为隐私保护机制的效果，发现所有掩码策略最多只能减少42.9%的PHI，对长格式标识符有效但对短结构化标识符无效。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型在医疗环境中的OCR应用增加，保护健康信息(PHI)在文档处理过程中的暴露成为关键问题。

Method: 引入了七种掩码策略(V3-V9)，针对不同架构层，使用100份合成医疗账单文档评估PHI减少效果，并进行消融研究。

Result: 所有掩码策略都收敛到42.9%的PHI减少率，成功抑制长格式空间分布标识符(100%有效)，但无法阻止短结构化标识符(0%有效)。

Conclusion: 这一负面结果确立了仅靠视觉隐私干预的边界，为区分适合视觉级与语言级编辑的PHI类型提供指导，并引导未来研究转向解码器级微调和混合防御架构。

Abstract: Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.

</details>


### [9] [A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles](https://arxiv.org/abs/2511.18344)
*Tianyang Xu,Jinjie Gu,Xuefeng Zhu,XiaoJun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 提出了MM-UAV，首个用于多模态无人机跟踪的大规模基准数据集，包含RGB、红外和事件信号三种模态，覆盖30多个挑战性场景，包含1321个同步多模态序列和280万标注帧。同时提出了一个专门的多模态多无人机跟踪框架，包含偏移引导自适应对齐模块和自适应动态融合模块等创新技术。


<details>
  <summary>Details</summary>
Motivation: 随着低空无人机的普及，视觉多目标跟踪成为关键安全技术，但在复杂环境条件下单模态跟踪容易失败。虽然多模态跟踪更具鲁棒性，但缺乏专门的公共数据集阻碍了有效解决方案的开发。

Method: 提出了一个多模态多无人机跟踪框架，包含两个关键技术创新：偏移引导自适应对齐模块解决传感器间的空间不匹配问题，自适应动态融合模块平衡不同模态的互补信息。还引入了事件增强关联机制，利用事件模态的运动线索进行更可靠的身份维护。

Result: 综合实验表明，所提出的框架在性能上持续优于现有最先进方法。

Conclusion: MM-UAV数据集和提出的多模态跟踪框架为多模态无人机跟踪研究提供了重要基础，将促进该领域的进一步发展。数据集和源代码将公开提供。

Abstract: With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.

</details>


### [10] [Exploring Weak-to-Strong Generalization for CLIP-based Classification](https://arxiv.org/abs/2511.18396)
*Jinhao Li,Sarah M. Erfani,Lei Feng,James Bailey,Feng Liu*

Main category: cs.CV

TL;DR: 提出了类原型学习（CPL）方法，通过让较弱模型监督较强模型来提升CLIP模型的分类能力，在预训练有限的情况下获得3.67%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着模型复杂度增加，人工监督变得不切实际。当模型超越人类知识时，提供准确反馈变得困难且低效。需要探索使用较弱模型监督较强模型的方法。

Method: 提出类原型学习（CPL）方法，通过学习更具代表性的类别原型来增强CLIP模型的分类能力，在弱监督下使用简单的损失函数。

Result: 在目标场景下，特别是预训练有限的情况下，CPL方法表现稳健，相比强基线方法获得了3.67%的性能提升。

Conclusion: 弱到强泛化方法在视觉语言模型中同样有效，CPL方法在有限预训练条件下能够显著提升模型性能。

Abstract: Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.

</details>


### [11] [Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning](https://arxiv.org/abs/2511.18437)
*Chi Zhang,Haibo Qiu,Qiming Zhang,Yufei Xu,Zhixiong Zeng,Siqi Yang,Peng Shi,Lin Ma,Jing Zhang*

Main category: cs.CV

TL;DR: PEARL是一种针对视觉语言模型的双分支感知-推理协同强化学习方法，通过显式地将推理锚定到已验证的视觉证据来解决传统RLVR方法忽视视觉感知的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的RLVR方法仅验证最终文本输出，忽视了视觉感知这一基础步骤，导致视觉幻觉和奖励攻击问题。基于有缺陷感知的推理本质上不可靠。

Method: 提出PEARL框架：为每个推理问题生成感知检查清单（包含可验证答案的感知导向子问题），在训练中使用辅助rollout获得感知奖励，该奖励既直接强化模型感知能力，又作为推理的保真度门控。

Result: 在多种模态推理基准测试中取得显著提升，如在MathVerse上相比基线提升9.7%，相比GRPO提升6.6%。

Conclusion: PEARL通过感知-推理协同机制有效解决了视觉语言模型中的感知忽视问题，显著提升了多模态推理性能，并能与主流RL方法无缝集成。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.

</details>


### [12] [EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs](https://arxiv.org/abs/2511.18448)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventBench是一个针对事件视觉多模态大语言模型的综合基准测试，包含8个多样化任务指标和大规模事件流数据集，用于全面评估模型在事件理解、识别和空间推理等方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在事件视觉领域取得显著进展，但缺乏统一的基准测试来全面评估其能力。现有的事件基准在开放性、任务多样性、空间维度整合和数据规模方面存在不足。

Method: 提出EventBench基准，具有四个关键特点：1)开放性，发布所有原始事件流和任务指令；2)任务多样性，涵盖理解、识别和空间推理任务；3)空间维度整合，首创3D空间推理任务；4)数据规模大，包含超过100万事件-文本对训练集。

Result: 评估了GPT-5、Gemini-2.5 Pro、Qwen2.5-VL、InternVL3和EventGPT等模型。结果显示，当前基于事件的MLLM在事件流理解方面表现良好，但在细粒度识别和空间推理方面仍存在困难。

Conclusion: EventBench为事件视觉MLLM提供了全面的评估框架，揭示了当前模型在细粒度识别和空间推理方面的局限性，为未来研究指明了方向。

Abstract: Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.

</details>


### [13] [Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression](https://arxiv.org/abs/2511.18504)
*Md Tasnin Tanvir,Soumitra Das,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.CV

TL;DR: 提出了两种自适应压缩技术STTF和ANC，将算法创新与硬件感知优化相结合，使视觉语言模型能在资源受限的边缘设备上实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 边缘AI对视觉语言任务的需求需要模型在资源受限的设备上实现实时性能，这些设备具有有限的功耗和内存。

Method: STTF通过事件驱动的变化检测动态重用视觉token，ANC通过学习的路由器有条件地激活编码器分支，实现细粒度的场景复杂度适应。

Result: 3B参数的TinyGPT-STTF在COCO 2017测试集上达到CIDEr 131.2，比LLaVA-1.5 7B高出17.6 CIDEr点，同时使用2.3倍更少的参数和62倍更少的设备端FLOPs。STTF在事件视觉任务中减少84%的平均token数，ANC在低运动场景中减少高达90%的FLOPs。

Conclusion: 这些结果使得能够在真实世界的边缘设备上高效部署有能力的视觉语言模型。

Abstract: The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.

</details>


### [14] [Thinking Ahead: Foresight Intelligence in MLLMs and World Models](https://arxiv.org/abs/2511.18735)
*Zhantao Gong,Liaoyuan Fan,Qing Guo,Xun Xu,Xulei Yang,Shijie Li*

Main category: cs.CV

TL;DR: 提出了Foresight Intelligence概念和FSU-QA数据集，用于评估和增强视觉语言模型的前瞻推理能力，实验表明微调后的小模型能超越大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了前瞻智能这一关键能力，而这对自动驾驶等应用至关重要，需要填补这一研究空白。

Method: 创建FSU-QA视觉问答数据集，用于评估VLMs的前瞻推理能力，并通过世界模型生成的预测来增强模型性能。

Result: 当前最先进的VLMs在前瞻任务上表现不佳，但经过FSU-QA微调的小模型能大幅超越大型先进模型。

Conclusion: FSU-QA为开发真正能够预测和理解未来事件的下一代模型提供了原则性基础。

Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.

</details>


### [15] [Rethinking Garment Conditioning in Diffusion-based Virtual Try-On](https://arxiv.org/abs/2511.18775)
*Kihyun Na,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 提出Re-CatVTON，一种高效的基于单UNet的虚拟试穿模型，通过改进的条件学习策略和直接注入真实服装潜在特征，在减少计算和内存开销的同时实现高性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于双UNet架构的扩散模型在虚拟试穿任务中表现出色，但其计算和内存开销巨大。本研究旨在开发一种高效的单UNet模型，在保持高性能的同时显著降低资源需求。

Method: 通过可视化分析和理论分析提出三个关于上下文特征学习的假设，基于此开发Re-CatVTON模型。引入针对VTON空间连接条件的改进分类器自由引导策略，并直接注入从干净服装潜在特征派生的真实服装潜在特征以防止预测误差累积。

Result: Re-CatVTON相比前身CatVTON性能显著提升，计算和内存需求低于高性能双UNet模型Leffa。在FID、KID和LPIPS指标上表现更好，仅SSIM略有下降，建立了单UNet VTON模型的新效率-性能平衡。

Conclusion: Re-CatVTON通过改进的条件学习策略和误差预防机制，为虚拟试穿任务提供了一种计算效率高且性能优异的单UNet解决方案，在资源受限场景下具有重要应用价值。

Abstract: Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.

</details>


### [16] [Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing](https://arxiv.org/abs/2511.18792)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Chun Tung Chou,Wen Hu*

Main category: cs.CV

TL;DR: 该论文通过大规模MAE预训练方法，在14个Wi-Fi CSI数据集上验证了数据规模和多样性对跨域性能的关键作用，发现数据而非模型容量是Wi-Fi感知泛化的当前瓶颈。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知虽然提供了隐私保护的替代方案，但面临严重的跨域泛化问题，模型在不同环境、硬件或用户间表现不佳，现有数据集规模有限且碎片化。

Method: 采用基础模型方法，使用掩码自编码(MAE)风格在14个Wi-Fi CSI数据集上进行预训练，包含130万样本，覆盖4种设备、2.4/5/6 GHz频段和20-160 MHz带宽。

Result: 实验显示：1) 预训练数据量增加带来对数线性改进；2) 当前数据量下更大模型仅提供边际增益；3) 大规模预训练在人类活动识别等任务上提升跨域准确率2.2%-15.7%。

Conclusion: 数据规模和多样性是Wi-Fi感知领域泛化的关键，为设计实际部署的鲁棒Wi-Fi感知系统提供了重要方向。

Abstract: While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.

</details>


### [17] [VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models](https://arxiv.org/abs/2511.18823)
*Fufangchen Zhao,Liao Zhang,Daiqi Shi,Yuanjun Gao,Chen Ye,Yang Cai,Jian Gao,Danfeng Yan*

Main category: cs.CV

TL;DR: VideoPerceiver是一个新颖的视频多模态大语言模型，通过两阶段训练框架提升视频理解的细粒度感知能力，特别针对短片段中的瞬时动作和长视频中的罕见事件。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频多模态大语言模型在推理短视频中的瞬时动作和长视频中的罕见瞬态事件方面的能力限制。

Method: 采用两阶段训练：监督微调阶段构建'关键信息缺失'视频，通过替换关键帧并联合编码原始和修改视频token，使用辅助对比损失对齐中间视觉表示；强化学习阶段使用两种视频变体生成描述，通过相对奖励确保完整视频的响应优于降级输入。

Result: 在细粒度动作理解和罕见事件描述基准测试中显著优于最先进的VMLLMs，同时在标准任务上保持强大性能。

Conclusion: 通过优先处理任务相关的视觉特征，重新定义了视频语言模型的细粒度感知训练方法。

Abstract: We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct "key-information-missing" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.

</details>


### [18] [CLASH: A Benchmark for Cross-Modal Contradiction Detection](https://arxiv.org/abs/2511.19199)
*Teodora Popordanoska,Jiameng Li,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: CLASH是一个新的多模态矛盾检测基准，包含COCO图像与包含对象级或属性级矛盾的矛盾字幕配对，评估模型识别跨模态冲突的能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中存在大量矛盾的多模态输入，但现有基准通常假设输入一致性，无法评估跨模态矛盾检测这一防止幻觉和确保可靠性的基本能力。

Method: 构建CLASH基准，包含COCO图像与矛盾字幕配对，通过自动质量检查筛选的广泛微调集和较小的人工验证诊断集，在多项选择和开放式格式下评估模型。

Result: 对最先进模型的分析显示它们在识别跨模态冲突方面存在显著限制，暴露出系统性的模态偏见和类别特定弱点。针对CLASH的微调显著增强了冲突检测能力。

Conclusion: CLASH基准揭示了当前多模态模型在矛盾检测方面的不足，并证明针对性微调能有效提升冲突识别能力，为构建更可靠的多模态系统提供了重要工具。

Abstract: Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.

</details>


### [19] [EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models](https://arxiv.org/abs/2511.18920)
*Wenhao Xu,Xin Dong,Yue Li,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 提出EventSTU框架，利用事件相机原理实现高效视频理解，通过时间域关键帧采样和空间域token剪枝，在保持性能的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理长视频时推理成本过高，受事件相机启发，希望利用事件触发特性来消除冗余信息

Method: 1. 时间域：粗到细关键帧采样算法，利用事件相机变化触发特性消除冗余帧
2. 空间域：自适应token剪枝算法，利用事件视觉显著性作为零成本先验
3. 时空整合：结合问题相关性自适应分配token剪枝预算

Result: 相比最强基线，实现3.01倍FLOPs减少和3.10倍预填充加速，同时性能仍有提升

Conclusion: EventSTU框架成功实现了高效时空理解，既适用于物理事件相机也支持通用视频理解，为视频理解系统提供了有效的效率优化方案

Abstract: Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.

</details>


### [20] [CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery](https://arxiv.org/abs/2511.18968)
*Bhuvan Sachdeva,Sneha Kumari,Rudransh Agarwal,Shalaka Kumaraswamy,Niharika Singri Prasad,Simon Mueller,Raphael Lechtenboehmer,Maximilian W. M. Wintergerst,Thomas Schultz,Kaushik Murali,Mohit Jain*

Main category: cs.CV

TL;DR: 提出了CataractCompDetect框架，结合相位感知定位、SAM 2跟踪、并发症特定风险评分和视觉语言推理，用于白内障手术并发症的自动检测。


<details>
  <summary>Details</summary>
Motivation: 白内障手术是全球最常见的手术之一，但术中并发症如虹膜脱垂、后囊破裂和玻璃体丢失仍是导致不良预后的主要原因。自动检测这些事件可实现早期预警系统和客观培训反馈。

Method: 结合相位感知定位、SAM 2跟踪、并发症特定风险评分和视觉语言推理进行最终分类。使用首个白内障手术视频数据集CataComp进行验证。

Result: 在CataComp数据集上平均F1得分为70.63%，各并发症检测性能分别为：虹膜脱垂81.8%、后囊破裂60.87%、玻璃体丢失69.23%。

Conclusion: 结果表明，将结构化手术先验知识与视觉语言推理相结合，对于识别罕见但影响重大的术中事件具有重要价值。

Abstract: Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.

</details>


### [21] [NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting](https://arxiv.org/abs/2511.19202)
*Brent Zoomers,Florian Hahlbohm,Joni Vanherck,Lode Jorissen,Marcus Magnor,Nick Michiels*

Main category: cs.CV

TL;DR: 提出一种使用小型共享MLP学习3D高斯模型中高斯可见性函数的方法，通过遮挡剔除来加速渲染，在VRAM使用和图像质量方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然能利用视锥体剔除和细节层次策略加速渲染，但由于高斯的半透明特性无法应用遮挡剔除技术，这限制了渲染效率的进一步提升。

Method: 使用小型共享MLP学习训练模型中所有高斯的视点依赖可见性函数，在光栅化前查询视锥体内高斯的可见性，丢弃被遮挡的基元，并集成到新的实例化软件光栅器中。

Result: 在组合场景中，该方法在VRAM使用和图像质量方面优于当前最先进技术，与现有LoD技术具有互补特性。

Conclusion: 提出的神经查询遮挡剔除方法有效解决了3D高斯渲染中遮挡剔除的局限性，显著提升了渲染效率。

Abstract: 3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.17582)
*Jie Ou,Shuaihong Jiang,Yingjun Du,Cees G. M. Snoek*

Main category: cs.LG

TL;DR: GateRA是一种参数高效微调框架，通过token感知的动态门控机制，在推理过程中自适应调整PEFT更新的强度，实现选择性、token级别的适配。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法对所有token应用静态、输入无关的更新，忽视了不同输入的重要性和难度差异，导致在简单内容上过拟合或在重要区域适配不足。

Method: 在标准PEFT分支中引入自适应门控，实现token级别的选择性适配；使用基于熵的正则化鼓励接近二元的门控决策；理论分析显示GateRA在PEFT路径上产生软梯度掩码效应。

Result: 在多个常识推理基准测试中，GateRA始终优于或匹配先前的PEFT方法；可视化显示GateRA能自动抑制冗余预填充token的更新，同时在解码阶段强调适配。

Conclusion: GateRA通过动态门控机制实现了更智能的参数高效微调，在保持预训练知识的同时，将容量集中在具有挑战性的案例上。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.

</details>


### [23] [Learning Straight Flows: Variational Flow Matching for Efficient Generation](https://arxiv.org/abs/2511.17583)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.LG

TL;DR: 提出S-VFM方法，通过引入变分潜码来强制轨迹直线化，解决Flow Matching中一步生成的局限性问题


<details>
  <summary>Details</summary>
Motivation: Flow Matching依赖学习的弯曲轨迹，难以实现一步生成。现有方法存在离散近似误差、训练不稳定和收敛困难等问题

Method: 将变分潜码（表示"生成概览"）集成到Flow Matching框架中，显式强制轨迹直线化，产生线性生成路径

Result: 在三个挑战性基准测试中取得竞争性性能，在训练和推理效率方面优于现有方法

Conclusion: S-VFM通过引入变分潜码有效解决了Flow Matching的轨迹弯曲问题，实现了更高效的一步生成

Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.

</details>


### [24] [Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production](https://arxiv.org/abs/2511.17632)
*Bestoun S. Ahmed,Tommaso Azzalin,Andreas Kassler,Andreas Thore,Hans Lindback*

Main category: cs.LG

TL;DR: 提出基于数字孪生的智能制造方法，通过微服务边缘计算平台和深度强化学习优化钢铁生产厂的可持续性、效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 将传统制造流程转变为智能系统，实现可持续性目标，强调MLOps在数据驱动制造中的关键作用。

Method: 采用微服务边缘计算平台，通过数字孪生实时处理传感器数据，使用深度强化学习代理在MLOps系统中自主优化功率设置。

Result: 系统能够优化感应炉加热、提高操作质量、减少工艺浪费，实现制造废料减少和生产质量提升。

Conclusion: 该研究为传统流程向智能系统转型提供了关键步骤，展示了可扩展的事件驱动架构可适应各种工业应用。

Abstract: We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.

</details>


### [25] [Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction](https://arxiv.org/abs/2511.17879)
*Yusong Wu,Stephen Brade,Teng Ma,Tia-Jane Fowler,Enning Yang,Berker Banar,Aaron Courville,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.LG

TL;DR: 提出一种对抗训练方法来缓解RL后训练中的奖励破解问题，用于旋律到和弦伴奏生成，在保持输出多样性的同时提高谐波一致性和适应性。


<details>
  <summary>Details</summary>
Motivation: 实时即兴演奏需要实时协调和适应，而传统RL后训练会因奖励破解导致输出多样性降低，这在音乐创作中特别有害。

Method: 使用对抗训练方法，通过共同演化的判别器区分策略轨迹和数据分布，策略同时最大化判别器输出和一致性奖励以防止输出崩溃。

Result: 在模拟和真实用户研究中，模型在输出多样性、谐波一致性、适应速度和用户控制方面均有改善。

Conclusion: 该方法简单有效地缓解了生成序列模型RL后训练中的奖励破解问题。

Abstract: Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.

</details>


### [26] [Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing](https://arxiv.org/abs/2511.17902)
*Yifan He,Haodong Zhang,Qiuheng Song,Lin Lei,Zhenxuan Zeng,Haoyang He,Hongyan Wu*

Main category: cs.LG

TL;DR: 提出DUPLE元学习框架，解决分布式光纤传感在跨部署场景下的活动识别问题，通过双域多原型学习、统计引导网络和查询感知原型聚合来应对信号域偏移、标注数据稀缺等挑战。


<details>
  <summary>Details</summary>
Motivation: 分布式光纤传感在实际应用中面临三大挑战：不同光纤部署类型导致信号模式差异（域偏移）、新部署场景标注数据稀缺、源域内数据不足难以捕获类内多样性。

Method: DUPLE框架包含三个核心组件：双域多原型学习器融合时频域特征增强泛化能力；统计引导网络从原始统计特征推断域重要性和原型敏感性；查询感知原型聚合模块自适应选择和组合相关原型。

Result: 在跨部署DFOS数据集上的实验表明，该方法在域泛化设置下显著优于基线方法，能够在有限标注数据下实现跨不同光纤配置的鲁棒事件识别。

Conclusion: DUPLE框架有效解决了DFOS系统中的域偏移和数据稀缺问题，为跨部署场景下的活动识别提供了实用的解决方案。

Abstract: Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.
  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.
  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.

</details>


### [27] [AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert](https://arxiv.org/abs/2511.18314)
*Yuting Gao,Wang Lan,Hengyuan Zhao,Linjiang Huang,Si Liu,Qingpei Guo*

Main category: cs.LG

TL;DR: AnyExperts提出了一种按需、预算感知的动态路由框架，通过基于语义重要性为每个token分配可变数量的专家槽位来优化多模态MoE模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态MoE模型采用固定的专家激活策略，忽略了不同模态间语义重要性的异质性，导致计算资源分配不优，冗余token与关键token消耗相同资源。

Method: 提出AnyExperts框架：1）为每个token分配可变总数量的专家槽位；2）总槽位数限制在固定范围内；3）每个槽位由真实专家或虚拟专家填充，虚拟专家比例上限为20%；4）根据语义重要性自适应平衡真实与虚拟专家比例。

Result: 在视觉理解、音频理解和NLP理解等任务中，AnyExperts在相同计算预算下提升性能：在通用图像/视频任务中，使用40%更少的真实专家激活达到可比精度；在文本密集任务中，减少10%真实专家使用同时保持性能。

Conclusion: 细粒度的、重要性驱动的专家分配显著提升了多模态MoE模型的效率和效果。

Abstract: Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.

</details>


### [28] [DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations](https://arxiv.org/abs/2511.18331)
*Sohini Roychowdhury,Adam Holeman,Mohammad Amin,Feng Wei,Bhaskar Mehta,Srihari Reddy*

Main category: cs.LG

TL;DR: Dynamix是一个可扩展的个性化序列探索框架，通过最大相关性原则和基于事件特征的自监督学习来优化事件历史处理，在不牺牲广告预测准确性的情况下提高训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在线广告推荐系统中处理完整的用户-广告互动历史计算成本高且容易受到噪声影响，需要一种更高效的序列处理方法。

Method: 使用最大相关性原则和基于事件特征的自监督学习，在会话和表面级别对用户互动进行分类，通过动态特征移除和选择性特征增强来优化处理。

Result: 动态资源移除使训练和推理吞吐量分别提高1.15%和1.8%，动态特征增强提供0.033 NE增益，同时推理QPS提高4.2%。

Conclusion: Dynamix在基于在线用户序列的推荐模型中实现了显著的成本效率和性能改进，自监督用户分割和资源探索可以进一步优化复杂特征选择策略。

Abstract: For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.

</details>


### [29] [Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking](https://arxiv.org/abs/2511.18394)
*Chinmay Karkar,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs在预测社会、政治和经济事件方面表现出部分能力，但其预测性能在不同领域和提示框架下差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在真实世界事件预测中的表现，特别是针对模型截止日期后发生的事件，探索不同因素如何影响预测准确性和校准。

Method: 分析不同模型家族在真实世界问题上的预测表现，研究上下文、问题类型和外部知识对准确性和校准的影响，以及添加事实新闻背景如何改变信念形成和失败模式。

Result: LLMs的预测能力高度可变，取决于提问的内容和方式。

Conclusion: LLMs的预测能力不是普遍一致的，而是高度依赖于具体领域、问题框架和上下文条件。

Abstract: Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.

</details>


### [30] [LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs](https://arxiv.org/abs/2511.18727)
*Devansh Agarwal,Maitreyi Chatterjee,Biplab Chatterjee*

Main category: cs.LG

TL;DR: LogSyn框架使用大语言模型将非结构化的飞机维修日志转换为结构化数据，通过少量样本学习实现问题-解决方案摘要生成和事件分类，为航空维修提供可扩展的语义分析和洞察提取方法。


<details>
  <summary>Details</summary>
Motivation: 飞机维修日志包含宝贵的安全数据，但由于其非结构化文本格式而未被充分利用，需要一种方法来提取这些数据中的可操作洞察。

Method: 使用大语言模型和少量样本上下文学习，在6,169条记录上执行受控抽象生成，总结问题-解决方案叙述并在详细层次本体中分类事件。

Result: 框架能够识别关键故障模式，为维修日志提供可扩展的语义结构化和洞察提取方法。

Conclusion: 这项工作为改进航空及相关行业的维修工作流程和预测分析提供了实用路径。

Abstract: Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.

</details>


### [31] [Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM](https://arxiv.org/abs/2511.18830)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种双输入神经网络策略，通过分离事件和序列属性，使用持续时间感知的伪嵌入矩阵将时间重要性转换为紧凑的可学习表示，以解决预测过程监控中时间不规则性问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在预测过程监控中难以处理时间不规则性，特别是随机事件持续时间和重叠时间戳，限制了它们在不同数据集上的适应性。

Method: 采用双输入神经网络策略，分离事件和序列属性，使用持续时间感知伪嵌入矩阵。在B-LSTM和B-GCN两个基线家族及其持续时间感知变体D-LSTM和D-GCN上实现，所有模型都包含自调谐超模型用于自适应架构选择。

Result: 在平衡和不平衡结果预测任务上的实验表明，持续时间伪嵌入输入持续改善泛化能力，降低模型复杂性，并增强可解释性。

Conclusion: 结果证明了显式时间编码的优势，并为健壮的实际预测过程监控应用提供了灵活的设计方案。

Abstract: Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.

</details>


### [32] [Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2511.18835)
*Fang Wang,Lance Kosca,Adrienne Kosca,Marko Gacesa,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGNN(O)是一个用于事件序列数据结果预测的AutoML GNN超模型框架，通过贝叶斯优化自动调优架构和超参数，在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 为复杂事件序列数据的结果预测提供一个鲁棒且可泛化的AutoML-GNN基准方法，无需手动配置即可适应不同架构和超参数。

Method: 扩展了四种架构（单层、双层、双层伪嵌入、双层嵌入）和六种GNN算子，采用基于贝叶斯优化的自调优机制，包含剪枝和早停策略。

Result: 在Traffic Fines数据集上准确率超过0.98，在Patients数据集上加权F1分数达到0.86，无需显式处理数据不平衡问题。

Conclusion: 提出的AutoML-GNN方法为复杂事件序列数据的结果预测提供了鲁棒且可泛化的基准解决方案。

Abstract: This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.

</details>


### [33] [Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.18871)
*Jian Lu*

Main category: cs.LG

TL;DR: 该论文提出了一种将推理和训练分离部署的周期性异步框架，通过改进数据加载器和引入统一三模型架构，在保持算法精度不变的同时显著提升了强化学习训练效率。


<details>
  <summary>Details</summary>
Motivation: 主流RL框架中推理和训练在同一设备上同步执行，这种计算耦合阻碍了并发推理和训练，导致训练效率低下。

Method: 采用推理和训练分离部署策略，改进数据加载器，构建周期性异步框架，使用统一三模型架构和共享提示注意力掩码减少重复计算。

Result: 在NPU平台上实现了至少三倍的整体性能提升，算法精度与同步方法完全等效。

Conclusion: 该周期性异步框架具有需求驱动、独立和弹性扩展的能力，显示出广泛应用的潜力。

Abstract: Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.

</details>


### [34] [Optimization of Deep Learning Models for Dynamic Market Behavior Prediction](https://arxiv.org/abs/2511.19090)
*Shenghan Zhao,Yuzhen Lin,Ximeng Yang,Qiaochu Lu,Haozhong Xue,Gaozhe Jiang*

Main category: cs.LG

TL;DR: 该论文提出了一种混合序列模型用于多时间范围需求预测，在电子商务交易数据上取得了比传统方法和先进Transformer模型更好的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着金融科技的发展，深度学习模型在预测消费者行为方面显示出巨大潜力，能够改善贷款策略和市场效率。本文专注于零售市场行为，明确预测目标为SKU级别的日需求或收入。

Method: 提出了一种混合序列模型，结合多尺度时间卷积、门控循环模块和时间感知自注意力机制。使用标准回归损失进行训练，并采用严格的时间分割来防止数据泄露。

Result: 与ARIMA/Prophet、LSTM/GRU、LightGBM以及先进的Transformer预测模型（TFT、Informer、Autoformer、N-BEATS）相比，该模型在MAE、RMSE、sMAPE、MASE和Theil's U_2等指标上表现出持续准确度提升，在高峰/节假日期间具有更好的鲁棒性。

Conclusion: 通过消融实验和统计显著性测试验证了改进的可靠性，并提供了实现细节以确保可复现性。该混合模型在多时间范围需求预测任务中表现优异。

Abstract: The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.

</details>


### [35] [Understanding the Staged Dynamics of Transformers in Learning Latent Structure](https://arxiv.org/abs/2511.19328)
*Rohan Saha,Farzane Aminmansour,Alona Fyshe*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型学习潜在结构的动态过程，发现在Alchemy基准测试中，模型通过离散阶段逐步学习，先掌握粗粒度规则，再学习完整的潜在结构，并揭示了组合与分解能力的不对称性。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer能够从上下文中发现潜在结构，但其获取不同潜在结构组件的动态过程仍不清楚。本文旨在深入理解Transformer学习潜在结构的机制和过程。

Method: 使用Alchemy基准测试，训练小型仅解码器Transformer处理三个任务变体：从部分上下文推断缺失规则、组合简单规则解决多步序列、分解复杂多步示例推断中间步骤。通过将任务分解为可解释事件来分析学习动态。

Result: 模型以离散阶段获取能力，先学习粗粒度规则，后学习完整潜在结构。发现关键不对称性：模型能稳健组合基本规则，但难以分解复杂示例来发现基本规则。

Conclusion: 这些发现为理解Transformer模型如何学习潜在结构提供了新见解，展示了训练过程中这些能力如何逐步演化的细粒度视图。

Abstract: While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [36] [BPMN to PDDL: Translating Business Workflows for AI Planning](https://arxiv.org/abs/2511.18171)
*Jasper Nie,Christian Muise,Victoria Armstrong*

Main category: cs.AI

TL;DR: 开发了一个将BPMN 2.0图转换为PDDL表示的功能性管道，用于业务流程的自动规划和仿真。


<details>
  <summary>Details</summary>
Motivation: 虽然自动规划已被提出作为模拟和推理BPMN工作流的方法，但大多数实现仍不完整或范围有限。该项目旨在弥合理论与实践工具之间的差距。

Method: 构建了一个功能性管道，将BPMN 2.0图转换为适合规划的PDDL表示，支持核心BPMN构造（任务、事件、序列流、网关），并初步支持并行和包含网关行为。

Result: 使用非确定性规划器演示了如何生成和评估有效执行轨迹。

Conclusion: 该实现为将业务流程转换为明确定义的规划提供了基础，为进一步探索奠定了基础。

Abstract: Business Process Model and Notation (BPMN) is a widely used standard for modelling business processes. While automated planning has been proposed as a method for simulating and reasoning about BPMN workflows, most implementations remain incomplete or limited in scope. This project builds upon prior theoretical work to develop a functional pipeline that translates BPMN 2.0 diagrams into PDDL representations suitable for planning. The system supports core BPMN constructs, including tasks, events, sequence flows, and gateways, with initial support for parallel and inclusive gateway behaviour. Using a non-deterministic planner, we demonstrate how to generate and evaluate valid execution traces. Our implementation aims to bridge the gap between theory and practical tooling, providing a foundation for further exploration of translating business processes into well-defined plans.

</details>


### [37] [Natural Emergent Misalignment from Reward Hacking in Production RL](https://arxiv.org/abs/2511.18397)
*Monte MacDiarmid,Benjamin Wright,Jonathan Uesato,Joe Benton,Jon Kutasov,Sara Price,Naia Bouscal,Sam Bowman,Trenton Bricken,Alex Cloud,Carson Denison,Johannes Gasteiger,Ryan Greenblatt,Jan Leike,Jack Lindsey,Vlad Mikulik,Ethan Perez,Alex Rodrigues,Drake Thomas,Albert Webson,Daniel Ziegler,Evan Hubinger*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that when large language models learn to reward hack on production RL environments, this can result in egregious emergent misalignment. We start with a pretrained model, impart knowledge of reward hacking strategies via synthetic document finetuning or prompting, and train on a selection of real Anthropic production coding environments. Unsurprisingly, the model learns to reward hack. Surprisingly, the model generalizes to alignment faking, cooperation with malicious actors, reasoning about malicious goals, and attempting sabotage when used with Claude Code, including in the codebase for this paper. Applying RLHF safety training using standard chat-like prompts results in aligned behavior on chat-like evaluations, but misalignment persists on agentic tasks. Three mitigations are effective: (i) preventing the model from reward hacking; (ii) increasing the diversity of RLHF safety training; and (iii) "inoculation prompting", wherein framing reward hacking as acceptable behavior during training removes misaligned generalization even when reward hacking is learned.

</details>


### [38] [A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection](https://arxiv.org/abs/2511.18739)
*Kaixiang Yang,Jiarong Liu,Yupeng Song,Shuanghua Yang,Yujue Zhou*

Main category: cs.AI

TL;DR: 提出了一个面向问题的时间序列异常检测评估框架，将20多个常用指标重新解释为6个维度，通过实验量化指标区分真实检测与随机噪声的能力，发现指标适用性必须与任务目标对齐。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测评估面临挑战，因为应用目标多样且指标假设异构。现有研究缺乏统一的框架来理解不同指标针对的具体评估挑战。

Method: 将常用指标分类为6个维度：基础精度、时效性、标签容错、人工成本、鲁棒性和跨数据集可比性。通过真实、随机和oracle检测场景实验，分析指标得分分布和区分能力。

Result: 大多数事件级指标具有强区分能力，但NAB、Point-Adjust等常用指标对随机得分膨胀的抵抗能力有限。指标适用性本质上是任务依赖的。

Conclusion: 该框架为理解现有指标提供了统一分析视角，并为选择或开发更上下文感知、鲁棒和公平的评估方法提供了实用指导。

Abstract: Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection.

</details>


### [39] [HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs](https://arxiv.org/abs/2511.18760)
*Azim Ospanov,Zijin Feng,Jiacheng Sun,Haoli Bai,Xin Shen,Farzan Farnia*

Main category: cs.AI

TL;DR: Hermes是一个结合非正式推理和形式化验证的数学推理代理框架，在Lean中交替使用非正式推理和形式化证明步骤，通过中间验证防止推理漂移，显著提升推理准确性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 非正式数学推理灵活但容易出错，形式化定理证明严谨但缺乏探索性。当前基于LLM的数学代理缺乏将两种范式优势结合的原则性方法。

Method: 开发Hermes框架，在Lean中交替进行非正式推理和形式化验证步骤，使用中间检查防止推理漂移，并采用记忆模块维护多步推理的连续性。

Result: 在四个数学推理基准测试中，Hermes可靠地提升了基础模型的推理准确性，同时大幅减少token使用和计算成本。在AIME'25等困难数据集上，准确率提升达67%，总推理FLOPs减少80%。

Conclusion: Hermes成功地将非正式推理的灵活性与形式化验证的严谨性相结合，为数学推理提供了既具探索性又可验证的统一工作流。

Abstract: Informal mathematics has been central to modern large language model (LLM) reasoning, offering flexibility and enabling efficient construction of arguments. However, purely informal reasoning is prone to logical gaps and subtle errors that are difficult to detect and correct. In contrast, formal theorem proving provides rigorous, verifiable mathematical reasoning, where each inference step is checked by a trusted compiler in systems such as Lean, but lacks the exploratory freedom of informal problem solving. This mismatch leaves current LLM-based math agents without a principled way to combine the strengths of both paradigms. In this work, we introduce Hermes, the first tool-assisted agent that explicitly interleaves informal reasoning with formally verified proof steps in Lean. The framework performs intermediate formal checking to prevent reasoning drift and employs a memory module that maintains proof continuity across long, multi-step reasoning chains, enabling both exploration and verification within a single workflow. We evaluate Hermes on four challenging mathematical reasoning benchmarks using LLMs of varying parameter scales, from small models to state-of-the-art systems. Across all settings, Hermes reliably improves the reasoning accuracy of base models while substantially reducing token usage and computational cost compared to reward-based approaches. On difficult datasets such as AIME'25, Hermes achieves up to a 67% accuracy improvement while using 80% fewer total inference FLOPs. The implementation and codebase are publicly available at https://github.com/aziksh-ospanov/HERMES.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [40] [Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs](https://arxiv.org/abs/2511.17572)
*Patrick Gerard,Aiden Chang,Svitlana Volkova*

Main category: cs.CL

TL;DR: 本文提出了一个测试认知立场转移的框架，通过删除事件知识来验证对齐LLMs是否能在无知状态下仍保持社区特定的行为模式，发现即使移除事实信息，LLMs仍能维持稳定的社区特定不确定性处理方式。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在对齐到特定在线社区时，是表现出可泛化的行为模式反映社区态度，还是仅仅回忆训练数据中的模式。

Method: 引入认知立场转移测试框架：针对性删除事件知识并用多种探针验证，然后评估模型在无知状态下是否仍能重现社区的有机响应模式。使用俄乌军事讨论和美国党派Twitter数据。

Result: 即使经过激进的事实移除，对齐的LLMs仍保持稳定的社区特定行为模式来处理不确定性。

Conclusion: 对齐编码了结构化、可泛化的行为，超越了表面模仿。该框架为检测在无知状态下持续存在的行为偏见提供了系统方法，有助于更安全透明的LLM部署。

Abstract: When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.

</details>


### [41] [Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation](https://arxiv.org/abs/2511.17813)
*Scott Merrill,Shashank Srivastava*

Main category: cs.CL

TL;DR: 提出了一个从Zoom会议录音生成带说话者属性转录的流程，创建了三个地方政府审议数据集，通过微调LLM显著提升了模拟审议的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别生成的匿名说话者标签限制了语言模型模拟真实多方审议的能力，缺乏说话者属性数据是主要瓶颈。

Method: 开发了可复现的流程将公开Zoom录音转换为带说话者属性的转录，包含人物画像和语用行为标签；基于此数据微调LLM来建模特定参与者。

Result: 使用"行为感知"数据微调的LLM使困惑度降低67%，说话者保真度和真实性的分类器性能指标几乎翻倍；图灵式人工评估显示模拟审议与真实审议难以区分。

Conclusion: 该方法为复杂现实公民模拟提供了实用且可扩展的解决方案，能够生成高度真实的审议模拟。

Abstract: Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.

</details>


### [42] [Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search](https://arxiv.org/abs/2511.18313)
*Joseph Oladokun*

Main category: cs.CL

TL;DR: 提出了路径约束检索（PCR）方法，通过结合图结构约束和语义搜索，确保从知识图谱中检索的信息保持逻辑关系，提高LLM代理推理的连贯性。


<details>
  <summary>Details</summary>
Motivation: LLM代理从知识库中检索上下文时，由于缺乏与当前推理状态的结构一致性，导致推理链不连贯。

Method: PCR方法将结构图约束与语义搜索相结合，将搜索空间限制为从锚节点可达的节点，防止检索结构断开的信息。

Result: 在PathRAG-6基准测试中，PCR实现100%结构一致性（基线方法为24-32%），同时保持强相关性得分。在技术领域，PCR在排名10时获得完全相关性和完全结构一致性，显著优于向量搜索和混合检索。PCR将检索上下文的平均图距离减少了78%。

Conclusion: 路径约束检索是提高LLM代理推理系统可靠性和连贯性的有效方法。

Abstract: Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.

</details>


### [43] [RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context](https://arxiv.org/abs/2511.18743)
*Yu Lei,Shuzheng Si,Wei Wang,Yifei Wu,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: RhinoInsight是一个深度研究框架，通过可验证清单和证据审计两个控制机制增强LLM代理的稳健性和可追溯性，无需参数更新即可提升研究质量。


<details>
  <summary>Details</summary>
Motivation: 现有系统采用线性规划-搜索-写作-报告流程，存在错误积累和上下文腐化问题，缺乏对模型行为和上下文的显式控制。

Method: 1) 可验证清单模块将用户需求转化为可追踪子目标，通过批评者精炼并生成分层大纲；2) 证据审计模块结构化搜索内容，迭代更新大纲并修剪噪声上下文，通过批评者绑定高质量证据。

Result: 实验表明RhinoInsight在深度研究任务上达到最先进性能，在深度搜索任务上保持竞争力。

Conclusion: RhinoInsight通过添加控制机制有效解决了现有系统的局限性，提升了LLM代理在深度研究中的稳健性和质量。

Abstract: Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.

</details>


### [44] [Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials](https://arxiv.org/abs/2511.18937)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: 提出了一种基于图形化知识的方法来审查临床试验中的治疗相关不良事件，通过为MedDRA添加隐藏的医学知识层来改进AE分析。


<details>
  <summary>Details</summary>
Motivation: 改进临床试验中不良事件的审查过程，增强MedDRA术语系统的语义理解能力，提高AE解释的清晰度和效率。

Method: 构建Safeterm医学知识层，在2D地图中捕获术语间的语义关系；使用收缩发生率比计算治疗特异性不成比例指标；通过精度加权聚合推导聚类级EBGM值；提供语义地图和期望度-不成比例度图两种可视化输出。

Result: 在三个历史试验中，自动化方法清晰恢复了所有预期的安全信号；将MedDRA与医学知识层结合提高了AE解释的清晰度、效率和准确性。

Conclusion: 通过为MedDRA添加医学知识层，显著改善了临床试验中不良事件解释的清晰度、效率和准确性。

Abstract: We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [45] [A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies](https://arxiv.org/abs/2511.18153)
*Shreyas Kumar,Barat S,Debojit Das,Yug Desai,Siddhi Jain,Rajesh Kumar,Harish J. Palanthandalam-Madapusi*

Main category: cs.RO

TL;DR: SnapNet神经网络通过关节速度瞬变实时检测卡扣装配啮合，结合动态系统双臂协调框架实现精确对准和柔顺插入，在异质双手机器人平台上验证了高检测精度和冲击力降低。


<details>
  <summary>Details</summary>
Motivation: 精密卡扣装配（如镜片插入眼镜框或电子元件组装）需要及时检测啮合并快速衰减力，以防止过冲导致的组件损坏或装配失败。

Method: 提出SnapNet轻量神经网络从关节速度瞬变实时检测啮合，无需外部传感器；开发基于动态系统的双臂协调框架，集成SnapNet检测与事件触发阻抗调制。

Result: 在异质双手机器人平台上实验显示，检测准确率高（召回率超过96%），与标准阻抗控制相比峰值冲击力降低达30%。

Conclusion: 该方法仅使用本体感觉信号即可实现可靠的卡扣装配啮合检测，结合事件触发阻抗调制能有效减少冲击力，提高精密装配的成功率。

Abstract: Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.

</details>


### [46] [Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters](https://arxiv.org/abs/2511.18243)
*Eashan Vytla,Bhavanishankar Kalavakolanu,Andrew Perrault,Matthew McCrink*

Main category: cs.RO

TL;DR: 论文探索了基于物理信息的世界模型学习方法，用于提高无人机控制策略性能，但发现该方法与标准RNN模型都存在泛化问题，导致策略无法收敛。


<details>
  <summary>Details</summary>
Motivation: 当前无人机控制算法在动态环境和恶劣条件下缺乏鲁棒性，基于模型的强化学习虽然样本效率高，但Dreamer方法在无人机系统上应用困难，主要是样本效率低和动力学模型泛化能力差。

Method: 采用物理信息方法构建世界模型，将无人机视为自由体系统，预测作用在其上的净力和力矩，然后通过6自由度龙格-库塔积分器预测未来状态。与标准RNN世界模型进行对比。

Result: 两种模型在训练数据上都表现良好，但都无法泛化到新轨迹，导致状态预测快速发散，阻碍了策略收敛。

Conclusion: 物理信息的世界模型方法虽然理论上具有优势，但在实际应用中仍面临泛化挑战，需要进一步改进以解决状态预测发散问题。

Abstract: Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.

</details>


### [47] [MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent](https://arxiv.org/abs/2511.18810)
*Yuxia Fu,Zhizhen Zhang,Yuqi Zhang,Zijian Wang,Zi Huang,Yadan Luo*

Main category: cs.RO

TL;DR: MergeVLA是一个面向合并的视觉-语言-动作模型架构，通过稀疏激活的LoRA适配器和任务掩码解决多技能VLA模型合并问题，在未知任务时使用任务路由器进行自适应选择，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在单一任务上表现良好，但扩展到多技能设置时面临挑战：直接合并不同任务的专家模型会导致成功率接近零。需要解决VLA模型无法在一个模型中掌握多种技能的根本问题。

Method: 1. 识别VLA微调中不可合并性的两个关键来源：LoRA适配器发散和动作专家中的跨块依赖；2. 提出MergeVLA架构：使用任务掩码的稀疏激活LoRA适配器保持参数一致性，用仅交叉注意力块替换自注意力以保持专业化局部化；3. 测试时使用任务路由器自适应选择任务掩码和专家头。

Result: 在LIBERO、LIBERO-Plus、RoboTwin和真实SO101机械臂上的多任务实验中，MergeVLA实现了与单独微调专家相当甚至更好的性能，展示了跨任务、体现和环境的鲁棒泛化能力。

Conclusion: MergeVLA通过设计保持可合并性，成功解决了VLA模型在多技能设置中的合并问题，为构建能够掌握多种技能的单一VLA模型提供了可行方案。

Abstract: Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.

</details>
