{"id": "2511.16911", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.16911", "abs": "https://arxiv.org/abs/2511.16911", "authors": ["Yendo Hu", "Yiliang Wu", "Weican Chen"], "title": "Multi-UAV Swarm Obstacle Avoidance Based on Potential Field Optimization", "comment": "12 pages, 13 figures, and 2 tables", "summary": "In multi UAV scenarios,the traditional Artificial Potential Field (APF) method often leads to redundant flight paths and frequent abrupt heading changes due to unreasonable obstacle avoidance path planning,and is highly prone to inter UAV collisions during the obstacle avoidance process.To address these issues,this study proposes a novel hybrid algorithm that combines the improved Multi-Robot Formation Obstacle Avoidance (MRF IAPF) algorithm with an enhanced APF optimized for single UAV path planning.Its core ideas are as follows:first,integrating three types of interaction forces from MRF IAPF obstacle repulsion force,inter UAV interaction force,and target attraction force;second,incorporating a refined single UAV path optimization mechanism,including collision risk assessment and an auxiliary sub goal strategy.When a UAV faces a high collision threat,temporary waypoints are generated to guide obstacle avoidance,ensuring eventual precise arrival at the actual target.Simulation results demonstrate that compared with traditional APF based formation algorithms,the proposed algorithm achieves significant improvements in path length optimization and heading stability,can effectively avoid obstacles and quickly restore the formation configuration,thus verifying its applicability and effectiveness in static environments with unknown obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6539\u8fdb\u591a\u673a\u5668\u4eba\u7f16\u961f\u907f\u969c\u7b97\u6cd5\u548c\u589e\u5f3a\u4eba\u5de5\u52bf\u573a\u6cd5\u7684\u6df7\u5408\u7b97\u6cd5\uff0c\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u4f20\u7edfAPF\u65b9\u6cd5\u7684\u8def\u5f84\u5197\u4f59\u3001\u822a\u5411\u7a81\u53d8\u548c\u78b0\u649e\u98ce\u9669\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u52bf\u573a\u6cd5\u5728\u591a\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u4f1a\u5bfc\u81f4\u98de\u884c\u8def\u5f84\u5197\u4f59\u3001\u822a\u5411\u9891\u7e41\u7a81\u53d8\uff0c\u4e14\u5728\u907f\u969c\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u53d1\u751f\u65e0\u4eba\u673a\u95f4\u78b0\u649e\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684MRF IAPF\u7f16\u961f\u907f\u969c\u7b97\u6cd5\u548c\u589e\u5f3a\u7684\u5355\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212APF\uff0c\u96c6\u6210\u969c\u788d\u7269\u6392\u65a5\u529b\u3001\u65e0\u4eba\u673a\u95f4\u76f8\u4e92\u4f5c\u7528\u529b\u548c\u76ee\u6807\u5438\u5f15\u529b\uff0c\u5e76\u52a0\u5165\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u548c\u8f85\u52a9\u5b50\u76ee\u6807\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edfAPF\u7f16\u961f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u4f18\u5316\u548c\u822a\u5411\u7a33\u5b9a\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u80fd\u6709\u6548\u907f\u969c\u5e76\u5feb\u901f\u6062\u590d\u7f16\u961f\u914d\u7f6e\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u5728\u672a\u77e5\u969c\u788d\u7269\u9759\u6001\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.16717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16717", "abs": "https://arxiv.org/abs/2511.16717", "authors": ["Asya Y. Akkus", "Bradley T. Wolfe", "Pinghan Chu", "Chengkun Huang", "Chris S. Campbell", "Mariana Alvarado Alvarez", "Petr Volegov", "David Fittinghoff", "Robert Reinovsky", "Zhehui Wang"], "title": "A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images", "comment": null, "summary": "Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\u548cCDF 97\u5c0f\u6ce2\u53d8\u6362\u7684\u6df7\u5408\u9ad8\u65af-\u6cca\u677e\u53bb\u566a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2d\u5b50\u6210\u50cf\u6570\u636e\u7684\u566a\u58f0\u53bb\u9664\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u91cd\u5efa\u8bef\u5dee\u548c\u66f4\u597d\u7684\u8fb9\u7f18\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u4e2d\u5b50\u6210\u50cf\u5728\u60ef\u6027\u7ea6\u675f\u805a\u53d8(ICF)\u5206\u6790\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u56fe\u50cf\u5e38\u88ab\u9ad8\u65af\u548c\u6cca\u677e\u566a\u58f0\u5171\u540c\u6c61\u67d3\uff0c\u4f20\u7edf\u6ee4\u6ce2\u548c\u9608\u503c\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u53bb\u9664\u8fd9\u4e9b\u91cd\u53e0\u566a\u58f0\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u4fdd\u6301\u56fe\u50cf\u4fdd\u771f\u5ea6\u7684\u53bb\u566a\u6280\u672f\u3002", "method": "\u4f7f\u7528\u65e0\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7ed3\u5408Cohen-Daubechies-Feauveau (CDF 97)\u5c0f\u6ce2\u53d8\u6362\uff0c\u4e13\u95e8\u9488\u5bf9\u6df7\u5408\u9ad8\u65af-\u6cca\u677e\u566a\u58f0\u8fdb\u884c\u53bb\u566a\u5904\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u53bb\u9664\u4e86\u4e2d\u5b50\u6210\u50cf\u6570\u636e\u4e2d\u7684\u566a\u58f0\uff0c\u76f8\u6bd4\u975e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5982BM3D\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u91cd\u5efa\u8bef\u5dee\u548c\u66f4\u4f18\u7684\u8fb9\u7f18\u4fdd\u6301\u6307\u6807\u3002", "conclusion": "\u8fd9\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u53bb\u566a\u65b9\u6cd5\u4e3a\u4e2d\u5b50\u56fe\u50cf\u566a\u58f0\u51cf\u5c11\u548cICF\u5b9e\u9a8c\u7684\u4e09\u7ef4\u91cd\u5efa\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.16699", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16699", "abs": "https://arxiv.org/abs/2511.16699", "authors": ["Juan P. Cadile"], "title": "Detecting and Steering LLMs' Empathy in Action", "comment": "14 pages, 9 figures", "summary": "We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).\n  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.\n  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).\n  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5171\u60c5\u884c\u4e3a\uff08\u727a\u7272\u4efb\u52a1\u6548\u7387\u6ee1\u8db3\u4eba\u7c7b\u9700\u6c42\uff09\u662fLLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u53ef\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u68c0\u6d4b\u548c\u64cd\u63a7\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u5728\u64cd\u63a7\u7a33\u5065\u6027\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u662f\u5426\u7f16\u7801\u4e86\u5171\u60c5\u884c\u4e3a\uff0c\u4ee5\u53ca\u8fd9\u79cd\u7f16\u7801\u662f\u5426\u72ec\u7acb\u4e8e\u5b89\u5168\u8bad\u7ec3\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u68c0\u6d4b\u548c\u64cd\u63a7\u8fd9\u79cd\u5171\u60c5\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eEIA\u57fa\u51c6\u7684\u5bf9\u6bd4\u63d0\u793a\uff0c\u5728Phi-3-mini-4k\u3001Qwen2.5-7B\u548cDolphin-Llama-3.1-8B\u4e09\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u68c0\u6d4b\u548c\u64cd\u63a7\u5b9e\u9a8c\uff0c\u5206\u6790\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u80fd\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u5171\u60c5\u884c\u4e3a\uff08AUROC 0.996-1.00\uff09\uff0c\u4f46\u64cd\u63a7\u6548\u679c\u5404\u5f02\uff1aQwen\u548cPhi-3\u53ef\u5b9e\u73b0\u53cc\u5411\u64cd\u63a7\uff0cDolphin\u4ec5\u652f\u6301\u589e\u5f3a\u5171\u60c5\u7684\u5355\u5411\u64cd\u63a7\uff0c\u53cd\u5171\u60c5\u64cd\u63a7\u4f1a\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u3002", "conclusion": "\u5171\u60c5\u7f16\u7801\u72ec\u7acb\u4e8e\u5b89\u5168\u8bad\u7ec3\uff0c\u4f46\u5b89\u5168\u8bad\u7ec3\u5f71\u54cd\u64cd\u63a7\u7684\u7a33\u5065\u6027\uff1b\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u5b9e\u73b0\u5171\u60c5\u7684\u65b9\u5f0f\u4e0d\u540c\uff0c\u68c0\u6d4b\u548c\u64cd\u63a7\u80fd\u529b\u5b58\u5728\u6a21\u578b\u7279\u5f02\u6027\u5dee\u5f02\u3002"}}
{"id": "2511.16824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16824", "abs": "https://arxiv.org/abs/2511.16824", "authors": ["Eva Neu", "Brian Dillon", "Katrin Erk"], "title": "Interpretable dimensions support an effect of agentivity and telicity on split intransitivity", "comment": null, "summary": "Intransitive verbs fall into two different syntactic classes, unergatives and unaccusatives. It has long been argued that verbs describing an agentive action are more likely to appear in an unergative syntax, and those describing a telic event to appear in an unaccusative syntax. However, recent work by Kim et al. (2024) found that human ratings for agentivity and telicity were a poor predictor of the syntactic behavior of intransitives. Here we revisit this question using interpretable dimensions, computed from seed words on opposite poles of the agentive and telic scales. Our findings support the link between unergativity/unaccusativity and agentivity/telicity, and demonstrate that using interpretable dimensions in conjunction with human judgments can offer valuable evidence for semantic properties that are not easily evaluated in rating tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cd\u65b0\u63a2\u8ba8\u4e86\u975e\u5bbe\u683c\u52a8\u8bcd\u4e0e\u65bd\u4e8b\u6027/\u7ec8\u7ed3\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f7f\u7528\u57fa\u4e8e\u79cd\u5b50\u8bcd\u7684\u53ef\u89e3\u91ca\u7ef4\u5ea6\u5206\u6790\uff0c\u53d1\u73b0\u4e24\u8005\u786e\u5b9e\u5b58\u5728\u5173\u8054\uff0c\u4e14\u53ef\u89e3\u91ca\u7ef4\u5ea6\u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\u80fd\u66f4\u597d\u5730\u8bc4\u4f30\u8bed\u4e49\u5c5e\u6027\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u63cf\u8ff0\u65bd\u4e8b\u884c\u4e3a\u7684\u52a8\u8bcd\u66f4\u503e\u5411\u4e8e\u975e\u4f5c\u683c\u53e5\u6cd5\uff0c\u800c\u63cf\u8ff0\u7ec8\u7ed3\u4e8b\u4ef6\u7684\u52a8\u8bcd\u66f4\u503e\u5411\u4e8e\u975e\u5bbe\u683c\u53e5\u6cd5\u3002\u4f46Kim\u7b49\u4eba(2024)\u53d1\u73b0\u4eba\u7c7b\u5bf9\u65bd\u4e8b\u6027\u548c\u7ec8\u7ed3\u6027\u7684\u8bc4\u5206\u5e76\u4e0d\u80fd\u5f88\u597d\u5730\u9884\u6d4b\u4e0d\u53ca\u7269\u52a8\u8bcd\u7684\u53e5\u6cd5\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u79cd\u5b50\u8bcd\u7684\u53ef\u89e3\u91ca\u7ef4\u5ea6\uff0c\u8ba1\u7b97\u65bd\u4e8b\u6027\u548c\u7ec8\u7ed3\u6027\u91cf\u8868\u5bf9\u7acb\u6781\u70b9\u7684\u7ef4\u5ea6\u503c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u975e\u4f5c\u683c\u6027/\u975e\u5bbe\u683c\u6027\u4e0e\u65bd\u4e8b\u6027/\u7ec8\u7ed3\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u4f7f\u7528\u53ef\u89e3\u91ca\u7ef4\u5ea6\u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\u53ef\u4ee5\u4e3a\u4e0d\u6613\u5728\u8bc4\u5206\u4efb\u52a1\u4e2d\u8bc4\u4f30\u7684\u8bed\u4e49\u5c5e\u6027\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8bc1\u636e\u3002"}}
{"id": "2511.17318", "categories": ["cs.RO", "cs.AI", "cs.CE", "cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.17318", "abs": "https://arxiv.org/abs/2511.17318", "authors": ["Mikael Lundb\u00e4ck", "Erik Wallin", "Carola H\u00e4ggstr\u00f6m", "Mattias Nystr\u00f6m", "Andreas Gr\u00f6nlund", "Mats Richardson", "Petrus J\u00f6nsson", "William Arnvik", "Lucas Hedstr\u00f6m", "Arvid F\u00e4lldin", "Martin Servin"], "title": "FORWARD: Dataset of a forwarder operating in rough terrain", "comment": "25 pages, 22 figures", "summary": "We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.", "AI": {"tldr": "FORWARD\u662f\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e86\u745e\u5178\u4e2d\u90e8\u4e24\u4e2a\u91c7\u4f10\u73b0\u573a\u7684\u4f10\u6728\u96c6\u6750\u673a\u4f5c\u4e1a\u6570\u636e\uff0c\u5305\u542bRTK-GNSS\u3001360\u5ea6\u6444\u50cf\u5934\u3001\u632f\u52a8\u4f20\u611f\u5668\u7b49\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u7528\u4e8e\u5f00\u53d1\u6797\u4e1a\u673a\u68b0\u7684\u4ea4\u901a\u6027\u3001\u611f\u77e5\u548c\u81ea\u4e3b\u63a7\u5236\u6a21\u578b\u3002", "motivation": "\u4e3a\u5f00\u53d1\u6797\u4e1a\u673a\u68b0\u7684\u4ea4\u901a\u6027\u3001\u611f\u77e5\u548c\u81ea\u4e3b\u63a7\u5236\u6a21\u578b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u652f\u6301\u4eba\u5de5\u667a\u80fd\u3001\u4eff\u771f\u548c\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u7684\u5927\u578bKomatsu\u96c6\u6750\u673a\u5728\u745e\u5178\u4e2d\u90e8\u91c7\u4f10\u73b0\u573a\u6536\u96c6\u6570\u636e\uff0c\u5305\u62ecRTK-GNSS\u3001360\u5ea6\u6444\u50cf\u5934\u3001\u632f\u52a8\u4f20\u611f\u5668\u3001CAN\u603b\u7ebf\u4fe1\u53f7\u548cIMU\u7b49\uff0c\u6570\u636e\u91c7\u96c6\u9891\u7387\u4e3a5Hz\uff0c\u6db5\u76d6\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u8f66\u8f86\u4f4d\u7f6e\u3001\u884c\u9a76\u901f\u5ea6\u3001\u71c3\u6cb9\u6d88\u8017\u7b49\u4fe1\u606f\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u7ea618\u5c0f\u65f6\u5e38\u89c4\u6728\u6750\u63d0\u53d6\u5de5\u4f5c\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u5206\u8fa8\u7387\u6fc0\u5149\u626b\u63cf\u5730\u5f62\u6570\u636e\uff08\u7ea61500\u70b9/\u5e73\u65b9\u7c73\uff09\u3001\u751f\u4ea7\u65e5\u5fd7\u6587\u4ef6\u3001\u89c6\u9891\u7d20\u6750\u548c\u5730\u5f62\u6570\u636e\uff0c\u5e76\u5bf9360\u5ea6\u89c6\u9891\u6750\u6599\u8fdb\u884c\u4e86\u5de5\u4f5c\u5143\u7d20\u6807\u6ce8\u3002", "conclusion": "FORWARD\u6570\u636e\u96c6\u4e3a\u6797\u4e1a\u673a\u68b0\u7684\u81ea\u52a8\u5316\u63a7\u5236\u3001\u4ea4\u901a\u6027\u5206\u6790\u548c\u4eff\u771f\u6821\u51c6\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u4f5c\u4e1a\u6548\u7387\u3001\u964d\u4f4e\u71c3\u6cb9\u6d88\u8017\u5e76\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u3002"}}
{"id": "2511.16901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16901", "abs": "https://arxiv.org/abs/2511.16901", "authors": ["Lu Zhu", "Tiantian Geng", "Yangye Chen", "Teng Wang", "Ping Lu", "Feng Zheng"], "title": "R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios", "comment": "Accepted by AAAI 2026. Project page: https://github.com/zhlllau/R-AVST", "summary": "Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86R-AVST\u6570\u636e\u96c6\u548cAVST-Zero\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u97f3\u9891-\u89c6\u9891\u65f6\u7a7a\u63a8\u7406\u4efb\u52a1\u3002R-AVST\u5305\u542b5K\u4e2a\u672a\u526a\u8f91\u89c6\u9891\u548c27K\u4e2a\u5bf9\u8c61\uff0c\u8986\u76d6100\u79cd\u97f3\u9891-\u89c6\u89c9\u4e8b\u4ef6\uff1bAVST-Zero\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u884c\u4e3a\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u89c6\u9891\u573a\u666f\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u591a\u6837\u7684\u97f3\u9891-\u89c6\u89c9\u4e8b\u4ef6\u3002\u9700\u8981\u6784\u5efa\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "method": "1) \u6784\u5efaR-AVST\u6570\u636e\u96c6\uff1a\u4f7f\u7528LLM\u63d0\u53d6\u5173\u952e\u5bf9\u8c61\u3001\u81ea\u52a8\u7a7a\u95f4\u6807\u6ce8\u548c\u4eba\u5de5\u8d28\u91cf\u68c0\u67e5\uff1b2) \u63d0\u51faAVST-Zero\u6a21\u578b\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u907f\u514d\u4e2d\u95f4\u76d1\u7763\uff0c\u901a\u8fc7\u591a\u7ef4\u5956\u52b1\u76f4\u63a5\u4f18\u5316\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86R-AVST\u6570\u636e\u96c6\u5728\u63a8\u8fdb\u97f3\u9891-\u89c6\u9891\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\uff0cAVST-Zero\u76f8\u6bd4\u73b0\u6709\u6a21\u578b\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "R-AVST\u662f\u9996\u4e2a\u4e13\u4e3a\u771f\u5b9e\u4e16\u754c\u97f3\u9891-\u89c6\u9891\u65f6\u7a7a\u63a8\u7406\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\uff0cAVST-Zero\u4e3a\u8be5\u9886\u57df\u672a\u6765\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u89c6\u89d2\u3002"}}
{"id": "2511.17012", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17012", "abs": "https://arxiv.org/abs/2511.17012", "authors": ["Junjie Hao", "Chun Wang", "Ying Qiao", "Qiuyue Zuo", "Qiya Song", "Hua Ma", "Xieping Gao"], "title": "Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities", "comment": null, "summary": "Large language models and knowledge graphs offer strong potential for advancing research on historical culture by supporting the extraction, analysis, and interpretation of cultural heritage. Using Hunan's modern historical celebrities shaped by Huxiang culture as a case study, pre-trained large models can help researchers efficiently extract key information, including biographical attributes, life events, and social relationships, from textual sources and construct structured knowledge graphs. However, systematic data resources for Hunan's historical celebrities remain limited, and general-purpose models often underperform in domain knowledge extraction and structured output generation in such low-resource settings. To address these issues, this study proposes a supervised fine-tuning approach for enhancing domain-specific information extraction. First, we design a fine-grained, schema-guided instruction template tailored to the Hunan historical celebrities domain and build an instruction-tuning dataset to mitigate the lack of domain-specific training corpora. Second, we apply parameter-efficient instruction fine-tuning to four publicly available large language models - Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct - and develop evaluation criteria for assessing their extraction performance. Experimental results show that all models exhibit substantial performance gains after fine-tuning. Among them, Qwen3-8B achieves the strongest results, reaching a score of 89.3866 with 100 samples and 50 training iterations. This study provides new insights into fine-tuning vertical large language models for regional historical and cultural domains and highlights their potential for cost-effective applications in cultural heritage knowledge extraction and knowledge graph construction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u63d0\u53d6\uff0c\u9488\u5bf9\u6e56\u5357\u5386\u53f2\u540d\u4eba\u9886\u57df\u8bbe\u8ba1\u7ec6\u7c92\u5ea6\u6307\u4ee4\u6a21\u677f\u5e76\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0cQwen3-8B\u5728\u5fae\u8c03\u540e\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u6e56\u5357\u5386\u53f2\u540d\u4eba\u7cfb\u7edf\u6570\u636e\u8d44\u6e90\u6709\u9650\uff0c\u901a\u7528\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u9886\u57df\u77e5\u8bc6\u63d0\u53d6\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u751f\u6210\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u63d0\u53d6\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u9488\u5bf9\u6e56\u5357\u5386\u53f2\u540d\u4eba\u9886\u57df\u7684\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u5f15\u5bfc\u6307\u4ee4\u6a21\u677f\uff0c\u6784\u5efa\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u5bf9\u56db\u4e2a\u516c\u5f00\u5927\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u6307\u4ee4\u5fae\u8c03\uff0c\u5e76\u5f00\u53d1\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5fae\u8c03\u540e\u6027\u80fd\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5176\u4e2dQwen3-8B\u8868\u73b0\u6700\u4f73\uff0c\u5728100\u4e2a\u6837\u672c\u548c50\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e0b\u8fbe\u523089.3866\u5206\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5782\u76f4\u5927\u6a21\u578b\u5728\u533a\u57df\u5386\u53f2\u6587\u5316\u9886\u57df\u7684\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6587\u5316\u9057\u4ea7\u77e5\u8bc6\u63d0\u53d6\u548c\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e2d\u6210\u672c\u6548\u76ca\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.16923", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.16923", "abs": "https://arxiv.org/abs/2511.16923", "authors": ["Ali Anaissi", "Deshao Liu", "Yuanzhe Jia", "Weidong Huang", "Widad Alyassine", "Junaid Akram"], "title": "A Hybrid Computational Intelligence Framework for scRNA-seq Imputation: Integrating scRecover and Random Forests", "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables transcriptomic profiling at cellular resolution but suffers from pervasive dropout events that obscure biological signals. We present SCR-MF, a modular two-stage workflow that combines principled dropout detection using scRecover with robust non-parametric imputation via missForest. Across public and simulated datasets, SCR-MF achieves robust and interpretable performance comparable to or exceeding existing imputation methods in most cases, while preserving biological fidelity and transparency. Runtime analysis demonstrates that SCR-MF provides a competitive balance between accuracy and computational efficiency, making it suitable for mid-scale single-cell datasets.", "AI": {"tldr": "SCR-MF\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u53cc\u9636\u6bb5\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u4e86scRecover\u7684dropout\u68c0\u6d4b\u548cmissForest\u7684\u975e\u53c2\u6570\u63d2\u8865\uff0c\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u4e2d\u5b9e\u73b0\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u63d2\u8865\u6027\u80fd\u3002", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u5b58\u5728\u666e\u904d\u7684dropout\u4e8b\u4ef6\uff0c\u4f1a\u63a9\u76d6\u751f\u7269\u4fe1\u53f7\uff0c\u9700\u8981\u6709\u6548\u7684\u63d2\u8865\u65b9\u6cd5\u6765\u6062\u590d\u8fd9\u4e9b\u4e22\u5931\u7684\u6570\u636e\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u53cc\u9636\u6bb5\u5de5\u4f5c\u6d41\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528scRecover\u8fdb\u884c\u539f\u7406\u6027dropout\u68c0\u6d4b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528missForest\u8fdb\u884c\u7a33\u5065\u7684\u975e\u53c2\u6570\u63d2\u8865\u3002", "result": "\u5728\u516c\u5171\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\uff0cSCR-MF\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8fbe\u5230\u6216\u8d85\u8fc7\u73b0\u6709\u63d2\u8865\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u7269\u4fdd\u771f\u5ea6\u548c\u900f\u660e\u5ea6\u3002\u8fd0\u884c\u65f6\u5206\u6790\u663e\u793a\u5176\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "conclusion": "SCR-MF\u9002\u5408\u4e2d\u7b49\u89c4\u6a21\u5355\u7ec6\u80de\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u63d2\u8865\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16933", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.16933", "abs": "https://arxiv.org/abs/2511.16933", "authors": ["Angelina Yan", "Matt L. Sampson", "Peter Melchior"], "title": "A novel approach to classification of ECG arrhythmia types with latent ODEs", "comment": "Accepted into NeurIPS 2025 Learning from Time Series for Health workshop", "summary": "12-lead ECGs with high sampling frequency are the clinical gold standard for arrhythmia detection, but their short-term, spot-check nature often misses intermittent events. Wearable ECGs enable long-term monitoring but suffer from irregular, lower sampling frequencies due to battery constraints, making morphology analysis challenging. We present an end-to-end classification pipeline to address these issues. We train a latent ODE to model continuous ECG waveforms and create robust feature vectors from high-frequency single-channel signals. We construct three latent vectors per waveform via downsampling the initial 360 Hz ECG to 90 Hz and 45 Hz. We then use a gradient boosted tree to classify these vectors and test robustness across frequencies. Performance shows minimal degradation, with macro-averaged AUC-ROC values of 0.984, 0.978, and 0.976 at 360 Hz, 90 Hz, and 45 Hz, respectively, suggesting a way to sidestep the trade-off between signal fidelity and battery life. This enables smaller wearables, promoting long-term monitoring of cardiac health.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5fc3\u7535\u56fe\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728ODE\u5efa\u6a21\u8fde\u7eedECG\u6ce2\u5f62\uff0c\u5728\u4e0d\u540c\u91c7\u6837\u9891\u7387\u4e0b\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u53ef\u7a7f\u6234\u8bbe\u5907\u7535\u6c60\u5bff\u547d\u4e0e\u4fe1\u53f7\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "12\u5bfc\u8054ECG\u867d\u7136\u91c7\u6837\u9891\u7387\u9ad8\u4f46\u53ea\u80fd\u77ed\u671f\u76d1\u6d4b\uff0c\u800c\u53ef\u7a7f\u6234ECG\u867d\u7136\u80fd\u957f\u671f\u76d1\u6d4b\u4f46\u53d7\u7535\u6c60\u9650\u5236\u91c7\u6837\u9891\u7387\u8f83\u4f4e\u4e14\u4e0d\u89c4\u5219\uff0c\u96be\u4ee5\u8fdb\u884c\u5f62\u6001\u5206\u6790\u3002", "method": "\u8bad\u7ec3\u6f5c\u5728ODE\u6a21\u578b\u6765\u5efa\u6a21\u8fde\u7eedECG\u6ce2\u5f62\uff0c\u4ece\u9ad8\u9891\u5355\u901a\u9053\u4fe1\u53f7\u521b\u5efa\u9c81\u68d2\u7279\u5f81\u5411\u91cf\uff0c\u901a\u8fc7\u5c06360Hz ECG\u4e0b\u91c7\u6837\u523090Hz\u548c45Hz\u6784\u5efa\u4e09\u4e2a\u6f5c\u5728\u5411\u91cf\uff0c\u7136\u540e\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u6811\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u4e0d\u540c\u9891\u7387\u4e0b\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\uff0c360Hz\u300190Hz\u548c45Hz\u7684\u5b8f\u5e73\u5747AUC-ROC\u503c\u5206\u522b\u4e3a0.984\u30010.978\u548c0.976\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7ed5\u8fc7\u4fe1\u53f7\u4fdd\u771f\u5ea6\u4e0e\u7535\u6c60\u5bff\u547d\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4f7f\u66f4\u5c0f\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u6210\u4e3a\u53ef\u80fd\uff0c\u4fc3\u8fdb\u5fc3\u810f\u5065\u5eb7\u7684\u957f\u671f\u76d1\u6d4b\u3002"}}
{"id": "2511.17008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17008", "abs": "https://arxiv.org/abs/2511.17008", "authors": ["Zexi Tan", "Xiaopeng Luo", "Yunlin Liu", "Yiqun Zhang"], "title": "Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering", "comment": "Accepted to AAAI 2026", "summary": "Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, with its model architecture composed of Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) representation learning modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the MEV-based reconstruction and contrastive learning pathways enhance the generalization. That is, the MEV reconstruction facilitates multi-perspective complementary to prevent the masking from premature convergence, and the clustering-guided contrastive learning facilitates the joint optimization of representation and clustering. Extensive experiments on 15 real benchmark datasets demonstrate the superiority of EMTC in comparison with eight SOTA methods, where the EMTC achieves an average improvement of 4.85% over the strongest baselines.", "AI": {"tldr": "\u63d0\u51faEMTC\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u7684\u53d8\u91cf\u7ea7\u63a9\u7801\u548c\u591a\u6e90\u89c6\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u4e2d\u5197\u4f59\u4fe1\u606f\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5305\u542b\u5927\u91cf\u5197\u4f59\u4fe1\u606f\uff08\u5982\u7a33\u6001\u673a\u5668\u8fd0\u884c\u8bb0\u5f55\u3001\u592a\u9633\u80fd\u53d1\u7535\u96f6\u8f93\u51fa\u671f\uff09\uff0c\u8fd9\u4e9b\u5197\u4f59\u964d\u4f4e\u4e86\u6a21\u578b\u5bf9\u5224\u522b\u6027\u65f6\u95f4\u6233\u7684\u5173\u6ce8\uff0c\u5bfc\u81f4\u805a\u7c7b\u6027\u80fd\u74f6\u9888\u3002\u73b0\u6709\u63a9\u7801\u7b56\u7565\u591a\u4e3a\u72ec\u7acb\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u805a\u7c7b\u5173\u952e\u65f6\u95f4\u6233\u7684\u91cd\u8981\u6027\u53d8\u5316\u3002", "method": "\u63d0\u51faEMTC\u65b9\u6cd5\uff0c\u5305\u542b\u91cd\u8981\u6027\u611f\u77e5\u53d8\u91cf\u7ea7\u63a9\u7801(IVM)\u548c\u591a\u6e90\u89c6\u56fe(MEV)\u8868\u793a\u5b66\u4e60\u6a21\u5757\u3002IVM\u81ea\u9002\u5e94\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u66f4\u5177\u5224\u522b\u6027\u7684\u805a\u7c7b\u8868\u793a\uff0cMEV\u901a\u8fc7\u91cd\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u8def\u5f84\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u572815\u4e2a\u771f\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEMTC\u4f18\u4e8e8\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e73\u5747\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u53474.85%\u3002", "conclusion": "EMTC\u901a\u8fc7\u52a8\u6001\u63a9\u7801\u548c\u591a\u91cd\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\u3002"}}
{"id": "2511.17170", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17170", "abs": "https://arxiv.org/abs/2511.17170", "authors": ["Vy Nguyen", "Ziqi Xu", "Jeffrey Chan", "Estrid He", "Feng Xia", "Xiuzhen Zhang"], "title": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models", "comment": "Accepted to AAAI 2026 (Main Technical Track)", "summary": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.", "AI": {"tldr": "\u63d0\u51faAspect-Based Causal Abstention (ABCA)\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u5206\u6790LLM\u5185\u90e8\u77e5\u8bc6\u591a\u6837\u6027\u6765\u5b9e\u73b0\u65e9\u671f\u5f03\u6743\uff0c\u9632\u6b62\u4ea7\u751f\u4e0d\u53ef\u9760\u56de\u7b54\u3002", "motivation": "\u73b0\u6709\u5f03\u6743\u65b9\u6cd5\u4f9d\u8d56\u751f\u6210\u540e\u4fe1\u53f7\uff0c\u65e0\u6cd5\u9884\u5148\u9632\u6b62\u4e0d\u53ef\u9760\u54cd\u5e94\u3002LLM\u4ece\u4e0d\u540c\u6765\u6e90\u83b7\u53d6\u7684\u77e5\u8bc6\u5177\u6709\u591a\u9762\u6027\uff0c\u8fd9\u79cd\u591a\u6837\u6027\u53ef\u7528\u4e8e\u8bc4\u4f30\u77e5\u8bc6\u53ef\u9760\u6027\u3002", "method": "ABCA\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u77e5\u8bc6\u7684\u4e0d\u540c\u65b9\u9762\uff08\u5982\u5b66\u79d1\u3001\u6cd5\u5f8b\u80cc\u666f\u3001\u65f6\u95f4\u6846\u67b6\uff09\u7684\u591a\u6837\u6027\uff0c\u4f7f\u7528\u56e0\u679c\u63a8\u7406\u4f30\u8ba1\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u6765\u8bc4\u4f30\u67e5\u8be2\u76f8\u5173\u77e5\u8bc6\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cABCA\u63d0\u9ad8\u4e86\u5f03\u6743\u53ef\u9760\u6027\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u5f03\u6743\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ABCA\u6846\u67b6\u901a\u8fc7\u5229\u7528LLM\u5185\u90e8\u77e5\u8bc6\u591a\u6837\u6027\u7684\u56e0\u679c\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u65e9\u671f\u5f03\u6743\u673a\u5236\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.17040", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17040", "abs": "https://arxiv.org/abs/2511.17040", "authors": ["Wenzhang Du"], "title": "Step-E: A Differentiable Data Cleaning Framework for Robust Learning with Noisy Labels", "comment": "12 pages, 4 figures", "summary": "Training data collected in the wild often contain noisy labels and outliers that substantially degrade the performance and reliability of deep neural networks. While data cleaning is commonly applied as a separate preprocessing stage, such two-stage pipelines neither fully exploit feedback from the downstream model nor adapt to unknown noise patterns. We propose Step-E, a simple framework that integrates sample selection and model learning into a single optimization process. At each epoch, Step-E ranks samples by loss and gradually increases the fraction of high-loss examples that are excluded from gradient updates after a brief warm-up stage, yielding an online curriculum that focuses on easy and consistent examples and eventually ignores persistent outliers. On CIFAR-100N, Step-E improves the test accuracy of a ResNet-18 model from 43.3% (+/- 0.7%) to 50.4% (+/- 0.9%), clearly outperforming loss truncation, self-paced learning, and one-shot filtering while approaching the clean-label oracle at 60.5% (+/- 0.2%). On CIFAR-10N (aggre), Step-E also improves over the noisy baseline (85.3% vs. 83.9%) and nearly matches the clean-label oracle (85.9%), with only moderate training-time overhead.", "AI": {"tldr": "Step-E\u662f\u4e00\u4e2a\u5c06\u6837\u672c\u9009\u62e9\u548c\u6a21\u578b\u5b66\u4e60\u96c6\u6210\u5230\u5355\u4e00\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u635f\u5931\u7684\u6837\u672c\u6392\u5e8f\u548c\u6e10\u8fdb\u5f0f\u6392\u9664\u9ad8\u635f\u5931\u6837\u672c\uff0c\u6709\u6548\u5904\u7406\u566a\u58f0\u6807\u7b7e\u548c\u5f02\u5e38\u503c\u3002", "motivation": "\u91ce\u5916\u6536\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u5e38\u5305\u542b\u566a\u58f0\u6807\u7b7e\u548c\u5f02\u5e38\u503c\uff0c\u8fd9\u4f1a\u4e25\u91cd\u964d\u4f4e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002\u4f20\u7edf\u7684\u6570\u636e\u6e05\u6d17\u4f5c\u4e3a\u72ec\u7acb\u9884\u5904\u7406\u9636\u6bb5\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0b\u6e38\u6a21\u578b\u7684\u53cd\u9988\uff0c\u4e5f\u65e0\u6cd5\u9002\u5e94\u672a\u77e5\u7684\u566a\u58f0\u6a21\u5f0f\u3002", "method": "Step-E\u5728\u6bcf\u4e2aepoch\u6839\u636e\u635f\u5931\u5bf9\u6837\u672c\u8fdb\u884c\u6392\u5e8f\uff0c\u7ecf\u8fc7\u77ed\u6682\u9884\u70ed\u9636\u6bb5\u540e\uff0c\u9010\u6b65\u589e\u52a0\u4ece\u68af\u5ea6\u66f4\u65b0\u4e2d\u6392\u9664\u7684\u9ad8\u635f\u5931\u6837\u672c\u6bd4\u4f8b\uff0c\u5f62\u6210\u5728\u7ebf\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\uff0c\u4e13\u6ce8\u4e8e\u7b80\u5355\u4e00\u81f4\u7684\u6837\u672c\u5e76\u6700\u7ec8\u5ffd\u7565\u6301\u7eed\u5f02\u5e38\u503c\u3002", "result": "\u5728CIFAR-100N\u4e0a\uff0cStep-E\u5c06ResNet-18\u6a21\u578b\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u4ece43.3%\u63d0\u5347\u523050.4%\uff0c\u660e\u663e\u4f18\u4e8e\u635f\u5931\u622a\u65ad\u3001\u81ea\u5b9a\u6b65\u8c03\u5b66\u4e60\u548c\u4e00\u6b21\u6027\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u63a5\u8fd1\u5e72\u51c0\u6807\u7b7eoracle\u768460.5%\u3002\u5728CIFAR-10N\u4e0a\u4e5f\u4f18\u4e8e\u566a\u58f0\u57fa\u7ebf(85.3% vs. 83.9%)\uff0c\u63a5\u8fd1\u5e72\u51c0\u6807\u7b7eoracle(85.9%)\uff0c\u8bad\u7ec3\u65f6\u95f4\u5f00\u9500\u9002\u4e2d\u3002", "conclusion": "Step-E\u901a\u8fc7\u96c6\u6210\u6837\u672c\u9009\u62e9\u548c\u6a21\u578b\u5b66\u4e60\uff0c\u6709\u6548\u5904\u7406\u566a\u58f0\u6807\u7b7e\u6570\u636e\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u63a5\u8fd1\u5e72\u51c0\u6807\u7b7e\u7684\u6027\u80fd\u6c34\u5e73\u3002"}}
{"id": "2511.17208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17208", "abs": "https://arxiv.org/abs/2511.17208", "authors": ["Sizhe Zhou"], "title": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents", "comment": "Work in progress", "summary": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8b\u4ef6\u8bed\u4e49\u7684\u5bf9\u8bdd\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5c06\u5bf9\u8bdd\u5386\u53f2\u8868\u793a\u4e3a\u7b80\u77ed\u7684\u4e8b\u4ef6\u547d\u9898\u800c\u975e\u72ec\u7acb\u4e09\u5143\u7ec4\u6216\u4e0d\u900f\u660e\u6458\u8981\uff0c\u901a\u8fc7\u6784\u5efa\u5f02\u8d28\u56fe\u652f\u6301\u5173\u8054\u68c0\u7d22\uff0c\u5728\u957f\u5bf9\u8bdd\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLM\u5bf9\u8bdd\u4ee3\u7406\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4fdd\u6301\u8fde\u8d2f\u6027\u548c\u4e2a\u6027\u5316\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5916\u90e8\u8bb0\u5fc6\u65b9\u6cd5\u5728\u7c97\u7c92\u5ea6\u68c0\u7d22\u548c\u7ec6\u7c92\u5ea6\u788e\u7247\u5316\u89c6\u56fe\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u5c06\u6bcf\u4e2a\u4f1a\u8bdd\u5206\u89e3\u4e3a\u4e30\u5bcc\u7684\u8bed\u7bc7\u57fa\u672c\u5355\u5143\uff08EDUs\uff09\uff0c\u5305\u542b\u5f52\u4e00\u5316\u5b9e\u4f53\u548c\u6765\u6e90\u8f6e\u6b21\u5c5e\u6027\uff0c\u7ec4\u7ec7\u4f1a\u8bdd\u3001EDUs\u53ca\u5176\u53c2\u6570\u5230\u5f02\u8d28\u56fe\u4e2d\uff0c\u652f\u6301\u5bc6\u96c6\u76f8\u4f3c\u6027\u641c\u7d22\u548cLLM\u8fc7\u6ee4\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u5728LoCoMo\u548cLongMemEval_S\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e8b\u4ef6\u4e2d\u5fc3\u8bb0\u5fc6\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u77ed\u7684QA\u4e0a\u4e0b\u6587\uff0c\u8868\u660e\u4e8b\u4ef6\u7ea7\u8bb0\u5fc6\u4e3a\u957f\u89c6\u91ce\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5b9e\u7528\u57fa\u7840\u3002", "conclusion": "\u7ed3\u6784\u7b80\u5355\u7684\u4e8b\u4ef6\u7ea7\u8bb0\u5fc6\u4e3a\u957f\u89c6\u91ce\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u5b9e\u7528\u6027\u7684\u57fa\u7840\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u6301\u5bf9\u8bdd\u7684\u8fde\u8d2f\u6027\u548c\u4e2a\u6027\u5316\u3002"}}
{"id": "2511.17315", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17315", "abs": "https://arxiv.org/abs/2511.17315", "authors": ["Mateusz Jacniacki", "Mart\u00ed Carmona Serrat"], "title": "Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats", "comment": "9 pages, 4 figures", "summary": "Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.\n  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.", "AI": {"tldr": "\u63d0\u51fa\u4e86HUMA\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u7528\u6237\u5bf9\u8bdd\u4ee3\u7406\uff0c\u80fd\u591f\u5728\u7fa4\u804a\u4e2d\u4f7f\u7528\u7c7b\u4eba\u7b56\u7565\u548c\u65f6\u673a\u8fdb\u884c\u81ea\u7136\u4ea4\u4e92\uff0c\u5728\u5b9e\u9a8c\u4e2d\u53c2\u4e0e\u8005\u96be\u4ee5\u533a\u5206AI\u4e0e\u4eba\u7c7b\u793e\u533a\u7ba1\u7406\u8005\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u4e3a\u4e00\u5bf9\u4e00\u7684\u8f6e\u8f6c\u4ea4\u6d41\uff0c\u800c\u975e\u81ea\u7136\u7684\u5f02\u6b65\u7fa4\u804a\u3002\u968f\u7740AI\u52a9\u624b\u5728\u6570\u5b57\u5e73\u53f0\u4e2d\u666e\u53ca\uff0c\u5f00\u53d1\u81ea\u7136\u3001\u7c7b\u4eba\u7684\u4ea4\u4e92\u6a21\u5f0f\u5bf9\u4e8e\u7ef4\u6301\u7528\u6237\u4fe1\u4efb\u548c\u53c2\u4e0e\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "HUMA\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u8def\u7531\u5668\u3001\u884c\u52a8\u4ee3\u7406\u548c\u53cd\u601d\u6a21\u5757\uff0c\u80fd\u591f\u5904\u7406\u6d88\u606f\u3001\u56de\u590d\u3001\u53cd\u5e94\u5e76\u5f15\u5165\u771f\u5b9e\u7684\u54cd\u5e94\u65f6\u95f4\u6a21\u62df\uff0c\u9002\u5e94\u7fa4\u804a\u52a8\u6001\u3002", "result": "\u572897\u540d\u53c2\u4e0e\u8005\u7684\u56db\u4eba\u89d2\u8272\u626e\u6f14\u804a\u5929\u7814\u7a76\u4e2d\uff0c\u53c2\u4e0e\u8005\u5bf9AI\u548c\u4eba\u7c7b\u793e\u533a\u7ba1\u7406\u8005\u7684\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u65e0\u6cd5\u53ef\u9760\u533a\u5206HUMA\u4ee3\u7406\u4e0e\u4eba\u7c7b\u3002\u4e3b\u89c2\u4f53\u9a8c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5dee\u5f02\u5f88\u5c0f\uff0c\u6548\u5e94\u91cf\u8f83\u5c0f\u3002", "conclusion": "\u5728\u81ea\u7136\u7fa4\u804a\u73af\u5883\u4e2d\uff0cAI\u534f\u8c03\u8005\u80fd\u591f\u8fbe\u5230\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u96be\u4ee5\u88ab\u8bc6\u522b\u4e3a\u975e\u4eba\u7c7b\u3002"}}
{"id": "2511.17339", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17339", "abs": "https://arxiv.org/abs/2511.17339", "authors": ["Yassir Bendou", "Omar Ezzahir", "Eduardo Fernandes Montesuma", "Gabriel Mahuas", "Victoria Shevchenko", "Mike Gartrell"], "title": "ReBaPL: Repulsive Bayesian Prompt Learning", "comment": "Under review", "summary": "Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.", "AI": {"tldr": "\u63d0\u51faRepulsive Bayesian Prompt Learning (ReBaPL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6\u4f18\u5316\u63d0\u793a\u5b66\u4e60\uff0c\u7ed3\u5408\u5faa\u73af\u6b65\u957f\u8c03\u5ea6\u548c\u968f\u673a\u68af\u5ea6\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u8868\u793a\u7a7a\u95f4\u7684\u6392\u65a5\u529b\u6765\u589e\u5f3a\u63a2\u7d22\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u8d1d\u53f6\u65af\u63d0\u793a\u5b66\u4e60\u901a\u8fc7\u6982\u7387\u63a8\u7406\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u9700\u8981\u66f4\u6709\u6548\u5730\u63a2\u7d22\u590d\u6742\u7684\u591a\u6a21\u6001\u540e\u9a8c\u5206\u5e03\u3002", "method": "\u7ed3\u5408\u5faa\u73af\u6b65\u957f\u8c03\u5ea6\u4e0eSGHMC\u7b97\u6cd5\uff0c\u4ea4\u66ff\u8fdb\u884c\u63a2\u7d22\u548c\u5229\u7528\u9636\u6bb5\uff1b\u5f15\u5165\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\u548cWasserstein\u8ddd\u79bb\u7684\u8868\u793a\u7a7a\u95f4\u6392\u65a5\u529b\uff0c\u9632\u6b62\u8fc7\u65e9\u6536\u655b\u5230\u5355\u4e00\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86ReBaPL\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "ReBaPL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u8d1d\u53f6\u65af\u6269\u5c55\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u523b\u753b\u63d0\u793a\u540e\u9a8c\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u4f5c\u4e3a\u73b0\u6709\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u7684\u5373\u63d2\u5373\u7528\u6269\u5c55\u3002"}}
{"id": "2511.17094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17094", "abs": "https://arxiv.org/abs/2511.17094", "authors": ["He Huang", "Zixuan Hu", "Dongxiao Li", "Yao Xiao", "Ling-Yu Duan"], "title": "Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models", "comment": null, "summary": "Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\\% and 16.04\\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.", "AI": {"tldr": "ReCoVAD\u662f\u4e00\u4e2a\u8bad\u7ec3\u81ea\u7531\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u53d7\u4eba\u7c7b\u795e\u7ecf\u7cfb\u7edf\u542f\u53d1\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5e27\u5904\u7406\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301SOTA\u6027\u80fd\u7684\u540c\u65f6\u4ec5\u5904\u7406\u5c11\u91cf\u5e27\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u7684VAD\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u5e27\u7ea7\u63a8\u7406\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u63a2\u8ba8\u5728\u4f7f\u7528\u5f3a\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u5bc6\u96c6\u63a8\u7406\u662f\u5426\u771f\u6b63\u5fc5\u8981\u3002", "method": "\u91c7\u7528\u53cc\u901a\u8def\u67b6\u6784\uff1aReflex\u901a\u8def\u4f7f\u7528\u8f7b\u91cfCLIP\u6a21\u5757\u878d\u5408\u89c6\u89c9\u7279\u5f81\u548c\u539f\u578b\u63d0\u793a\uff0c\u67e5\u8be2\u52a8\u6001\u8bb0\u5fc6\u8fdb\u884c\u5feb\u901f\u54cd\u5e94\uff1bConscious\u901a\u8def\u4f7f\u7528\u4e2d\u89c4\u6a21VLM\u751f\u6210\u4e8b\u4ef6\u63cf\u8ff0\u548c\u7cbe\u70bc\u5f02\u5e38\u5206\u6570\uff0c\u901a\u8fc7LLM\u5b9a\u671f\u5ba1\u67e5\u8bc6\u522b\u672a\u89c1\u5f02\u5e38\u3002", "result": "\u5728UCF-Crime\u548cXD-Violence\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u8bad\u7ec3\u81ea\u7531\u6027\u80fd\uff0c\u5206\u522b\u4ec5\u5904\u740628.55%\u548c16.04%\u7684\u5e27\u6570\u3002", "conclusion": "\u7a00\u758f\u63a8\u7406\u5bf9\u4e8e\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u6709\u6548VAD\u662f\u8db3\u591f\u7684\uff0cReCoVAD\u8bc1\u660e\u4e86\u9009\u62e9\u6027\u5e27\u5904\u7406\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.17106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17106", "abs": "https://arxiv.org/abs/2511.17106", "authors": ["Yuan Zhang", "Ming Lu", "Junwen Pan", "Tao Huang", "Kuan Cheng", "Qi She", "Shanghang Zhang"], "title": "ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better", "comment": "16 pages", "summary": "Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\\%$ and shortening output token length by $24.5\\%$.", "AI": {"tldr": "ChainV\u662f\u4e00\u4e2a\u52a8\u6001\u6574\u5408\u89c6\u89c9\u63d0\u793a\u7684\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8865\u4e01\u9009\u62e9\u548c\u6ce8\u610f\u529b\u673a\u5236\u4f7f\u63a8\u7406\u66f4\u77ed\u66f4\u51c6\u786e\uff0c\u5728\u6570\u5b66\u5bc6\u96c6\u578b\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5728\u751f\u6210\u957f\u63a8\u7406\u94fe\u65f6\u5b58\u5728\u5197\u4f59\u81ea\u53cd\u601d\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u9759\u6001\u89c6\u89c9\u53c2\u8003\u7684CoT\u538b\u7f29\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u63a8\u7406\u589e\u76ca\u6709\u9650\u3002", "method": "ChainV\u9996\u5148\u57fa\u4e8e\u524d\u4e00\u6b65\u63a8\u7406\u8fdb\u884c\u7c97\u7c92\u5ea6\u89c6\u89c9\u8865\u4e01\u9009\u62e9\uff0c\u7136\u540e\u901a\u8fc7\u5e73\u5747\u6ce8\u610f\u529b\u5f3a\u5ea6\u8bc6\u522b\u6700\u5177\u4ee3\u8868\u6027\u7684\u539f\u5b50\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u673a\u5236\u6765\u8c03\u6574\u81ea\u53cd\u601d\u7a0b\u5ea6\u3002", "result": "\u5728MathVista\u57fa\u51c6\u4e0a\u5b9e\u73b02.3%\u7cbe\u5ea6\u63d0\u5347\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e51.4%\uff0c\u8f93\u51fatoken\u957f\u5ea6\u7f29\u77ed24.5%\u3002", "conclusion": "ChainV\u901a\u8fc7\u52a8\u6001\u6574\u5408\u89c6\u89c9\u63d0\u793a\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u89c6\u89c9\u63d0\u793a\u7684\u591a\u6b65\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2511.17116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17116", "abs": "https://arxiv.org/abs/2511.17116", "authors": ["Yijun Xu", "Jingrui Zhang", "Hongyi Liu", "Yuhan Chen", "Yuanyang Wang", "Qingyao Guo", "Dingwen Wang", "Lei Yu", "Chu He"], "title": "PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting", "comment": null, "summary": "Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.", "AI": {"tldr": "PEGS\u662f\u4e00\u4e2a\u5c06\u7269\u7406\u5148\u9a8c\u4e0e\u4e8b\u4ef6\u6d41\u589e\u5f3a\u96c6\u6210\u52303D\u9ad8\u65af\u6e85\u5c04\u7ba1\u9053\u4e2d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6267\u884c\u53bb\u6a21\u7cca\u7684\u76ee\u6807\u805a\u7126\u5efa\u6a21\u548c\u8fd0\u52a8\u6062\u590d\uff0c\u5728\u91cd\u5efa\u5927\u65f6\u7a7a\u5c3a\u5ea6\u8fd0\u52a8\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41\u52a8\u6001\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u5efa\u6a21\u8303\u5f0f\u7684\u9650\u5236\u3001\u4e25\u91cd\u7684\u8fd0\u52a8\u6a21\u7cca\u548c\u7269\u7406\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u5927\u65f6\u7a7a\u5c3a\u5ea6\u4e0a\u7684\u521a\u6027\u8fd0\u52a8\u91cd\u5efa\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u91cd\u76d1\u7763\u65b9\u6848\uff1a\u901a\u8fc7\u52a0\u901f\u5ea6\u7ea6\u675f\u5f3a\u5236\u6267\u884c\u7269\u7406\u5408\u7406\u6027\uff0c\u5229\u7528\u4e8b\u4ef6\u6d41\u8fdb\u884c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u6307\u5bfc\uff0c\u5e76\u4f7f\u7528\u5361\u5c14\u66fc\u6b63\u5219\u5316\u5668\u878d\u5408\u591a\u6e90\u89c2\u6d4b\u3002\u8fd8\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5b9e\u65f6\u8fd0\u52a8\u72b6\u6001\u81ea\u9002\u5e94\u8c03\u5ea6\u8bad\u7ec3\u8fc7\u7a0b\u7684\u8fd0\u52a8\u611f\u77e5\u6a21\u62df\u9000\u706b\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e3b\u6d41\u52a8\u6001\u65b9\u6cd5\u76f8\u6bd4\uff0cPEGS\u5728\u91cd\u5efa\u5927\u65f6\u7a7a\u5c3a\u5ea6\u8fd0\u52a8\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "PEGS\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7269\u7406\u5148\u9a8c\u548c\u4e8b\u4ef6\u6d41\u589e\u5f3a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u65f6\u7a7a\u5c3a\u5ea6\u521a\u6027\u8fd0\u52a8\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5e76\u8d21\u732e\u4e86\u9996\u4e2a\u9488\u5bf9\u81ea\u7136\u5feb\u901f\u521a\u6027\u8fd0\u52a8\u7684RGB-Event\u914d\u5bf9\u6570\u636e\u96c6\u3002"}}
{"id": "2511.17171", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17171", "abs": "https://arxiv.org/abs/2511.17171", "authors": ["Mario Markov", "Stefan Maria Ailuro", "Luc Van Gool", "Konrad Schindler", "Danda Pani Paudel"], "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle", "comment": null, "summary": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.", "AI": {"tldr": "FireScope\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u54e8\u51752\u53f7\u5f71\u50cf\u548c\u6c14\u5019\u6570\u636e\u6765\u9884\u6d4b\u91ce\u706b\u98ce\u9669\u56fe\uff0c\u5e76\u751f\u6210\u4e92\u8865\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u8de8\u5927\u9646\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u56e0\u679c\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u96be\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u6cdb\u5316\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6574\u5408\u89c6\u89c9\u3001\u6c14\u5019\u548c\u5730\u7406\u56e0\u7d20\u7684\u7efc\u5408\u6846\u67b6\u3002", "method": "\u63d0\u51faFireScope-Bench\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u7ed3\u5408Sentinel-2\u5f71\u50cf\u3001\u6c14\u5019\u6570\u636e\u548c\u4e13\u5bb6\u5b9a\u4e49\u7684\u98ce\u9669\u6805\u683c\uff1b\u5f00\u53d1FireScope\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u89c9\u76d1\u7763\u5b66\u4e60\u9884\u6d4b\u98ce\u9669\u6805\u683c\u5e76\u751f\u6210\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u7f8e\u56fd\u8bad\u7ec3\u5e76\u5728\u6b27\u6d32\u6d4b\u8bd5\u65f6\uff0cFireScope\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e13\u5bb6\u53cd\u9988\u548c\u81ea\u52a8\u5206\u6790\u786e\u8ba4\u5176\u63a8\u7406\u8f68\u8ff9\u5177\u6709\u5fe0\u5b9e\u6027\u548c\u8bed\u4e49\u610f\u4e49\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u53ef\u4ee5\u63d0\u5347\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0cFireScope-Bench\u6709\u671b\u6210\u4e3a\u63a8\u8fdb\u63a8\u7406\u9a71\u52a8\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7a7a\u95f4\u5efa\u6a21\u7684\u57fa\u7840\u3002"}}
{"id": "2511.17421", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17421", "abs": "https://arxiv.org/abs/2511.17421", "authors": ["Christopher Boland", "Sotirios Tsaftaris", "Sonia Dahdouh"], "title": "Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers", "comment": "Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:020", "summary": "Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u5728\u4efb\u52a1\u76f8\u5173\u6570\u636e\u5b50\u96c6\u4e0a\u5fae\u8c03\u7684\u6559\u5e08\u7f51\u7edc\u6765\u7f13\u89e3\u5728\u5927\u89c4\u6a21\u504f\u5dee\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5b66\u751f\u7f51\u7edc\u7684\u6377\u5f84\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u4e2d\u865a\u5047\u76f8\u5173\u7684\u6377\u5f84\u7279\u5f81\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4e0d\u4f7f\u7528\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\uff0c\u5f71\u54cd\u7a33\u5065\u6027\u5e76\u5bf9\u60a3\u8005\u9020\u6210\u5371\u5bb3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5728\u65e0\u504f\u6570\u636e\u5b50\u96c6\u4e0a\u5fae\u8c03\u7684\u6559\u5e08\u7f51\u7edc\u6765\u6307\u5bfc\u5728\u504f\u5dee\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5b66\u751f\u7f51\u7edc\uff0c\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u6377\u5f84\u7279\u5f81\u5728\u7f51\u7edc\u4e0d\u540c\u5c42\u7684\u8868\u73b0\u7279\u70b9\u8fdb\u884c\u7f13\u89e3\u3002", "result": "\u5728CheXpert\u3001ISIC 2017\u548cSimBA\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u591a\u79cd\u67b6\u6784\u8fdb\u884c\u5b9e\u9a8c\uff0c\u76f8\u6bd4\u4f20\u7edf\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u3001\u57fa\u4e8e\u589e\u5f3a\u548c\u57fa\u4e8e\u7ec4\u522b\u7684\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\uff0c\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u8fbe\u5230\u4e0e\u5728\u65e0\u504f\u6570\u636e\u4e0a\u8bad\u7ec3\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u533b\u5b66\u5f71\u50cf\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u504f\u5dee\u6807\u6ce8\u6709\u9650\u4e14\u6377\u5f84\u7279\u5f81\u96be\u4ee5\u5148\u9a8c\u8bc6\u522b\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2511.17481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17481", "abs": "https://arxiv.org/abs/2511.17481", "authors": ["Yiqing Shen", "Aiza Maksutova", "Chenjia Li", "Mathias Unberath"], "title": "Counterfactual World Models via Digital Twin-conditioned Video Diffusion", "comment": null, "summary": "World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as \"what would happen if this object was removed?\", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.", "AI": {"tldr": "CWMDT\u6846\u67b6\u5c06\u6807\u51c6\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u5316\u4e3a\u53cd\u4e8b\u5b9e\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u573a\u666f\u7684\u6570\u5b57\u5b6a\u751f\u8868\u793a\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u751f\u6210\u53cd\u4e8b\u5b9e\u89c6\u89c9\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u89c2\u5bdf\u7684\u524d\u5411\u6a21\u62df\uff0c\u65e0\u6cd5\u56de\u7b54\u53cd\u4e8b\u5b9e\u67e5\u8be2\uff08\u5982\"\u5982\u679c\u79fb\u9664\u8fd9\u4e2a\u5bf9\u8c61\u4f1a\u53d1\u751f\u4ec0\u4e48\"\uff09\uff0c\u8fd9\u5728\u8bc4\u4f30\u7269\u7406AI\u884c\u4e3a\u7b49\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "1) \u6784\u5efa\u573a\u666f\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u663e\u5f0f\u7f16\u7801\u5bf9\u8c61\u53ca\u5176\u5173\u7cfb\u4e3a\u7ed3\u6784\u5316\u6587\u672c\uff1b2) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53cd\u4e8b\u5b9e\u5e72\u9884\u5982\u4f55\u968f\u65f6\u95f4\u4f20\u64ad\uff1b3) \u7528\u4fee\u6539\u540e\u7684\u8868\u793a\u6761\u4ef6\u5316\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u89c6\u89c9\u5e8f\u5217\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCWMDT\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u660e\u6570\u5b57\u5b6a\u751f\u7b49\u66ff\u4ee3\u89c6\u9891\u8868\u793a\u4e3a\u57fa\u4e8e\u89c6\u9891\u524d\u5411\u6a21\u62df\u7684\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u63a7\u5236\u4fe1\u53f7\u3002", "conclusion": "CWMDT\u6846\u67b6\u6210\u529f\u5730\u5c06\u6807\u51c6\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u548c\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5b9e\u73b0\u4e86\u5bf9\u7279\u5b9a\u573a\u666f\u5c5e\u6027\u7684\u9488\u5bf9\u6027\u5e72\u9884\u3002"}}
{"id": "2511.17492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17492", "abs": "https://arxiv.org/abs/2511.17492", "authors": ["Weilun Li", "Lei Sun", "Ruixi Gao", "Qi Jiang", "Yuqin Ma", "Kaiwei Wang", "Ming-Hsuan Yang", "Luc Van Gool", "Danda Pani Paudel"], "title": "EvDiff: High Quality Video with an Event Camera", "comment": null, "summary": "As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.", "AI": {"tldr": "EvDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u6b65\u524d\u5411\u6269\u6563\u548c\u4ee3\u7406\u8bad\u7ec3\u6846\u67b6\uff0c\u4ece\u5355\u8272\u4e8b\u4ef6\u6d41\u751f\u6210\u9ad8\u8d28\u91cf\u5f69\u8272\u89c6\u9891\uff0c\u5728\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u8bb0\u5f55\u7a00\u758f\u4e8b\u4ef6\u6d41\uff0c\u4f46\u4ece\u4e2d\u91cd\u5efa\u5f3a\u5ea6\u56fe\u50cf\u662f\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u95ee\u9898\u3002\u73b0\u6709\u7aef\u5230\u7aef\u56de\u5f52\u65b9\u6cd5\u4ea7\u751f\u611f\u77e5\u8d28\u91cf\u8f83\u5dee\u7684\u7ed3\u679c\uff0c\u4e14\u96be\u4ee5\u6269\u5c55\u6a21\u578b\u5bb9\u91cf\u548c\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51faEvDiff\u4e8b\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u5355\u6b65\u524d\u5411\u6269\u6563\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u914d\u5907\u65f6\u95f4\u4e00\u81f4\u7684EvEncoder\u3002\u65b0\u9896\u7684\u4ee3\u7406\u8bad\u7ec3\u6846\u67b6\u6d88\u9664\u5bf9\u914d\u5bf9\u4e8b\u4ef6-\u56fe\u50cf\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u53ef\u5229\u7528\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u3002", "result": "\u80fd\u591f\u4ec5\u4ece\u5355\u8272\u4e8b\u4ef6\u6d41\u751f\u6210\u9ad8\u8d28\u91cf\u5f69\u8272\u89c6\u9891\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u50cf\u7d20\u7ea7\u548c\u611f\u77e5\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EvDiff\u5728\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u4e4b\u95f4\u627e\u5230\u4e86\u5e73\u8861\u70b9\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
