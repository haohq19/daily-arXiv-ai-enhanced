<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities](https://arxiv.org/abs/2507.16151)
*Yasser Ashraf,Ahmed Sharshar,Velibor Bojkovic,Bin Gu*

Main category: cs.CV

TL;DR: 本文介绍了首个基于脉冲相机的视频动作识别数据集，同时包含RGB和热成像模态，为脉冲神经网络提供综合基准测试平台，推动超低功耗视频理解研究。


<details>
  <summary>Details</summary>
Motivation: 脉冲相机作为生物启发的视觉传感器具有超高能效和卓越时间分辨率，但缺乏专门的视频动作识别数据集来支持脉冲神经网络的研究和多模态视频理解的探索。

Method: 构建了首个脉冲相机视频动作识别数据集，同时包含同步的RGB和热成像模态数据，保持脉冲数据固有的稀疏性和时间精度，为脉冲神经网络提供综合基准测试平台。

Result: 成功创建了三个数据集，提供了独特的多模态视频理解平台，能够直接比较脉冲、热成像和RGB三种模态的性能表现。

Conclusion: 该数据集将推动基于脉冲数据的节能、超低功耗视频理解研究，特别是在动作识别任务方面，为脉冲神经网络研究提供宝贵资源。

Abstract: Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by
accumulating light intensities at each pixel, offering ultra-high energy
efficiency and exceptional temporal resolution. Unlike event cameras, which
record changes in light intensity to capture motion, spike cameras provide even
finer spatiotemporal resolution and a more precise representation of continuous
changes. In this paper, we introduce the first video action recognition (VAR)
dataset using spike camera, alongside synchronized RGB and thermal modalities,
to enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By
preserving the inherent sparsity and temporal precision of spiking data, our
three datasets offer a unique platform for exploring multimodal video
understanding and serve as a valuable resource for directly comparing spiking,
thermal, and RGB modalities. This work contributes a novel dataset that will
drive research in energy-efficient, ultra-low-power video understanding,
specifically for action recognition tasks using spike-based data.

</details>


### [2] [Explicit Context Reasoning with Supervision for Visual Tracking](https://arxiv.org/abs/2507.16191)
*Fansheng Zeng,Bineng Zhong,Haiying Xia,Yufei Tan,Xiantao Hu,Liangtao Shi,Shuxiang Song*

Main category: cs.CV

TL;DR: RSTrack是一个视觉跟踪算法，通过三个核心机制（上下文推理、前向监督、高效状态建模）解决传统跟踪中上下文关联发散问题，在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪算法仅通过堆叠历史信息来关联上下文，缺乏对关联过程的显式监督，难以有效建模目标的演化动态，导致跨帧建模中时间一致性不足的问题。

Method: 提出RSTrack，包含三个核心机制：1）上下文推理机制：构建目标状态推理管道，将无约束的上下文关联转换为基于历史状态预测当前表示的时间推理过程；2）前向监督策略：利用真实目标特征作为锚点约束推理管道，指导预测输出趋向真实目标分布；3）高效状态建模：采用压缩-重构机制提取目标核心特征，去除冗余信息。

Result: 在多个基准数据集上实现了最先进的性能，同时保持实时运行速度，有效缓解了传统时间建模中上下文关联发散的问题。

Conclusion: RSTrack通过显式建模和监督上下文推理，成功解决了视觉跟踪中的时间一致性问题，三个机制的协同作用有效提升了跟踪性能，为视觉跟踪领域提供了新的解决方案。

Abstract: Contextual reasoning with constraints is crucial for enhancing temporal
consistency in cross-frame modeling for visual tracking. However, mainstream
tracking algorithms typically associate context by merely stacking historical
information without explicitly supervising the association process, making it
difficult to effectively model the target's evolving dynamics. To alleviate
this problem, we propose RSTrack, which explicitly models and supervises
context reasoning via three core mechanisms. \textit{1) Context Reasoning
Mechanism}: Constructs a target state reasoning pipeline, converting
unconstrained contextual associations into a temporal reasoning process that
predicts the current representation based on historical target states, thereby
enhancing temporal consistency. \textit{2) Forward Supervision Strategy}:
Utilizes true target features as anchors to constrain the reasoning pipeline,
guiding the predicted output toward the true target distribution and
suppressing drift in the context reasoning process. \textit{3) Efficient State
Modeling}: Employs a compression-reconstruction mechanism to extract the core
features of the target, removing redundant information across frames and
preventing ineffective contextual associations. These three mechanisms
collaborate to effectively alleviate the issue of contextual association
divergence in traditional temporal modeling. Experimental results show that
RSTrack achieves state-of-the-art performance on multiple benchmark datasets
while maintaining real-time running speeds. Our code is available at
https://github.com/GXNU-ZhongLab/RSTrack.

</details>


### [3] [MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing](https://arxiv.org/abs/2507.16228)
*Shreelekha Revankar,Utkarsh Mall,Cheng Perng Phoo,Kavita Bala,Bharath Hariharan*

Main category: cs.CV

TL;DR: 研究者构建了MONITRS数据集，包含超过10,000个FEMA灾害事件的时序卫星图像和自然语言标注，用于改进机器学习辅助的灾害监测系统性能


<details>
  <summary>Details</summary>
Motivation: 现有的灾害响应系统受限于难以进入受灾地区、计算机视觉方法专注于特定灾害类型、依赖人工专家解释，以及缺乏具有足够时间粒度和自然语言标注的数据集来跟踪灾害进展

Method: 构建MONITRS多模态数据集，包含超过10,000个FEMA灾害事件，结合时序卫星图像、新闻文章的自然语言标注、地理标记位置和问答对，并在此数据集上微调现有的多模态大语言模型(MLLMs)

Result: 在MONITRS数据集上微调的多模态大语言模型在灾害监测任务上取得了显著的性能提升，为机器学习辅助灾害响应系统建立了新的基准

Conclusion: MONITRS数据集成功解决了现有灾害监测系统的局限性，通过多模态数据融合和深度学习方法，为自动化灾害响应系统提供了有效的解决方案和新的性能基准

Abstract: Natural disasters cause devastating damage to communities and infrastructure
every year. Effective disaster response is hampered by the difficulty of
accessing affected areas during and after events. Remote sensing has allowed us
to monitor natural disasters in a remote way. More recently there have been
advances in computer vision and deep learning that help automate satellite
imagery analysis, However, they remain limited by their narrow focus on
specific disaster types, reliance on manual expert interpretation, and lack of
datasets with sufficient temporal granularity or natural language annotations
for tracking disaster progression. We present MONITRS, a novel multimodal
dataset of more than 10,000 FEMA disaster events with temporal satellite
imagery and natural language annotations from news articles, accompanied by
geotagged locations, and question-answer pairs. We demonstrate that fine-tuning
existing MLLMs on our dataset yields significant performance improvements for
disaster monitoring tasks, establishing a new benchmark for machine
learning-assisted disaster response systems. Code can be found at:
https://github.com/ShreelekhaR/MONITRS

</details>


### [4] [ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference](https://arxiv.org/abs/2507.16260)
*Haoyue Zhang,Jie Zhang,Song Guo*

Main category: cs.CV

TL;DR: 本文提出ToFe（Token Freezing and Reusing）框架，通过在不同阶段识别重要token并临时冻结不重要token，允许后续重用，在保持性能的同时降低视觉Transformer的计算成本。该方法在LV-ViT模型上实现了50%的计算成本降低，Top-1准确率仅下降不到2%。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉Transformer虽然在各种视觉任务中表现出色，但其计算开销巨大的自注意力机制阻碍了在资源受限设备上的部署。现有token缩减方法不可逆地丢弃不重要token，无法在后续块中重用，而考虑到Transformer在不同块中关注不同信息，早期被缩减的token可能在后续阶段有用。

Method: 提出Token Freezing and Reusing（ToFe）框架，包含：1）在每个阶段识别重要token并临时冻结不重要token；2）设计预测模块进行token识别；3）设计近似模块恢复冻结的token；4）通过计算预算感知的端到端训练与骨干网络联合优化，使模型能够自适应地在每个块处理必要的token。

Result: 在LV-ViT模型上实现了50%的计算成本降低，Top-1准确率仅下降不到2%。与现有最先进方法相比，在性能和复杂性之间实现了更好的权衡。

Conclusion: ToFe框架通过临时冻结和重用token的策略，有效解决了现有token缩减方法的局限性，在显著降低计算成本的同时保持了模型性能，为在资源受限设备上部署视觉Transformer提供了有效解决方案。

Abstract: Although vision transformers (ViT) have shown remarkable success in various
vision tasks, their computationally expensive self-attention hinder their
deployment on resource-constrained devices. Token reduction, which discards
less important tokens during forward propagation, has been proposed to enhance
the efficiency of transformer models. However, existing methods handle
unimportant tokens irreversibly, preventing their reuse in subsequent blocks.
Considering that transformers focus on different information among blocks,
tokens reduced in early blocks might be useful later. Furthermore, to adapt
transformer models for resource-constrained devices, it is crucial to strike a
balance between model performance and computational overhead. To address these
challenges, in this paper, we introduce a novel Token Freezing and Reusing
(ToFe) framework, where we identify important tokens at each stage and
temporarily freeze the unimportant ones, allowing their lagged reusing at a
later stage. Specifically, we design a prediction module for token
identification and an approximate module for recovery of the frozen tokens. By
jointly optimizing with the backbone through computation budget-aware
end-to-end training, ToFe can adaptively process the necessary tokens at each
block, thereby reducing computational cost while maintaining performance.
Extensive experiments demonstrate that ToFe reduces the computational cost of
LV-ViT model by 50% with less than 2% drop in Top-1 accuracy, achieving a
better trade-off between performance and complexity compared to
state-of-the-art methods.

</details>


### [5] [MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning in Vision Tasks](https://arxiv.org/abs/2507.16279)
*Junhao Su,Feiyu Zhu,Hengyu Shi,Tianyang Han,Yurui Qiu,Junfeng Luo,Xiaoming Wei,Jialin Gao*

Main category: cs.CV

TL;DR: 本文提出了Momentum Auxiliary Network++ (MAN++)方法，通过引入指数移动平均(EMA)机制和可学习缩放偏置来改进监督局部学习，在保持与端到端训练相当性能的同时显著降低GPU内存消耗。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端反向传播训练存在参数优化时的更新锁定、高GPU内存消耗和缺乏生物合理性等问题。虽然监督局部学习试图通过将网络分割为多个局部块来缓解这些问题，但由于梯度仅在单个局部块内传播，导致性能下降，无法替代端到端反向传播。

Method: 提出Momentum Auxiliary Network++ (MAN++)方法，通过使用相邻块参数的指数移动平均(EMA)引入动态交互机制，增强网络间的通信。辅助网络通过EMA更新，有效弥合块间信息差距。同时引入可学习缩放偏置来平衡局部块间的特征差异，进一步提升性能。

Result: 在图像分类、目标检测和图像分割任务上进行广泛实验验证，使用多种网络架构。实验结果表明MAN++在显著降低GPU内存使用的同时，实现了与端到端训练相当的性能。

Conclusion: MAN++为监督局部学习提供了新的视角，是传统训练方法的可行替代方案。该方法成功解决了局部学习中的性能下降问题，同时保持了内存效率的优势。

Abstract: Deep learning typically relies on end-to-end backpropagation for training, a
method that inherently suffers from issues such as update locking during
parameter optimization, high GPU memory consumption, and a lack of biological
plausibility. In contrast, supervised local learning seeks to mitigate these
challenges by partitioning the network into multiple local blocks and designing
independent auxiliary networks to update each block separately. However,
because gradients are propagated solely within individual local blocks,
performance degradation occurs, preventing supervised local learning from
supplanting end-to-end backpropagation. To address these limitations and
facilitate inter-block information flow, we propose the Momentum Auxiliary
Network++ (MAN++). MAN++ introduces a dynamic interaction mechanism by
employing the Exponential Moving Average (EMA) of parameters from adjacent
blocks to enhance communication across the network. The auxiliary network,
updated via EMA, effectively bridges the information gap between blocks.
Notably, we observed that directly applying EMA parameters can be suboptimal
due to feature discrepancies between local blocks. To resolve this issue, we
introduce a learnable scaling bias that balances feature differences, thereby
further improving performance. We validate MAN++ through extensive experiments
on tasks that include image classification, object detection, and image
segmentation, utilizing multiple network architectures. The experimental
results demonstrate that MAN++ achieves performance comparable to end-to-end
training while significantly reducing GPU memory usage. Consequently, MAN++
offers a novel perspective for supervised local learning and presents a viable
alternative to conventional training methods.

</details>


### [6] [ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement](https://arxiv.org/abs/2507.16397)
*Kahim Wong,Jicheng Zhou,Haiwei Wu,Yain-Whar Si,Jiantao Zhou*

Main category: cs.CV

TL;DR: 本文提出了ADCD-Net，一种鲁棒的文档图像篡改检测定位模型，通过自适应利用RGB/DCT取证痕迹和分层内容解耦来提升文档篡改检测性能，在5种失真类型上平均优于最先进方法20.79%。


<details>
  <summary>Details</summary>
Motivation: 现有的自然图像篡改检测器难以处理文档图像，因为篡改区域可以无缝融入统一的文档背景和结构化文本中；而现有的文档特定方法对各种退化缺乏足够的鲁棒性，限制了其实际部署应用。

Method: 提出ADCD-Net模型，包含三个关键技术：1)基于预测对齐分数自适应调节DCT特征贡献，解决DCT痕迹对块错位的敏感性；2)分层内容解耦方法，通过缓解文本-背景差异来提升定位性能；3)构建原始原型捕获未篡改区域的痕迹，增强定位准确性和鲁棒性。

Result: ADCD-Net在文档篡改定位任务上表现优异，在5种失真类型上平均优于最先进方法20.79%，展现了出色的定位性能和对各种失真的鲁棒性，包括调整大小和裁剪等。

Conclusion: ADCD-Net成功解决了文档图像篡改检测中的关键挑战，通过自适应DCT特征调节、分层内容解耦和原始原型构建，实现了对文档篡改的准确定位和鲁棒检测，为文档图像安全提供了有效的技术解决方案。

Abstract: The advancement of image editing tools has enabled malicious manipulation of
sensitive document images, underscoring the need for robust document image
forgery detection.Though forgery detectors for natural images have been
extensively studied, they struggle with document images, as the tampered
regions can be seamlessly blended into the uniform document background (BG) and
structured text. On the other hand, existing document-specific methods lack
sufficient robustness against various degradations, which limits their
practical deployment. This paper presents ADCD-Net, a robust document forgery
localization model that adaptively leverages the RGB/DCT forensic traces and
integrates key characteristics of document images. Specifically, to address the
DCT traces' sensitivity to block misalignment, we adaptively modulate the DCT
feature contribution based on a predicted alignment score, resulting in much
improved resilience to various distortions, including resizing and cropping.
Also, a hierarchical content disentanglement approach is proposed to boost the
localization performance via mitigating the text-BG disparities. Furthermore,
noticing the predominantly pristine nature of BG regions, we construct a
pristine prototype capturing traces of untampered regions, and eventually
enhance both the localization accuracy and robustness. Our proposed ADCD-Net
demonstrates superior forgery localization performance, consistently
outperforming state-of-the-art methods by 20.79\% averaged over 5 types of
distortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net.

</details>


### [7] [Sparse-View 3D Reconstruction: Recent Advances and Open Challenges](https://arxiv.org/abs/2507.16406)
*Tanveer Younis,Zhanglin Cheng*

Main category: cs.CV

TL;DR: 这篇论文是关于稀疏视角3D重建的综述，回顾了神经隐式模型、显式点云方法和混合框架的最新进展，分析了几何正则化、显式形状建模和生成推理如何缓解稀疏视角设置中的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人技术、AR/VR和自动驾驶系统等应用中，密集图像采集不切实际，最小图像重叠导致传统的运动恢复结构(SfM)和多视角立体视觉(MVS)方法失效，因此需要解决稀疏视角3D重建这一关键问题。

Method: 综述了三类主要方法：1)神经隐式模型(如NeRF及其正则化版本)；2)显式基于点云的方法(如3D高斯散射)；3)利用扩散模型和视觉基础模型先验的混合框架。分析了几何正则化、显式形状建模和生成推理技术。

Result: 在标准基准测试上的对比结果揭示了重建精度、效率和泛化能力之间的关键权衡。提供了几何方法、神经隐式方法和基于扩散的生成方法的统一视角，突出了领域泛化和无姿态重建的持续挑战。

Conclusion: 指出了该领域面临的持续挑战，包括领域泛化和无姿态重建问题，并概述了未来发展方向，包括开发3D原生生成先验和实现实时、无约束的稀疏视角重建。

Abstract: Sparse-view 3D reconstruction is essential for applications in which dense
image acquisition is impractical, such as robotics, augmented/virtual reality
(AR/VR), and autonomous systems. In these settings, minimal image overlap
prevents reliable correspondence matching, causing traditional methods, such as
structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey
reviews the latest advances in neural implicit models (e.g., NeRF and its
regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian
Splatting), and hybrid frameworks that leverage priors from diffusion and
vision foundation models (VFMs).We analyze how geometric regularization,
explicit shape modeling, and generative inference are used to mitigate
artifacts such as floaters and pose ambiguities in sparse-view settings.
Comparative results on standard benchmarks reveal key trade-offs between the
reconstruction accuracy, efficiency, and generalization. Unlike previous
reviews, our survey provides a unified perspective on geometry-based, neural
implicit, and generative (diffusion-based) methods. We highlight the persistent
challenges in domain generalization and pose-free reconstruction and outline
future directions for developing 3D-native generative priors and achieving
real-time, unconstrained sparse-view reconstruction.

</details>


### [8] [CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast Cardiac Risk Prediction Using Cine MRIs](https://arxiv.org/abs/2507.16612)
*Haoyang Su,Shaohao Rui,Jinyi Xiang,Lianming Wu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 提出了一种基于码本的时空学习框架(CTSL)，无需造影剂和分割掩码即可从Cine MRI序列中准确预测主要不良心脏事件(MACE)，通过自监督学习和多视图蒸馏策略实现了优于传统方法的心脏风险评估。


<details>
  <summary>Details</summary>
Motivation: 现有的MACE预测方法通常需要基于人工精细化心室心肌掩码的监督学习，在没有造影剂的情况下变得不实用。因此需要开发一种无需分割掩码、可直接从原始Cine数据学习的方法，实现准确且无造影剂的心脏风险评估。

Method: 提出了基于码本的时空学习(CTSL)自监督框架，通过多视图蒸馏策略解耦时间和空间特征，其中教师模型处理多个Cine视图，学生模型从降维的Cine-SA序列中学习。利用基于码本的特征表示和通过运动线索进行动态病变自检测来捕获复杂的时间依赖性和运动模式。

Result: CTSL模型实现了高置信度的MACE风险预测，性能优于传统的依赖造影剂的方法，为心脏风险评估提供了快速、无创的解决方案。

Conclusion: 该研究成功开发了无需造影剂和分割掩码的MACE预测方法，通过自监督学习框架实现了优异的预测性能，为临床环境中及时和便捷的心脏疾病诊断提供了新的技术路径。

Abstract: Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction
from Cine MRI sequences remains a critical challenge. Existing methods
typically necessitate supervised learning based on human-refined masks in the
ventricular myocardium, which become impractical without contrast agents. We
introduce a self-supervised framework, namely Codebook-based Temporal-Spatial
Learning (CTSL), that learns dynamic, spatiotemporal representations from raw
Cine data without requiring segmentation masks. CTSL decouples temporal and
spatial features through a multi-view distillation strategy, where the teacher
model processes multiple Cine views, and the student model learns from
reduced-dimensional Cine-SA sequences. By leveraging codebook-based feature
representations and dynamic lesion self-detection through motion cues, CTSL
captures intricate temporal dependencies and motion patterns. High-confidence
MACE risk predictions are achieved through our model, providing a rapid,
non-invasive solution for cardiac risk assessment that outperforms traditional
contrast-dependent methods, thereby enabling timely and accessible heart
disease diagnosis in clinical settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Bipartite Patient-Modality Graph Learning with Event-Conditional Modelling of Censoring for Cancer Survival Prediction](https://arxiv.org/abs/2507.16363)
*Hailin Yue,Hulin Kuang,Jin Liu,Junjian Li,Lanlan Wang,Mengshen He,Jianxin Wang*

Main category: cs.LG

TL;DR: 提出了CenSurv方法，通过双分图患者-模态学习和事件条件删失建模来预测癌症患者生存期，在模态缺失场景下表现出色，平均C-index比最佳基线提升3.1%


<details>
  <summary>Details</summary>
Motivation: 现有癌症生存预测研究仅关注已知生存风险样本之间的关系，未充分利用删失样本的价值；在模态缺失场景下性能下降，推理过程存在困难

Method: 设计双分图结构建模多模态数据获取表征；使用完整-不完整对齐策略探索模态无关特征；设计即插即用的事件条件删失建模(ECMC)模块，通过动态动量累积置信度选择可靠删失数据并分配更准确的生存时间

Result: 在5个公开癌症数据集上，CenSurv比最佳基线方法平均C-index提升3.1%；在各种模态缺失场景下表现出优异的鲁棒性；ECMC模块使8个基线方法在5个数据集上的平均C-index提升1.3%

Conclusion: CenSurv通过双分图学习和事件条件删失建模有效提升了癌症生存预测性能，特别是在模态缺失场景下的鲁棒性，ECMC模块具有良好的通用性和即插即用特性

Abstract: Accurately predicting the survival of cancer patients is crucial for
personalized treatment. However, existing studies focus solely on the
relationships between samples with known survival risks, without fully
leveraging the value of censored samples. Furthermore, these studies may suffer
performance degradation in modality-missing scenarios and even struggle during
the inference process. In this study, we propose a bipartite patient-modality
graph learning with event-conditional modelling of censoring for cancer
survival prediction (CenSurv). Specifically, we first use graph structure to
model multimodal data and obtain representation. Then, to alleviate performance
degradation in modality-missing scenarios, we design a bipartite graph to
simulate the patient-modality relationship in various modality-missing
scenarios and leverage a complete-incomplete alignment strategy to explore
modality-agnostic features. Finally, we design a plug-and-play
event-conditional modeling of censoring (ECMC) that selects reliable censored
data using dynamic momentum accumulation confidences, assigns more accurate
survival times to these censored data, and incorporates them as uncensored data
into training. Comprehensive evaluations on 5 publicly cancer datasets showcase
the superiority of CenSurv over the best state-of-the-art by 3.1% in terms of
the mean C-index, while also exhibiting excellent robustness under various
modality-missing scenarios. In addition, using the plug-and-play ECMC module,
the mean C-index of 8 baselines increased by 1.3% across 5 datasets. Code of
CenSurv is available at https://github.com/yuehailin/CenSurv.

</details>


### [10] [Improving Predictions on Highly Unbalanced Data Using Open Source Synthetic Data Upsampling](https://arxiv.org/abs/2507.16419)
*Ivona Krchova,Michael Platzer,Paul Tiwald*

Main category: cs.LG

TL;DR: 本研究针对不平衡表格数据集的预测建模挑战，评估了AI生成的合成数据在少数类上采样中的效果。通过对比合成数据与传统方法（如朴素过采样和SMOTE-NC），发现合成数据能够显著提升少数类的预测准确性，特别是在混合类型数据集中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多应用场景（如欺诈检测、医疗诊断、罕见事件预测）存在严重的数据不平衡问题，少数类样本极度缺乏，导致传统机器学习算法偏向多数类，难以准确表示少数类。需要通过合成数据生成技术来解决少数类代表性不足的问题。

Method: 使用MOSTLY AI的开源Synthetic Data SDK进行合成数据生成，该工具为混合类型数据提供灵活且用户友好的合成上采样方法。将使用合成数据上采样的数据集训练的预测模型与使用标准方法（朴素过采样和SMOTE-NC）的模型进行对比评估。

Result: 合成数据能够通过在特征空间的稀疏区域生成多样化数据点来填补空缺，从而改善少数群体的预测准确性。使用合成数据上采样的训练数据持续产生表现最佳的预测模型，特别是对于包含极少数少数类样本的混合类型数据集。

Conclusion: AI生成的合成数据是解决高度不平衡表格数据集问题的有效方法，相比传统的上采样技术，合成数据在提升少数类预测性能方面表现更优，为处理不平衡数据提供了新的解决方案。

Abstract: Unbalanced tabular data sets present significant challenges for predictive
modeling and data analysis across a wide range of applications. In many
real-world scenarios, such as fraud detection, medical diagnosis, and rare
event prediction, minority classes are vastly underrepresented, making it
difficult for traditional machine learning algorithms to achieve high accuracy.
These algorithms tend to favor the majority class, leading to biased models
that struggle to accurately represent minority classes. Synthetic data holds
promise for addressing the under-representation of minority classes by
providing new, diverse, and highly realistic samples. This paper presents a
benchmark study on the use of AI-generated synthetic data for upsampling highly
unbalanced tabular data sets.
  We evaluate the effectiveness of an open-source solution, the Synthetic Data
SDK by MOSTLY AI, which provides a flexible and user-friendly approach to
synthetic upsampling for mixed-type data. We compare predictive models trained
on data sets upsampled with synthetic records to those using standard methods,
such as naive oversampling and SMOTE-NC. Our results demonstrate that synthetic
data can improve predictive accuracy for minority groups by generating diverse
data points that fill gaps in sparse regions of the feature space. We show that
upsampled synthetic training data consistently results in top-performing
predictive models, particularly for mixed-type data sets containing very few
minority samples.

</details>


### [11] [Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs](https://arxiv.org/abs/2507.16672)
*Yushang Zhao,Huijie Shen,Dannier Li,Lu Chang,Chengrui Zhou,Yinuo Yang*

Main category: cs.LG

TL;DR: 本文提出了一个基于元学习的框架，通过参数高效的提示调优来快速个性化LLM推荐系统，特别解决冷启动用户问题，实现了275毫秒的实时适应能力，可应用于金融风险分析等场景。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的推荐系统在冷启动用户场景下表现不佳，当前的监督微调和协同过滤方法成本高昂且难以维护更新，需要一种能够快速适应新用户的个性化推荐方法。

Method: 提出元学习框架，使用一阶优化（Reptile）和二阶优化（MAML）来学习软提示嵌入，将每个用户视为独立任务。通过情节采样、内循环适应和外循环泛化进行元优化，学习可微分的控制变量来表示用户行为先验。

Result: 在MovieLens-1M、Amazon Reviews和Recbole数据集上，该自适应模型在NDCG@10、HR@10和MRR指标上优于强基线，在消费级GPU上实现实时运行（低于300毫秒）。支持零历史个性化，275毫秒的适应速度可用于金融系统实时风险分析。

Conclusion: 该框架成功解决了LLM推荐系统的冷启动问题，实现了快速个性化和实时适应能力。275毫秒的适应速度使其能够应用于金融风险分析，缩短检测延迟，提高支付网络稳定性，增强国家金融基础设施韧性。

Abstract: Generative, explainable, and flexible recommender systems, derived using
Large Language Models (LLM) are promising and poorly adapted to the cold-start
user situation, where there is little to no history of interaction. The current
solutions i.e. supervised fine-tuning and collaborative filtering are
dense-user-item focused and would be expensive to maintain and update. This
paper introduces a meta-learning framework, that can be used to perform
parameter-efficient prompt-tuning, to effectively personalize LLM-based
recommender systems quickly at cold-start. The model learns soft prompt
embeddings with first-order (Reptile) and second-order (MAML) optimization by
treating each of the users as the tasks. As augmentations to the input tokens,
these learnable vectors are the differentiable control variables that represent
user behavioral priors. The prompts are meta-optimized through episodic
sampling, inner-loop adaptation, and outer-loop generalization. On
MovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model
outperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in
real-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization
is also supported by this scalable solution, and its 275 ms rate of adaptation
allows successful real-time risk profiling of financial systems by shortening
detection latency and improving payment network stability. Crucially, the 275
ms adaptation capability can enable real-time risk profiling for financial
institutions, reducing systemic vulnerability detection latency significantly
versus traditional compliance checks. By preventing contagion in payment
networks (e.g., Fedwire), the framework strengthens national financial
infrastructure resilience.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Why Braking? Scenario Extraction and Reasoning Utilizing LLM](https://arxiv.org/abs/2507.15874)
*Yin Wu,Daniel Slieter,Vivek Subramanian,Ahmed Abouelazm,Robin Bohn,J. Marius Zöllner*

Main category: cs.AI

TL;DR: 本文提出了一个基于大语言模型的驾驶场景理解框架，用于识别和分析安全关键的驾驶场景，特别是制动事件，通过双路径检索方法处理已知和未知场景，在Argoverse 2数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着ADAS车辆数量增长，驾驶数据急剧增加，但大多数数据捕获的是常规驾驶行为。在海量数据中识别和理解安全关键的极端情况仍然是重大挑战。现有基于规则的启发式方法在复杂城市环境中缺乏泛化能力，因此需要新的方法来理解"车辆为什么制动"这一关键问题。

Method: 提出了一个利用大语言模型进行场景理解和推理的新框架。该方法将低级数值信号与自然语言描述相结合，使LLM能够解释和分类驾驶场景。设计了双路径场景检索机制：支持基于类别的已知场景搜索和基于嵌入的未知分布外(OOD)场景检索。

Result: 在Argoverse 2传感器数据集上进行实验验证，构建了场景标注数据。实验结果表明，该方法优于基于规则的基线方法，并且在OOD场景上具有良好的泛化能力。

Conclusion: 基于大语言模型的驾驶场景理解框架能够有效识别和分析安全关键场景，特别是制动事件。双路径检索机制成功解决了已知和未知场景的处理问题，为自动驾驶系统的安全性评估提供了新的解决方案。

Abstract: The growing number of ADAS-equipped vehicles has led to a dramatic increase
in driving data, yet most of them capture routine driving behavior. Identifying
and understanding safety-critical corner cases within this vast dataset remains
a significant challenge. Braking events are particularly indicative of
potentially hazardous situations, motivating the central question of our
research: Why does a vehicle brake? Existing approaches primarily rely on
rule-based heuristics to retrieve target scenarios using predefined condition
filters. While effective in simple environments such as highways, these methods
lack generalization in complex urban settings. In this paper, we propose a
novel framework that leverages Large Language Model (LLM) for scenario
understanding and reasoning. Our method bridges the gap between low-level
numerical signals and natural language descriptions, enabling LLM to interpret
and classify driving scenarios. We propose a dual-path scenario retrieval that
supports both category-based search for known scenarios and embedding-based
retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate
evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.
Experimental results show that our method outperforms rule-based baselines and
generalizes well to OOD scenarios.

</details>


### [13] [Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery](https://arxiv.org/abs/2507.16229)
*Bo Wen,Chen Wang,Qiwei Han,Raquel Norel,Julia Liu,Thaddeus Stappenbeck,Jeffrey L. Rogers*

Main category: cs.AI

TL;DR: 本研究探讨了基于语音的AI代理在医疗保健中的应用，通过开发Agent PULSE系统并进行试点研究，证明了AI语音助手在预防护理和持续患者监测方面的潜力，特别是在服务不足的人群中具有成本效益和可及性优势。


<details>
  <summary>Details</summary>
Motivation: 医疗保健中存在经济和可及性差距，传统的数字健康服务难以覆盖服务不足的人群。需要探索基于大语言模型的语音AI代理如何在人工干预经济上不可行的情况下提供成本效益的医疗服务，特别是在预防护理和持续患者监测方面。

Method: 开发了Agent PULSE（患者理解和联络支持引擎）系统，这是IBM研究院、克利夫兰诊所基金会和莫尔豪斯医学院的合作项目。对33名炎症性肠病患者进行试点研究，分析患者对AI驱动监测的接受度。建立经济模型分析成本效益，并评估技术挑战、政策考虑和伦理框架。

Result: 试点研究显示70%的患者接受AI驱动的监测，37%更偏好AI监测而非传统方式。成本效用分析显示在常规监测任务中具有巨大的潜在节约。技术方面识别了实时对话AI处理、医疗系统集成和隐私合规等挑战。AI语音代理提高了医疗保健的可扩展性、效率、患者参与度和可及性。

Conclusion: 基于语音的AI代理可以作为公平、可持续数字医疗解决方案的关键入口点。通过解决当前限制并使AI开发与伦理和监管框架保持一致，这些系统不仅能够弥合医疗保健中的经济和可及性差距，还能显著提高预防护理和患者监测的效率和质量。

Abstract: The integration of voice-based AI agents in healthcare presents a
transformative opportunity to bridge economic and accessibility gaps in digital
health delivery. This paper explores the role of large language model
(LLM)-powered voice assistants in enhancing preventive care and continuous
patient monitoring, particularly in underserved populations. Drawing insights
from the development and pilot study of Agent PULSE (Patient Understanding and
Liaison Support Engine) -- a collaborative initiative between IBM Research,
Cleveland Clinic Foundation, and Morehouse School of Medicine -- we present an
economic model demonstrating how AI agents can provide cost-effective
healthcare services where human intervention is economically unfeasible. Our
pilot study with 33 inflammatory bowel disease patients revealed that 70\%
expressed acceptance of AI-driven monitoring, with 37\% preferring it over
traditional modalities. Technical challenges, including real-time
conversational AI processing, integration with healthcare systems, and privacy
compliance, are analyzed alongside policy considerations surrounding
regulation, bias mitigation, and patient autonomy. Our findings suggest that
AI-driven voice agents not only enhance healthcare scalability and efficiency
but also improve patient engagement and accessibility. For healthcare
executives, our cost-utility analysis demonstrates huge potential savings for
routine monitoring tasks, while technologists can leverage our framework to
prioritize improvements yielding the highest patient impact. By addressing
current limitations and aligning AI development with ethical and regulatory
frameworks, voice-based AI agents can serve as a critical entry point for
equitable, sustainable digital healthcare solutions.

</details>


### [14] [Novel Multi-Agent Action Masked Deep Reinforcement Learning for General Industrial Assembly Lines Balancing Problems](https://arxiv.org/abs/2507.16635)
*Ali Mohamed Ali,Luca Tirel,Hashim A. Hashim*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度强化学习的工业装配线优化方案，通过马尔可夫决策过程建模和多智能体方法，实现了任务和资源调度的高效优化，相比传统方法显著提升了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统的整数规划方法在大规模场景下计算成本过高，而遗传算法等启发式方法往往产生次优解。现有的装配线模型大多对装配线类型有特定假设，缺乏通用性。因此需要一种高效、通用的工业装配线优化方法。

Method: 提出了一种通用的工业装配线数学模型，将其表述为马尔可夫决策过程(MDP)，不对装配线类型施加假设。采用深度强化学习训练智能体进行任务和资源调度优化。引入两种创新工具：1)动作掩码技术确保智能体只选择可行动作；2)多智能体方法，每个工作站由独立智能体管理，减少状态和动作空间。采用集中训练分散执行的框架。

Result: 通过数值仿真验证了所提方案的有效性，与可比较的基于模型的方法相比，在收敛到最优解方面表现出显著更快的速度。该框架能够让智能体离线学习并在运行期间通过神经网络提供实时解决方案。

Conclusion: 本文成功建立了一个可扩展的学习架构来优化工业装配线，所提出的深度强化学习方法结合动作掩码和多智能体技术，在保证解决方案质量的同时显著提高了计算效率，为工业装配线的智能化管理提供了有效途径。

Abstract: Efficient planning of activities is essential for modern industrial assembly
lines to uphold manufacturing standards, prevent project constraint violations,
and achieve cost-effective operations. While exact solutions to such challenges
can be obtained through Integer Programming (IP), the dependence of the search
space on input parameters often makes IP computationally infeasible for
large-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also
be applied, but they frequently produce suboptimal solutions in extensive
cases. This paper introduces a novel mathematical model of a generic industrial
assembly line formulated as a Markov Decision Process (MDP), without imposing
assumptions on the type of assembly line a notable distinction from most
existing models. The proposed model is employed to create a virtual environment
for training Deep Reinforcement Learning (DRL) agents to optimize task and
resource scheduling. To enhance the efficiency of agent training, the paper
proposes two innovative tools. The first is an action-masking technique, which
ensures the agent selects only feasible actions, thereby reducing training
time. The second is a multi-agent approach, where each workstation is managed
by an individual agent, as a result, the state and action spaces were reduced.
A centralized training framework with decentralized execution is adopted,
offering a scalable learning architecture for optimizing industrial assembly
lines. This framework allows the agents to learn offline and subsequently
provide real-time solutions during operations by leveraging a neural network
that maps the current factory state to the optimal action. The effectiveness of
the proposed scheme is validated through numerical simulations, demonstrating
significantly faster convergence to the optimal solution compared to a
comparable model-based approach.

</details>
