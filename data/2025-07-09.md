<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 论文提出了一种结合RGB和事件传感器的融合方法，用于航天器姿态估计，解决了单一传感器在极端光照条件下的局限性。


<details>
  <summary>Details</summary>
Motivation: 航天器姿态估计对自主空间操作至关重要，但传统RGB传感器在极端光照条件下表现不佳，而事件传感器虽动态范围高，但空间分辨率低且信号噪声比差。

Method: 采用光束分束棱镜实现光学和时间对齐，开发了基于RANSAC的融合技术，结合RGB和事件传感器的优势，并利用dropout不确定性估计检测极端条件。

Result: 在实验室多种挑战性光照条件下收集的数据集上验证了方法的有效性，证明了事件-RGB融合的优越性。

Conclusion: 事件-RGB融合方法有效提升了航天器姿态估计的鲁棒性，支持事件传感器在该领域的应用，并公开数据集以促进社区研究。

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [2] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种集成结构提示（ISP）方法，通过自结构和跨结构提示模块增强视觉语言模型（VLM）中文本和图像分支的信息交互，同时通过样本探测模块动态调整损失系数，提升新类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法忽视了可学习提示与模态内外标记的结构关系，且难以平衡基类和新类的性能。

Method: 引入自结构和跨结构提示模块建模模态内外关系，并提出样本探测模块动态调整损失系数。

Result: 在基类到新类泛化、跨数据集评估和领域泛化三个任务中，ISP表现优于现有方法。

Conclusion: ISP通过结构关系和动态损失调整，显著提升了VLM的泛化能力和性能。

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [3] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: 提出了一种结合视觉基础模型（VFM）和大语言模型（LLM）的新框架，通过高级融合模块实现视频理解中的因果推理和未来预测。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型缺乏常识性世界知识，难以完成高级认知任务。

Method: 采用两阶段训练策略：先在大规模视频-文本数据上进行对齐预训练，再通过指令微调提升推理和预测能力。

Result: 模型在多个挑战性基准测试中表现优异，并展现出零样本泛化能力。

Conclusion: 该研究推动了机器感知从简单识别向认知理解的进步，为更智能的AI系统铺平了道路。

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 该论文探讨了使用大型语言模型（如GPT-4o-mini）生成反事实解释（CFs）的方法，并在零样本和三样本设置下验证其效果。与传统方法相比，该方法在合理性和有效性上表现优异，并能通过数据增强提升下游分类器性能。


<details>
  <summary>Details</summary>
Motivation: 反事实解释（CFs）能为机器学习预测提供直观的干预建议和数据增强手段，但传统方法在生成CFs时存在局限性。本文旨在探索大型语言模型在生成高质量CFs方面的潜力。

Method: 使用GPT-4o-mini在零样本和三样本设置下生成CFs，并在两个数据集（AI-Readi压力预测数据集和心脏病检测公开数据集）上进行评估。

Result: 该方法在合理性（高达99%）和有效性（高达0.99）上优于传统方法（如DiCE、CFNOW和NICE），且通过数据增强使下游分类器准确率平均提升5%。

Conclusion: 基于提示的生成技术能显著提升临床和生理预测任务的可解释性和鲁棒性，展示了大型语言模型在生成CFs中的潜力。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [5] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: Re5是一个自评估和修订框架，旨在提升大型语言模型（LLM）遵循指令的能力，同时保持生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂指令遵循和成本效率上存在不足，Re5旨在解决这些问题。

Method: Re5通过提取任务和约束组件、结构评估和选择性修订，实现精确改进。

Result: 实验显示Re5在少量数据下表现接近GPT-4o-mini，且64.24%优于未修订响应。

Conclusion: Re5是一种高效且有效的方法，能以最小外部监督提升指令遵循能力。

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [6] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: 提出了一种名为DAEDCMD的新方法，用于解决持续多模态虚假信息检测中的知识遗忘和未来泛化问题。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在社交媒体上广泛传播，现有基于离线数据的方法无法适应新事件，导致检测效果不佳。

Method: 通过基于Dirichlet过程的混合专家结构隔离事件特定参数，并学习连续时间动态模型以预测未来环境分布。

Result: DAEDCMD在实验中显著优于六种MMD基线和三种持续学习方法。

Conclusion: DAEDCMD有效解决了持续多模态虚假信息检测中的挑战，提升了检测性能。

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>
