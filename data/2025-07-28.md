<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems](https://arxiv.org/abs/2507.18656)
*Muhammad Zaeem Shahzad,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: 论文提出了一种名为ShrinkBox的新型后门攻击方法，针对基于机器学习的ADAS中的物体检测系统，通过缩小边界框来破坏距离估计功能。


<details>
  <summary>Details</summary>
Motivation: 传统ADAS依赖昂贵传感器，而基于机器学习的ADAS（ML-ADAS）提供了一种低成本替代方案，但其物体检测系统存在安全漏洞。

Method: 提出ShrinkBox攻击方法，通过缩小边界框而非改变类别标签或物体存在性，实现对YOLOv9m等物体检测器的攻击。

Result: 在KITTI数据集上，ShrinkBox攻击成功率达到96%，仅需4%的投毒比例，且显著增加下游距离估计的误差。

Conclusion: ShrinkBox攻击对ML-ADAS的碰撞避免功能构成严重威胁，突显了物体检测系统安全性的重要性。

Abstract: Advanced Driver Assistance Systems (ADAS) significantly enhance road safety
by detecting potential collisions and alerting drivers. However, their reliance
on expensive sensor technologies such as LiDAR and radar limits accessibility,
particularly in low- and middle-income countries. Machine learning-based ADAS
(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera
input, offers a cost-effective alternative. Critical to ML-ADAS is the
collision avoidance feature, which requires the ability to detect objects and
estimate their distances accurately. This is achieved with specialized DNNs
like YOLO, which provides real-time object detection, and a lightweight,
detection-wise distance estimation approach that relies on key features
extracted from the detections like bounding box dimensions and size. However,
the robustness of these systems is undermined by security vulnerabilities in
object detectors. In this paper, we introduce ShrinkBox, a novel backdoor
attack targeting object detection in collision avoidance ML-ADAS. Unlike
existing attacks that manipulate object class labels or presence, ShrinkBox
subtly shrinks ground truth bounding boxes. This attack remains undetected in
dataset inspections and standard benchmarks while severely disrupting
downstream distance estimation. We demonstrate that ShrinkBox can be realized
in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with
only a 4% poisoning ratio in the training instances of the KITTI dataset.
Furthermore, given the low error targets introduced in our relaxed poisoning
strategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in
downstream distance estimation by more than 3x on poisoned samples, potentially
resulting in delays or prevention of collision warnings altogether.

</details>


### [2] [SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time](https://arxiv.org/abs/2507.18713)
*Yun Chen,Matthew Haines,Jingkang Wang,Krzysztof Baron-Lis,Sivabalan Manivasagam,Ze Yang,Raquel Urtasun*

Main category: cs.CV

TL;DR: SaLF是一种新型体积表示方法，支持光栅化和光线追踪，用于高效模拟多传感器自动驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有NeRF和3DGS方法在训练速度、渲染效率和传感器兼容性上的不足。

Method: 使用稀疏3D体素基元表示体积，每个体素为局部隐式场，支持自适应修剪和密集化。

Result: 快速训练（<30分钟）和高效渲染（相机50+ FPS，LiDAR 600+ FPS），支持非针孔相机和旋转LiDAR。

Conclusion: SaLF在保持真实感的同时，提高了效率和功能，适用于更广泛的自动驾驶模拟场景。

Abstract: High-fidelity sensor simulation of light-based sensors such as cameras and
LiDARs is critical for safe and accurate autonomy testing. Neural radiance
field (NeRF)-based methods that reconstruct sensor observations via ray-casting
of implicit representations have demonstrated accurate simulation of driving
scenes, but are slow to train and render, hampering scale. 3D Gaussian
Splatting (3DGS) has demonstrated faster training and rendering times through
rasterization, but is primarily restricted to pinhole camera sensors,
preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both
NeRF and 3DGS couple the representation with the rendering procedure (implicit
networks for ray-based evaluation, particles for rasterization), preventing
interoperability, which is key for general usage. In this work, we present
Sparse Local Fields (SaLF), a novel volumetric representation that supports
rasterization and raytracing. SaLF represents volumes as a sparse set of 3D
voxel primitives, where each voxel is a local implicit field. SaLF has fast
training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS
LiDAR), has adaptive pruning and densification to easily handle large scenes,
and can support non-pinhole cameras and spinning LiDARs. We demonstrate that
SaLF has similar realism as existing self-driving sensor simulation methods
while improving efficiency and enhancing capabilities, enabling more scalable
simulation. https://waabi.ai/salf/

</details>


### [3] [Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization](https://arxiv.org/abs/2507.19459)
*Pol Francesch Huc,Emily Bates,Simone D'Amico*

Main category: cs.CV

TL;DR: 提出了一种基于CNN的3D高斯泼溅（3DGS）初始化方法，支持单目图像输入，减少训练成本和所需图像数量，适用于空间应用。


<details>
  <summary>Details</summary>
Motivation: 解决NeRF和3DGS在空间应用中需要精确姿态和高计算成本的问题。

Method: 使用CNN从单目图像生成粗略3D模型和姿态估计，用于初始化3DGS，并分析不同姿态估计变体的效果。

Result: 即使姿态估计不完美，也能学习高保真3D表示，显著减少训练迭代和输入图像需求。

Conclusion: 该方法为空间应用中的新颖视图合成提供了可行解决方案。

Abstract: The advent of novel view synthesis techniques such as NeRF and 3D Gaussian
Splatting (3DGS) has enabled learning precise 3D models only from posed
monocular images. Although these methods are attractive, they hold two major
limitations that prevent their use in space applications: they require poses
during training, and have high computational cost at training and inference. To
address these limitations, this work contributes: (1) a Convolutional Neural
Network (CNN) based primitive initializer for 3DGS using monocular images; (2)
a pipeline capable of training with noisy or implicit pose estimates; and (3)
and analysis of initialization variants that reduce the training cost of
precise 3D models. A CNN takes a single image as input and outputs a coarse 3D
model represented as an assembly of primitives, along with the target's pose
relative to the camera. This assembly of primitives is then used to initialize
3DGS, significantly reducing the number of training iterations and input images
needed -- by at least an order of magnitude. For additional flexibility, the
CNN component has multiple variants with different pose estimation techniques.
This work performs a comparison between these variants, evaluating their
effectiveness for downstream 3DGS training under noisy or implicit pose
estimates. The results demonstrate that even with imperfect pose supervision,
the pipeline is able to learn high-fidelity 3D representations, opening the
door for the use of novel view synthesis in space applications.

</details>


### [4] [Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform](https://arxiv.org/abs/2507.18870)
*Keke Tang,Yuze Gao,Weilong Peng,Xiaofei Wang,Meie Fang,Peican Zhu*

Main category: cs.CV

TL;DR: MAT-Adv是一种新的对抗攻击框架，通过扰动点云的中轴变换（MAT）表示，增强对抗样本的迁移性和不可防御性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在白盒设置下表现良好，但在迁移性和对抗防御机制方面存在不足。

Method: 使用自编码器将点云投影为紧凑的MAT表示，并通过扰动这些表示引入结构级对抗特性，同时采用dropout策略优化扰动。

Result: 实验表明，MAT-Adv在迁移性和不可防御性上显著优于现有方法。

Conclusion: MAT-Adv通过扰动MAT表示，有效提升了对抗攻击的迁移性和不可防御性。

Abstract: Studying adversarial attacks on point clouds is essential for evaluating and
improving the robustness of 3D deep learning models. However, most existing
attack methods are developed under ideal white-box settings and often suffer
from limited transferability to unseen models and insufficient robustness
against common defense mechanisms. In this paper, we propose MAT-Adv, a novel
adversarial attack framework that enhances both transferability and
undefendability by explicitly perturbing the medial axis transform (MAT)
representations, in order to induce inherent adversarialness in the resulting
point clouds. Specifically, we employ an autoencoder to project input point
clouds into compact MAT representations that capture the intrinsic geometric
structure of point clouds. By perturbing these intrinsic representations,
MAT-Adv introduces structural-level adversarial characteristics that remain
effective across diverse models and defense strategies. To mitigate overfitting
and prevent perturbation collapse, we incorporate a dropout strategy into the
optimization of MAT perturbations, further improving transferability and
undefendability. Extensive experiments demonstrate that MAT-Adv significantly
outperforms existing state-of-the-art methods in both transferability and
undefendability. Codes will be made public upon paper acceptance.

</details>


### [5] [Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes](https://arxiv.org/abs/2507.19304)
*Muhammad Ibrahim,Naveed Akhtar,Haitian Wang,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种多流检测网络（MuStD），通过融合LiDAR和RGB数据提升户外3D物体检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决户外3D物体检测中LiDAR和RGB数据融合的挑战，提升检测精度。

Method: 采用三流结构：LiDAR-PillarNet提取稀疏2D支柱特征，LiDAR-Height Compression计算鸟瞰图特征，3D Multimodal流通过UV映射和极坐标索引融合RGB与LiDAR特征。

Result: 在KITTI物体检测基准测试中取得最新或极具竞争力的结果，同时保持高效性。

Conclusion: MuStD网络通过多模态特征融合，显著提升了户外3D物体检测的性能。

Abstract: Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object
detection accuracy. To address real-world challenges in outdoor 3D object
detection, fusion of LiDAR and RGB input has started gaining traction. However,
effective integration of these modalities for precise object detection task
still remains a largely open problem. To address that, we propose a MultiStream
Detection (MuStD) network, that meticulously extracts task-relevant information
from both data modalities. The network follows a three-stream structure. Its
LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input
while the LiDAR-Height Compression stream computes Bird's-Eye View features. An
additional 3D Multimodal stream combines RGB and LiDAR features using UV
mapping and polar coordinate indexing. Eventually, the features containing
comprehensive spatial, textural and geometric information are carefully fused
and fed to a detection head for 3D object detection. Our extensive evaluation
on the challenging KITTI Object Detection Benchmark using public testing server
at
https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0
establishes the efficacy of our method by achieving new state-of-the-art or
highly competitive results in different categories while remaining among the
most efficient methods. Our code will be released through MuStD GitHub
repository at https://github.com/IbrahimUWA/MuStD.git

</details>


### [6] [Multi-Task Dense Prediction Fine-Tuning with Mixture of Fine-Grained Experts](https://arxiv.org/abs/2507.19077)
*Yangyang Xu,Xi Ye,Duo Su*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的细粒度混合专家（FGMoE）架构，通过三项关键创新和微调方法，解决了多任务学习中共享表示与任务特定专业化之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多任务学习（MTL）在密集预测中表现良好，但仍面临共享表示与任务特定专业化之间的平衡挑战。

Method: 提出FGMoE架构，包括：1）任务内专家，沿MLP中间隐藏维度划分；2）共享专家，整合任务内共同信息；3）全局专家，促进跨任务知识转移。此外，采用微调方法仅训练解码器参数以提高效率。

Result: 实验表明，FGMoE在参数更少的情况下，显著优于现有基于MoE的MTL模型，在NYUD-v2和PASCAL-Context数据集上表现优异。

Conclusion: FGMoE通过细粒度任务分解和高效知识共享，为多任务学习提供了更优的解决方案。

Abstract: Multi-task learning (MTL) for dense prediction has shown promising results
but still faces challenges in balancing shared representations with
task-specific specialization. In this paper, we introduce a novel Fine-Grained
Mixture of Experts (FGMoE) architecture that explores MoE-based MTL models
through a combination of three key innovations and fine-tuning. First, we
propose intra-task experts that partition along intermediate hidden dimensions
of MLPs, enabling finer decomposition of task information while maintaining
parameter efficiency. Second, we introduce shared experts that consolidate
common information across different contexts of the same task, reducing
redundancy, and allowing routing experts to focus on unique aspects. Third, we
design a global expert that facilitates adaptive knowledge transfer across
tasks based on both input feature and task requirements, promoting beneficial
information sharing while preventing harmful interference. In addition, we use
the fine-tuning approach to improve parameter efficiency only by training the
parameters of the decoder. Extensive experimental results show that the
proposed FGMoE uses fewer parameters and significantly outperforms current
MoE-based competitive MTL models on two dense prediction datasets
(\textit{i.e.,} NYUD-v2, PASCAL-Context) in various metrics.

</details>


### [7] [Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation](https://arxiv.org/abs/2507.19140)
*Tianyu Zou,Shengwu Xiong,Ruilin Yao,Yi Rong*

Main category: cs.CV

TL;DR: 本文提出了一种原型-亲和混合网络（PAHNet），通过平衡原型学习和亲和学习两种方法的保守与激进特性，提升少样本分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本分割方法中，原型学习预测保守，亲和学习预测激进，作者希望平衡两者以提升分割效果。

Method: PAHNet结合原型预测器和亲和学习器，引入原型引导特征增强（PFE）模块和注意力分数校准（ASC）模块，优化特征表示。

Result: 实验表明，PAHNet在PASCAL-5$^i$和COCO-20$^i$数据集上优于现有方法。

Conclusion: PAHNet通过平衡保守与激进信息，有效提升了少样本分割的准确性。

Abstract: This paper studies the few-shot segmentation (FSS) task, which aims to
segment objects belonging to unseen categories in a query image by learning a
model on a small number of well-annotated support samples. Our analysis of two
mainstream FSS paradigms reveals that the predictions made by prototype
learning methods are usually conservative, while those of affinity learning
methods tend to be more aggressive. This observation motivates us to balance
the conservative and aggressive information captured by these two types of FSS
frameworks so as to improve the segmentation performance. To achieve this, we
propose a **P**rototype-**A**ffinity **H**ybrid **Net**work (PAHNet), which
introduces a Prototype-guided Feature Enhancement (PFE) module and an Attention
Score Calibration (ASC) module in each attention block of an affinity learning
model (called affinity learner). These two modules utilize the predictions
generated by a pre-trained prototype learning model (called prototype
predictor) to enhance the foreground information in support and query image
representations and suppress the mismatched foreground-background (FG-BG)
relationships between them, respectively. In this way, the aggressiveness of
the affinity learner can be effectively mitigated, thereby eventually
increasing the segmentation accuracy of our PAHNet method. Experimental results
show that PAHNet outperforms most recently proposed methods across 1-shot and
5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its
effectiveness. The code is available at: [GitHub - tianyu-zou/PAHNet: Balancing
Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot
Segmentation (ICCV'25)](https://github.com/tianyu-zou/PAHNet)

</details>


### [8] [Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene](https://arxiv.org/abs/2507.19232)
*Donggeun Lim,Jinseok Bae,Inwoo Hwang,Seungmin Lee,Hwanhee Lee,Young Min Kim*

Main category: cs.CV

TL;DR: 提出一个框架，通过大型语言模型（LLM）生成多人类动态场景的上下文运动，解决人类-人类和人类-场景交互的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 生成多人类上下文运动需要对动态关系进行整体推理，现有方法难以处理大规模和多样化的场景。

Method: 利用LLM解析文本输入，将任务分解为子问题；事件生成器将动态场景分解为小事件序列，合成角色运动；高层模块提供可扩展的上下文描述。

Result: 框架在基准测试和用户研究中表现出色，能高效捕捉场景上下文并具备高扩展性。

Conclusion: 该框架首次在大规模和多样性上解决了多人类动态场景生成问题，并提供了评估基准。

Abstract: In this work, we propose a framework that creates a lively virtual dynamic
scene with contextual motions of multiple humans. Generating multi-human
contextual motion requires holistic reasoning over dynamic relationships among
human-human and human-scene interactions. We adapt the power of a large
language model (LLM) to digest the contextual complexity within textual input
and convert the task into tangible subproblems such that we can generate
multi-agent behavior beyond the scale that was not considered before.
Specifically, our event generator formulates the temporal progression of a
dynamic scene into a sequence of small events. Each event calls for a
well-defined motion involving relevant characters and objects. Next, we
synthesize the motions of characters at positions sampled based on spatial
guidance. We employ a high-level module to deliver scalable yet comprehensive
context, translating events into relative descriptions that enable the
retrieval of precise coordinates. As the first to address this problem at scale
and with diversity, we offer a benchmark to assess diverse aspects of
contextual reasoning. Benchmark results and user studies show that our
framework effectively captures scene context with high scalability. The code
and benchmark, along with result videos, are available at our project page:
https://rms0329.github.io/Event-Driven-Storytelling/.

</details>


### [9] [GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting](https://arxiv.org/abs/2507.19451)
*Baijun Ye,Minghui Qin,Saining Zhang,Moonjun Gong,Shaoting Zhu,Zebang Shen,Luan Zhang,Lu Zhang,Hao Zhao,Hang Zhao*

Main category: cs.CV

TL;DR: GS-Occ3D是一种仅基于视觉的可扩展框架，用于直接重建占用率，解决了现有方法依赖LiDAR标注的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖LiDAR标注，限制了可扩展性和利用众包数据的能力，因此需要一种仅基于视觉的解决方案。

Method: GS-Occ3D使用基于八叉树的高斯曲面体表示法优化显式占用率表示，并将场景分解为静态背景、地面和动态物体。

Result: 在Waymo数据集上，GS-Occ3D实现了最先进的几何重建效果，并在Occ3D-Waymo和Occ3D-nuScenes上展示了优异的泛化能力。

Conclusion: GS-Occ3D展示了大规模基于视觉的占用率重建作为自动驾驶感知新范式的潜力。

Abstract: Occupancy is crucial for autonomous driving, providing essential geometric
priors for perception and planning. However, existing methods predominantly
rely on LiDAR-based occupancy annotations, which limits scalability and
prevents leveraging vast amounts of potential crowdsourced data for
auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only
framework that directly reconstructs occupancy. Vision-only occupancy
reconstruction poses significant challenges due to sparse viewpoints, dynamic
scene elements, severe occlusions, and long-horizon motion. Existing
vision-based methods primarily rely on mesh representation, which suffer from
incomplete geometry and additional post-processing, limiting scalability. To
overcome these issues, GS-Occ3D optimizes an explicit occupancy representation
using an Octree-based Gaussian Surfel formulation, ensuring efficiency and
scalability. Additionally, we decompose scenes into static background, ground,
and dynamic objects, enabling tailored modeling strategies: (1) Ground is
explicitly reconstructed as a dominant structural element, significantly
improving large-area consistency; (2) Dynamic vehicles are separately modeled
to better capture motion-related occupancy patterns. Extensive experiments on
the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry
reconstruction results. By curating vision-only binary occupancy labels from
diverse urban scenes, we show their effectiveness for downstream occupancy
models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.
It highlights the potential of large-scale vision-based occupancy
reconstruction as a new paradigm for autonomous driving perception. Project
Page: https://gs-occ3d.github.io/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts](https://arxiv.org/abs/2507.19477)
*Sang-Woo Lee,Sohee Yang,Donghyun Kwak,Noah Y. Siegel*

Main category: cs.LG

TL;DR: 本文主张研究大规模训练超级预测级事件预测LLMs的时机已成熟，提出了训练方法和数据获取两个关键方向，并讨论了技术进展如何使AI为社会提供更广泛的预测智能。


<details>
  <summary>Details</summary>
Motivation: 早期研究的方法问题引发了对LLMs用于事件预测的质疑，但近期研究表明先进LLMs正逐步达到超级预测水平，且强化学习等技术能进一步提升预测能力。

Method: 提出了解决LLM事件预测训练中噪声稀疏性、知识截止和简单奖励结构问题的思路，如假设事件贝叶斯网络、利用低回忆和反事实事件、辅助奖励信号。

Result: 通过积极利用市场、公共和爬取数据集，支持大规模训练和评估，技术进展有望使AI为社会提供更广泛的预测智能。

Conclusion: 本文为接近超级预测级AI技术提供了具体路径和考虑，呼吁研究者关注这些方向。

Abstract: Many recent papers have studied the development of superforecaster-level
event forecasting LLMs. While methodological problems with early studies cast
doubt on the use of LLMs for event forecasting, recent studies with improved
evaluation methods have shown that state-of-the-art LLMs are gradually reaching
superforecaster-level performance, and reinforcement learning has also been
reported to improve future forecasting. Additionally, the unprecedented success
of recent reasoning models and Deep Research-style models suggests that
technology capable of greatly improving forecasting performance has been
developed. Therefore, based on these positive recent trends, we argue that the
time is ripe for research on large-scale training of superforecaster-level
event forecasting LLMs. We discuss two key research directions: training
methods and data acquisition. For training, we first introduce three
difficulties of LLM-based event forecasting training: noisiness-sparsity,
knowledge cut-off, and simple reward structure problems. Then, we present
related ideas to mitigate these problems: hypothetical event Bayesian networks,
utilizing poorly-recalled and counterfactual events, and auxiliary reward
signals. For data, we propose aggressive use of market, public, and crawling
datasets to enable large-scale training and evaluation. Finally, we explain how
these technical advances could enable AI to provide predictive intelligence to
society in broader areas. This position paper presents promising specific paths
and considerations for getting closer to superforecaster-level AI technology,
aiming to call for researchers' interest in these directions.

</details>


### [11] [SILS: Strategic Influence on Liquidity Stability and Whale Detection in Concentrated-Liquidity DEXs](https://arxiv.org/abs/2507.19411)
*Ali RajabiNekoo,Laleh Rasoul,Amirfarhad Farhadi,Azadeh Zamanifar*

Main category: cs.LG

TL;DR: SILS框架通过动态、影响导向的方法分析流动性提供者（LPs），超越传统静态指标，提供更准确的风险评估。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖名义资本或表面活动，导致风险分析不准确，需更动态、全面的LP评估方法。

Method: 利用链上事件日志和智能合约执行痕迹，计算指数时间加权流动性（ETWL）并应用无监督异常检测，定义流动性稳定性影响分数（LSIS）。

Result: SILS能更准确地识别高影响力LPs，减少误报和漏报，提升DeFi生态系统的透明度和风险管理。

Conclusion: SILS为DeFi协议提供了主动风险管理机制，显著改善对不对称流动性行为的防护。

Abstract: Traditional methods for identifying impactful liquidity providers (LPs) in
Concentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as
nominal capital size or surface-level activity, which often lead to inaccurate
risk analysis. The SILS framework offers a significantly more detailed
approach, characterizing LPs not just as capital holders but as dynamic
systemic agents whose actions directly impact market stability. This represents
a fundamental paradigm shift from the static, volume-based analysis to a
dynamic, impact-focused understanding. This advanced approach uses on-chain
event logs and smart contract execution traces to compute Exponential
Time-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly
detection. Most importantly, it defines an LP's functional importance through
the Liquidity Stability Impact Score (LSIS), a counterfactual metric that
measures the potential degradation of the market if the LP withdraws. This
combined approach provides a more detailed and realistic characterization of an
LP's impact, moving beyond the binary and often misleading classifications used
by existing methods. This impact-focused and comprehensive approach enables
SILS to accurately identify high-impact LPs-including those missed by
traditional methods and supports essential applications like a protective
oracle layer and actionable trader signals, thereby significantly enhancing
DeFi ecosystem. The framework provides unprecedented transparency into the
underlying liquidity structure and associated risks, effectively reducing the
common false positives and uncovering critical false negatives found in
traditional models. Therefore, SILS provides an effective mechanism for
proactive risk management, transforming how DeFi protocols safeguard their
ecosystems against asymmetric liquidity behavior.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study](https://arxiv.org/abs/2507.19396)
*Rachel M. Murphy,Nishant Mishra,Nicolette F. de Keizer,Dave A. Dongelmans,Kitty J. Jager,Ameen Abu-Hanna,Joanna E. Klopotowska,Iacer Calixto*

Main category: cs.CL

TL;DR: 该研究为荷兰临床自由文本中的药物不良事件（ADE）检测设定了基准，使用多种Transformer模型和性能指标，MedRoBERTa.nl表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为荷兰临床自由文本中的ADE检测建立基准，评估不同模型在命名实体识别（NER）和关系分类（RC）任务中的表现。

Method: 使用Bi-LSTM和四种Transformer模型（BERTje、RobBERT、MedRoBERTa.nl、NuNER），基于102份荷兰ICU临床进展笔记进行训练和评估。

Result: MedRoBERTa.nl在ADE RC任务中表现最佳，宏平均F1分数为0.63（黄金标准）和0.62（预测实体），外部验证中召回率为0.67至0.74。

Conclusion: 研究为ADE检测提供了稳健且临床相关的方法，强调了使用适合任务的性能指标的重要性。

Abstract: In this study, we set a benchmark for adverse drug event (ADE) detection in
Dutch clinical free text documents using several transformer models, clinical
scenarios and fit-for-purpose performance measures. We trained a Bidirectional
Long Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or
multilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the
tasks of named entity recognition (NER) and relation classification (RC) using
102 richly annotated Dutch ICU clinical progress notes. Anonymized free text
clinical progress notes of patients admitted to intensive care unit (ICU) of
one academic hospital and discharge letters of patients admitted to Internal
Medicine wards of two non-academic hospitals were reused. We evaluated our ADE
RC models internally using gold standard (two-step task) and predicted entities
(end-to-end task). In addition, all models were externally validated on
detecting ADEs at the document level. We report both micro- and macro-averaged
F1 scores, given the imbalance of ADEs in the datasets. Although differences
for the ADE RC task between the models were small, MedRoBERTa.nl was the best
performing model with macro-averaged F1 score of 0.63 using gold standard and
0.62 using predicted entities. The MedRoBERTa.nl models also performed the best
in our external validation and achieved recall of between 0.67 to 0.74 using
predicted entities, meaning between 67 to 74% of discharge letters with ADEs
were detected. Our benchmark study presents a robust and clinically meaningful
approach for evaluating language models for ADE detection in clinical free text
documents. Our study highlights the need to use appropriate performance
measures fit for the task of ADE detection in clinical free-text documents and
envisioned future clinical use.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [13] [ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination](https://arxiv.org/abs/2507.19151)
*Michael Amir,Guang Yang,Zhan Gao,Keisuke Okumura,Heedo Woo,Amanda Prorok*

Main category: cs.RO

TL;DR: ReCoDe是一个结合优化控制器和多智能体强化学习的混合框架，通过学习动态约束改进专家控制器，提升多智能体导航任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，手工设计的约束可能无法满足复杂协调需求，因此需要一种更灵活的方法。

Method: ReCoDe通过局部通信学习动态约束，改进专家控制器，实现更有效的协调。

Result: 在复杂导航任务中，ReCoDe优于手工控制器、其他混合方法和标准MARL基线。

Conclusion: 保留用户定义的控制器并结合动态约束学习，比从头学习更高效。

Abstract: Constraint-based optimization is a cornerstone of robotics, enabling the
design of controllers that reliably encode task and safety requirements such as
collision avoidance or formation adherence. However, handcrafted constraints
can fail in multi-agent settings that demand complex coordination. We introduce
ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid
framework that merges the reliability of optimization-based controllers with
the adaptability of multi-agent reinforcement learning. Rather than discarding
expert controllers, ReCoDe improves them by learning additional, dynamic
constraints that capture subtler behaviors, for example, by constraining agent
movements to prevent congestion in cluttered scenarios. Through local
communication, agents collectively constrain their allowed actions to
coordinate more effectively under changing conditions. In this work, we focus
on applications of ReCoDe to multi-agent navigation tasks requiring intricate,
context-based movements and consensus, where we show that it outperforms purely
handcrafted controllers, other hybrid approaches, and standard MARL baselines.
We give empirical (real robot) and theoretical evidence that retaining a
user-defined controller, even when it is imperfect, is more efficient than
learning from scratch, especially because ReCoDe can dynamically change the
degree to which it relies on this controller.

</details>
