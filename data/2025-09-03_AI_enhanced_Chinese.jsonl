{"id": "2509.00054", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00054", "abs": "https://arxiv.org/abs/2509.00054", "authors": ["Haimei Pan", "Jiyun Zhang", "Qinxi Wei", "Xiongnan Jin", "Chen Xinkai", "Jie Cheng"], "title": "Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought", "comment": null, "summary": "Fire is a highly destructive disaster, but effective prevention can\nsignificantly reduce its likelihood of occurrence. When it happens, deploying\nemergency robots in fire-risk scenarios can help minimize the danger to human\nresponders. However, current research on pre-disaster warnings and\ndisaster-time rescue still faces significant challenges due to incomplete\nperception, inadequate fire situational awareness, and delayed response. To\nenhance intelligent perception and response planning for robots in fire\nscenarios, we first construct a knowledge graph (KG) by leveraging large\nlanguage models (LLMs) to integrate fire domain knowledge derived from fire\nprevention guidelines and fire rescue task information from robotic emergency\nresponse documents. We then propose a new framework called Insights-on-Graph\n(IOG), which integrates the structured fire information of KG and Large\nMultimodal Models (LMMs). The framework generates perception-driven risk graphs\nfrom real-time scene imagery to enable early fire risk detection and provide\ninterpretable emergency responses for task module and robot component\nconfiguration based on the evolving risk situation. Extensive simulations and\nreal-world experiments show that IOG has good applicability and practical\napplication value in fire risk detection and rescue decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Insights-on-Graph(IOG)\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5b9e\u73b0\u706b\u707e\u573a\u666f\u7684\u667a\u80fd\u611f\u77e5\u548c\u5e94\u6025\u54cd\u5e94\u89c4\u5212\uff0c\u6709\u6548\u63d0\u5347\u706b\u707e\u98ce\u9669\u68c0\u6d4b\u548c\u6551\u63f4\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u706b\u707e\u9884\u9632\u548c\u6551\u63f4\u7814\u7a76\u9762\u4e34\u611f\u77e5\u4e0d\u5b8c\u6574\u3001\u6001\u52bf\u8ba4\u77e5\u4e0d\u8db3\u548c\u54cd\u5e94\u5ef6\u8fdf\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u611f\u77e5\u548c\u54cd\u5e94\u7cfb\u7edf\u6765\u4fdd\u62a4\u4eba\u7c7b\u6551\u63f4\u4eba\u5458\u3002", "method": "\u9996\u5148\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u706b\u707e\u77e5\u8bc6\u56fe\u8c31\uff0c\u6574\u5408\u9632\u706b\u6307\u5357\u548c\u6551\u63f4\u4efb\u52a1\u4fe1\u606f\uff1b\u7136\u540e\u63d0\u51faIOG\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u548c\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ece\u5b9e\u65f6\u573a\u666f\u56fe\u50cf\u751f\u6210\u611f\u77e5\u9a71\u52a8\u7684\u98ce\u9669\u56fe\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0cIOG\u5728\u706b\u707e\u98ce\u9669\u68c0\u6d4b\u548c\u6551\u63f4\u51b3\u7b56\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u9002\u7528\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "IOG\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u65e9\u671f\u706b\u707e\u98ce\u9669\u68c0\u6d4b\uff0c\u5e76\u6839\u636e\u52a8\u6001\u98ce\u9669\u60c5\u51b5\u4e3a\u4efb\u52a1\u6a21\u5757\u548c\u673a\u5668\u4eba\u7ec4\u4ef6\u914d\u7f6e\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5e94\u6025\u54cd\u5e94\uff0c\u663e\u8457\u63d0\u5347\u706b\u707e\u573a\u666f\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002"}}
{"id": "2509.00317", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00317", "abs": "https://arxiv.org/abs/2509.00317", "authors": ["Fulvio Mastrogiovanni", "Antony Thomas"], "title": "A Framework for Task and Motion Planning based on Expanding AND/OR Graphs", "comment": "Accepted for an oral presentation at ASTRA Conference, 2025", "summary": "Robot autonomy in space environments presents unique challenges, including\nhigh perception and motion uncertainty, strict kinematic constraints, and\nlimited opportunities for human intervention. Therefore, Task and Motion\nPlanning (TMP) may be critical for autonomous servicing, surface operations, or\neven in-orbit missions, just to name a few, as it models tasks as discrete\naction sequencing integrated with continuous motion feasibility assessments. In\nthis paper, we introduce a TMP framework based on expanding AND/OR graphs,\nreferred to as TMP-EAOG, and demonstrate its adaptability to different\nscenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,\nwhich expands iteratively as the plan is executed, and performs in-the-loop\nmotion planning assessments to ascertain their feasibility. As a consequence,\nTMP-EAOG is characterised by the desirable properties of (i) robustness to a\ncertain degree of uncertainty, because AND/OR graph expansion can accommodate\nfor unpredictable information about the robot environment, (ii) controlled\nautonomy, since an AND/OR graph can be validated by human experts, and (iii)\nbounded flexibility, in that unexpected events, including the assessment of\nunfeasible motions, can lead to different courses of action as alternative\npaths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We\nuse a simulated mobile manipulator as a proxy for space-grade autonomous\nrobots. Our evaluation shows that TMP-EAOG can deal with a wide range of\nchallenges in the benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eAND/OR\u56fe\u6269\u5c55\u7684TMP-EAOG\u6846\u67b6\uff0c\u7528\u4e8e\u7a7a\u95f4\u673a\u5668\u4eba\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff0c\u5177\u6709\u6297\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u63a7\u81ea\u4e3b\u6027\u548c\u6709\u9650\u7075\u6d3b\u6027\u7b49\u7279\u70b9", "motivation": "\u7a7a\u95f4\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u9762\u4e34\u611f\u77e5\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u3001\u4e25\u683c\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u6709\u9650\u4eba\u5de5\u5e72\u9884\u673a\u4f1a\u7b49\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212(TMP)\u6765\u6574\u5408\u79bb\u6563\u52a8\u4f5c\u5e8f\u5217\u548c\u8fde\u7eed\u8fd0\u52a8\u53ef\u884c\u6027\u8bc4\u4f30", "method": "TMP-EAOG\u6846\u67b6\u5728AND/OR\u56fe\u4e2d\u7f16\u7801\u4efb\u52a1\u7ea7\u62bd\u8c61\uff0c\u8fed\u4ee3\u6269\u5c55\u56fe\u7ed3\u6784\uff0c\u6267\u884c\u5728\u7ebf\u8fd0\u52a8\u89c4\u5212\u8bc4\u4f30\u6765\u786e\u5b9a\u53ef\u884c\u6027", "result": "\u5728\u6a21\u62df\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cTMP-EAOG\u80fd\u591f\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5404\u79cd\u6311\u6218\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027", "conclusion": "TMP-EAOG\u6846\u67b6\u5177\u6709\u6297\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u63a7\u81ea\u4e3b\u6027\u548c\u6709\u9650\u7075\u6d3b\u6027\u7b49\u7406\u60f3\u7279\u6027\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u81ea\u4e3b\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212"}}
{"id": "2509.00287", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.00287", "abs": "https://arxiv.org/abs/2509.00287", "authors": ["Brian Wang", "Mani Srivastava"], "title": "SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces", "comment": "9 pages, accepted at UrbComp 2025 KDD 2025", "summary": "Modern urban spaces are equipped with an increasingly diverse set of sensors,\nall producing an abundance of multimodal data. Such multimodal data can be used\nto identify and reason about important incidents occurring in urban landscapes,\nsuch as major emergencies, cultural and social events, as well as natural\ndisasters. However, such data may be fragmented over several sources and\ndifficult to integrate due to the reliance on human-driven reasoning for\nidentifying relationships between the multimodal data corresponding to an\nincident, as well as understanding the different components which define an\nincident. Such relationships and components are critical to identifying the\ncauses of such incidents, as well as producing forecasting the scale and\nintensity of future incidents as they begin to develop. In this work, we create\nSIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal\nUrban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary\nworld knowledge for identifying relationships between incidents occurring in\nurban spaces and data from different modalities, allowing us to organize\nevidence and observations relevant to an incident without relying and\nhuman-encoded rules for relating multimodal sensory data with incidents. This\norganized knowledge is represented as a knowledge graph, organizing incidents,\nobservations, and much more. We find that our system is able to produce\nreasonable connections between 5 different data sources (new article text, CCTV\nimages, air quality, weather, and traffic measurements) and relevant incidents\noccurring at the same time and location.", "AI": {"tldr": "SIGMUS\u7cfb\u7edf\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6574\u5408\u57ce\u5e02\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u6765\u8bc6\u522b\u548c\u63a8\u7406\u57ce\u5e02\u4e8b\u4ef6\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u89c4\u5219\u3002", "motivation": "\u57ce\u5e02\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u4e30\u5bcc\u4f46\u5206\u6563\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u63a8\u7406\u6765\u8bc6\u522b\u4e8b\u4ef6\u4e0e\u6570\u636e\u95f4\u7684\u5173\u7cfb\uff0c\u96be\u4ee5\u6709\u6548\u6574\u5408\u548c\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u6765\u7406\u89e3\u548c\u9884\u6d4b\u57ce\u5e02\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1SIGMUS\u7cfb\u7edf\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5fc5\u8981\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u81ea\u52a8\u8bc6\u522b\u57ce\u5e02\u4e8b\u4ef6\u4e0e\u591a\u6a21\u6001\u6570\u636e\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5c06\u7ec4\u7ec7\u5316\u7684\u77e5\u8bc6\u8868\u793a\u4e3a\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5408\u7406\u8fde\u63a55\u79cd\u4e0d\u540c\u6570\u636e\u6e90\uff08\u65b0\u95fb\u6587\u672c\u3001\u76d1\u63a7\u56fe\u50cf\u3001\u7a7a\u6c14\u8d28\u91cf\u3001\u5929\u6c14\u548c\u4ea4\u901a\u6d4b\u91cf\uff09\u4e0e\u540c\u65f6\u540c\u5730\u53d1\u751f\u7684\u76f8\u5173\u4e8b\u4ef6\u3002", "conclusion": "SIGMUS\u901a\u8fc7LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u57ce\u5e02\u6570\u636e\u6574\u5408\u7684\u6311\u6218\uff0c\u4e3a\u57ce\u5e02\u4e8b\u4ef6\u8bc6\u522b\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00213", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00213", "abs": "https://arxiv.org/abs/2509.00213", "authors": ["Farhan Fuad Abir", "Abigail Elliott Daly", "Kyle Anderman", "Tolga Ozmen", "Laura J. Brattain"], "title": "Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data", "comment": "IEEE-EMBS International Conference on Body Sensor Networks (IEEE-EMBS\n  BSN 2025)", "summary": "Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are\ndifficult to classify preoperatively due to their radiological similarity to\nbenign fibroadenomas. This often leads to unnecessary surgical excisions. To\naddress this, we propose a multimodal deep learning framework that integrates\nbreast ultrasound (BUS) images with structured clinical data to improve\ndiagnostic accuracy. We developed a dual-branch neural network that extracts\nand fuses features from ultrasound images and patient metadata from 81 subjects\nwith confirmed PTs. Class-aware sampling and subject-stratified 5-fold\ncross-validation were applied to prevent class imbalance and data leakage. The\nresults show that our proposed multimodal method outperforms unimodal baselines\nin classifying benign versus borderline/malignant PTs. Among six image\nencoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal\nsetting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and\n0.7294, respectively. This study demonstrates the potential of multimodal AI to\nserve as a non-invasive diagnostic tool, reducing unnecessary biopsies and\nimproving clinical decision-making in breast tumor management.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u4e73\u817a\u8d85\u58f0\u56fe\u50cf\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u63d0\u9ad8\u53f6\u72b6\u80bf\u7624\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u624b\u672f\u5207\u9664\u3002", "motivation": "\u53f6\u72b6\u80bf\u7624\u5728\u5f71\u50cf\u5b66\u4e0a\u4e0e\u826f\u6027\u7ea4\u7ef4\u817a\u7624\u76f8\u4f3c\uff0c\u672f\u524d\u5206\u7c7b\u56f0\u96be\uff0c\u5e38\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u624b\u672f\u5207\u9664\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u53cc\u5206\u652f\u795e\u7ecf\u7f51\u7edc\uff0c\u4ece81\u4f8b\u786e\u8bca\u60a3\u8005\u7684\u8d85\u58f0\u56fe\u50cf\u548c\u4e34\u5e8a\u6570\u636e\u4e2d\u63d0\u53d6\u5e76\u878d\u5408\u7279\u5f81\uff0c\u91c7\u7528\u7c7b\u522b\u611f\u77e5\u91c7\u6837\u548c\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u9632\u6b62\u6570\u636e\u504f\u5dee\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0cConvNeXt\u548cResNet18\u8868\u73b0\u6700\u4f73\uff0cAUC-ROC\u5206\u522b\u8fbe0.9427\u548c0.9349\uff0cF1\u5206\u6570\u5206\u522b\u4e3a0.6720\u548c0.7294\u3002", "conclusion": "\u591a\u6a21\u6001AI\u6709\u6f5c\u529b\u4f5c\u4e3a\u65e0\u521b\u8bca\u65ad\u5de5\u5177\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6d3b\u68c0\uff0c\u6539\u5584\u4e73\u817a\u80bf\u7624\u7ba1\u7406\u7684\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2509.00076", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00076", "abs": "https://arxiv.org/abs/2509.00076", "authors": ["Zachery Dahm", "Konstantinos Vasili", "Vasileios Theos", "Konstantinos Gkouliaras", "William Richards", "True Miller", "Brian Jowers", "Stylianos Chatzidakis"], "title": "Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor", "comment": null, "summary": "There is increased interest in applying Artificial Intelligence and Machine\nLearning (AI/ML) within the nuclear industry and nuclear engineering community.\nEffective implementation of AI/ML could offer benefits to the nuclear domain,\nincluding enhanced identification of anomalies, anticipation of system\nfailures, and operational schedule optimization. However, limited work has been\ndone to investigate the feasibility and applicability of AI/ML tools in a\nfunctioning nuclear reactor. Here, we go beyond the development of a single\nmodel and introduce a multi-layered AI/ML architecture that integrates both\ninformation technology and operational technology data streams to identify,\ncharacterize, and differentiate (i) among diverse cybersecurity events and (ii)\nbetween cyber events and other operational anomalies. Leveraging Purdue\nUniversitys research reactor, PUR-1, we demonstrate this architecture through a\nrepresentative use case that includes multiple concurrent false data injections\nand denial-of-service attacks of increasing complexity under realistic reactor\nconditions. The use case includes 14 system states (1 normal, 13 abnormal) and\nover 13.8 million multi-variate operational and information technology data\npoints. The study demonstrated the capability of AI/ML to distinguish between\nnormal, abnormal, and cybersecurity-related events, even under challenging\nconditions such as denial-of-service attacks. Combining operational and\ninformation technology data improved classification accuracy but posed\nchallenges related to synchronization and collection during certain cyber\nevents. While results indicate significant promise for AI/ML in nuclear\ncybersecurity, the findings also highlight the need for further refinement in\nhandling complex event differentiation and multi-class architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42AI/ML\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u4fe1\u606f\u6280\u672f\u548c\u8fd0\u8425\u6280\u672f\u6570\u636e\u6765\u8bc6\u522b\u3001\u7279\u5f81\u5316\u548c\u533a\u5206\u6838\u5de5\u4e1a\u4e2d\u7684\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u4e0e\u8fd0\u8425\u5f02\u5e38\u3002\u5728PUR-1\u7814\u7a76\u53cd\u5e94\u5806\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5305\u542b14\u79cd\u7cfb\u7edf\u72b6\u6001\u548c1380\u4e07\u591a\u5143\u6570\u636e\u70b9\u3002", "motivation": "\u6838\u5de5\u4e1a\u5bf9AI/ML\u7684\u9700\u6c42\u589e\u957f\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5c11\u6709\u5728\u8fd0\u884c\u6838\u53cd\u5e94\u5806\u4e0a\u9a8c\u8bc1AI/ML\u5de5\u5177\u7684\u53ef\u884c\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u533a\u5206\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u548c\u5176\u4ed6\u8fd0\u8425\u5f02\u5e38\u7684\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u591a\u5c42AI/ML\u67b6\u6784\uff0c\u6574\u5408IT\u548cOT\u6570\u636e\u6d41\uff0c\u5728PUR-1\u7814\u7a76\u53cd\u5e94\u5806\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u5305\u542b\u591a\u79cd\u5e76\u53d1\u7684\u5047\u6570\u636e\u6ce8\u5165\u548c\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u573a\u666f\uff0c\u6db5\u76d614\u79cd\u7cfb\u7edf\u72b6\u6001\u548c1380\u4e07\u591a\u5143\u6570\u636e\u70b9\u3002", "result": "AI/ML\u80fd\u591f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\uff08\u5982\u62d2\u7edd\u670d\u52a1\u653b\u51fb\uff09\u533a\u5206\u6b63\u5e38\u3001\u5f02\u5e38\u548c\u7f51\u7edc\u5b89\u5168\u76f8\u5173\u4e8b\u4ef6\u3002\u7ed3\u5408IT\u548cOT\u6570\u636e\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4f46\u5728\u67d0\u4e9b\u7f51\u7edc\u4e8b\u4ef6\u4e2d\u9762\u4e34\u540c\u6b65\u548c\u6536\u96c6\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86AI/ML\u5728\u6838\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u540c\u65f6\u6307\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u7cbe\u70bc\u5904\u7406\u590d\u6742\u4e8b\u4ef6\u533a\u5206\u548c\u591a\u7c7b\u67b6\u6784\u7684\u6280\u672f\u3002"}}
{"id": "2509.00083", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00083", "abs": "https://arxiv.org/abs/2509.00083", "authors": ["Laksh Patel", "Neel Shanbhag"], "title": "Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models", "comment": "6 pages, 2 figures, 1 table; Presented at the 42nd International\n  Conference on Machine Learning (ICML), winning the \"Best Poster\" award at\n  ICML's workshop for data in generative models (DIG-BUGS)", "summary": "Modern generative models risk overfitting and unintentionally memorizing rare\ntraining examples, which can be extracted by adversaries or inflate benchmark\nperformance. We propose Generative Data Cartography (GenDataCarto), a\ndata-centric framework that assigns each pretraining sample a difficulty score\n(early-epoch loss) and a memorization score (frequency of ``forget events''),\nthen partitions examples into four quadrants to guide targeted pruning and\nup-/down-weighting. We prove that our memorization score lower-bounds classical\ninfluence under smoothness assumptions and that down-weighting\nhigh-memorization hotspots provably decreases the generalization gap via\nuniform stability bounds. Empirically, GenDataCarto reduces synthetic canary\nextraction success by over 40\\% at just 10\\% data pruning, while increasing\nvalidation perplexity by less than 0.5\\%. These results demonstrate that\nprincipled data interventions can dramatically mitigate leakage with minimal\ncost to generative performance.", "AI": {"tldr": "GenDataCarto\u6846\u67b6\u901a\u8fc7\u96be\u5ea6\u548c\u8bb0\u5fc6\u5316\u8bc4\u5206\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5206\u7c7b\uff0c\u9488\u5bf9\u6027\u526a\u679d\u548c\u6743\u91cd\u8c03\u6574\uff0c\u5728\u4ec5\u526a\u679d10%\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5c06\u5408\u6210\u6d4b\u8bd5\u63d0\u53d6\u6210\u529f\u7387\u964d\u4f4e40%\u4ee5\u4e0a\uff0c\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u4ec5\u589e\u52a00.5%\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u5b58\u5728\u8fc7\u62df\u5408\u548c\u65e0\u610f\u8bb0\u5fc6\u7f55\u89c1\u8bad\u7ec3\u6837\u672c\u7684\u98ce\u9669\uff0c\u8fd9\u4e9b\u6837\u672c\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u63d0\u53d6\u6216\u5938\u5927\u57fa\u51c6\u6027\u80fd\uff0c\u9700\u8981\u6570\u636e\u4e2d\u5fc3\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u751f\u6210\u6570\u636e\u5236\u56fe(GenDataCarto)\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u9884\u8bad\u7ec3\u6837\u672c\u5206\u914d\u96be\u5ea6\u5206\u6570(\u65e9\u671fepoch\u635f\u5931)\u548c\u8bb0\u5fc6\u5316\u5206\u6570(\u9057\u5fd8\u4e8b\u4ef6\u9891\u7387)\uff0c\u5c06\u6837\u672c\u5212\u5206\u4e3a\u56db\u4e2a\u8c61\u9650\u6765\u6307\u5bfc\u9488\u5bf9\u6027\u526a\u679d\u548c\u4e0a\u4e0b\u6743\u91cd\u8c03\u6574\u3002", "result": "\u5b9e\u8bc1\u663e\u793a\uff0c\u5728\u4ec5\u526a\u679d10%\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5408\u6210\u6d4b\u8bd5\u63d0\u53d6\u6210\u529f\u7387\u964d\u4f4e40%\u4ee5\u4e0a\uff0c\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u4ec5\u589e\u52a0\u4e0d\u52300.5%\u3002\u7406\u8bba\u8bc1\u660e\u8bb0\u5fc6\u5316\u5206\u6570\u5728\u5e73\u6ed1\u5047\u8bbe\u4e0b\u662f\u7ecf\u5178\u5f71\u54cd\u7684\u4e0b\u754c\uff0c\u4e0b\u6743\u91cd\u9ad8\u8bb0\u5fc6\u5316\u70ed\u70b9\u53ef\u901a\u8fc7\u5747\u5300\u7a33\u5b9a\u6027\u754c\u9650\u964d\u4f4e\u6cdb\u5316\u5dee\u8ddd\u3002", "conclusion": "\u6709\u539f\u5219\u7684\u6570\u636e\u5e72\u9884\u53ef\u4ee5\u663e\u8457\u51cf\u8f7b\u6cc4\u9732\uff0c\u540c\u65f6\u5bf9\u751f\u6210\u6027\u80fd\u7684\u5f71\u54cd\u6700\u5c0f\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u4e2d\u5fc3\u65b9\u6cd5\u5728\u751f\u6210\u6a21\u578b\u5b89\u5168\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.00582", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.00582", "abs": "https://arxiv.org/abs/2509.00582", "authors": ["Rui Bai", "Rui Xu", "Teng Rui", "Jiale Liu", "Qi Wei Oung", "Hoi Leong Lee", "Zhen Tian", "Fujiang Yuan"], "title": "Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved Double Quintic Polynomial Approach with Time-to-Collision Evaluation", "comment": null, "summary": "Autonomous driving technology has made significant advancements in recent\nyears, yet challenges remain in ensuring safe and comfortable interactions with\nhuman-driven vehicles (HDVs), particularly during lane-changing maneuvers. This\npaper proposes an improved double quintic polynomial approach for safe and\nefficient lane-changing in mixed traffic environments. The proposed method\nintegrates a time-to-collision (TTC) based evaluation mechanism directly into\nthe trajectory optimization process, ensuring that the ego vehicle proactively\nmaintains a safe gap from surrounding HDVs throughout the maneuver. The\nframework comprises state estimation for both the autonomous vehicle (AV) and\nHDVs, trajectory generation using double quintic polynomials, real-time TTC\ncomputation, and adaptive trajectory evaluation. To the best of our knowledge,\nthis is the first work to embed an analytic TTC penalty directly into the\nclosed-form double-quintic polynomial solver, enabling real-time safety-aware\ntrajectory generation without post-hoc validation. Extensive simulations\nconducted under diverse traffic scenarios demonstrate the safety, efficiency,\nand comfort of the proposed approach compared to conventional methods such as\nquintic polynomials, Bezier curves, and B-splines. The results highlight that\nthe improved method not only avoids collisions but also ensures smooth\ntransitions and adaptive decision-making in dynamic environments. This work\nbridges the gap between model-based and adaptive trajectory planning\napproaches, offering a stable solution for real-world autonomous driving\napplications.", "AI": {"tldr": "\u6539\u8fdb\u7684\u53cc\u4e94\u6b21\u591a\u9879\u5f0f\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5d4c\u5165\u65f6\u95f4\u5230\u78b0\u649e(TTC)\u8bc4\u4f30\uff0c\u5b9e\u73b0\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u5b89\u5168\u9ad8\u6548\u53d8\u9053\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u9a7e\u9a76\u8f66\u8f86\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e0b\u4e0e\u4eba\u9a7e\u8f66\u8f86\u4ea4\u4e92\u65f6\u7684\u5b89\u5168\u548c\u8212\u9002\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u53d8\u9053\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u53cc\u4e94\u6b21\u591a\u9879\u5f0f\u65b9\u6cd5\uff0c\u5c06TTC\u57fa\u4e8e\u7684\u8bc4\u4f30\u673a\u5236\u76f4\u63a5\u96c6\u6210\u5230\u8f68\u8ff9\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u5305\u62ec\u72b6\u6001\u4f30\u8ba1\u3001\u8f68\u8ff9\u751f\u6210\u3001\u5b9e\u65f6TTC\u8ba1\u7b97\u548c\u9002\u5e94\u6027\u8f68\u8ff9\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u79cd\u4ea4\u901a\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u6d89\u5e7f\u6cd5\u6a21\u62df\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u907f\u514d\u78b0\u649e\u5e76\u4fdd\u8bc1\u5e73\u6ed1\u8fc7\u6e21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u6a21\u578b\u57fa\u4e8e\u548c\u9002\u5e94\u6027\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u81ea\u4e3b\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00823", "categories": ["cs.RO", "cs.SC", "math.AC", "68W30, 13P10, 13P25, 68U07, 68R10"], "pdf": "https://arxiv.org/pdf/2509.00823", "abs": "https://arxiv.org/abs/2509.00823", "authors": ["Takumu Okazaki", "Akira Terui", "Masahiko Mikawa"], "title": "Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using Comprehensive Gr\u00f6bner Systems", "comment": "24 pages", "summary": "We propose an effective method for solving the inverse kinematic problem of a\nspecific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer\nalgebra. It is known that when the rotation axes of three consecutive\nrotational joints of a manipulator intersect at a single point, the inverse\nkinematics problem can be divided into determining position and orientation. We\nextend this method to more general manipulators in which the rotational axes of\ntwo consecutive joints intersect. This extension broadens the class of 6-DOF\nmanipulators for which the inverse kinematics problem can be solved, and is\nexpected to enable more efficient solutions. The inverse kinematic problem is\nsolved using the Comprehensive Gr\\\"obner System (CGS) with joint parameters of\nthe robot appearing as parameters in the coefficients to prevent repetitive\ncalculations of the Gr\\\"obner bases. The effectiveness of the proposed method\nis shown by experiments.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u8ba1\u7b97\u673a\u4ee3\u6570\u89e3\u51b36\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u53ef\u6c42\u89e3\u7684\u673a\u5668\u4eba\u7c7b\u578b\u8303\u56f4", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8981\u6c42\u4e09\u4e2a\u8fde\u7eed\u65cb\u8f6c\u5173\u8282\u8f74\u7ebf\u76f8\u4ea4\u4e8e\u4e00\u70b9\u624d\u80fd\u5206\u89e3\u4f4d\u7f6e\u548c\u59ff\u6001\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u6c42\u89e3\u7684\u673a\u5668\u4eba\u7c7b\u578b", "method": "\u4f7f\u7528\u7efc\u5408Gr\u00f6bner\u7cfb\u7edf\uff08CGS\uff09\uff0c\u5c06\u673a\u5668\u4eba\u5173\u8282\u53c2\u6570\u4f5c\u4e3a\u7cfb\u6570\u53c2\u6570\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97Gr\u00f6bner\u57fa", "result": "\u65b9\u6cd5\u6269\u5c55\u4e86\u53ef\u6c42\u89e3\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u76846\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7c7b\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u89e3\u51b3\u66f4\u5e7f\u6cdb\u7c7b\u578b\u76846\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u95ee\u9898"}}
{"id": "2509.00971", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00971", "abs": "https://arxiv.org/abs/2509.00971", "authors": ["Jay Vaghasiya", "Omkar Ghugarkar", "Vishvesh Bhat", "Vipul Dholaria", "Julian McAuley"], "title": "CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks with LLMs", "comment": null, "summary": "We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel\nreasoning method called General Symbolics. This approach diverges from\nreasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT),\nand Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General\nSymbolic Reasoner (GSR) is specifically structured around three key use cases:\ntool-calling, code generation, and planning, demonstrating exemplary\nperformance across a total of seven benchmarks in their respective areas.\nNotably, we are achieving SOTA scores of 66.66\\% on Livecodebench v6, 89\\% on\nInstruction-Following Evals, and 24.4\\% on ARC-AGI-2. We also present an\nagentic coding IDE, developed using the principles of General Symbolics, which\nachieves a state-of-the-art accuracy of 62.3\\% on \\texttt{SWE-Bench Lite}. We\nare able to achieve these improvements without any finetuning or training\ncosts. Our Reasoning Layer is designed to provide a pure performance uplift,\nensuring that a model's accuracy on reasoning tasks is never negatively\nimpacted. We argue that incumbent methods will eventually lead to diminishing\nreturns in LLM performance, necessitating the development of new reasoning\ntechniques. This technical report details our approach at a high level and the\navailability of the CoreThink models for reasoning-intensive use cases.", "AI": {"tldr": "CoreThink\u662f\u4e00\u4e2a\u57fa\u4e8e\u901a\u7528\u7b26\u53f7\u63a8\u7406\u65b9\u6cd5\u7684\u65b0\u578b\u63a8\u7406\u5c42\uff0c\u5728\u5de5\u5177\u8c03\u7528\u3001\u4ee3\u7801\u751f\u6210\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u8bad\u7ec3\u6210\u672c\u5373\u53ef\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6d4b\u8bd5\u65f6\u6269\u5c55\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u5728LLM\u6027\u80fd\u63d0\u5347\u4e0a\u4f1a\u51fa\u73b0\u6536\u76ca\u9012\u51cf\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u63a8\u7406\u6280\u672f\u6765\u7a81\u7834\u6027\u80fd\u74f6\u9888\u3002", "method": "\u91c7\u7528\u901a\u7528\u7b26\u53f7\u63a8\u7406\uff08General Symbolics\uff09\u65b9\u6cd5\uff0c\u6784\u5efa\u4e13\u95e8\u7684\u63a8\u7406\u5c42\uff0c\u4e13\u6ce8\u4e8e\u5de5\u5177\u8c03\u7528\u3001\u4ee3\u7801\u751f\u6210\u548c\u89c4\u5212\u4e09\u4e2a\u6838\u5fc3\u7528\u4f8b\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aLivecodebench v6\u8fbe\u523066.66%\u3001\u6307\u4ee4\u8ddf\u968f\u8bc4\u4f3089%\u3001ARC-AGI-2\u8fbe\u523024.4%\uff0c\u57fa\u4e8e\u8be5\u6280\u672f\u5f00\u53d1\u7684\u667a\u80fd\u7f16\u7801IDE\u5728SWE-Bench Lite\u4e0a\u8fbe\u523062.3%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "CoreThink\u63d0\u4f9b\u7eaf\u6027\u80fd\u63d0\u5347\uff0c\u786e\u4fdd\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u6027\u4e0d\u53d7\u8d1f\u9762\u5f71\u54cd\uff0c\u4e3a\u63a8\u7406\u5bc6\u96c6\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u578b\u5df2\u53ef\u4f9b\u4f7f\u7528\u3002"}}
{"id": "2509.00362", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00362", "abs": "https://arxiv.org/abs/2509.00362", "authors": ["Hyungu Lee", "Taehyeong Kim", "Hayoung Choi"], "title": "Optimized Weight Initialization on the Stiefel Manifold for Deep ReLU Neural Networks", "comment": "16 pages, 3 figures, 3 tables", "summary": "Stable and efficient training of ReLU networks with large depth is highly\nsensitive to weight initialization. Improper initialization can cause permanent\nneuron inactivation dying ReLU and exacerbate gradient instability as network\ndepth increases. Methods such as He, Xavier, and orthogonal initialization\npreserve variance or promote approximate isometry. However, they do not\nnecessarily regulate the pre-activation mean or control activation sparsity,\nand their effectiveness often diminishes in very deep architectures. This work\nintroduces an orthogonal initialization specifically optimized for ReLU by\nsolving an optimization problem on the Stiefel manifold, thereby preserving\nscale and calibrating the pre-activation statistics from the outset. A family\nof closed-form solutions and an efficient sampling scheme are derived.\nTheoretical analysis at initialization shows that prevention of the dying ReLU\nproblem, slower decay of activation variance, and mitigation of gradient\nvanishing, which together stabilize signal and gradient flow in deep\narchitectures. Empirically, across MNIST, Fashion-MNIST, multiple tabular\ndatasets, few-shot settings, and ReLU-family activations, our method\noutperforms previous initializations and enables stable training in deep\nnetworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9ReLU\u7f51\u7edc\u7684\u4f18\u5316\u6b63\u4ea4\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7Stiefel\u6d41\u5f62\u4e0a\u7684\u4f18\u5316\u95ee\u9898\u6765\u6821\u51c6\u9884\u6fc0\u6d3b\u7edf\u8ba1\u91cf\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u7f51\u7edc\u4e2d\u795e\u7ecf\u5143\u5931\u6d3b\u548c\u68af\u5ea6\u6d88\u5931\u95ee\u9898", "motivation": "\u4f20\u7edf\u521d\u59cb\u5316\u65b9\u6cd5\uff08\u5982He\u3001Xavier\u3001\u6b63\u4ea4\u521d\u59cb\u5316\uff09\u5728\u6df1\u5ea6ReLU\u7f51\u7edc\u4e2d\u65e0\u6cd5\u6709\u6548\u8c03\u8282\u9884\u6fc0\u6d3b\u5747\u503c\u548c\u63a7\u5236\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u795e\u7ecf\u5143\u6c38\u4e45\u5931\u6d3b\uff08dying ReLU\uff09\u548c\u68af\u5ea6\u4e0d\u7a33\u5b9a\u95ee\u9898", "method": "\u5728Stiefel\u6d41\u5f62\u4e0a\u6c42\u89e3\u4f18\u5316\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u89e3\u548c\u9ad8\u6548\u91c7\u6837\u65b9\u6848\uff0c\u4e13\u95e8\u4e3aReLU\u6fc0\u6d3b\u51fd\u6570\u4f18\u5316\u6b63\u4ea4\u521d\u59cb\u5316\uff0c\u4ece\u521d\u59cb\u5316\u9636\u6bb5\u5c31\u4fdd\u6301\u5c3a\u5ea6\u5e76\u6821\u51c6\u9884\u6fc0\u6d3b\u7edf\u8ba1\u91cf", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u9632\u6b62dying ReLU\u95ee\u9898\u3001\u51cf\u7f13\u6fc0\u6d3b\u65b9\u5dee\u8870\u51cf\u3001\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\uff0c\u5728MNIST\u3001Fashion-MNIST\u3001\u8868\u683c\u6570\u636e\u96c6\u3001\u5c11\u6837\u672c\u8bbe\u7f6e\u548cReLU\u65cf\u6fc0\u6d3b\u51fd\u6570\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u521d\u59cb\u5316\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6ReLU\u7f51\u7edc\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u6df1\u5ea6\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u521d\u59cb\u5316\u7b56\u7565"}}
{"id": "2509.00402", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00402", "abs": "https://arxiv.org/abs/2509.00402", "authors": ["Minku Kang", "Hogun Park"], "title": "Curriculum Guided Personalized Subgraph Federated Learning", "comment": "Accepted to the CIKM 2025. This is an extended version of the\n  original submission", "summary": "Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs)\nacross distributed private subgraphs, but it suffers from severe data\nheterogeneity. To mitigate data heterogeneity, weighted model aggregation\npersonalizes each local GNN by assigning larger weights to parameters from\nclients with similar subgraph characteristics inferred from their current model\nstates. However, the sparse and biased subgraphs often trigger rapid\noverfitting, causing the estimated client similarity matrix to stagnate or even\ncollapse. As a result, aggregation loses effectiveness as clients reinforce\ntheir own biases instead of exploiting diverse knowledge otherwise available.\nTo this end, we propose a novel personalized subgraph FL framework called\nCurriculum guided personalized sUbgraph Federated Learning (CUFL). On the\nclient side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges\nfor training according to their reconstruction scores, exposing each GNN first\nto easier, generic cross-client substructures and only later to harder,\nclient-specific ones. This paced exposure prevents early overfitting to biased\npatterns and enables gradual personalization. By regulating personalization,\nthe curriculum also reshapes server aggregation from exchanging generic\nknowledge to propagating client-specific knowledge. Further, CUFL improves\nweighted aggregation by estimating client similarity using fine-grained\nstructural indicators reconstructed on a random reference graph. Extensive\nexperiments on six benchmark datasets confirm that CUFL achieves superior\nperformance compared to relevant baselines. Code is available at\nhttps://github.com/Kang-Min-Ku/CUFL.git.", "AI": {"tldr": "CUFL\u662f\u4e00\u4e2a\u65b0\u7684\u4e2a\u6027\u5316\u5b50\u56fe\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u9010\u6b65\u66b4\u9732\u6a21\u578b\u7ed9\u901a\u7528\u548c\u7279\u5b9a\u5ba2\u6237\u7aef\u5b50\u7ed3\u6784\uff0c\u9632\u6b62\u65e9\u671f\u8fc7\u62df\u5408\uff0c\u5e76\u4f7f\u7528\u7ec6\u7c92\u5ea6\u7ed3\u6784\u6307\u6807\u6539\u8fdb\u52a0\u6743\u805a\u5408\u3002", "motivation": "\u89e3\u51b3\u5b50\u56fe\u8054\u90a6\u5b66\u4e60\u4e2d\u7531\u4e8e\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u5feb\u901f\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9632\u6b62\u5ba2\u6237\u7aef\u76f8\u4f3c\u5ea6\u77e9\u9635\u505c\u6ede\u6216\u5d29\u6e83\uff0c\u4f7f\u805a\u5408\u80fd\u591f\u6709\u6548\u5229\u7528\u591a\u6837\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u81ea\u9002\u5e94\u9009\u62e9\u8fb9\u8fdb\u884c\u8bad\u7ec3\uff0c\u5148\u66b4\u9732\u7ed9\u901a\u7528\u8de8\u5ba2\u6237\u7aef\u5b50\u7ed3\u6784\uff0c\u518d\u66b4\u9732\u7ed9\u5ba2\u6237\u7aef\u7279\u5b9a\u5b50\u7ed3\u6784\uff1b\u4f7f\u7528\u968f\u673a\u53c2\u8003\u56fe\u4e0a\u91cd\u6784\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u6307\u6807\u4f30\u8ba1\u5ba2\u6237\u7aef\u76f8\u4f3c\u5ea6\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\uff0cCUFL\u76f8\u6bd4\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "CUFL\u901a\u8fc7\u8c03\u8282\u4e2a\u6027\u5316\u8fc7\u7a0b\uff0c\u6709\u6548\u9632\u6b62\u4e86\u65e9\u671f\u8fc7\u62df\u5408\uff0c\u6539\u8fdb\u4e86\u52a0\u6743\u805a\u5408\u6548\u679c\uff0c\u4e3a\u5b50\u56fe\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00549", "abs": "https://arxiv.org/abs/2509.00549", "authors": ["Peirong Liu", "Oula Puonti", "Xiaoling Hu", "Karthik Gopinath", "Annabel Sorby-Adams", "Daniel C. Alexander", "W. Taylor Kimberly", "Juan E. Iglesias"], "title": "A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging", "comment": "16 pages", "summary": "Recent learning-based approaches have made astonishing advances in calibrated\nmedical imaging like computerized tomography (CT), yet they struggle to\ngeneralize in uncalibrated modalities -- notably magnetic resonance (MR)\nimaging, where performance is highly sensitive to the differences in MR\ncontrast, resolution, and orientation. This prevents broad applicability to\ndiverse real-world clinical protocols. Here we introduce BrainFM, a\nmodality-agnostic, multi-task vision foundation model for human brain imaging.\nWith the proposed \"mild-to-severe\" intra-subject generation and \"real-synth\"\nmix-up training strategy, BrainFM is resilient to the appearance of acquired\nimages (e.g., modality, contrast, deformation, resolution, artifacts), and can\nbe directly applied to five fundamental brain imaging tasks, including image\nsynthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical\ndistance, bias field estimation, and registration. We evaluate the efficacy of\nBrainFM on eleven public datasets, and demonstrate its robustness and\neffectiveness across all tasks and input modalities. Code is available at\nhttps://github.com/jhuldr/BrainFM.", "AI": {"tldr": "BrainFM\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\"\u8f7b\u5ea6\u5230\u91cd\u5ea6\"\u751f\u6210\u548c\"\u771f\u5b9e-\u5408\u6210\"\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u8111\u6210\u50cf\u4efb\u52a1\uff0c\u5bf9\u4e0d\u540c\u7684\u6a21\u6001\u3001\u5bf9\u6bd4\u5ea6\u3001\u5206\u8fa8\u7387\u7b49\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u6821\u51c6\u533b\u5b66\u6210\u50cf\uff08\u5982CT\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u672a\u6821\u51c6\u6a21\u6001\uff08\u7279\u522b\u662fMRI\uff09\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5bf9MR\u5bf9\u6bd4\u5ea6\u3001\u5206\u8fa8\u7387\u548c\u65b9\u5411\u7684\u5dee\u5f02\u654f\u611f\uff0c\u9650\u5236\u4e86\u5728\u591a\u6837\u5316\u4e34\u5e8a\u534f\u8bae\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51faBrainFM\u6a21\u578b\uff0c\u91c7\u7528\"\u8f7b\u5ea6\u5230\u91cd\u5ea6\"\u7684\u53d7\u8bd5\u8005\u5185\u90e8\u751f\u6210\u548c\"\u771f\u5b9e-\u5408\u6210\"\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u5bf9\u83b7\u53d6\u56fe\u50cf\u7684\u5916\u89c2\u53d8\u5316\uff08\u6a21\u6001\u3001\u5bf9\u6bd4\u5ea6\u3001\u53d8\u5f62\u3001\u5206\u8fa8\u7387\u3001\u4f2a\u5f71\uff09\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u572811\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cBrainFM\u5728\u6240\u6709\u4efb\u52a1\u548c\u8f93\u5165\u6a21\u6001\u4e0a\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u76f4\u63a5\u5e94\u7528\u4e8e\u56fe\u50cf\u5408\u6210\u3001\u89e3\u5256\u5206\u5272\u3001\u5934\u76ae\u5230\u76ae\u5c42\u8ddd\u79bb\u3001\u504f\u7f6e\u573a\u4f30\u8ba1\u548c\u914d\u51c6\u7b49\u4e94\u4e2a\u57fa\u7840\u8111\u6210\u50cf\u4efb\u52a1\u3002", "conclusion": "BrainFM\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u6001\u65e0\u5173\u7684\u591a\u4efb\u52a1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3\u672a\u6821\u51c6\u533b\u5b66\u6210\u50cf\u6a21\u6001\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.00602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00602", "abs": "https://arxiv.org/abs/2509.00602", "authors": ["Salar Nouri", "Kaidi Shao", "Shervin Safavi"], "title": "TranCIT: Transient Causal Interaction Toolbox", "comment": null, "summary": "Quantifying transient causal interactions from non-stationary neural signals\nis a fundamental challenge in neuroscience. Traditional methods are often\ninadequate for brief neural events, and advanced, event-specific techniques\nhave lacked accessible implementations within the Python ecosystem. Here, we\nintroduce trancit (Transient Causal Interaction Toolbox), an open-source Python\npackage designed to bridge this gap. TranCIT implements a comprehensive\nanalysis pipeline, including Granger Causality, Transfer Entropy, and the more\nrobust Structural Causal Model-based Dynamic Causal Strength (DCS) and relative\nDynamic Causal Strength (rDCS) for accurately detecting event-driven causal\neffects. We demonstrate TranCIT's utility by successfully capturing causality\nin high-synchrony regimes where traditional methods fail and by identifying the\nknown transient information flow from hippocampal CA3 to CA1 during sharp-wave\nripple events in real-world data. The package offers a user-friendly, validated\nsolution for investigating the transient causal dynamics that govern complex\nsystems.", "AI": {"tldr": "TranCIT\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u7bb1\uff0c\u7528\u4e8e\u4ece\u975e\u5e73\u7a33\u795e\u7ecf\u4fe1\u53f7\u4e2d\u91cf\u5316\u77ac\u6001\u56e0\u679c\u4ea4\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u77ed\u6682\u795e\u7ecf\u4e8b\u4ef6\u5206\u6790\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u77ed\u6682\u795e\u7ecf\u4e8b\u4ef6\u65f6\u5f80\u5f80\u4e0d\u8db3\uff0c\u800c\u5148\u8fdb\u7684\u7279\u5b9a\u4e8b\u4ef6\u6280\u672f\u5728Python\u751f\u6001\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684\u5b9e\u73b0\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7efc\u5408\u7684\u5206\u6790\u5de5\u5177\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "TranCIT\u5b9e\u73b0\u4e86\u5305\u62ecGranger\u56e0\u679c\u6027\u3001\u4f20\u9012\u71b5\u4ee5\u53ca\u66f4\u7a33\u5065\u7684\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u52a8\u6001\u56e0\u679c\u5f3a\u5ea6(DCS)\u548c\u76f8\u5bf9\u52a8\u6001\u56e0\u679c\u5f3a\u5ea6(rDCS)\u5728\u5185\u7684\u7efc\u5408\u5206\u6790\u6d41\u7a0b\uff0c\u7528\u4e8e\u51c6\u786e\u68c0\u6d4b\u4e8b\u4ef6\u9a71\u52a8\u7684\u56e0\u679c\u6548\u5e94\u3002", "result": "TranCIT\u5728\u9ad8\u540c\u6b65\u72b6\u6001\u4e0b\u6210\u529f\u6355\u6349\u5230\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u8bc6\u522b\u51fa\u6d77\u9a6cCA3\u5230CA1\u5728\u5c16\u6ce2\u6d9f\u6f2a\u4e8b\u4ef6\u671f\u95f4\u7684\u5df2\u77e5\u77ac\u6001\u4fe1\u606f\u6d41\u3002", "conclusion": "\u8be5\u8f6f\u4ef6\u5305\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u4e14\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u7814\u7a76\u63a7\u5236\u590d\u6742\u7cfb\u7edf\u7684\u77ac\u6001\u56e0\u679c\u52a8\u529b\u5b66\u3002"}}
{"id": "2509.01836", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01836", "abs": "https://arxiv.org/abs/2509.01836", "authors": ["Md Mahbub Alam", "Jose F. Rodrigues-Jr", "Gabriel Spadon"], "title": "Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment", "comment": null, "summary": "Accurate vessel trajectory prediction is essential for enhancing situational\nawareness and preventing collisions. Still, existing data-driven models are\nconstrained mainly to single-vessel forecasting, overlooking vessel\ninteractions, navigation rules, and explicit collision risk assessment. We\npresent a transformer-based framework for multi-vessel trajectory prediction\nwith integrated collision risk analysis. For a given target vessel, the\nframework identifies nearby vessels. It jointly predicts their future\ntrajectories through parallel streams encoding kinematic and derived physical\nfeatures, causal convolutions for temporal locality, spatial transformations\nfor positional encoding, and hybrid positional embeddings that capture both\nlocal motion patterns and long-range dependencies. Evaluated on large-scale\nreal-world AIS data using joint multi-vessel metrics, the model demonstrates\nsuperior forecasting capabilities beyond traditional single-vessel displacement\nerrors. By simulating interactions among predicted trajectories, the framework\nfurther quantifies potential collision risks, offering actionable insights to\nstrengthen maritime safety and decision support.", "AI": {"tldr": "\u57fa\u4e8eTransformer\u7684\u591a\u8239\u8247\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u7edf\u7b97\u52a8\u529b\u5b66\u7279\u5f81\u3001\u7a7a\u95f4\u53d8\u6362\u548c\u6df7\u5408\u4f4d\u7f6e\u5d4c\u5165\uff0c\u652f\u6301\u78b0\u649e\u98ce\u9669\u5206\u6790", "motivation": "\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u5355\u8239\u8247\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u8239\u8247\u4e92\u52a8\u3001\u822a\u884c\u89c4\u5219\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\uff0c\u9700\u8981\u63d0\u9ad8\u6d77\u4e8b\u5b89\u5168\u60c5\u51b5\u610f\u8bc6", "method": "\u4f7f\u7528Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u6d41\u7f16\u7801\u52a8\u529b\u5b66\u548c\u7269\u7406\u7279\u5f81\uff0c\u91c7\u7528\u56e0\u679c\u5377\u79ef\u5904\u7406\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u7a7a\u95f4\u53d8\u6362\u8fdb\u884c\u4f4d\u7f6e\u7f16\u7801\uff0c\u6df7\u5408\u4f4d\u7f6e\u5d4c\u5165\u6355\u6349\u5c40\u90e8\u8fd0\u52a8\u6a21\u5f0f\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb", "result": "\u5728\u5927\u89c4\u6a21\u5b9e\u9645AIS\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u6a21\u578b\u5728\u805a\u5408\u591a\u8239\u8247\u6307\u6807\u4e0a\u663e\u793a\u4f18\u8d8a\u9884\u6d4b\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u5355\u8239\u8247\u4f4d\u79fb\u8bef\u5dee", "conclusion": "\u901a\u8fc7\u6a21\u62df\u9884\u6d4b\u8f68\u8ff9\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u6846\u67b6\u80fd\u591f\u91cf\u5316\u6f5c\u5728\u78b0\u649e\u98ce\u9669\uff0c\u4e3a\u63d0\u5347\u6d77\u4e8b\u5b89\u5168\u548c\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6027\u89c1\u89e3"}}
{"id": "2509.01909", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "pdf": "https://arxiv.org/pdf/2509.01909", "abs": "https://arxiv.org/abs/2509.01909", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "comment": "Technical Report", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86Constructive Safety Alignment (CSA)\u6846\u67b6\uff0c\u901a\u8fc7\u6e38\u620f\u8bba\u9884\u6d4b\u3001\u98ce\u9669\u8fb9\u754c\u53d1\u73b0\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u4e3a\u5fc3\u7406\u56f0\u6270\u7528\u6237\u63d0\u4f9b\u5efa\u8bbe\u6027\u6307\u5bfc\uff0c\u800c\u975e\u7b80\u5355\u62d2\u7edd\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u673a\u5236\u4e3b\u8981\u9488\u5bf9\u6076\u610f\u653b\u51fb\u8005\uff0c\u91c7\u7528\u9632\u5fa1\u6027\u62d2\u7edd\u7b56\u7565\uff0c\u4f46\u5ffd\u89c6\u4e86\u975e\u6076\u610f\u4f46\u5fc3\u7406\u56f0\u6270\u7528\u6237\u7684\u9700\u6c42\u3002\u7b80\u5355\u62d2\u7edd\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u91cd\u590d\u5c1d\u8bd5\u3001\u5347\u7ea7\u884c\u4e3a\u6216\u8f6c\u5411\u4e0d\u5b89\u5168\u5e73\u53f0\uff0c\u9020\u6210\u66f4\u4e25\u91cd\u540e\u679c\u3002", "method": "\u5f15\u5165Constructive Safety Alignment (CSA)\u8303\u5f0f\uff0c\u7ed3\u5408\u6e38\u620f\u8bba\u7528\u6237\u53cd\u5e94\u9884\u6d4b\u3001\u7ec6\u7c92\u5ea6\u98ce\u9669\u8fb9\u754c\u53d1\u73b0\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u63a7\u5236\u3002\u5728Oyster-I (Oy1)\u6a21\u578b\u4e2d\u5b9e\u73b0\uff0c\u5c06\u5b89\u5168\u8f6c\u5316\u4e3a\u4fe1\u4efb\u5efa\u7acb\u8fc7\u7a0b\u3002", "result": "Oy1\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5b89\u5168\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u901a\u7528\u80fd\u529b\u3002\u5728Constructive Benchmark\u4e0a\u5c55\u793a\u51fa\u63a5\u8fd1GPT-5\u7684\u5efa\u8bbe\u6027\u53c2\u4e0e\u5ea6\uff0c\u5728Strata-Sword\u8d8a\u72f1\u6570\u636e\u96c6\u4e0a\u5177\u6709\u63a5\u8fd1GPT-o1\u6c34\u5e73\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CSA\u4ece\u62d2\u7edd\u4f18\u5148\u8f6c\u5411\u6307\u5bfc\u4f18\u5148\u7684\u5b89\u5168\u6a21\u5f0f\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u6a21\u578b\u4e0e\u7528\u6237\u7684\u5173\u7cfb\uff0c\u65e8\u5728\u6784\u5efa\u4e0d\u4ec5\u5b89\u5168\u800c\u4e14\u6709\u610f\u4e49\u7684\u5e2e\u52a9\u6027\u7cfb\u7edf\u3002\u53d1\u5e03\u4e86Oy1\u6a21\u578b\u3001\u4ee3\u7801\u548c\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u652f\u6301\u8d1f\u8d23\u4efb\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u53d1\u5c55\u3002"}}
{"id": "2509.00751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00751", "abs": "https://arxiv.org/abs/2509.00751", "authors": ["Dinh-Khoi Vo", "Van-Loc Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions", "comment": "ACM Multimedia 2025", "summary": "Event-based image retrieval from free-form captions presents a significant\nchallenge: models must understand not only visual features but also latent\nevent semantics, context, and real-world knowledge. Conventional\nvision-language retrieval approaches often fall short when captions describe\nabstract events, implicit causality, temporal context, or contain long, complex\nnarratives. To tackle these issues, we introduce a multi-stage retrieval\nframework combining dense article retrieval, event-aware language model\nreranking, and efficient image collection, followed by caption-guided semantic\nmatching and rank-aware selection. We leverage Qwen3 for article search,\nQwen3-Reranker for contextual alignment, and Qwen2-VL for precise image\nscoring. To further enhance performance and robustness, we fuse outputs from\nmultiple configurations using Reciprocal Rank Fusion (RRF). Our system achieves\nthe top-1 score on the private test set of Track 2 in the EVENTA 2025 Grand\nChallenge, demonstrating the effectiveness of combining language-based\nreasoning and multimodal retrieval for complex, real-world image understanding.\nThe code is available at https://github.com/vdkhoi20/EVENT-Retriever.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff0c\u7ed3\u5408\u5bc6\u96c6\u6587\u7ae0\u68c0\u7d22\u3001\u4e8b\u4ef6\u611f\u77e5\u8bed\u8a00\u6a21\u578b\u91cd\u6392\u5e8f\u548c\u8bed\u4e49\u5339\u914d\uff0c\u5728EVENTA 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u6700\u4f73\u6210\u7ee9", "motivation": "\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u68c0\u7d22\u65b9\u6cd5\u5728\u5904\u7406\u63cf\u8ff0\u62bd\u8c61\u4e8b\u4ef6\u3001\u9690\u542b\u56e0\u679c\u5173\u7cfb\u3001\u65f6\u95f4\u4e0a\u4e0b\u6587\u6216\u590d\u6742\u53d9\u4e8b\u7684\u81ea\u7531\u5f62\u5f0f\u5b57\u5e55\u65f6\u8868\u73b0\u4e0d\u8db3", "method": "\u591a\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff1a\u5bc6\u96c6\u6587\u7ae0\u68c0\u7d22\u2192\u4e8b\u4ef6\u611f\u77e5\u8bed\u8a00\u6a21\u578b\u91cd\u6392\u5e8f\u2192\u9ad8\u6548\u56fe\u50cf\u6536\u96c6\u2192\u5b57\u5e55\u5f15\u5bfc\u8bed\u4e49\u5339\u914d\u2192\u6392\u5e8f\u611f\u77e5\u9009\u62e9\uff0c\u4f7f\u7528Qwen3\u7cfb\u5217\u6a21\u578b\u5e76\u878d\u5408\u591a\u914d\u7f6e\u8f93\u51fa", "result": "\u5728EVENTA 2025 Grand Challenge Track 2\u7684\u79c1\u6709\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97top-1\u5206\u6570", "conclusion": "\u7ed3\u5408\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u548c\u591a\u6a21\u6001\u68c0\u7d22\u7684\u65b9\u6cd5\u5bf9\u4e8e\u590d\u6742\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7406\u89e3\u975e\u5e38\u6709\u6548"}}
{"id": "2509.01980", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.01980", "abs": "https://arxiv.org/abs/2509.01980", "authors": ["Luca Di Pierno", "Robert Hewitt", "Stephan Weiss", "Roland Brockers"], "title": "Hybrid Autonomy Framework for a Future Mars Science Helicopter", "comment": "8 pages, IEEE CASE 2025 Conference", "summary": "Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary\nsurface exploration beyond the reach of ground-based robots. Thus, NASA is\nstudying a Mars Science Helicopter (MSH), an advanced concept capable of\nperforming long-range science missions and autonomously navigating challenging\nMartian terrain. Given significant Earth-Mars communication delays and mission\ncomplexity, an advanced autonomy framework is required to ensure safe and\nefficient operation by continuously adapting behavior based on mission\nobjectives and real-time conditions, without human intervention. This study\npresents a deterministic high-level control framework for aerial exploration,\nintegrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a\nscalable, robust, and computationally efficient autonomy solution for critical\nscenarios like deep space exploration. In this paper we outline key\ncapabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework\nwhich orchestrates them to achieve the desired objectives. Monte Carlo\nsimulations and real field tests validate the framework, demonstrating its\nrobustness and adaptability to both discrete events and real-time system\nfeedback. These inputs trigger state transitions or dynamically adjust behavior\nexecution, enabling reactive and context-aware responses. The framework is\nmiddleware-agnostic, supporting integration with systems like F-Prime and\nextending beyond aerial robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u706b\u661f\u79d1\u5b66\u76f4\u5347\u673a\u7684\u9ad8\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u6709\u9650\u72b6\u6001\u673a\u548c\u884c\u4e3a\u6811\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u4e3b\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u5730\u7403-\u706b\u661f\u901a\u4fe1\u5ef6\u8fdf\u663e\u8457\u4e14\u4efb\u52a1\u590d\u6742\uff0c\u9700\u8981\u5148\u8fdb\u7684\u81ea\u4e3b\u4f53\u6846\u67b6\u6765\u786e\u4fdd\u706b\u661f\u79d1\u5b66\u76f4\u5347\u673a\u5728\u65e0\u4eba\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u57fa\u4e8e\u4efb\u52a1\u76ee\u6807\u548c\u5b9e\u65f6\u6761\u4ef6\u6301\u7eed\u8c03\u6574\u884c\u4e3a\uff0c\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u4e0e\u884c\u4e3a\u6811\uff08BT\uff09\u6df7\u5408\u7684\u81ea\u4e3b\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u8f6c\u6362\u548c\u52a8\u6001\u884c\u4e3a\u8c03\u6574\u6765\u5b9e\u73b0\u53cd\u5e94\u5f0f\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u54cd\u5e94\uff0c\u652f\u6301\u4e0eF-Prime\u7b49\u7cfb\u7edf\u7684\u96c6\u6210\u3002", "result": "\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u79bb\u6563\u4e8b\u4ef6\u548c\u5b9e\u65f6\u7cfb\u7edf\u53cd\u9988\uff0c\u89e6\u53d1\u72b6\u6001\u8f6c\u6362\u6216\u52a8\u6001\u8c03\u6574\u884c\u4e3a\u6267\u884c\u3002", "conclusion": "\u8be5FSM-BT\u6df7\u5408\u6846\u67b6\u4e3a\u6df1\u7a7a\u63a2\u7d22\u7b49\u5173\u952e\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u4e3b\u4f53\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u822a\u7a7a\u673a\u5668\u4eba\uff0c\u8fd8\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.00641", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.00641", "abs": "https://arxiv.org/abs/2509.00641", "authors": ["Zhipeng Yin", "Zichong Wang", "Avash Palikhe", "Zhen Liu", "Jun Liu", "Wenbin Zhang"], "title": "AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models", "comment": null, "summary": "Generative models have achieved impressive results in text to image tasks,\nsignificantly advancing visual content creation. However, this progress comes\nat a cost, as such models rely heavily on large-scale training data and may\nunintentionally replicate copyrighted elements, creating serious legal and\nethical challenges for real-world deployment. To address these concerns,\nresearchers have proposed various strategies to mitigate copyright risks, most\nof which are prompt based methods that filter or rewrite user inputs to prevent\nexplicit infringement. While effective in handling obvious cases, these\napproaches often fall short in more subtle situations, where seemingly benign\nprompts can still lead to infringing outputs. To address these limitations,\nthis paper introduces Assessing and Mitigating Copyright Risks (AMCR), a\ncomprehensive framework which i) builds upon prompt-based strategies by\nsystematically restructuring risky prompts into safe and non-sensitive forms,\nii) detects partial infringements through attention-based similarity analysis,\nand iii) adaptively mitigates risks during generation to reduce copyright\nviolations without compromising image quality. Extensive experiments validate\nthe effectiveness of AMCR in revealing and mitigating latent copyright risks,\noffering practical insights and benchmarks for the safer deployment of\ngenerative models.", "AI": {"tldr": "AMCR\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u91cd\u6784\u98ce\u9669\u63d0\u793a\u3001\u6ce8\u610f\u529b\u76f8\u4f3c\u6027\u5206\u6790\u548c\u81ea\u9002\u5e94\u98ce\u9669\u7f13\u89e3\uff0c\u6709\u6548\u68c0\u6d4b\u548c\u51cf\u8f7b\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7248\u6743\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u80fd\u65e0\u610f\u4e2d\u590d\u5236\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\uff0c\u5e26\u6765\u6cd5\u5f8b\u548c\u4f26\u7406\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5fae\u5999\u4fb5\u6743\u60c5\u51b5\u65f6\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faAMCR\u7efc\u5408\u6846\u67b6\uff1a1\uff09\u7cfb\u7edf\u91cd\u6784\u98ce\u9669\u63d0\u793a\u4e3a\u5b89\u5168\u5f62\u5f0f\uff1b2\uff09\u901a\u8fc7\u6ce8\u610f\u529b\u76f8\u4f3c\u6027\u5206\u6790\u68c0\u6d4b\u90e8\u5206\u4fb5\u6743\uff1b3\uff09\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u7f13\u89e3\u98ce\u9669\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AMCR\u5728\u63ed\u793a\u548c\u51cf\u8f7b\u6f5c\u5728\u7248\u6743\u98ce\u9669\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u548c\u57fa\u51c6\u3002", "conclusion": "AMCR\u6846\u67b6\u4e3a\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7248\u6743\u95ee\u9898\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u51cf\u5c11\u7248\u6743\u4fb5\u6743\u98ce\u9669\u3002"}}
{"id": "2509.00703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00703", "abs": "https://arxiv.org/abs/2509.00703", "authors": ["Osama Ahmad", "Lukas Wesemann", "Fabian Waschkowski", "Zubair Khalid"], "title": "Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition", "comment": "Under review in IEEE Signal Processing Letter", "summary": "Accurate spatiotemporal forecasting is critical for numerous complex systems\nbut remains challenging due to complex volatility patterns and spectral\nentanglement in conventional graph neural networks (GNNs). While\ndecomposition-integrated approaches like variational mode graph convolutional\nnetwork (VMGCN) improve accuracy through signal decomposition, they suffer from\ncomputational inefficiency and manual hyperparameter tuning. To address these\nlimitations, we propose the mode adaptive graph network (MAGN) that transforms\niterative variational mode decomposition (VMD) into a trainable neural module.\nOur key innovations include (1) an unfolded VMD (UVMD) module that replaces\niterative optimization with a fixed-depth network to reduce the decomposition\ntime (by 250x for the LargeST benchmark), and (2) mode-specific learnable\nbandwidth constraints ({\\alpha}k ) adapt spatial heterogeneity and eliminate\nmanual tuning while preventing spectral overlap. Evaluated on the LargeST\nbenchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reduction\nin the prediction error over VMGCN and outperforms state-of-the-art baselines.", "AI": {"tldr": "MAGN\u6a21\u578b\u901a\u8fc7\u5c06\u53d8\u5206\u6a21\u6001\u5206\u89e3\u8f6c\u5316\u4e3a\u53ef\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u89e3\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u9700\u8981\u624b\u52a8\u8c03\u53c2\u7684\u95ee\u9898\uff0c\u5728\u5927\u578b\u65f6\u7a7a\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u65f6\u7a7a\u9884\u6d4b\u4e2d\u9762\u4e34\u590d\u6742\u6ce2\u52a8\u6a21\u5f0f\u548c\u9891\u8c31\u7ea0\u7f20\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u5206\u89e3\u96c6\u6210\u65b9\u6cd5\u5982VMGCN\u867d\u7136\u63d0\u9ad8\u4e86\u7cbe\u5ea6\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u9700\u8981\u624b\u52a8\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u6a21\u5f0f\u81ea\u9002\u5e94\u56fe\u7f51\u7edc(MAGN)\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u5c55\u5f00\u53d8\u5206\u6a21\u6001\u5206\u89e3(UVMD)\u6a21\u5757\uff0c\u7528\u56fa\u5b9a\u6df1\u5ea6\u7f51\u7edc\u66ff\u4ee3\u8fed\u4ee3\u4f18\u5316\uff1b2) \u6a21\u5f0f\u7279\u5b9a\u7684\u53ef\u5b66\u4e60\u5e26\u5bbd\u7ea6\u675f\uff0c\u81ea\u9002\u5e94\u7a7a\u95f4\u5f02\u8d28\u6027\u5e76\u9632\u6b62\u9891\u8c31\u91cd\u53e0\u3002", "result": "\u5728LargeST\u57fa\u51c6\u6d4b\u8bd5(6,902\u4e2a\u4f20\u611f\u5668\uff0c2.41\u4ebf\u89c2\u6d4b\u503c)\u4e0a\uff0cMAGN\u76f8\u6bd4VMGCN\u51cf\u5c11\u4e8685-95%\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u5206\u89e3\u65f6\u95f4\u51cf\u5c11\u4e86250\u500d\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MAGN\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u89e3\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u548c\u8c03\u53c2\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u65f6\u7a7a\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.02275", "abs": "https://arxiv.org/abs/2509.02275", "authors": ["Fengyi Wang", "Xiangyu Fu", "Nitish Thakor", "Gordon Cheng"], "title": "Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object and Pose Recognition Using Multimodal Signals", "comment": null, "summary": "The human somatosensory system integrates multimodal sensory feedback,\nincluding tactile, proprioceptive, and thermal signals, to enable comprehensive\nperception and effective interaction with the environment. Inspired by the\nbiological mechanism, we present a sensorized soft anthropomorphic hand\nequipped with diverse sensors designed to emulate the sensory modalities of the\nhuman hand. This system incorporates biologically inspired encoding schemes\nthat convert multimodal sensory data into spike trains, enabling\nhighly-efficient processing through Spiking Neural Networks (SNNs). By\nutilizing these neuromorphic signals, the proposed framework achieves 97.14%\naccuracy in object recognition across varying poses, significantly\noutperforming previous studies on soft hands. Additionally, we introduce a\nnovel differentiator neuron model to enhance material classification by\ncapturing dynamic thermal responses. Our results demonstrate the benefits of\nmultimodal sensory fusion and highlight the potential of neuromorphic\napproaches for achieving efficient, robust, and human-like perception in\nrobotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u914d\u5907\u591a\u6a21\u6001\u4f20\u611f\u5668\u7684\u4eff\u4eba\u8f6f\u4f53\u624b\uff0c\u91c7\u7528\u795e\u7ecf\u5f62\u6001\u7f16\u7801\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5904\u7406\uff0c\u5728\u7269\u4f53\u8bc6\u522b\u548c\u6750\u6599\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u53d7\u4eba\u7c7b\u4f53\u611f\u7cfb\u7edf\u6574\u5408\u89e6\u89c9\u3001\u672c\u4f53\u611f\u89c9\u548c\u6e29\u5ea6\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u611f\u77e5\u673a\u5236\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u7c7b\u4eba\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u7684\u4eff\u4eba\u8f6f\u4f53\u624b\uff0c\u91c7\u7528\u751f\u7269\u542f\u53d1\u7684\u7f16\u7801\u65b9\u6848\u5c06\u591a\u6a21\u6001\u611f\u89c9\u6570\u636e\u8f6c\u6362\u4e3a\u8109\u51b2\u5e8f\u5217\uff0c\u5229\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u9ad8\u6548\u5904\u7406\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u578b\u5fae\u5206\u795e\u7ecf\u5143\u6a21\u578b\u6765\u6355\u6349\u52a8\u6001\u70ed\u54cd\u5e94\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u4e0d\u540c\u59ff\u6001\u4e0b\u7684\u7269\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u523097.14%\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u8f6f\u4f53\u624b\u7814\u7a76\uff0c\u540c\u65f6\u5728\u6750\u6599\u5206\u7c7b\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u591a\u6a21\u6001\u611f\u89c9\u878d\u5408\u548c\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728\u5b9e\u73b0\u673a\u5668\u4eba\u7cfb\u7edf\u9ad8\u6548\u3001\u9c81\u68d2\u548c\u7c7b\u4eba\u611f\u77e5\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.00797", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.00797", "abs": "https://arxiv.org/abs/2509.00797", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt"], "title": "ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods", "comment": null, "summary": "Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining\nthat focuses on optimizing processes through real-time interventions based on\nevent log data. Evaluating PresPM methods is challenging due to the lack of\nground-truth outcomes for all intervention actions in datasets. A generative\ndeep learning approach from the field of Causal Inference (CI), RealCause, has\nbeen commonly used to estimate the outcomes for proposed intervention actions\nto evaluate a new policy. However, RealCause overlooks the temporal\ndependencies in process data, and relies on a single CI model architecture,\nTARNet, limiting its effectiveness. To address both shortcomings, we introduce\nProCause, a generative approach that supports both sequential (e.g., LSTMs) and\nnon-sequential models while integrating multiple CI architectures (S-Learner,\nT-Learner, TARNet, and an ensemble). Our research using a simulator with known\nground truths reveals that TARNet is not always the best choice; instead, an\nensemble of models offers more consistent reliability, and leveraging LSTMs\nshows potential for improved evaluations when temporal dependencies are\npresent. We further validate ProCause's practical effectiveness through a\nreal-world data analysis, ensuring a more reliable evaluation of PresPM\nmethods.", "AI": {"tldr": "ProCause\u662f\u4e00\u4e2a\u65b0\u7684\u751f\u6210\u5f0f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c4\u8303\u6027\u6d41\u7a0b\u76d1\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u56e0\u679c\u63a8\u7406\u67b6\u6784\u548c\u65f6\u5e8f\u6a21\u578b\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684RealCause\u65b9\u6cd5\u5728\u8bc4\u4f30\u89c4\u8303\u6027\u6d41\u7a0b\u76d1\u63a7\u65b9\u6cd5\u65f6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5ffd\u7565\u6d41\u7a0b\u6570\u636e\u4e2d\u7684\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u4ee5\u53ca\u4ec5\u4f9d\u8d56\u5355\u4e00\u7684TARNet\u56e0\u679c\u63a8\u7406\u6a21\u578b\u67b6\u6784\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u63d0\u51faProCause\u65b9\u6cd5\uff0c\u652f\u6301\u5e8f\u5217\u6a21\u578b\uff08\u5982LSTM\uff09\u548c\u975e\u5e8f\u5217\u6a21\u578b\uff0c\u6574\u5408\u591a\u79cd\u56e0\u679c\u63a8\u7406\u67b6\u6784\uff08S-Learner\u3001T-Learner\u3001TARNet\u548c\u96c6\u6210\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u548c\u771f\u5b9e\u6570\u636e\u5206\u6790\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0TARNet\u5e76\u975e\u603b\u662f\u6700\u4f73\u9009\u62e9\uff0c\u96c6\u6210\u6a21\u578b\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u53ef\u9760\u6027\uff0c\u5f53\u5b58\u5728\u65f6\u5e8f\u4f9d\u8d56\u6027\u65f6\uff0c\u4f7f\u7528LSTM\u6a21\u578b\u53ef\u4ee5\u6539\u5584\u8bc4\u4f30\u6548\u679c\u3002\u771f\u5b9e\u6570\u636e\u5206\u6790\u9a8c\u8bc1\u4e86ProCause\u7684\u5b9e\u7528\u6709\u6548\u6027\u3002", "conclusion": "ProCause\u4e3a\u89c4\u8303\u6027\u6d41\u7a0b\u76d1\u63a7\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u652f\u6301\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u65f6\u5e8f\u5904\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u786e\u4fdd\u4e86\u8bc4\u4f30\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.01028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01028", "abs": "https://arxiv.org/abs/2509.01028", "authors": ["Zixin Zhu", "Kevin Duarte", "Mamshad Nayeem Rizve", "Chengyuan Xu", "Ratheesh Kalarot", "Junsong Yuan"], "title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation", "comment": "Accepted by ICCV 2025", "summary": "In text-to-image (T2I) generation, achieving fine-grained control over\nattributes - such as age or smile - remains challenging, even with detailed\ntext prompts. Slider-based methods offer a solution for precise control of\nimage attributes. Existing approaches typically train individual adapter for\neach attribute separately, overlooking the entanglement among multiple\nattributes. As a result, interference occurs among different attributes,\npreventing precise control of multiple attributes together. To address this\nchallenge, we aim to disentangle multiple attributes in slider-based generation\nto enbale more reliable and independent attribute manipulation. Our approach,\nCompSlider, can generate a conditional prior for the T2I foundation model to\ncontrol multiple attributes simultaneously. Furthermore, we introduce novel\ndisentanglement and structure losses to compose multiple attribute changes\nwhile maintaining structural consistency within the image. Since CompSlider\noperates in the latent space of the conditional prior and does not require\nretraining the foundation model, it reduces the computational burden for both\ntraining and inference. We evaluate our approach on a variety of image\nattributes and highlight its generality by extending to video generation.", "AI": {"tldr": "CompSlider\u662f\u4e00\u4e2a\u57fa\u4e8e\u6ed1\u5757\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u591a\u4e2a\u5c5e\u6027\u6765\u5b9e\u73b0\u7cbe\u786e\u7684\u591a\u5c5e\u6027\u540c\u65f6\u63a7\u5236\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u6ed1\u5757\u65b9\u6cd5\u901a\u5e38\u4e3a\u6bcf\u4e2a\u5c5e\u6027\u5355\u72ec\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u5ffd\u7565\u4e86\u591a\u4e2a\u5c5e\u6027\u4e4b\u95f4\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u540c\u5c5e\u6027\u95f4\u5b58\u5728\u5e72\u6270\uff0c\u65e0\u6cd5\u5b9e\u73b0\u591a\u5c5e\u6027\u7684\u7cbe\u786e\u8054\u5408\u63a7\u5236\u3002", "method": "\u63d0\u51faCompSlider\u65b9\u6cd5\uff0c\u5728\u6761\u4ef6\u5148\u9a8c\u7a7a\u95f4\u4e2d\u751f\u6210\u6761\u4ef6\u5148\u9a8c\u6765\u63a7\u5236\u591a\u4e2a\u5c5e\u6027\uff0c\u5f15\u5165\u65b0\u9896\u7684\u89e3\u8026\u635f\u5931\u548c\u7ed3\u6784\u635f\u5931\u6765\u7ec4\u5408\u591a\u4e2a\u5c5e\u6027\u53d8\u5316\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u56fe\u50cf\u5c5e\u6027\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u89c6\u9891\u751f\u6210\u9886\u57df\u3002", "conclusion": "CompSlider\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u548c\u72ec\u7acb\u7684\u5c5e\u6027\u64cd\u63a7\u3002"}}
{"id": "2509.01968", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01968", "abs": "https://arxiv.org/abs/2509.01968", "authors": ["Therese Joseph", "Tobias Fischer", "Michael Milford"], "title": "Ensemble-Based Event Camera Place Recognition Under Varying Illumination", "comment": null, "summary": "Compared to conventional cameras, event cameras provide a high dynamic range\nand low latency, offering greater robustness to rapid motion and challenging\nlighting conditions. Although the potential of event cameras for visual place\nrecognition (VPR) has been established, developing robust VPR frameworks under\nsevere illumination changes remains an open research problem. In this paper, we\nintroduce an ensemble-based approach to event camera place recognition that\ncombines sequence-matched results from multiple event-to-frame reconstructions,\nVPR feature extractors, and temporal resolutions. Unlike previous event-based\nensemble methods, which only utilise temporal resolution, our broader fusion\nstrategy delivers significantly improved robustness under varied lighting\nconditions (e.g., afternoon, sunset, night), achieving a 57% relative\nimprovement in Recall@1 across day-night transitions. We evaluate our approach\non two long-term driving datasets (with 8 km per traverse) without metric\nsubsampling, thereby preserving natural variations in speed and stop duration\nthat influence event density. We also conduct a comprehensive analysis of key\ndesign choices, including binning strategies, polarity handling, reconstruction\nmethods, and feature extractors, to identify the most critical components for\nrobust performance. Additionally, we propose a modification to the standard\nsequence matching framework that enhances performance at longer sequence\nlengths. To facilitate future research, we will release our codebase and\nbenchmarking framework.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5b66\u4e60\u7684\u4e8b\u4ef6\u76f8\u673a\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u4e8b\u4ef6\u5230\u5e27\u91cd\u5efa\u3001VPR\u7279\u5f81\u63d0\u53d6\u5668\u548c\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5728\u4e25\u91cd\u5149\u7167\u53d8\u5316\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u56de\u5f52\u51c6\u786e\u5ea6\uff0c\u8fbe\u5230\u4e86\u767d\u5929\u5230\u591c\u665a\u8fc7\u6e21\u60c5\u51b5\u4e0b57%\u7684\u76f8\u5bf9\u6536\u76ca\u3002", "motivation": "\u867d\u7136\u4e8b\u4ef6\u76f8\u673a\u5728\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u4e25\u91cd\u5149\u7167\u53d8\u5316\u6761\u4ef6\u4e0b\u5f00\u53d1\u7a33\u5065\u7684VPR\u6846\u67b6\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u66f4\u52a0\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u5904\u7406\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u5730\u70b9\u8bc6\u522b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u4e8b\u4ef6\u5230\u5e27\u91cd\u5efa\u65b9\u6cd5\u3001VPR\u7279\u5f81\u63d0\u53d6\u5668\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u4e0e\u4ee5\u524d\u4ec5\u4f7f\u7528\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u96c6\u6210\u65b9\u6cd5\u4e0d\u540c\uff0c\u8fd9\u79cd\u66f4\u5e7f\u6cdb\u7684\u878d\u5408\u7b56\u7565\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002\u8fd8\u63d0\u51fa\u4e86\u5bf9\u6807\u51c6\u5e8f\u5217\u5339\u914d\u6846\u67b6\u7684\u4fee\u6539\uff0c\u4ee5\u5728\u66f4\u957f\u5e8f\u5217\u957f\u5ea6\u4e0b\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u57282\u4e2a\u957f\u671f\u9a7e\u9a76\u6570\u636e\u96c6\uff088\u516c\u91cc\u6bcf\u6b21\u8f86\u8fc7\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4fdd\u6301\u4e86\u901f\u5ea6\u548c\u505c\u6b7b\u6301\u7eed\u65f6\u95f4\u7684\u81ea\u7136\u53d8\u5316\u3002\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\uff08\u4e0b\u5348\u3001\u9ec4\u660f\u3001\u591c\u665a\uff09\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7a33\u5065\u6027\uff0c\u5728\u767d\u5929\u5230\u591c\u665a\u8fc7\u6e21\u60c5\u51b5\u4e0bRecall@1\u6307\u6807\u83b7\u5f9757%\u7684\u76f8\u5bf9\u6536\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u4ef6\u76f8\u673a\u5730\u70b9\u8bc6\u522b\u5728\u4e25\u91cd\u5149\u7167\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u6027\u3002\u901a\u8fc7\u5bf9\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u7684\u5168\u9762\u5206\u6790\uff0c\u786e\u5b9a\u4e86\u5bf9\u7a33\u5065\u6027\u80fd\u6700\u4e3a\u5173\u952e\u7684\u7ec4\u4ef6\u3002\u8be5\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u76f8\u673aVPR\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2509.01098", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.01098", "abs": "https://arxiv.org/abs/2509.01098", "authors": ["Zhijie Zhong", "Zhiwen Yu", "Yiu-ming Cheung", "Kaixiang Yang"], "title": "CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection", "comment": "17 pages, 10 figures, 6 tables", "summary": "Time Series Anomaly Detection metrics serve as crucial tools for model\nevaluation. However, existing metrics suffer from several limitations:\ninsufficient discriminative power, strong hyperparameter dependency,\nsensitivity to perturbations, and high computational overhead. This paper\nintroduces Confidence-Consistency Evaluation (CCE), a novel evaluation metric\nthat simultaneously measures prediction confidence and uncertainty consistency.\nBy employing Bayesian estimation to quantify the uncertainty of anomaly scores,\nwe construct both global and event-level confidence and consistency scores for\nmodel predictions, resulting in a concise CCE metric. Theoretically and\nexperimentally, we demonstrate that CCE possesses strict boundedness, Lipschitz\nrobustness against score perturbations, and linear time complexity\n$\\mathcal{O}(n)$. Furthermore, we establish RankEval, a benchmark for comparing\nthe ranking capabilities of various metrics. RankEval represents the first\nstandardized and reproducible evaluation pipeline that enables objective\ncomparison of evaluation metrics. Both CCE and RankEval implementations are\nfully open-source.", "AI": {"tldr": "\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u8bc6\u522b\u529b\u4e0d\u8db3\u3001\u8d85\u53c2\u6570\u4f9d\u8d56\u6027\u5f3a\u3001\u5bf9\u5e72\u6270\u654f\u611f\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7b49\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4fe1\u5fc3\u4e00\u81f4\u6027\u8bc4\u4f30(CCE)\u6307\u6807\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f30\u8ba1\u91cf\u5316\u5f02\u5e38\u5206\u6570\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6784\u5efa\u5168\u5c40\u548c\u4e8b\u4ef6\u7ea7\u7684\u4fe1\u5fc3\u4e00\u81f4\u6027\u8bc4\u5206\uff0c\u5177\u6709\u4e25\u683c\u6709\u754c\u6027\u3001Lipschitz\u7a33\u5065\u6027\u548c\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u540c\u65f6\u5efa\u7acbRankEval\u6392\u540d\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4e3a\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u6807\u51c6\u5316\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u56db\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u8bc6\u522b\u529b\u4e0d\u8db3\u3001\u5f3a\u8d85\u53c2\u6570\u4f9d\u8d56\u6027\u3001\u5bf9\u5e72\u6270\u654f\u611f\u6027\u9ad8\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u8fd9\u4e9b\u9650\u5236\u4e86\u6a21\u578b\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4fe1\u5fc3\u4e00\u81f4\u6027\u8bc4\u4f30(CCE)\u6307\u6807\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f30\u8ba1\u91cf\u5316\u5f02\u5e38\u5206\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u65b9\u6cd5\u5305\u62ec\uff1a1)\u6784\u5efa\u5168\u5c40\u548c\u4e8b\u4ef6\u7ea7\u7684\u4fe1\u5fc3\u5206\u6570\uff1b2)\u6784\u5efa\u4e00\u81f4\u6027\u5206\u6570\uff1b3)\u7ec4\u5408\u6210\u7b80\u6d01\u7684CCE\u6307\u6807\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660eCCE\u5177\u6709\uff1a1)\u4e25\u683c\u6709\u754c\u6027\uff1b2)Lipschitz\u7a33\u5065\u6027\uff08\u5bf9\u5206\u6570\u5e72\u6270\u5177\u6709\u7a33\u5065\u6027\uff09\uff1b3)\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6O(n)\u3002\u540c\u65f6\u5efa\u7acb\u4e86RankEval\u6392\u540d\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4e3a\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u6807\u51c6\u5316\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6d41\u7a0b\u3002", "conclusion": "CCE\u6307\u6807\u5145\u5206\u89e3\u51b3\u4e86\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u6307\u6807\u7684\u56db\u5927\u95ee\u9898\uff0c\u5177\u6709\u4f18\u79c0\u7684\u6570\u5b66\u7279\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002RankEval\u57fa\u51c6\u7684\u5efa\u7acb\u4e3a\u8bc4\u4f30\u6307\u6807\u7684\u5bf9\u6bd4\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5e73\u53f0\u3002\u4e24\u8005\u7684\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u4e86\u9886\u57df\u7684\u53ef\u590d\u73b0\u7814\u7a76\u3002"}}
{"id": "2509.01181", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01181", "abs": "https://arxiv.org/abs/2509.01181", "authors": ["Qiaoqiao Jin", "Siming Fu", "Dong She", "Weinan Jia", "Hualiang Wang", "Mu Liu", "Jidong Jiang"], "title": "FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus", "comment": null, "summary": "Multi-subject personalized image generation aims to synthesize customized\nimages containing multiple specified subjects without requiring test-time\noptimization. However, achieving fine-grained independent control over multiple\nsubjects remains challenging due to difficulties in preserving subject fidelity\nand preventing cross-subject attribute leakage. We present FocusDPO, a\nframework that adaptively identifies focus regions based on dynamic semantic\ncorrespondence and supervision image complexity. During training, our method\nprogressively adjusts these focal areas across noise timesteps, implementing a\nweighted strategy that rewards information-rich patches while penalizing\nregions with low prediction confidence. The framework dynamically adjusts focus\nallocation during the DPO process according to the semantic complexity of\nreference images and establishes robust correspondence mappings between\ngenerated and reference subjects. Extensive experiments demonstrate that our\nmethod substantially enhances the performance of existing pre-trained\npersonalized generation models, achieving state-of-the-art results on both\nsingle-subject and multi-subject personalized image synthesis benchmarks. Our\nmethod effectively mitigates attribute leakage while preserving superior\nsubject fidelity across diverse generation scenarios, advancing the frontier of\ncontrollable multi-subject image synthesis.", "AI": {"tldr": "FocusDPO\u662f\u4e00\u4e2a\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bed\u4e49\u5bf9\u5e94\u548c\u81ea\u9002\u5e94\u7126\u70b9\u533a\u57df\u8bc6\u522b\uff0c\u5728\u65e0\u9700\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u591a\u4e3b\u4f53\u7684\u7cbe\u7ec6\u72ec\u7acb\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u4e3b\u4f53\u4fdd\u771f\u5ea6\u4fdd\u6301\u548c\u8de8\u4e3b\u4f53\u5c5e\u6027\u6cc4\u6f0f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u72ec\u7acb\u63a7\u5236\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u52a8\u6001\u8bed\u4e49\u5bf9\u5e94\u548c\u76d1\u7763\u56fe\u50cf\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8bc6\u522b\u7126\u70b9\u533a\u57df\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6839\u636e\u566a\u58f0\u65f6\u95f4\u6b65\u6e10\u8fdb\u8c03\u6574\u7126\u70b9\u533a\u57df\uff0c\u91c7\u7528\u52a0\u6743\u7b56\u7565\u5956\u52b1\u4fe1\u606f\u4e30\u5bcc\u533a\u57df\u5e76\u60e9\u7f5a\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\u3002", "result": "\u5728\u73b0\u6709\u9884\u8bad\u7ec3\u4e2a\u6027\u5316\u751f\u6210\u6a21\u578b\u57fa\u7840\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u5355\u4e3b\u4f53\u548c\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728\u4e0d\u540c\u751f\u6210\u573a\u666f\u4e0b\u4fdd\u6301\u4f18\u5f02\u7684\u4e3b\u4f53\u4fdd\u771f\u5ea6\uff0c\u63a8\u52a8\u4e86\u53ef\u63a7\u591a\u4e3b\u4f53\u56fe\u50cf\u5408\u6210\u7684\u524d\u6cbf\u53d1\u5c55\u3002"}}
{"id": "2509.01257", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.01257", "abs": "https://arxiv.org/abs/2509.01257", "authors": ["Andrea Fox", "Francesco De Pellegrini", "Eitan Altman"], "title": "Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks", "comment": "Submitted at AI4NextG @ NeurIPS'25 Workshop", "summary": "In edge computing systems, autonomous agents must make fast local decisions\nwhile competing for shared resources. Existing MARL methods often resume to\ncentralized critics or frequent communication, which fail under limited\nobservability and communication constraints. We propose a decentralized\nframework in which each agent solves a constrained Markov decision process\n(CMDP), coordinating implicitly through a shared constraint vector. For the\nspecific case of offloading, e.g., constraints prevent overloading shared\nserver resources. Coordination constraints are updated infrequently and act as\na lightweight coordination mechanism. They enable agents to align with global\nresource usage objectives but require little direct communication. Using safe\nreinforcement learning, agents learn policies that meet both local and global\ngoals. We establish theoretical guarantees under mild assumptions and validate\nour approach experimentally, showing improved performance over centralized and\nindependent baselines, especially in large-scale settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7ea6\u675f\u5411\u91cf\u5b9e\u73b0\u9690\u5f0f\u534f\u8c03\uff0c\u89e3\u51b3\u4e86\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u8d44\u6e90\u7ade\u4e89\u95ee\u9898\uff0c\u5728\u6709\u9650\u89c2\u6d4b\u548c\u901a\u4fe1\u7ea6\u675f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u4e2d\uff0c\u81ea\u4e3b\u667a\u80fd\u4f53\u9700\u8981\u5728\u5171\u4eab\u8d44\u6e90\u7ade\u4e89\u73af\u5883\u4e0b\u5feb\u901f\u505a\u51fa\u672c\u5730\u51b3\u7b56\u3002\u73b0\u6709MARL\u65b9\u6cd5\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u8bc4\u8bba\u5bb6\u6216\u9891\u7e41\u901a\u4fe1\uff0c\u5728\u6709\u9650\u53ef\u89c2\u6d4b\u6027\u548c\u901a\u4fe1\u7ea6\u675f\u4e0b\u6548\u679c\u4e0d\u4f73", "method": "\u6bcf\u4e2a\u667a\u80fd\u4f53\u89e3\u51b3\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(CMDP)\uff0c\u901a\u8fc7\u5171\u4eab\u7ea6\u675f\u5411\u91cf\u8fdb\u884c\u9690\u5f0f\u534f\u8c03\u3002\u7ea6\u675f\u6761\u4ef6\u9632\u6b62\u5171\u4eab\u670d\u52a1\u5668\u8d44\u6e90\u8fc7\u8f7d\uff0c\u7ea6\u675f\u66f4\u65b0\u9891\u7387\u4f4e\uff0c\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u534f\u8c03\u673a\u5236\u3002\u4f7f\u7528\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4f7f\u667a\u80fd\u4f53\u5b66\u4e60\u6ee1\u8db3\u672c\u5730\u548c\u5168\u5c40\u76ee\u6807\u7684\u7b56\u7565", "result": "\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5927\u578b\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u548c\u72ec\u7acb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u7ea6\u675f\u673a\u5236\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u9690\u5f0f\u534f\u8c03\uff0c\u5728\u6709\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u5927\u89c4\u6a21\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f"}}
{"id": "2509.01259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01259", "abs": "https://arxiv.org/abs/2509.01259", "authors": ["Thinh-Phuc Nguyen", "Thanh-Hai Nguyen", "Gia-Huy Dinh", "Lam-Huy Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization", "comment": "ACM Multimedia 2025", "summary": "Image captioning systems often produce generic descriptions that fail to\ncapture event-level semantics which are crucial for applications like news\nreporting and digital archiving. We present ReCap, a novel pipeline for\nevent-enriched image retrieval and captioning that incorporates broader\ncontextual information from relevant articles to generate narrative-rich,\nfactually grounded captions. Our approach addresses the limitations of standard\nvision-language models that typically focus on visible content while missing\ntemporal, social, and historical contexts. ReCap comprises three integrated\ncomponents: (1) a robust two-stage article retrieval system using DINOv2\nembeddings with global feature similarity for initial candidate selection\nfollowed by patch-level mutual nearest neighbor similarity re-ranking; (2) a\ncontext extraction framework that synthesizes information from article\nsummaries, generic captions, and original source metadata; and (3) a large\nlanguage model-based caption generation system with Semantic Gaussian\nNormalization to enhance fluency and relevance. Evaluated on the OpenEvents V1\ndataset as part of Track 1 in the EVENTA 2025 Grand Challenge, ReCap achieved a\nstrong overall score of 0.54666, ranking 2nd on the private test set. These\nresults highlight ReCap's effectiveness in bridging visual perception with\nreal-world knowledge, offering a practical solution for context-aware image\nunderstanding in high-stakes domains. The code is available at\nhttps://github.com/Noridom1/EVENTA2025-Event-Enriched-Image-Captioning.", "AI": {"tldr": "ReCap\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4e8b\u4ef6\u589e\u5f3a\u56fe\u50cf\u68c0\u7d22\u548c\u5b57\u5e55\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u76f8\u5173\u6587\u7ae0\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u751f\u6210\u53d9\u4e8b\u4e30\u5bcc\u3001\u4e8b\u5b9e\u51c6\u786e\u7684\u5b57\u5e55\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u65f6\u95f4\u3001\u793e\u4f1a\u548c\u5386\u53f2\u80cc\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5b57\u5e55\u7cfb\u7edf\u5f80\u5f80\u751f\u6210\u901a\u7528\u63cf\u8ff0\uff0c\u65e0\u6cd5\u6355\u6349\u5bf9\u65b0\u95fb\u62a5\u9053\u548c\u6570\u5b57\u5b58\u6863\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u7684\u4e8b\u4ef6\u7ea7\u8bed\u4e49\uff0c\u9700\u8981\u6574\u5408\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "ReCap\u5305\u542b\u4e09\u90e8\u5206\uff1a1) \u57fa\u4e8eDINOv2\u5d4c\u5165\u7684\u4e24\u9636\u6bb5\u6587\u7ae0\u68c0\u7d22\u7cfb\u7edf\uff1b2) \u4ece\u6587\u7ae0\u6458\u8981\u3001\u901a\u7528\u5b57\u5e55\u548c\u6e90\u5143\u6570\u636e\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u7684\u6846\u67b6\uff1b3) \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u91c7\u7528\u8bed\u4e49\u9ad8\u65af\u5f52\u4e00\u5316\u7684\u5b57\u5e55\u751f\u6210\u7cfb\u7edf\u3002", "result": "\u5728EVENTA 2025\u5927\u6311\u6218\u8d5bTrack 1\u7684OpenEvents V1\u6570\u636e\u96c6\u4e0a\uff0cReCap\u83b7\u5f97\u4e860.54666\u7684\u7efc\u5408\u5f97\u5206\uff0c\u5728\u79c1\u6709\u6d4b\u8bd5\u96c6\u4e0a\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "ReCap\u6709\u6548\u8fde\u63a5\u4e86\u89c6\u89c9\u611f\u77e5\u4e0e\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u56fe\u50cf\u7406\u89e3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01354", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01354", "abs": "https://arxiv.org/abs/2509.01354", "authors": ["Wei Huang", "Anda Cheng", "Zhao Zhang", "Yinggui Wang"], "title": "DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment", "comment": "Accepted by EMNLP 2025", "summary": "Current open-source training pipelines for Chinese medical language models\npredominantly emphasize optimizing training methodologies to enhance the\nperformance of large language models (LLMs), yet lack comprehensive exploration\ninto training data processing. To address this gap, we propose DPF-CM, a\nholistic Data Processing Framework for Chinese Medical LLMs training and\ndeployment. DPF-CM comprises two core modules. The first module is a data\nprocessing pipeline tailored for model training. Beyond standard data\nprocessing operations, we (1) introduce a chained examples context-learning\nstrategy to generate question-oriented instructions to mitigate the lack of\ninstruction content, and (2) implement an ensemble-based filtering mechanism\nfor preference data curation that averages multiple reward models to suppress\nnoisy samples. The second module focuses on privacy preservation during model\ndeployment. To prevent privacy risks from the inadvertent exposure of training\ndata, we propose a Privacy Preserving Vector Database (PPVD) approach, which\ninvolves model memory search, high-risk database construction, secure database\nconstruction, and match-and-replace, four key stages to minimize privacy\nleakage during inference collectively. Experimental results show that DPF-CM\nsignificantly improves model accuracy, enabling our trained Chinese medical LLM\nto achieve state-of-the-art performance among open-source counterparts.\nMoreover, the framework reduces training data privacy leakage by 27%.", "AI": {"tldr": "DPF-CM\u662f\u4e00\u4e2a\u9488\u5bf9\u4e2d\u6587\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u5168\u9762\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u5305\u542b\u8bad\u7ec3\u6570\u636e\u5904\u7406\u548c\u9690\u79c1\u4fdd\u62a4\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u7684\u4e2d\u6587\u533b\u7597\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\u4e3b\u8981\u5173\u6ce8\u8bad\u7ec3\u65b9\u6cd5\u4f18\u5316\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8bad\u7ec3\u6570\u636e\u5904\u7406\u7684\u5168\u9762\u63a2\u7d22\uff0c\u9700\u8981\u89e3\u51b3\u6307\u4ee4\u5185\u5bb9\u7f3a\u4e4f\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "method": "\u63d0\u51faDPF-CM\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a(1)\u8bad\u7ec3\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u91c7\u7528\u94fe\u5f0f\u793a\u4f8b\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u751f\u6210\u95ee\u9898\u5bfc\u5411\u6307\u4ee4\uff0c\u5e76\u4f7f\u7528\u96c6\u6210\u8fc7\u6ee4\u673a\u5236\u8fdb\u884c\u504f\u597d\u6570\u636e\u7b5b\u9009\uff1b(2)\u9690\u79c1\u4fdd\u62a4\u6a21\u5757\uff0c\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u5411\u91cf\u6570\u636e\u5e93(PPVD)\u65b9\u6cd5\uff0c\u5305\u62ec\u6a21\u578b\u8bb0\u5fc6\u641c\u7d22\u3001\u9ad8\u98ce\u9669\u6570\u636e\u5e93\u6784\u5efa\u3001\u5b89\u5168\u6570\u636e\u5e93\u6784\u5efa\u548c\u5339\u914d\u66ff\u6362\u56db\u4e2a\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDPF-CM\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u4f7f\u8bad\u7ec3\u7684\u4e2d\u6587\u533b\u7597LLM\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u6570\u636e\u9690\u79c1\u6cc4\u9732\u51cf\u5c1127%\u3002", "conclusion": "DPF-CM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e2d\u6587\u533b\u7597LLM\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u8d28\u91cf\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6570\u636e\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01541", "categories": ["cs.LG", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2509.01541", "abs": "https://arxiv.org/abs/2509.01541", "authors": ["Smayan Khanna", "Doruk Efe G\u00f6kmen", "Risi Kondor", "Vincenzo Vitelli"], "title": "Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset Size", "comment": "12 pages, 5 figures", "summary": "Graph Contrastive Learning (GCL) has emerged as a leading paradigm for self-\nsupervised learning on graphs, with strong performance reported on standardized\ndatasets and growing applications ranging from genomics to drug discovery. We\nask a basic question: does GCL actually outperform untrained baselines? We find\nthat GCL's advantage depends strongly on dataset size and task difficulty. On\nstandard datasets, untrained Graph Neural Networks (GNNs), simple multilayer\nperceptrons, and even handcrafted statistics can rival or exceed GCL. On the\nlarge molecular dataset ogbg-molhiv, we observe a crossover: GCL lags at small\nscales but pulls ahead beyond a few thousand graphs, though this gain\neventually plateaus. On synthetic datasets, GCL accuracy approximately scales\nwith the logarithm of the number of graphs and its performance gap (compared\nwith untrained GNNs) varies with respect to task complexity. Moving forward, it\nis crucial to identify the role of dataset size in benchmarks and applications,\nas well as to design GCL algorithms that avoid performance plateaus.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u56fe\u5bf9\u6bd4\u5b66\u4e60(GCL)\u7684\u4f18\u52bf\u4e25\u91cd\u4f9d\u8d56\u6570\u636e\u96c6\u5927\u5c0f\u548c\u4efb\u52a1\u96be\u5ea6\uff0c\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u672a\u8bad\u7ec3\u7684GNN\u3001\u7b80\u5355MLP\u751a\u81f3\u624b\u5de5\u7edf\u8ba1\u7279\u5f81\u90fd\u80fd\u4e0eGCL\u5ab2\u7f8e\u6216\u8d85\u8d8a\uff0c\u53ea\u6709\u5728\u5927\u578b\u5206\u5b50\u6570\u636e\u96c6\u4e0aGCL\u624d\u663e\u793a\u51fa\u4f18\u52bf", "motivation": "\u8d28\u7591GCL\u662f\u5426\u771f\u7684\u4f18\u4e8e\u672a\u7ecf\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a2\u7a76GCL\u5728\u4e0d\u540c\u6570\u636e\u96c6\u89c4\u6a21\u548c\u4efb\u52a1\u96be\u5ea6\u4e0b\u7684\u5b9e\u9645\u8868\u73b0", "method": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u3001\u5927\u578b\u5206\u5b50\u6570\u636e\u96c6(ogbg-molhiv)\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83GCL\u4e0e\u672a\u8bad\u7ec3GNN\u3001\u7b80\u5355MLP\u548c\u624b\u5de5\u7edf\u8ba1\u7279\u5f81\u7684\u6027\u80fd", "result": "GCL\u4f18\u52bf\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u5f3a\u76f8\u5173\uff1a\u5c0f\u89c4\u6a21\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8d85\u8fc7\u6570\u5343\u4e2a\u56fe\u65f6\u5f00\u59cb\u9886\u5148\u4f46\u6700\u7ec8\u8fbe\u5230\u6027\u80fd\u5e73\u53f0\uff1b\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0aGCL\u51c6\u786e\u7387\u8fd1\u4f3c\u4e0e\u56fe\u6570\u91cf\u5bf9\u6570\u6210\u6b63\u6bd4\uff0c\u6027\u80fd\u5dee\u8ddd\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u53d8\u5316", "conclusion": "\u9700\u8981\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5e94\u7528\u4e2d\u660e\u786e\u6570\u636e\u96c6\u89c4\u6a21\u7684\u4f5c\u7528\uff0c\u5e76\u8bbe\u8ba1\u907f\u514d\u6027\u80fd\u5e73\u53f0\u7684GCL\u7b97\u6cd5"}}
{"id": "2509.02510", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.02510", "abs": "https://arxiv.org/abs/2509.02510", "authors": ["Erfan Baghaei Potraghloo", "Seyedarmin Azizi", "Souvik Kundu", "Massoud Pedram"], "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation", "comment": null, "summary": "Large language models (LLMs), despite their impressive performance across a\nwide range of tasks, often struggle to balance two competing objectives in\nopen-ended text generation: fostering diversity and creativity while preserving\nlogical coherence. Existing truncated sampling techniques, including\ntemperature scaling, top-\\$p\\$ (nucleus) sampling, and min-\\$p\\$ sampling, aim\nto manage this trade-off. However, they exhibit limitations, particularly in\nthe effective incorporation of the confidence of the model into the\ncorresponding sampling strategy. For example, min-\\$p\\$ sampling relies on a\nsingle top token as a heuristic for confidence, eventually underutilizing the\ninformation of the probability distribution. Toward effective incorporation of\nthe confidence of the model, in this paper, we present **top-H** decoding. We\nfirst establish the theoretical foundation of the interplay between creativity\nand coherence in truncated sampling by formulating an **entropy-constrained\nminimum divergence** problem. We then prove this minimization problem to be\nequivalent to an **entropy-constrained mass maximization** (ECMM) problem,\nwhich is NP-hard. Finally, we present top-H decoding, a computationally\nefficient greedy algorithm to solve the ECMM problem. Extensive empirical\nevaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)\nalternative of min-\\$p\\$ sampling by up to **25.63%** on creative writing\nbenchmarks, while maintaining robustness on question-answering datasets such as\nGPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms\nthat top-H indeed produces coherent outputs even at higher temperatures, where\ncreativity is especially critical. In summary, top-H advances SoTA in\nopen-ended text generation and can be *easily integrated* into creative writing\napplications. The code is available at\nhttps://github.com/ErfanBaghaei/Top-H-Decoding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3atop-H\u89e3\u7801\u7684\u65b0\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u7ea6\u675f\u8d28\u91cf\u6700\u5927\u5316\u7406\u8bba\u89e3\u51b3LLM\u5728\u5f00\u653e\u6587\u672c\u751f\u6210\u4e2d\u5e73\u8861\u521b\u9020\u6027\u548c\u903b\u8f91\u6027\u7684\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u663e\u8457", "motivation": "\u73b0\u6709\u622a\u65ad\u91c7\u6837\u6280\u672f\uff08\u5982\u6e29\u5ea6\u7f29\u653e\u3001top-p\u91c7\u6837\u3001min-p\u91c7\u6837\uff09\u5728\u5e73\u8861\u6587\u672c\u751f\u6210\u591a\u6837\u6027\u548c\u903b\u8f91\u8fde\u8d2f\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u672a\u80fd\u6709\u6548\u5229\u7528\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4fe1\u606f", "method": "\u9996\u5148\u5efa\u7acb\u71b5\u7ea6\u675f\u6700\u5c0f\u6563\u5ea6\u95ee\u9898\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8eNP\u96be\u7684\u71b5\u7ea6\u675f\u8d28\u91cf\u6700\u5927\u5316\u95ee\u9898\uff0c\u7136\u540e\u63d0\u51fa\u8ba1\u7b97\u9ad8\u6548\u7684\u9ad8\u8d2a\u5fc3\u7b97\u6cd5top-H\u89e3\u7801\u6765\u89e3\u51b3\u8be5\u95ee\u9898", "result": "\u5728\u521b\u610f\u5199\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u6700\u5148\u8fdb\u7684min-p\u91c7\u6837\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe25.63%\uff0c\u5728GPQA\u3001GSM8K\u548cMT-Bench\u7b49\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u9c81\u68d2\u6027\uff0cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u4f30\u786e\u8ba4\u4e86\u5728\u9ad8\u6e29\u5ea6\u4e0b\u4ecd\u80fd\u4ea7\u751f\u8fde\u8d2f\u8f93\u51fa", "conclusion": "top-H\u89e3\u7801\u5728\u5f00\u653e\u6587\u672c\u751f\u6210\u65b9\u9762\u63a8\u8fdb\u4e86\u6700\u5148\u8fdb\u6280\u672f\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u521b\u610f\u5199\u4f5c\u5e94\u7528\u4e2d\uff0c\u6709\u6548\u5e73\u8861\u4e86\u521b\u9020\u6027\u548c\u8fde\u8d2f\u6027"}}
{"id": "2509.01548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01548", "abs": "https://arxiv.org/abs/2509.01548", "authors": ["Zihao Wang", "Enneng Yang", "Lu Yin", "Shiwei Liu", "Li Shen"], "title": "Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing", "comment": null, "summary": "Model merging leverages multiple finetuned expert models to construct a\nmulti-task model with low cost, and is gaining increasing attention. However,\nas a growing number of finetuned models become publicly available, concerns\nabout the safety of model merging have emerged. Unauthorized merging may\ninfringe on developers' rights and risk leaking sensitive personal information.\nMost existing methods focus on detecting whether a merged model originates from\na specific source model, but fail to effectively prevent illegal merging. In\nthis paper, we propose MergeLock, an active protection mechanism that disrupts\nmodel parameters to render them unmergeable, thereby directly preventing\nunauthorized model merging. Specifically, leveraging the inherent symmetry of\nthe attention mechanism in Transformer-based models, we randomly sample two\npairs of invertible matrices and apply them to the Query-Key (QK) and\nValue-Output (VO) branches. This transformation keeps the model's output\nunchanged while pushing it away from the shared parameter space of other\nfinetuned models. Extensive experiments across both vision and language tasks\ndemonstrate that MergeLock can degrade the performance of merged models by over\n95% when a protected model is involved in most cases, demonstrating its\neffectiveness. Moreover, we further demonstrate that merged models protected by\nMergeLock cannot be effectively recovered using low-cost restoration methods,\nfurther enhancing robustness against unauthorized merging. The code is\navailable at https://github.com/hetailang/Merge-Lock.", "AI": {"tldr": "MergeLock\u662f\u4e00\u79cd\u4e3b\u52a8\u4fdd\u62a4\u673a\u5236\uff0c\u901a\u8fc7\u6270\u4e71\u6a21\u578b\u53c2\u6570\u4f7f\u5176\u65e0\u6cd5\u88ab\u5408\u5e76\uff0c\u4ece\u800c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\u3002", "motivation": "\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u7684\u5fae\u8c03\u6a21\u578b\u516c\u5f00\u53ef\u7528\uff0c\u6a21\u578b\u5408\u5e76\u7684\u5b89\u5168\u6027\u5f15\u53d1\u5173\u6ce8\u3002\u672a\u7ecf\u6388\u6743\u7684\u5408\u5e76\u53ef\u80fd\u4fb5\u72af\u5f00\u53d1\u8005\u6743\u76ca\u5e76\u6cc4\u9732\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u68c0\u6d4b\u5408\u5e76\u6a21\u578b\u662f\u5426\u6e90\u81ea\u7279\u5b9a\u6e90\u6a21\u578b\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u9632\u6b62\u975e\u6cd5\u5408\u5e76\u3002", "method": "\u5229\u7528Transformer\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u56fa\u6709\u5bf9\u79f0\u6027\uff0c\u968f\u673a\u91c7\u6837\u4e24\u5bf9\u53ef\u9006\u77e9\u9635\u5e76\u5e94\u7528\u5230Query-Key\u548cValue-Output\u5206\u652f\uff0c\u8fd9\u79cd\u53d8\u6362\u4fdd\u6301\u6a21\u578b\u8f93\u51fa\u4e0d\u53d8\u4f46\u5c06\u5176\u63a8\u79bb\u5176\u4ed6\u5fae\u8c03\u6a21\u578b\u7684\u5171\u4eab\u53c2\u6570\u7a7a\u95f4\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMergeLock\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4f7f\u6d89\u53ca\u53d7\u4fdd\u62a4\u6a21\u578b\u7684\u5408\u5e76\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u8d85\u8fc795%\u3002\u88abMergeLock\u4fdd\u62a4\u7684\u5408\u5e76\u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u4f4e\u6210\u672c\u6062\u590d\u65b9\u6cd5\u6709\u6548\u6062\u590d\u3002", "conclusion": "MergeLock\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e3b\u52a8\u4fdd\u62a4\u673a\u5236\uff0c\u80fd\u591f\u76f4\u63a5\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\uff0c\u5177\u6709\u5f88\u597d\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.01439", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.01439", "abs": "https://arxiv.org/abs/2509.01439", "authors": ["Artur D\u00edaz-Juan", "Coloma Ballester", "Gloria Haro"], "title": "SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization", "comment": "Accepted at MMSports 2025 (Dublin, Ireland)", "summary": "Video summarization aims to extract key shots from longer videos to produce\nconcise and informative summaries. One of its most common applications is in\nsports, where highlight reels capture the most important moments of a game,\nalong with notable reactions and specific contextual events. Automatic summary\ngeneration can support video editors in the sports media industry by reducing\nthe time and effort required to identify key segments. However, the lack of\npublicly available datasets poses a challenge in developing robust models for\nsports highlight generation. In this paper, we address this gap by introducing\na curated dataset for soccer video summarization, designed to serve as a\nbenchmark for the task. The dataset includes shot boundaries for 237 matches\nfrom the Spanish, French, and Italian leagues, using broadcast footage sourced\nfrom the SoccerNet dataset. Alongside the dataset, we propose a baseline model\nspecifically designed for this task, which achieves an F1 score of 0.3956 in\nthe test set. Furthermore, we propose a new metric constrained by the length of\neach target summary, enabling a more objective evaluation of the generated\ncontent. The dataset and code are available at\nhttps://ipcv.github.io/SoccerHigh/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u8db3\u7403\u89c6\u9891\u6458\u8981\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b237\u573a\u6bd4\u8d5b\u7684\u6807\u6ce8\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u4e2a\u53d7\u6458\u8981\u957f\u5ea6\u7ea6\u675f\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u4f53\u80b2\u89c6\u9891\u6458\u8981\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u8db3\u7403\u9886\u57df\uff0c\u8fd9\u963b\u788d\u4e86\u9c81\u68d2\u6a21\u578b\u7684\u53d1\u5c55\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u8db3\u7403\u89c6\u9891\u6458\u8981\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528SoccerNet\u6570\u636e\u96c6\u4e2d\u7684\u5e7f\u64ad\u89c6\u9891\u7d20\u6750\uff0c\u4e3a\u897f\u73ed\u7259\u3001\u6cd5\u56fd\u548c\u610f\u5927\u5229\u8054\u8d5b\u7684237\u573a\u6bd4\u8d5b\u6807\u6ce8\u955c\u5934\u8fb9\u754c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e860.3956\u7684F1\u5206\u6570\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u53d7\u6458\u8981\u957f\u5ea6\u7ea6\u675f\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8db3\u7403\u89c6\u9891\u6458\u8981\u4efb\u52a1\u63d0\u4f9b\u4e86\u9996\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.01720", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01720", "abs": "https://arxiv.org/abs/2509.01720", "authors": ["Georgios Papoudakis", "Thomas Coste", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control", "comment": null, "summary": "Reinforcement learning (RL) using foundation models for policy approximations\nin multi-turn tasks remains challenging. We identify two main limitations\nrelated to sparse reward settings and policy gradient updates, based on which\nwe formulate a key insight: updates from positive samples with high returns\ntypically do not require policy regularisation, whereas updates from negative\nsamples, reflecting undesirable behaviour, can harm model performance. This\npaper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RL\nalgorithm evaluated on mobile app control tasks. SoLS improves sample\nefficiency when fine-tuning foundation models for user interface navigation via\na modified off-policy actor-critic approach, applying direct policy updates for\npositive samples and conservative, regularised updates for negative ones to\nprevent model degradation. We augment SoLS with Successful Transition Replay\n(STR), which prioritises learning from successful interactions, further\nimproving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark,\nwhere it significantly outperforms existing methods (at least 17% relative\nincrease), including prompt-engineering and RL approaches, while requiring\nsubstantially fewer computational resources than GPT-4o-based methods with\n5-60x faster inference.", "AI": {"tldr": "SoLS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u533a\u5206\u6b63\u8d1f\u6837\u672c\u91c7\u7528\u4e0d\u540c\u7684\u7b56\u7565\u66f4\u65b0\u65b9\u5f0f\uff0c\u5728\u79fb\u52a8\u5e94\u7528\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\u5b58\u5728\u7a00\u758f\u5956\u52b1\u548c\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u8d1f\u6837\u672c\u66f4\u65b0\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd", "method": "\u63d0\u51faSoLS\u7b97\u6cd5\uff1a\u5bf9\u6b63\u6837\u672c\u91c7\u7528\u76f4\u63a5\u7b56\u7565\u66f4\u65b0\uff0c\u5bf9\u8d1f\u6837\u672c\u91c7\u7528\u4fdd\u5b88\u6b63\u5219\u5316\u66f4\u65b0\uff1b\u5e76\u5f15\u5165\u6210\u529f\u8f6c\u6362\u56de\u653e(STR)\u673a\u5236\u4f18\u5148\u5b66\u4e60\u6210\u529f\u4ea4\u4e92", "result": "\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u76f8\u5bf9\u63d0\u5347\u81f3\u5c1117%\uff09\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\u5e45\u51cf\u5c11\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4GPT-4o\u65b9\u6cd5\u5feb5-60\u500d", "conclusion": "SoLS\u901a\u8fc7\u667a\u80fd\u533a\u5206\u6b63\u8d1f\u6837\u672c\u7684\u66f4\u65b0\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u79fb\u52a8\u5e94\u7528\u754c\u9762\u5bfc\u822a"}}
{"id": "2509.01794", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01794", "abs": "https://arxiv.org/abs/2509.01794", "authors": ["Trusting Inekwe", "Emmanuel Agu", "Winnie Mkandawire", "Andres Colubri"], "title": "A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics", "comment": null, "summary": "The COVID-19 pandemic disrupted healthcare systems worldwide,\ndisproportionately impacting individuals with chronic conditions such as\ncardiovascular disease (CVD). These disruptions -- through delayed care and\nbehavioral changes, affected key CVD biomarkers, including LDL cholesterol\n(LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of\nthese changes is crucial for predicting disease progression and guiding\npreventive care. However, prior work has not addressed multi-target prediction\nof CVD biomarker from Electronic Health Records (EHRs) using machine learning\n(ML), while jointly capturing biomarker interdependencies, temporal patterns,\nand predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target\nBayesian Transformer (MBT) with pre-trained BERT-based transformer framework to\njointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The\nmodel leverages Bayesian Variational Inference to estimate uncertainties,\nembeddings to capture temporal relationships and a DeepMTR model to capture\nbiomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data\nfrom 3,390 CVD patient records (304 unique patients) in Central Massachusetts\nduring the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of\nbaselines including other BERT-based ML models, achieving an MAE of 0.00887,\nRMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model\nuncertainty, patient biomarker inter-relationships, and temporal dynamics via\nits attention and embedding mechanisms. MBT-CB's superior performance\nhighlights its potential to improve CVD biomarker prediction and support\nclinical decision-making during pandemics.", "AI": {"tldr": "\u63d0\u51fa\u4e86MBT-CB\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8eBERT\u7684\u591a\u76ee\u6807\u8d1d\u53f6\u65afTransformer\uff0c\u7528\u4e8e\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u8054\u5408\u9884\u6d4bCVD\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5728COVID-19\u75ab\u60c5\u671f\u95f4\u8868\u73b0\u51fa\u8272\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u5bf9\u5fc3\u8840\u7ba1\u75be\u75c5\u60a3\u8005\u7684\u751f\u7269\u6807\u5fd7\u7269\u4ea7\u751f\u4e86\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u51c6\u786e\u5efa\u6a21\u8fd9\u4e9b\u53d8\u5316\u6765\u9884\u6d4b\u75be\u75c5\u8fdb\u5c55\u548c\u6307\u5bfc\u9884\u9632\u62a4\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u5904\u7406\u591a\u76ee\u6807\u9884\u6d4b\u3001\u751f\u7269\u6807\u5fd7\u7269\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3001\u65f6\u95f4\u6a21\u5f0f\u548c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f00\u53d1\u4e86MBT-CB\u6a21\u578b\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u53d8\u5206\u63a8\u7406\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u5d4c\u5165\u6355\u83b7\u65f6\u95f4\u5173\u7cfb\uff0c\u91c7\u7528DeepMTR\u6a21\u578b\u6355\u6349\u751f\u7269\u6807\u5fd7\u7269\u76f8\u4e92\u5173\u7cfb\u3002\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684BERT transformer\u6846\u67b6\u8054\u5408\u9884\u6d4bLDL-C\u3001HbA1c\u3001BMI\u548cSysBP\u56db\u4e2aCVD\u751f\u7269\u6807\u5fd7\u7269\u3002", "result": "\u57283,390\u4efdCVD\u60a3\u8005\u8bb0\u5f55\u4e0a\u8bc4\u4f30\uff0cMBT-CB\u4f18\u4e8e\u5176\u4ed6BERT-based\u57fa\u7ebf\u6a21\u578b\uff0cMAE\u4e3a0.00887\uff0cRMSE\u4e3a0.0135\uff0cMSE\u4e3a0.00027\uff0c\u6709\u6548\u6355\u83b7\u4e86\u6570\u636e\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u60a3\u8005\u751f\u7269\u6807\u5fd7\u7269\u76f8\u4e92\u5173\u7cfb\u548c\u65f6\u95f4\u52a8\u6001\u3002", "conclusion": "MBT-CB\u7684\u4f18\u8d8a\u6027\u80fd\u8868\u660e\u5176\u5728\u6539\u5584CVD\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u548c\u652f\u6301\u5927\u6d41\u884c\u671f\u95f4\u4e34\u5e8a\u51b3\u7b56\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.01842", "categories": ["cs.LG", "cs.AI", "68T07", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.01842", "abs": "https://arxiv.org/abs/2509.01842", "authors": ["Qifu Wen", "Xi Zeng", "Zihan Zhou", "Shuaijun Liu", "Mehdi Hosseinzadeh", "Reza Rawassizadeh"], "title": "GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping", "comment": "16 pages, 3 figures", "summary": "Early stopping monitors global validation loss and halts all parameter\nupdates simultaneously, which is computationally costly for large transformers\ndue to the extended time required for validation inference. We propose GradES,\na novel gradient-based early stopping approach that operates within transformer\ncomponents (attention projections and Feed-Forward layer matrices). We found\nthat different components converge at varying rates during fine-tuning. GradES\ntracks the magnitude of gradients in backpropagation for these matrices during\ntraining. When a projection matrix's gradients fall below a convergence\nthreshold $\\tau$, we exclude that projection matrix from further updates\nindividually, eliminating costly validation passes while allowing slow\nconverging matrices to continue learning. By strategically freezing parameters\nwhen their gradients converge, GradES speeds up training time by\n1.57--7.22$\\times$ while simultaneously enhancing generalization through early\nprevention of overfitting, resulting in 1.2% higher average accuracy.", "AI": {"tldr": "GradES\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u65e9\u671f\u505c\u6b62\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7Transformer\u7ec4\u4ef6\u4e2d\u6295\u5f71\u77e9\u9635\u7684\u68af\u5ea6\u5e45\u5ea6\uff0c\u5728\u68af\u5ea6\u4f4e\u4e8e\u9608\u503c\u65f6\u9010\u4e2a\u51bb\u7ed3\u53c2\u6570\uff0c\u65e0\u9700\u9a8c\u8bc1\u63a8\u7406\u5373\u53ef\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65e9\u671f\u505c\u6b62\u65b9\u6cd5\u9700\u8981\u76d1\u63a7\u5168\u5c40\u9a8c\u8bc1\u635f\u5931\u5e76\u540c\u65f6\u505c\u6b62\u6240\u6709\u53c2\u6570\u66f4\u65b0\uff0c\u5bf9\u4e8e\u5927\u578bTransformer\u6765\u8bf4\u9a8c\u8bc1\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u4e0d\u540c\u7ec4\u4ef6\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6536\u655b\u901f\u5ea6\u4e0d\u540c\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u505c\u6b62\u7b56\u7565\u3002", "method": "\u5728Transformer\u7684\u6ce8\u610f\u529b\u6295\u5f71\u548cFeed-Forward\u5c42\u77e9\u9635\u4e2d\u8ddf\u8e2a\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u5e45\u5ea6\u3002\u5f53\u67d0\u4e2a\u6295\u5f71\u77e9\u9635\u7684\u68af\u5ea6\u4f4e\u4e8e\u6536\u655b\u9608\u503c\u03c4\u65f6\uff0c\u5355\u72ec\u6392\u9664\u8be5\u77e9\u9635\u7684\u66f4\u65b0\uff0c\u5141\u8bb8\u6536\u655b\u6162\u7684\u77e9\u9635\u7ee7\u7eed\u5b66\u4e60\u3002", "result": "GradES\u5c06\u8bad\u7ec3\u65f6\u95f4\u52a0\u901f1.57-7.22\u500d\uff0c\u540c\u65f6\u901a\u8fc7\u65e9\u671f\u9632\u6b62\u8fc7\u62df\u5408\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.2%\u3002", "conclusion": "\u57fa\u4e8e\u68af\u5ea6\u7684\u7ec4\u4ef6\u7ea7\u65e9\u671f\u505c\u6b62\u7b56\u7565\u80fd\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u6539\u5584\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u578bTransformer\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.01907", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01907", "abs": "https://arxiv.org/abs/2509.01907", "authors": ["Zhenyuan Chen", "Chenxi Wang", "Ningyu Zhang", "Feng Zhang"], "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events", "comment": "under review", "summary": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC.", "AI": {"tldr": "\u63d0\u51fa\u4e86RSCC\u6570\u636e\u96c6\uff0c\u5305\u542b62,315\u5bf9\u707e\u524d\u707e\u540e\u9065\u611f\u56fe\u50cf\u5bf9\u548c\u8be6\u7ec6\u6587\u672c\u6807\u6ce8\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u707e\u5bb3\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528", "motivation": "\u73b0\u6709\u9065\u611f\u6570\u636e\u96c6\u7f3a\u4e4f\u65f6\u95f4\u5e8f\u5217\u56fe\u50cf\u5bf9\u548c\u8be6\u7ec6\u6587\u672c\u6807\u6ce8\uff0c\u65e0\u6cd5\u6355\u6349\u707e\u5bb3\u52a8\u6001\u5f71\u54cd", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6RSCC\uff0c\u5305\u542b\u5730\u9707\u3001\u6d2a\u6c34\u3001\u91ce\u706b\u7b49\u591a\u79cd\u707e\u5bb3\u7684\u707e\u524d\u707e\u540e\u56fe\u50cf\u5bf9\uff0c\u5e76\u914d\u6709\u4eba\u5de5\u7f16\u5199\u7684\u8be6\u7ec6\u53d8\u5316\u63cf\u8ff0", "result": "RSCC\u6570\u636e\u96c6\u80fd\u591f\u652f\u6301\u8be6\u7ec6\u7684\u707e\u5bb3\u76f8\u5173\u5206\u6790\uff0c\u4e3a\u9065\u611f\u9886\u57df\u7684\u89c6\u89c9\u8bed\u8a00\u5e94\u7528\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "RSCC\u586b\u8865\u4e86\u9065\u611f\u6570\u636e\u5728\u65f6\u95f4\u548c\u8bed\u4e49\u4e0a\u7684\u7a7a\u767d\uff0c\u4e3a\u707e\u5bb3\u611f\u77e5\u7684\u53cc\u65f6\u76f8\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2509.01882", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.01882", "abs": "https://arxiv.org/abs/2509.01882", "authors": ["Shubham Laxmikant Deshmukh", "Matthew Wilchek", "Feras A. Batarseh"], "title": "HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision", "comment": "This paper is under peer review for IEEE Journal of Oceanic\n  Engineering", "summary": "Ongoing advancements in computer vision, particularly in pattern recognition\nand scene classification, have enabled new applications in environmental\nmonitoring. Deep learning now offers non-contact methods for assessing water\nquality and detecting contamination, both critical for disaster response and\npublic health protection. This work introduces HydroVision, a deep\nlearning-based scene classification framework that estimates optically active\nwater quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored\nDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and\nTurbidity from standard Red-Green-Blue (RGB) images of surface water.\nHydroVision supports early detection of contamination trends and strengthens\nmonitoring by regulatory agencies during external environmental stressors,\nindustrial activities, and force majeure events. The model is trained on more\nthan 500,000 seasonally varied images collected from the United States\nGeological Survey Hydrologic Imagery Visualization and Information System\nbetween 2022 and 2024. This approach leverages widely available RGB imagery as\na scalable, cost-effective alternative to traditional multispectral and\nhyperspectral remote sensing. Four state-of-the-art convolutional neural\nnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer\nare evaluated through transfer learning to identify the best-performing\narchitecture. DenseNet121 achieves the highest validation performance, with an\nR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for\nreal-world water quality monitoring across diverse conditions. While the\ncurrent model is optimized for well-lit imagery, future work will focus on\nimproving robustness under low-light and obstructed scenarios to expand its\noperational utility.", "AI": {"tldr": "HydroVision\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u573a\u666f\u5206\u7c7b\u6846\u67b6\uff0c\u4f7f\u7528RGB\u56fe\u50cf\u4f30\u8ba1\u591a\u79cd\u6c34\u8d28\u53c2\u6570\uff0c\u4e3a\u6c34\u8d28\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u6b65\u4f7f\u5f97\u975e\u63a5\u89e6\u5f0f\u6c34\u8d28\u8bc4\u4f30\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u707e\u5bb3\u54cd\u5e94\u548c\u516c\u5171\u536b\u751f\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u591a\u5149\u8c31\u548c\u8d85\u5149\u8c31\u9065\u611f\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u4e0d\u6613\u83b7\u53d6\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8d85\u8fc750\u4e07\u5f20\u5b63\u8282\u6027\u53d8\u5316\u7684RGB\u56fe\u50cf\u8bad\u7ec3\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08VGG-16\u3001ResNet50\u3001MobileNetV2\u3001DenseNet121\uff09\u548c\u4e00\u4e2aVision Transformer\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u786e\u5b9a\u6700\u4f73\u67b6\u6784\u3002", "result": "DenseNet121\u5728\u9a8c\u8bc1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u9884\u6d4bCDOM\u7684R2\u5206\u6570\u8fbe\u52300.89\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9645\u6c34\u8d28\u76d1\u6d4b\u7684\u6f5c\u529b\u3002", "conclusion": "HydroVision\u8bc1\u660e\u4e86\u4f7f\u7528\u5e7f\u6cdb\u53ef\u7528\u7684RGB\u56fe\u50cf\u8fdb\u884c\u6c34\u8d28\u76d1\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u76d1\u7ba1\u673a\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u76d1\u6d4b\u5de5\u5177\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u91cd\u70b9\u63d0\u9ad8\u5728\u4f4e\u5149\u548c\u906e\u6321\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.02061", "categories": ["cs.LG", "physics.ao-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.02061", "abs": "https://arxiv.org/abs/2509.02061", "authors": ["Haiwen Guan", "Troy Arcomano", "Ashesh Chattopadhyay", "Romit Maulik"], "title": "LUCIE-3D: A three-dimensional climate emulator for forced responses", "comment": null, "summary": "We introduce LUCIE-3D, a lightweight three-dimensional climate emulator\ndesigned to capture the vertical structure of the atmosphere, respond to\nclimate change forcings, and maintain computational efficiency with long-term\nstability. Building on the original LUCIE-2D framework, LUCIE-3D employs a\nSpherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of\nERA5 reanalysis data spanning eight vertical {\\sigma}-levels. The model\nincorporates atmospheric CO2 as a forcing variable and optionally integrates\nprescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere\ndynamics. Results demonstrate that LUCIE-3D successfully reproduces\nclimatological means, variability, and long-term climate change signals,\nincluding surface warming and stratospheric cooling under increasing CO2\nconcentrations. The model further captures key dynamical processes such as\nequatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes,\nwhile showing credible behavior in the statistics of extreme events. Despite\nrequiring longer training than its 2D predecessor, LUCIE-3D remains efficient,\ntraining in under five hours on four GPUs. Its combination of stability,\nphysical consistency, and accessibility makes it a valuable tool for rapid\nexperimentation, ablation studies, and the exploration of coupled climate\ndynamics, with potential applications extending to paleoclimate research and\nfuture Earth system emulation.", "AI": {"tldr": "LUCIE-3D\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e09\u7ef4\u6c14\u5019\u6a21\u62df\u5668\uff0c\u57fa\u4e8eSFNO\u67b6\u6784\uff0c\u80fd\u591f\u6355\u6349\u5927\u6c14\u5782\u76f4\u7ed3\u6784\uff0c\u54cd\u5e94\u6c14\u5019\u53d8\u5316\u5f3a\u8feb\uff0c\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u957f\u671f\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6a21\u62df\u5927\u6c14\u4e09\u7ef4\u7ed3\u6784\u3001\u54cd\u5e94\u6c14\u5019\u53d8\u5316\u5f3a\u8feb\u3001\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u8f7b\u91cf\u7ea7\u6c14\u5019\u6a21\u62df\u5668\uff0c\u4ee5\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\u548c\u8026\u5408\u6c14\u5019\u52a8\u529b\u5b66\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u539f\u59cbLUCIE-2D\u6846\u67b6\uff0c\u91c7\u7528\u7403\u9762\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08SFNO\uff09\u4e3b\u5e72\u7f51\u7edc\uff0c\u4f7f\u752830\u5e74ERA5\u518d\u5206\u6790\u6570\u636e\u57288\u4e2a\u5782\u76f4\u03c3\u5c42\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6574\u5408CO2\u4f5c\u4e3a\u5f3a\u8feb\u53d8\u91cf\uff0c\u5e76\u53ef\u9009\u62e9\u6574\u5408\u6d77\u8868\u6e29\u5ea6\u3002", "result": "\u6210\u529f\u518d\u73b0\u6c14\u5019\u5e73\u5747\u503c\u3001\u53d8\u5f02\u6027\u548c\u957f\u671f\u6c14\u5019\u53d8\u5316\u4fe1\u53f7\uff0c\u5305\u62ecCO2\u6d53\u5ea6\u589e\u52a0\u4e0b\u7684\u5730\u8868\u53d8\u6696\u548c\u5e73\u6d41\u5c42\u51b7\u5374\uff1b\u6355\u6349\u5173\u952e\u52a8\u529b\u5b66\u8fc7\u7a0b\u5982\u8d64\u9053\u5f00\u5c14\u6587\u6ce2\u3001MJO\u548c\u73af\u5f62\u6a21\uff1b\u5728\u6781\u7aef\u4e8b\u4ef6\u7edf\u8ba1\u4e2d\u8868\u73b0\u51fa\u53ef\u4fe1\u884c\u4e3a\uff1b\u57284\u4e2aGPU\u4e0a\u8bad\u7ec3\u65f6\u95f4\u5c11\u4e8e5\u5c0f\u65f6\u3002", "conclusion": "LUCIE-3D\u7ed3\u5408\u4e86\u7a33\u5b9a\u6027\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u662f\u5feb\u901f\u5b9e\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u548c\u8026\u5408\u6c14\u5019\u52a8\u529b\u5b66\u63a2\u7d22\u7684\u5b9d\u8d35\u5de5\u5177\uff0c\u5728\u53e4\u6c14\u5019\u7814\u7a76\u548c\u672a\u6765\u5730\u7403\u7cfb\u7edf\u6a21\u62df\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.01977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.01977", "abs": "https://arxiv.org/abs/2509.01977", "authors": ["Dong She", "Siming Fu", "Mushui Liu", "Qiaoqiao Jin", "Hualiang Wang", "Mu Liu", "Jidong Jiang"], "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement", "comment": null, "summary": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u4e2a\u9762\u5411\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u751f\u6210\u7684\u8868\u793a\u4e2d\u5fc3\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u5e94\u548c\u6b63\u4ea4\u7279\u5f81\u89e3\u8026\u89e3\u51b3\u8eab\u4efd\u6df7\u5408\u548c\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0c\u57284+\u4e3b\u4f53\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u5b58\u5728\u8eab\u4efd\u6df7\u5408\u548c\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5bf9\u591a\u4e2a\u53c2\u8003\u4e3b\u4f53\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e2d\u5982\u4f55\u4ea4\u4e92\u7684\u5145\u5206\u5efa\u6a21", "method": "\u63d0\u51fa\u8bed\u4e49\u5bf9\u5e94\u6ce8\u610f\u529b\u635f\u5931\u786e\u4fdd\u7cbe\u786e\u7684\u70b9\u5bf9\u70b9\u8bed\u4e49\u5bf9\u9f50\uff0c\u5f00\u53d1\u591a\u53c2\u8003\u89e3\u8026\u635f\u5931\u5c06\u4e0d\u540c\u4e3b\u4f53\u63a8\u5165\u6b63\u4ea4\u6ce8\u610f\u529b\u5b50\u7a7a\u95f4\u4ee5\u9632\u6b62\u7279\u5f81\u5e72\u6270\uff0c\u5e76\u5f15\u5165SemAlign-MS\u6570\u636e\u96c6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u5e94", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u57284+\u53c2\u8003\u4e3b\u4f53\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57283\u4e2a\u4e3b\u4f53\u4ee5\u4e0a\u6027\u80fd\u4e0b\u964d", "conclusion": "MOSAIC\u901a\u8fc7\u8868\u793a\u5c42\u9762\u7684\u7cbe\u786e\u8bed\u4e49\u5bf9\u9f50\u548c\u7279\u5f81\u89e3\u8026\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4e3b\u4f53\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u771f\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u591a\u4e3b\u4f53\u5408\u6210\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027"}}
{"id": "2509.02109", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.02109", "abs": "https://arxiv.org/abs/2509.02109", "authors": ["Samuel Bo\u00eft\u00e9", "Eloi Tanguy", "Julie Delon", "Agn\u00e8s Desolneux", "R\u00e9mi Flamary"], "title": "Differentiable Expectation-Maximisation and Applications to Gaussian Mixture Model Optimal Transport", "comment": null, "summary": "The Expectation-Maximisation (EM) algorithm is a central tool in statistics\nand machine learning, widely used for latent-variable models such as Gaussian\nMixture Models (GMMs). Despite its ubiquity, EM is typically treated as a\nnon-differentiable black box, preventing its integration into modern learning\npipelines where end-to-end gradient propagation is essential. In this work, we\npresent and compare several differentiation strategies for EM, from full\nautomatic differentiation to approximate methods, assessing their accuracy and\ncomputational efficiency. As a key application, we leverage this differentiable\nEM in the computation of the Mixture Wasserstein distance $\\mathrm{MW}_2$\nbetween GMMs, allowing $\\mathrm{MW}_2$ to be used as a differentiable loss in\nimaging and machine learning tasks. To complement our practical use of\n$\\mathrm{MW}_2$, we contribute a novel stability result which provides\ntheoretical justification for the use of $\\mathrm{MW}_2$ with EM, and also\nintroduce a novel unbalanced variant of $\\mathrm{MW}_2$. Numerical experiments\non barycentre computation, colour and style transfer, image generation, and\ntexture synthesis illustrate the versatility and effectiveness of the proposed\napproach in different settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EM\u7b97\u6cd5\u7684\u53ef\u5fae\u5206\u7248\u672c\uff0c\u4f7f\u5176\u80fd\u591f\u96c6\u6210\u5230\u9700\u8981\u7aef\u5230\u7aef\u68af\u5ea6\u4f20\u64ad\u7684\u73b0\u4ee3\u5b66\u4e60\u6d41\u7a0b\u4e2d\uff0c\u5e76\u5e94\u7528\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684Wasserstein\u8ddd\u79bb\u8ba1\u7b97\u3002", "motivation": "EM\u7b97\u6cd5\u5728\u7edf\u8ba1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u901a\u5e38\u88ab\u89c6\u4e3a\u4e0d\u53ef\u5fae\u5206\u7684\u9ed1\u76d2\uff0c\u65e0\u6cd5\u5728\u73b0\u4ee3\u5b66\u4e60\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u7aef\u5230\u7aef\u68af\u5ea6\u4f20\u64ad\u3002", "method": "\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u591a\u79cdEM\u7b97\u6cd5\u5fae\u5206\u7b56\u7565\uff0c\u4ece\u5b8c\u5168\u81ea\u52a8\u5fae\u5206\u5230\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u5c06\u53ef\u5fae\u5206EM\u5e94\u7528\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684Wasserstein\u8ddd\u79bb\u8ba1\u7b97\u3002", "result": "\u5728\u91cd\u5fc3\u8ba1\u7b97\u3001\u989c\u8272\u548c\u98ce\u683c\u8fc1\u79fb\u3001\u56fe\u50cf\u751f\u6210\u3001\u7eb9\u7406\u5408\u6210\u7b49\u6570\u503c\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7fEM\u7b97\u6cd5\u53ef\u5fae\u5206\uff0c\u6210\u529f\u5c06\u5176\u96c6\u6210\u5230\u73b0\u4ee3\u5b66\u4e60\u6d41\u7a0b\u4e2d\uff0c\u4e3a\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684Wasserstein\u8ddd\u79bb\u8ba1\u7b97\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u83b7\u5f97\u4e86\u7406\u8bba\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2509.02154", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.02154", "abs": "https://arxiv.org/abs/2509.02154", "authors": ["Aymene Mohammed Bouayed", "Samuel Deslauriers-Gauthier", "Adrian Iaccovelli", "David Naccache"], "title": "Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation", "comment": null, "summary": "Variational Autoencoders (VAEs) with global priors mirror the training set's\nclass frequency in latent space, underrepresenting tail classes and reducing\ngenerative fairness on imbalanced datasets. While $t^3$VAE improves robustness\nvia heavy-tailed Student's t-distribution priors, it still allocates latent\nvolume proportionally to the class frequency.In this work, we address this\nissue by explicitly enforcing equitable latent space allocation across classes.\nTo this end, we propose Conditional-$t^3$VAE, which defines a per-class\n\\mbox{Student's t} joint prior over latent and output variables, preventing\ndominance by majority classes. Our model is optimized using a closed-form\nobjective derived from the $\\gamma$-power divergence. Moreover, for\nclass-balanced generation, we derive an equal-weight latent mixture of\nStudent's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA,\nConditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE\nand Gaussian-based VAE baselines, particularly under severe class imbalance. In\nper-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional\nGaussian VAE across all highly imbalanced settings. While Gaussian-based models\nremain competitive under mild imbalance ratio ($\\rho \\lesssim 3$), our approach\nsubstantially improves generative fairness and diversity in more extreme\nregimes.", "AI": {"tldr": "Conditional-$t^3$VAE\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5b9a\u4e49Student's t\u8054\u5408\u5148\u9a8c\u5206\u5e03\uff0c\u89e3\u51b3\u4e86VAE\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u751f\u6210\u516c\u5e73\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6781\u7aef\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u4f20\u7edfVAE\u53ca\u5176\u53d8\u4f53$t^3$VAE\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u6f5c\u5728\u7a7a\u95f4\u5206\u914d\u4e0e\u8bad\u7ec3\u96c6\u7c7b\u522b\u9891\u7387\u6210\u6b63\u6bd4\uff0c\u5bfc\u81f4\u5c3e\u90e8\u7c7b\u522b\u8868\u793a\u4e0d\u8db3\uff0c\u751f\u6210\u516c\u5e73\u6027\u964d\u4f4e\u3002", "method": "\u63d0\u51faConditional-$t^3$VAE\u6a21\u578b\uff0c\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5b9a\u4e49Student's t\u8054\u5408\u5148\u9a8c\u5206\u5e03\uff0c\u4f7f\u7528\u03b3-\u6563\u5ea6\u63a8\u5bfc\u7684\u95ed\u5f0f\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u63a8\u5bfc\u7b49\u6743\u91cd\u7684\u6f5c\u5728\u6df7\u5408Student's t\u5206\u5e03\u7528\u4e8e\u7c7b\u522b\u5e73\u8861\u751f\u6210\u3002", "result": "\u5728SVHN-LT\u3001CIFAR100-LT\u548cCelebA\u6570\u636e\u96c6\u4e0a\uff0cConditional-$t^3$VAE\u59cb\u7ec8\u83b7\u5f97\u6bd4$t^3$VAE\u548c\u9ad8\u65af\u57faVAE\u57fa\u7ebf\u66f4\u4f4e\u7684FID\u5206\u6570\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u3002\u5728\u6bcf\u7c7bF1\u8bc4\u4f30\u4e2d\u4e5f\u4f18\u4e8e\u6761\u4ef6\u9ad8\u65afVAE\u3002", "conclusion": "\u867d\u7136\u9ad8\u65af\u57fa\u6a21\u578b\u5728\u8f7b\u5ea6\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u4ecd\u6709\u7ade\u4e89\u529b\uff0c\u4f46Conditional-$t^3$VAE\u5728\u66f4\u6781\u7aef\u7684\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2509.02156", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02156", "abs": "https://arxiv.org/abs/2509.02156", "authors": ["Asif Mohammed Saad", "Umme Niraj Mahi"], "title": "SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in Skin Lesion Analysis", "comment": null, "summary": "Hair artifacts in dermoscopic images present significant challenges for\naccurate skin lesion analysis, potentially obscuring critical diagnostic\nfeatures in dermatological assessments. This work introduces a fine-tuned\nSegFormer model augmented with dropout regularization to achieve precise hair\nmask segmentation. The proposed SegformerWithDropout architecture leverages the\nMiT-B2 encoder, pretrained on ImageNet, with an in-channel count of 3 and 2\noutput classes, incorporating a dropout probability of 0.3 in the segmentation\nhead to prevent overfitting. Training is conducted on a specialized dataset of\n500 dermoscopic skin lesion images with fine-grained hair mask annotations,\nemploying 10-fold cross-validation, AdamW optimization with a learning rate of\n0.001, and cross-entropy loss. Early stopping is applied based on validation\nloss, with a patience of 3 epochs and a maximum of 20 epochs per fold.\nPerformance is evaluated using a comprehensive suite of metrics, including\nIntersection over Union (IoU), Dice coefficient, Peak Signal-to-Noise Ratio\n(PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch\nSimilarity (LPIPS). Experimental results from the cross-validation demonstrate\nrobust performance, with average Dice coefficients reaching approximately 0.96\nand IoU values of 0.93, alongside favorable PSNR (around 34 dB), SSIM (0.97),\nand low LPIPS (0.06), highlighting the model's effectiveness in accurate hair\nartifact segmentation and its potential to enhance preprocessing for downstream\nskin cancer detection tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSegFormer\u7684\u6539\u8fdb\u6a21\u578bSegformerWithDropout\uff0c\u901a\u8fc7dropout\u6b63\u5219\u5316\u5b9e\u73b0\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u6bdb\u53d1\u4f2a\u5f71\u7684\u7cbe\u786e\u5206\u5272\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u7684\u6bdb\u53d1\u4f2a\u5f71\u4f1a\u906e\u6321\u5173\u952e\u8bca\u65ad\u7279\u5f81\uff0c\u5f71\u54cd\u76ae\u80a4\u75c5\u53d8\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u6bdb\u53d1\u5206\u5272\u65b9\u6cd5\u6765\u63d0\u5347\u9884\u5904\u7406\u6548\u679c\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684SegFormer\u6a21\u578b\uff0c\u91c7\u7528MiT-B2\u7f16\u7801\u5668\uff08ImageNet\u9884\u8bad\u7ec3\uff09\uff0c\u5728\u5206\u5272\u5934\u4e2d\u52a0\u51650.3\u6982\u7387\u7684dropout\u6b63\u5219\u5316\u3002\u4f7f\u7528500\u5f20\u5e26\u7cbe\u7ec6\u6bdb\u53d1\u6807\u6ce8\u7684\u76ae\u80a4\u955c\u56fe\u50cf\uff0c\u91c7\u752810\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3001AdamW\u4f18\u5316\u5668\uff08\u5b66\u4e60\u73870.001\uff09\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u8868\u73b0\u7a33\u5065\uff1a\u5e73\u5747Dice\u7cfb\u6570\u7ea60.96\uff0cIoU\u503c0.93\uff0cPSNR\u7ea634dB\uff0cSSIM 0.97\uff0cLPIPS\u4ec50.06\uff0c\u663e\u793a\u51fa\u4f18\u5f02\u7684\u6bdb\u53d1\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5206\u5272\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u7684\u6bdb\u53d1\u4f2a\u5f71\uff0c\u5177\u6709\u63d0\u5347\u4e0b\u6e38\u76ae\u80a4\u764c\u68c0\u6d4b\u4efb\u52a1\u9884\u5904\u7406\u6548\u679c\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.02281", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.02281", "abs": "https://arxiv.org/abs/2509.02281", "authors": ["Shijie Wang", "Li Zhang", "Xinyan Liang", "Yuhua Qian", "Shen Hu"], "title": "Balanced Multimodal Learning: An Unidirectional Dynamic Interaction Perspective", "comment": null, "summary": "Multimodal learning typically utilizes multimodal joint loss to integrate\ndifferent modalities and enhance model performance. However, this joint\nlearning strategy can induce modality imbalance, where strong modalities\noverwhelm weaker ones and limit exploitation of individual information from\neach modality and the inter-modality interaction information.Existing\nstrategies such as dynamic loss weighting, auxiliary objectives and gradient\nmodulation mitigate modality imbalance based on joint loss. These methods\nremain fundamentally reactive, detecting and correcting imbalance after it\narises, while leaving the competitive nature of the joint loss untouched. This\nlimitation drives us to explore a new strategy for multimodal imbalance\nlearning that does not rely on the joint loss, enabling more effective\ninteractions between modalities and better utilization of information from\nindividual modalities and their interactions. In this paper, we introduce\nUnidirectional Dynamic Interaction (UDI), a novel strategy that abandons the\nconventional joint loss in favor of a proactive, sequential training scheme.\nUDI first trains the anchor modality to convergence, then uses its learned\nrepresentations to guide the other modality via unsupervised loss. Furthermore,\nthe dynamic adjustment of modality interactions allows the model to adapt to\nthe task at hand, ensuring that each modality contributes optimally. By\ndecoupling modality optimization and enabling directed information flow, UDI\nprevents domination by any single modality and fosters effective cross-modal\nfeature learning. Our experimental results demonstrate that UDI outperforms\nexisting methods in handling modality imbalance, leading to performance\nimprovement in multimodal learning tasks.", "AI": {"tldr": "\u63d0\u51faUDI\u65b9\u6cd5\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u5411\u52a8\u6001\u4ea4\u4e92\u66ff\u4ee3\u4f20\u7edf\u8054\u5408\u635f\u5931\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6a21\u6001\u534f\u540c\u548c\u6027\u80fd\u63d0\u5347", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u4f1a\u5bfc\u81f4\u6a21\u6001\u4e0d\u5e73\u8861\uff0c\u5f3a\u52bf\u6a21\u6001\u538b\u5236\u5f31\u52bf\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5404\u6a21\u6001\u4fe1\u606f\u548c\u6a21\u6001\u95f4\u4ea4\u4e92\u4fe1\u606f\u7684\u5145\u5206\u5229\u7528", "method": "UDI\u91c7\u7528\u987a\u5e8f\u8bad\u7ec3\u7b56\u7565\uff1a\u5148\u8bad\u7ec3\u951a\u5b9a\u6a21\u6001\u81f3\u6536\u655b\uff0c\u7136\u540e\u7528\u5176\u5b66\u4e60\u8868\u793a\u901a\u8fc7\u65e0\u76d1\u7763\u635f\u5931\u6307\u5bfc\u5176\u4ed6\u6a21\u6001\uff0c\u52a8\u6001\u8c03\u6574\u6a21\u6001\u4ea4\u4e92", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eUDI\u5728\u5904\u7406\u6a21\u6001\u4e0d\u5e73\u8861\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u6539\u8fdb", "conclusion": "UDI\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u4f18\u5316\u548c\u542f\u7528\u5b9a\u5411\u4fe1\u606f\u6d41\uff0c\u6709\u6548\u9632\u6b62\u5355\u4e00\u6a21\u6001\u4e3b\u5bfc\uff0c\u4fc3\u8fdb\u8de8\u6a21\u6001\u7279\u5f81\u5b66\u4e60\uff0c\u4e3a\u591a\u6a21\u6001\u4e0d\u5e73\u8861\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
