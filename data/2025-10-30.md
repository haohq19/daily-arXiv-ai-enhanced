<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation](https://arxiv.org/abs/2510.25174)
*Huadong Tang,Youpeng Zhao,Min Xu,Jun Wang,Qiang Wu*

Main category: cs.CV

TL;DR: 提出了一种扩展上下文感知分类器(ECAC)，通过动态调整分类器来解决语义分割中固定参数分类器无法适应图像特定类别分布的问题。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割方法使用固定参数分类器，无法适应不同图像的独特类别分布，且数据集级别的类别不平衡会导致模型偏向多数类，限制了对少数类区域的识别能力。

Method: 使用内存库学习数据集级别的上下文信息，结合当前图像的类别特定上下文信息来改进分类器。采用师生网络范式，教师网络动态调整上下文信息并传递知识给学生网络。

Result: 在ADE20K、COCO-Stuff10K和Pascal-Context等多个数据集上实现了最先进的性能。

Conclusion: ECAC通过动态调整分类器，有效解决了语义分割中的类别分布差异和类别不平衡问题，显著提升了模型性能。

Abstract: Prevalent semantic segmentation methods generally adopt a vanilla classifier
to categorize each pixel into specific classes.
  Although such a classifier learns global information from the training data,
this information is represented by a set of fixed parameters (weights and
biases).
  However, each image has a different class distribution, which prevents the
classifier from addressing the unique characteristics of individual images.
  At the dataset level, class imbalance leads to segmentation results being
biased towards majority classes, limiting the model's effectiveness in
identifying and segmenting minority class regions.
  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that
dynamically adjusts the classifier using global (dataset-level) and local
(image-level) contextual information.
  Specifically, we leverage a memory bank to learn dataset-level contextual
information of each class, incorporating the class-specific contextual
information from the current image to improve the classifier for precise pixel
labeling.
  Additionally, a teacher-student network paradigm is adopted, where the domain
expert (teacher network) dynamically adjusts contextual information with ground
truth and transfers knowledge to the student network.
  Comprehensive experiments illustrate that the proposed ECAC can achieve
state-of-the-art performance across several datasets, including ADE20K,
COCO-Stuff10K, and Pascal-Context.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: BRIDGE是一个统一的端到端架构，将序列编码器与图神经网络耦合在单一目标下，通过token级交叉注意力层实现邻居序列间的事件级消息传递，在社交网络和欺诈检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常同时具有序列性和关系性，但现有方法往往忽视其中一个模态。作者认为序列和图不是分离的问题，而是同一数据集的互补方面，应该联合学习。

Method: 提出BRIDGE架构，将序列编码器与GNN在单一目标下耦合，允许梯度在两个模块间流动。添加TOKENXATTN层实现邻居序列间token级的细粒度消息传递。

Result: 在两个任务设置中（Brightkite的友谊预测和Amazon的欺诈检测），BRIDGE在排序和分类指标上持续优于静态GNN、时序图方法和纯序列基线。

Conclusion: 序列和图应该联合建模，BRIDGE通过耦合序列编码和GNN学习任务对齐的表征，证明了在关系序列数据上的有效性。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [3] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出卷积脉冲GRU（CS-GRU）单元，结合卷积操作和脉冲神经元，在保持局部结构的同时提升时序数据处理性能，在多个基准测试中优于现有GRU变体。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时会丢失局部细节，现有方法如SpikGRU无法有效捕捉基于事件的时空数据中的细粒度局部依赖关系。

Method: 设计卷积脉冲GRU（CS-GRU）单元，利用卷积操作保持局部结构和依赖关系，同时整合脉冲神经元的时间精度和GRU的高效门控机制。

Result: 在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进GRU变体提升4.35%，在MNIST上达到99.31%准确率，效率比SpikGRU高69%。

Conclusion: CS-GRU是一个多功能架构，在保持局部结构依赖性的同时，有效整合了脉冲神经元的时间精度和GRU的门控机制，在时序和时空数据处理任务中均表现出色。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: FELA是一个基于LLM的多代理进化系统，用于从复杂工业事件日志数据中自动提取高性能特征，通过专业代理协作和进化算法实现可解释的自适应特征工程。


<details>
  <summary>Details</summary>
Motivation: 工业事件日志数据复杂异构，现有自动特征工程方法存在可解释性差、操作僵化、适应性弱等问题，需要更智能的解决方案。

Method: 采用多代理系统（Idea、Code、Critic、Evaluation代理）协作，结合强化学习和遗传算法原理的进化算法，通过分层知识库和双记忆系统实现持续改进。

Result: 在真实工业数据集上的实验表明，FELA能生成可解释、领域相关的特征，显著提升模型性能并减少人工工作量。

Conclusion: 基于LLM的多代理系统为复杂现实环境中的自动化、可解释和自适应特征工程提供了通用框架。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [5] [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://arxiv.org/abs/2510.24824)
*Bohong Wu,Mengzhao Chen,Xiang Luo,Shen Yan,Qifan Yu,Fan Xia,Tianqi Zhang,Hongrui Zhan,Zheng Zhong,Xun Zhou,Siyuan Qiao,Xingyan Bin*

Main category: cs.CL

TL;DR: 提出了Parallel Loop Transformer (PLT)架构，通过跨循环并行和高效表示增强技术，在保持循环transformer性能优势的同时，实现与非循环模型相近的低延迟和内存开销。


<details>
  <summary>Details</summary>
Motivation: 循环transformer虽然能通过权重复用节省参数，但循环间的顺序依赖导致推理延迟和内存需求随循环次数增加，限制了其在快速应用中的实用性。

Method: 1. 跨循环并行(CLP)：在同一前向传播中并行计算不同token的不同循环；2. 高效表示增强：共享首个循环的KV缓存，使用门控滑动窗口注意力(G-SWA)结合全局和局部信息。

Result: 实验表明PLT实现了传统循环模型的高精度，同时与非循环transformer相比几乎没有额外的延迟或内存成本。

Conclusion: PLT成功解决了循环transformer的延迟和内存问题，为高效LLM推理提供了可行方案。

Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.

</details>
