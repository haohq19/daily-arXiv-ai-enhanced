<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LAND: A Longitudinal Analysis of Neuromorphic Datasets](https://arxiv.org/abs/2602.15973)
*Gregory Cohen,Alexandre Marcireau*

Main category: cs.CV

TL;DR: 该综述分析了423个神经形态数据集，指出当前领域面临数据标准化不足、访问困难、合成数据集激增等问题，并提出了元数据集作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 神经形态工程面临严重的数据问题：尽管数据集数量快速增长，但研究论文仍不断呼吁需要更多、更大的数据集。现有数据集存在标准化不足、难以查找和理解、下载使用困难等问题，同时合成数据集的激增可能带来新的挑战。

Method: 1. 收集并分析423个现有神经形态数据集；2. 探索这些数据集的任务性质和数据结构；3. 分析数据集大小、标准化程度和访问难度；4. 研究合成数据集（模拟或视频转事件生成）的增长趋势；5. 提出元数据集概念，通过组合现有数据集来减少对新数据的需求并消除任务定义偏差。

Result: 分析发现：1. 数据集规模不断增大但缺乏标准化；2. 数据访问存在实际困难；3. 合成数据集快速增长，虽然有助于算法测试，但可能限制新应用探索；4. 元数据集可作为减少数据需求、消除偏差的有效方法。

Conclusion: 神经形态工程领域需要更好的数据管理策略而非单纯更多数据。元数据集提供了一种减少数据需求、消除任务定义偏差的解决方案，而合成数据集的使用需要谨慎以避免限制创新应用探索。

Abstract: Neuromorphic engineering has a data problem. Despite the meteoric rise in the number of neuromorphic datasets published over the past ten years, the conclusion of a significant portion of neuromorphic research papers still states that there is a need for yet more data and even larger datasets. Whilst this need is driven in part by the sheer volume of data required by modern deep learning approaches, it is also fuelled by the current state of the available neuromorphic datasets and the difficulties in finding them, understanding their purpose, and determining the nature of their underlying task. This is further compounded by practical difficulties in downloading and using these datasets. This review starts by capturing a snapshot of the existing neuromorphic datasets, covering over 423 datasets, and then explores the nature of their tasks and the underlying structure of the presented data. Analysing these datasets shows the difficulties arising from their size, the lack of standardisation, and difficulties in accessing the actual data. This paper also highlights the growth in the size of individual datasets and the complexities involved in working with the data. However, a more important concern is the rise of synthetic datasets, created by either simulation or video-to-events methods. This review explores the benefits of simulated data for testing existing algorithms and applications, highlighting the potential pitfalls for exploring new applications of neuromorphic technologies. This review also introduces the concepts of meta-datasets, created from existing datasets, as a way of both reducing the need for more data, and to remove potential bias arising from defining both the dataset and the task.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Adaptive Semi-Supervised Training of P300 ERP-BCI Speller System with Minimum Calibration Effort](https://arxiv.org/abs/2602.15955)
*Shumeng Chen,Jane E. Huggins,Tianwen Ma*

Main category: cs.LG

TL;DR: 提出基于自适应半监督EM-GMM算法的P300 BCI拼写器框架，减少校准需求，在有限标注数据下提升拼写效率


<details>
  <summary>Details</summary>
Motivation: 传统P300 BCI拼写器需要冗长的校准过程来构建二分类器，降低了整体效率，需要减少校准工作量

Method: 提出统一框架，使用少量标注校准数据，采用自适应半监督EM-GMM算法更新二分类器

Result: 15名参与者中，9人达到最低字符级准确率0.7，其中7人显示自适应方法优于基准方法

Conclusion: 提出的半监督学习框架为实时BCI拼写系统提供了实用高效的选择，特别适用于标注数据有限的情况

Abstract: A P300 ERP-based Brain-Computer Interface (BCI) speller is an assistive communication tool. It searches for the P300 event-related potential (ERP) elicited by target stimuli, distinguishing it from the neural responses to non-target stimuli embedded in electroencephalogram (EEG) signals. Conventional methods require a lengthy calibration procedure to construct the binary classifier, which reduced overall efficiency. Thus, we proposed a unified framework with minimum calibration effort such that, given a small amount of labeled calibration data, we employed an adaptive semi-supervised EM-GMM algorithm to update the binary classifier. We evaluated our method based on character-level prediction accuracy, information transfer rate (ITR), and BCI utility. We applied calibration on training data and reported results on testing data. Our results indicate that, out of 15 participants, 9 participants exceed the minimum character-level accuracy of 0.7 using either on our adaptive method or the benchmark, and 7 out of these 9 participants showed that our adaptive method performed better than the benchmark. The proposed semi-supervised learning framework provides a practical and efficient alternative to improve the overall spelling efficiency in the real-time BCI speller system, particularly in contexts with limited labeled data.

</details>


### [3] [R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions](https://arxiv.org/abs/2602.15961)
*Zhi Sheng,Yuan Yuan,Guozhen Zhang,Yong Li*

Main category: cs.LG

TL;DR: R²Energy是一个大规模的可再生能源预测基准，包含中国902个风能和太阳能电站的1070万小时记录，提供标准化、无泄漏的NWP辅助预测框架，揭示了极端天气下模型鲁棒性与复杂性的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源（特别是风能和太阳能）的快速发展，可靠的预测对电力系统运行至关重要。虽然深度学习模型在平均精度上表现良好，但日益频繁和剧烈的极端天气事件对电网稳定性和运行安全构成严重威胁，因此需要开发能够应对波动条件的鲁棒预测模型。

Method: 提出了R²Energy基准，包含中国四个省份902个风能和太阳能电站的1070万小时高保真记录；建立了标准化、无泄漏的预测范式，确保所有模型都能公平访问未来的数值天气预报信号；采用基于专家标注的极端天气分区域评估方法。

Result: 揭示了被平均指标掩盖的"鲁棒性差距"，发现了一个关键的鲁棒性-复杂性权衡：在极端条件下，模型的可靠性取决于其气象集成策略而非架构复杂性，复杂模型在极端天气下表现可能不如简单模型。

Conclusion: R²Energy为评估和开发面向安全关键电力系统应用的预测模型提供了原则性基础，强调了在极端天气条件下模型鲁棒性的重要性，而不仅仅是平均精度。

Abstract: The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and intensity of climate-driven extreme weather events pose severe threats to grid stability and operational security. Consequently, developing robust forecasting models that can withstand volatile conditions has become a paramount challenge. In this paper, we present R$^2$Energy, a large-scale benchmark for NWP-assisted renewable energy forecasting. It comprises over 10.7 million high-fidelity hourly records from 902 wind and solar stations across four provinces in China, providing the diverse meteorological conditions necessary to capture the wide-ranging variability of renewable generation. We further establish a standardized, leakage-free forecasting paradigm that grants all models identical access to future Numerical Weather Prediction (NWP) signals, enabling fair and reproducible comparison across state-of-the-art representative forecasting architectures. Beyond aggregate accuracy, we incorporate regime-wise evaluation with expert-aligned extreme weather annotations, uncovering a critical ``robustness gap'' typically obscured by average metrics. This gap reveals a stark robustness-complexity trade-off: under extreme conditions, a model's reliability is driven by its meteorological integration strategy rather than its architectural complexity. R$^2$Energy provides a principled foundation for evaluating and developing forecasting models for safety-critical power system applications.

</details>


### [4] [Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks](https://arxiv.org/abs/2602.15997)
*Jayadev Billa*

Main category: cs.LG

TL;DR: 论文通过几何测量追踪神经网络训练中的能力涌现现象，发现表示几何在任务难度预测中起主导作用，但存在预测边界条件。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练过程中能力涌现的机制仍然不透明，需要系统性地追踪几何测量来理解这一现象。

Method: 在五个模型规模（405K-85M参数）、八个算法任务和三个Pythia语言模型上追踪五种几何测量，分析120多个涌现事件。

Result: 发现表示几何在硬任务中具有75-100%的先导率，而局部学习系数同步出现，Hessian测量滞后；几何模式能预测粗略任务难度但无法预测精细时间。

Conclusion: 研究揭示了能力涌现的几何解剖结构及其边界条件，但这不是一个预测工具；自然预训练无法提供任务训练对齐所需的前兆关系。

Abstract: Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K-85M parameters), 120+ emergence events in eight algorithmic tasks, and three Pythia language models (160M-2.8B). We find: (1) training begins with a universal representation collapse to task-specific floors that are scale-invariant across a 210X parameter range (e.g., modular arithmetic collapses to RANKME ~ 2.0 regardless of model size); (2) collapse propagates top-down through layers (32/32 task X model consistency), contradicting bottom-up feature-building intuition; (3) a geometric hierarchy in which representation geometry leads emergence (75-100% precursor rate for hard tasks), while the local learning coefficient is synchronous (0/24 precursor) and Hessian measures lag. We also delineate prediction limits: geometric measures encode coarse task difficulty but not fine-grained timing (within-class concordance 27%; when task ordering reverses across scales, prediction fails at 26%). On Pythia, global geometric patterns replicate but per-task precursor signals do not -- the precursor relationship requires task-training alignment that naturalistic pre-training does not provide. Our contribution is the geometric anatomy of emergence and its boundary conditions, not a prediction tool.

</details>


### [5] [Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research](https://arxiv.org/abs/2602.16072)
*Chenda Duan,Yipeng Zhang,Sotaro Kanai,Yuanyi Ding,Atsuro Daida,Pengyue Yu,Tiancheng Zheng,Naoto Kuroda,Shaun A. Hussain,Eishi Asano,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.LG

TL;DR: Omni-iEEG是一个大规模、标准化的术前颅内脑电图数据集，包含302名患者、178小时高分辨率记录，提供超过36K个专家验证的病理事件标注，旨在解决癫痫研究中数据格式不一致、缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 癫痫影响全球超过5000万人，其中三分之一患者患有药物难治性癫痫，手术是最佳治疗选择。目前临床工作流程依赖劳动密集型的颅内脑电图手动分析，而现有数据驱动方法通常基于单中心数据集，存在格式不一致、缺乏标准化基准、很少发布病理事件标注等问题，阻碍了可重复性、跨中心验证和临床相关性。

Method: 通过协调公开可用来源中的异构颅内脑电图格式、元数据和记录，构建了Omni-iEEG数据集。该数据集包含302名患者、178小时高分辨率记录，提供统一的临床元数据（如癫痫发作起始区、切除区域、手术结果）和超过36K个专家验证的病理事件标注。定义了基于临床先验的统一评估指标和临床相关任务。

Result: 创建了一个大规模、标准化的术前颅内脑电图资源，可作为机器学习和癫痫研究的桥梁。展示了在长颅内脑电图片段上进行端到端建模的潜力，并突出了在非神经生理学领域预训练表示的可迁移性。

Conclusion: Omni-iEEG为可重复、可泛化和临床可转化的癫痫研究奠定了基础，通过提供标准化数据集、临床相关任务定义和统一评估指标，促进了跨中心验证和临床相关性研究。

Abstract: Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained by labor-intensive manual review. At the same time, existing data-driven approaches are typically developed on single-center datasets that are inconsistent in format and metadata, lack standardized benchmarks, and rarely release pathological event annotations, creating barriers to reproducibility, cross-center validation, and clinical relevance. With extensive efforts to reconcile heterogeneous iEEG formats, metadata, and recordings across publicly available sources, we present $\textbf{Omni-iEEG}$, a large-scale, pre-surgical iEEG resource comprising $\textbf{302 patients}$ and $\textbf{178 hours}$ of high-resolution recordings. The dataset includes harmonized clinical metadata such as seizure onset zones, resections, and surgical outcomes, all validated by board-certified epileptologists. In addition, Omni-iEEG provides over 36K expert-validated annotations of pathological events, enabling robust biomarker studies. Omni-iEEG serves as a bridge between machine learning and epilepsy research. It defines clinically meaningful tasks with unified evaluation metrics grounded in clinical priors, enabling systematic evaluation of models in clinically relevant settings. Beyond benchmarking, we demonstrate the potential of end-to-end modeling on long iEEG segments and highlight the transferability of representations pretrained on non-neurophysiological domains. Together, these contributions establish Omni-iEEG as a foundation for reproducible, generalizable, and clinically translatable epilepsy research. The project page with dataset and code links is available at omni-ieeg.github.io/omni-ieeg.

</details>


### [6] [Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA](https://arxiv.org/abs/2602.16442)
*Kamil Jeziorek,Piotr Wzorek,Krzysztof Blachut,Hiroshi Nakano,Manon Dampfhoffer,Thomas Mesquida,Hiroaki Nishi,Thomas Dalgaty,Tomasz Kryjak*

Main category: cs.LG

TL;DR: FPGA实现的事件图神经网络用于音频处理，利用人工耳蜗将时域信号转换为稀疏事件数据，在低功耗、低延迟下实现高效分类和关键词检测。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式边缘传感器数据量增加，特别是神经形态设备产生的离散事件流，需要硬件感知的神经架构来实现高效、低延迟、节能的本地处理。

Method: 采用FPGA实现事件图神经网络，利用人工耳蜗将时域音频信号转换为稀疏事件数据，结合图卷积层和循环序列建模，在SoC FPGA上实现量化模型。

Result: 在SHD数据集上达到92.7%准确率（仅比SOTA低2.4%），参数减少10-67倍；在SSC上达到66.9-71.0%准确率；关键词检测系统达到95%词尾检测准确率，仅10.53微秒延迟和1.18W功耗。

Conclusion: 该工作展示了事件图神经网络在FPGA上的高效实现，为节能的事件驱动关键词检测建立了强基准，在资源使用、延迟和功耗方面优于现有FPGA实现的脉冲神经网络。

Abstract: As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.

</details>


### [7] [Capacity-constrained demand response in smart grids using deep reinforcement learning](https://arxiv.org/abs/2602.16525)
*Shafagh Abband Pashaki,Sepehr Maleki,Amir Badiee*

Main category: cs.LG

TL;DR: 提出一种基于容量约束的激励型需求响应方法，通过深度强化学习优化实时激励费率，有效降低住宅智能电网的峰值负荷和平滑负荷曲线。


<details>
  <summary>Details</summary>
Motivation: 解决住宅智能电网中的容量限制和拥堵问题，通过经济激励引导用户调整用电行为，同时考虑服务提供商和终端用户的经济利益。

Method: 采用分层架构，服务提供商根据批发电价和聚合住宅负荷调整小时激励费率；使用深度强化学习在明确容量约束下学习最优实时激励费率；通过设备级家庭能源管理系统和不满成本建模异质用户偏好。

Result: 基于三个家庭的实际用电和价格数据进行仿真，结果显示该方法能有效降低峰值需求、平滑聚合负荷曲线，相比无需求响应情况，峰均比降低了约22.82%。

Conclusion: 提出的容量约束激励型需求响应框架能有效管理住宅电网容量，平衡服务提供商和用户利益，为智能电网需求响应提供了可行的解决方案。

Abstract: This paper presents a capacity-constrained incentive-based demand response approach for residential smart grids. It aims to maintain electricity grid capacity limits and prevent congestion by financially incentivising end users to reduce or shift their energy consumption. The proposed framework adopts a hierarchical architecture in which a service provider adjusts hourly incentive rates based on wholesale electricity prices and aggregated residential load. The financial interests of both the service provider and end users are explicitly considered. A deep reinforcement learning approach is employed to learn optimal real-time incentive rates under explicit capacity constraints. Heterogeneous user preferences are modelled through appliance-level home energy management systems and dissatisfaction costs. Using real-world residential electricity consumption and price data from three households, simulation results show that the proposed approach effectively reduces peak demand and smooths the aggregated load profile. This leads to an approximately 22.82% reduction in the peak-to-average ratio compared to the no-demand-response case.

</details>


### [8] [AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS](https://arxiv.org/abs/2602.16579)
*Maria Luisa Taccari,Kenza Tazi,Oisín M. Morrison,Andreas Grafberger,Juan Colonese,Corentin Carton de Wiart,Christel Prudhomme,Cinzia Mazzetti,Matthew Chantry,Florian Pappenberger*

Main category: cs.LG

TL;DR: AIFL是一个基于LSTM的确定性全球日径流预报模型，采用两阶段训练策略解决再分析到预报的领域偏移问题，在CARAVAN数据集上训练，表现优于现有全球系统。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型在从历史再分析过渡到业务预报产品时存在性能差距，需要可靠的全球径流预报系统用于洪水准备和水资源管理。

Method: 基于LSTM的确定性模型，采用两阶段训练：先在ERA5-Land再分析数据（1980-2019）上预训练，然后在IFS业务控制预报（2016-2019）上微调，使用CARAVAN数据集中的18,588个流域。

Result: 在独立测试集（2021-2024）上，AIFL获得中位数KGE'为0.66，NSE为0.53，与当前最先进的全球系统竞争力相当，在极端事件检测方面表现优异。

Conclusion: AIFL是第一个在CARAVAN生态系统中端到端训练的全球模型，为全球水文社区提供了一个透明、可复现且业务稳健的基准系统。

Abstract: Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection](https://arxiv.org/abs/2602.16037)
*Cameron Cagan,Pedram Fard,Jiazi Tian,Jingya Cheng,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: 自主代理工作流在持续自我优化时可能出现性能退化，特别是在低患病率分类任务中，传统评估指标可能掩盖严重失败模式，而回顾性选择比主动干预更有效。


<details>
  <summary>Details</summary>
Motivation: 自主代理工作流具有迭代优化自身行为的潜力，但其失败模式尚未得到充分研究。本文旨在研究优化不稳定性现象，即持续自主改进反而导致分类器性能下降的问题。

Method: 使用Pythia开源框架进行自动提示优化，评估三种不同患病率的临床症状（呼吸困难23%、胸痛12%、长新冠脑雾3%）。测试两种干预措施：指导代理主动重定向优化，以及选择代理回顾性识别最佳迭代。

Result: 验证灵敏度在迭代间在1.0和0.0之间振荡，严重程度与类别患病率成反比。在3%患病率下，系统达到95%准确率但未检测到任何阳性病例。选择代理干预成功防止灾难性失败，在脑雾检测上比专家策划词典提高331%（F1），胸痛检测提高7%。

Conclusion: 自主AI系统存在关键失败模式，在低患病率分类任务中，回顾性选择比主动干预更有效地稳定系统性能，传统评估指标可能掩盖严重失败。

Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.

</details>


### [10] [Verifiable Semantics for Agent-to-Agent Communication](https://arxiv.org/abs/2602.16424)
*Philipp Schoenegger,Matt Carlson,Chris Schneider,Chris Daly*

Main category: cs.AI

TL;DR: 提出基于刺激-意义模型的认证协议，通过测试智能体在可观测事件上的表现来验证术语理解一致性，使用核心保护推理可显著减少分歧


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统需要一致的通信，但缺乏验证智能体对术语理解是否一致的方法。自然语言可解释但易受语义漂移影响，而学习协议高效但不透明。

Method: 基于刺激-意义模型的认证协议：测试智能体在共享可观测事件上的表现，如果经验分歧低于统计阈值则认证术语。智能体限制其推理于认证术语（核心保护推理）可实现可证明的有界分歧。还包括检测漂移的重新认证机制和恢复共享词汇的重新协商机制。

Result: 在具有不同程度语义分歧的模拟中，核心保护将分歧减少了72-96%。在使用微调语言模型的验证中，分歧减少了51%。

Conclusion: 该框架为可验证的智能体间通信提供了第一步，通过统计认证确保术语理解一致性，减少语义分歧。

Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms ("core-guarded reasoning") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [The Validity of Coreference-based Evaluations of Natural Language Understanding](https://arxiv.org/abs/2602.16200)
*Ian Porada*

Main category: cs.CL

TL;DR: 该论文分析指代消解评估的局限性，提出新的事件可能性推理评估方法，发现当前语言模型在标准基准上表现良好但泛化能力有限


<details>
  <summary>Details</summary>
Motivation: 现有指代消解评估存在测量效度问题（定义争议和收敛效度不足），导致结论不可泛化，需要更全面的评估方法来理解模型真实能力

Method: 1) 分析标准指代消解评估的测量效度问题；2) 提出并实施新的事件相对可能性推理评估，测试模型在事件指代消解关键方面的能力

Result: 当代语言模型在标准基准上表现优于早期基线系统，但对评估条件敏感，在轻微修改的上下文环境中泛化能力不足，无法达到人类水平

Conclusion: 当前NLP范式既有优势（标准评估准确率提升）也有局限（测量效度弱、泛化能力不足），需要开发更好的评估方法和真正可泛化的系统

Abstract: In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.

</details>
