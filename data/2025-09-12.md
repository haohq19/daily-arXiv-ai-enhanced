<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: PromptGuard是一个模块化提示框架，通过VulnGuard Prompt技术使用对比学习防止LLM生成有害信息，特别保护LGBTQ+、单亲家庭等弱势群体。


<details>
  <summary>Details</summary>
Motivation: 现有安全方法依赖事后过滤或通用对齐技术，无法在生成源头主动预防有害输出，特别是对弱势群体造成伤害的风险。

Method: 提出PromptGuard框架，包含VulnGuard Prompt混合技术：整合GitHub精选数据、伦理思维链推理和自适应角色提示，采用多目标优化理论。

Result: 通过熵边界和帕累托最优性证明可实现25-30%的分析性伤害减少，建立了系统实证研究的数学基础。

Conclusion: PromptGuard提供了一个智能专家系统，通过六个核心模块实现实时伤害预防，为LLM安全保护提供了理论框架和实践方案。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [2] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event是首个基于事件相机数据的大规模语言驱动目标定位基准，包含5,567个场景、13,458个标注目标和30,000+验证过的指代表达式，支持动态环境中的上下文推理。


<details>
  <summary>Details</summary>
Motivation: 事件相机在动态场景建模中具有微秒级精度和抗运动模糊的优势，但其与自然语言理解的结合尚未得到充分探索，存在多模态感知的空白。

Method: 构建基于真实驾驶场景的大规模基准数据集，每个指代表达式包含外观、状态、与观察者关系、与周围物体关系四个结构化属性，支持可解释的组合式定位。

Result: 创建了包含丰富标注信息的大规模数据集，支持从简单目标识别到动态环境中上下文推理的分析。

Conclusion: Talk2Event为推进多模态和时间感知感知提供了基础，在机器人、人机交互等领域具有应用前景。

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [3] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: RRDataset是一个用于评估AI生成图像检测模型在真实世界复杂条件下的数据集，涵盖场景泛化、网络传输鲁棒性和重数字化鲁棒性三个维度，揭示了当前检测方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，高度逼真的图像合成对数字安全和媒体可信度提出了新挑战。现有AI生成图像检测方法在复杂真实世界条件下的性能评估存在研究空白。

Method: 构建RRDataset数据集，包含7个主要场景的高质量图像，评估17个检测器和10个视觉语言模型，并进行192名参与者的大规模人类研究，考察人类在检测AI生成图像方面的少样本学习能力。

Result: 基准测试结果揭示了当前AI检测方法在真实世界条件下的局限性，强调了借鉴人类适应性来开发更鲁棒检测算法的重要性。

Conclusion: 该研究强调了在复杂真实世界条件下评估AI生成图像检测的重要性，并指出需要开发更具鲁棒性的检测方法，可以从人类的学习能力中获得启发。

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [4] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 提出CoAtNeXt混合模型用于胃组织图像分类，结合CoAtNet架构和ConvNeXtV2块，集成CBAM注意力机制，在两个公开数据集上超越所有CNN和ViT模型，性能优异。


<details>
  <summary>Details</summary>
Motivation: 胃病早期诊断至关重要，但传统组织病理学检查完全手动操作，工作强度大且存在病理学家间差异，需要自动化、可靠、高效的胃组织分析方法。

Method: 基于CoAtNet架构，用增强的ConvNeXtV2块替换MBConv层，集成CBAM注意力模块改进局部特征提取，在计算效率和分类性能间取得平衡。

Result: 在HMU-GC-HE-30K八分类数据集上达到96.47%准确率，在GasHisSDB二分类数据集上达到98.29%准确率，超越所有测试的CNN和ViT模型。

Conclusion: CoAtNeXt是胃组织图像组织病理学分类的强大架构，在二分类和多分类任务上表现优异，有潜力辅助病理学家提高诊断准确性并减少工作量。

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [5] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: 提出了DATE方法，通过时间戳注入和语义引导的时间感知采样策略，增强多模态大语言模型在长视频理解中的时间感知能力


<details>
  <summary>Details</summary>
Motivation: 现有方法采用均匀帧采样和隐式位置编码，难以处理长视频中的长距离依赖关系，导致关键信息丢失和时间理解能力下降

Method: DATE方法包含时间戳注入机制(TIM)和时序感知相似性采样(TASS)策略，将视频帧嵌入与文本时间戳交错构建连续时间参考系统，并采用两阶段算法确保语义相关性和时间覆盖

Result: 在7B和72B模型上实现了小时级长视频基准测试的最先进性能，7B模型在某些基准上甚至超过了许多72B模型

Conclusion: DATE方法通过显式时间建模和语义引导采样，显著提升了多模态大语言模型的长视频时间理解和事件定位能力

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [6] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: 本文提出Code-as-Thought(CaT)方法，通过学习性视觉程序化能力，让VLM动态选择代码路径或直接视觉路径来解决图表理解问题，并使用双重奖励系统进行强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有图表理解方法的局限性：外部工具方法容易出错且受限于预定工具集，而专门模型通常只采用单一的文本链式思维策略，其中间步骤难以验证，影响了基于事实准确性的强化学习信号的使用。

Method: 提出Code-as-Thought(CaT)方法，将图表的视觉信息转换为可验证的符号格式。重点是学习性视觉程序化能力，让VLM动态选择代码路径或直接视觉识别路径。使用双重奖励系统（数据准确性奖励+决策奖励）通过强化学习训练模型的选择策略。

Result: 实验结果显示，该方法在多样化的图表理解测试集上表现出强劲且稳健的性能。

Conclusion: 该研究证明，VLM不仅可以被教会如何进行推理，还能够学习如何动态选择最优的推理路径来应对不同的任务。

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [7] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种用于密集自盛盛学习的明确语义聚集方法(CoTAP)，通过排名损失和对象识别筛选机制来解决现有方法的过分散问题，在多个下游任务上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 图像级别自盛盛学习已取得重要进展，但学习密集表征仍面临挑战。主流方法遇到过分散现象，导致同一实例/类别的补丁分散，影响下游密集任务的性能。图像级SSL通过隐式语义聚集避免了这个问题，但这些方法在密集SSL中不可行。

Method: 1. 通过萌生补丁对应关系来打破严格空间对齐；2. 提出噪声容忍排名损失，将平均精度(AP)损失扩展到连续目标，避免学生模型被错误指导；3. 提出对象识别筛选器，通过跨注意力机制将补丁映射到可学习的对象原型空间。

Result: 在多个下游任务上进行了实证研究，统计支持了方法的有效性。代码已在GitHub上开源。

Conclusion: 该方法通过明确语义聚集成功解决了密集SSL中的过分散问题，为密集表征学习提供了有效的解决方案。

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 这篇论文提出了一种上下文条件异常检测框架，通过自动识别上下文特征和模型条件数据分布，在多个表格数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 在网络安全和金融等领域，面对大规模表格数据时，无监督异常检测仍是个大挑战。现有深度学习方法建模全局分布时忽略了异质性上下文，导致检测性能下降。

Method: 提出上下文条件异常检测框架，自动识别上下文特征，使用简单深度自动编码器模型条件数据分布。

Result: 在多个表格数据集上进行的广泛实验表明，该方法超越了现有最先进方法。

Conclusion: 该研究强调了上下文在准确区分异常与正常实例中的重要性，提出的框架有效解决了异质性上下文带来的挑战。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [9] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 深度学习天气预测模型存在"统计相似性陷阱"，奖励模糊预测却漏掉稀有高影响事件。DART框架通过双解码器结构和专门损失函数，显著提升极端对流检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测评估指标存在统计相似性陷阱，导致模型过度治疗而错过重要的极端天气事件。需要专门优化极端对流检测的方法。

Method: 提出DART框架：双解码器结构进行背景/极端分解，物理动力学重量采样，任务特定损失函数。发现离子水气运输可大幅提升性能。

Result: DART在极端对流检测上达到CSI=0.273（基线模型为0.00），偏差从6.72降至2.52。在实际灾害案例中验证有效性，训练时间小于10分钟。

Conclusion: DART系统性解决了天气预测中的统计相似性陷阱问题，为极端天气预备提供了可靠的AI解决方案，具有强大的应用潜力。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [10] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过知识蒸馏将LLM的领域知识和推理能力转移到传统序列模型中，在保持快速推理的同时提升问答模拟器的性能


<details>
  <summary>Details</summary>
Motivation: 解决现有问答模拟器中LLM-free方法性能不佳而LLM-based方法推理速度慢、资源消耗高的问题，寻求性能与效率的平衡

Method: 采用知识蒸馏技术，从大型语言模型(LLM)中提取领域知识和推理能力，用于增强传统序列模型的预测能力

Result: 在模拟任务和知识追踪任务上都取得了强劲的结果，实现了性能提升的同时保持了较快的推理速度

Conclusion: LDSim方法成功地将LLM的优势蒸馏到轻量级模型中，为教育推荐系统提供了高效且准确的问答模拟解决方案

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [11] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个解决算法选择问题中计算成本高的实验数据集，通过提供9,408个管道在300个数据集上的完整实验结果，克服了OpenML在数据预处理技术多样性和代表性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 机器学习算法选择问题面临高昂的计算成本，而现有的OpenML等在线存储库在数据预处理技术多样性方面存在不足，实验样本不平衡，缺乏代表性。

Method: 提出PIPES实验数据集，通过设计代表所有技术组合的多样化管道，在300个数据集上执行9,408个管道的实验，记录详细的管道块信息、训练测试时间、预测结果、性能指标和错误信息。

Result: 创建了一个包含详细实验结果的综合性数据集，支持研究人员进行多样化和代表性管道的分析，并具有进一步扩展的潜力。

Conclusion: PIPES为元学习社区提供了一个比OpenML更全面和多样化的实验数据集，有助于解决算法选择问题中的计算成本挑战，并支持未来的研究扩展。

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [12] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 深度学习模拟器，通过预训练和小数据集微调，能够在幻灵和随机时空系统中跨参数范围进行高效模拟，并支持不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统数值汇汇水方法计算成本高，特别是在需要探索幻灵和随机系统的参数空间时。需要一种能够在幻灵时空系统中跨参数范围进行高效模拟的方法。

Method: 使用深度学习模型，先在单个参数域预训练，然后在小而多样的数据集上进行微调。模型采用局部注意力机制，支持不同域大小和分辨率。还提供概率版本进行不确定性量化。

Result: 在Kuramoto-Sivashinsky方程和随机驱动的测面激流系统中进行测试，模型能够捕捉插值参数下的现象，实现了显著的计算速度提升，并支持稀有事件的统计研究。

Conclusion: 该深度学习模拟器提供了一种高效的方法来模拟幻灵和随机时空系统，能够在广泛的参数范围内进行插值预测，并通过概率版本提供不确定性量化，为参数空间探索提供了强大工具。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 提出了一种截断交叉熵（TCE）损失函数来缓解生成式AI模型在合成数据上重复训练导致的模型崩溃问题，通过降低对高置信度预测的权重来显著延迟模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型生成合成数据的速率加快，预计到203年大部分新训练数据将是机器生成的。在合成数据上重复训练会导致模型崩溃现象，使模型性能逐渐退化。现有缓解策略有限，需要新的解决方案。

Method: 识别模型对其自生成数据的过度自信是崩溃的关键驱动因素，提出置信度感知的损失函数TCE，在训练过程中降低高置信度预测的权重。

Result: TCE显著延迟了递归训练中的模型崩溃，可将模型保真度区间延长2.3倍以上，且该方法在不同模态上都有效。

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具，TCE方法具有模型无关性，理论分析和实证验证都证明了其有效性。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 基于NeuroTac感应器的神经形态触觉感知系统，通过间刻卷积神经网络实现94.33%的滑动状态分类精度，能够提前360ms检测初始滑动


<details>
  <summary>Details</summary>
Motivation: 解决在边缘计算平台上部署初始滑动检测系统时遇到的能源约束问题

Method: 使用NeuroTac感应器（具有童犬粗大皮肤结构）和间刻卷积神经网络（SCNN）进行滑动状态分类

Result: 在感应器运动引起的滑动条件下，SCNN模型对三种滑动状态达到94.33%的分类精度；在动态重力引起的滑动验证条件下，系统能够提前至360ms检测到初始滑动

Conclusion: 该神经形态系统具有稳定且响应快速的初始滑动检测能力，适合在能源受限的边缘设备上部署

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>
