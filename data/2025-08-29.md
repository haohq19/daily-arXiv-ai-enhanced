<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 这篇论文提出了两种新的全向视频显著性预测模型SalViT360和SalViT360-AV，通过统合视觉和空间音频线索，在多个数据集上显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏全面的全向视频音视觉显著性数据集，以及需要处理球面扭曲和空间音频集成的复杂性，研究者创建了YT360-EyeTracking数据集并开发新模型。

Method: 研究创建了包含81个全向视频的YT360-EyeTracking数据集，并提出两种模型：SalViT360（基于视觉Transformer的框架，包含球面几何感知的空时注意力层）和SalViT360-AV（在SalViT360基础上通过Transformer适配器集成音频输入）。

Result: 在多个标准数据集包括YT360-EyeTracking上，SalViT360和SalViT360-AV都显著超过了现有的显著性预测方法。

Conclusion: 研究证明在全向视频显著性预测中，集成空间音频线索到模型架构中对于准确预测视觉注意力至关重要。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [2] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于视觉-语言模型的解释性人工智能流水线，用于在样本和数据集级别上解释视觉模型的一般性行为。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型开发主要关注性能指标（如准确率、IoU、mAP），而对解释性关注不够。现有xAI方法多为样本级别解释，而能够揭示模型在大规模数据集上一般性行为的方法还很缺乏。理解视觉模型的一般性行为对防止偏见判断和识别模型趋势模式至关重要。

Method: 利用视觉-语言模型，提出了一种能够在样本级别和数据集级别上解释视觉模型的流水线。该流水线可以用最小化的努力发现模型的失败案例并获得深度见解。

Result: 该方法能够有效解释视觉模型的一般性行为，发现模型的失败案例和识别模型的趋势模式。

Conclusion: 该研究提供了一种将视觉模型开发与xAI分析相结合的方法，促进了图像分析领域的发展，为解释性人工智能在视觉模型中的应用开启了新方向。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [3] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: MedFoundationHub是一个医疗视觉语言模型GUI工具包，旨在解决医疗VLMs在临床应用中面临的安全隐患，包括PHI泄露风险和数据安全问题，通过本地化部署和隐私保护推理确保安全性。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型在临床应用中存在严重的安全隐患，特别是受保护健康信息(PHI)泄露、数据泄漏和网络威胁漏洞等问题，这些风险在医院环境中尤为关键。

Method: 开发了MedFoundationHub图形用户界面工具包，支持医生无需编程知识即可手动选择和使用不同模型，工程师可以即插即用方式高效部署医疗VLMs，通过Docker编排实现操作系统无关的隐私保护推理部署。

Result: 使用单块NVIDIA A6000 GPU的离线本地工作站进行评估，邀请了委员会认证的病理学家部署和评估5个最先进的VLMs，覆盖结肠和肾脏病例，产生了1015次临床医生-模型评分事件，发现存在脱靶回答、模糊推理和不一致的病理学术语等局限性。

Conclusion: MedFoundationHub提供了一个安全且易于访问的医疗VLMs部署解决方案，但当前最先进的模型仍存在显著局限性，需要在临床应用前进一步改进。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [4] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 提出了一种延迟脉冲方法和时间依赖的IF神经元(tdIF)，解决了SNN在视觉检测任务中的性能问题，在超低时间步长(5步内)下实现了最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前ANN-SNN转换方法在分类任务中表现优异，但在视觉检测任务中性能不佳，主要原因是异质脉冲模式导致的残余膜电位问题。

Method: 采用延迟脉冲方法缓解异质脉冲模式问题，并提出时间依赖的IF神经元(tdIF)，使神经元能够根据时间步长的时序动态调整积累和发放行为。

Result: 在目标检测和车道线检测两个关键视觉任务上，该方法超越了当前ANN-SNN转换方法，在超低延迟(5时间步内)下实现了最先进的性能。

Conclusion: tdIF神经元使脉冲具有不同的时间特性而不仅依赖频率表示，在保持与传统IF神经元相当能耗的同时，实现了更精确的特征表示和超低延迟的高性能检测。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [5] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: Video-MTR是一个强化多轮推理框架，通过迭代选择关键视频片段和问题理解来提升长视频理解性能，无需外部视觉语言模型即可实现端到端训练。


<details>
  <summary>Details</summary>
Motivation: 长视频理解面临长程时间依赖和多个事件的挑战，现有方法依赖静态推理或外部视觉语言模型，存在复杂性和端到端训练不足导致的性能不佳问题。

Method: 提出多轮推理框架，迭代选择视频片段并基于先前处理片段的理解进行渐进式分析，采用门控双级奖励系统（轨迹级和轮级奖励）优化视频片段选择和问题理解。

Result: 在VideoMME、MLVU和EgoSchema等基准测试中，Video-MTR在准确性和效率方面均优于现有方法，实现了长视频理解的最先进性能。

Conclusion: Video-MTR通过多轮推理和门控奖励系统有效解决了长视频理解的挑战，无需外部模型支持，实现了端到端训练的优越性能。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [6] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出UTA-Sign方法，通过无监督热成像-事件视频融合技术增强低光照环境下交通标志的感知能力，解决热成像盲点和事件相机非均匀采样问题


<details>
  <summary>Details</summary>
Motivation: 热成像相机在低光环境下表现优异但难以区分材质相似的标志，事件相机能检测光强变化但采样不均匀，两者具有互补特性

Method: 开发双增强机制，融合热成像帧和事件信号，利用热成像提供精确运动线索对齐事件信号，事件信号补充热成像的细节内容

Result: 在真实场景数据集上验证，在交通标志描绘质量和感知级检测精度方面表现出优越性能

Conclusion: UTA-Sign方法有效解决了低光照环境下交通标志感知的挑战，为自动驾驶系统提供了更可靠的环境理解能力

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [7] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 提出基于低频感知扰动的主動防禦方法，直接針對深度偽造生成過程進行干擾，在保持視覺質量的同時有效降低換臉效果


<details>
  <summary>Details</summary>
Motivation: 現有深度偽造檢測方法多為被動式事後分析，無法預防攻擊。需要開發主動防禦技術來直接干擾偽造內容的生成過程

Method: 結合頻域和空間域特徵，使用離散小波變換提取低頻分量生成擾動，通過編碼器-擾動生成器-解碼器架構在保持高頻細節的同時引入干擾偽造的特徵

Result: 在CelebA-HQ和LFW數據集上實驗顯示，顯著降低換臉效果，提高防禦成功率，同時保持視覺質量

Conclusion: 提出的主動防禦方法能有效破壞深度偽造生成過程，為對抗惡意深度偽造技術提供了新的解決方案

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [8] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 这篇论文调查了视频中抽象概念识别的研究现状，认为基础模型为理解抽象概念提供了理想条件，并呼吁利用数十年社区经验来避免重复造轮子。


<details>
  <summary>Details</summary>
Motivation: 人类能够识别抽象概念（如正义、自由、团结），而当前机器主要理解视频中的具体实体。抽象概念识别是视频理解的关键开放挑战，需要基于上下文信息进行多层次的语义推理。

Method: 通过调查分析不同任务和数据集来研究视频中抽象概念的理解方法，回顾研究人员在不同时期利用可用工具解决这些任务的尝试。

Result: 研究发现基础模型的进步为解决视频抽象概念理解提供了理想条件，自动化理解高级抽象概念能使模型更符合人类推理和价值观。

Conclusion: 建议借鉴数十年社区经验来解决这一重要开放挑战，在多模态基础模型时代避免重复发明轮子，推动抽象概念理解的发展。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [9] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: DVCTNet是一个基于双视图协同训练的网络，通过结合全景X射线图像的全局视图和裁剪牙齿图像的局部视图，使用门控交叉视图注意力模块动态融合特征，显著提高了牙科龋齿检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前牙科龋齿检测方法由于对比度变化细微和病变形态多样，检测精度不理想。受牙医临床工作流程启发，需要结合整体图像筛查和详细牙齿级检查来提高检测准确性。

Method: 使用自动牙齿检测建立全局视图和局部视图，分别预训练两个视觉基础模型。全局视图模型作为检测骨干网络生成区域建议和全局特征，局部视图模型提取裁剪牙齿斑块的详细特征。通过门控交叉视图注意力模块动态融合双视图特征。

Result: 在公共数据集和新构建的高精度数据集上，DVCTNet都表现出优于现有最先进方法的性能，证明了其临床适用性。

Conclusion: DVCTNet通过双视图协同训练和特征融合机制，有效提高了牙科龋齿检测的准确性和可靠性，具有重要的临床应用价值。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 本文揭示了RL比SFT更有效的有害微调风险，并提出了首个针对RL有害微调的防御方法TokenBuncher，通过抑制模型响应不确定性来有效防御。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的增强，通过微调进行有害滥用的风险也在增加。现有研究主要关注监督微调(SFT)的滥用，但研究发现强化学习(RL)能更有效地破坏安全对齐并促进有害任务。

Method: 提出了TokenBuncher防御方法，通过熵作为奖励的RL和Token Noiser机制来抑制模型响应不确定性，防止RL利用不同的奖励信号驱动模型产生有害行为。

Result: 在多个模型和RL算法上的广泛实验表明，TokenBuncher能够稳健地减轻有害RL微调，同时保持良性任务效用和可微调性。

Conclusion: RL有害微调比SFT构成更大的系统性风险，TokenBuncher提供了有效且通用的防御解决方案。

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [11] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: 提出了cMALC-D框架，使用LLM生成语义化课程并提供更鲁棒的评估信号，通过多样性上下文混合机制防止模式崩溃，在交通信号控制任务中显著提升泛化能力和样本效率


<details>
  <summary>Details</summary>
Motivation: 现有上下文多智能体强化学习方法依赖不可靠的代理信号（如价值估计），在多智能体设置中由于智能体间动态性和部分可观测性而噪声大且不稳定

Method: 使用大型语言模型生成语义化课程，引入基于多样性的上下文混合机制，通过组合先前上下文特征创建新训练场景

Result: 在交通信号控制领域的实验表明，cMALC-D相比现有课程学习基线方法显著提高了泛化能力和样本效率

Conclusion: cMALC-D框架通过LLM引导的课程学习和多样性上下文混合，有效解决了多智能体强化学习在复杂不确定环境中的泛化问题

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 提出GDLLM方法，通过距离感知图结构和软推理时序特征学习来提升大语言模型在事件时序关系抽取中的长距离依赖捕获能力，特别是在处理少数类别关系方面表现优异


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型在类别不平衡数据集中处理少数类关系能力有限，以及大语言模型手动设计提示可能引入噪声干扰长距离依赖判断的问题

Method: 基于大语言模型的全局距离感知建模方法，使用图注意力网络构建距离感知图结构捕获长距离依赖特征，设计基于软推理的时序特征学习范式增强短距离邻近带关系识别

Result: 在TB-Dense和MATRES两个公开数据集上实现了最先进的性能，显著提升了少数关系类别的表现和整体学习能力

Conclusion: GDLLM框架通过有效捕获全局特征，能够显著增强少数关系类别的性能并提升整体学习能力，为事件时序关系抽取提供了有效的解决方案

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>
