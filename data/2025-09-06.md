<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 提出GFP框架，用高级特征预测替代传统低级坐标重建，实现高效骨架动作识别


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法局限于原始关节坐标重建，导致计算冗余和语义表示有限

Method: 使用轻量级目标生成网络动态产生多样化监督信号，采用约束优化确保特征多样性

Result: 训练速度提升6.2倍，在NTU RGB+D 60/120和PKU-MMD数据集上达到SOTA性能

Conclusion: 高级特征预测框架在计算效率和表示质量方面均优于传统方法

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [2] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于事件相机的激光融合氛流减弱方法(EGTM)，通过事件流的异步特性提取像素级可靠指导，在保持最高恢复质量的同时实现了710倍、214倍和224倍的模型效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习氛流减弱方法需要高容量网络学习同步帧之间粗粒度的激流动力学，计算和存储效率低。事件相机具有微秒级别的时间分辨率，有潜力根本解决这个瓶颈。

Method: 首先提出"事件激光洞察"揭示激流形变与事件流逆时空分布的相关性，然后构建EGTM框架，从噪声激流事件中提取像素级可靠激流免指导进行时间激光融合。还构建了首个真实世界事件驱动氛流数据集。

Result: 在真实世界EGTM数据集上，方法在恢复质量上达到最高水平(+0.94 PSNR和+0.08 SSIM)，同时模型大小、推理延迟和模型复杂度分别减少了710倍、214倍和224倍。

Conclusion: 证明事件摸态在氛流减弱任务中具有重要的效率优势，通过事件流的稀疏异步成像机制根本解决了传统方法的效率瓶颈。

Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [3] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

TL;DR: FocusMamba提出了一种自适应多模态特征稀疏化方法，通过事件引导的稀疏化策略和跨模态融合模块，在RGB-事件检测任务中实现了精度和效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-事件检测方法在处理两种模态的低信息区域（图像背景和事件数据中的非事件区域）时采用统一处理方式，导致计算成本高且性能不佳。固定阈值或数量的token稀疏化方法无法适应不同复杂度的样本。

Method: 提出Event-Guided Multimodal Sparsification (EGMS)策略，利用事件相机感知的场景变化来识别和自适应丢弃每个模态中的低信息区域；设计Cross-Modality Focus Fusion (CMFF)模块来有效捕获和整合两种模态的互补特征。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，该方法在准确性和效率方面均优于现有方法。

Conclusion: FocusMamba通过自适应协作稀疏化和高效融合策略，成功解决了RGB-事件检测中的计算冗余问题，实现了更好的性能平衡。

Abstract: Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [4] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出结合双向文本融合模块和类别感知时序图模块的新方法，在音频-视觉视频解析任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将噪声段级伪标签视为可靠监督，要么让无差别注意力将伪标签传播到所有帧，导致初始错误在训练中被反复放大

Method: 使用BiT模块进行语义注入和动态校准来定位和净化更干净的语义线索，然后使用CATS模块进行语义传播和连接，实现精确的跨时间语义信息传播

Result: 在两个基准数据集LLP和UnAV-100上的多个关键指标中实现了最先进的性能

Conclusion: 通过结合两种研究方向的优势，有效解决了现有方法中错误放大的问题，提升了音频-视觉视频解析的性能

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [5] [DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset](https://arxiv.org/abs/2509.04117)
*Mustafa Sakhai,Kaung Sithu,Min Khant Soe Oke,Maciej Wielgosz*

Main category: cs.CV

TL;DR: DVS-PedX是一个用于行人检测和过街意图分析的神经形态数据集，包含合成和真实事件流数据，支持恶劣天气条件下的研究。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、高动态范围和运动鲁棒性等优势，但缺乏专门用于行人安全研究的神经形态数据集，特别是在恶劣天气条件下。

Method: 通过两种互补来源构建数据集：(1) CARLA模拟器生成合成事件流，控制天气和光照条件；(2) 使用v2e工具将真实JAAD行车记录仪视频转换为事件流，保留自然行为。提供RGB帧、事件帧和标注标签。

Result: 数据集包含配对的RGB帧、事件帧和帧级标注，提供原始事件文件和元数据。基线SNN实验展示了数据集可用性，并揭示了模拟到现实的差距。

Conclusion: DVS-PedX数据集旨在加速基于事件的行人安全、意图预测和神经形态感知研究，为领域适应和多模态融合提供基础。

Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.

</details>


### [6] [Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage](https://arxiv.org/abs/2509.04370)
*Dor Cohen,Inga Efrosman,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: 通过单目SLAM和帧穿繁技术，将身体摄像头拍摄的长视频转换为信息丰富的全景图像摘要，支持突发事件的快速决策和后续分析。


<details>
  <summary>Details</summary>
Motivation: 解决突发事件中经营者需要快速获取现场情况而无法完整查看长视频的问题，需要一种能够快速解释的简洁视觉摘要。

Method: 采用单目同时定位与地图构建(SLAM)估计摄像头轨迹和环境空间布局，通过聚类找到关键视点，从每个聚类中选择代表性帧，使用多帧穿繁技术将这些帧融合成空间一致的全景图像。

Result: 生成的摘要全景图像能够支持快速理解复杂环境，提高决策效率和事件回顾效果。

Conclusion: 该方法为突发事件情况感知提供了一种高效的视觉摘要方案，通过将长视频转换为简洁的全景图像，有助于快速决策和事后分析。

Abstract: First responders widely adopt body-worn cameras to document incident scenes
and support post-event analysis. However, reviewing lengthy video footage is
impractical in time-critical situations. Effective situational awareness
demands a concise visual summary that can be quickly interpreted. This work
presents a computer vision pipeline that transforms body-camera footage into
informative panoramic images summarizing the incident scene. Our method
leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate
camera trajectories and reconstruct the spatial layout of the environment. Key
viewpoints are identified by clustering camera poses along the trajectory, and
representative frames from each cluster are selected. These frames are fused
into spatially coherent panoramic images using multi-frame stitching
techniques. The resulting summaries enable rapid understanding of complex
environments and facilitate efficient decision-making and incident review.

</details>


### [7] [Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.04446)
*Kiymet Akdemir,Jing Shi,Kushal Kafle,Brian Price,Pinar Yanardag*

Main category: cs.CV

TL;DR: Plot'n Polish是一个零样本框架，用于实现一致的故事可视化生成，提供细粒度的控制能力，支持生成后修改并保持视觉和叙事一致性。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在创意领域的广泛应用，需要提供更好的控制、精炼和生成后修改能力，同时保持多帧之间的视觉和叙事一致性。现有方法缺乏这种灵活性。

Method: 提出了Plot'n Polish零样本框架，能够在不同细节层次上对故事可视化进行细粒度控制，支持一致的故事生成和编辑。

Result: 该框架能够实现一致的故事可视化生成，提供fine和coarse级别的编辑能力，同时保持跨多帧的视觉和叙事一致性。

Conclusion: Plot'n Polish解决了现有方法在故事可视化中缺乏灵活性和一致性的问题，为创作者提供了无缝制作和精炼视觉故事的能力。

Abstract: Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Insights from Gradient Dynamics: Gradient Autoscaled Normalization](https://arxiv.org/abs/2509.03677)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 本文提出了一种无超参数的梯度归一化方法，通过分析梯度方差和标准差在训练中的演化规律，使梯度缩放与其自然演化保持一致，从而稳定优化过程并保持收敛保证。


<details>
  <summary>Details</summary>
Motivation: 梯度动态在深度神经网络的稳定性和泛化性中起核心作用。研究发现梯度方差和标准差在训练过程中存在一致的演化模式，但现有方法可能导致意外的梯度放大问题。

Method: 基于梯度统计量的经验观察，提出超参数自由的梯度归一化方法，使梯度缩放与其自然演化对齐，防止意外放大并稳定优化。

Result: 在CIFAR-100基准测试中，使用ResNet-20、ResNet-56和VGG-16-BN等网络，该方法保持或提高了测试精度，即使在强泛化条件下也表现良好。

Conclusion: 研究强调了直接跟踪梯度动态的重要性，旨在弥合理论期望与实证行为之间的差距，为未来优化研究提供见解，同时提出的方法具有实际性能优势。

Abstract: Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.

</details>


### [9] [Predicting Traffic Accident Severity with Deep Neural Networks](https://arxiv.org/abs/2509.03819)
*Meghan Bibb,Pablo Rivas,Mahee Tayba*

Main category: cs.LG

TL;DR: 基于神经网络的深度学习模型在交通事故严重程度分类中达到92%准确率


<details>
  <summary>Details</summary>
Motivation: 利用机器学习新进展研究交通事故数据，减少事故风险，并在不平衡数据上实现良好的预测性能

Method: 首先分析特征共线性，使用自编码器进行无监督维度降低，然后通过密集网络进行交通事故严重程度分类

Result: 实验结果显示，使用提出的深度神经网络模型在交叉验证中达到了最92%的准确率

Conclusion: 神经网络基础模型在交通事故严重程度分类任务中表现出艹，为交通安全风险管理提供了有效的解决方案

Abstract: Traffic accidents can be studied to mitigate the risk of further events.
Recent advances in machine learning have provided an alternative way to study
data associated with traffic accidents. New models achieve good generalization
and high predictive power over imbalanced data. In this research, we study
neural network-based models on data related to traffic accidents. We begin
analyzing relative feature colinearity and unsupervised dimensionality
reduction through autoencoders, followed by a dense network. The features are
related to traffic accident data and the target is to classify accident
severity. Our experiments show cross-validated results of up to 92% accuracy
when classifying accident severity using the proposed deep neural network.

</details>


### [10] [Characteristic Energy Behavior Profiling of Non-Residential Buildings](https://arxiv.org/abs/2509.04322)
*Haley Dozier,Althea Henslee*

Main category: cs.LG

TL;DR: 美军基地基础设施面临气候变化风险，需要数据驱动的能源使用行为模型来评估弹性和创建基准测量


<details>
  <summary>Details</summary>
Motivation: 美军基地依赖商业能源和水源，面临极端天气威胁，需要提高对能源系统中断的弹性能力

Method: 提出数据驱动的行为模型，使用多模态数据来分析、预测和聚类非住宅建筑的能源使用行为

Result: 建立个体建筑行为模型，能够准确分析多模态能源数据，为弹性措施提供基准评估

Conclusion: 该方法论文为美军基地能源系统的气候弹性提供了数据驱动的解决方案，可用于预测中断影响和标待未来强化措施

Abstract: Due to the threat of changing climate and extreme weather events, the
infrastructure of the United States Army installations is at risk. More than
ever, climate resilience measures are needed to protect facility assets that
support critical missions and help generate readiness. As most of the Army
installations within the continental United States rely on commercial energy
and water sources, resilience to the vulnerabilities within independent energy
resources (electricity grids, natural gas pipelines, etc) along with a baseline
understanding of energy usage within installations must be determined. This
paper will propose a data-driven behavioral model to determine behavior
profiles of energy usage on installations. These profiles will be used 1) to
create a baseline assessment of the impact of unexpected disruptions on energy
systems and 2) to benchmark future resiliency measures. In this methodology,
individual building behavior will be represented with models that can
accurately analyze, predict, and cluster multimodal data collected from energy
usage of non-residential buildings. Due to the nature of Army installation
energy usage data, similarly structured open access data will be used to
illustrate this methodology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 基于Model Context Protocol的零训练映射系统，通过外部资源查询提高OMOP CDM标准化映射的效率和准确性，避免大语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 解决OMOP CDM标准化过程中手动映射医学术语的资源浪费和错误风险，而大语言模型直接应用又存在幻觉风险

Method: 基于Model Context Protocol(MCP)开发零训练映射系统，允许LLM与外部资源和工具交互，实现实时词汇查询和结构化推理输出

Result: 系统显著提高了映射效率和准确性，支持可解释的映射过程，适用于探索性和生产环境

Conclusion: 该方法为OMOP CDM标准化映射提供了一种高效、准确且安全的解决方案，免去了训练成本和专家验证的需求

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [12] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: 通过构建确定性知识图和LLM生成知识图的对比分析，提出了一种可扩展的实时监控方法来评估生成式AI的可靠性问题


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI模型存在的幻觉、语义偏移和偏见等可靠性问题，充当黑盒模型缺乏透明评估方法的挑战

Method: 构建两个并行知识图：确定性KG（规则基础）和LLM生成KG（实时数据流），使用ICR、IPR、CI等KG指标进行结构和语义偏差量化

Result: 开发了自动化实时监控框架，能够主动识别和标记显著偏差，及时检测语义异常或幻觉

Conclusion: 该结构化、指标驱动的对比方法为生成式AI提供了稳健且可扩展的可靠性评估框架

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [13] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: 人工智能虽然可能在计算能力方面超越人类，但缺乏生物中央神经系统导致无法真正体验情感和理解行为后果，因此不具备领导宇宙的豪格。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统在达到通用智能后是否会超越人类成为宇宙的正当领导者，以及人类与AI的根本差异所在。

Method: 通过对比分析人类生物权威（DNA）与AI硬件基础（硅）的本质差异，重点考察中央神经系统在情感体验和道德理解中的关键作用。

Result: 识别出中央神经系统是人类与AI的核心区别，证明生物体验是发展可持续伦理系统的前提，因此人类更适合作为宇宙领导者。

Conclusion: 即使AI在技术能力方面超越人类，但由于缺乏生物中央神经系统所提供的情感体验能力，AI系统永远无法发展出真正的道德理解和豪格领导力，人类的DNA基因依然是宇宙领导权的最佳基础。

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
*Matilde Contestabile,Chiara Ferrara,Alberto Giovannetti,Giovanni Parrillo,Andrea Vandin*

Main category: cs.CL

TL;DR: ProLiFIC是一个从1987-2022年意大利立法过程的综合事件日志，使用LLM从非结构化数据创建，为法律流程挖掘提供基准数据集。


<details>
  <summary>Details</summary>
Motivation: 流程挖掘在工业领域应用成熟，但在法律领域受限于数据可获取性和质量。意大利立法过程缺乏结构化的事件日志数据，限制了流程挖掘在法律系统的应用效果。

Method: 从Normattiva门户网站获取非结构化数据，使用大型语言模型(LLMs)进行结构化处理，创建了名为ProLiFIC的意大利立法过程事件日志数据集。

Result: 成功构建了涵盖1987-2022年意大利立法过程的综合事件日志，展示了初步分析示例，证明了数据集的有效性和实用性。

Conclusion: ProLiFIC数据集为法律流程挖掘提供了有价值的基准，促进了流程挖掘与大型语言模型的结合，推动了法律领域流程挖掘的新发展。

Abstract: Process Mining (PM), initially developed for industrial and business
contexts, has recently been applied to social systems, including legal ones.
However, PM's efficacy in the legal domain is limited by the accessibility and
quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in
Italian Chambers), a comprehensive event log of the Italian lawmaking process
from 1987 to 2022. Created from unstructured data from the Normattiva portal
and structured using large language models (LLMs), ProLiFIC aligns with recent
efforts in integrating PM with LLMs. We exemplify preliminary analyses and
propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [15] [Explicit and Implicit Data Augmentation for Social Event Detection](https://arxiv.org/abs/2509.04202)
*Congbo Ma,Yuxia Wang,Jia Wu,Jian Yang,Jing Du,Zitai Qiu,Qing Li,Hu Wang,Preslav Nakov*

Main category: cs.CL

TL;DR: SED-Aug是一个用于社交媒体事件检测的双重增强框架，通过显式文本增强和隐式特征空间增强来解决标注数据稀缺问题，在两个Twitter数据集上F1分数显著提升15-17%。


<details>
  <summary>Details</summary>
Motivation: 社交媒体事件检测依赖标注数据，但人工标注成本高且费时，需要解决数据稀缺问题以提升模型性能。

Method: 提出双重增强框架：显式增强使用大语言模型进行5种文本生成策略；隐式增强在特征空间对结构融合嵌入设计5种扰动技术，保持语义和关系属性。

Result: 在Twitter2012数据集上平均F1分数比最佳基线模型提升约17.67%，在Twitter2018数据集上提升约15.57%。

Conclusion: SED-Aug框架通过双重增强有效提升数据多样性和模型鲁棒性，显著改善了社交媒体事件检测性能。

Abstract: Social event detection involves identifying and categorizing important events
from social media, which relies on labeled data, but annotation is costly and
labor-intensive. To address this problem, we propose Augmentation framework for
Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework,
which combines explicit text-based and implicit feature-space augmentation to
enhance data diversity and model robustness. The explicit augmentation utilizes
large language models to enhance textual information through five diverse
generation strategies. For implicit augmentation, we design five novel
perturbation techniques that operate in the feature space on structural fused
embeddings. These perturbations are crafted to keep the semantic and relational
properties of the embeddings and make them more diverse. Specifically, SED-Aug
outperforms the best baseline model by approximately 17.67% on the Twitter2012
dataset and by about 15.57% on the Twitter2018 dataset in terms of the average
F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [16] [Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach](https://arxiv.org/abs/2509.03563)
*Quan Quan,Jiwen Xu,Runxiao Liu,Yi Ding,Jiaxing Che,Kai-Yuan Cai*

Main category: cs.RO

TL;DR: 本文提出了一种基于物理启发的无人机集群协同运输方法，模仿桌腿负载分配的耗散力学机制，实现了无需显式通信的自主编队稳定和自适应负载分配。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、通信依赖性和动态故障鲁棒性方面存在不足，而无人机集群协同运输在物流和灾难响应领域具有变革潜力。

Method: 开发了分散式耗散力模型，每个机器人根据邻近机器人和悬挂负载动态调整位置，类似于能量耗散的桌腿反应机制。

Result: 仿真显示跟踪误差比现有方法降低20%-68.5%；真实实验中6架无人机在单机故障、断连事件、25%负载变化和40%线长不确定条件下达到94%成功率，在4级风力下仍保持强鲁棒性。

Conclusion: 该方法将群体智能与机械稳定性原理相结合，为异构空中系统在通信受限环境中处理复杂运输任务提供了可扩展框架。

Abstract: In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.

</details>


### [17] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 提出了DRLR框架，通过改进IBRL算法的动作选择模块来减少自举误差，使用SAC替代TD3防止策略陷入次优解，在机器人任务中验证了有效性并实现了sim2real部署。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在机器人任务中探索效率低的问题，特别是自举误差导致的低效探索和策略收敛到次优解的问题。

Method: 基于IBRL算法改进动作选择模块，提供校准的Q值来减少自举误差；使用SAC作为RL策略替代TD3；在桶装和开抽屉两个机器人任务上进行验证。

Result: 实验证明该方法能有效减少自举误差和防止过拟合，在不同状态-动作维度和演示质量的任务中都表现出鲁棒性，并成功实现了真实轮式装载机上的部署。

Conclusion: DRLR框架通过改进的动作选择模块和SAC策略，有效提高了探索效率，在仿真和真实机器人任务中都取得了良好性能，证明了其在工业机器人应用中的实用性。

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>
