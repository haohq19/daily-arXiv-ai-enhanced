<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 15]
- [cs.LG](#cs.LG) [Total: 15]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.RO](#cs.RO) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2510.09867)
*Zhi Chen,Xin Yu,Xiaohui Tao,Yan Li,Zi Huang*

Main category: cs.CV

TL;DR: 提出Cluster-Aware Prompt Ensemble Learning (CAPEL)框架，通过在分类logits空间进行集成而非特征空间平均，更好地保持上下文提示的聚类特性，提升视觉语言模型的零样本迁移性能。


<details>
  <summary>Details</summary>
Motivation: 传统的提示集成方法通过平均文本特征来代表类别，但这种方法会使类别质心偏离真实分布，导致次优结果。

Method: CAPEL框架将图像分类到多个类别聚类中，每个聚类由不同的提示表示；在分类logits空间进行集成；引入聚类保持正则化项；集成自适应提示加权技术。

Result: 该方法能更好地对齐视觉特征分布，保持提示的区分性，对存在缺陷或模糊的提示具有鲁棒性。

Conclusion: CAPEL框架通过保持上下文提示的聚类特性，在多个数据集和任务上实现了更稳健的零样本分类性能。

Abstract: Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across
various tasks by pre-training on numerous image-text pairs. These models often
benefit from using an ensemble of context prompts to represent a class. Despite
being effective, conventional prompt ensembling that averages textual features
of context prompts often yields suboptimal results. This is because feature
averaging shifts the class centroids away from the true class distribution. To
address this issue, we propose the Cluster-Aware Prompt Ensemble Learning
(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL
classifies images into one of several class clusters, each represented by a
distinct prompt. Instead of ensembling prompts in the feature space, we perform
ensembling in the classification logits space, aligning better with the visual
feature distribution. To further optimize prompt fine-tuning while maintaining
cluster-specific discriminative power, we introduce a cluster-preserving
regularization term. This ensures that prompts remain distinct and specialized
for different clusters, preventing collapse into a uniform direction.
Additionally, we integrate an adaptive prompt weighting technique to
dynamically adjust the attention weights for flawed or ambiguous prompts,
ensuring robust performance across diverse datasets and tasks.

</details>


### [2] [Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting](https://arxiv.org/abs/2510.10257)
*Abdelrhman Elrawy,Emad A. Mohammed*

Main category: cs.CV

TL;DR: 提出一个改进3D高斯泼溅在少样本场景下效率的框架，通过新的致密化触发机制和保守修剪策略，显著减少基元数量同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在少样本场景下存在过拟合和重建膨胀问题，现有方法虽然提升质量但显著增加基元数量，需要更高效的优化方法。

Method: 用不透明度梯度作为渲染误差的轻量级代理替代位置梯度启发式，结合保守修剪策略防止破坏性优化循环，并使用标准深度相关损失提供几何指导。

Result: 在3视图LLFF数据集上比FSGS紧凑40%以上（32k vs 57k基元），在Mip-NeRF 360数据集上减少约70%基元，在质量-效率帕累托前沿达到新最优。

Conclusion: 该框架通过重新设计核心优化策略，在少样本视图合成中实现了基元数量的大幅减少，仅以适度的重建指标损失为代价，建立了新的质量-效率平衡标准。

Abstract: 3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its
standard adaptive density control (ADC) can lead to overfitting and bloated
reconstructions. While state-of-the-art methods like FSGS improve quality, they
often do so by significantly increasing the primitive count. This paper
presents a framework that revises the core 3DGS optimization to prioritize
efficiency. We replace the standard positional gradient heuristic with a novel
densification trigger that uses the opacity gradient as a lightweight proxy for
rendering error. We find this aggressive densification is only effective when
paired with a more conservative pruning schedule, which prevents destructive
optimization cycles. Combined with a standard depth-correlation loss for
geometric guidance, our framework demonstrates a fundamental improvement in
efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k
vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a
reduction of approximately 70%. This dramatic gain in compactness is achieved
with a modest trade-off in reconstruction metrics, establishing a new
state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view
synthesis.

</details>


### [3] [AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration](https://arxiv.org/abs/2510.10395)
*Xinlong Chen,Yue Ding,Weihong Lin,Jingyun Hua,Linli Yao,Yang Shi,Bozhou Li,Yuanxing Zhang,Qiang Liu,Pengfei Wan,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: AVoCaDO是一个强大的视听视频字幕生成器，通过两阶段后训练流程实现音频和视觉模态的时间编排，在多个基准测试中显著优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 视听视频字幕旨在生成语义丰富的描述，实现视觉和听觉事件的时间对齐，从而有益于视频理解和生成。

Method: 提出两阶段后训练流程：AVoCaDO SFT在10.7万高质量时间对齐的视听字幕数据集上微调模型；AVoCaDO GRPO利用定制奖励函数增强时间一致性和对话准确性，同时规范化字幕长度和减少崩溃。

Result: AVoCaDO在四个视听视频字幕基准测试中显著优于现有开源模型，在仅视觉设置的VDC和DREAM-1K基准上也实现了有竞争力的性能。

Conclusion: AVoCaDO通过时间编排驱动的视听视频字幕生成方法，在多个基准测试中表现出色，证明了其有效性。

Abstract: Audiovisual video captioning aims to generate semantically rich descriptions
with temporal alignment between visual and auditory events, thereby benefiting
both video understanding and generation. In this paper, we present AVoCaDO, a
powerful audiovisual video captioner driven by the temporal orchestration
between audio and visual modalities. We propose a two-stage post-training
pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated
dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)
AVoCaDO GRPO, which leverages tailored reward functions to further enhance
temporal coherence and dialogue accuracy while regularizing caption length and
reducing collapse. Experimental results demonstrate that AVoCaDO significantly
outperforms existing open-source models across four audiovisual video
captioning benchmarks, and also achieves competitive performance on the VDC and
DREAM-1K benchmark under visual-only settings.

</details>


### [4] [GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction](https://arxiv.org/abs/2510.10546)
*Zuha Fatima,Muhammad Anser Sohaib,Muhammad Talha,Sidra Sultana,Ayesha Kanwal,Nazia Perwaiz*

Main category: cs.CV

TL;DR: GLOFNet是一个用于冰川湖溃决洪水监测和预测的多模态数据集，整合了Sentinel-2多光谱影像、NASA冰川速度数据和MODIS地表温度数据，专注于喀喇昆仑山脉的Shisper冰川。


<details>
  <summary>Details</summary>
Motivation: 冰川湖溃决洪水是高山地区罕见但破坏性强的灾害，现有研究受限于碎片化和单模态数据，缺乏预测能力。需要结合视觉指标和物理前兆的协调数据集。

Method: 整合三种互补数据源：Sentinel-2多光谱影像用于空间监测，NASA ITS_LIVE速度产品用于冰川运动学，MODIS地表温度记录覆盖二十多年。预处理包括云掩膜、质量过滤、归一化、时间插值、数据增强和周期性编码，然后进行多模态协调。

Result: 探索性分析揭示了冰川速度的季节性周期、每十年约0.8K的长期升温趋势，以及冰冻圈条件的空间异质性。GLOFNet数据集已公开可用。

Conclusion: GLOFNet通过解决类别不平衡、云污染和粗分辨率等挑战，为罕见灾害预测的多模态深度学习方法提供了结构化基准基础。

Abstract: Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high
mountain regions, yet predictive research is hindered by fragmented and
unimodal data. Most prior efforts emphasize post-event mapping, whereas
forecasting requires harmonized datasets that combine visual indicators with
physical precursors. We present GLOFNet, a multimodal dataset for GLOF
monitoring and prediction, focused on the Shisper Glacier in the Karakoram. It
integrates three complementary sources: Sentinel-2 multispectral imagery for
spatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and
MODIS Land Surface Temperature records spanning over two decades. Preprocessing
included cloud masking, quality filtering, normalization, temporal
interpolation, augmentation, and cyclical encoding, followed by harmonization
across modalities. Exploratory analysis reveals seasonal glacier velocity
cycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in
cryospheric conditions. The resulting dataset, GLOFNet, is publicly available
to support future research in glacial hazard prediction. By addressing
challenges such as class imbalance, cloud contamination, and coarse resolution,
GLOFNet provides a structured foundation for benchmarking multimodal deep
learning approaches to rare hazard prediction.

</details>


### [5] [Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes](https://arxiv.org/abs/2510.10577)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 提出基于扩散模型和帧-事件外观-边界融合的光流估计框架Diff-ABFlow，解决高速和低光场景下运动模糊和光照不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统光流估计在高速和低光场景中面临运动模糊和光照不足的挑战，导致纹理减弱、噪声放大、外观饱和度和边界完整性恶化。帧相机提供密集外观但边界稀疏，事件相机提供稀疏外观但边界密集。现有方法通过特征融合或域适应引入事件数据改善边界完整性，但外观特征仍然恶化，影响判别式和生成式模型的性能。

Method: 采用扩散模型学习从噪声流到清晰流的映射，不受恶化视觉特征影响。提出Diff-ABFlow框架，结合帧-事件外观-边界融合，利用帧相机的外观饱和度和事件相机的边界完整性。

Result: 该方法通过扩散模型避免了恶化视觉特征对光流估计的影响，利用帧-事件融合同时获得良好的外观和边界信息。

Conclusion: 基于扩散模型的帧-事件外观-边界融合框架能够有效解决高速和低光场景下的光流估计问题，克服传统方法在恶化视觉条件下的局限性。

Abstract: Optical flow estimation has achieved promising results in conventional scenes
but faces challenges in high-speed and low-light scenes, which suffer from
motion blur and insufficient illumination. These conditions lead to weakened
texture and amplified noise and deteriorate the appearance saturation and
boundary completeness of frame cameras, which are necessary for motion feature
matching. In degraded scenes, the frame camera provides dense appearance
saturation but sparse boundary completeness due to its long imaging time and
low dynamic range. In contrast, the event camera offers sparse appearance
saturation, while its short imaging time and high dynamic range gives rise to
dense boundary completeness. Traditionally, existing methods utilize feature
fusion or domain adaptation to introduce event to improve boundary
completeness. However, the appearance features are still deteriorated, which
severely affects the mostly adopted discriminative models that learn the
mapping from visual features to motion fields and generative models that
generate motion fields based on given visual features. So we introduce
diffusion models that learn the mapping from noising flow to clear flow, which
is not affected by the deteriorated visual features. Therefore, we propose a
novel optical flow estimation framework Diff-ABFlow based on diffusion models
with frame-event appearance-boundary fusion.

</details>


### [6] [A Machine Learning Perspective on Automated Driving Corner Cases](https://arxiv.org/abs/2510.10653)
*Sebastian Schmidt,Julius Körner,Stephan Günnemann*

Main category: cs.CV

TL;DR: 提出了一种基于数据分布视角的角点案例识别方法，统一了现有基于场景的角点案例分类法，在标准基准测试中表现出色，并支持组合角点案例分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于示例的角点案例分类方法不可扩展且缺乏数据覆盖视角，无法考虑机器学习模型训练数据的泛化能力。需要一种考虑底层数据分布的新方法来实现安全的自动驾驶操作。

Method: 提出了一种新颖的机器学习方法，考虑底层数据分布，开发了用于个体样本感知的有效角点案例识别框架。

Result: 该方法(i)统一了现有基于场景的角点案例分类法，(ii)在标准基准测试中实现强大的角点案例检测性能，(iii)通过新引入的雾增强Lost & Found数据集支持组合角点案例分析。

Conclusion: 这些结果为角点案例识别提供了原则性基础，强调了无需手动规范的定义方法。

Abstract: For high-stakes applications, like autonomous driving, a safe operation is
necessary to prevent harm, accidents, and failures. Traditionally, difficult
scenarios have been categorized into corner cases and addressed individually.
However, this example-based categorization is not scalable and lacks a data
coverage perspective, neglecting the generalization to training data of machine
learning models. In our work, we propose a novel machine learning approach that
takes the underlying data distribution into account. Based on our novel
perspective, we present a framework for effective corner case recognition for
perception on individual samples. In our evaluation, we show that our approach
(i) unifies existing scenario-based corner case taxonomies under a
distributional perspective, (ii) achieves strong performance on corner case
detection tasks across standard benchmarks for which we extend established
out-of-distribution detection benchmarks, and (iii) enables analysis of
combined corner cases via a newly introduced fog-augmented Lost & Found
dataset. These results provide a principled basis for corner case recognition,
underlining our manual specification-free definition.

</details>


### [7] [Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection](https://arxiv.org/abs/2510.10750)
*Laura Weihl,Nejc Novak,Stefan H. Bengtson,Malte Pedersen*

Main category: cs.CV

TL;DR: 提出AURA首个多标注者水下视觉异常检测基准数据集，评估四种VAD模型在两种海洋场景中的表现，发现模型性能对训练数据量和正常场景定义高度敏感。


<details>
  <summary>Details</summary>
Motivation: 水下视频监测是评估海洋生物多样性的重要方法，但海量无事件视频使得人工检查极不实用，需要自动识别有趣或异常事件。

Method: 引入AURA多标注者基准数据集，评估四种基于深度神经网络的视觉异常检测模型，采用鲁棒的帧选择策略提取有意义的视频片段。

Result: 当前VAD模型性能差异显著，对训练数据量和正常场景的视觉内容变异性高度敏感，软标签和共识标签具有重要价值。

Conclusion: 研究为支持科学探索和可扩展生物多样性监测提供了实用方法，强调了多标注者基准和鲁棒帧选择的重要性。

Abstract: Underwater video monitoring is a promising strategy for assessing marine
biodiversity, but the vast volume of uneventful footage makes manual inspection
highly impractical. In this work, we explore the use of visual anomaly
detection (VAD) based on deep neural networks to automatically identify
interesting or anomalous events. We introduce AURA, the first multi-annotator
benchmark dataset for underwater VAD, and evaluate four VAD models across two
marine scenes. We demonstrate the importance of robust frame selection
strategies to extract meaningful video segments. Our comparison against
multiple annotators reveals that VAD performance of current models varies
dramatically and is highly sensitive to both the amount of training data and
the variability in visual content that defines "normal" scenes. Our results
highlight the value of soft and consensus labels and offer a practical approach
for supporting scientific exploration and scalable biodiversity monitoring.

</details>


### [8] [Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation](https://arxiv.org/abs/2510.11005)
*Kai Han,Siqi Ma,Chengxuan Qian,Jun Chen,Chongwen Lyu,Yuqing Song,Zhe Liu*

Main category: cs.CV

TL;DR: 提出了FASS框架，通过前景感知模块、小波变换频率增强模块和边缘约束模块，解决医学图像中低对比度肿瘤分割的挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型在复杂低对比度背景下难以聚焦前景区域，特别是当恶性肿瘤与正常器官相似时，上下文区分变得困难。

Method: 1. 前景感知模块放大背景与整个体积空间的区分；2. 基于小波变换的特征级频率增强模块提取判别性高频特征；3. 边缘约束模块保持分割边界的几何连续性。

Result: 在多个医学数据集上的广泛实验显示，所有指标均表现出优越性能，特别是在复杂条件下的鲁棒性和精细结构识别方面。

Conclusion: 该框架显著提升了低对比度图像的分割效果，为更多样化和复杂的医学成像场景应用铺平了道路。

Abstract: Accurate segmentation of tumors and adjacent normal tissues in medical images
is essential for surgical planning and tumor staging. Although foundation
models generally perform well in segmentation tasks, they often struggle to
focus on foreground areas in complex, low-contrast backgrounds, where some
malignant tumors closely resemble normal organs, complicating contextual
differentiation. To address these challenges, we propose the Foreground-Aware
Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware
module to amplify the distinction between background and the entire volume
space, allowing the model to concentrate more effectively on target areas.
Next, a feature-level frequency enhancement module, based on wavelet transform,
extracts discriminative high-frequency features to enhance boundary recognition
and detail perception. Eventually, we introduce an edge constraint module to
preserve geometric continuity in segmentation boundaries. Extensive experiments
on multiple medical datasets demonstrate superior performance across all
metrics, validating the effectiveness of our framework, particularly in
robustness under complex conditions and fine structure recognition. Our
framework significantly enhances segmentation of low-contrast images, paving
the way for applications in more diverse and complex medical imaging scenarios.

</details>


### [9] [LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation](https://arxiv.org/abs/2510.11063)
*Chang Liu,Henghui Ding,Kaining Ying,Lingyi Hong,Ning Xu,Linjie Yang,Yuchen Fan,Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han,Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Chang Soo Lim,Joonyoung Moon,Donghyeon Cho,Tingmin Li,Yixuan Li,Yang Yang,An Yan,Leilei Cao,Feng Lu,Ran Hong,Youhai Jiang,Fengjie Zhu,Yujie Xie,Hongyang Zhang,Zhihui Liu,Shihai Ruan,Quanzhu Niu,Dengxian Gong,Shihao Chen,Tao Zhang,Yikang Zhou,Haobo Yuan,Lu Qi,Xiangtai Li,Shunping Ji,Ran Hong,Feng Lu,Leilei Cao,An Yan,Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: 本文介绍了ICCV 2025第7届大规模视频目标分割挑战赛，包含传统VOS和RVOS赛道，以及新增的复杂VOS赛道MOSEv2，后者引入了更现实的挑战场景。


<details>
  <summary>Details</summary>
Motivation: 推动视频目标分割在真实场景中的鲁棒性，通过增加更具挑战性的场景来测试长期一致性和泛化能力。

Method: 保留传统VOS和RVOS赛道，新增MOSEv2赛道，采用更严格的评估指标J&F和J&Ḟ来衡量不同尺度和消失情况下的性能。

Result: 总结了数据集和协议，突出了顶级解决方案，并提炼出LLM/MLLM组件和内存感知传播等新兴趋势。

Conclusion: 为在野外环境中实现弹性的、语言感知的视频分割指明了未来发展方向。

Abstract: This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard ${J}$, $F$, and ${J\&F}$ metrics for
VOS and RVOS, while MOSEv2 adopts ${J\&\dot{F}}$ as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.

</details>


### [10] [LightPneumoNet: Lightweight Pneumonia Classifier](https://arxiv.org/abs/2510.11232)
*Neilansh Chauhan,Piyush Kumar Gupta,Faraz Doja*

Main category: cs.CV

TL;DR: LightPneumoNet是一个轻量级CNN模型，专门用于肺炎X光片诊断，仅38.8万参数，在资源受限环境中实现高精度肺炎检测。


<details>
  <summary>Details</summary>
Motivation: 解决在资源有限环境下部署大型深度学习模型进行肺炎诊断的困难，提供可访问且准确的诊断方案。

Method: 构建轻量级CNN架构，包含4个卷积块，仅388,082个可训练参数。使用5,856张胸部X光片数据集，进行图像预处理（224x224调整、灰度转换、像素归一化）和数据增强（旋转、缩放、剪切）。

Result: 在独立测试集上表现优异：准确率0.942，精确率0.92，F1分数0.96，敏感度（召回率）0.99，内存占用仅1.48MB。

Conclusion: LightPneumoNet在保持高敏感度的同时实现了轻量化，可在低成本硬件上部署，为资源匮乏诊所提供可靠的计算机辅助诊断工具。

Abstract: Effective pneumonia diagnosis is often challenged by the difficulty of
deploying large, computationally expensive deep learning models in
resource-limited settings. This study introduces LightPneumoNet, an efficient,
lightweight convolutional neural network (CNN) built from scratch to provide an
accessible and accurate diagnostic solution for pneumonia detection from chest
X-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.
Preprocessing included image resizing to 224x224, grayscale conversion, and
pixel normalization, with data augmentation (rotation, zoom, shear) to prevent
overfitting. The custom architecture features four blocks of stacked
convolutional layers and contains only 388,082 trainable parameters, resulting
in a minimal 1.48 MB memory footprint. On the independent test set, our model
delivered exceptional performance, achieving an overall accuracy of 0.942,
precision of 0.92, and an F1-Score of 0.96. Critically, it obtained a
sensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify
true pneumonia cases and minimize clinically significant false negatives.
Notably, LightPneumoNet achieves this high recall on the same dataset where
existing approaches typically require significantly heavier architectures or
fail to reach comparable sensitivity levels. The model's efficiency enables
deployment on low-cost hardware, making advanced computer-aided diagnosis
accessible in underserved clinics and serving as a reliable second-opinion tool
to improve patient outcomes.

</details>


### [11] [When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models](https://arxiv.org/abs/2510.11302)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 本文首次对监督学习检测器(YOLO)与零样本视觉语言模型(Gemini)进行成本效益分析，建立了架构选择的定量平衡点阈值。


<details>
  <summary>Details</summary>
Motivation: 传统监督检测需要大量标注成本，而零样本VLM无需标注但精度较低，需要系统比较两者的成本效益。

Method: 在1000张分层COCO图像和200张多样化产品图像上进行系统评估，结合详细的总拥有成本建模，分析准确率和成本效益。

Result: 监督YOLO准确率91.2%，零样本Gemini准确率68.5%，YOLO优势需要5500万次推理才合理；Gemini在多样化产品类别准确率52.3%，YOLO为0%；Gemini每次正确检测成本$0.00050远低于YOLO的$0.143。

Conclusion: 最优架构选择取决于部署量、类别稳定性、预算约束和精度要求，而非纯粹技术性能指标。

Abstract: Object detection systems have traditionally relied on supervised learning
with manually annotated bounding boxes, achieving high accuracy at the cost of
substantial annotation investment. The emergence of Vision-Language Models
(VLMs) offers an alternative paradigm enabling zero-shot detection through
natural language queries, eliminating annotation requirements but operating
with reduced accuracy. This paper presents the first comprehensive
cost-effectiveness analysis comparing supervised detection (YOLO) with
zero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on
1,000 stratified COCO images and 200 diverse product images spanning consumer
electronics and rare categories, combined with detailed Total Cost of Ownership
modeling, we establish quantitative break-even thresholds governing
architecture selection. Our findings reveal that supervised YOLO achieves 91.2%
accuracy versus 68.5% for zero-shot Gemini on standard categories, representing
a 22.7 percentage point advantage that costs $10,800 in annotation for
100-category systems. However, this advantage justifies investment only beyond
55 million inferences, equivalent to 151,000 images daily for one year.
Zero-shot Gemini demonstrates 52.3% accuracy on diverse product categories
(ranging from highly web-prevalent consumer electronics at 75-85% to rare
specialized equipment at 25-40%) where supervised YOLO achieves 0% due to
architectural constraints preventing detection of untrained classes. Cost per
Correct Detection analysis reveals substantially lower per-detection costs for
Gemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We
develop decision frameworks demonstrating that optimal architecture selection
depends critically on deployment volume, category stability, budget
constraints, and accuracy requirements rather than purely technical performance
metrics.

</details>


### [12] [Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation](https://arxiv.org/abs/2510.11305)
*Jean-Paul Travert,Cédric Goeury,Sébastien Boyaval,Vito Bacchi,Fabrice Zaoui*

Main category: cs.CV

TL;DR: 本研究评估了SAR图像预处理、洪水制图和水深估计方法对洪水模拟的影响，发现方法选择和参数调整会显著影响结果精度，建议采用集成方法而非单一配置。


<details>
  <summary>Details</summary>
Motivation: 洪水制图和水深估计对校准和验证水力模型至关重要，但SAR图像处理中不同方法和参数选择的影响尚未系统研究。

Method: 使用SAR图像评估多种预处理（特别是斑点噪声去除）、洪水制图和水深估计方法，通过考虑预处理图像、洪水图和水深场的集成来研究方法和超参数选择的影响。

Result: 斑点滤波器的选择会改变数平方公里的洪水范围估计；监督方法优于无监督方法，但调优的无监督方法（如局部阈值或变化检测）可获得相当结果；预处理和洪水制图步骤的累积不确定性导致水深估计高度可变。

Conclusion: 必须考虑整个处理流程（预处理、洪水制图和水深估计方法及其超参数），建议采用集成方法并考虑方法不确定性；洪水制图中方法选择影响最大，水深估计中洪水图输入和方法超参数最具影响力。

Abstract: Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)
imagery are crucial for calibrating and validating hydraulic models. This study
uses SAR imagery to evaluate various preprocessing (especially speckle noise
reduction), flood mapping, and water depth estimation methods. The impact of
the choice of method at different steps and its hyperparameters is studied by
considering an ensemble of preprocessed images, flood maps, and water depth
fields. The evaluation is conducted for two flood events on the Garonne River
(France) in 2019 and 2021, using hydrodynamic simulations and in-situ
observations as reference data. Results show that the choice of speckle filter
alters flood extent estimations with variations of several square kilometers.
Furthermore, the selection and tuning of flood mapping methods also affect
performance. While supervised methods outperformed unsupervised ones, tuned
unsupervised approaches (such as local thresholding or change detection) can
achieve comparable results. The compounded uncertainty from preprocessing and
flood mapping steps also introduces high variability in the water depth field
estimates. This study highlights the importance of considering the entire
processing pipeline, encompassing preprocessing, flood mapping, and water depth
estimation methods and their associated hyperparameters. Rather than relying on
a single configuration, adopting an ensemble approach and accounting for
methodological uncertainty should be privileged. For flood mapping, the method
choice has the most influence. For water depth estimation, the most influential
processing step was the flood map input resulting from the flood mapping step
and the hyperparameters of the methods.

</details>


### [13] [SNAP: Towards Segmenting Anything in Any Point Cloud](https://arxiv.org/abs/2510.11565)
*Aniket Gupta,Hanhui Wang,Charles Saunders,Aruni RoyChowdhury,Hanumant Singh,Huaizu Jiang*

Main category: cs.CV

TL;DR: SNAP是一个统一的交互式3D点云分割模型，支持跨域（室内、室外、航空）的点基和文本基提示，通过多数据集训练和域自适应归一化实现泛化能力，在零样本基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前3D点云分割方法局限于单一域和单一交互形式的问题，以及多数据集训练导致的负迁移和泛化能力不足。

Method: 在7个跨域数据集上训练，使用域自适应归一化防止负迁移；对于文本提示分割，自动生成掩码提案并与CLIP文本查询嵌入匹配，支持全景和开放词汇分割。

Result: 在9个零样本空间提示分割基准中的8个达到最先进性能，在5个文本提示基准中均取得竞争性结果，证明统一模型可匹敌或超越专用域特定方法。

Conclusion: SNAP展示了统一模型在跨域3D点云分割中的有效性，为可扩展的3D标注提供了实用工具。

Abstract: Interactive 3D point cloud segmentation enables efficient annotation of
complex 3D scenes through user-guided prompts. However, current approaches are
typically restricted in scope to a single domain (indoor or outdoor), and to a
single form of user interaction (either spatial clicks or textual prompts).
Moreover, training on multiple datasets often leads to negative transfer,
resulting in domain-specific tools that lack generalizability. To address these
limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in
\textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D
segmentation that supports both point-based and text-based prompts across
diverse domains. Our approach achieves cross-domain generalizability by
training on 7 datasets spanning indoor, outdoor, and aerial environments, while
employing domain-adaptive normalization to prevent negative transfer. For
text-prompted segmentation, we automatically generate mask proposals without
human intervention and match them against CLIP embeddings of textual queries,
enabling both panoptic and open-vocabulary segmentation. Extensive experiments
demonstrate that SNAP consistently delivers high-quality segmentation results.
We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for
spatial-prompted segmentation and demonstrate competitive results on all 5
text-prompted benchmarks. These results show that a unified model can match or
exceed specialized domain-specific approaches, providing a practical tool for
scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/

</details>


### [14] [MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.11579)
*Hongyu Zhu,Lin Chen,Mounim A. El-Yacoubi,Mingsheng Shang*

Main category: cs.CV

TL;DR: MS-Mix是一个情感感知的多模态数据增强框架，通过情感感知样本选择、情感强度引导的混合比例计算和情感对齐损失，解决了传统Mixup在多模态情感分析中的语义不一致问题。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临标注数据稀缺的问题，传统Mixup方法在多模态场景下会产生标签模糊和语义不一致的问题，需要情感感知的混合机制。

Method: 提出MS-Mix框架，包含三个关键组件：情感感知样本选择策略防止情感矛盾样本混合；情感强度引导模块动态计算模态特定混合比例；情感对齐损失通过KL散度正则化联合训练情感强度预测器和主干网络。

Result: 在三个基准数据集和六个最先进主干网络上进行的广泛实验表明，MS-Mix始终优于现有方法，为鲁棒的多模态情感增强建立了新标准。

Conclusion: MS-Mix通过情感感知的混合机制有效解决了多模态情感分析中的数据增强问题，显著提升了模型性能。

Abstract: Multimodal Sentiment Analysis (MSA) aims to identify and interpret human
emotions by integrating information from heterogeneous data sources such as
text, video, and audio. While deep learning models have advanced in network
architecture design, they remain heavily limited by scarce multimodal annotated
data. Although Mixup-based augmentation improves generalization in unimodal
tasks, its direct application to MSA introduces critical challenges: random
mixing often amplifies label ambiguity and semantic inconsistency due to the
lack of emotion-aware mixing mechanisms. To overcome these issues, we propose
MS-Mix, an adaptive, emotion-sensitive augmentation framework that
automatically optimizes sample mixing in multimodal settings. The key
components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)
strategy that effectively prevents semantic confusion caused by mixing samples
with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module
using multi-head self-attention to compute modality-specific mixing ratios
dynamically based on their respective emotional intensities. (3) a Sentiment
Alignment Loss (SAL) that aligns the prediction distributions across
modalities, and incorporates the Kullback-Leibler-based loss as an additional
regularization term to train the emotion intensity predictor and the backbone
network jointly. Extensive experiments on three benchmark datasets with six
state-of-the-art backbones confirm that MS-Mix consistently outperforms
existing methods, establishing a new standard for robust multimodal sentiment
augmentation. The source code is available at:
https://github.com/HongyuZhu-s/MS-Mix.

</details>


### [15] [Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams](https://arxiv.org/abs/2510.11717)
*Takuya Nakabayashi,Navami Kairanda,Hideo Saito,Vladislav Golyanik*

Main category: cs.CV

TL;DR: Ev4DGS是首个仅从单目事件流中渲染非刚性变形物体新视角的方法，使用可变形3D高斯泼溅表示，无需RGB输入。


<details>
  <summary>Details</summary>
Motivation: 现有方法处理非刚性物体时需要稀疏RGB输入，这限制了实用性。本文探索是否仅从事件流中学习类似模型。

Method: 通过1) 将估计模型输出与2D事件观测空间关联的损失函数，2) 从事件生成的二值掩码训练的粗糙3D变形模型，回归可变形3D高斯泼溅表示。

Result: 在合成和真实数据集上的实验验证了Ev4DGS的有效性，相比多种朴素基线方法表现更优。

Conclusion: Ev4DGS证明了仅从事件流渲染非刚性变形物体新视角的可行性，为事件相机在动态场景中的应用开辟了新途径。

Abstract: Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction](https://arxiv.org/abs/2510.09668)
*Maryam Abdollahi Shamami,Babak Teimourpour,Farshad Sharifi*

Main category: cs.LG

TL;DR: 提出一个可解释且高效的药物相互作用预测框架，结合分子嵌入和临床知识，使用元启发式优化策略，在真实数据集上取得高精度预测性能。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用是导致可预防不良事件的主要原因，同时了解哪些药物不相互作用对于安全处方和改善患者预后同样重要。

Method: 结合两种互补的分子嵌入（Mol2Vec和SMILES-BERT）和无泄漏的基于规则的临床评分（RBScore），使用三阶段元启发式策略（RSmpl-ACO-PSO）优化轻量级神经网络分类器。

Result: 在真实数据集上实现高预测精度（DrugBank上ROC-AUC 0.911，PR-AUC 0.867），并在2型糖尿病队列中表现出良好的泛化能力。

Conclusion: 该研究为构建可靠、可解释且计算高效的模型提供了实用途径，能够支持更安全的药物治疗和临床决策。

Abstract: Drug-drug interactions (DDIs) are a leading cause of preventable adverse
events, often complicating treatment and increasing healthcare costs. At the
same time, knowing which drugs do not interact is equally important, as such
knowledge supports safer prescriptions and better patient outcomes. In this
study, we propose an interpretable and efficient framework that blends modern
machine learning with domain knowledge to improve DDI prediction. Our approach
combines two complementary molecular embeddings - Mol2Vec, which captures
fragment-level structural patterns, and SMILES-BERT, which learns contextual
chemical features - together with a leakage-free, rule-based clinical score
(RBScore) that injects pharmacological knowledge without relying on interaction
labels. A lightweight neural classifier is then optimized using a novel
three-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global
exploration and local refinement for stable performance. Experiments on
real-world datasets demonstrate that the model achieves high predictive
accuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a
clinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,
studies show how embedding fusion, RBScore, and the optimizer each contribute
to precision and robustness. Together, these results highlight a practical
pathway for building reliable, interpretable, and computationally efficient
models that can support safer drug therapies and clinical decision-making.

</details>


### [17] [A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation](https://arxiv.org/abs/2510.09705)
*Sudip Khadka,L. S. Paudel*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的特征选择框架，将偏差缓解和特征选择整合到单一学习过程中，通过动态平衡预测性能和公平性来防止隐藏依赖关系导致的偏见。


<details>
  <summary>Details</summary>
Motivation: 传统的静态特征排除策略无法有效防止隐藏依赖关系对模型预测的偏见影响，需要一种能够动态平衡准确性、泛化性和公平性的方法。

Method: 采用强化学习框架，构建多组件奖励函数，定义特征子集的动作空间，并与集成学习相结合，通过奖励信号同时考虑预测性能和公平性。

Result: 该方法能够自适应地选择特征，在训练过程中动态平衡泛化能力、准确性和公平性，避免仅依赖预处理调整或事后修正机制。

Conclusion: 该强化学习框架提供了一种灵活通用的特征选择方法，特别适用于预测变量相关且偏见可能无意中重新出现的环境。

Abstract: Static feature exclusion strategies often fail to prevent bias when hidden
dependencies influence the model predictions. To address this issue, we explore
a reinforcement learning (RL) framework that integrates bias mitigation and
automated feature selection within a single learning process. Unlike
traditional heuristic-driven filter or wrapper approaches, our RL agent
adaptively selects features using a reward signal that explicitly integrates
predictive performance with fairness considerations. This dynamic formulation
allows the model to balance generalization, accuracy, and equity throughout the
training process, rather than rely exclusively on pre-processing adjustments or
post hoc correction mechanisms. In this paper, we describe the construction of
a multi-component reward function, the specification of the agents action space
over feature subsets, and the integration of this system with ensemble
learning. We aim to provide a flexible and generalizable way to select features
in environments where predictors are correlated and biases can inadvertently
re-emerge.

</details>


### [18] [Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction](https://arxiv.org/abs/2510.09732)
*P. van Oerle,R. H. Bemthuis,F. A. Bukhsh*

Main category: cs.LG

TL;DR: 评估大型语言模型在过程模型解释中的质量与计算效率权衡，通过逐步减少行为输入来测试解释质量保持情况


<details>
  <summary>Details</summary>
Motivation: 直接从大型行为抽象（如直接跟随图或Petri网）生成解释计算成本高，需要探索在资源受限环境下更高效的过程分析方法

Method: 建立管道：(i)从固定日志的逐步缩小前缀发现模型，(ii)用LLM生成解释，(iii)用第二个LLM评估完整性、瓶颈识别和改进建议

Result: 在合成日志上，适度减少输入时解释质量基本保持，表明存在实用的成本-质量权衡

Conclusion: 研究为资源受限环境下更高效的计算辅助过程分析提供了可行路径，但需注意评估基于LLM评分且数据为合成的探索性限制

Abstract: Large Language Models (LLMs) are increasingly used to generate textual
explanations of process models discovered from event logs. Producing
explanations from large behavioral abstractions (e.g., directly-follows graphs
or Petri nets) can be computationally expensive. This paper reports an
exploratory evaluation of explanation quality under progressive
behavioral-input reduction, where models are discovered from progressively
smaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple
input sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a
second LLM to assess completeness, bottleneck identification, and suggested
improvements. On synthetic logs, explanation quality is largely preserved under
moderate reduction, indicating a practical cost-quality trade-off. The study is
exploratory, as the scores are LLM-based (comparative signals rather than
ground truth) and the data are synthetic. The results suggest a path toward
more computationally efficient, LLM-assisted process analysis in
resource-constrained settings.

</details>


### [19] [Building a Foundational Guardrail for General Agentic Systems via Synthetic Data](https://arxiv.org/abs/2510.09781)
*Yue Huang,Hang Hua,Yujun Zhou,Pengcheng Jing,Manish Nagireddy,Inkit Padhi,Greta Dolcetti,Zhangchen Xu,Subhajit Chaudhury,Ambrish Rawat,Liubov Nedoshivina,Pin-Yu Chen,Prasanna Sattigeri,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 该论文提出了AuraGen数据生成引擎和Safiron预执行安全护栏模型，通过解决数据、模型和评估三个关键差距，实现在计划阶段而非执行后检测和预防AI代理风险。


<details>
  <summary>Details</summary>
Motivation: 现有护栏大多在执行后操作，难以扩展且无法在计划级别进行可控监督。某些风险一旦执行会造成严重后果，因此在计划阶段干预是最安全的预防方式。

Method: 1) AuraGen：可控数据生成引擎，合成良性轨迹、注入分类风险、通过奖励模型过滤输出；2) Safiron：基础护栏模型，结合跨规划器适配器和紧凑守护模型，分两阶段训练；3) Pre-Exec Bench：涵盖多样化工具和分支轨迹的基准测试。

Result: 广泛实验表明，所提出的护栏在Pre-Exec Bench上相比强基线获得一致增益，消融实验进一步提炼出可操作实践。

Conclusion: 该方法为更安全的智能代理系统提供了实用模板，通过预执行安全机制有效预防风险。

Abstract: While LLM agents can plan multi-step tasks, intervening at the planning
stage-before any action is executed-is often the safest way to prevent harm,
since certain risks can lead to severe consequences once carried out. However,
existing guardrails mostly operate post-execution, which is difficult to scale
and leaves little room for controllable supervision at the plan level. To
address this challenge, we highlight three critical gaps in current research:
data gap, model gap, and evaluation gap. To close the data gap, we introduce
AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)
injects category-labeled risks with calibrated difficulty, and (iii) filters
outputs via an automated reward model, producing large and reliable corpora for
pre-execution safety. To close the guardian model gap, we propose a
foundational guardrail Safiron, combining a cross-planner adapter with a
compact guardian model. The adapter unifies different input formats, while
Safiron flags risky cases, assigns risk types, and generates rationales;
trained in two stages with a broadly explored data recipe, Safiron achieves
robust transfer across settings. To close the evaluation gap, we release
Pre-Exec Bench, a realistic benchmark covering diverse tools and branching
trajectories, which measures detection, fine-grained categorization,
explanation, and cross-planner generalization in human-verified scenarios.
Extensive experiments demonstrate consistent gains of the proposed guardrail
over strong baselines on Pre-Exec Bench, and ablations further distill
actionable practices, providing a practical template for safer agentic systems.

</details>


### [20] [Combined Representation and Generation with Diffusive State Predictive Information Bottleneck](https://arxiv.org/abs/2510.09784)
*Richard John,Yunrui Qiu,Lukas Herron,Pratyush Tiwary*

Main category: cs.LG

TL;DR: 提出D-SPIB方法，结合时间延迟信息瓶颈和扩散模型，在分子科学中平衡表示学习和生成目标，能够从不同温度模拟轨迹中学习连贯的热力学表示。


<details>
  <summary>Details</summary>
Motivation: 分子科学中数据收集昂贵且重要事件罕见，在高维空间中需要压缩到低维流形以支持生成等下游任务。

Method: 结合时间延迟信息瓶颈和扩散模型的联合训练目标，构建D-SPIB协议，能够整合不同温度模拟轨迹信息。

Result: 在多个分子任务上进行了基准测试，展示了探索训练集外物理条件的潜力。

Conclusion: D-SPIB提供了一个灵活的架构，能够平衡表示学习和生成目标，并学习连贯有用的热力学内部表示。

Abstract: Generative modeling becomes increasingly data-intensive in high-dimensional
spaces. In molecular science, where data collection is expensive and important
events are rare, compression to lower-dimensional manifolds is especially
important for various downstream tasks, including generation. We combine a
time-lagged information bottleneck designed to characterize molecular important
representations and a diffusion model in one joint training objective. The
resulting protocol, which we term Diffusive State Predictive Information
Bottleneck (D-SPIB), enables the balancing of representation learning and
generation aims in one flexible architecture. Additionally, the model is
capable of combining temperature information from different molecular
simulation trajectories to learn a coherent and useful internal representation
of thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase
its potential for exploring physical conditions outside the training set.

</details>


### [21] [Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration](https://arxiv.org/abs/2510.09891)
*Parsa Gooya,Reinel Sospedra-Alfonso*

Main category: cs.LG

TL;DR: 提出了一个基于条件变分自编码器的概率误差校正框架，用于北极海冰浓度的季节性预测，能够生成大量调整后的预测集合，比传统方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 北极海冰浓度的季节性预测对缓解海冰快速减少的负面影响至关重要。现有基于气候模型的预测系统存在系统性偏差和复杂的时空误差，需要偏差校正和校准。决策制定需要适当的不确定性量化和极端事件概率评估。

Method: 使用条件变分自编码器模型构建概率误差校正框架，将观测值的条件分布映射到有偏差的模型预测上，自然生成大量调整后的预测集合。

Result: 调整后的预测在校准性、与观测分布的接近程度和误差大小方面均优于基于气候平均值的调整预测。

Conclusion: 该概率误差校正框架能够有效改善北极海冰浓度季节性预测的准确性和不确定性量化，为决策提供更可靠的概率信息。

Abstract: Seasonal forecast of Arctic sea ice concentration is key to mitigate the
negative impact and assess potential opportunities posed by the rapid decline
of sea ice coverage. Seasonal prediction systems based on climate models often
show systematic biases and complex spatio-temporal errors that grow with the
forecasts. Consequently, operational predictions are routinely bias corrected
and calibrated using retrospective forecasts. For predictions of Arctic sea ice
concentration, error corrections are mainly based on one-to-one post-processing
methods including climatological mean or linear regression correction and, more
recently, machine learning. Such deterministic adjustments are confined at best
to the limited number of costly-to-run ensemble members of the raw forecast.
However, decision-making requires proper quantification of uncertainty and
likelihood of events, particularly of extremes. We introduce a probabilistic
error correction framework based on a conditional Variational Autoencoder model
to map the conditional distribution of observations given the biased model
prediction. This method naturally allows for generating large ensembles of
adjusted forecasts. We evaluate our model using deterministic and probabilistic
metrics and show that the adjusted forecasts are better calibrated, closer to
the observational distribution, and have smaller errors than climatological
mean adjusted forecasts.

</details>


### [22] [MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation](https://arxiv.org/abs/2510.09930)
*Ching Chang,Ming-Chih Lo,Chiao-Tung Chan,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.LG

TL;DR: MemPromptTSS是一个用于多粒度时间序列分割的框架，通过引入持久提示记忆来解决现有方法中提示影响快速衰减的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分割方法中的提示机制只能在局部上下文中起作用，提示的影响会快速消失，无法在整个序列中指导预测。

Method: 提出MemPromptTSS框架，使用记忆编码器将提示及其周围子序列转换为记忆令牌存储在记忆库中，使每个新预测都能基于所有累积的提示。

Result: 在6个数据集上的实验显示，MemPromptTSS在单次迭代推理中比最佳基线在单粒度和多粒度分割上分别提升23%和85%的准确率，在迭代推理中平均每次迭代增益为2.66个百分点。

Conclusion: 持久记忆对于提示引导的分割至关重要，MemPromptTSS为实际应用提供了一个实用有效的框架。

Abstract: Web platforms, mobile applications, and connected sensing systems generate
multivariate time series with states at multiple levels of granularity, from
coarse regimes to fine-grained events. Effective segmentation in these settings
requires integrating across granularities while supporting iterative refinement
through sparse prompt signals, which provide a compact mechanism for injecting
domain knowledge. Yet existing prompting approaches for time series
segmentation operate only within local contexts, so the effect of a prompt
quickly fades and cannot guide predictions across the entire sequence. To
overcome this limitation, we propose MemPromptTSS, a framework for iterative
multi-granularity segmentation that introduces persistent prompt memory. A
memory encoder transforms prompts and their surrounding subsequences into
memory tokens stored in a bank. This persistent memory enables each new
prediction to condition not only on local cues but also on all prompts
accumulated across iterations, ensuring their influence persists across the
entire sequence. Experiments on six datasets covering wearable sensing and
industrial monitoring show that MemPromptTSS achieves 23% and 85% accuracy
improvements over the best baseline in single- and multi-granularity
segmentation under single iteration inference, and provides stronger refinement
in iterative inference with average per-iteration gains of 2.66 percentage
points compared to 1.19 for PromptTSS. These results highlight the importance
of persistent memory for prompt-guided segmentation, establishing MemPromptTSS
as a practical and effective framework for real-world applications.

</details>


### [23] [Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction](https://arxiv.org/abs/2510.10140)
*Yue Deng,Francisco Santos,Pang-Ning Tan,Lifeng Luo*

Main category: cs.LG

TL;DR: 本文提出了Cyc-Attack方法，通过扰动深度学习天气预报模型的上游预报来生成对抗性热带气旋轨迹，解决了传统攻击方法在非可微TC检测系统和类别不平衡问题上的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习天气预报模型对对抗攻击的脆弱性，特别是如何通过微扰上游天气预测来改变下游热带气旋轨迹预测，现有方法在非可微TC检测系统和类别不平衡问题上存在挑战。

Method: 提出Cyc-Attack方法：预训练可微替代模型近似TC检测器输出，使用偏度感知损失函数和核膨胀策略解决不平衡问题，采用基于距离的梯度加权方案和正则化确保对抗预报的物理一致性。

Result: Cyc-Attack能够有效生成对抗性轨迹，成功扰动上游天气预报以产生攻击者指定的热带气旋轨迹，同时保持物理一致性。

Conclusion: Cyc-Attack方法克服了传统对抗攻击在天气预报领域的局限性，证明了深度学习天气预报模型对上游扰动的高度敏感性，对气象安全具有重要意义。

Abstract: Deep learning based weather forecasting (DLWF) models leverage past weather
observations to generate future forecasts, supporting a wide range of
downstream tasks, including tropical cyclone (TC) trajectory prediction. In
this paper, we investigate their vulnerability to adversarial attacks, where
subtle perturbations to the upstream weather forecasts can alter the downstream
TC trajectory predictions. Although research on adversarial attacks in DLWF
models has grown recently, generating perturbed upstream forecasts that
reliably steer downstream output toward attacker-specified trajectories remains
a challenge. First, conventional TC detection systems are opaque,
non-differentiable black boxes, making standard gradient-based attacks
infeasible. Second, the extreme rarity of TC events leads to severe class
imbalance problem, making it difficult to develop efficient attack methods that
will produce the attacker's target trajectories. Furthermore, maintaining
physical consistency in adversarially generated forecasts presents another
significant challenge. To overcome these limitations, we propose Cyc-Attack, a
novel method that perturbs the upstream forecasts of DLWF models to generate
adversarial trajectories. First, we pre-train a differentiable surrogate model
to approximate the TC detector's output, enabling the construction of
gradient-based attacks. Cyc-Attack also employs skewness-aware loss function
with kernel dilation strategy to address the imbalance problem. Finally, a
distance-based gradient weighting scheme and regularization are used to
constrain the perturbations and eliminate spurious trajectories to ensure the
adversarial forecasts are realistic and not easily detectable.

</details>


### [24] [Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective](https://arxiv.org/abs/2510.10150)
*Zhezheng Hao,Hong Wang,Haoyang Liu,Jian Luo,Jiarui Yu,Hande Dong,Qiang Lin,Can Wang,Jiawei Chen*

Main category: cs.LG

TL;DR: 本文分析了强化学习可验证奖励(RLVR)中的熵崩溃问题，提出了一种新的熵变化感知重加权方法STEER，通过细粒度的token级调整来稳定熵动态，有效缓解熵崩溃并提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: RLVR训练过程中存在熵崩溃风险，即策略多样性快速丧失，导致泛化能力不足。现有熵干预方法机制不明确且效果有限，需要更直接有效的熵动态控制方法。

Method: 提出STEER方法，通过熵变化感知的重加权方案，在token级别自适应稳定熵动态，缓解过度利用同时促进稳健探索。

Result: 实验表明STEER显著缓解了熵崩溃，稳定了熵动态，并在多个数学推理基准测试中取得了更强的下游性能。

Conclusion: STEER通过直接控制熵动态，有效解决了现有方法的局限性，为RLVR训练提供了更稳定和有效的熵控制机制。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM
reasoning, its training process poses a critical risk: entropy collapse. This
phenomenon is a rapid loss of policy diversity, stemming from the
exploration-exploitation imbalance and leading to a lack of generalization.
Recent entropy-intervention methods aim to prevent \coloredtext{entropy
collapse}, yet their underlying mechanisms remain unclear. In this paper, we
conduct a quantitative analysis to reveal token-level entropy changes and how
existing entropy intervention methods help avoid entropy collapse. Our findings
point out a fundamental limitation of existing methods: they attempt to control
entropy dynamics indirectly. By only affecting related factors, such as the
advantage signal and generation probability, their effectiveness is inherently
limited and could potentially fail. To address this limitation, we introduce an
entropy-change-aware reweighting scheme, namely Stabilizing Token-level
Entropy-changE via Reweighting (STEER), that adaptively stabilizes entropy
dynamics through fine-grained token-level adjustments. Our approach mitigates
over-exploitation while fostering robust exploration. Extensive experiments
demonstrate that STEER significantly mitigates entropy collapse, stabilizes
entropy dynamics, and achieves stronger downstream performance across various
mathematical reasoning benchmarks \footnote{Our code is available at
https://github.com/zz-haooo/STEER.

</details>


### [25] [SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification](https://arxiv.org/abs/2510.10232)
*Xuening Wu,Shenqin Yin,Yanlan Kang,Xinhang Zhang,Qianya Xu,Zeping Chen,Wenqiang Zhang*

Main category: cs.LG

TL;DR: 提出统计哥德尔机(SGM)，首个用于递归编辑的统计安全层，用统计置信度测试替代证明要求，在选定置信水平下认证改进后才允许修改，并通过全局错误预算控制累积风险。


<details>
  <summary>Details</summary>
Motivation: 递归自修改在AutoML、神经架构搜索和自适应优化中日益重要，但现有框架无法确保此类更改的安全性。哥德尔机虽然提供原则性保障，但在随机高维环境中无法获得形式化证明。

Method: SGM使用统计置信度测试(e值、Hoeffding界)替代证明要求，仅在改进以选定置信水平被认证时才允许修改，同时分配全局错误预算来限制多轮累积风险。提出确认触发调和支出(CTHS)方法，按确认事件而非轮次索引支出，将错误预算集中在有希望的编辑上。

Result: 在监督学习、强化学习和黑盒优化的实验中验证了SGM的作用：在CIFAR-100上认证真实增益，在ImageNet-100上拒绝虚假改进，在RL和优化基准上表现出鲁棒性。

Conclusion: SGM为学习系统中持续、风险感知的自修改提供了基础性基础设施。

Abstract: Recursive self-modification is increasingly central in AutoML, neural
architecture search, and adaptive optimization, yet no existing framework
ensures that such changes are made safely. Godel machines offer a principled
safeguard by requiring formal proofs of improvement before rewriting code;
however, such proofs are unattainable in stochastic, high-dimensional settings.
We introduce the Statistical Godel Machine (SGM), the first statistical safety
layer for recursive edits. SGM replaces proof-based requirements with
statistical confidence tests (e-values, Hoeffding bounds), admitting a
modification only when superiority is certified at a chosen confidence level,
while allocating a global error budget to bound cumulative risk across
rounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which
indexes spending by confirmation events rather than rounds, concentrating the
error budget on promising edits while preserving familywise
validity.Experiments across supervised learning, reinforcement learning, and
black-box optimization validate this role: SGM certifies genuine gains on
CIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates
robustness on RL and optimization benchmarks.Together, these results position
SGM as foundational infrastructure for continual, risk-aware self-modification
in learning systems.Code is available at:
https://github.com/gravitywavelet/sgm-anon.

</details>


### [26] [Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant](https://arxiv.org/abs/2510.10952)
*Xi Mao,Zhendong Wang,Jingyu Li,Lingchao Mao,Utibe Essien,Hairong Wang,Xuelei Sherry Ni*

Main category: cs.LG

TL;DR: 该研究使用社会健康决定因素预测认知表现，通过XGBoost模型在墨西哥老龄化健康研究数据上取得了优于现有方法的性能，识别出地板材料等关键预测因子。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期检测至关重要，因为其神经退行性效应不可逆转，且神经病理和社会行为风险因素在诊断前多年就已积累。早期识别高风险个体有助于预防、及时护理和公平资源分配。

Method: 使用基于奇异值分解的插值管道处理缺失值，分别处理连续和分类变量。评估多种方法后选择XGBoost进行预测建模，因其预测性能最优。

Result: 该框架优于现有方法和数据挑战排行榜，表现出高准确性、鲁棒性和可解释性。SHAP分析识别出地板材料、年龄、社会经济地位、生活方式等关键影响因素。

Conclusion: 研究证明了社会健康决定因素在认知老化中的多因素性质，以及可解释、数据驱动的SDOH建模的价值，为早期阿尔茨海默病检测提供了新方法。

Abstract: Early detection of Alzheimer's disease (AD) is crucial because its
neurodegenerative effects are irreversible, and neuropathologic and
social-behavioral risk factors accumulate years before diagnosis. Identifying
higher-risk individuals earlier enables prevention, timely care, and equitable
resource allocation. We predict cognitive performance from social determinants
of health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset
derived from the nationally representative Mex-Cog cohort of the 2003 and 2012
Mexican Health and Aging Study (MHAS).
  Data: The target is a validated composite cognitive score across seven
domains-orientation, memory, attention, language, constructional praxis, and
executive function-derived from the 2016 and 2021 MHAS waves. Predictors span
demographic, socioeconomic, health, lifestyle, psychosocial, and healthcare
access factors.
  Methodology: Missingness was addressed with a singular value decomposition
(SVD)-based imputation pipeline treating continuous and categorical variables
separately. This approach leverages latent feature correlations to recover
missing values while balancing reliability and scalability. After evaluating
multiple methods, XGBoost was chosen for its superior predictive performance.
  Results and Discussion: The framework outperformed existing methods and the
data challenge leaderboard, demonstrating high accuracy, robustness, and
interpretability. SHAP-based post hoc analysis identified top contributing SDOH
factors and age-specific feature patterns. Notably, flooring material emerged
as a strong predictor, reflecting socioeconomic and environmental disparities.
Other influential factors, age, SES, lifestyle, social interaction, sleep,
stress, and BMI, underscore the multifactorial nature of cognitive aging and
the value of interpretable, data-driven SDOH modeling.

</details>


### [27] [Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization](https://arxiv.org/abs/2510.10982)
*Zihan Wang,Zhiyong Ma,Zhongkui Ma,Shuofeng Liu,Akide Liu,Derui Wang,Minhui Xue,Guangdong Bai*

Main category: cs.LG

TL;DR: 该论文提出了一种名为非可转移示例(NEs)的输入侧使用控制机制，通过将输入重新编码到模型特定的低敏感子空间，在保持授权模型性能的同时降低未授权模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么通过扰动数据使其不可学习，要么重新训练模型以抑制迁移，但都无法控制未知模型的推理，且通常需要控制训练过程。需要一种无需训练、数据无关的输入侧使用控制机制。

Method: 在模型特定的低敏感子空间中对输入进行重新编码，保持授权模型的输出同时通过子空间不对齐来降低未授权模型的性能。使用Hoffman-Wielandt不等式将性能下降与谱差异联系起来。

Result: 实验表明NEs在多种视觉骨干网络和最先进的视觉语言模型上保持性能，而非目标模型即使尝试重建也会性能崩溃。

Conclusion: NEs是一种实用的方法，可以在保持预期数据效用的同时防止未经授权的利用。

Abstract: Recent AI regulations call for data that remain useful for innovation while
resistant to misuse, balancing utility with protection at the model level.
Existing approaches either perturb data to make it unlearnable or retrain
models to suppress transfer, but neither governs inference by unknown models,
and both typically require control over training. We propose non-transferable
examples (NEs), a training-free and data-agnostic input-side usage-control
mechanism. We recode inputs within a model-specific low-sensitivity subspace,
preserving outputs for the authorized model while reducing performance on
unauthorized models through subspace misalignment. We establish formal bounds
that guarantee utility for the authorized model and quantify deviation for
unauthorized ones, with the Hoffman-Wielandt inequality linking degradation to
spectral differences. Empirically, NEs retain performance on diverse vision
backbones and state-of-the-art vision-language models under common
preprocessing, whereas non-target models collapse even with reconstruction
attempts. These results establish NEs as a practical means to preserve intended
data utility while preventing unauthorized exploitation. Our project is
available at https://trusted-system-lab.github.io/model-specificity

</details>


### [28] [ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding](https://arxiv.org/abs/2510.11498)
*Yuhang Li,Chenchen Zhang,Ruilin Lv,Ao Liu,Ken Deng,Yuanxing Zhang,Jiaheng Liu,Wiggin Zhou,Bo Zhou*

Main category: cs.LG

TL;DR: ReLook是一个基于视觉的强化学习框架，通过多模态大语言模型作为工具，实现前端代码生成的生成-诊断-优化循环，显著提升了视觉前端代码生成的质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在算法代码生成方面表现出色，但在前端开发中表现不佳，因为前端代码的正确性需要通过渲染像素和交互来判断，需要视觉反馈来评估代码质量。

Method: 使用多模态大语言模型作为视觉批评家，对代码进行截图评分并提供基于视觉的反馈；引入强制优化策略，只接受改进的修订版本；在推理阶段解耦批评家，运行轻量级的自编辑循环。

Result: 在三个广泛使用的基准测试中，ReLook始终优于强基线方法，在基于视觉的前端代码生成方面表现出色。

Conclusion: ReLook框架展示了智能感知、视觉奖励以及训练-推理解耦在前端代码生成中的优势，为视觉基础代码生成提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) excel at algorithmic code generation, they
struggle with front-end development, where correctness is judged on rendered
pixels and interaction. We present ReLook, an agentic, vision-grounded
reinforcement learning framework that empowers an agent to close a robust
generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.
During training, the agent uses the MLLM-in-the-loop both as a visual
critic--scoring code with screenshots--and as a source of actionable,
vision-grounded feedback; a strict zero-reward rule for invalid renders anchors
renderability and prevents reward hacking. To prevent behavioral collapse, we
introduce Forced Optimization, a strict acceptance rule that admits only
improving revisions, yielding monotonically better trajectories. At inference,
we decouple the critic and run a lightweight, critic-free self-edit cycle,
keeping latency comparable to base decoding while retaining most of the gains.
Across three widely used benchmarks, ReLook consistently outperforms strong
baselines in vision-grounded front-end code generation, highlighting the
benefits of agentic perception, visual rewards, and training-inference
decoupling.

</details>


### [29] [MIEO: encoding clinical data to enhance cardiovascular event prediction](https://arxiv.org/abs/2510.11257)
*Davide Borghini,Davide Marchi,Angelo Nardone,Giordano Scerra,Silvia Giulia Galfrè,Alessandro Pingitore,Giuseppe Prencipe,Corrado Priami,Alina Sîrbu*

Main category: cs.LG

TL;DR: 使用自监督自动编码器处理临床数据中的标签稀缺和数据异构性问题，通过无标签数据构建潜在空间，提高心血管死亡预测的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 临床数据存在标签稀缺和数据异构导致缺失值的问题，需要有效方法处理这些挑战。

Method: 采用自监督自动编码器，利用无标签数据构建潜在空间，然后在该空间上训练神经网络分类器预测心血管死亡。

Result: 相比直接在原始数据上应用分类器，该方法在平衡准确率方面表现更好。

Conclusion: 该方法在无标签数据可用性增加的情况下具有良好前景。

Abstract: As clinical data are becoming increasingly available, machine learning
methods have been employed to extract knowledge from them and predict clinical
events. While promising, approaches suffer from at least two main issues: low
availability of labelled data and data heterogeneity leading to missing values.
This work proposes the use of self-supervised auto-encoders to efficiently
address these challenges. We apply our methodology to a clinical dataset from
patients with ischaemic heart disease. Patient data is embedded in a latent
space, built using unlabelled data, which is then used to train a neural
network classifier to predict cardiovascular death. Results show improved
balanced accuracy compared to applying the classifier directly to the raw data,
demonstrating that this solution is promising, especially in conditions where
availability of unlabelled data could increase.

</details>


### [30] [Event-Aware Prompt Learning for Dynamic Graphs](https://arxiv.org/abs/2510.11339)
*Xingtong Yu,Ruijuan Liang,Xinming Zhang,Yuan Fang*

Main category: cs.LG

TL;DR: 提出了EVP框架，一种事件感知的动态图提示学习框架，可作为现有方法的插件，增强其利用历史事件知识的能力。


<details>
  <summary>Details</summary>
Motivation: 现有动态图学习方法主要关注节点与时间的关系，但忽视了历史事件的影响。

Method: 1. 为每个节点提取历史事件序列，引入事件适应机制对齐事件特征与下游任务；2. 提出事件聚合机制将历史知识整合到节点表示中。

Result: 在四个公共数据集上进行了广泛实验来评估和分析EVP。

Conclusion: EVP框架能够有效增强现有动态图学习方法对历史事件知识的利用能力。

Abstract: Real-world graph typically evolve via a series of events, modeling dynamic
interactions between objects across various domains. For dynamic graph
learning, dynamic graph neural networks (DGNNs) have emerged as popular
solutions. Recently, prompt learning methods have been explored on dynamic
graphs. However, existing methods generally focus on capturing the relationship
between nodes and time, while overlooking the impact of historical events. In
this paper, we propose EVP, an event-aware dynamic graph prompt learning
framework that can serve as a plug-in to existing methods, enhancing their
ability to leverage historical events knowledge. First, we extract a series of
historical events for each node and introduce an event adaptation mechanism to
align the fine-grained characteristics of these events with downstream tasks.
Second, we propose an event aggregation mechanism to effectively integrate
historical knowledge into node representations. Finally, we conduct extensive
experiments on four public datasets to evaluate and analyze EVP.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 提出了一种新的LLM工作流优化范式，将问题重新定义为分布优化问题，通过最小化期望失败质量而非最大化标量分数来提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工作流优化方法存在信息坍缩问题，将丰富的多步执行轨迹简化为简单的成功/失败信号，无法建模工作流的失败分布。

Method: 提出CE-Graph框架，通过反例池近似失败分布，识别密集区域作为重复失败模式，使用提议-验证机制应用有针对性的图编辑来贪婪地减少失败质量。

Result: 在数学、代码和问答基准测试中，CE-Graph以显著更低的成本实现了比强基线更高的鲁棒性。

Conclusion: 系统的可靠性并非来自避免失败，而是通过系统性地学习和重塑其失败分布的几何结构来实现。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [32] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: SwarmSys是一个受群体智能启发的分布式多智能体推理框架，通过探索者、工作者和验证者三个角色的迭代交互实现自组织协调，在符号推理、研究综合和科学编程任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架通常依赖固定角色或集中控制，限制了长时程推理的可扩展性和适应性。

Method: 集成自适应智能体和事件档案、基于嵌入的概率匹配以及信息素启发的强化机制，支持动态任务分配和无全局监督的自组织收敛。

Result: 在符号推理、研究综合和科学编程任务中，SwarmSys持续优于基线方法，提高了准确性和推理稳定性。

Conclusion: 群体启发的协调是推进可扩展、鲁棒和自适应多智能体推理的有前景范式，协调扩展可能与模型扩展在推进LLM智能方面相媲美。

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [33] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: Traj-CoA是一个用于患者轨迹建模的多智能体系统，通过链式智能体处理电子健康记录数据，减少噪声并保持完整时间线，在零样本肺癌风险预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理电子健康记录时面临数据长且噪声多的问题，特别是在时序推理方面存在挑战。

Method: 采用多智能体系统，包括工作智能体链式处理EHR数据块，将关键事件提取到共享长期记忆模块EHRMem中，最后由管理智能体综合信息进行预测。

Result: 在基于五年EHR数据的零样本一年肺癌风险预测任务中，Traj-CoA优于四类基线方法，展现出临床对齐的时序推理能力。

Conclusion: Traj-CoA为复杂患者轨迹建模提供了一个强大且可泛化的方法。

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [34] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis,Indrajith Ekanayake*

Main category: cs.AI

TL;DR: 提出一个结合可解释AI、生存分析和RFM分析的三组件框架，用于客户流失分析，从单纯预测转向设计个性化留存策略。


<details>
  <summary>Details</summary>
Motivation: 传统流失模型多为黑箱，缺乏对流失原因、干预时机和高风险客户群体的洞察，需要转向基于可解释证据的个性化留存策略设计。

Method: 集成可解释AI量化特征贡献、生存分析建模流失时间风险、RFM分析按交易行为细分客户的三组件框架。

Result: 该框架能够识别流失驱动因素、估计干预窗口、优先考虑目标细分市场，支持减少流失和增强客户忠诚度的策略。

Conclusion: 该集成框架为设计基于证据的个性化客户留存策略提供了可行方法，超越了单纯预测的局限性。

Abstract: In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.

</details>


### [35] [Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots](https://arxiv.org/abs/2509.16444)
*Chenhan Lyu,Yutong Song,Pengfei Zhang,Amir M. Rahmani*

Main category: cs.AI

TL;DR: 本文提出了一种基于宪法AI训练的领域特定方法，用于在计算心理健康应用中构建安全、领域适应的AI系统，以解决通用AI安全措施在心理健康领域中的不足。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题日益严重，AI在心理护理中的整合以及服务不足社区对可扩展解决方案的需求推动了心理健康应用的发展。这些应用处理敏感数据，需要专门的AI安全措施，因为通用安全措施无法充分应对心理健康特有的挑战，如情绪脆弱性、误诊风险、症状恶化以及危机干预准确性等问题。

Method: 采用宪法AI训练方法，结合领域特定的心理健康原则，为计算心理健康应用开发安全、领域适应的CAI系统。

Result: 该方法旨在提高心理健康AI系统的安全性，特别是在危机干预准确性、治疗指南遵守、资源受限环境下的可扩展性以及对细微对话的适应性方面。

Conclusion: 通过领域特定的宪法AI训练，可以构建更安全、更适合心理健康应用需求的AI系统，有效应对通用AI安全措施无法解决的领域特有挑战。

Abstract: Mental health applications have emerged as a critical area in computational
health, driven by rising global rates of mental illness, the integration of AI
in psychological care, and the need for scalable solutions in underserved
communities. These include therapy chatbots, crisis detection, and wellness
platforms handling sensitive data, requiring specialized AI safety beyond
general safeguards due to emotional vulnerability, risks like misdiagnosis or
symptom exacerbation, and precise management of vulnerable states to avoid
severe outcomes such as self-harm or loss of trust. Despite AI safety advances,
general safeguards inadequately address mental health-specific challenges,
including crisis intervention accuracy to avert escalations, therapeutic
guideline adherence to prevent misinformation, scale limitations in
resource-constrained settings, and adaptation to nuanced dialogues where
generics may introduce biases or miss distress signals. We introduce an
approach to apply Constitutional AI training with domain-specific mental health
principles for safe, domain-adapted CAI systems in computational mental health
applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [36] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: NG-Router是一个基于知识图谱的多智能体协作框架，通过图神经网络学习任务感知的路由分布，解决营养问答中的推理能力不足和上下文过载问题。


<details>
  <summary>Details</summary>
Motivation: 现有营养问答方法面临单智能体系统推理能力有限、多智能体架构设计复杂以及上下文过载影响决策准确性的挑战。

Method: 将智能体节点集成到异构知识图谱中，使用图神经网络学习任务感知路由分布，并提出基于梯度的子图检索机制来识别关键证据。

Result: 在多个基准测试和骨干模型上的广泛实验表明，NG-Router一致优于单智能体和集成基线方法。

Conclusion: NG-Router为复杂营养健康任务提供了一种基于领域的多智能体推理原则性方法。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [37] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: NarraBench是一个基于理论构建的叙事理解任务分类法，调查了78个现有基准，发现当前评估在叙事事件、风格、视角和揭示等方面存在严重不足，只有27%的叙事任务被现有基准充分覆盖。


<details>
  <summary>Details</summary>
Motivation: 当前叙事理解评估存在显著空白，许多重要方面被忽视或与现有指标不匹配，特别是构成性主观和视角性方面缺乏适当基准。

Method: 开发理论指导的叙事理解任务分类法，对78个现有基准进行系统性调查和分析。

Result: 发现仅27%的叙事任务被现有基准充分覆盖，叙事事件、风格、视角和揭示等关键方面几乎完全缺失。

Conclusion: 需要开发更多能够评估构成性主观和视角性方面的基准，该分类法、调查和方法论对测试LLM叙事理解具有重要价值。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [38] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出PerAugy数据增强技术，通过跨轨迹混洗和摘要内容扰动，显著提升个性化摘要中用户编码器的性能，并证明数据集多样性是性能提升的关键因素。


<details>
  <summary>Details</summary>
Motivation: 个性化摘要面临训练数据稀缺的挑战，现有数据集如MS/CAS PENS缺乏目标摘要且主题转换多样性有限，限制了模型的泛化能力。

Method: 提出PerAugy数据增强技术，包含跨轨迹混洗和摘要内容扰动，并引入三个数据集多样性指标(TP、RTC、DegreeD)来量化增强效果。

Result: PerAugy显著提升了四种SOTA用户编码器的准确性(最佳AUC提升0.132)，增强后的摘要框架个性化程度平均提升61.2%(PSE-SU4指标)。

Conclusion: 数据集多样性是提升个性化摘要性能的关键因素，TP和DegreeD指标与用户编码器性能强相关，验证了PerAugy方法的有效性。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [39] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 该研究通过大规模比较CNN和福克斯新闻，分析了媒体的事实报道策略，发现CNN使用更多事实陈述并更依赖外部来源，而福克斯新闻偏好新闻报道和直接引用，揭示了党派媒体使用不同认知策略构建现实。


<details>
  <summary>Details</summary>
Motivation: 虽然媒体偏见被广泛研究，但事实报道背后的认知策略在计算层面仍未被充分探索，需要量化分析不同媒体如何通过系统性的认知策略来构建现实。

Method: 采用文章匹配策略比较同一事件的报道，应用FactAppeal框架分析超过47万篇文章，涵盖COVID-19大流行和以色列-哈马斯战争两个高度政治化时期。

Result: CNN的报道包含更多事实陈述且更倾向于基于外部来源，CNN通过引用专家和专家文件构建正式权威的吸引力，而福克斯新闻偏好新闻报道和直接引用。

Conclusion: 这项研究量化了党派媒体如何系统性地使用不同的认知策略来构建现实，为媒体偏见研究增添了新的维度。

Abstract: While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.

</details>


### [40] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei,Ke Han,Yueming Lyu,Yu Luo,Yue Jiang,Caifeng Shan,Nicu Sebe*

Main category: cs.CL

TL;DR: EASE是一个用于实时假新闻检测的新框架，通过动态评估证据充分性来自适应决策过程，包含证据评估、推理评估和情感回退三个视角，在证据稀缺情况下显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决实时假新闻检测中证据稀缺的问题，现有方法过度依赖外部证据，在证据不足时泛化能力差。

Method: 提出EASE框架，包含三个独立评估视角：证据评估、推理评估和情感回退，通过指令调优和伪标签增强评估准确性，实现评估感知的决策。

Result: 在多个基准测试中达到最先进性能，显著提升对实时新闻的泛化能力，并发布了RealTimeNews-25新基准数据集。

Conclusion: EASE通过动态评估证据充分性和多视角决策机制，有效解决了实时假新闻检测中的证据稀缺问题，具有优异的泛化性能。

Abstract: Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.

</details>


### [41] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang,Yixuan Zhang,Ruiji Yu,Yufei Zheng,Lang Gao,Zirui Song,Zixiang Xu,Gus Xia,Huishuai Zhang,Dongyan Zhao,Xiuying Chen*

Main category: cs.CL

TL;DR: 该研究首次系统地揭示并验证了大型语言模型中的情感电路，通过识别神经元和注意力头构建全局情感电路，实现了99.65%的情感表达准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型对情感智能需求的增长，需要理解情感表达的内部机制并控制生成文本中的情感。

Method: 构建SEV数据集提取上下文无关情感方向，通过分析分解和因果分析识别神经元和注意力头，整合为全局情感电路，并进行调制干预。

Result: 识别出情感计算的具体神经元和注意力头，构建了驱动情感表达的全局电路，直接调制这些电路在测试集上达到99.65%的情感表达准确率。

Conclusion: 这是首个系统揭示和验证LLM中情感电路的研究，为可解释性和可控情感智能提供了新见解。

Abstract: As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.

</details>


### [42] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 提出了Rollout Routing Replay (R3)方法来解决MoE模型在强化学习训练中的路由不稳定性问题，通过记录推理阶段的路由分布并在训练时重放，显著降低了训练-推理策略差异。


<details>
  <summary>Details</summary>
Motivation: MoE模型的路由机制在强化学习训练中会导致不稳定性甚至灾难性训练崩溃，主要原因是训练和推理阶段的路由行为存在显著差异，以及相同条件下多次前向传播会产生不同的专家选择。

Method: 提出R3方法：记录推理引擎中的路由分布，在训练阶段重放这些分布，从而减少训练-推理策略KL散度，缓解极端差异，同时不牺牲训练速度。

Result: 在各种设置下的广泛实验证实，R3成功稳定了RL训练，防止了崩溃，并且在性能上超越了GSPO和TIS等方法。

Conclusion: 这项工作为稳定MoE模型中的强化学习训练提供了一个新的解决方案。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [43] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen,Rong Pan,Haoran Li*

Main category: cs.CL

TL;DR: 提出了一种混合自底向上的长故事生成方法，使用多智能体模拟来生成超过10,000字的长篇故事，通过智能体在动态沙盒环境中的互动产生涌现事件，实现有机的角色发展和情节推进。


<details>
  <summary>Details</summary>
Motivation: 受人类作家创作过程中先构建整体心理场景的启发，旨在解决传统自上而下方法结构僵化的问题，实现更自然、自发的故事生成。

Method: 采用多智能体模拟方法，智能体在动态沙盒环境中互动，其行为和与环境及其他智能体的交互产生涌现事件，这些事件构成故事基础。

Result: 系统能够生成超过10,000字的长篇故事，同时保持连贯性和一致性，在多个指标上达到最先进性能。

Conclusion: 该方法为创建动态、沉浸式的长篇故事提供了一种可扩展的创新解决方案，故事从智能体驱动的互动中有机演化而来。

Abstract: Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [44] [Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds](https://arxiv.org/abs/2510.10455)
*Jiayu Ding,Xulin Chen,Garrett E. Katz,Zhenyu Gan*

Main category: cs.RO

TL;DR: 提出基于对称性的强化学习框架，无需预定义轨迹即可生成多种四足机器人步态，包括小跑、跳跃、半跳跃和疾驰等。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要专家手动调整大量变量来生成特定步态序列，过程繁琐且耗时。

Method: 利用动态腿式系统的内在对称性和速度-周期关系，设计对称性引导的奖励函数，包含时间、形态和时间反转对称性。

Result: 在Unitree Go2机器人上实现，在仿真和硬件测试中表现出鲁棒性能，显著提高了步态适应性。

Conclusion: 揭示了动态运动策略的见解，强调了对称性在机器人步态设计中的关键作用。

Abstract: Quadrupedal robots exhibit a wide range of viable gaits, but generating
specific footfall sequences often requires laborious expert tuning of numerous
variables, such as touch-down and lift-off events and holonomic constraints for
each leg. This paper presents a unified reinforcement learning framework for
generating versatile quadrupedal gaits by leveraging the intrinsic symmetries
and velocity-period relationship of dynamic legged systems. We propose a
symmetry-guided reward function design that incorporates temporal,
morphological, and time-reversal symmetries. By focusing on preserved
symmetries and natural dynamics, our approach eliminates the need for
predefined trajectories, enabling smooth transitions between diverse locomotion
patterns such as trotting, bounding, half-bounding, and galloping. Implemented
on the Unitree Go2 robot, our method demonstrates robust performance across a
range of speeds in both simulations and hardware tests, significantly improving
gait adaptability without extensive reward tuning or explicit foot placement
control. This work provides insights into dynamic locomotion strategies and
underscores the crucial role of symmetries in robotic gait design.

</details>


### [45] [Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control](https://arxiv.org/abs/2510.10516)
*Kanishkha Jaisankar,Xiaoyang Jiang,Feifan Liao,Jeethu Sreenivas Amuthan*

Main category: cs.RO

TL;DR: 提出了一种结合群体编码脉冲神经网络(SNN)和深度强化学习(DRL)的新框架，在保持控制性能的同时实现高达96.10%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 解决机器人控制中能耗效率与高性能之间的平衡问题，特别是在资源受限环境下，传统DRL方法计算需求大、能耗高。

Method: 使用群体编码脉冲演员网络(PopSAN)，将高维观测编码为神经元群体活动，通过基于梯度的更新实现最优策略学习，结合SNN的事件驱动异步计算和DRL的鲁棒策略优化能力。

Result: 在Isaac Gym平台的PixMC基准测试中，Franka机械臂实验显示相比传统人工神经网络(ANN)节省96.10%能耗，同时保持相当的控制性能，实现了稳定的手指位置跟踪和目标高度维持。

Conclusion: 群体编码SNN为资源受限应用中的节能高性能机器人控制提供了有前景的解决方案，为现实世界机器人系统的可扩展部署铺平了道路。

Abstract: Energy-efficient and high-performance motor control remains a critical
challenge in robotics, particularly for high-dimensional continuous control
tasks with limited onboard resources. While Deep Reinforcement Learning (DRL)
has achieved remarkable results, its computational demands and energy
consumption limit deployment in resource-constrained environments. This paper
introduces a novel framework combining population-coded Spiking Neural Networks
(SNNs) with DRL to address these challenges. Our approach leverages the
event-driven, asynchronous computation of SNNs alongside the robust policy
optimization capabilities of DRL, achieving a balance between energy efficiency
and control performance. Central to this framework is the Population-coded
Spiking Actor Network (PopSAN), which encodes high-dimensional observations
into neuronal population activities and enables optimal policy learning through
gradient-based updates. We evaluate our method on the Isaac Gym platform using
the PixMC benchmark with complex robotic manipulation tasks. Experimental
results on the Franka robotic arm demonstrate that our approach achieves energy
savings of up to 96.10% compared to traditional Artificial Neural Networks
(ANNs) while maintaining comparable control performance. The trained SNN
policies exhibit robust finger position tracking with minimal deviation from
commanded trajectories and stable target height maintenance during
pick-and-place operations. These results position population-coded SNNs as a
promising solution for energy-efficient, high-performance robotic control in
resource-constrained applications, paving the way for scalable deployment in
real-world robotics systems.

</details>


### [46] [SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams](https://arxiv.org/abs/2510.10602)
*Zhuoheng Gao,Jiyao Zhang,Zhiyong Xie,Hao Dong,Zhaofei Yu,Rongmei Chen,Guozhang Chen,Tiejun Huang*

Main category: cs.RO

TL;DR: SpikeGrasp是一个受神经启发的6-DoF抓取检测框架，直接处理来自立体脉冲相机的原始异步事件，无需重建3D点云，在杂乱和无纹理场景中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器人抓取系统依赖将传感器数据转换为显式3D点云，这一计算步骤在生物智能中不存在。本文探索一种根本上不同的、受神经启发的范式。

Method: 引入SpikeGrasp框架，模拟生物视觉运动通路，处理来自立体脉冲相机的原始异步事件，使用循环脉冲神经网络迭代优化抓取假设，无需重建点云。

Result: 实验表明SpikeGrasp超越传统基于点云的基线方法，特别是在杂乱和无纹理场景中，并显示出显著的数据效率。

Conclusion: 通过验证这种端到端、受神经启发方法的可行性，SpikeGrasp为未来能够实现自然界中流畅高效操作的系统铺平了道路，特别是针对动态物体。

Abstract: Most robotic grasping systems rely on converting sensor data into explicit 3D
point clouds, which is a computational step not found in biological
intelligence. This paper explores a fundamentally different, neuro-inspired
paradigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that
mimics the biological visuomotor pathway, processing raw, asynchronous events
from stereo spike cameras, similarly to retinas, to directly infer grasp poses.
Our model fuses these stereo spike streams and uses a recurrent spiking neural
network, analogous to high-level visual processing, to iteratively refine grasp
hypotheses without ever reconstructing a point cloud. To validate this
approach, we built a large-scale synthetic benchmark dataset. Experiments show
that SpikeGrasp surpasses traditional point-cloud-based baselines, especially
in cluttered and textureless scenes, and demonstrates remarkable data
efficiency. By establishing the viability of this end-to-end, neuro-inspired
approach, SpikeGrasp paves the way for future systems capable of the fluid and
efficient manipulation seen in nature, particularly for dynamic objects.

</details>


### [47] [Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis](https://arxiv.org/abs/2510.10781)
*Douglas Hutchings,Luai Abuelsamen,Karthik Rajgopal*

Main category: cs.RO

TL;DR: 提出了一种双层Voronoi覆盖控制方法，用于协调混合空中-地面机器人团队在危险材料应急响应场景中的部署。该方法通过分离优化空中和地面机器人定位，显著提高了响应速度。


<details>
  <summary>Details</summary>
Motivation: 传统Voronoi覆盖控制在应急场景中存在三个关键限制：异构代理能力（速度差异大）、集群初始部署配置、以及紧急时间约束需要快速响应而非渐进收敛。

Method: 采用解耦的双层架构，分别优化空中和地面机器人定位，通过空投方式将地面传感器部署到高优先级位置。包括有界Voronoi单元计算、重要性加权质心的高效数值积分技术以及防止代理陷入困境的鲁棒控制策略。

Result: 仿真结果显示响应时间减少了88%，在25秒内实现目标传感器覆盖（初始传感器损失18.5%），而仅地面部署需要220秒。

Conclusion: 该方法有效解决了应急响应中的关键挑战，显著提高了混合机器人团队的部署效率和响应速度。

Abstract: We present a comprehensive two-layer Voronoi coverage control approach for
coordinating hybrid aerial-ground robot teams in hazardous material emergency
response scenarios. Traditional Voronoi coverage control methods face three
critical limitations in emergency contexts: heterogeneous agent capabilities
with vastly different velocities, clustered initial deployment configurations,
and urgent time constraints requiring rapid response rather than eventual
convergence. Our method addresses these challenges through a decoupled
two-layer architecture that separately optimizes aerial and ground robot
positioning, with aerial agents delivering ground sensors via airdrop to
high-priority locations. We provide detailed implementation of bounded Voronoi
cell computation, efficient numerical integration techniques for
importance-weighted centroids, and robust control strategies that prevent agent
trapping. Simulation results demonstrate an 88% reduction in response time,
achieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds
compared to 220 seconds for ground-only deployment. Complete implementation
code is available at https://github.com/dHutchings/ME292B.

</details>
