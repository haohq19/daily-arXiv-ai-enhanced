<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA提出了一种通过视觉重编程实现无监督领域自适应的方法，仅在骨干网络前添加轻量的视觉提示层，无需微调骨干网络参数，显著减少了可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需要为每个源-目标对微调骨干网络，导致参数数量和存储需求线性增长，且无法复用训练好的骨干网络参数。

Method: 在骨干网络前添加领域特定的视觉重编程层，生成视觉提示作为纹理偏置来调整输入图像的风格，通过优化域内和域间分布差异来训练这些层，骨干网络参数保持不变。

Result: 在Office-31数据集上达到92.8%的平均准确率，仅需1.5M可训练参数，优于PDA方法1.6%准确率且参数减少54%，相比全骨干微调方法仅需1.7%-2.8%的参数。

Conclusion: VirDA通过视觉重编程实现了参数高效的领域自适应，在保持竞争力的性能同时大幅减少了可训练参数，支持骨干网络的跨域复用。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [2] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两种方法，分别解决长视频生成中的长程依赖建模和自回归解码错误累积问题，提升视频生成的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临两个关键挑战：需要捕捉长程依赖关系，同时防止自回归解码过程中的错误累积。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖关系；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理过程中的错误传播。

Result: 实现了分钟级别的时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing的结合显著增强了自回归视频模型在长视频生成中的实用性和可靠性。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [3] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT是一种统一的多模态大语言模型范式，通过视觉参考令牌直接生成文本和多样视觉输出，解决了现有方法依赖间接表示的限制。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉任务中依赖间接表示（如将坐标生成为文本），限制了性能并阻碍了密集预测任务如分割。

Method: 引入视觉参考令牌(VRTs)，从查询图像的视觉补丁嵌入中派生，与LLM输出文本令牌交织；使用轻量级解码器将LLM输出转换为检测、分割和定位预测；动态扩展嵌入表。

Result: 在四个视觉感知和理解任务中，PaDT持续实现最先进性能，甚至优于显著更大的MLLM模型。

Conclusion: PaDT为MLLMs提供了一种统一且有效的范式，能够直接生成多样视觉输出，在多个任务上表现优异。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [4] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 本文提出F2C方法，通过选择关键片段而非孤立关键帧来提升视频理解性能，同时采用自适应分辨率策略保持固定计算预算。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型面临'大海捞针'问题：原始视频帧产生的大量视觉标记耗尽模型上下文窗口。现有解决方案通过选择稀疏帧集来减少标记数量，但这种逐帧选择丢弃了基本的时间动态，导致对运动和事件连续性的推理效果不佳。

Method: 提出F2C方法：1）从孤立关键帧扩展到关键片段选择；2）采用自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定。

Result: 在三个长视频基准测试上，F2C方法比均匀采样分别提升了8.1%（Video-MME）、5.6%（LongVideoBench）和10.3%（MLVU）。

Conclusion: 保持时间连贯性在帧选择中至关重要，为将视频LLM扩展到实际视频理解应用提供了实用途径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: 该论文从变分互信息下界出发，推导出两种自监督表示学习范式：自蒸馏互信息(SDMI)和联合互信息(JMI)，为现有方法的架构组件提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 自监督表示学习虽然取得了显著经验成功，但其基本原理仍不够清晰。现有工作多将预测器网络、停止梯度操作等架构元素视为经验性补充，缺乏理论依据。

Method: 从变分互信息下界出发，推导出SDMI和JMI两种训练范式。SDMI需要交替优化，理论上必须使用停止梯度操作；JMI允许通过对称架构进行联合优化。

Result: 证明了预测器网络在SDMI中、统计正则化器在JMI中分别是互信息目标的可行替代方案。许多现有自监督学习方法都是这两种范式的具体实例或近似。

Conclusion: 为现有自监督表示学习方法的不同架构组件选择提供了超越启发式便利的理论解释，统一了多种算法的理论基础。

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [6] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: 本文提出了一种量化数据集各向异性（对称性破缺）的度量方法，通过双样本神经网络分类器测试来区分原始数据集与其随机增强版本，并探讨了对称性破缺如何影响等变方法的性能。


<details>
  <summary>Details</summary>
Motivation: 对称感知方法（如数据增强和等变架构）假设变换后的数据点在测试分布中具有高概率或"重要性"。本文旨在批判性评估这一假设，量化数据集中的对称性破缺程度。

Method: 提出基于双样本神经网络分类器的度量方法，通过区分原始数据集和随机增强版本量化各向异性。在合成数据集上验证后，应用于基准点云数据集分析。

Result: 在多个基准点云数据集中发现了意外的高度对齐性。理论分析表明，分布对称性破缺可能阻止不变方法达到最优性能，即使底层标签是真正不变的。实证结果显示等变方法的效益具有数据集依赖性。

Conclusion: 理解等变性何时有效及其原因可能需要重新思考数据中的对称性偏置，对称性破缺对等变方法的影响因数据集而异。

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [7] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: Edge AI通过在网络边缘设备中嵌入智能，实现实时处理、提升隐私保护和降低延迟。本文通过多维度分类法系统回顾了Edge AI的发展历程、现状和未来方向，包括部署位置、处理能力、应用领域和硬件类型。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和边缘计算的发展，需要在网络边缘实现智能处理以应对实时性、隐私保护和延迟等挑战，因此有必要系统梳理Edge AI的技术演进和发展现状。

Method: 采用PRISMA指南进行系统分析，通过多维度分类法（部署位置、处理能力、应用领域、硬件类型）来考察Edge AI技术，从早期内容分发网络和雾计算到现代设备端智能的发展历程。

Result: 识别了Edge AI的核心使能技术（专用硬件加速器、优化软件、通信协议），评估了资源限制、安全、模型管理、功耗和连接性等关键挑战，并发现了神经形态硬件、持续学习算法、边云协作和可信集成等新兴机遇。

Conclusion: Edge AI为研究人员和从业者提供了一个全面框架，通过系统分析揭示了该领域的技术演进路径、当前挑战和未来发展机遇，为边缘智能的进一步发展指明了方向。

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [8] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 本文提出了Flock模型，通过引入概率性节点-关系等变性来解决知识图谱零样本链接预测问题，克服了传统确定性等变性在表达能力上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱基础模型(KGFMs)的确定性等变性限制了其表达能力，无法区分结构相似但语义不同的关系，这阻碍了模型在零样本链接预测中的性能。

Method: 提出概率性节点-关系等变性，在保持分布等变性的同时引入原则性随机化来打破推理过程中的对称性。Flock模型通过迭代采样随机游走、编码序列、序列建模和池化聚合来学习节点和关系的表示。

Result: Flock在新诊断数据集Petals上完美解决现有KGFMs失败的问题，在54个不同领域的知识图谱上实现了实体和关系预测任务的最先进性能。

Conclusion: 概率性节点-关系等变性显著提升了知识图谱基础模型的表达能力，Flock模型在零样本链接预测任务中表现出色，证明了该方法在泛化到新实体和新关系方面的有效性。

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [9] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: 开发了一个预测兽医药物安全结果的框架，使用FDA的128万份报告数据，结合机器学习模型和可解释AI，准确预测动物用药后的死亡与康复结果。


<details>
  <summary>Details</summary>
Motivation: 保护动物福利和人类食品安全，通过预测药物不良事件结果来早期识别高风险药物事件，减少食品链中的违规残留风险。

Method: 预处理管道整合关系表并标准化不良事件，使用随机森林、CatBoost、XGBoost、ExcelFormer和大型语言模型等监督学习模型，采用欠采样和过采样处理类别不平衡，集成AUM伪标签方法改进少数类检测。

Result: 集成方法和CatBoost表现最佳，精确度、召回率和F1分数均达到0.95，SHAP可解释性分析识别出与致命结果相关的生物学合理预测因子。

Conclusion: 结合严格数据工程、先进机器学习和可解释AI的框架能够准确预测兽医安全结果，支持FARAD使命，加强残留风险评估和监管决策。

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [10] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的攻击方法，能够绕过大型语言模型的提示防护机制，成功越狱多个生产级模型，揭示了轻量级提示防护的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，AI安全和对齐变得至关重要。提示防护作为一种轻量级机制被广泛采用，但其有效性需要验证。

Method: 利用提示防护与主模型之间的资源不对称性，编码一个轻量级防护无法解码但主模型可以理解的越狱提示。

Result: 该方法成功越狱了Google Gemini、DeepSeek Chat、Grok和Mistral Le Chat等高度保护的聊天接口，同时保持响应质量。

Conclusion: 轻量级提示防护在现代LLM架构中存在固有的攻击面，需要将防御重点从阻止恶意输入转向防止恶意输出。

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [11] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: MIRA是一种无需训练、在推理时对齐扩散模型的方法，通过图像空间的KL代理正则化来防止奖励黑客问题，在保持提示遵循性的同时提升奖励分数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像虽然符合文本提示，但往往无法满足用户特定的标量奖励标准（如美学评分）。现有的推理时对齐方法存在奖励黑客问题，即模型产生高奖励分数但严重偏离原始提示的图像。

Method: 提出了MIRA方法，引入图像空间的基于分数的KL代理正则化，使用冻结的主干网络约束采样轨迹，防止输出分布漂移。还提出了MIRA-DPO，将偏好优化映射到推理时间，适用于不可微分奖励。

Result: 在SDv1.5和SDXL模型、多种奖励函数（美学评分、HPSv2、PickScore）和公共数据集上，MIRA相对于强基线实现了超过60%的胜率，同时保持提示遵循性。机制图显示奖励增益接近零漂移，而DNO方法随着计算增加而漂移。

Conclusion: MIRA提供了一种有效的推理时对齐方法，能够在不进行微调的情况下提升扩散模型生成图像的质量，同时避免奖励黑客问题，为不可微分奖励的对齐提供了可行方案。

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [12] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: 本文分析了RLHF中KL散度正则化的不同实现方式，证明了'k1 in reward'和'k2 as loss'在策略条件下梯度等价，都是RKL正则化的理论正确实现，而'k3 as loss'只是有偏的一阶近似。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法中KL正则化的实现存在混淆，有些方法将KL项作为数值估计系数而非优化损失，需要建立统一框架来分析不同实现方式的理论基础。

Method: 建立统一框架连接两种实现风格：将数学项kn作为策略得分函数的分离系数或作为直接损失函数，证明后者可以通过前者的等效梯度系数来分析。

Result: 证明在策略条件下'k2 as loss'与'k1 in reward'梯度等价，都是RKL目标的正确实现；而'k3 as loss'是有偏近似；同时指出离策略实现因忽略重要性采样而存在偏差。

Conclusion: 为KL正则化的选择和正确实现提供了基于梯度的理论依据，有助于开发更稳健有效的RLHF系统。

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [13] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fernández*

Main category: cs.LG

TL;DR: Poolformer是一个序列到序列模型，用循环层和池化操作替代自注意力机制，解决了长序列处理中的二次复杂度问题，在音频处理任务中表现优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长序列处理中存在二次复杂度问题，限制了其实际应用。需要开发更高效的序列建模方法来处理长序列数据。

Method: 使用循环层替代自注意力，引入池化操作减少序列长度。模型采用递归定义的SkipBlocks结构，包含残差块、下采样池化层、嵌套SkipBlock、上采样池化层等组件。

Result: 池化操作显著加速训练，改善感知指标（FID和IS），防止过拟合。在原始音频处理任务中，Poolformer超越了SaShiMi和Mamba等最先进模型。

Conclusion: Poolformer为长序列处理提供了高效解决方案，深层处理长程依赖，浅层处理短期特征。未来可扩展到文本、视觉和多模态应用。

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [14] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: 使用强化学习来对抗动态反应式干扰，通过自适应调整发射功率、调制方式和信道选择来优化吞吐量


<details>
  <summary>Details</summary>
Motivation: 解决反应式干扰问题，干扰者采用动态策略选择信道和感知阈值来检测和干扰传输，需要学习避免干扰并优化吞吐量

Method: 使用Q-learning处理离散干扰事件状态，使用深度Q网络(DQN)处理基于接收功率的连续状态，通过不同奖励函数和动作集进行学习

Result: 强化学习能够快速适应频谱动态变化，在信道和干扰策略随时间变化时维持高传输速率

Conclusion: 强化学习是应对动态反应式干扰的有效方法，能够自适应地优化通信性能

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个用于评估多平台代理环境中长期记忆和状态跟踪的基准测试，模拟真实组织工作流程，集成Slack、Linear和Git等平台的异步事件，测试记忆获取、选择和冲突解决等能力。


<details>
  <summary>Details</summary>
Motivation: 现有上下文和记忆基准测试主要关注对话场景，但评估动态企业环境中的记忆能力对于实际应用至关重要。

Method: 通过专家手动设计和基于代理的可扩展合成方法构建MEMTRACK数据集，创建基于真实软件开发流程的生态有效场景，引入正确性、效率和冗余度等评估指标。

Result: 在SoTA LLM和记忆后端上的实验显示，在长时程记忆利用、跨平台依赖处理和矛盾解决方面存在挑战，表现最佳的GPT-5模型在MEMTRACK上仅达到60%的正确性得分。

Conclusion: 该工作为记忆增强代理的评估研究提供了一个可扩展框架，超越了现有对话设置的局限，为复杂组织环境中的多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [16] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个基于LLM的AI代理，将原始系统日志转换为基于本体的知识图谱，并映射到MITRE ATT&CK战术框架，用于提取可操作的网络威胁情报。


<details>
  <summary>Details</summary>
Motivation: 系统日志是宝贵的网络威胁情报来源，但由于缺乏结构、语义不一致和跨设备碎片化等问题，其效用受到限制。需要能够将噪声异构数据转化为连贯可互操作表示的方法。

Method: 集成轻量级日志本体与检索增强生成(RAG)和迭代校正步骤，确保生成的知识图谱在语法和语义上有效。系统将知识图谱聚合为会话，并使用LLM预测MITRE ATT&CK战术。

Result: 在公共基准和真实世界蜜罐数据集上的评估表明，OntoLogX能够跨多个知识图谱后端稳健生成知识图谱，并准确将对抗活动映射到ATT&CK战术。检索和校正提高了精确率和召回率。

Conclusion: 基于本体的表示为可操作的网络威胁情报提取提供了价值，代码导向模型在结构化日志分析中表现有效，检索和校正对精度和召回率有显著益处。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [17] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种结合稀疏自编码器和聚类技术的方法，通过分析LLM内部token表示来指导数学推理任务的生成，平衡利用和探索以获得更高准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过分析大型语言模型的内部token表示，识别和引导数学推理中的推理轨迹，平衡利用已知推理路径和探索新路径的需求。

Method: 训练稀疏自编码器生成稀疏向量表示，应用k-means聚类构建token簇图，基于边权重定义奖励函数量化推理轨迹遵循度，并测量生成多样性评估探索程度。

Result: 研究发现平衡利用和探索对数学推理任务的高准确性至关重要，稀疏自编码器可作为可扩展的奖励模型指导生成过程。

Conclusion: 该方法能防止极端行为，促进更高质量的推理过程，在LLM中实现利用和探索的平衡权衡。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [18] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文提出了一种对抗性逆强化学习方法，为大型语言模型学习密集的token级推理奖励，用于过程监督而非风格模仿。该奖励既在训练时优化推理策略，又在推理时作为评判者重排采样轨迹。


<details>
  <summary>Details</summary>
Motivation: 重新构建和操作化对抗性逆强化学习，直接从专家演示中学习密集的token级奖励模型，而不是通过监督微调模仿风格，以优先考虑正确性而非表面形式。

Method: 使用对抗性逆强化学习框架，学习密集的token级推理奖励模型。该奖励在训练时提供步骤级反馈优化推理策略，在推理时作为评判者重排采样轨迹。

Result: 在GSM8K数据集上使用Llama3和Qwen2.5骨干网络，证明：(i)密集推理奖励可用作学习信号激发推理；(ii)通过奖励引导的重排提高了预测性能（特别是基于Llama的策略）。

Conclusion: 通过将训练信号、推理时选择和token级诊断统一到单个推理奖励中，这项工作表明可重用的过程级奖励具有增强语言模型中多步推理的广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [19] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个表格异常检测基准，通过恢复文本语义来支持上下文感知的研究，提供20个带有文本元数据的表格数据集和零样本LLM框架。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义上下文，而实际应用中异常定义与领域特定语境密切相关，限制了模型利用领域知识进行检测的能力。

Method: 构建20个带有结构化文本元数据的表格数据集，实现包括经典、深度学习和LLM方法在内的最先进异常检测算法，并提供零样本LLM框架。

Result: 实验表明语义上下文能提高检测性能，并通过支持领域感知推理增强可解释性。

Conclusion: ReTabAD为系统探索上下文感知异常检测建立了基准，展示了文本元数据在异常检测中的作用和实用性。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出基于置信度感知的路由系统，在生成前评估模型不确定性，根据可靠性估计重定向查询，显著减少幻觉并降低计算成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，生成看似合理但事实错误的内容。现有后生成修正方法计算成本高且无法阻止不可靠内容的生成

Method: 结合三种互补信号：内部表示与参考嵌入的语义对齐、跨模型层的内部收敛分析、学习置信度估计，统一置信度分数决定四个路由路径

Result: 在知识密集型QA基准测试中，幻觉检测显著改善(0.74 vs 0.42基线)，计算成本降低40%，F1分数从0.61提高到0.82，假阳性率低(0.09)

Conclusion: 从被动修正转向主动评估的范式转变，为LLM可靠性增强提供了计算高效的方法

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [21] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob是一个基于LLM的语义合成管道，用于动态移动性预测，通过多智能体框架从在线文本中提取时空相关信息，并结合时空数据实现更准确的事件驱动预测。


<details>
  <summary>Details</summary>
Motivation: 现有时空模型难以利用描述外部事件的文本信息来预测人类移动性中的突变变化。

Method: 采用多智能体框架，LLM智能体自动从复杂在线文本中提取和推理时空相关文本，通过渐进融合架构将细粒度相关上下文与时空数据结合。

Result: 在构建的数据集上评估，SeMob相比时空模型在MAE和RMSE上分别最大减少13.92%和11.12%，在事件发生时空区域附近表现尤为突出。

Conclusion: SeMob框架通过结合文本语义和时空数据，显著提升了事件驱动移动性预测的准确性。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [22] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: 提出了渐进式自反思（PSR）方法，一种推理时技术，让大语言模型能够动态自我监控和修正输出，显著降低有害内容生成风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成连贯文本方面表现出色，但部署时可能产生有害或不适当内容，需要有效的安全防护机制。

Method: 采用渐进式自反思技术，通过多轮自我评估动态修正输出，并引入轻量级自反思预测器来根据输入复杂度自适应调整反思轮数。

Result: 在多个模型上显著降低攻击成功率：Llama-3.1-8B-Instruct从77.5%降至5.9%，Llama-3.1-8B基础版从89.7%降至5.6%，Qwen2.5-7B-Instruct从44.4%降至3.8%，同时保持良性任务性能。

Conclusion: 渐进式自反思是一种可扩展的测试时方法，通过按输入风险状况动态分配计算资源来增强大语言模型的安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [23] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: TAG-EQA是一个提示框架，通过将因果事件图转换为自然语言语句注入LLM输入，在事件问答任务中平均提升5%准确率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于事件的问题上表现不佳，特别是需要因果或时间推理的任务

Method: 提出TAG-EQA框架，将结构化因果事件图转换为自然语言语句注入LLM输入，包含9种提示配置（3种策略×3种输入模态）

Result: 在TORQUESTRA基准测试中，相比纯文本基线平均提升5%准确率，零样本设置下最高提升12%，图增强CoT提示有效时提升18%

Conclusion: 因果图可以在不微调的情况下增强LLM的事件推理能力，为基于提示的问答提供灵活的结构编码方式

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [24] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: 本研究采用过程导向方法，通过约束性决策分析LLM作为计算作者的创造力，发现LLM普遍更重视风格而非其他叙事元素。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM创造力的评估主要关注输出质量而非创作过程，本研究旨在通过叙事学视角分析LLM作为计算作者的创作过程。

Method: 使用受控提示分配作者角色，分析模型的创作偏好，并通过约束性决策框架考察其选择理由。

Result: LLM一致性地强调风格元素，优于角色、事件和场景等其他叙事要素，不同模型展现出独特的创作特征。

Conclusion: 该方法为分析AI作者创造力提供了新颖的系统性工具，揭示了LLM在创作过程中的偏好模式。

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill是一个统一的学习框架，结合模仿学习和任务运动规划的优势，实现组合泛化和实时故障恢复。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中多步操作的挑战：模仿学习缺乏组合泛化能力，而任务运动规划存在规划延迟问题。

Method: 从无标签、无分割的演示中联合学习谓词、操作符和技能；运行时使用符号规划器组合和重新排序技能；在运动和符号层面进行实时恢复。

Result: 在RoboCasa模拟中执行12个单步任务成功率85%；无需额外数据即可组合成最多6次技能重组的复杂计划；在真实机器人上从5分钟无标签数据学习后能执行多个任务。

Conclusion: SymSkill框架成功结合了IL和TAMP的优势，实现了组合泛化、实时故障恢复和安全执行能力。

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>
