{"id": "2509.14544", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14544", "abs": "https://arxiv.org/abs/2509.14544", "authors": ["Zisen Kong", "Bo Zhong", "Pengyuan Li", "Dongxia Chang", "Yiming Wang"], "title": "MemEvo: Memory-Evolving Incremental Multi-view Clustering", "comment": null, "summary": "Incremental multi-view clustering aims to achieve stable clustering results\nwhile addressing the stability-plasticity dilemma (SPD) in incremental views.\nAt the core of SPD is the challenge that the model must have enough plasticity\nto quickly adapt to new data, while maintaining sufficient stability to\nconsolidate long-term knowledge and prevent catastrophic forgetting. Inspired\nby the hippocampal-prefrontal cortex collaborative memory mechanism in\nneuroscience, we propose a Memory-Evolving Incremental Multi-view Clustering\nmethod (MemEvo) to achieve this balance. First, we propose a\nhippocampus-inspired view alignment module that captures the gain information\nof new views by aligning structures in continuous representations. Second, we\nintroduce a cognitive forgetting mechanism that simulates the decay patterns of\nhuman memory to modulate the weights of historical knowledge. Additionally, we\ndesign a prefrontal cortex-inspired knowledge consolidation memory module that\nleverages temporal tensor stability to gradually consolidate historical\nknowledge. By integrating these modules, MemEvo achieves strong knowledge\nretention capabilities in scenarios with a growing number of views. Extensive\nexperiments demonstrate that MemEvo exhibits remarkable advantages over\nexisting state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faMemEvo\u65b9\u6cd5\u89e3\u51b3\u589e\u91cf\u591a\u89c6\u56fe\u805a\u7c7b\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u901a\u8fc7\u6a21\u62df\u6d77\u9a6c\u4f53-\u524d\u989d\u53f6\u76ae\u5c42\u534f\u4f5c\u8bb0\u5fc6\u673a\u5236\uff0c\u5b9e\u73b0\u65b0\u89c6\u56fe\u9002\u5e94\u548c\u5386\u53f2\u77e5\u8bc6\u4fdd\u7559\u7684\u5e73\u8861", "motivation": "\u89e3\u51b3\u589e\u91cf\u591a\u89c6\u56fe\u805a\u7c7b\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883(SPD)\uff0c\u6a21\u578b\u9700\u8981\u8db3\u591f\u7684\u53ef\u5851\u6027\u6765\u5feb\u901f\u9002\u5e94\u65b0\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u7a33\u5b9a\u6027\u6765\u5de9\u56fa\u957f\u671f\u77e5\u8bc6\u5e76\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8", "method": "1) \u6d77\u9a6c\u4f53\u542f\u53d1\u7684\u89c6\u56fe\u5bf9\u9f50\u6a21\u5757\uff1a\u901a\u8fc7\u5bf9\u9f50\u8fde\u7eed\u8868\u793a\u4e2d\u7684\u7ed3\u6784\u6765\u6355\u83b7\u65b0\u89c6\u56fe\u7684\u589e\u76ca\u4fe1\u606f\uff1b2) \u8ba4\u77e5\u9057\u5fd8\u673a\u5236\uff1a\u6a21\u62df\u4eba\u7c7b\u8bb0\u5fc6\u8870\u51cf\u6a21\u5f0f\u6765\u8c03\u8282\u5386\u53f2\u77e5\u8bc6\u6743\u91cd\uff1b3) \u524d\u989d\u53f6\u76ae\u5c42\u542f\u53d1\u7684\u77e5\u8bc6\u5de9\u56fa\u8bb0\u5fc6\u6a21\u5757\uff1a\u5229\u7528\u65f6\u95f4\u5f20\u91cf\u7a33\u5b9a\u6027\u9010\u6b65\u5de9\u56fa\u5386\u53f2\u77e5\u8bc6", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eMemEvo\u5728\u89c6\u56fe\u6570\u91cf\u589e\u957f\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf", "conclusion": "MemEvo\u901a\u8fc7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u8bb0\u5fc6\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u589e\u91cf\u591a\u89c6\u56fe\u805a\u7c7b\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u52a8\u6001\u591a\u89c6\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.14516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14516", "abs": "https://arxiv.org/abs/2509.14516", "authors": ["Adam D. Hines", "Alejandro Fontan", "Michael Milford", "Tobias Fischer"], "title": "Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods", "comment": "8 pages, 6 figures, under review", "summary": "Event-based localization research and datasets are a rapidly growing area of\ninterest, with a tenfold increase in the cumulative total number of published\npapers on this topic over the past 10 years. Whilst the rapid expansion in the\nfield is exciting, it brings with it an associated challenge: a growth in the\nvariety of required code and package dependencies as well as data formats,\nmaking comparisons difficult and cumbersome for researchers to implement\nreliably. To address this challenge, we present Event-LAB: a new and unified\nframework for running several event-based localization methodologies across\nmultiple datasets. Event-LAB is implemented using the Pixi package and\ndependency manager, that enables a single command-line installation and\ninvocation for combinations of localization methods and datasets. To\ndemonstrate the capabilities of the framework, we implement two common\nevent-based localization pipelines: Visual Place Recognition (VPR) and\nSimultaneous Localization and Mapping (SLAM). We demonstrate the ability of the\nframework to systematically visualize and analyze the results of multiple\nmethods and datasets, revealing key insights such as the association of\nparameters that control event collection counts and window sizes for frame\ngeneration to large variations in performance. The results and analysis\ndemonstrate the importance of fairly comparing methodologies with consistent\nevent image generation parameters. Our Event-LAB framework provides this\nability for the research community, by contributing a streamlined workflow for\neasily setting up multiple conditions.", "AI": {"tldr": "Event-LAB\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u4e8b\u4ef6\u76f8\u673a\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u4ee3\u7801\u4f9d\u8d56\u548c\u6570\u636e\u683c\u5f0f\u591a\u6837\u5316\u5e26\u6765\u7684\u6bd4\u8f83\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5b9a\u4f4d\u7814\u7a76\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u4ee3\u7801\u4f9d\u8d56\u548c\u6570\u636e\u683c\u5f0f\u591a\u6837\u5316\u4f7f\u5f97\u65b9\u6cd5\u6bd4\u8f83\u548c\u5b9e\u65bd\u53d8\u5f97\u56f0\u96be\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528Pixi\u5305\u7ba1\u7406\u5668\u5b9e\u73b0\u5355\u547d\u4ee4\u884c\u5b89\u88c5\u548c\u8c03\u7528\uff0c\u652f\u6301\u591a\u79cd\u5b9a\u4f4d\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86VPR\u548cSLAM\u4e24\u79cd\u5e38\u89c1\u4e8b\u4ef6\u5b9a\u4f4d\u6d41\u7a0b\u3002", "result": "\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u53ef\u89c6\u5316\u5206\u6790\u591a\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7ed3\u679c\uff0c\u53d1\u73b0\u4e8b\u4ef6\u6536\u96c6\u8ba1\u6570\u548c\u5e27\u751f\u6210\u7a97\u53e3\u5927\u5c0f\u7b49\u53c2\u6570\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "Event-LAB\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u516c\u5e73\u6bd4\u8f83\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u7b80\u5316\u591a\u6761\u4ef6\u8bbe\u7f6e\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5f3a\u8c03\u4e86\u4f7f\u7528\u4e00\u81f4\u4e8b\u4ef6\u56fe\u50cf\u751f\u6210\u53c2\u6570\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14264", "categories": ["cs.CL", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.14264", "abs": "https://arxiv.org/abs/2509.14264", "authors": ["Gautam Kishore Shahi", "Tim A. Majchrzak"], "title": "Defining, Understanding, and Detecting Online Toxicity: Challenges and Machine Learning Approaches", "comment": "Paper is accepted at LNCS Porceedings", "summary": "Online toxic content has grown into a pervasive phenomenon, intensifying\nduring times of crisis, elections, and social unrest. A significant amount of\nresearch has been focused on detecting or analyzing toxic content using\nmachine-learning approaches. The proliferation of toxic content across digital\nplatforms has spurred extensive research into automated detection mechanisms,\nprimarily driven by advances in machine learning and natural language\nprocessing. Overall, the present study represents the synthesis of 140\npublications on different types of toxic content on digital platforms. We\npresent a comprehensive overview of the datasets used in previous studies\nfocusing on definitions, data sources, challenges, and machine learning\napproaches employed in detecting online toxicity, such as hate speech,\noffensive language, and harmful discourse. The dataset encompasses content in\n32 languages, covering topics such as elections, spontaneous events, and\ncrises. We examine the possibility of using existing cross-platform data to\nimprove the performance of classification models. We present the\nrecommendations and guidelines for new research on online toxic consent and the\nuse of content moderation for mitigation. Finally, we present some practical\nguidelines to mitigate toxic content from online platforms.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4efd\u5173\u4e8e\u5728\u7ebf\u6bd2\u6027\u5185\u5bb9\u68c0\u6d4b\u7684\u7efc\u8ff0\u6027\u7814\u7a76\uff0c\u7efc\u5408\u4e86140\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u5206\u6790\u4e86\u6570\u636e\u96c6\u3001\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u591a\u8bed\u8a00\u6bd2\u6027\u5185\u5bb9\u68c0\u6d4b\u7684\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u7ebf\u6bd2\u6027\u5185\u5bb9\u5728\u5371\u673a\u65f6\u671f\u3001\u9009\u4e3e\u548c\u793e\u4f1a\u52a8\u8361\u671f\u95f4\u6076\u5316\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u6765\u63d0\u9ad8\u81ea\u52a8\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u7efc\u5408\u5206\u6790140\u7bc7\u76f8\u5173\u7814\u7a76\u6587\u732e\uff0c\u5305\u62ec\u6570\u636e\u96c6\u5b9a\u4e49\u3001\u6570\u636e\u6e90\u3001\u6311\u6218\u5206\u6790\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u6db5\u76d632\u79cd\u8bed\u8a00\u7684\u6bd2\u6027\u5185\u5bb9\uff0c\u63a2\u8ba8\u4e86\u8de8\u5e73\u53f0\u6570\u636e\u5728\u63d0\u5347\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6bd2\u6027\u5185\u5bb9\u7814\u7a76\u6307\u5357\u548c\u5185\u5bb9\u5ba1\u67e5\u51cf\u5c11\u5efa\u8bae\uff0c\u4e3a\u5e73\u53f0\u6bd2\u6027\u5185\u5bb9\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.14531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14531", "abs": "https://arxiv.org/abs/2509.14531", "authors": ["Haoran Xiao", "Xue Wang", "Huimin Lu", "Zhiwen Zeng", "Zirui Guo", "Ziqi Ni", "Yicong Ye", "Wei Dai"], "title": "Dual-Arm Hierarchical Planning for Laboratory Automation: Vibratory Sieve Shaker Operations", "comment": null, "summary": "This paper addresses the challenges of automating vibratory sieve shaker\noperations in a materials laboratory, focusing on three critical tasks: 1)\ndual-arm lid manipulation in 3 cm clearance spaces, 2) bimanual handover in\noverlapping workspaces, and 3) obstructed powder sample container delivery with\norientation constraints. These tasks present significant challenges, including\ninefficient sampling in narrow passages, the need for smooth trajectories to\nprevent spillage, and suboptimal paths generated by conventional methods. To\novercome these challenges, we propose a hierarchical planning framework\ncombining Prior-Guided Path Planning and Multi-Step Trajectory Optimization.\nThe former uses a finite Gaussian mixture model to improve sampling efficiency\nin narrow passages, while the latter refines paths by shortening, simplifying,\nimposing joint constraints, and B-spline smoothing. Experimental results\ndemonstrate the framework's effectiveness: planning time is reduced by up to\n80.4%, and waypoints are decreased by 89.4%. Furthermore, the system completes\nthe full vibratory sieve shaker operation workflow in a physical experiment,\nvalidating its practical applicability for complex laboratory automation.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u89c4\u5212\u6846\u67b6\u89e3\u51b3\u632f\u52a8\u7b5b\u81ea\u52a8\u5316\u64cd\u4f5c\u4e2d\u7684\u72ed\u7a84\u7a7a\u95f4\u53cc\u81c2\u64cd\u4f5c\u3001\u53cc\u624b\u4ea4\u63a5\u548c\u53d7\u9650\u5bb9\u5668\u9012\u9001\u7b49\u6311\u6218\uff0c\u89c4\u5212\u65f6\u95f4\u51cf\u5c1180.4%\uff0c\u8def\u5f84\u70b9\u51cf\u5c1189.4%\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u6750\u6599\u5b9e\u9a8c\u5ba4\u632f\u52a8\u7b5b\u81ea\u52a8\u5316\u64cd\u4f5c\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u72ed\u7a84\u7a7a\u95f4\u7684\u53cc\u81c2\u76d6\u64cd\u4f5c\u3001\u91cd\u53e0\u5de5\u4f5c\u7a7a\u95f4\u7684\u53cc\u624b\u4ea4\u63a5\u4ee5\u53ca\u6709\u65b9\u5411\u7ea6\u675f\u7684\u53d7\u963b\u7c89\u672b\u5bb9\u5668\u9012\u9001\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5b58\u5728\u91c7\u6837\u6548\u7387\u4f4e\u3001\u8f68\u8ff9\u4e0d\u5e73\u6ed1\u548c\u4f20\u7edf\u65b9\u6cd5\u8def\u5f84\u4e0d\u4f73\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5148\u9a8c\u5f15\u5bfc\u8def\u5f84\u89c4\u5212\u548c\u591a\u6b65\u8f68\u8ff9\u4f18\u5316\u3002\u524d\u8005\u4f7f\u7528\u6709\u9650\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u63d0\u9ad8\u72ed\u7a84\u901a\u9053\u91c7\u6837\u6548\u7387\uff0c\u540e\u8005\u901a\u8fc7\u8def\u5f84\u7f29\u77ed\u3001\u7b80\u5316\u3001\u65bd\u52a0\u5173\u8282\u7ea6\u675f\u548cB\u6837\u6761\u5e73\u6ed1\u6765\u4f18\u5316\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u89c4\u5212\u65f6\u95f4\u51cf\u5c1180.4%\uff0c\u8def\u5f84\u70b9\u6570\u91cf\u51cf\u5c1189.4%\u3002\u7cfb\u7edf\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u6210\u529f\u5b8c\u6210\u4e86\u5b8c\u6574\u7684\u632f\u52a8\u7b5b\u64cd\u4f5c\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u5206\u5c42\u89c4\u5212\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u6548\u7387\u548c\u8def\u5f84\u8d28\u91cf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.14942", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14942", "abs": "https://arxiv.org/abs/2509.14942", "authors": ["Minh-Khoi Pham", "Tai Tan Mai", "Martin Crane", "Rob Brennan", "Marie E. Ward", "Una Geary", "Declan Byrne", "Brian O Connell", "Colm Bergin", "Donncha Creagh", "Nick McDonald", "Marija Bezbradica"], "title": "Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers", "comment": "Accepted to BMC Medical Informatics and Decision Making on September\n  18th 2025", "summary": "Carbapenemase-Producing Enterobacteriace poses a critical concern for\ninfection prevention and control in hospitals. However, predictive modeling of\npreviously highlighted CPE-associated risks such as readmission, mortality, and\nextended length of stay (LOS) remains underexplored, particularly with modern\ndeep learning approaches. This study introduces an eXplainable AI modeling\nframework to investigate CPE impact on patient outcomes from Electronic Medical\nRecords data of an Irish hospital. We analyzed an inpatient dataset from an\nIrish acute hospital, incorporating diagnostic codes, ward transitions, patient\ndemographics, infection-related variables and contact network features. Several\nTransformer-based architectures were benchmarked alongside traditional machine\nlearning models. Clinical outcomes were predicted, and XAI techniques were\napplied to interpret model decisions. Our framework successfully demonstrated\nthe utility of Transformer-based models, with TabTransformer consistently\noutperforming baselines across multiple clinical prediction tasks, especially\nfor CPE acquisition (AUROC and sensitivity). We found infection-related\nfeatures, including historical hospital exposure, admission context, and\nnetwork centrality measures, to be highly influential in predicting patient\noutcomes and CPE acquisition risk. Explainability analyses revealed that\nfeatures like \"Area of Residence\", \"Admission Ward\" and prior admissions are\nkey risk factors. Network variables like \"Ward PageRank\" also ranked highly,\nreflecting the potential value of structural exposure information. This study\npresents a robust and explainable AI framework for analyzing complex EMR data\nto identify key risk factors and predict CPE-related outcomes. Our findings\nunderscore the superior performance of the Transformer models and highlight the\nimportance of diverse clinical and network features.", "AI": {"tldr": "\u8fd9\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684AI\u6846\u67b6\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u5206\u6790\u533b\u9662\u7535\u5b50\u75c5\u5386\u6570\u636e\uff0c\u9884\u6d4b\u80a1\u80f6\u9709\u7d20\u97e9\u751f\u7269\u6d88\u5316\u83cc\u83cc\u751f\u7269\u6d88\u5316\u83cc\uff08CPE\uff09\u76f8\u5173\u75c5\u4eba\u7ed3\u679c\uff0cTabTransformer\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u80a1\u80f6\u9709\u7d20\u97e9\u751f\u7269\u6d88\u5316\u83cc\u83cc\u751f\u7269\u6d88\u5316\u83cc\uff08CPE\uff09\u5bf9\u533b\u9662\u611f\u67d3\u63a7\u5236\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u98ce\u9669\u9884\u6d4b\u7684\u7814\u7a76\u8fd8\u5f88\u5c11\u3002", "method": "\u5206\u6790\u7231\u5c14\u5170\u6025\u8bca\u533b\u9662\u7684\u4f4f\u9662\u60a3\u8005\u6570\u636e\uff0c\u5305\u62ec\u8bca\u65ad\u4ee3\u7801\u3001\u75c5\u623f\u8f6c\u6362\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u611f\u67d3\u76f8\u5173\u53d8\u91cf\u548c\u63a5\u89e6\u7f51\u7edc\u7279\u5f81\u3002\u5bf9\u6bd4\u4e86Transformer\u67b6\u6784\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "TabTransformer\u6a21\u578b\u5728\u591a\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728CPE\u611f\u67d3\u9884\u6d4b\u4e0a\uff08AUROC\u548c\u654f\u611f\u5ea6\uff09\u3002\u611f\u67d3\u76f8\u5173\u7279\u5f81\u3001\u5386\u53f2\u4f4f\u9662\u66dd\u9732\u3001\u63a5\u89e6\u7f51\u7edc\u4e2d\u5fc3\u6027\u6307\u6807\u7b49\u662f\u5173\u952e\u98ce\u9669\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5065\u58ee\u4e14\u53ef\u89e3\u91ca\u7684AI\u6846\u67b6\uff0c\u8bc1\u660e\u4e86Transformer\u6a21\u578b\u5728\u533b\u7597\u6570\u636e\u5206\u6790\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u786e\u5b9a\u4e86\u4e34\u5e8a\u548c\u7f51\u7edc\u7279\u5f81\u5bf9\u9884\u6d4bCPE\u76f8\u5173\u7ed3\u679c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14269", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14269", "abs": "https://arxiv.org/abs/2509.14269", "authors": ["Zhang Jianbin", "Yulin Zhu", "Wai Lun Lo", "Richard Tai-Chiu Hsung", "Harris Sik-Ho Tsang", "Kai Zhou"], "title": "SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved great success in medical question\nanswering and clinical decision-making, promoting the efficiency and\npopularization of the personalized virtual doctor in society. However, the\ntraditional fine-tuning strategies on LLM require the updates of billions of\nparameters, substantially increasing the training cost, including the training\ntime and utility cost. To enhance the efficiency and effectiveness of the\ncurrent medical LLMs and explore the boundary of the representation capability\nof the LLMs on the medical domain, apart from the traditional fine-tuning\nstrategies from the data perspective (i.e., supervised fine-tuning or\nreinforcement learning from human feedback), we instead craft a novel sparse\nmedical LLM named SparseDoctor armed with contrastive learning enhanced\nLoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end,\nthe crafted automatic routing mechanism can scientifically allocate the\ncomputational resources among different LoRA experts supervised by the\ncontrastive learning. Additionally, we also introduce a novel expert memory\nqueue mechanism to further boost the efficiency of the overall framework and\nprevent the memory overflow during training. We conduct comprehensive\nevaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med.\nExperimental results demonstrate that the proposed LLM can consistently\noutperform the strong baselines such as the HuatuoGPT series.", "AI": {"tldr": "\u63d0\u51faSparseDoctor\uff0c\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684LoRA-MoE\u67b6\u6784\u7684\u7a00\u758f\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u8def\u7531\u673a\u5236\u548c\u4e13\u5bb6\u8bb0\u5fc6\u961f\u5217\u63d0\u9ad8\u6548\u7387\uff0c\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edfLLM\u5fae\u8c03\u9700\u8981\u66f4\u65b0\u6570\u5341\u4ebf\u53c2\u6570\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u4e3a\u63d0\u5347\u533b\u7597LLM\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u63a2\u7d22LLM\u5728\u533b\u7597\u9886\u57df\u7684\u8868\u793a\u80fd\u529b\u8fb9\u754c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684LoRA-MoE\u67b6\u6784\uff0c\u8bbe\u8ba1\u81ea\u52a8\u8def\u7531\u673a\u5236\u79d1\u5b66\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5f15\u5165\u4e13\u5bb6\u8bb0\u5fc6\u961f\u5217\u673a\u5236\u63d0\u5347\u6548\u7387\u5e76\u9632\u6b62\u8bad\u7ec3\u65f6\u5185\u5b58\u6ea2\u51fa\u3002", "result": "\u5728CMB\u3001CMExam\u548cCMMLU-Med\u4e09\u4e2a\u5178\u578b\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u6a21\u578b\u6301\u7eed\u4f18\u4e8eHuatuoGPT\u7cfb\u5217\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SparseDoctor\u901a\u8fc7\u521b\u65b0\u7684\u7a00\u758f\u67b6\u6784\u548c\u4f18\u5316\u673a\u5236\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u533b\u7597LLM\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u533b\u7597\u9886\u57dfLLM\u7684\u9ad8\u6548\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14775", "abs": "https://arxiv.org/abs/2509.14775", "authors": ["Shuangshuang He", "Yuanting Zhang", "Hongli Liang", "Qingye Meng", "Xingyuan Yuan"], "title": "FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration", "comment": null, "summary": "Accurate hourly weather forecasting is critical for numerous applications.\nRecent deep learning models have demonstrated strong capability on 6-hour\nintervals, yet achieving accurate and stable hourly predictions remains a\ncritical challenge. This is primarily due to the rapid accumulation of errors\nin autoregressive rollouts and temporal discontinuities within the ERA5 data's\n12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a\nframework that models atmospheric state evolution as a continuous flow.\nFlowCast-ODE learns the conditional flow path directly from the previous state,\nan approach that aligns more naturally with physical dynamic systems and\nenables efficient computation. A coarse-to-fine strategy is introduced to train\nthe model on 6-hour data using dynamic flow matching and then refined on hourly\ndata that incorporates an Ordinary Differential Equation (ODE) solver to\nachieve temporally coherent forecasts. In addition, a lightweight low-rank\nAdaLN-Zero modulation mechanism is proposed and reduces model size by 15%\nwithout compromising accuracy. Experiments demonstrate that FlowCast-ODE\noutperforms strong baselines, yielding lower root mean square error (RMSE) and\nbetter energy conservation, which reduces blurring and preserves more\nfine-scale spatial details. It also shows comparable performance to the\nstate-of-the-art model in forecasting extreme events like typhoons.\nFurthermore, the model alleviates temporal discontinuities associated with\nassimilation cycle transitions.", "AI": {"tldr": "FlowCast-ODE\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fde\u7eed\u6d41\u5efa\u6a21\u7684\u5929\u6c14\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7ODE\u6c42\u89e3\u5668\u548c\u7c97\u5230\u7ec6\u7b56\u7565\u89e3\u51b3\u5c0f\u65f6\u7ea7\u9884\u6d4b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u65f6\u95f4\u4e0d\u8fde\u7eed\u95ee\u9898\uff0c\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5c0f\u65f6\u7ea7\u5929\u6c14\u9884\u62a5\u4e2d\u7531\u4e8e\u81ea\u56de\u5f52\u8bef\u5dee\u7d2f\u79ef\u548cERA5\u6570\u636e12\u5c0f\u65f6\u540c\u5316\u5468\u671f\u5bfc\u81f4\u7684\u65f6\u95f4\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7a33\u5b9a\u7684\u9884\u6d4b\u3002", "method": "\u63d0\u51faFlowCast-ODE\u6846\u67b6\uff0c\u5c06\u5927\u6c14\u72b6\u6001\u6f14\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u6d41\uff1b\u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\uff1a\u5148\u57286\u5c0f\u65f6\u6570\u636e\u4e0a\u8bad\u7ec3\u52a8\u6001\u6d41\u5339\u914d\uff0c\u518d\u5728\u5c0f\u65f6\u6570\u636e\u4e0a\u4f7f\u7528ODE\u6c42\u89e3\u5668\u8fdb\u884c\u7cbe\u8c03\uff1b\u5f15\u5165\u8f7b\u91cf\u7ea7\u4f4e\u79e9AdaLN-Zero\u8c03\u5236\u673a\u5236\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u3002", "result": "\u6a21\u578b\u5728RMSE\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5177\u6709\u66f4\u597d\u7684\u80fd\u91cf\u5b88\u6052\u7279\u6027\uff0c\u51cf\u5c11\u6a21\u7cca\u5e76\u4fdd\u7559\u66f4\u591a\u7a7a\u95f4\u7ec6\u8282\uff1b\u5728\u53f0\u98ce\u7b49\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e0a\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff1b\u6709\u6548\u7f13\u89e3\u4e86\u540c\u5316\u5468\u671f\u8f6c\u6362\u5e26\u6765\u7684\u65f6\u95f4\u4e0d\u8fde\u7eed\u95ee\u9898\u3002", "conclusion": "FlowCast-ODE\u901a\u8fc7\u8fde\u7eed\u6d41\u5efa\u6a21\u548cODE\u6c42\u89e3\u5668\u7684\u7ed3\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5c0f\u65f6\u7ea7\u5929\u6c14\u9884\u62a5\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u7cbe\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u5929\u6c14\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14786", "abs": "https://arxiv.org/abs/2509.14786", "authors": ["Konwoo Kim", "Suhas Kotha", "Percy Liang", "Tatsunori Hashimoto"], "title": "Pre-training under infinite compute", "comment": null, "summary": "Since compute grows much faster than web text available for language model\npre-training, we ask how one should approach pre-training under fixed data and\nno compute constraints. We first show that existing data-constrained approaches\nof increasing epoch count and parameter count eventually overfit, and we\nsignificantly improve upon such recipes by properly tuning regularization,\nfinding that the optimal weight decay is $30\\times$ larger than standard\npractice. Since our regularized recipe monotonically decreases loss following a\nsimple power law in parameter count, we estimate its best possible performance\nvia the asymptote of its scaling law rather than the performance at a fixed\ncompute budget. We then identify that ensembling independently trained models\nachieves a significantly lower loss asymptote than the regularized recipe. Our\nbest intervention combining epoching, regularization, parameter scaling, and\nensemble scaling achieves an asymptote at 200M tokens using $5.17\\times$ less\ndata than our baseline, and our data scaling laws predict that this improvement\npersists at higher token budgets. We find that our data efficiency gains can be\nrealized at much smaller parameter counts as we can distill an ensemble into a\nstudent model that is 8$\\times$ smaller and retains $83\\%$ of the ensembling\nbenefit. Finally, our interventions designed for validation loss generalize to\ndownstream benchmarks, achieving a $9\\%$ improvement for pre-training evals and\na $17.5\\times$ data efficiency improvement over continued pre-training on math\nmid-training data. Our results show that simple algorithmic improvements can\nenable significantly more data-efficient pre-training in a compute-rich future.", "AI": {"tldr": "\u5728\u8ba1\u7b97\u8d44\u6e90\u4e30\u5bcc\u4f46\u8bad\u7ec3\u6570\u636e\u56fa\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4f18\u5316\u6b63\u5219\u5316\u3001\u53c2\u6570\u7f29\u653e\u3001\u96c6\u6210\u5b66\u4e60\u7b49\u7b80\u5355\u7b97\u6cd5\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u3002\u6700\u4f73\u65b9\u6848\u5728200M\u6807\u8bb0\u6570\u636e\u4e0a\u83b7\u5f975.17\u500d\u6570\u636e\u8282\u7701\uff0c\u5e76\u901a\u8fc7\u77e9\u9635\u7f29\u653e\u6280\u672f\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u4fdd\u7559\u4e8683%\u7684\u96c6\u6210\u6548\u679c\u3002", "motivation": "\u8ba1\u7b97\u8d44\u6e90\u7684\u589e\u957f\u901f\u5ea6\u8fdc\u8d85\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u53ef\u7528\u7684\u7f51\u7edc\u6587\u672c\u6570\u636e\uff0c\u9700\u8981\u7814\u7a76\u5728\u6570\u636e\u56fa\u5b9a\u4e14\u8ba1\u7b97\u8d44\u6e90\u65e0\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u8fdb\u884c\u6700\u4f73\u9884\u8bad\u7ec3\u3002", "method": "1. \u8bc6\u522b\u73b0\u6709\u589e\u52a0\u8bad\u7ec3\u6b21\u6570\u548c\u6a21\u578b\u53c2\u6570\u7684\u65b9\u6cd5\u4f1a\u8fc7\u62df\u5408\uff1b2. \u901a\u8fc7\u4f18\u5316\u6b63\u5219\u5316\uff08\u6743\u91cd\u8870\u51cf\u7cfb\u6570\u8c03\u6574\u4e3a\u6807\u51c6\u5b9e\u8df530\u500d\uff09\u6539\u8fdb\u6027\u80fd\uff1b3. \u91c7\u7528\u72ec\u7acb\u8bad\u7ec3\u6a21\u578b\u7684\u96c6\u6210\u5b66\u4e60\uff1b4. \u7ed3\u5408\u591a\u6b21\u8fed\u4ee3\u8bad\u7ec3\u3001\u6b63\u5219\u5316\u3001\u53c2\u6570\u7f29\u653e\u548c\u96c6\u6210\u7f29\u653e\u7b49\u6280\u672f\uff1b5. \u901a\u8fc7\u77e9\u9635\u7f29\u653e\u6280\u672f\u5c06\u96c6\u6210\u6a21\u578b\u77e5\u8bc6\u7cbe\u70bc\u5230\u5c0f\u578b\u6a21\u578b\u3002", "result": "1. \u6700\u4f73\u65b9\u6848\u5728200M\u6807\u8bb0\u6570\u636e\u4e0a\u5b9e\u73b05.17\u500d\u6570\u636e\u6548\u7387\u63d0\u5347\uff1b2. \u77e9\u9635\u7f29\u653e\u6280\u672f\u80fd\u5c06\u96c6\u6210\u6a21\u578b\u77e5\u8bc6\u7cbe\u70bc\u5230\u6bd48\u500d\u5c0f\u7684\u5b66\u751f\u6a21\u578b\uff0c\u4fdd\u755983%\u7684\u96c6\u6210\u6548\u679c\uff1b3. \u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u83b7\u5f979%\u6027\u80fd\u63d0\u5347\uff0c\u6570\u5b66\u4efb\u52a1\u4e0a\u83b7\u5f9717.5\u500d\u6570\u636e\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u7b80\u5355\u7684\u7b97\u6cd5\u6539\u8fdb\u80fd\u591f\u5728\u8ba1\u7b97\u8d44\u6e90\u4e30\u5bcc\u7684\u672a\u6765\u5b9e\u73b0\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65b9\u6848\u3002"}}
{"id": "2509.14863", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14863", "abs": "https://arxiv.org/abs/2509.14863", "authors": ["Zhengwei Wang", "Gang Wu"], "title": "Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study", "comment": null, "summary": "Graph Transformers (GTs) show considerable potential in graph representation\nlearning. The architecture of GTs typically integrates Graph Neural Networks\n(GNNs) with global attention mechanisms either in parallel or as a precursor to\nattention mechanisms, yielding a local-and-global or local-to-global attention\nscheme. However, as the global attention mechanism primarily captures\nlong-range dependencies between nodes, these integration schemes may suffer\nfrom information loss, where the local neighborhood information learned by GNN\ncould be diluted by the attention mechanism. Therefore, we propose G2LFormer,\nfeaturing a novel global-to-local attention scheme where the shallow network\nlayers use attention mechanisms to capture global information, while the deeper\nlayers employ GNN modules to learn local structural information, thereby\npreventing nodes from ignoring their immediate neighbors. An effective\ncross-layer information fusion strategy is introduced to allow local layers to\nretain beneficial information from global layers and alleviate information\nloss, with acceptable trade-offs in scalability. To validate the feasibility of\nthe global-to-local attention scheme, we compare G2LFormer with\nstate-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The\nresults indicate that G2LFormer exhibits excellent performance while keeping\nlinear complexity.", "AI": {"tldr": "G2LFormer\u662f\u4e00\u79cd\u65b0\u578b\u56feTransformer\uff0c\u91c7\u7528\u5168\u5c40\u5230\u5c40\u90e8\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6d45\u5c42\u7528\u6ce8\u610f\u529b\u6355\u83b7\u5168\u5c40\u4fe1\u606f\uff0c\u6df1\u5c42\u7528GNN\u5b66\u4e60\u5c40\u90e8\u7ed3\u6784\u4fe1\u606f\uff0c\u901a\u8fc7\u8de8\u5c42\u4fe1\u606f\u878d\u5408\u7b56\u7565\u9632\u6b62\u4fe1\u606f\u4e22\u5931\uff0c\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u56feTransformer\u5c06GNN\u4e0e\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u5e76\u884c\u6216\u987a\u5e8f\u96c6\u6210\uff0c\u53ef\u80fd\u5bfc\u81f4\u5c40\u90e8\u90bb\u57df\u4fe1\u606f\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u88ab\u7a00\u91ca\uff0c\u9020\u6210\u4fe1\u606f\u635f\u5931\u3002", "method": "\u63d0\u51fa\u5168\u5c40\u5230\u5c40\u90e8\u6ce8\u610f\u529b\u65b9\u6848\uff1a\u6d45\u5c42\u7f51\u7edc\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u5168\u5c40\u4fe1\u606f\uff0c\u6df1\u5c42\u7f51\u7edc\u4f7f\u7528GNN\u6a21\u5757\u5b66\u4e60\u5c40\u90e8\u7ed3\u6784\u4fe1\u606f\uff1b\u5f15\u5165\u8de8\u5c42\u4fe1\u606f\u878d\u5408\u7b56\u7565\uff0c\u8ba9\u5c40\u90e8\u5c42\u4fdd\u7559\u5168\u5c40\u5c42\u7684\u6709\u76ca\u4fe1\u606f\u3002", "result": "\u5728\u8282\u70b9\u7ea7\u548c\u56fe\u7ea7\u4efb\u52a1\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u7ebf\u6027\u56feTransformer\u548cGNN\u8fdb\u884c\u6bd4\u8f83\uff0cG2LFormer\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "conclusion": "\u5168\u5c40\u5230\u5c40\u90e8\u6ce8\u610f\u529b\u65b9\u6848\u662f\u53ef\u884c\u7684\uff0cG2LFormer\u80fd\u591f\u6709\u6548\u9632\u6b62\u8282\u70b9\u5ffd\u7565\u5176\u76f4\u63a5\u90bb\u5c45\uff0c\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.14597", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14597", "abs": "https://arxiv.org/abs/2509.14597", "authors": ["Seungjun Yi", "Joakim Nguyen", "Terence Lim", "Andrew Well", "Joseph Skrovan", "Mehak Beri", "YongGeon Lee", "Kavita Radhakrishnan", "Liu Leqi", "Mia Markey", "Ying Ding"], "title": "Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models", "comment": "Submitted to GenAI4Health@NeurIPS 2025", "summary": "This position paper examines how large language models (LLMs) can support\nthematic analysis of unstructured clinical transcripts, a widely used but\nresource-intensive method for uncovering patterns in patient and provider\nnarratives. We conducted a systematic review of recent studies applying LLMs to\nthematic analysis, complemented by an interview with a practicing clinician.\nOur findings reveal that current approaches remain fragmented across multiple\ndimensions including types of thematic analysis, datasets, prompting strategies\nand models used, most notably in evaluation. Existing evaluation methods vary\nwidely (from qualitative expert review to automatic similarity metrics),\nhindering progress and preventing meaningful benchmarking across studies. We\nargue that establishing standardized evaluation practices is critical for\nadvancing the field. To this end, we propose an evaluation framework centered\non three dimensions: validity, reliability, and interpretability.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u8f6c\u5f55\u672c\u4e3b\u9898\u5206\u6790\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u53d1\u73b0\u5f53\u524d\u65b9\u6cd5\u5728\u5206\u6790\u7c7b\u578b\u3001\u6570\u636e\u96c6\u3001\u63d0\u793a\u7b56\u7565\u548c\u8bc4\u4f30\u7b49\u65b9\u9762\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4ee5\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e3a\u6838\u5fc3\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u4e34\u5e8a\u8f6c\u5f55\u672c\u7684\u4e3b\u9898\u5206\u6790\u662f\u8d44\u6e90\u5bc6\u96c6\u578b\u65b9\u6cd5\uff0c\u9700\u8981\u63a2\u7d22LLMs\u5982\u4f55\u652f\u6301\u8fd9\u4e00\u8fc7\u7a0b\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u5206\u6790\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u8fd1\u671f\u5c06LLMs\u5e94\u7528\u4e8e\u4e3b\u9898\u5206\u6790\u7684\u7814\u7a76\uff0c\u5e76\u8f85\u4ee5\u4e34\u5e8a\u533b\u751f\u8bbf\u8c08\uff0c\u5206\u6790\u5f53\u524d\u65b9\u6cd5\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5b58\u5728\u788e\u7247\u5316\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u65b9\u6cd5\u5dee\u5f02\u5f88\u5927\uff08\u4ece\u5b9a\u6027\u4e13\u5bb6\u8bc4\u5ba1\u5230\u81ea\u52a8\u76f8\u4f3c\u6027\u6307\u6807\uff09\uff0c\u963b\u788d\u4e86\u9886\u57df\u8fdb\u5c55\u548c\u8de8\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u5b9e\u8df5\u5bf9\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u4e86\u4ee5\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e09\u4e2a\u7ef4\u5ea6\u4e3a\u6838\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2509.14954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14954", "abs": "https://arxiv.org/abs/2509.14954", "authors": ["Xingchen Xu", "Ao Li", "Benjamin Ward-Cherrier"], "title": "Exploratory Movement Strategies for Texture Discrimination with a Neuromorphic Tactile Sensor", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems 2025. Please cite the proceedings version", "summary": "We propose a neuromorphic tactile sensing framework for robotic texture\nclassification that is inspired by human exploratory strategies. Our system\nutilizes the NeuroTac sensor to capture neuromorphic tactile data during a\nseries of exploratory motions. We first tested six distinct motions for texture\nclassification under fixed environment: sliding, rotating, tapping, as well as\nthe combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.\nWe chose sliding and sliding+rotating as the best motions based on final\naccuracy and the sample timing length needed to reach converged accuracy. In\nthe second experiment designed to simulate complex real-world conditions, these\ntwo motions were further evaluated under varying contact depth and speeds.\nUnder these conditions, our framework attained the highest accuracy of 87.33\\%\nwith sliding+rotating while maintaining an extremely low power consumption of\nonly 8.04 mW. These results suggest that the sliding+rotating motion is the\noptimal exploratory strategy for neuromorphic tactile sensing deployment in\ntexture classification tasks and holds significant promise for enhancing\nrobotic environmental interaction.", "AI": {"tldr": "\u63d0\u51fa\u53d7\u4eba\u7c7b\u63a2\u7d22\u7b56\u7565\u542f\u53d1\u7684\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7NeuroTac\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff0c\u6d4b\u8bd5\u591a\u79cd\u63a2\u7d22\u52a8\u4f5c\uff0c\u53d1\u73b0\u6ed1\u52a8+\u65cb\u8f6c\u7ec4\u5408\u52a8\u4f5c\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8fbe\u523087.33%\u51c6\u786e\u7387\u4e14\u529f\u8017\u4ec58.04mW", "motivation": "\u53d7\u4eba\u7c7b\u63a2\u7d22\u7b56\u7565\u542f\u53d1\uff0c\u5f00\u53d1\u673a\u5668\u4eba\u7eb9\u7406\u5206\u7c7b\u7684\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u80fd\u529b", "method": "\u4f7f\u7528NeuroTac\u4f20\u611f\u5668\u91c7\u96c6\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u6570\u636e\uff0c\u6d4b\u8bd5\u516d\u79cd\u63a2\u7d22\u52a8\u4f5c\uff08\u6ed1\u52a8\u3001\u65cb\u8f6c\u3001\u6572\u51fb\u53ca\u5176\u7ec4\u5408\uff09\uff0c\u5728\u56fa\u5b9a\u73af\u5883\u548c\u53d8\u5316\u6761\u4ef6\u4e0b\u8bc4\u4f30\u6027\u80fd", "result": "\u6ed1\u52a8+\u65cb\u8f6c\u7ec4\u5408\u52a8\u4f5c\u5728\u53d8\u5316\u63a5\u89e6\u6df1\u5ea6\u548c\u901f\u5ea6\u6761\u4ef6\u4e0b\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u738787.33%\uff0c\u529f\u8017\u6781\u4f4e\u4ec58.04mW", "conclusion": "\u6ed1\u52a8+\u65cb\u8f6c\u662f\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u5728\u7eb9\u7406\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6700\u4f18\u63a2\u7d22\u7b56\u7565\uff0c\u5bf9\u589e\u5f3a\u673a\u5668\u4eba\u73af\u5883\u4ea4\u4e92\u5177\u6709\u91cd\u8981\u524d\u666f"}}
{"id": "2509.14936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14936", "abs": "https://arxiv.org/abs/2509.14936", "authors": ["Rohan Veit", "Michael Lones"], "title": "A Comparative Analysis of Transformer Models in Social Bot Detection", "comment": "To appear in proceedings of UKCI 2025", "summary": "Social media has become a key medium of communication in today's society.\nThis realisation has led to many parties employing artificial users (or bots)\nto mislead others into believing untruths or acting in a beneficial manner to\nsuch parties. Sophisticated text generation tools, such as large language\nmodels, have further exacerbated this issue. This paper aims to compare the\neffectiveness of bot detection models based on encoder and decoder\ntransformers. Pipelines are developed to evaluate the performance of these\nclassifiers, revealing that encoder-based classifiers demonstrate greater\naccuracy and robustness. However, decoder-based models showed greater\nadaptability through task-specific alignment, suggesting more potential for\ngeneralisation across different use cases in addition to superior observa.\nThese findings contribute to the ongoing effort to prevent digital environments\nbeing manipulated while protecting the integrity of online discussion.", "AI": {"tldr": "\u6bd4\u8f83\u57fa\u4e8e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u53d8\u6362\u5668\u7684\u673a\u5668\u4eba\u68c0\u6d4b\u6a21\u578b\u6548\u679c\uff0c\u53d1\u73b0\u7f16\u7801\u5668\u6a21\u578b\u66f4\u51c6\u786e\u9c81\u68d2\uff0c\u89e3\u7801\u5668\u6a21\u578b\u66f4\u5177\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6f5c\u529b", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u4eba\u5de5\u667a\u80fd\u7528\u6237\uff08\u673a\u5668\u4eba\uff09\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7b49\u5148\u8fdb\u6587\u672c\u751f\u6210\u5de5\u5177\u8bef\u5bfc\u4ed6\u4eba\uff0c\u9700\u8981\u6709\u6548\u68c0\u6d4b\u65b9\u6cd5\u6765\u4fdd\u62a4\u5728\u7ebf\u8ba8\u8bba\u7684\u5b8c\u6574\u6027", "method": "\u5f00\u53d1\u8bc4\u4f30\u7ba1\u9053\u6bd4\u8f83\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u53d8\u6362\u5668\u5206\u7c7b\u5668\u7684\u6027\u80fd", "result": "\u7f16\u7801\u5668\u5206\u7c7b\u5668\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u89e3\u7801\u5668\u6a21\u578b\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5bf9\u9f50\u663e\u793a\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6f5c\u529b", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u9632\u6b62\u6570\u5b57\u73af\u5883\u88ab\u64cd\u7eb5\uff0c\u4fdd\u62a4\u5728\u7ebf\u8ba8\u8bba\u7684\u5b8c\u6574\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u68c0\u6d4b\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc"}}
{"id": "2509.14966", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14966", "abs": "https://arxiv.org/abs/2509.14966", "authors": ["Xingwu Zhang", "Guanxuan Li", "Zhuocheng Zhang", "Zijun Long"], "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching", "comment": null, "summary": "The rapidly growing number of product categories in large-scale e-commerce\nmakes accurate object identification for automated packing in warehouses\nsubstantially more difficult. As the catalog grows, intra-class variability and\na long tail of rare or visually similar items increase, and when combined with\ndiverse packaging, cluttered containers, frequent occlusion, and large\nviewpoint changes-these factors amplify discrepancies between query and\nreference images, causing sharp performance drops for methods that rely solely\non 2D appearance features. Thus, we propose RoboEye, a two-stage identification\nframework that dynamically augments 2D semantic features with domain-adapted 3D\nreasoning and lightweight adapters to bridge training deployment gaps. In the\nfirst stage, we train a large vision model to extract 2D features for\ngenerating candidate rankings. A lightweight 3D-feature-awareness module then\nestimates 3D feature quality and predicts whether 3D re-ranking is necessary,\npreventing performance degradation and avoiding unnecessary computation. When\ninvoked, the second stage uses our robot 3D retrieval transformer, comprising a\n3D feature extractor that produces geometry-aware dense features and a\nkeypoint-based matcher that computes keypoint-correspondence confidences\nbetween query and reference images instead of conventional cosine-similarity\nscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior\nstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,\navoiding reliance on explicit 3D inputs and reducing deployment costs. The code\nused in this paper is publicly available at:\nhttps://github.com/longkukuhi/RoboEye.", "AI": {"tldr": "RoboEye\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7269\u4f53\u8bc6\u522b\u6846\u67b6\uff0c\u7ed3\u54082D\u8bed\u4e49\u7279\u5f81\u548c3D\u63a8\u7406\uff0c\u7528\u4e8e\u7535\u5546\u4ed3\u5e93\u81ea\u52a8\u5316\u5305\u88c5\u4e2d\u7684\u7269\u4f53\u8bc6\u522b\uff0c\u5728\u4ec5\u4f7f\u7528RGB\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u7535\u5546\u4ea7\u54c1\u7c7b\u522b\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u4ed3\u5e93\u81ea\u52a8\u5316\u5305\u88c5\u4e2d\u7684\u7269\u4f53\u8bc6\u522b\u66f4\u52a0\u56f0\u96be\uff0c\u4f20\u7edf\u4ec5\u4f9d\u8d562D\u5916\u89c2\u7279\u5f81\u7684\u65b9\u6cd5\u5728\u7c7b\u5185\u53d8\u5f02\u3001\u89c6\u89c9\u76f8\u4f3c\u7269\u54c1\u3001\u906e\u6321\u548c\u89c6\u89d2\u53d8\u5316\u7b49\u60c5\u51b5\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "method": "\u4e24\u9636\u6bb5\u8bc6\u522b\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5927\u89c6\u89c9\u6a21\u578b\u63d0\u53d62D\u7279\u5f81\u751f\u6210\u5019\u9009\u6392\u540d\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8f7b\u91cf\u7ea73D\u7279\u5f81\u611f\u77e5\u6a21\u5757\u5224\u65ad\u662f\u5426\u9700\u89813D\u91cd\u6392\u5e8f\uff0c\u4f7f\u7528\u673a\u5668\u4eba3D\u68c0\u7d22\u53d8\u6362\u5668\u8fdb\u884c\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRoboEye\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5(RoboLLM)\u5728Recall@1\u6307\u6807\u4e0a\u63d0\u5347\u4e867.1%\uff0c\u4e14\u4ec5\u4f7f\u7528RGB\u56fe\u50cf\uff0c\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u3002", "conclusion": "RoboEye\u901a\u8fc7\u52a8\u6001\u7ed3\u54082D\u8bed\u4e49\u7279\u5f81\u548c\u9886\u57df\u9002\u5e94\u76843D\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7535\u5546\u73af\u5883\u4e0b\u7684\u7269\u4f53\u8bc6\u522b\u6311\u6218\uff0c\u5728\u6027\u80fd\u548c\u90e8\u7f72\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.14651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14651", "abs": "https://arxiv.org/abs/2509.14651", "authors": ["Siyu Yan", "Long Zeng", "Xuecheng Wu", "Chengcheng Han", "Kongcheng Zhang", "Chong Peng", "Xuezhi Cao", "Xunliang Cai", "Chenjuan Guo"], "title": "MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models", "comment": "EMNLP 2025 main conference", "summary": "As large language models~(LLMs) become widely adopted, ensuring their\nalignment with human values is crucial to prevent jailbreaks where adversaries\nmanipulate models to produce harmful content. While most defenses target\nsingle-turn attacks, real-world usage often involves multi-turn dialogues,\nexposing models to attacks that exploit conversational context to bypass safety\nmeasures. We introduce MUSE, a comprehensive framework tackling multi-turn\njailbreaks from both attack and defense angles. For attacks, we propose MUSE-A,\na method that uses frame semantics and heuristic tree search to explore diverse\nsemantic trajectories. For defense, we present MUSE-D, a fine-grained safety\nalignment approach that intervenes early in dialogues to reduce\nvulnerabilities. Extensive experiments on various models show that MUSE\neffectively identifies and mitigates multi-turn vulnerabilities. Code is\navailable at\n\\href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.", "AI": {"tldr": "MUSE\u6846\u67b6\u4ece\u653b\u51fb\u548c\u9632\u5fa1\u4e24\u4e2a\u89d2\u5ea6\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8d8a\u72f1\u95ee\u9898\uff0cMUSE-A\u5229\u7528\u6846\u67b6\u8bed\u4e49\u548c\u542f\u53d1\u5f0f\u6811\u641c\u7d22\u8fdb\u884c\u653b\u51fb\uff0cMUSE-D\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b89\u5168\u5bf9\u9f50\u8fdb\u884c\u65e9\u671f\u9632\u5fa1", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u91c7\u7528\uff0c\u9700\u8981\u786e\u4fdd\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u4ee5\u9632\u6b62\u8d8a\u72f1\u653b\u51fb\u3002\u73b0\u6709\u9632\u5fa1\u4e3b\u8981\u9488\u5bf9\u5355\u8f6e\u653b\u51fb\uff0c\u4f46\u73b0\u5b9e\u4f7f\u7528\u4e2d\u591a\u8f6e\u5bf9\u8bdd\u5bb9\u6613\u8ba9\u653b\u51fb\u8005\u5229\u7528\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7ed5\u8fc7\u5b89\u5168\u63aa\u65bd", "method": "\u63d0\u51faMUSE\u6846\u67b6\uff1a1\uff09MUSE-A\u653b\u51fb\u65b9\u6cd5\uff1a\u4f7f\u7528\u6846\u67b6\u8bed\u4e49\u548c\u542f\u53d1\u5f0f\u6811\u641c\u7d22\u63a2\u7d22\u591a\u6837\u5316\u8bed\u4e49\u8f68\u8ff9\uff1b2\uff09MUSE-D\u9632\u5fa1\u65b9\u6cd5\uff1a\u7ec6\u7c92\u5ea6\u5b89\u5168\u5bf9\u9f50\uff0c\u5728\u5bf9\u8bdd\u65e9\u671f\u8fdb\u884c\u5e72\u9884", "result": "\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMUSE\u80fd\u6709\u6548\u8bc6\u522b\u548c\u7f13\u89e3\u591a\u8f6e\u6f0f\u6d1e", "conclusion": "MUSE\u6846\u67b6\u4e3a\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u653b\u51fb\u548c\u9632\u5fa1\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2509.14989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14989", "abs": "https://arxiv.org/abs/2509.14989", "authors": ["Benedikt Kolbeinsson", "Krystian Mikolajczyk"], "title": "UCorr: Wire Detection and Depth Estimation for Autonomous Drones", "comment": "Published in Proceedings of the 4th International Conference on\n  Robotics, Computer Vision and Intelligent Systems (ROBOVIS), 2024", "summary": "In the realm of fully autonomous drones, the accurate detection of obstacles\nis paramount to ensure safe navigation and prevent collisions. Among these\nchallenges, the detection of wires stands out due to their slender profile,\nwhich poses a unique and intricate problem. To address this issue, we present\nan innovative solution in the form of a monocular end-to-end model for wire\nsegmentation and depth estimation. Our approach leverages a temporal\ncorrelation layer trained on synthetic data, providing the model with the\nability to effectively tackle the complex joint task of wire detection and\ndepth estimation. We demonstrate the superiority of our proposed method over\nexisting competitive approaches in the joint task of wire detection and depth\nestimation. Our results underscore the potential of our model to enhance the\nsafety and precision of autonomous drones, shedding light on its promising\napplications in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u7ebf\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u7684\u5355\u76ee\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u5c42\u548c\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u7535\u7ebf\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1\u8054\u5408\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5168\u81ea\u4e3b\u65e0\u4eba\u673a\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u969c\u788d\u7269\u4ee5\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\uff0c\u7535\u7ebf\u56e0\u5176\u7ec6\u957f\u8f6e\u5ed3\u800c\u6210\u4e3a\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u68c0\u6d4b\u76ee\u6807", "method": "\u4f7f\u7528\u5355\u76ee\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u95f4\u76f8\u5173\u5c42\u5e76\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u7535\u7ebf\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u7684\u8054\u5408\u4efb\u52a1", "result": "\u5728\u7535\u7ebf\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1\u8054\u5408\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u7ade\u4e89\u65b9\u6cd5\u7684\u6027\u80fd", "conclusion": "\u8be5\u6a21\u578b\u6709\u6f5c\u529b\u63d0\u5347\u81ea\u4e3b\u65e0\u4eba\u673a\u7684\u5b89\u5168\u6027\u548c\u7cbe\u786e\u6027\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f"}}
{"id": "2509.15153", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15153", "abs": "https://arxiv.org/abs/2509.15153", "authors": ["Yating Lin", "Zixuan Huang", "Fan Yang", "Dmitry Berenson"], "title": "AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use", "comment": null, "summary": "Multivariate time-series anomaly detection, which is critical for identifying\nunexpected events, has been explored in the field of machine learning for\nseveral decades. However, directly applying these methods to data from forceful\ntool use tasks is challenging because streaming sensor data in the real world\ntends to be inherently noisy, exhibits non-stationary behavior, and varies\nacross different tasks and tools. To address these challenges, we propose a\nmethod, AnoF-Diff, based on the diffusion model to extract force-torque\nfeatures from time-series data and use force-torque features to detect\nanomalies. We compare our method with other state-of-the-art methods in terms\nof F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)\non four forceful tool-use tasks, demonstrating that our method has better\nperformance and is more robust to a noisy dataset. We also propose the method\nof parallel anomaly score evaluation based on one-step diffusion and\ndemonstrate how our method can be used for online anomaly detection in several\nforceful tool use experiments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684AnoF-Diff\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63d0\u53d6\u529b-\u626d\u77e9\u7279\u5f81\u5e76\u68c0\u6d4b\u5f02\u5e38\uff0c\u5728\u5608\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u5f3a\u529b\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\uff0c\u56e0\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u4f20\u611f\u5668\u6570\u636e\u5177\u6709\u566a\u58f0\u5927\u3001\u975e\u5e73\u7a33\u4e14\u4efb\u52a1\u95f4\u5dee\u5f02\u5927\u7684\u7279\u70b9\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u63d0\u53d6\u529b-\u626d\u77e9\u7279\u5f81\uff0c\u91c7\u7528\u5e76\u884c\u5f02\u5e38\u8bc4\u5206\u8bc4\u4f30\u65b9\u6cd5\uff0c\u652f\u6301\u4e00\u6b65\u6269\u6563\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u5f3a\u529b\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e0a\uff0cF1\u5206\u6570\u548cAUROC\u6307\u6807\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5bf9\u566a\u58f0\u6570\u636e\u96c6\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "AnoF-Diff\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u529b\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u4e3a\u5728\u7ebf\u5b9e\u65f6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.14797", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14797", "abs": "https://arxiv.org/abs/2509.14797", "authors": ["Alba Maria Marmol-Romero", "Flor Miriam Plaza-del-Arco", "Arturo Montejo-Raez"], "title": "SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural Language Processing", "comment": "9 pages, 2 figures, 4 tables. CLEF (Working Notes). 2023", "summary": "This paper describes the participation of the SINAI team in the eRisk@CLEF\nlab. Specifically, one of the proposed tasks has been addressed: Task 2 on the\nearly detection of signs of pathological gambling. The approach presented in\nTask 2 is based on pre-trained models from Transformers architecture with\ncomprehensive preprocessing data and data balancing techniques. Moreover, we\nintegrate Long-short Term Memory (LSTM) architecture with automodels from\nTransformers. In this Task, our team has been ranked in seventh position, with\nan F1 score of 0.126, out of 49 participant submissions and achieves the\nhighest values in recall metrics and metrics related to early detection.", "AI": {"tldr": "SINAI\u56e2\u961f\u5728eRisk@CLEF\u5b9e\u9a8c\u5ba4\u4efb\u52a12\u4e2d\uff0c\u4f7f\u7528Transformer\u9884\u8bad\u7ec3\u6a21\u578b\u7ed3\u5408LSTM\u67b6\u6784\u8fdb\u884c\u75c5\u7406\u6027\u8d4c\u535a\u65e9\u671f\u68c0\u6d4b\uff0c\u572849\u4e2a\u53c2\u8d5b\u63d0\u4ea4\u4e2d\u6392\u540d\u7b2c7\uff0cF1\u5206\u65700.126\uff0c\u4f46\u5728\u53ec\u56de\u7387\u548c\u65e9\u671f\u68c0\u6d4b\u76f8\u5173\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73", "motivation": "\u53c2\u4e0eeRisk@CLEF\u5b9e\u9a8c\u5ba4\u7684\u4efb\u52a12\uff0c\u4e13\u6ce8\u4e8e\u75c5\u7406\u6027\u8d4c\u535a\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898", "method": "\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u5408\u5168\u9762\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u6570\u636e\u5e73\u8861\u6280\u672f\uff0c\u5e76\u96c6\u6210LSTM\u67b6\u6784\u4e0eTransformer\u7684automodels", "result": "\u572849\u4e2a\u53c2\u8d5b\u63d0\u4ea4\u4e2d\u6392\u540d\u7b2c7\u4f4d\uff0cF1\u5206\u6570\u4e3a0.126\uff0c\u4f46\u5728\u53ec\u56de\u7387\u6307\u6807\u548c\u65e9\u671f\u68c0\u6d4b\u76f8\u5173\u6307\u6807\u4e0a\u83b7\u5f97\u4e86\u6700\u9ad8\u503c", "conclusion": "\u867d\u7136\u6574\u4f53\u6392\u540d\u4e2d\u7b49\uff0c\u4f46\u5728\u5173\u952e\u7684\u65e9\u671f\u68c0\u6d4b\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u75c5\u7406\u6027\u8d4c\u535a\u65e9\u671f\u8ff9\u8c61\u65b9\u9762\u7684\u6709\u6548\u6027"}}
{"id": "2509.15096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15096", "abs": "https://arxiv.org/abs/2509.15096", "authors": ["Bo-Wen Yin", "Jiao-Long Cao", "Xuying Zhang", "Yuming Chen", "Ming-Ming Cheng", "Qibin Hou"], "title": "OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation", "comment": "Accepted by NeurIPS 2025", "summary": "Recent research on representation learning has proved the merits of\nmulti-modal clues for robust semantic segmentation. Nevertheless, a flexible\npretrain-and-finetune pipeline for multiple visual modalities remains\nunexplored. In this paper, we propose a novel multi-modal learning framework,\ntermed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, we\nassemble a large-scale dataset for multi-modal pretraining, called ImageNeXt,\nwhich contains five popular visual modalities. 2) We provide an efficient\npretraining manner to endow the model with the capacity to encode different\nmodality information in the ImageNeXt. For the first time, we introduce a\nuniversal multi-modal pretraining framework that consistently amplifies the\nmodel's perceptual capabilities across various scenarios, regardless of the\narbitrary combination of the involved modalities. Remarkably, our OmniSegmentor\nachieves new state-of-the-art records on a wide range of multi-modal semantic\nsegmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER,\nSUNRGBD, and KITTI-360.", "AI": {"tldr": "\u63d0\u51fa\u4e86OmniSegmentor\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542bImageNeXt\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u9ad8\u6548\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u7814\u7a76\u8bc1\u660e\u591a\u6a21\u6001\u7ebf\u7d22\u5bf9\u9c81\u68d2\u8bed\u4e49\u5206\u5272\u6709\u76ca\uff0c\u4f46\u7f3a\u4e4f\u7075\u6d3b\u7684\u591a\u89c6\u89c9\u6a21\u6001\u9884\u8bad\u7ec3-\u5fae\u8c03\u6d41\u7a0b", "method": "1) \u57fa\u4e8eImageNet\u6784\u5efa\u5305\u542b5\u79cd\u89c6\u89c9\u6a21\u6001\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6ImageNeXt\uff1b2) \u8bbe\u8ba1\u9ad8\u6548\u9884\u8bad\u7ec3\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u7f16\u7801\u4e0d\u540c\u6a21\u6001\u4fe1\u606f", "result": "\u5728NYU Depthv2\u3001EventScape\u3001MFNet\u3001DeLiVER\u3001SUNRGBD\u3001KITTI-360\u7b49\u591a\u4e2a\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "\u9996\u6b21\u63d0\u51fa\u4e86\u901a\u7528\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u65e0\u8bba\u6d89\u53ca\u6a21\u6001\u7684\u4efb\u610f\u7ec4\u5408\uff0c\u90fd\u80fd\u6301\u7eed\u589e\u5f3a\u6a21\u578b\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u611f\u77e5\u80fd\u529b"}}
{"id": "2509.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14900", "abs": "https://arxiv.org/abs/2509.14900", "authors": ["Jiayi Han", "Liang Du", "Yinda Chen", "Xiao Kang", "Weiyang Ding", "Donghong Han"], "title": "FURINA: Free from Unmergeable Router via LINear Aggregation of mixed experts", "comment": "15 pages, 4 figures", "summary": "The Mixture of Experts (MoE) paradigm has been successfully integrated into\nLow-Rank Adaptation (LoRA) for parameter-efficient fine-tuning (PEFT),\ndelivering performance gains with minimal parameter overhead. However, a key\nlimitation of existing MoE-LoRA methods is their reliance on a discrete router,\nwhich prevents the integration of the MoE components into the backbone model.\nTo overcome this, we propose FURINA, a novel Free from Unmergeable Router\nframework based on the LINear Aggregation of experts. FURINA eliminates the\nrouter by introducing a Self-Routing mechanism. This is achieved through three\ncore innovations: (1) decoupled learning of the direction and magnitude for\nLoRA adapters, (2) a shared learnable magnitude vector for consistent\nactivation scaling, and (3) expert selection loss that encourages divergent\nexpert activation. The proposed mechanism leverages the angular similarity\nbetween the input and each adapter's directional component to activate experts,\nwhich are then scaled by the shared magnitude vector. This design allows the\noutput norm to naturally reflect the importance of each expert, thereby\nenabling dynamic, router-free routing. The expert selection loss further\nsharpens this behavior by encouraging sparsity and aligning it with standard\nMoE activation patterns. We also introduce a shared expert within the MoE-LoRA\nblock that provides stable, foundational knowledge. To the best of our\nknowledge, FURINA is the first router-free, MoE-enhanced LoRA method that can\nbe fully merged into the backbone model, introducing zero additional\ninference-time cost or complexity. Extensive experiments demonstrate that\nFURINA not only significantly outperforms standard LoRA but also matches or\nsurpasses the performance of existing MoE-LoRA methods, while eliminating the\nextra inference-time overhead of MoE.", "AI": {"tldr": "FURINA\u662f\u4e00\u79cd\u65e0\u9700\u8def\u7531\u5668\u7684MoE-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u805a\u5408\u4e13\u5bb6\u5b9e\u73b0\u52a8\u6001\u8def\u7531\uff0c\u53ef\u5b8c\u5168\u5408\u5e76\u5230\u4e3b\u5e72\u6a21\u578b\u4e2d\uff0c\u96f6\u989d\u5916\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709MoE-LoRA\u65b9\u6cd5\u4f9d\u8d56\u79bb\u6563\u8def\u7531\u5668\uff0c\u65e0\u6cd5\u5c06MoE\u7ec4\u4ef6\u96c6\u6210\u5230\u4e3b\u5e72\u6a21\u578b\u4e2d\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u81ea\u8def\u7531\u673a\u5236\uff1a1)\u89e3\u8026LoRA\u9002\u914d\u5668\u7684\u65b9\u5411\u548c\u5e45\u5ea6\u5b66\u4e60\uff1b2)\u5171\u4eab\u53ef\u5b66\u4e60\u5e45\u5ea6\u5411\u91cf\uff1b3)\u4e13\u5bb6\u9009\u62e9\u635f\u5931\u4fc3\u8fdb\u4e13\u5bb6\u6fc0\u6d3b\u5206\u5316\uff1b\u5229\u7528\u8f93\u5165\u4e0e\u9002\u914d\u5668\u65b9\u5411\u5206\u91cf\u7684\u89d2\u5ea6\u76f8\u4f3c\u6027\u6fc0\u6d3b\u4e13\u5bb6\u3002", "result": "FURINA\u663e\u8457\u4f18\u4e8e\u6807\u51c6LoRA\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709MoE-LoRA\u65b9\u6cd5\u6027\u80fd\uff0c\u540c\u65f6\u6d88\u9664MoE\u7684\u989d\u5916\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "FURINA\u662f\u9996\u4e2a\u53ef\u5b8c\u5168\u5408\u5e76\u5230\u4e3b\u5e72\u6a21\u578b\u7684\u65e0\u8def\u7531\u5668MoE\u589e\u5f3aLoRA\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u96f6\u989d\u5916\u63a8\u7406\u6210\u672c\u7684\u9ad8\u6027\u80fd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002"}}
{"id": "2509.15113", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15113", "abs": "https://arxiv.org/abs/2509.15113", "authors": ["Andrei Chertkov", "Artem Basharin", "Mikhail Saygin", "Evgeny Frolov", "Stanislav Straupe", "Ivan Oseledets"], "title": "Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers", "comment": null, "summary": "The growing demand for energy-efficient, high-performance AI systems has led\nto increased attention on alternative computing platforms (e.g., photonic,\nneuromorphic) due to their potential to accelerate learning and inference.\nHowever, integrating such physical components into deep learning pipelines\nremains challenging, as physical devices often offer limited expressiveness,\nand their non-differentiable nature renders on-device backpropagation difficult\nor infeasible. This motivates the development of hybrid architectures that\ncombine digital neural networks with reconfigurable physical layers, which\neffectively behave as black boxes. In this work, we present a framework for the\nend-to-end training of such hybrid networks. This framework integrates\nstochastic zeroth-order optimization for updating the physical layer's internal\nparameters with a dynamic low-rank surrogate model that enables gradient\npropagation through the physical layer. A key component of our approach is the\nimplicit projector-splitting integrator algorithm, which updates the\nlightweight surrogate model after each forward pass with minimal hardware\nqueries, thereby avoiding costly full matrix reconstruction. We demonstrate our\nmethod across diverse deep learning tasks, including: computer vision, audio\nclassification, and language modeling. Notably, across all modalities, the\nproposed approach achieves near-digital baseline accuracy and consistently\nenables effective end-to-end training of hybrid models incorporating various\nnon-differentiable physical components (spatial light modulators, microring\nresonators, and Mach-Zehnder interferometers). This work bridges hardware-aware\ndeep learning and gradient-free optimization, thereby offering a practical\npathway for integrating non-differentiable physical components into scalable,\nend-to-end trainable AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u96f6\u9636\u4f18\u5316\u548c\u52a8\u6001\u4f4e\u79e9\u4ee3\u7406\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8fde\u63a5\u975e\u53ef\u5fae\u7269\u7406\u8bbe\u5907\u4e0e\u6570\u5b57\u795e\u7ecf\u7f51\u7edc\u7684\u7aef\u5230\u7aef\u8bad\u7ec3", "motivation": "\u89e3\u51b3\u7269\u7406\u8bbe\u5907\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u6d41\u6c34\u7ebf\u7684\u6311\u6218\uff0c\u56e0\u7269\u7406\u8bbe\u5907\u5f80\u5f80\u8868\u8fbe\u529b\u6709\u9650\u4e14\u975e\u53ef\u5fae\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5728\u8bbe\u5907\u4e0a\u8fdb\u884c\u53cd\u5411\u4f20\u64ad", "method": "\u4f7f\u7528\u968f\u673a\u96f6\u9636\u4f18\u5316\u66f4\u65b0\u7269\u7406\u5c42\u53c2\u6570\uff0c\u901a\u8fc7\u52a8\u6001\u4f4e\u79e9\u4ee3\u7406\u6a21\u578b\u5b9e\u73b0\u68af\u5ea6\u4f20\u64ad\uff0c\u91c7\u7528\u9690\u5f0f\u6295\u5f71\u5668\u5206\u89e3\u79ef\u5206\u7b97\u6cd5\u6700\u5c0f\u5316\u786c\u4ef6\u67e5\u8be2\u6210\u672c", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u97f3\u9891\u5206\u7c7b\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u90fd\u8fbe\u5230\u63a5\u8fd1\u6570\u5b57\u57fa\u7ebf\u7684\u51c6\u786e\u6027\uff0c\u6210\u529f\u8bad\u7ec3\u5305\u542b\u5404\u79cd\u975e\u53ef\u5fae\u7269\u7406\u7ec4\u4ef6\u7684\u6df7\u5408\u6a21\u578b", "conclusion": "\u8be5\u5de5\u4f5c\u6865\u63a5\u4e86\u786c\u4ef6\u611f\u77e5\u6df1\u5ea6\u5b66\u4e60\u4e0e\u65e0\u68af\u5ea6\u4f18\u5316\uff0c\u4e3a\u96c6\u6210\u975e\u53ef\u5fae\u7269\u7406\u7ec4\u4ef6\u5230\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84"}}
{"id": "2509.15224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15224", "abs": "https://arxiv.org/abs/2509.15224", "authors": ["Luca Bartolomei", "Enrico Mannocci", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia"], "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation", "comment": "ICCV 2025. Code: https://github.com/bartn8/depthanyevent/ Project\n  Page: https://bartn8.github.io/depthanyevent/", "summary": "Event cameras capture sparse, high-temporal-resolution visual information,\nmaking them particularly suitable for challenging environments with high-speed\nmotion and strongly varying lighting conditions. However, the lack of large\ndatasets with dense ground-truth depth annotations hinders learning-based\nmonocular depth estimation from event data. To address this limitation, we\npropose a cross-modal distillation paradigm to generate dense proxy labels\nleveraging a Vision Foundation Model (VFM). Our strategy requires an event\nstream spatially aligned with RGB frames, a simple setup even available\noff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,\nwe propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),\nor deriving from it a novel recurrent architecture to infer depth from\nmonocular event cameras. We evaluate our approach with synthetic and real-world\ndatasets, demonstrating that i) our cross-modal paradigm achieves competitive\nperformance compared to fully supervised methods without requiring expensive\ndepth annotations, and ii) our VFM-based models achieve state-of-the-art\nperformance.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u6a21\u6001\u840e\u53d6\u6280\u672f\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e3a\u4e8b\u4ef6\u76f8\u673a\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u4ee3\u7406\u6807\u7b7e\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6602\u8d35\u6df1\u5ea6\u6ce8\u91ca\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u9ad8\u901f\u8fd0\u52a8\u548c\u5149\u7167\u53d8\u5316\u73af\u5883\u4e2d\u4f18\u52bf\u663e\u8457\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u5e26\u6709\u5bc6\u96c6\u6df1\u5ea6\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u9650\u5236\u4e86\u5b66\u4e60\u57fa\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u840e\u53d6\u8303\u5f0f\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u751f\u6210\u5bc6\u96c6\u4ee3\u7406\u6807\u7b7e\u3002\u91c7\u7528\u4e8b\u4ef6\u6d41\u4e0eRGB\u5e27\u5bf9\u9f50\u7684\u7b80\u5355\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u6a21\u578b\uff1a\u666e\u901aDepth Anything v2\u6a21\u578b\u548c\u65b0\u7684\u5faa\u73af\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0ci)\u8de8\u6a21\u6001\u65b9\u6cd5\u4e0d\u9700\u6602\u8d35\u6df1\u5ea6\u6ce8\u91ca\u5373\u53ef\u8fbe\u5230\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff1bii)\u57fa\u4e8eVFM\u7684\u6a21\u578b\u8fbe\u5230\u4e86\u72b6\u6001\u524d\u6cbf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u6ce8\u91ca\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7a33\u5065\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2509.15194", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15194", "abs": "https://arxiv.org/abs/2509.15194", "authors": ["Yujun Zhou", "Zhenwen Liang", "Haolin Liu", "Wenhao Yu", "Kishan Panaganti", "Linfeng Song", "Dian Yu", "Xiangliang Zhang", "Haitao Mi", "Dong Yu"], "title": "Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation", "comment": null, "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.", "AI": {"tldr": "EVOL-RL\u662f\u4e00\u79cd\u65e0\u6807\u7b7e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6570\u6295\u7968\u7a33\u5b9a\u6027\u4e0e\u8bed\u4e49\u7a7a\u95f4\u65b0\u9896\u6027\u5956\u52b1\uff0c\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6539\u8fdb\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u71b5\u5d29\u6e83\uff0c\u4fdd\u6301\u63a2\u7d22\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u6807\u7b7e\u65b9\u6cd5\uff08\u5982\u7f6e\u4fe1\u5ea6\u6700\u5c0f\u5316\u3001\u81ea\u4e00\u81f4\u6027\u6216\u591a\u6570\u6295\u7968\u76ee\u6807\uff09\u867d\u7136\u7a33\u5b9a\u5b66\u4e60\uff0c\u4f46\u4f1a\u9010\u6e10\u7f29\u5c0f\u63a2\u7d22\u8303\u56f4\uff0c\u5bfc\u81f4\u71b5\u5d29\u6e83\uff1a\u751f\u6210\u5185\u5bb9\u53d8\u77ed\u3001\u591a\u6837\u6027\u51cf\u5c11\u4e14\u8106\u5f31\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u901a\u7528\u6539\u8fdb\u53c8\u4e0d\u727a\u7272\u6a21\u578b\u56fa\u6709\u63a2\u7d22\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEVOL-RL\u65b9\u6cd5\uff1a\u4f7f\u7528\u591a\u6570\u6295\u7968\u7b54\u6848\u4f5c\u4e3a\u7a33\u5b9a\u951a\u70b9\uff08\u9009\u62e9\uff09\uff0c\u540c\u65f6\u6dfb\u52a0\u65b0\u9896\u6027\u611f\u77e5\u5956\u52b1\uff0c\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u504f\u597d\u4e0e\u5df2\u751f\u6210\u5185\u5bb9\u4e0d\u540c\u7684\u54cd\u5e94\uff08\u53d8\u5f02\uff09\u3002\u91c7\u7528GRPO\u5b9e\u73b0\uff0c\u4f7f\u7528\u975e\u5bf9\u79f0\u88c1\u526a\u4fdd\u7559\u5f3a\u4fe1\u53f7\u548c\u71b5\u6b63\u5219\u5316\u7ef4\u6301\u641c\u7d22\u3002", "result": "EVOL-RL\u5728\u65e0\u6807\u7b7eAIME24\u8bad\u7ec3\u4e2d\uff0c\u5c06Qwen3-4B-Base\u6a21\u578b\u5728AIME25\u4e0a\u7684pass@1\u4eceTTRL\u76844.6%\u63d0\u5347\u81f316.4%\uff0cpass@16\u4ece18.5%\u63d0\u5347\u81f337.9%\u3002\u4e0d\u4ec5\u9632\u6b62\u591a\u6837\u6027\u5d29\u6e83\uff0c\u8fd8\u5728\u8de8\u9886\u57df\uff08\u5982GPQA\uff09\u5c55\u73b0\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u5728RLVR\u8bbe\u7f6e\u4e2d\u4e5f\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "EVOL-RL\u901a\u8fc7\u9009\u62e9\u4e0e\u53d8\u5f02\u8026\u5408\u7684\u8bbe\u8ba1\u6709\u6548\u9632\u6b62\u5d29\u6e83\uff0c\u4fdd\u6301\u66f4\u957f\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u601d\u7ef4\u94fe\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2509.15199", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.15199", "abs": "https://arxiv.org/abs/2509.15199", "authors": ["Ying Zheng", "Yangfan Jiang", "Kian-Lee Tan"], "title": "CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness", "comment": null, "summary": "Causal fairness in databases is crucial to preventing biased and inaccurate\noutcomes in downstream tasks. While most prior work assumes a known causal\nmodel, recent efforts relax this assumption by enforcing additional\nconstraints. However, these approaches often fail to capture broader attribute\nrelationships that are critical to maintaining utility. This raises a\nfundamental question: Can we harness the benefits of causal reasoning to design\nefficient and effective fairness solutions without relying on strong\nassumptions about the underlying causal model? In this paper, we seek to answer\nthis question by introducing CausalPre, a scalable and effective\ncausality-guided data pre-processing framework that guarantees justifiable\nfairness, a strong causal notion of fairness. CausalPre extracts causally fair\nrelationships by reformulating the originally complex and computationally\ninfeasible extraction task into a tailored distribution estimation problem. To\nensure scalability, CausalPre adopts a carefully crafted variant of\nlow-dimensional marginal factorization to approximate the joint distribution,\ncomplemented by a heuristic algorithm that efficiently tackles the associated\ncomputational challenge. Extensive experiments on benchmark datasets\ndemonstrate that CausalPre is both effective and scalable, challenging the\nconventional belief that achieving causal fairness requires trading off\nrelationship coverage for relaxed model assumptions.", "AI": {"tldr": "CausalPre\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u56e0\u679c\u5f15\u5bfc\u6570\u636e\u9884\u5904\u7406\u6846\u67b6\uff0c\u65e0\u9700\u5f3a\u56e0\u679c\u6a21\u578b\u5047\u8bbe\u5373\u53ef\u4fdd\u8bc1\u56e0\u679c\u516c\u5e73\u6027", "motivation": "\u73b0\u6709\u56e0\u679c\u516c\u5e73\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u56e0\u679c\u6a21\u578b\u6216\u4f9d\u8d56\u5f3a\u5047\u8bbe\uff0c\u4e14\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5173\u952e\u5c5e\u6027\u5173\u7cfb\uff0c\u5f71\u54cd\u6570\u636e\u6548\u7528", "method": "\u5c06\u590d\u6742\u7684\u56e0\u679c\u516c\u5e73\u5173\u7cfb\u63d0\u53d6\u95ee\u9898\u8f6c\u5316\u4e3a\u5b9a\u5236\u5316\u7684\u5206\u5e03\u4f30\u8ba1\u95ee\u9898\uff0c\u91c7\u7528\u4f4e\u7ef4\u8fb9\u9645\u5206\u89e3\u8fd1\u4f3c\u8054\u5408\u5206\u5e03\uff0c\u914d\u5408\u542f\u53d1\u5f0f\u7b97\u6cd5\u89e3\u51b3\u8ba1\u7b97\u6311\u6218", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCausalPre\u65e2\u6709\u6548\u53c8\u53ef\u6269\u5c55", "conclusion": "\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u5ff5\uff0c\u8bc1\u660e\u65e0\u9700\u5728\u5173\u7cfb\u8986\u76d6\u5ea6\u548c\u6a21\u578b\u5047\u8bbe\u4e4b\u95f4\u6743\u8861\u5373\u53ef\u5b9e\u73b0\u56e0\u679c\u516c\u5e73"}}
{"id": "2506.11445", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11445", "abs": "https://arxiv.org/abs/2506.11445", "authors": ["Xuan Duy Ta", "Bang Giang Le", "Thanh Ha Le", "Viet Cuong Ta"], "title": "Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention", "comment": null, "summary": "In mixed-traffic environments, autonomous vehicles must adapt to\nhuman-controlled vehicles and other unusual driving situations. This setting\ncan be framed as a multi-agent reinforcement learning (MARL) environment with\nfull cooperative reward among the autonomous vehicles. While methods such as\nMulti-agent Proximal Policy Optimization can be effective in training MARL\ntasks, they often fail to resolve local conflict between agents and are unable\nto generalize to stochastic events. In this paper, we propose a Local State\nAttention module to assist the input state representation. By relying on the\nself-attention operator, the module is expected to compress the essential\ninformation of nearby agents to resolve the conflict in traffic situations.\nUtilizing a simulated highway merging scenario with the priority vehicle as the\nunexpected event, our approach is able to prioritize other vehicles'\ninformation to manage the merging process. The results demonstrate significant\nimprovements in merging efficiency compared to popular baselines, especially in\nhigh-density traffic settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5c40\u90e8\u72b6\u6001\u6ce8\u610f\u529b\u6a21\u5757\u7684MARL\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u51b2\u7a81\u534f\u8c03\u95ee\u9898\uff0c\u5728\u9ad8\u901f\u516c\u8def\u6c47\u5165\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6548\u7387", "motivation": "\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u9002\u5e94\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u548c\u5f02\u5e38\u60c5\u51b5\uff0c\u4f20\u7edfMARL\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u5c40\u90e8\u51b2\u7a81\u548c\u6cdb\u5316\u5230\u968f\u673a\u4e8b\u4ef6", "method": "\u4f7f\u7528\u5c40\u90e8\u72b6\u6001\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u7b97\u5b50\u538b\u7f29\u9644\u8fd1\u8f66\u8f86\u7684\u5173\u952e\u4fe1\u606f\u6765\u89e3\u51b3\u4ea4\u901a\u51b2\u7a81\uff0c\u5728\u9ad8\u901f\u516c\u8def\u6c47\u5165\u573a\u666f\u4e2d\u4f18\u5148\u5904\u7406\u5176\u4ed6\u8f66\u8f86\u4fe1\u606f", "result": "\u5728\u9ad8\u901f\u516c\u8def\u6c47\u5165\u573a\u666f\u4e2d\u76f8\u6bd4\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6c47\u5165\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5bc6\u5ea6\u4ea4\u901a\u73af\u5883\u4e0b\u6548\u679c\u66f4\u660e\u663e", "conclusion": "\u5c40\u90e8\u72b6\u6001\u6ce8\u610f\u529b\u6a21\u5757\u80fd\u6709\u6548\u63d0\u5347MARL\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
