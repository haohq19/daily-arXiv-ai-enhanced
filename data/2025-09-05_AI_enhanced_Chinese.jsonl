{"id": "2509.03563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03563", "abs": "https://arxiv.org/abs/2509.03563", "authors": ["Quan Quan", "Jiwen Xu", "Runxiao Liu", "Yi Ding", "Jiaxing Che", "Kai-Yuan Cai"], "title": "Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach", "comment": null, "summary": "In comparison with existing approaches, which struggle with scalability,\ncommunication dependency, and robustness against dynamic failures, cooperative\naerial transportation via robot swarms holds transformative potential for\nlogistics and disaster response. Here, we present a physics-inspired\ncooperative transportation approach for flying robot swarms that imitates the\ndissipative mechanics of table-leg load distribution. By developing a\ndecentralized dissipative force model, our approach enables autonomous\nformation stabilization and adaptive load allocation without the requirement of\nexplicit communication. Based on local neighbor robots and the suspended\npayload, each robot dynamically adjusts its position. This is similar to\nenergy-dissipating table leg reactions. The stability of the resultant control\nsystem is rigorously proved. Simulations demonstrate that the tracking errors\nof the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches\nunder the cases of capability variation, cable uncertainty, limited vision, and\npayload variation, respectively. In real-world experiments with six flying\nrobots, the cooperative aerial transportation system achieved a 94% success\nrate under single-robot failure, disconnection events, 25% payload variation,\nand 40% cable length uncertainty, demonstrating strong robustness under outdoor\nwinds up to Beaufort scale 4. Overall, this physics-inspired approach bridges\nswarm intelligence and mechanical stability principles, offering a scalable\nframework for heterogeneous aerial systems to collectively handle complex\ntransportation tasks in communication-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u542f\u53d1\u7684\u65e0\u4eba\u673a\u7fa4\u534f\u540c\u8fd0\u8f93\u65b9\u6cd5\uff0c\u6a21\u4eff\u684c\u817f\u8d1f\u8f7d\u5206\u914d\u7684\u8017\u6563\u529b\u5b66\u539f\u7406\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u663e\u5f0f\u901a\u4fe1\u7684\u81ea\u4e3b\u7f16\u961f\u7a33\u5b9a\u548c\u81ea\u9002\u5e94\u8d1f\u8f7d\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u901a\u4fe1\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u6545\u969c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u65e0\u4eba\u673a\u7fa4\u534f\u540c\u8fd0\u8f93\u5728\u7269\u6d41\u548c\u707e\u96be\u54cd\u5e94\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u53bb\u4e2d\u5fc3\u5316\u8017\u6563\u529b\u6a21\u578b\uff0c\u6bcf\u4e2a\u673a\u5668\u4eba\u6839\u636e\u90bb\u5c45\u673a\u5668\u4eba\u548c\u60ac\u6302\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u4f4d\u7f6e\uff0c\u7c7b\u4f3c\u4e8e\u80fd\u91cf\u8017\u6563\u7684\u684c\u817f\u53cd\u5e94\u673a\u5236\u3002", "result": "\u4eff\u771f\u663e\u793a\u8ddf\u8e2a\u8bef\u5dee\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e20%-68.5%\uff0c\u771f\u5b9e\u5b9e\u9a8c\u5728\u5355\u673a\u6545\u969c\u3001\u65ad\u8fde\u4e8b\u4ef6\u300125%\u8d1f\u8f7d\u53d8\u5316\u548c40%\u7f06\u957f\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fbe\u523094%\u6210\u529f\u7387\uff0c\u57284\u7ea7\u98ce\u529b\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u7fa4\u4f53\u667a\u80fd\u4e0e\u673a\u68b0\u7a33\u5b9a\u6027\u539f\u7406\u76f8\u7ed3\u5408\uff0c\u4e3a\u5f02\u6784\u7a7a\u4e2d\u7cfb\u7edf\u5728\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\u5904\u7406\u590d\u6742\u8fd0\u8f93\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2509.03609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03609", "abs": "https://arxiv.org/abs/2509.03609", "authors": ["Shengkai Sun", "Zefan Zhang", "Jianfeng Dong", "Zhiyong Cheng", "Xiaojun Chang", "Meng Wang"], "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling", "comment": "Accepted by ICCV 2025", "summary": "Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.", "AI": {"tldr": "\u63d0\u51faGFP\u6846\u67b6\uff0c\u7528\u9ad8\u7ea7\u7279\u5f81\u9884\u6d4b\u66ff\u4ee3\u4f20\u7edf\u4f4e\u7ea7\u91cd\u5efa\uff0c\u5b9e\u73b0\u9ad8\u6548\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b", "motivation": "\u73b0\u6709MAE\u65b9\u6cd5\u5c40\u9650\u4e8e\u539f\u59cb\u5173\u8282\u5750\u6807\u91cd\u5efa\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u8bed\u4e49\u8868\u793a\u6709\u9650", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u76ee\u6807\u751f\u6210\u7f51\u7edc\u52a8\u6001\u4ea7\u751f\u591a\u6837\u5316\u76d1\u7763\u4fe1\u53f7\uff0c\u91c7\u7528\u7ea6\u675f\u4f18\u5316\u786e\u4fdd\u7279\u5f81\u591a\u6837\u6027", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u53476.2\u500d\uff0c\u5728NTU RGB+D 60/120\u548cPKU-MMD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "conclusion": "GFP\u6846\u67b6\u901a\u8fc7\u9ad8\u7ea7\u7279\u5f81\u9884\u6d4b\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8d28\u91cf\u7684\u53cc\u91cd\u63d0\u5347"}}
{"id": "2509.03528", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03528", "abs": "https://arxiv.org/abs/2509.03528", "authors": ["Matilde Contestabile", "Chiara Ferrara", "Alberto Giovannetti", "Giovanni Parrillo", "Andrea Vandin"], "title": "The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process", "comment": null, "summary": "Process Mining (PM), initially developed for industrial and business\ncontexts, has recently been applied to social systems, including legal ones.\nHowever, PM's efficacy in the legal domain is limited by the accessibility and\nquality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in\nItalian Chambers), a comprehensive event log of the Italian lawmaking process\nfrom 1987 to 2022. Created from unstructured data from the Normattiva portal\nand structured using large language models (LLMs), ProLiFIC aligns with recent\nefforts in integrating PM with LLMs. We exemplify preliminary analyses and\npropose ProLiFIC as a benchmark for legal PM, fostering new developments.", "AI": {"tldr": "ProLiFIC\u662f\u4e00\u4e2a\u4ece1987\u5e74\u52302022\u5e74\u610f\u5927\u5229\u7acb\u6cd5\u8fc7\u7a0b\u7684\u7efc\u5408\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u4f7f\u7528LLM\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u521b\u5efa\uff0c\u65e8\u5728\u4e3a\u6cd5\u5f8b\u6d41\u7a0b\u6316\u6398\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u6d41\u7a0b\u6316\u6398\u5728\u5de5\u4e1a\u9886\u57df\u5e94\u7528\u6210\u719f\uff0c\u4f46\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u548c\u8d28\u91cf\u3002\u610f\u5927\u5229\u7acb\u6cd5\u8fc7\u7a0b\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u4e8b\u4ef6\u65e5\u5fd7\u6570\u636e\uff0c\u9650\u5236\u4e86\u6d41\u7a0b\u6316\u6398\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "method": "\u4eceNormattiva\u95e8\u6237\u83b7\u53d6\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u8fdb\u884c\u7ed3\u6784\u5316\u5904\u7406\uff0c\u521b\u5efa\u4e86ProLiFIC\u4e8b\u4ef6\u65e5\u5fd7\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u6db5\u76d635\u5e74\u610f\u5927\u5229\u7acb\u6cd5\u8fc7\u7a0b\u7684\u7efc\u5408\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5c55\u793a\u4e86\u521d\u6b65\u5206\u6790\u793a\u4f8b\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProLiFIC\u4e3a\u6cd5\u5f8b\u6d41\u7a0b\u6316\u6398\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u4e86\u6d41\u7a0b\u6316\u6398\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86\u6cd5\u5f8b\u9886\u57df\u6d41\u7a0b\u6316\u6398\u7684\u65b0\u53d1\u5c55\u3002"}}
{"id": "2509.03677", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03677", "abs": "https://arxiv.org/abs/2509.03677", "authors": ["Vincent-Daniel Yun"], "title": "Insights from Gradient Dynamics: Gradient Autoscaled Normalization", "comment": null, "summary": "Gradient dynamics play a central role in determining the stability and\ngeneralization of deep neural networks. In this work, we provide an empirical\nanalysis of how variance and standard deviation of gradients evolve during\ntraining, showing consistent changes across layers and at the global scale in\nconvolutional networks. Motivated by these observations, we propose a\nhyperparameter-free gradient normalization method that aligns gradient scaling\nwith their natural evolution. This approach prevents unintended amplification,\nstabilizes optimization, and preserves convergence guarantees. Experiments on\nthe challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN\ndemonstrate that our method maintains or improves test accuracy even under\nstrong generalization. Beyond practical performance, our study highlights the\nimportance of directly tracking gradient dynamics, aiming to bridge the gap\nbetween theoretical expectations and empirical behaviors, and to provide\ninsights for future optimization research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u52a8\u6001\u5206\u6790\u7684\u65e0\u8d85\u53c2\u6570\u68af\u5ea6\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5339\u914d\u68af\u5ea6\u7684\u81ea\u7136\u6f14\u5316\u6765\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u5728CIFAR-100\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7cbe\u5ea6\u3002", "motivation": "\u68af\u5ea6\u52a8\u6001\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u53d1\u73b0\u68af\u5ea6\u65b9\u5dee\u548c\u6807\u51c6\u5dee\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5448\u73b0\u4e00\u81f4\u7684\u53d8\u5316\u6a21\u5f0f\uff0c\u8fd9\u542f\u53d1\u4e86\u5bf9\u68af\u5ea6\u7f29\u653e\u8fdb\u884c\u81ea\u7136\u5bf9\u9f50\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8d85\u53c2\u6570\u81ea\u7531\u7684\u68af\u5ea6\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u4f7f\u68af\u5ea6\u7f29\u653e\u4e0e\u5176\u81ea\u7136\u6f14\u5316\u4fdd\u6301\u4e00\u81f4\uff0c\u9632\u6b62\u610f\u5916\u653e\u5927\uff0c\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728CIFAR-100\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528ResNet-20\u3001ResNet-56\u548cVGG-16-BN\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5373\u4f7f\u5728\u5f3a\u6cdb\u5316\u6761\u4ef6\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6216\u63d0\u9ad8\u6d4b\u8bd5\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u76f4\u63a5\u8ddf\u8e2a\u68af\u5ea6\u52a8\u6001\u7684\u91cd\u8981\u6027\uff0c\u65e8\u5728\u5f25\u5408\u7406\u8bba\u9884\u671f\u4e0e\u5b9e\u8bc1\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u89c1\u89e3\u3002"}}
{"id": "2509.04069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04069", "abs": "https://arxiv.org/abs/2509.04069", "authors": ["Chengyandan Shen", "Christoffer Sloth"], "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning", "comment": null, "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.", "AI": {"tldr": "\u63d0\u51faDRLR\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdbIBRL\u7b97\u6cd5\u7684\u52a8\u4f5c\u9009\u62e9\u6a21\u5757\u6765\u51cf\u5c11bootstrapping\u8bef\u5dee\uff0c\u4f7f\u7528SAC\u66ff\u4ee3TD3\u9632\u6b62\u7b56\u7565\u6536\u655b\u5230\u6b21\u4f18\u89e3\uff0c\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u548cbootstrapping\u8bef\u5dee\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u793a\u8303\u6570\u636e\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "\u57fa\u4e8eIBRL\u7b97\u6cd5\u6539\u8fdb\u52a8\u4f5c\u9009\u62e9\u6a21\u5757\uff0c\u63d0\u4f9b\u6821\u51c6\u7684Q\u503c\u6765\u51cf\u5c11bootstrapping\u8bef\u5dee\uff1b\u4f7f\u7528SAC\u4f5c\u4e3aRL\u7b56\u7565\u9632\u6b62\u6536\u655b\u5230\u6b21\u4f18\u89e3\uff1b\u5728bucket loading\u548copen drawer\u4e24\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11bootstrapping\u8bef\u5dee\u548c\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u5728\u4f4e\u7ef4\u548c\u9ad8\u7ef4\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4efb\u52a1\u4e2d\u90fd\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u5bf9\u4e0d\u540c\u8d28\u91cf\u7684\u793a\u8303\u6570\u636e\u90fd\u6709\u6548\uff1b\u5728\u771f\u5b9e\u8f6e\u5f0f\u88c5\u8f7d\u673a\u4e0a\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "DRLR\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u52a8\u4f5c\u9009\u62e9\u548c\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u6210\u529f\u8fc1\u79fb\u3002"}}
{"id": "2509.03808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03808", "abs": "https://arxiv.org/abs/2509.03808", "authors": ["Huanan Li", "Rui Fan", "Juntao Guan", "Weidong Hao", "Lai Rui", "Tong Wu", "Yikai Wang", "Lin Gu"], "title": "EGTM: Event-guided Efficient Turbulence Mitigation", "comment": null, "summary": "Turbulence mitigation (TM) aims to remove the stochastic distortions and\nblurs introduced by atmospheric turbulence into frame cameras. Existing\nstate-of-the-art deep-learning TM methods extract turbulence cues from multiple\ndegraded frames to find the so-called \"lucky'', not distorted patch, for \"lucky\nfusion''. However, it requires high-capacity network to learn from\ncoarse-grained turbulence dynamics between synchronous frames with limited\nframe-rate, thus fall short in computational and storage efficiency. Event\ncameras, with microsecond-level temporal resolution, have the potential to\nfundamentally address this bottleneck with efficient sparse and asynchronous\nimaging mechanism. In light of this, we (i) present the fundamental\n\\textbf{``event-lucky insight''} to reveal the correlation between turbulence\ndistortions and inverse spatiotemporal distribution of event streams. Then,\nbuild upon this insight, we (ii) propose a novel EGTM framework that extracts\npixel-level reliable turbulence-free guidance from the explicit but noisy\nturbulent events for temporal lucky fusion. Moreover, we (iii) build the first\nturbulence data acquisition system to contribute the first real-world\nevent-driven TM dataset. Extensive experimental results demonstrate that our\napproach significantly surpass the existing SOTA TM method by 710 times, 214\ntimes and 224 times in model size, inference latency and model complexity\nrespectively, while achieving the state-of-the-art in restoration quality\n(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating\nthe great efficiency merit of introducing event modality into TM task. Demo\ncode and data have been uploaded in supplementary material and will be released\nonce accepted.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u6fc0\u5149\u878d\u5408\u6c1b\u6d41\u51cf\u7f13\u65b9\u6cd5(EGTM)\uff0c\u901a\u8fc7\u4e8b\u4ef6\u6d41\u7684\u5f02\u6b65\u7279\u6027\u63d0\u53d6\u50cf\u7d20\u7ea7\u53ef\u9760\u65e0\u6c1b\u6d41\u6307\u5bfc\uff0c\u5728\u4fdd\u6301\u9884\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8fd8\u539f\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6c1b\u6d41\u51cf\u7f13\u65b9\u6cd5\u9700\u8981\u9ad8\u5bb9\u91cf\u7f51\u7edc\u5b66\u4e60\u540c\u6b65\u5e27\u4e4b\u95f4\u7c97\u7c92\u5ea6\u7684\u6c1b\u6d41\u52a8\u529b\u5b66\uff0c\u8ba1\u7b97\u548c\u5b58\u50a8\u6548\u7387\u4f4e\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u6709\u6f5c\u529b\u6839\u672c\u89e3\u51b3\u8fd9\u4e2a\u74f6\u9888\u3002", "method": "\u9996\u5148\u63d0\u51fa\"\u4e8b\u4ef6\u5e78\u8fd0\u89c1\u89e3\"\u63ed\u793a\u6c1b\u6d41\u626d\u66f2\u4e0e\u4e8b\u4ef6\u6d41\u9006\u65f6\u7a7a\u5206\u5e03\u7684\u76f8\u5173\u6027\uff0c\u7136\u540e\u6784\u5efaEGTM\u6846\u67b6\u4ece\u566a\u58f0\u6c1b\u6d41\u4e8b\u4ef6\u4e2d\u63d0\u53d6\u50cf\u7d20\u7ea7\u53ef\u9760\u65e0\u6c1b\u6d41\u6307\u5bfc\u8fdb\u884c\u65f6\u95f4\u5e78\u8fd0\u878d\u5408\uff0c\u5e76\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u9a71\u52a8\u6c1b\u6d41\u6570\u636e\u96c6\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cEGTM\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u5ef6\u8fdf\u548c\u6a21\u578b\u590d\u6742\u5ea6\u4e0a\u5206\u522b\u8d85\u8fc7\u73b0\u6709SOTA\u65b9\u6cd5710\u500d\u3001214\u500d\u548c224\u500d\uff0c\u8fd8\u539f\u8d28\u91cf\u4e5f\u8fbe\u5230\u6700\u4f73\u6c34\u5e73(+0.94 PSNR\u548c+0.08 SSIM)\u3002", "conclusion": "\u8fd9\u4e00\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u4e8b\u4ef6\u624b\u6bb5\u5f15\u5165\u6c1b\u6d41\u51cf\u7f13\u4efb\u52a1\u7684\u5de8\u5927\u6548\u7387\u4f18\u52bf\uff0c\u4e3a\u9ad8\u6548\u6c1b\u6d41\u51cf\u7f13\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03828", "abs": "https://arxiv.org/abs/2509.03828", "authors": ["Jaerong Ahn", "Andrew Wen", "Nan Wang", "Heling Jia", "Zhiyi Yue", "Sunyang Fu", "Hongfang Liu"], "title": "An Agentic Model Context Protocol Framework for Medical Concept Standardization", "comment": null, "summary": "The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)\nprovides a standardized representation of heterogeneous health data to support\nlarge-scale, multi-institutional research. One critical step in data\nstandardization using OMOP CDM is the mapping of source medical terms to OMOP\nstandard concepts, a procedure that is resource-intensive and error-prone.\nWhile large language models (LLMs) have the potential to facilitate this\nprocess, their tendency toward hallucination makes them unsuitable for clinical\ndeployment without training and expert validation. Here, we developed a\nzero-training, hallucination-preventive mapping system based on the Model\nContext Protocol (MCP), a standardized and secure framework allowing LLMs to\ninteract with external resources and tools. The system enables explainable\nmapping and significantly improves efficiency and accuracy with minimal effort.\nIt provides real-time vocabulary lookups and structured reasoning outputs\nsuitable for immediate use in both exploratory and production environments.", "AI": {"tldr": "\u57fa\u4e8eModel Context Protocol\u7684\u65e0\u8bad\u7ec3\u3001\u9632\u5e7b\u89c9\u7684OMOP CDM\u533b\u5b66\u672f\u8bed\u6620\u5c04\u7cfb\u7edf\uff0c\u901a\u8fc7\u5916\u90e8\u8d44\u6e90\u67e5\u8be2\u63d0\u9ad8\u6620\u5c04\u6548\u7387\u548c\u51c6\u786e\u6027", "motivation": "OMOP CDM\u6807\u51c6\u5316\u8fc7\u7a0b\u4e2d\u7684\u6e90\u533b\u5b66\u672f\u8bed\u6620\u5c04\u5de5\u4f5c\u8017\u8d39\u8d44\u6e90\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u4e0d\u9002\u5408\u76f4\u63a5\u4e34\u5e8a\u90e8\u7f72", "method": "\u91c7\u7528Model Context Protocol(MCP)\u6807\u51c6\u5316\u6846\u67b6\uff0c\u5141\u8bb8LLM\u4e0e\u5916\u90e8\u8d44\u6e90\u548c\u5de5\u5177\u4ea4\u4e92\uff0c\u5b9e\u73b0\u96f6\u8bad\u7ec3\u3001\u53ef\u89e3\u91ca\u7684\u6620\u5c04\u7cfb\u7edf", "result": "\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u5b9e\u65f6\u8bcd\u6c47\u67e5\u627e\u548c\u7ed3\u6784\u5316\u63a8\u7406\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6620\u5c04\u6548\u7387\u548c\u51c6\u786e\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u533b\u5b66\u672f\u8bed\u6620\u5c04\uff0c\u9002\u7528\u4e8e\u63a2\u7d22\u548c\u751f\u4ea7\u73af\u5883"}}
{"id": "2509.03872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03872", "abs": "https://arxiv.org/abs/2509.03872", "authors": ["Nan Yang", "Yang Wang", "Zhanwen Liu", "Yuchao Dai", "Yang Liu", "Xiangmo Zhao"], "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection", "comment": null, "summary": "Existing RGB-Event detection methods process the low-information regions of\nboth modalities (background in images and non-event regions in event data)\nuniformly during feature extraction and fusion, resulting in high computational\ncosts and suboptimal performance. To mitigate the computational redundancy\nduring feature extraction, researchers have respectively proposed token\nsparsification methods for the image and event modalities. However, these\nmethods employ a fixed number or threshold for token selection, hindering the\nretention of informative tokens for samples with varying complexity. To achieve\na better balance between accuracy and efficiency, we propose FocusMamba, which\nperforms adaptive collaborative sparsification of multimodal features and\nefficiently integrates complementary information. Specifically, an Event-Guided\nMultimodal Sparsification (EGMS) strategy is designed to identify and\nadaptively discard low-information regions within each modality by leveraging\nscene content changes perceived by the event camera. Based on the\nsparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed\nto effectively capture and integrate complementary features from both\nmodalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate\nthat the proposed method achieves superior performance in both accuracy and\nefficiency compared to existing methods. The code will be available at\nhttps://github.com/Zizzzzzzz/FocusMamba.", "AI": {"tldr": "FocusMamba\u901a\u8fc7\u4e8b\u4ef6\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u7a00\u758f\u5316\u548c\u8de8\u6a21\u6001\u805a\u7126\u878d\u5408\uff0c\u81ea\u9002\u5e94\u5730\u4e22\u5f03RGB\u548c\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u4f4e\u4fe1\u606f\u533a\u57df\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "motivation": "\u73b0\u6709RGB-\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u6240\u6709\u533a\u57df\u8fdb\u884c\u7edf\u4e00\u5904\u7406\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6027\u80fd\u4e0d\u4f73\u3002\u56fa\u5b9a\u9608\u503c\u7684\u5206\u8bcd\u7a00\u758f\u5316\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u590d\u6742\u5ea6\u6837\u672c\u7684\u9700\u6c42", "method": "\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u591a\u6a21\u6001\u7a00\u758f\u5316(EGMS)\u7b56\u7565\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u611f\u77e5\u7684\u573a\u666f\u53d8\u5316\u81ea\u9002\u5e94\u8bc6\u522b\u548c\u4e22\u5f03\u4f4e\u4fe1\u606f\u533a\u57df\uff1b\u8bbe\u8ba1\u8de8\u6a21\u6001\u805a\u7126\u878d\u5408(CMFF)\u6a21\u5757\u6709\u6548\u6574\u5408\u4e92\u8865\u7279\u5f81", "result": "\u5728DSEC-Det\u548cPKU-DAVIS-SOD\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u6027\u80fd", "conclusion": "FocusMamba\u901a\u8fc7\u81ea\u9002\u5e94\u534f\u4f5c\u7a00\u758f\u5316\u548c\u9ad8\u6548\u878d\u5408\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86RGB-\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u66f4\u597d\u5e73\u8861"}}
{"id": "2509.03857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03857", "abs": "https://arxiv.org/abs/2509.03857", "authors": ["Kishor Datta Gupta", "Mohd Ariful Haque", "Hasmot Ali", "Marufa Kamal", "Syed Bahauddin Alam", "Mohammad Ashiqur Rahman"], "title": "Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures", "comment": null, "summary": "Generative AI (GEN AI) models have revolutionized diverse application domains\nbut present substantial challenges due to reliability concerns, including\nhallucinations, semantic drift, and inherent biases. These models typically\noperate as black-boxes, complicating transparent and objective evaluation.\nCurrent evaluation methods primarily depend on subjective human assessment,\nlimiting scalability, transparency, and effectiveness. This research proposes a\nsystematic methodology using deterministic and Large Language Model\n(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN\nAI reliability. We construct two parallel KGs: (i) a deterministic KG built\nusing explicit rule-based methods, predefined ontologies, domain-specific\ndictionaries, and structured entity-relation extraction rules, and (ii) an\nLLM-generated KG dynamically derived from real-time textual data streams such\nas live news articles. Utilizing real-time news streams ensures authenticity,\nmitigates biases from repetitive training, and prevents adaptive LLMs from\nbypassing predefined benchmarks through feedback memorization. To quantify\nstructural deviations and semantic discrepancies, we employ several established\nKG metrics, including Instantiated Class Ratio (ICR), Instantiated Property\nRatio (IPR), and Class Instantiation (CI). An automated real-time monitoring\nframework continuously computes deviations between deterministic and\nLLM-generated KGs. By establishing dynamic anomaly thresholds based on\nhistorical structural metric distributions, our method proactively identifies\nand flags significant deviations, thus promptly detecting semantic anomalies or\nhallucinations. This structured, metric-driven comparison between deterministic\nand dynamically generated KGs delivers a robust and scalable evaluation\nframework.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03819", "categories": ["cs.LG", "68T07, 62H30", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.03819", "abs": "https://arxiv.org/abs/2509.03819", "authors": ["Meghan Bibb", "Pablo Rivas", "Mahee Tayba"], "title": "Predicting Traffic Accident Severity with Deep Neural Networks", "comment": "The 17th International Conference on Data Science (ICDATA 2021)", "summary": "Traffic accidents can be studied to mitigate the risk of further events.\nRecent advances in machine learning have provided an alternative way to study\ndata associated with traffic accidents. New models achieve good generalization\nand high predictive power over imbalanced data. In this research, we study\nneural network-based models on data related to traffic accidents. We begin\nanalyzing relative feature colinearity and unsupervised dimensionality\nreduction through autoencoders, followed by a dense network. The features are\nrelated to traffic accident data and the target is to classify accident\nseverity. Our experiments show cross-validated results of up to 92% accuracy\nwhen classifying accident severity using the proposed deep neural network.", "AI": {"tldr": "\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5206\u6790\u4ea4\u901a\u4e8b\u6545\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u4e8b\u6545\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8fbe\u523092%", "motivation": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u7814\u7a76\u4ea4\u901a\u4e8b\u6545\u6570\u636e\uff0c\u964d\u4f4e\u672a\u6765\u4e8b\u6545\u98ce\u9669\uff0c\u5e76\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898", "method": "\u9996\u5148\u5206\u6790\u7279\u5f81\u5171\u7ebf\u6027\uff0c\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u65e0\u76d1\u7763\u7ef4\u5ea6\u964d\u7f29\uff0c\u7136\u540e\u6784\u5efa\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e8b\u6545\u4e25\u91cd\u7a0b\u5ea4\u5206\u7c7b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe\u523092%\uff0c\u5728\u4e8b\u6545\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4ea4\u901a\u4e8b\u6545\u6570\u636e\u5206\u6790\u4e2d\u5177\u6709\u9ad8\u51c6\u786e\u7387\u548c\u826f\u597d\u7684\u6a21\u6027\u80fd\uff0c\u4e3a\u4e8b\u6545\u9884\u9632\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u652f\u6491"}}
{"id": "2509.04130", "categories": ["cs.AI", "cs.CY", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.04130", "abs": "https://arxiv.org/abs/2509.04130", "authors": ["William Stewart"], "title": "The human biological advantage over AI", "comment": "12 pages", "summary": "Recent advances in AI raise the possibility that AI systems will one day be\nable to do anything humans can do, only better. If artificial general\nintelligence (AGI) is achieved, AI systems may be able to understand, reason,\nproblem solve, create, and evolve at a level and speed that humans will\nincreasingly be unable to match, or even understand. These possibilities raise\na natural question as to whether AI will eventually become superior to humans,\na successor \"digital species\", with a rightful claim to assume leadership of\nthe universe. However, a deeper consideration suggests the overlooked\ndifferentiator between human beings and AI is not the brain, but the central\nnervous system (CNS), providing us with an immersive integration with physical\nreality. It is our CNS that enables us to experience emotion including pain,\njoy, suffering, and love, and therefore to fully appreciate the consequences of\nour actions on the world around us. And that emotional understanding of the\nconsequences of our actions is what is required to be able to develop\nsustainable ethical systems, and so be fully qualified to be the leaders of the\nuniverse. A CNS cannot be manufactured or simulated; it must be grown as a\nbiological construct. And so, even the development of consciousness will not be\nsufficient to make AI systems superior to humans. AI systems may become more\ncapable than humans on almost every measure and transform our society. However,\nthe best foundation for leadership of our universe will always be DNA, not\nsilicon.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5373\u4f7fAI\u5728\u80fd\u529b\u4e0a\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\uff08CNS\uff09\u5e26\u6765\u7684\u60c5\u611f\u4f53\u9a8c\u548c\u9053\u5fb7\u7406\u89e3\uff0cAI\u6c38\u8fdc\u65e0\u6cd5\u771f\u6b63\u53d6\u4ee3\u4eba\u7c7b\u6210\u4e3a\u5b87\u5b99\u7684\u9886\u5bfc\u8005\u3002", "motivation": "\u63a2\u8ba8AI\u662f\u5426\u4f1a\u6210\u4e3a\u8d85\u8d8a\u4eba\u7c7b\u7684\"\u6570\u5b57\u7269\u79cd\"\u5e76\u63a5\u7ba1\u5b87\u5b99\u9886\u5bfc\u6743\u7684\u95ee\u9898\uff0c\u6307\u51fa\u5f53\u524d\u8ba8\u8bba\u4e2d\u5ffd\u7565\u4e86\u4eba\u7c7b\u4e0eAI\u7684\u6839\u672c\u533a\u522b\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u5206\u6790\u548c\u751f\u7269\u5b66\u89c6\u89d2\uff0c\u6bd4\u8f83\u4eba\u7c7b\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u4e0eAI\u7cfb\u7edf\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u8bba\u8bc1\u60c5\u611f\u4f53\u9a8c\u548c\u9053\u5fb7\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002", "result": "AI\u53ef\u80fd\u5728\u51e0\u4e4e\u6240\u6709\u80fd\u529b\u6307\u6807\u4e0a\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4f46\u7531\u4e8e\u65e0\u6cd5\u62e5\u6709\u771f\u6b63\u7684\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u548c\u60c5\u611f\u4f53\u9a8c\uff0c\u65e0\u6cd5\u53d1\u5c55\u51fa\u53ef\u6301\u7eed\u7684\u4f26\u7406\u4f53\u7cfb\uff0c\u56e0\u6b64\u4e0d\u5177\u5907\u9886\u5bfc\u5b87\u5b99\u7684\u8d44\u683c\u3002", "conclusion": "\u5b87\u5b99\u9886\u5bfc\u6743\u7684\u6700\u4f73\u57fa\u7840\u6c38\u8fdc\u662fDNA\u800c\u975e\u7845\u57fa\u6280\u672f\uff0c\u4eba\u7c7b\u7684\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u548c\u60c5\u611f\u4f53\u9a8c\u662fAI\u65e0\u6cd5\u590d\u5236\u7684\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2509.04086", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.04086", "abs": "https://arxiv.org/abs/2509.04086", "authors": ["Yaru Chen", "Faegheh Sardari", "Peiliang Zhang", "Ruohao Guo", "Yang Xiang", "Zhenbo Li", "Wenwu Wang"], "title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph", "comment": null, "summary": "Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u53cc\u5411\u6587\u672c\u878d\u5408\u548c\u7c7b\u522b\u611f\u77e5\u65f6\u5e8f\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4e2d\u4f2a\u6807\u7b7e\u566a\u58f0\u4f20\u64ad\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06\u566a\u58f0\u6bb5\u7ea7\u4f2a\u6807\u7b7e\u89c6\u4e3a\u53ef\u9760\u76d1\u7763\uff0c\u8981\u4e48\u8ba9\u65e0\u5dee\u522b\u6ce8\u610f\u529b\u5c06\u4f2a\u6807\u7b7e\u4f20\u64ad\u5230\u6240\u6709\u5e27\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u521d\u59cb\u9519\u8bef\u88ab\u53cd\u590d\u653e\u5927", "method": "\u7ed3\u5408\u53cc\u5411\u6587\u672c\u878d\u5408\u6a21\u5757(BiT)\u8fdb\u884c\u8bed\u4e49\u6ce8\u5165\u548c\u52a8\u6001\u6821\u51c6\u6765\u5b9a\u4f4d\u548c\u51c0\u5316\u8bed\u4e49\u7ebf\u7d22\uff0c\u4f7f\u7528\u7c7b\u522b\u611f\u77e5\u65f6\u5e8f\u56fe\u6a21\u5757(CATS)\u8fdb\u884c\u8bed\u4e49\u4f20\u64ad\u548c\u8fde\u63a5", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6LLP\u548cUnAV-100\u7684\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6574\u5408\u4e24\u79cd\u7814\u7a76\u65b9\u5411\u7684\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4e2d\u7684\u566a\u58f0\u4f20\u64ad\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd"}}
{"id": "2509.04117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04117", "abs": "https://arxiv.org/abs/2509.04117", "authors": ["Mustafa Sakhai", "Kaung Sithu", "Min Khant Soe Oke", "Maciej Wielgosz"], "title": "DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset", "comment": "12 pages, 8 figures, 3 tables; dataset descriptor paper introducing\n  DVS-PedX (synthetic-and-real event-based pedestrian dataset with baselines)\n  External URL: https://doi.org/10.5281/zenodo.17030898", "summary": "Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness\nchanges instead of full frames, offering low latency, high dynamic range, and\nmotion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a\nneuromorphic dataset designed for pedestrian detection and crossing-intention\nanalysis in normal and adverse weather conditions across two complementary\nsources: (1) synthetic event streams generated in the CARLA simulator for\ncontrolled \"approach-cross\" scenes under varied weather and lighting; and (2)\nreal-world JAAD dash-cam videos converted to event streams using the v2e tool,\npreserving natural behaviors and backgrounds. Each sequence includes paired RGB\nframes, per-frame DVS \"event frames\" (33 ms accumulations), and frame-level\nlabels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0\nevent files and AVI DVS video files and metadata for flexible re-processing.\nBaseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset\nusability and reveal a sim-to-real gap, motivating domain adaptation and\nmultimodal fusion. DVS-PedX aims to accelerate research in event-based\npedestrian safety, intention prediction, and neuromorphic perception.", "AI": {"tldr": "DVS-PedX\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u884c\u4eba\u68c0\u6d4b\u548c\u8fc7\u8857\u610f\u56fe\u5206\u6790\u6570\u636e\u96c6\uff0c\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u4e8b\u4ef6\u6d41\u6570\u636e\uff0c\u652f\u6301SNN\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u8fd0\u52a8\u9c81\u68d2\u6027\u7b49\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u884c\u4eba\u5b89\u5168\u548c\u610f\u56fe\u9884\u6d4b\u7684\u6570\u636e\u96c6\u3002DVS-PedX\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u884c\u4eba\u5b89\u5168\u7814\u7a76\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6765\u6e90\uff1a(1) CARLA\u4eff\u771f\u5668\u751f\u6210\u7684\u5408\u6210\u4e8b\u4ef6\u6d41\uff0c\u63a7\u5236\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\uff1b(2) JAAD\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u901a\u8fc7v2e\u5de5\u5177\u8f6c\u6362\u7684\u771f\u5b9e\u4e8b\u4ef6\u6d41\u3002\u6bcf\u4e2a\u5e8f\u5217\u5305\u542b\u914d\u5bf9\u7684RGB\u5e27\u3001\u4e8b\u4ef6\u5e27\u548c\u5e27\u7ea7\u6807\u7b7e\u3002", "result": "\u63d0\u4f9b\u4e86\u57fa\u7ebf\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u6570\u636e\u96c6\u7684\u53ef\u4f7f\u7528\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u9886\u57df\u81ea\u9002\u5e94\u548c\u591a\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u4e86\u52a8\u673a\u3002", "conclusion": "DVS-PedX\u6570\u636e\u96c6\u5c06\u52a0\u901f\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u884c\u4eba\u5b89\u5168\u3001\u610f\u56fe\u9884\u6d4b\u548c\u795e\u7ecf\u5f62\u6001\u611f\u77e5\u7814\u7a76\uff0c\u4e3a\u9886\u57df\u81ea\u9002\u5e94\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2509.04202", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.04202", "abs": "https://arxiv.org/abs/2509.04202", "authors": ["Congbo Ma", "Yuxia Wang", "Jia Wu", "Jian Yang", "Jing Du", "Zitai Qiu", "Qing Li", "Hu Wang", "Preslav Nakov"], "title": "Explicit and Implicit Data Augmentation for Social Event Detection", "comment": null, "summary": "Social event detection involves identifying and categorizing important events\nfrom social media, which relies on labeled data, but annotation is costly and\nlabor-intensive. To address this problem, we propose Augmentation framework for\nSocial Event Detection (SED-Aug), a plug-and-play dual augmentation framework,\nwhich combines explicit text-based and implicit feature-space augmentation to\nenhance data diversity and model robustness. The explicit augmentation utilizes\nlarge language models to enhance textual information through five diverse\ngeneration strategies. For implicit augmentation, we design five novel\nperturbation techniques that operate in the feature space on structural fused\nembeddings. These perturbations are crafted to keep the semantic and relational\nproperties of the embeddings and make them more diverse. Specifically, SED-Aug\noutperforms the best baseline model by approximately 17.67% on the Twitter2012\ndataset and by about 15.57% on the Twitter2018 dataset in terms of the average\nF1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.", "AI": {"tldr": "SED-Aug\u662f\u4e00\u4e2a\u7528\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e8b\u4ef6\u68c0\u6d4b\u7684\u53cc\u91cd\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6587\u672c\u589e\u5f3a\u548c\u9690\u5f0f\u7279\u5f81\u7a7a\u95f4\u589e\u5f3a\u6765\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u4e24\u4e2aTwitter\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86F1\u5206\u6570\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e8b\u4ef6\u68c0\u6d4b\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u8d39\u65f6\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u589e\u5f3a\u6846\u67b6\uff1a1\uff09\u663e\u5f0f\u6587\u672c\u589e\u5f3a\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc75\u79cd\u751f\u6210\u7b56\u7565\u589e\u5f3a\u6587\u672c\u4fe1\u606f\uff1b2\uff09\u9690\u5f0f\u7279\u5f81\u7a7a\u95f4\u589e\u5f3a\u8bbe\u8ba15\u79cd\u65b0\u9896\u7684\u6270\u52a8\u6280\u672f\uff0c\u5728\u7ed3\u6784\u878d\u5408\u5d4c\u5165\u4e0a\u64cd\u4f5c\u4ee5\u4fdd\u6301\u8bed\u4e49\u548c\u5173\u7cfb\u5c5e\u6027\u3002", "result": "\u5728Twitter2012\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5206\u6570\u6bd4\u6700\u4f73\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u7ea617.67%\uff0c\u5728Twitter2018\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u7ea615.57%\u3002", "conclusion": "SED-Aug\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u589e\u5f3a\u6709\u6548\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u4e8b\u4ef6\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.04322", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04322", "abs": "https://arxiv.org/abs/2509.04322", "authors": ["Haley Dozier", "Althea Henslee"], "title": "Characteristic Energy Behavior Profiling of Non-Residential Buildings", "comment": null, "summary": "Due to the threat of changing climate and extreme weather events, the\ninfrastructure of the United States Army installations is at risk. More than\never, climate resilience measures are needed to protect facility assets that\nsupport critical missions and help generate readiness. As most of the Army\ninstallations within the continental United States rely on commercial energy\nand water sources, resilience to the vulnerabilities within independent energy\nresources (electricity grids, natural gas pipelines, etc) along with a baseline\nunderstanding of energy usage within installations must be determined. This\npaper will propose a data-driven behavioral model to determine behavior\nprofiles of energy usage on installations. These profiles will be used 1) to\ncreate a baseline assessment of the impact of unexpected disruptions on energy\nsystems and 2) to benchmark future resiliency measures. In this methodology,\nindividual building behavior will be represented with models that can\naccurately analyze, predict, and cluster multimodal data collected from energy\nusage of non-residential buildings. Due to the nature of Army installation\nenergy usage data, similarly structured open access data will be used to\nillustrate this methodology.", "AI": {"tldr": "\u7f8e\u519b\u57fa\u5730\u57fa\u7840\u8bbe\u65bd\u9762\u4e34\u6c14\u5019\u53d8\u5316\u98ce\u9669\uff0c\u9700\u8981\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u80fd\u6e90\u4f7f\u7528\u884c\u4e3a\u6a21\u578b\u6765\u8bc4\u4f30\u5f39\u6027\u548c\u5efa\u7acb\u57fa\u51c6", "motivation": "\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u548c\u6781\u7aef\u5929\u6c14\u5a01\u80c1\uff0c\u4fdd\u62a4\u7f8e\u519b\u57fa\u5730\u7684\u5173\u952e\u8bbe\u65bd\u8d44\u4ea7\uff0c\u786e\u4fdd\u6218\u5907\u751f\u4ea7\u529b", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u5efa\u7acb\u80fd\u6e90\u4f7f\u7528\u884c\u4e3a\u6a21\u578b\uff0c\u8fdb\u884c\u5206\u6790\u3001\u9884\u6d4b\u548c\u805a\u7c7b\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u76f8\u4f3c\u7684\u516c\u5f00\u6570\u636e\u8fdb\u884c\u65b9\u6cd5\u8bed\u4f8b", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5206\u6790\u3001\u9884\u6d4b\u548c\u805a\u7c7b\u975e\u4f4f\u5b85\u5efa\u7b51\u80fd\u6e90\u4f7f\u7528\u6570\u636e\u7684\u884c\u4e3a\u6a21\u578b\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4e3a\u7f8e\u519b\u57fa\u5730\u80fd\u6e90\u7cfb\u7edf\u7684\u5f39\u6027\u8bc4\u4f30\u63d0\u4f9b\u57fa\u51c6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5f39\u6027\u63aa\u65bd\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2509.04370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04370", "abs": "https://arxiv.org/abs/2509.04370", "authors": ["Dor Cohen", "Inga Efrosman", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage", "comment": "5 pages, 3 figures", "summary": "First responders widely adopt body-worn cameras to document incident scenes\nand support post-event analysis. However, reviewing lengthy video footage is\nimpractical in time-critical situations. Effective situational awareness\ndemands a concise visual summary that can be quickly interpreted. This work\npresents a computer vision pipeline that transforms body-camera footage into\ninformative panoramic images summarizing the incident scene. Our method\nleverages monocular Simultaneous Localization and Mapping (SLAM) to estimate\ncamera trajectories and reconstruct the spatial layout of the environment. Key\nviewpoints are identified by clustering camera poses along the trajectory, and\nrepresentative frames from each cluster are selected. These frames are fused\ninto spatially coherent panoramic images using multi-frame stitching\ntechniques. The resulting summaries enable rapid understanding of complex\nenvironments and facilitate efficient decision-making and incident review.", "AI": {"tldr": "\u901a\u8fc7\u5355\u76eeSLAM\u548c\u805a\u7c7b\u6280\u672f\u5c06\u8eab\u4f53\u6444\u50cf\u5934\u62cd\u6444\u7684\u957f\u65f6\u95f4\u89c6\u9891\u8f6c\u6362\u4e3a\u7b80\u6d01\u7684\u5168\u666f\u56fe\u50cf\u6458\u8981\uff0c\u63d0\u9ad8\u5e94\u6025\u60c5\u51b5\u4e0b\u7684\u60c5\u51b5\u610f\u8bc6\u548c\u51b3\u7b56\u6548\u7387", "motivation": "\u89e3\u51b3\u5e94\u6025\u54e1\u5728\u65f6\u95f4\u538b\u529b\u4e0b\u5bf9\u957f\u65f6\u95f4\u89c6\u9891\u8d44\u6599\u8fdb\u884c\u5feb\u901f\u7406\u89e3\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u80fd\u591f\u5feb\u901f\u89e3\u91ca\u7684\u7b80\u6d01\u89c6\u89c9\u6458\u8981", "method": "\u4f7f\u7528\u5355\u76eeSLAM\u4f30\u8ba1\u6444\u50cf\u5934\u8f68\u8ff9\u548c\u73af\u5883\u7a7a\u95f4\u5e03\u5c40\uff0c\u901a\u8fc7\u805a\u7c7b\u8bc6\u522b\u5173\u952e\u89c6\u70b9\uff0c\u9009\u62e9\u4ee3\u8868\u6027\u5e27\uff0c\u91c7\u7528\u591a\u5e27\u7f29\u7ee3\u6280\u672f\u5408\u6210\u7a7a\u95f4\u4e00\u81f4\u7684\u5168\u666f\u56fe\u50cf", "result": "\u5f97\u5230\u80fd\u591f\u5feb\u901f\u7406\u89e3\u590d\u6742\u73af\u5883\u7684\u89c6\u89c9\u6458\u8981\uff0c\u652f\u6301\u9ad8\u6548\u51b3\u7b56\u548c\u4e8b\u4ef6\u56de\u987e", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e94\u6025\u54e1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9\u6458\u8981\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u51b5\u610f\u8bc6\u80fd\u529b\u548c\u4e8b\u6545\u5206\u6790\u6548\u7387"}}
{"id": "2509.04446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04446", "abs": "https://arxiv.org/abs/2509.04446", "authors": ["Kiymet Akdemir", "Jing Shi", "Kushal Kafle", "Brian Price", "Pinar Yanardag"], "title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image diffusion models have demonstrated significant capabilities to\ngenerate diverse and detailed visuals in various domains, and story\nvisualization is emerging as a particularly promising application. However, as\ntheir use in real-world creative domains increases, the need for providing\nenhanced control, refinement, and the ability to modify images post-generation\nin a consistent manner becomes an important challenge. Existing methods often\nlack the flexibility to apply fine or coarse edits while maintaining visual and\nnarrative consistency across multiple frames, preventing creators from\nseamlessly crafting and refining their visual stories. To address these\nchallenges, we introduce Plot'n Polish, a zero-shot framework that enables\nconsistent story generation and provides fine-grained control over story\nvisualizations at various levels of detail.", "AI": {"tldr": "Plot'n Polish\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u4e00\u81f4\u7684\u6545\u4e8b\u53ef\u89c6\u5316\u751f\u6210\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u63a7\u5236\u80fd\u529b\uff0c\u652f\u6301\u751f\u6210\u540e\u4fee\u6539\u5e76\u4fdd\u6301\u89c6\u89c9\u548c\u53d9\u4e8b\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u6545\u4e8b\u53ef\u89c6\u5316\u5e94\u7528\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u5728\u4fdd\u6301\u591a\u5e27\u89c6\u89c9\u548c\u53d9\u4e8b\u4e00\u81f4\u6027\u7684\u540c\u65f6\u8fdb\u884c\u7cbe\u7ec6\u6216\u7c97\u7565\u7684\u7f16\u8f91\uff0c\u9650\u5236\u4e86\u521b\u4f5c\u8005\u5bf9\u89c6\u89c9\u6545\u4e8b\u7684\u6d41\u7545\u521b\u4f5c\u548c\u7cbe\u70bc", "method": "\u63d0\u51fa\u4e86Plot'n Polish\u96f6\u6837\u672c\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7ec6\u8282\u5c42\u6b21\u4e0a\u5bf9\u6545\u4e8b\u53ef\u89c6\u5316\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u652f\u6301\u751f\u6210\u540e\u7684\u4e00\u81f4\u4fee\u6539", "result": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u591a\u5e27\u4e00\u81f4\u6027\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u65b9\u9762\u7684\u4e0d\u8db3", "conclusion": "Plot'n Polish\u4e3a\u6545\u4e8b\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u63a7\u5236\u548c\u7cbe\u70bc\u80fd\u529b\uff0c\u4f7f\u521b\u4f5c\u8005\u80fd\u591f\u65e0\u7f1d\u5730\u5236\u4f5c\u548c\u5b8c\u5584\u89c6\u89c9\u6545\u4e8b"}}
