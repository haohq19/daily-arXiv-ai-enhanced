{"id": "2510.11907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11907", "abs": "https://arxiv.org/abs/2510.11907", "authors": ["Blessing Agyei Kyem", "Neema Jakisa Owor", "Andrews Danyo", "Joshua Kofi Asamoah", "Eugene Denteh", "Tanner Muturi", "Anthony Dontoh", "Yaw Adu-Gyamfi", "Armstrong Aboah"], "title": "Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis", "comment": "This paper was accepted at ICCV 2025", "summary": "Traffic safety analysis requires complex video understanding to capture\nfine-grained behavioral patterns and generate comprehensive descriptions for\naccident prevention. In this work, we present a unique dual-model framework\nthat strategically utilizes the complementary strengths of VideoLLaMA and\nQwen2.5-VL through task-specific optimization to address this issue. The core\ninsight behind our approach is that separating training for captioning and\nvisual question answering (VQA) tasks minimizes task interference and allows\neach model to specialize more effectively. Experimental results demonstrate\nthat VideoLLaMA is particularly effective in temporal reasoning, achieving a\nCIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a\nVQA accuracy of 60.80\\%. Through extensive experiments on the WTS dataset, our\nmethod achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,\nplacing 10th on the challenge leaderboard. Ablation studies validate that our\nseparate training strategy outperforms joint training by 8.6\\% in VQA accuracy\nwhile maintaining captioning quality.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u6a21\u578b\u6846\u67b6\uff0c\u5206\u522b\u4f18\u5316VideoLLaMA\u548cQwen2.5-VL\u5728\u4ea4\u901a\u89c6\u9891\u5206\u6790\u4e2d\u7684captioning\u548cVQA\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u79bb\u8bad\u7ec3\u907f\u514d\u4efb\u52a1\u5e72\u6270\uff0c\u5728AI City Challenge Track 2\u4e2d\u83b7\u5f97\u7b2c10\u540d\u3002", "motivation": "\u4ea4\u901a\u89c6\u9891\u5b89\u5168\u5206\u6790\u9700\u8981\u7ec6\u7c92\u5ea6\u884c\u4e3a\u6a21\u5f0f\u8bc6\u522b\u548c\u5168\u9762\u63cf\u8ff0\u751f\u6210\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u6a21\u578b\u6846\u67b6\uff0c\u5206\u522b\u8bad\u7ec3VideoLLaMA\u8d1f\u8d23captioning\u4efb\u52a1\uff0cQwen2.5-VL\u8d1f\u8d23VQA\u4efb\u52a1\uff0c\u5229\u7528\u5404\u81ea\u4f18\u52bf\u5b9e\u73b0\u4efb\u52a1\u4e13\u4e1a\u5316\u3002", "result": "VideoLLaMA\u5728\u65f6\u95f4\u63a8\u7406\u4e0aCIDEr\u5f97\u52061.1001\uff0cQwen2.5-VL\u5728\u89c6\u89c9\u7406\u89e3\u4e0aVQA\u51c6\u786e\u738760.80%\uff0c\u5728WTS\u6570\u636e\u96c6\u4e0aS2\u5f97\u520645.7572\uff0c\u6392\u540d\u7b2c10\u3002", "conclusion": "\u5206\u79bb\u8bad\u7ec3\u7b56\u7565\u6bd4\u8054\u5408\u8bad\u7ec3\u5728VQA\u51c6\u786e\u7387\u4e0a\u63d0\u53478.6%\uff0c\u540c\u65f6\u4fdd\u6301captioning\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1\u4e13\u4e1a\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.12061", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12061", "abs": "https://arxiv.org/abs/2510.12061", "authors": ["Yiheng Chen", "Lingyao Li", "Zihui Ma", "Qikai Hu", "Yilun Zhu", "Min Deng", "Runlong Yu"], "title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response", "comment": null, "summary": "Effective disaster response is essential for safeguarding lives and property.\nExisting statistical approaches often lack semantic context, generalize poorly\nacross events, and offer limited interpretability. While Large language models\n(LLMs) provide few-shot generalization, they remain text-bound and blind to\ngeography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)\nthat grounds LLM agents in structured earth data. Starting from raw wildfire\ndetections, GAL automatically retrieves and integrates infrastructure,\ndemographic, terrain, and weather information from external geodatabases,\nassembling them into a concise, unit-annotated perception script. This enriched\ncontext enables agents to produce evidence-based resource-allocation\nrecommendations (e.g., personnel assignments, budget allocations), further\nreinforced by historical analogs and daily change signals for incremental\nupdates. We evaluate the framework in real wildfire scenarios across multiple\nLLM models, showing that geospatially grounded agents can outperform baselines.\nThe proposed framework can generalize to other hazards such as floods and\nhurricanes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5730\u7406\u7a7a\u95f4\u611f\u77e5\u5c42(GAL)\uff0c\u5c06LLM\u667a\u80fd\u4f53\u4e0e\u7ed3\u6784\u5316\u5730\u7403\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u7840\u8bbe\u65bd\u3001\u4eba\u53e3\u3001\u5730\u5f62\u548c\u5929\u6c14\u4fe1\u606f\u6765\u6539\u8fdb\u707e\u5bb3\u54cd\u5e94\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u7edf\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\u3001\u8de8\u4e8b\u4ef6\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u800cLLM\u867d\u7136\u5177\u6709\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u6587\u672c\u4e14\u5bf9\u5730\u7406\u4fe1\u606f\u4e0d\u654f\u611f\u3002", "method": "\u4ece\u539f\u59cb\u91ce\u706b\u68c0\u6d4b\u5f00\u59cb\uff0cGAL\u81ea\u52a8\u4ece\u5916\u90e8\u5730\u7406\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u5e76\u6574\u5408\u57fa\u7840\u8bbe\u65bd\u3001\u4eba\u53e3\u7edf\u8ba1\u3001\u5730\u5f62\u548c\u5929\u6c14\u4fe1\u606f\uff0c\u5c06\u5176\u7ec4\u88c5\u6210\u7b80\u6d01\u7684\u5e26\u5355\u4f4d\u6807\u6ce8\u7684\u611f\u77e5\u811a\u672c\u3002", "result": "\u5728\u771f\u5b9e\u91ce\u706b\u573a\u666f\u4e2d\u8bc4\u4f30\u8868\u660e\uff0c\u5730\u7406\u7a7a\u95f4\u63a5\u5730\u7684\u667a\u80fd\u4f53\u80fd\u591f\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u4ea7\u751f\u57fa\u4e8e\u8bc1\u636e\u7684\u8d44\u6e90\u5206\u914d\u5efa\u8bae\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4ee5\u63a8\u5e7f\u5230\u6d2a\u6c34\u3001\u98d3\u98ce\u7b49\u5176\u4ed6\u707e\u5bb3\u7c7b\u578b\uff0c\u4e3a\u707e\u5bb3\u54cd\u5e94\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2510.11856", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11856", "abs": "https://arxiv.org/abs/2510.11856", "authors": ["Aurelie Leribaux", "Rafael Oyamada", "Johannes De Smedt", "Zahra Dasht Bozorgi", "Artem Polyvyanyy", "Jochen De Weerdt"], "title": "Actor-Enriched Time Series Forecasting of Process Performance", "comment": "Accepted at ICPM 2025", "summary": "Predictive Process Monitoring (PPM) is a key task in Process Mining that aims\nto predict future behavior, outcomes, or performance indicators. Accurate\nprediction of the latter is critical for proactive decision-making. Given that\nprocesses are often resource-driven, understanding and incorporating actor\nbehavior in forecasting is crucial. Although existing research has incorporated\naspects of actor behavior, its role as a time-varying signal in PPM remains\nlimited. This study investigates whether incorporating actor behavior\ninformation, modeled as time series, can improve the predictive performance of\nthroughput time (TT) forecasting models. Using real-life event logs, we\nconstruct multivariate time series that include TT alongside actor-centric\nfeatures, i.e., actor involvement, the frequency of continuation, interruption,\nand handover behaviors, and the duration of these behaviors. We train and\ncompare several models to study the benefits of adding actor behavior. The\nresults show that actor-enriched models consistently outperform baseline\nmodels, which only include TT features, in terms of RMSE, MAE, and R2. These\nfindings demonstrate that modeling actor behavior over time and incorporating\nthis information into forecasting models enhances performance indicator\npredictions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5c06\u53c2\u4e0e\u8005\u884c\u4e3a\u4fe1\u606f\u5efa\u6a21\u4e3a\u65f6\u95f4\u5e8f\u5217\u662f\u5426\u80fd\u63d0\u9ad8\u541e\u5410\u65f6\u95f4\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002\u901a\u8fc7\u6784\u5efa\u5305\u542b\u53c2\u4e0e\u8005\u884c\u4e3a\u7279\u5f81\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u7814\u7a76\u53d1\u73b0\u52a0\u5165\u53c2\u4e0e\u8005\u884c\u4e3a\u4fe1\u606f\u7684\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u541e\u5410\u65f6\u95f4\u7279\u5f81\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5728\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u4e2d\uff0c\u867d\u7136\u73b0\u6709\u7814\u7a76\u5df2\u8003\u8651\u53c2\u4e0e\u8005\u884c\u4e3a\uff0c\u4f46\u5176\u4f5c\u4e3a\u65f6\u53d8\u4fe1\u53f7\u7684\u4f5c\u7528\u4ecd\u6709\u9650\u3002\u7531\u4e8e\u6d41\u7a0b\u901a\u5e38\u7531\u8d44\u6e90\u9a71\u52a8\uff0c\u7406\u89e3\u548c\u6574\u5408\u53c2\u4e0e\u8005\u884c\u4e3a\u5bf9\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u6784\u5efa\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff0c\u5305\u542b\u541e\u5410\u65f6\u95f4\u53ca\u53c2\u4e0e\u8005\u76f8\u5173\u7279\u5f81\uff08\u53c2\u4e0e\u7a0b\u5ea6\u3001\u7ee7\u7eed/\u4e2d\u65ad/\u4ea4\u63a5\u884c\u4e3a\u7684\u9891\u7387\u548c\u6301\u7eed\u65f6\u95f4\uff09\u3002\u8bad\u7ec3\u5e76\u6bd4\u8f83\u591a\u4e2a\u6a21\u578b\uff0c\u7814\u7a76\u6dfb\u52a0\u53c2\u4e0e\u8005\u884c\u4e3a\u7684\u597d\u5904\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u52a0\u5165\u53c2\u4e0e\u8005\u884c\u4e3a\u7684\u6a21\u578b\u5728RMSE\u3001MAE\u548cR2\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u4ec5\u5305\u542b\u541e\u5410\u65f6\u95f4\u7279\u5f81\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5c06\u53c2\u4e0e\u8005\u884c\u4e3a\u5efa\u6a21\u4e3a\u65f6\u95f4\u5e8f\u5217\u5e76\u6574\u5408\u5230\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u6307\u6807\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.11903", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11903", "abs": "https://arxiv.org/abs/2510.11903", "authors": ["Rizal Fathony", "Igor Melnyk", "Owen Reinert", "Nam H. Nguyen", "Daniele Rosa", "C. Bayan Bruss"], "title": "Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks", "comment": null, "summary": "User event modeling plays a central role in many machine learning\napplications, with use cases spanning e-commerce, social media, finance,\ncybersecurity, and other domains. User events can be broadly categorized into\npersonal events, which involve individual actions, and relational events, which\ninvolve interactions between two users. These two types of events are typically\nmodeled separately, using sequence-based methods for personal events and\ngraph-based methods for relational events. Despite the need to capture both\nevent types in real-world systems, prior work has rarely considered them\ntogether. This is often due to the convenient simplification that user behavior\ncan be adequately represented by a single formalization, either as a sequence\nor a graph. To address this gap, there is a need for public datasets and\nprediction tasks that explicitly incorporate both personal and relational\nevents. In this work, we introduce a collection of such datasets, propose a\nunified formalization, and empirically show that models benefit from\nincorporating both event types. Our results also indicate that current methods\nleave a notable room for improvements. We release these resources to support\nfurther research in unified user event modeling and encourage progress in this\ndirection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u540c\u65f6\u5efa\u6a21\u4e2a\u4eba\u4e8b\u4ef6\u548c\u5173\u7cfb\u4e8b\u4ef6\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u96c6\uff0c\u5b9e\u8bc1\u8868\u660e\u7ed3\u5408\u4e24\u79cd\u4e8b\u4ef6\u7c7b\u578b\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7528\u6237\u4e8b\u4ef6\u5305\u542b\u4e2a\u4eba\u4e8b\u4ef6\u548c\u5173\u7cfb\u4e8b\u4ef6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5b83\u4eec\u5206\u5f00\u5efa\u6a21\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5904\u7406\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165\u5305\u542b\u4e24\u79cd\u4e8b\u4ef6\u7c7b\u578b\u7684\u6570\u636e\u96c6\u96c6\u5408\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u7ed3\u5408\u4e24\u79cd\u4e8b\u4ef6\u7c7b\u578b\u7684\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u540c\u65f6\u8003\u8651\u4e2a\u4eba\u4e8b\u4ef6\u548c\u5173\u7cfb\u4e8b\u4ef6\u7684\u6a21\u578b\u6027\u80fd\u66f4\u597d\uff0c\u4e14\u5f53\u524d\u65b9\u6cd5\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u7edf\u4e00\u7528\u6237\u4e8b\u4ef6\u5efa\u6a21\u65b9\u6cd5\uff0c\u53d1\u5e03\u7684\u6570\u636e\u96c6\u548c\u6846\u67b6\u5c06\u652f\u6301\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.12190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12190", "abs": "https://arxiv.org/abs/2510.12190", "authors": ["Shingo Yokoi", "Kento Sasaki", "Yu Yamaguchi"], "title": "Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos", "comment": "2nd Place Winner, ICCV 2025 2COOOL Competition", "summary": "Recent advances in end-to-end (E2E) autonomous driving have been enabled by\ntraining on diverse large-scale driving datasets, yet autonomous driving models\nstill struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark\ntargets this gap by encouraging hazard understanding beyond closed taxonomies,\nand the 2COOOL challenge extends it to generating human-interpretable incident\nreports. We present a hierarchical reasoning framework for incident report\ngeneration from dashcam videos that integrates frame-level captioning, incident\nframe detection, and fine-grained reasoning within vision-language models\n(VLMs). We further improve factual accuracy and readability through model\nensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL\nopen leaderboard, our method ranks 2nd among 29 teams and achieves the best\nCIDEr-D score, producing accurate and coherent incident narratives. These\nresults indicate that hierarchical reasoning with VLMs is a promising direction\nfor accident analysis and for broader understanding of safety-critical traffic\nevents. The implementation and code are available at\nhttps://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u89c6\u9891\u751f\u6210\u4e8b\u6545\u62a5\u544a\u7684\u5206\u5c42\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u5e27\u7ea7\u63cf\u8ff0\u3001\u4e8b\u6545\u5e27\u68c0\u6d4b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\uff0c\u57282COOOL\u6311\u6218\u4e2d\u6392\u540d\u7b2c2\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u95ee\u9898\uff0c\u7279\u522b\u662f\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u4e8b\u6545\u62a5\u544a\u3002", "method": "\u5206\u5c42\u63a8\u7406\u6846\u67b6\uff0c\u5305\u62ec\u5e27\u7ea7\u63cf\u8ff0\u3001\u4e8b\u6545\u5e27\u68c0\u6d4b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\uff0c\u4ee5\u53ca\u6a21\u578b\u96c6\u6210\u548c\u76f2A/B\u8bc4\u5206\u9009\u62e9\u534f\u8bae\u3002", "result": "\u5728\u5b98\u65b92COOOL\u5f00\u653e\u6392\u884c\u699c\u4e2d\u6392\u540d\u7b2c2\uff08\u517129\u4e2a\u56e2\u961f\uff09\uff0c\u83b7\u5f97\u6700\u4f73CIDEr-D\u5206\u6570\uff0c\u751f\u6210\u51c6\u786e\u8fde\u8d2f\u7684\u4e8b\u6545\u53d9\u8ff0\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5c42\u63a8\u7406\u662f\u4e8b\u6545\u5206\u6790\u548c\u5b89\u5168\u5173\u952e\u4ea4\u901a\u4e8b\u4ef6\u7406\u89e3\u7684\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2510.12560", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12560", "abs": "https://arxiv.org/abs/2510.12560", "authors": ["Xiaoji Zheng", "Ziyuan Yang", "Yanhao Chen", "Yuhang Peng", "Yuanrong Tang", "Gengyuan Liu", "Bokui Chen", "Jiangtao Gong"], "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving", "comment": "18 pages, 17 figures", "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.", "AI": {"tldr": "\u63d0\u51faCoIRL-AD\u6846\u67b6\uff0c\u901a\u8fc7\u7ade\u4e89\u673a\u5236\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd", "motivation": "\u6a21\u4eff\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u4f4e\u4e14\u6536\u655b\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf", "method": "\u7ade\u4e89\u5f0f\u53cc\u7b56\u7565\u6846\u67b6\uff0c\u8ba9IL\u548cRL\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u4e2d\u4ea4\u4e92\uff0c\u901a\u8fc7\u7ade\u4e89\u673a\u5236\u4fc3\u8fdb\u77e5\u8bc6\u4ea4\u6362\u5e76\u907f\u514d\u68af\u5ea6\u51b2\u7a81", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u78b0\u649e\u7387\u964d\u4f4e18%\uff0c\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a", "conclusion": "CoIRL-AD\u6846\u67b6\u6709\u6548\u7ed3\u5408IL\u548cRL\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd"}}
{"id": "2510.12167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12167", "abs": "https://arxiv.org/abs/2510.12167", "authors": ["Minghan Wang", "Thuy-Trang Vu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "Towards Inference-time Scaling for Continuous Space Reasoning", "comment": null, "summary": "Inference-time scaling through multiple sample generation in combination with\nProcess- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective\nfor text-based reasoning in large language models. This paper investigates\nwhether such established techniques can be successfully adapted to reasoning in\nthe continuous space, using COCONUT (Hao et al. 2024) continuous space\nreasoning LM as the backbone. We demonstrate the feasibility of generating\ndiverse reasoning paths through dropout-based sampling. Our Pass@N analysis on\nthe generated samples reveals the potential that could enable a significant\ngain in performance akin to observed gain in the discrete space. However, we\nhighlight unique challenges faced for materializing this gain in the continuous\nthought space. In particular, working recipes for data generation and training\nPRM and ORM models in the discrete space unlocks only marginal improvements in\nthe continuous space. Through probing various aspects including geometric\nproperties and trajectory dynamics we identify the underlying reasons that\nprevent effective discrimination between correct and incorrect reasoning\n(essential for the functioning of PRM and ORM). Our findings reveal that\ncurrent limitations stem from the absence of key inductive biases in continuous\nthought representations. We argue that the training frameworks for continuous\nreasoning LMs require not only to optimize for accuracy but also to explicitly\nincorporate inductive biases that could be utilized during inference-time for\ndiscrimination of correct and incorrect thoughts.\\footnote{Our code and data\nwill be publicly available.}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u79bb\u6563\u63a8\u7406\u7a7a\u95f4\u7684\u63a8\u7406\u6280\u672f\uff08\u5982\u591a\u6837\u672c\u751f\u6210\u548cPRM/ORM\u91cd\u6392\u5e8f\uff09\u5e94\u7528\u4e8e\u8fde\u7eed\u7a7a\u95f4\u63a8\u7406\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u867d\u7136\u80fd\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4f46\u7531\u4e8e\u8fde\u7eed\u601d\u7ef4\u8868\u793a\u7f3a\u4e4f\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\uff0c\u5bfc\u81f4PRM/ORM\u6a21\u578b\u96be\u4ee5\u6709\u6548\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u3002", "motivation": "\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u5c06\u79bb\u6563\u63a8\u7406\u7a7a\u95f4\u4e2d\u8bc1\u660e\u6709\u6548\u7684\u63a8\u7406\u6280\u672f\uff08\u591a\u6837\u672c\u751f\u6210\u548cPRM/ORM\u91cd\u6392\u5e8f\uff09\u6210\u529f\u9002\u914d\u5230\u8fde\u7eed\u7a7a\u95f4\u63a8\u7406\u4e2d\uff0c\u4ee5\u63d0\u5347\u8fde\u7eed\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528COCONUT\u8fde\u7eed\u7a7a\u95f4\u63a8\u7406LM\u4f5c\u4e3a\u9aa8\u5e72\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8edropout\u7684\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5c1d\u8bd5\u5728\u8fde\u7eed\u601d\u7ef4\u7a7a\u95f4\u4e2d\u8bad\u7ec3PRM\u548cORM\u6a21\u578b\u8fdb\u884c\u91cd\u6392\u5e8f\u3002", "result": "Pass@N\u5206\u6790\u663e\u793a\u751f\u6210\u6837\u672c\u5177\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u7684\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2dPRM\u548cORM\u6a21\u578b\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\u3002\u7814\u7a76\u53d1\u73b0\u8fde\u7eed\u601d\u7ef4\u8868\u793a\u7f3a\u4e4f\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\uff0c\u5bfc\u81f4\u96be\u4ee5\u6709\u6548\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u3002", "conclusion": "\u8fde\u7eed\u63a8\u7406LM\u7684\u8bad\u7ec3\u6846\u67b6\u4e0d\u4ec5\u9700\u8981\u4f18\u5316\u51c6\u786e\u6027\uff0c\u8fd8\u5e94\u660e\u786e\u7eb3\u5165\u53ef\u5728\u63a8\u7406\u65f6\u7528\u4e8e\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u601d\u7ef4\u7684\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2510.12185", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12185", "abs": "https://arxiv.org/abs/2510.12185", "authors": ["Jiayu Yao", "Shenghua Liu", "Yiwei Wang", "Rundong Cheng", "Lingrui Mei", "Baolong Bi", "Zhen Xiong", "Xueqi Cheng"], "title": "Not in Sync: Unveiling Temporal Bias in Audio Chat Models", "comment": null, "summary": "Large Audio Language Models (LALMs) are increasingly applied to audio\nunderstanding and multimodal reasoning, yet their ability to locate when events\noccur remains underexplored. We present the first systematic study of temporal\nbias in LALMs, revealing a key limitation in their timestamp prediction. For\nexample, when asked \"At which second does the lecturer introduce the key\nformula?\", models often predict timestamps that are consistently earlier or\nlater than the ground truth. Through controlled experiments on timestamped\ndatasets, we find that temporal bias (i) is prevalent across datasets and\nmodels, (ii) increases with audio length - even accumulating to tens of seconds\nin extended recordings, and (iii) varies across event types and positions. We\nquantify this effect with the Temporal Bias Index (TBI), measuring systematic\nmisalignment in predicted event timings, and complement it with a visualization\nframework. Our findings highlight a fundamental limitation in current LALMs and\ncall for the development of temporally robust architectures.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u65f6\u95f4\u504f\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u9884\u6d4b\u4e8b\u4ef6\u65f6\u95f4\u6233\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u504f\u5dee\u968f\u97f3\u9891\u957f\u5ea6\u589e\u52a0\u800c\u7d2f\u79ef\uff0c\u5e76\u63d0\u51faTemporal Bias Index\u91cf\u5316\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5b9a\u4f4d\u4e8b\u4ef6\u53d1\u751f\u65f6\u95f4\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u65f6\u95f4\u6233\u9884\u6d4b\u4e2d\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u5e26\u65f6\u95f4\u6233\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u91cf\u5316\u5206\u6790\u65f6\u95f4\u504f\u5dee\u73b0\u8c61\uff0c\u63d0\u51faTemporal Bias Index(TBI)\u6765\u6d4b\u91cf\u9884\u6d4b\u4e8b\u4ef6\u65f6\u95f4\u4e0e\u771f\u5b9e\u65f6\u95f4\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5e76\u5f00\u53d1\u53ef\u89c6\u5316\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u65f6\u95f4\u504f\u5dee(i)\u5728\u5404\u7c7b\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c(ii)\u968f\u97f3\u9891\u957f\u5ea6\u589e\u52a0\u800c\u7d2f\u79ef\uff0c\u5728\u957f\u5f55\u97f3\u4e2d\u53ef\u8fbe\u6570\u5341\u79d2\uff0c(iii)\u5728\u4e0d\u540c\u4e8b\u4ef6\u7c7b\u578b\u548c\u4f4d\u7f6e\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u57fa\u672c\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u65f6\u95f4\u9c81\u68d2\u6027\u7684\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2510.12144", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12144", "abs": "https://arxiv.org/abs/2510.12144", "authors": ["Ali Parsaee", "Bei Jiang", "Zachary Friggstad", "Russell Greiner"], "title": "Budget-constrained Active Learning to Effectively De-censor Survival Data", "comment": null, "summary": "Standard supervised learners attempt to learn a model from a labeled dataset.\nGiven a small set of labeled instances, and a pool of unlabeled instances, a\nbudgeted learner can use its given budget to pay to acquire the labels of some\nunlabeled instances, which it can then use to produce a model. Here, we explore\nbudgeted learning in the context of survival datasets, which include (right)\ncensored instances, where we know only a lower bound on an instance's\ntime-to-event. Here, that learner can pay to (partially) label a censored\ninstance -- e.g., to acquire the actual time for an instance [perhaps go from\n(3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about\none more year, so go from (3 yr, censored) to either (4 yr, censored) or\nperhaps (3.2 yr, uncensored)]. This serves as a model of real world data\ncollection, where follow-up with censored patients does not always lead to\nuncensoring, and how much information is given to the learner model during data\ncollection is a function of the budget and the nature of the data itself. We\nprovide both experimental and theoretical results for how to apply\nstate-of-the-art budgeted learning algorithms to survival data and the\nrespective limitations that exist in doing so. Our approach provides bounds and\ntime complexity asymptotically equivalent to the standard active learning\nmethod BatchBALD. Moreover, empirical analysis on several survival tasks show\nthat our model performs better than other potential approaches on several\nbenchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u751f\u5b58\u6570\u636e\u96c6\u7684\u9884\u7b97\u5b66\u4e60\u95ee\u9898\uff0c\u63a2\u7d22\u4e86\u5982\u4f55\u5728\u5305\u542b\u53f3\u5220\u5931\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u901a\u8fc7\u9884\u7b97\u6765\u83b7\u53d6\u90e8\u5206\u6807\u7b7e\u4fe1\u606f\u4ee5\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5b8c\u6574\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u751f\u5b58\u6570\u636e\u5f80\u5f80\u5305\u542b\u53f3\u5220\u5931\u5b9e\u4f8b\uff0c\u53ea\u80fd\u83b7\u5f97\u4e8b\u4ef6\u65f6\u95f4\u7684\u4e0b\u754c\u3002\u9884\u7b97\u5b66\u4e60\u5141\u8bb8\u5728\u6709\u9650\u9884\u7b97\u4e0b\u83b7\u53d6\u66f4\u591a\u6807\u7b7e\u4fe1\u606f\uff0c\u6a21\u62df\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u968f\u8bbf\u60a3\u8005\u6570\u636e\u6536\u96c6\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u5c06\u6700\u5148\u8fdb\u7684\u9884\u7b97\u5b66\u4e60\u7b97\u6cd5\u5e94\u7528\u4e8e\u751f\u5b58\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5141\u8bb8\u652f\u4ed8\u9884\u7b97\u6765\u90e8\u5206\u6807\u6ce8\u5220\u5931\u5b9e\u4f8b\uff0c\u6bd4\u5982\u4ece(3\u5e74\uff0c\u5220\u5931)\u66f4\u65b0\u4e3a(7.2\u5e74\uff0c\u975e\u5220\u5931)\u6216\u5176\u4ed6\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u751f\u5b58\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6f5c\u5728\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e0e\u6807\u51c6\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5BatchBALD\u76f8\u5f53\u7684\u8fb9\u754c\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u5b58\u6570\u636e\u96c6\u7684\u9884\u7b97\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u7684\u73b0\u5b9e\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.12422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12422", "abs": "https://arxiv.org/abs/2510.12422", "authors": ["Jialong Zuo", "Yongtai Deng", "Lingdong Kong", "Jingkang Yang", "Rui Jin", "Yiwei Zhang", "Nong Sang", "Liang Pan", "Ziwei Liu", "Changxin Gao"], "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding", "comment": "NeurIPS-2025 Accepted Paper", "summary": "Recent studies have shown that agent-based systems leveraging large language\nmodels (LLMs) for key information retrieval and integration have emerged as a\npromising approach for long video understanding. However, these systems face\ntwo major challenges. First, they typically perform modeling and reasoning on\nindividual frames, struggling to capture the temporal context of consecutive\nframes. Second, to reduce the cost of dense frame-level captioning, they adopt\nsparse frame sampling, which risks discarding crucial information. To overcome\nthese limitations, we propose VideoLucy, a deep memory backtracking framework\nfor long video understanding. Inspired by the human recollection process from\ncoarse to fine, VideoLucy employs a hierarchical memory structure with\nprogressive granularity. This structure explicitly defines the detail level and\ntemporal scope of memory at different hierarchical depths. Through an\nagent-based iterative backtracking mechanism, VideoLucy systematically mines\nvideo-wide, question-relevant deep memories until sufficient information is\ngathered to provide a confident answer. This design enables effective temporal\nunderstanding of consecutive frames while preserving critical details. In\naddition, we introduce EgoMem, a new benchmark for long video understanding.\nEgoMem is designed to comprehensively evaluate a model's ability to understand\ncomplex events that unfold over time and capture fine-grained details in\nextremely long videos. Extensive experiments demonstrate the superiority of\nVideoLucy. Built on open-source models, VideoLucy significantly outperforms\nstate-of-the-art methods on multiple long video understanding benchmarks,\nachieving performance even surpassing the latest proprietary models such as\nGPT-4o. Our code and dataset will be made publicly at\nhttps://videolucy.github.io", "AI": {"tldr": "VideoLucy\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u8bb0\u5fc6\u56de\u6eaf\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u65f6\u95f4\u4e0a\u4e0b\u6587\u4e22\u5931\u548c\u5173\u952e\u4fe1\u606f\u9057\u6f0f\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\u548c\u8fed\u4ee3\u56de\u6eaf\u673a\u5236\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u5728\u5355\u4e2a\u5e27\u4e0a\u8fdb\u884c\u5efa\u6a21\u548c\u63a8\u7406\uff0c\u96be\u4ee5\u6355\u6349\u8fde\u7eed\u5e27\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff1b2) \u4e3a\u964d\u4f4e\u5bc6\u96c6\u5e27\u7ea7\u6807\u6ce8\u6210\u672c\u800c\u91c7\u7528\u7a00\u758f\u5e27\u91c7\u6837\uff0c\u53ef\u80fd\u4e22\u5931\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51faVideoLucy\u6846\u67b6\uff0c\u53d7\u4eba\u7c7b\u4ece\u7c97\u5230\u7ec6\u7684\u56de\u5fc6\u8fc7\u7a0b\u542f\u53d1\uff0c\u91c7\u7528\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\uff0c\u5728\u4e0d\u540c\u5c42\u6b21\u6df1\u5ea6\u5b9a\u4e49\u8bb0\u5fc6\u7684\u7ec6\u8282\u7ea7\u522b\u548c\u65f6\u95f4\u8303\u56f4\u3002\u901a\u8fc7\u57fa\u4e8e\u4ee3\u7406\u7684\u8fed\u4ee3\u56de\u6eaf\u673a\u5236\uff0c\u7cfb\u7edf\u6316\u6398\u89c6\u9891\u8303\u56f4\u5185\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u6df1\u5ea6\u8bb0\u5fc6\uff0c\u76f4\u5230\u6536\u96c6\u8db3\u591f\u4fe1\u606f\u63d0\u4f9b\u53ef\u9760\u7b54\u6848\u3002", "result": "\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8e\u5f00\u6e90\u6a21\u578b\u7684VideoLucy\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6027\u80fd\u751a\u81f3\u8d85\u8fc7GPT-4o\u7b49\u6700\u65b0\u4e13\u6709\u6a21\u578b\u3002\u540c\u65f6\u63d0\u51fa\u4e86EgoMem\u65b0\u57fa\u51c6\u6765\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5bf9\u590d\u6742\u65f6\u95f4\u4e8b\u4ef6\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "VideoLucy\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\u548c\u8fed\u4ee3\u56de\u6eaf\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u6355\u6349\u548c\u5173\u952e\u4fe1\u606f\u4fdd\u7559\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.12493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12493", "abs": "https://arxiv.org/abs/2510.12493", "authors": ["An Zhao", "Piaopiao Yu", "Zhe Zhu", "Mingqiang Wei"], "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring", "comment": null, "summary": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene\nreconstruction.However, reconstructing high-quality 3D scenes from\nmotion-blurred images caused by camera motion poses a significant challenge.The\nperformance of existing 3DGS-based deblurring methods are limited due to their\ninherent mechanisms, such as extreme dependence on the accuracy of camera poses\nand inability to effectively control erroneous Gaussian primitives\ndensification caused by motion blur.To solve these problems, we introduce a\nnovel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D\nscenes from motion-blurred images.BSGS contains two stages. First, Camera Pose\nRefinement roughly optimizes camera poses to reduce motion-induced distortions.\nSecond, with fixed rough camera poses, Global RigidTransformation further\ncorrects motion-induced blur distortions.To alleviate multi-subframe gradient\nconflicts, we propose a subframe gradient aggregation strategy to optimize both\nstages.Furthermore, a space-time bi-stage optimization strategy is introduced\nto dynamically adjust primitive densification thresholds and prevent premature\nnoisy Gaussian generation in blurred regions. Comprehensive experiments verify\nthe effectiveness of our proposed deblurring method and show its superiority\nover the state of the arts.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u9636\u6bb53D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u89e3\u51b3\u4ece\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u91cd\u5efa3D\u573a\u666f\u7684\u6311\u6218\uff0c\u901a\u8fc7\u76f8\u673a\u4f4d\u59ff\u4f18\u5316\u548c\u5168\u5c40\u521a\u6027\u53d8\u6362\u6765\u6821\u6b63\u8fd0\u52a8\u6a21\u7cca\u5931\u771f\u3002", "motivation": "\u73b0\u67093DGS\u53bb\u6a21\u7cca\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\uff0c\u5bf9\u76f8\u673a\u4f4d\u59ff\u7cbe\u5ea6\u4f9d\u8d56\u8fc7\u9ad8\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u63a7\u5236\u8fd0\u52a8\u6a21\u7cca\u5bfc\u81f4\u7684\u9519\u8bef\u9ad8\u65af\u57fa\u5143\u5bc6\u96c6\u5316\u95ee\u9898\u3002", "method": "\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7c97\u7565\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u51cf\u5c11\u8fd0\u52a8\u5931\u771f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u56fa\u5b9a\u7c97\u7565\u4f4d\u59ff\uff0c\u901a\u8fc7\u5168\u5c40\u521a\u6027\u53d8\u6362\u8fdb\u4e00\u6b65\u6821\u6b63\u6a21\u7cca\u5931\u771f\u3002\u91c7\u7528\u5b50\u5e27\u68af\u5ea6\u805a\u5408\u7b56\u7565\u548c\u65f6\u7a7a\u53cc\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u53bb\u6a21\u7cca\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u9636\u6bb53D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u80fd\u591f\u4ece\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u51c6\u786e\u91cd\u5efa3D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.12328", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12328", "abs": "https://arxiv.org/abs/2510.12328", "authors": ["Kiattikun Chobtham", "Kanoksri Sarinnapakorn", "Kritanai Torsri", "Prattana Deeprasertkul", "Jirawan Kamma"], "title": "Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand", "comment": null, "summary": "Accurate rainfall forecasting, particularly for extreme events, remains a\nsignificant challenge in climatology and the Earth system. This paper presents\nnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-value\nanalysis techniques to improve gauge-station rainfall predictions across\nThailand. The model leverages a graph-structured representation of gauge\nstations to capture complex spatiotemporal patterns, and it offers\nexplainability through teleconnections. We preprocess relevant climate indices\nthat potentially influence regional rainfall. The proposed Graph Attention\nNetwork with Long Short-Term Memory (Attention-LSTM) applies the attention\nmechanism using initial edge features derived from simple\norographic-precipitation physics formulation. The embeddings are subsequently\nprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold\n(POT) mapping using the novel Spatial Season-aware Generalized Pareto\nDistribution (GPD) method, which overcomes limitations of traditional\nmachine-learning models. Experiments demonstrate that our method outperforms\nwell-established baselines across most regions, including areas prone to\nextremes, and remains strongly competitive with the state of the art. Compared\nwith the operational forecasting system SEAS5, our real-world application\nimproves extreme-event prediction and offers a practical enhancement to produce\nfine-resolution maps that support decision-making in long-term water\nmanagement.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6781\u503c\u5206\u6790\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6cf0\u56fd\u96e8\u91cf\u7ad9\u7684\u964d\u96e8\u9884\u6d4b\uff0c\u7279\u522b\u9488\u5bf9\u6781\u7aef\u4e8b\u4ef6\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u964d\u96e8\u7279\u522b\u662f\u6781\u7aef\u4e8b\u4ef6\u5728\u6c14\u5019\u5b66\u548c\u5730\u7403\u7cfb\u7edf\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u6539\u8fdb\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u4e0eLSTM\u7ed3\u5408\uff0c\u6784\u5efa\u96e8\u91cf\u7ad9\u56fe\u7ed3\u6784\u6355\u6349\u65f6\u7a7a\u6a21\u5f0f\uff0c\u91c7\u7528\u57fa\u4e8e\u5730\u5f62\u964d\u6c34\u7269\u7406\u7684\u8fb9\u7f18\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u5b63\u8282\u611f\u77e5GPD\u65b9\u6cd5\u8fdb\u884c\u6781\u503c\u5206\u6790\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u533a\u57df\u5305\u62ec\u6781\u7aef\u6613\u53d1\u533a\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u4e0eSEAS5\u64cd\u4f5c\u9884\u62a5\u7cfb\u7edf\u76f8\u6bd4\u6539\u8fdb\u4e86\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\uff0c\u63d0\u4f9b\u652f\u6301\u957f\u671f\u6c34\u8d44\u6e90\u7ba1\u7406\u7684\u7cbe\u7ec6\u5206\u8fa8\u7387\u5730\u56fe\u3002", "conclusion": "\u63d0\u51fa\u7684\u7269\u7406\u4fe1\u606f\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u6781\u503c\u5206\u6790\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u5347\u964d\u96e8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u6781\u7aef\u4e8b\u4ef6\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u6c34\u8d44\u6e90\u7ba1\u7406\u51b3\u7b56\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.12343", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.12343", "abs": "https://arxiv.org/abs/2510.12343", "authors": ["Donghwan Rho", "Sieun Seo", "Hyewon Sung", "Chohong Min", "Ernest K. Ryu"], "title": "Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models", "comment": "34 pages", "summary": "As users increasingly interact with large language models (LLMs) using\nprivate information, secure and encrypted communication becomes essential.\nHomomorphic encryption (HE) provides a principled solution by enabling\ncomputation directly on encrypted data. Although prior work has explored\naspects of running LLMs under HE, the challenge of text generation,\nparticularly next-token prediction, has received limited attention and remains\na key obstacle to practical encrypted interaction. In this work, we propose a\nTSP-based token reordering strategy to address the difficulties of encrypted\ntext generation, together with a post-processing step that further reduces\napproximation error. Theoretical analysis and experimental results demonstrate\nthat our method prevents collapse, improves coherence in generated text, and\npreserves data privacy throughout. Overall, our contributions advance the\nfeasibility of practical and privacy-preserving LLM inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTSP\u7684\u4ee4\u724c\u91cd\u6392\u5e8f\u7b56\u7565\u548c\u540e\u7eed\u5904\u7406\u6b65\u9aa4\uff0c\u7528\u4e8e\u89e3\u51b3\u52a0\u5bc6\u6587\u672c\u751f\u6210\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u95ee\u9898\uff0c\u4ece\u800c\u63a8\u8fdb\u5b9e\u7528\u4e14\u4fdd\u62a4\u9690\u79c1\u7684LLM\u63a8\u7406\u3002", "motivation": "\u968f\u7740\u7528\u6237\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u79c1\u6709\u4fe1\u606f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\uff0c\u5b89\u5168\u52a0\u5bc6\u901a\u4fe1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u540c\u6001\u52a0\u5bc6\u867d\u7136\u63d0\u4f9b\u4e86\u5728\u52a0\u5bc6\u6570\u636e\u4e0a\u76f4\u63a5\u8ba1\u7b97\u7684\u539f\u7406\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u6587\u672c\u751f\u6210\u7279\u522b\u662f\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u95ee\u9898\u5728HE\u4e0b\u7684\u7814\u7a76\u6709\u9650\uff0c\u662f\u5b9e\u7528\u52a0\u5bc6\u4ea4\u4e92\u7684\u4e3b\u8981\u969c\u788d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u65c5\u884c\u5546\u95ee\u9898\uff08TSP\uff09\u7684\u4ee4\u724c\u91cd\u6392\u5e8f\u7b56\u7565\u6765\u5904\u7406\u52a0\u5bc6\u6587\u672c\u751f\u6210\u7684\u56f0\u96be\uff0c\u5e76\u7ed3\u5408\u4e00\u4e2a\u540e\u7eed\u5904\u7406\u6b65\u9aa4\u6765\u8fdb\u4e00\u6b65\u51cf\u5c11\u8fd1\u4f3c\u8bef\u5dee\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u9632\u6b62\u4e86\u5d29\u6e83\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6587\u672c\u7684\u8fde\u8d2f\u6027\uff0c\u5e76\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e86\u6570\u636e\u9690\u79c1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5b9e\u7528\u4e14\u4fdd\u62a4\u9690\u79c1\u7684LLM\u63a8\u7406\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u52a0\u5bc6\u73af\u5883\u4e0b\u7684\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12753", "abs": "https://arxiv.org/abs/2510.12753", "authors": ["Wenpu Li", "Bangyan Liao", "Yi Zhou", "Qi Xu", "Pian Wan", "Peidong Liu"], "title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)", "summary": "The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in\n3D vision, has typically been addressed independently. For neuromorphic vision\n(e.g., event cameras), however, the lack of robust data association makes\nsolving the two problems separately an ill-posed challenge, especially in the\nabsence of supervision via ground truth. Existing works mitigate this\nill-posedness by either enforcing the smoothness of the flow field via an\nexplicit variational regularizer or leveraging explicit structure-and-motion\npriors in the parametrization to improve event alignment. The former notably\nintroduces bias in results and computational overhead, while the latter, which\nparametrizes the optical flow in terms of the scene depth and the camera\nmotion, often converges to suboptimal local minima. To address these issues, we\npropose an unsupervised framework that jointly optimizes egomotion and optical\nflow via implicit spatial-temporal and geometric regularization. First, by\nmodeling camera's egomotion as a continuous spline and optical flow as an\nimplicit neural representation, our method inherently embeds spatial-temporal\ncoherence through inductive biases. Second, we incorporate structure-and-motion\npriors through differential geometric constraints, bypassing explicit depth\nestimation while maintaining rigorous geometric consistency. As a result, our\nframework (called E-MoFlow) unifies egomotion and optical flow estimation via\nimplicit regularization under a fully unsupervised paradigm. Experiments\ndemonstrate its versatility to general 6-DoF motion scenarios, achieving\nstate-of-the-art performance among unsupervised methods and competitive even\nwith supervised approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aE-MoFlow\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u5f0f\u65f6\u7a7a\u548c\u51e0\u4f55\u6b63\u5219\u5316\u8054\u5408\u4f18\u5316\u81ea\u8fd0\u52a8\u548c\u5149\u6d41\u4f30\u8ba1\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u53d8\u5206\u6b63\u5219\u5316\u6216\u7ed3\u6784-\u8fd0\u52a8\u53c2\u6570\u5316\u5e26\u6765\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5149\u6d41\u548c6-DoF\u81ea\u8fd0\u52a8\u4f30\u8ba1\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1\u5904\u7406\uff0c\u4f46\u5728\u4e8b\u4ef6\u76f8\u673a\u4e2d\u7531\u4e8e\u7f3a\u4e4f\u9c81\u68d2\u7684\u6570\u636e\u5173\u8054\uff0c\u5355\u72ec\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u53d8\u5f97\u4e0d\u9002\u5b9a\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5f15\u5165\u504f\u5dee\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u8981\u4e48\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u5c06\u76f8\u673a\u81ea\u8fd0\u52a8\u5efa\u6a21\u4e3a\u8fde\u7eed\u6837\u6761\uff0c\u5149\u6d41\u5efa\u6a21\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u901a\u8fc7\u5f52\u7eb3\u504f\u7f6e\u5d4c\u5165\u65f6\u7a7a\u4e00\u81f4\u6027\uff1b\u7ed3\u5408\u5fae\u5206\u51e0\u4f55\u7ea6\u675f\u5f15\u5165\u7ed3\u6784-\u8fd0\u52a8\u5148\u9a8c\uff0c\u907f\u514d\u663e\u5f0f\u6df1\u5ea6\u4f30\u8ba1\u4f46\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e00\u822c6-DoF\u8fd0\u52a8\u573a\u666f\uff0c\u5728\u65e0\u76d1\u7763\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u4e0e\u6709\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "E-MoFlow\u6846\u67b6\u901a\u8fc7\u9690\u5f0f\u6b63\u5219\u5316\u5728\u5b8c\u5168\u65e0\u76d1\u7763\u8303\u5f0f\u4e0b\u7edf\u4e00\u4e86\u81ea\u8fd0\u52a8\u548c\u5149\u6d41\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e2d\u7684\u4e0d\u9002\u5b9a\u95ee\u9898\u3002"}}
{"id": "2510.12640", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12640", "abs": "https://arxiv.org/abs/2510.12640", "authors": ["David Berghaus", "Patrick Seifner", "Kostadin Cvejoski", "Ramses J. Sanchez"], "title": "On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery", "comment": null, "summary": "Many scientific fields, from medicine to seismology, rely on analyzing\nsequences of events over time to understand complex systems. Traditionally,\nmachine learning models must be built and trained from scratch for each new\ndataset, which is a slow and costly process. We introduce a new approach: a\nsingle, powerful model that learns the underlying patterns of event data in\ncontext. We trained this \"foundation model\" on millions of simulated event\nsequences, teaching it a general-purpose understanding of how events can\nunfold. As a result, our model can analyze new scientific data instantly,\nwithout retraining, simply by looking at a few examples from the dataset. It\ncan also be quickly fine-tuned for even higher accuracy. This approach makes\nsophisticated event analysis more accessible and accelerates the pace of\nscientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u4e8b\u4ef6\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u65b0\u6570\u636e\u96c6\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5373\u53ef\u5feb\u901f\u5206\u6790\u65b0\u6570\u636e", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u6570\u636e\u96c6\u4ece\u5934\u6784\u5efa\u548c\u8bad\u7ec3\u6a21\u578b\uff0c\u8fc7\u7a0b\u7f13\u6162\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u901f\u5ea6", "method": "\u8bad\u7ec3\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u5728\u6570\u767e\u4e07\u4e2a\u6a21\u62df\u4e8b\u4ef6\u5e8f\u5217\u4e0a\u5b66\u4e60\u4e8b\u4ef6\u6570\u636e\u7684\u5e95\u5c42\u6a21\u5f0f\uff0c\u83b7\u5f97\u5bf9\u4e8b\u4ef6\u5982\u4f55\u5c55\u5f00\u7684\u901a\u7528\u7406\u89e3", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u5373\u65f6\u5206\u6790\u65b0\u7684\u79d1\u5b66\u6570\u636e\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u53ea\u9700\u67e5\u770b\u6570\u636e\u96c6\u4e2d\u7684\u5c11\u91cf\u793a\u4f8b\uff0c\u5e76\u4e14\u53ef\u4ee5\u5feb\u901f\u5fae\u8c03\u4ee5\u83b7\u5f97\u66f4\u9ad8\u7cbe\u5ea6", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u590d\u6742\u7684\u4e8b\u4ef6\u5206\u6790\u66f4\u52a0\u6613\u4e8e\u83b7\u53d6\uff0c\u5e76\u52a0\u901f\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u6b65\u4f10"}}
