<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs](https://arxiv.org/abs/2508.13461)
*Ivan Reyes-Amezcua,Francisco Lopez-Tiro,Clement Larose,Andres Mendez-Vazquez,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: cs.CV

TL;DR: 视觉Transformer在肾结石内镜图像分类中显著超越传统CNN模型，在复杂图像条件下准确率提升15-30%，提供了更好的可扩展性解决方案


<details>
  <summary>Details</summary>
Motivation: 肾结石分类对个性化治疗和复发预防至关重要，但传统CNN模型在处理长距离依赖关系时存在限制，影响在变化的成像条件下的性能

Method: 进行视觉Transformer(ViT)与CNN基础模型(ResNet50)的对比分析，在两个ex vivo数据集(包括CCD相机和缓性尿道镜图像)上评估性能，采用ImageNet-21k预训练的ViT-base模型

Result: ViT模型在各种成像条件下均超过ResNet50基准。在最复杂的内镜图像子集中，ViT达到95.2%准确率和95.1% F1分数(ResNet50仅64.5%和59.3%)；在混合视图子集中，ViT达到87.1%准确率(ResNet50为78.4%)，在精度和召回率上也都有显著提升

Conclusion: ViT基础架构提供了更优称的分类性能，为肾结石图像分析提供了一种可扩展的传统CNN替代方案

Abstract: Kidney stone classification from endoscopic images is critical for
personalized treatment and recurrence prevention. While convolutional neural
networks (CNNs) have shown promise in this task, their limited ability to
capture long-range dependencies can hinder performance under variable imaging
conditions. This study presents a comparative analysis between Vision
Transformers (ViTs) and CNN-based models, evaluating their performance on two
ex vivo datasets comprising CCD camera and flexible ureteroscope images. The
ViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50
baseline across multiple imaging conditions. For instance, in the most visually
complex subset (Section patches from endoscopic images), the ViT model achieved
95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50.
In the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracy
versus 78.4% with CNN. These improvements extend across precision and recall as
well. The results demonstrate that ViT-based architectures provide superior
classification performance and offer a scalable alternative to conventional
CNNs for kidney stone image analysis.

</details>


### [2] [Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer](https://arxiv.org/abs/2508.13558)
*Hsieh Ching-Teng,Wang Yuan-Kai*

Main category: cs.CV

TL;DR: 通过模仿生物神经元运作原理的神经元样编码方法，生成包含颜色和亮度信息的视觉刺刺信号，提升刺刺神经网络性能且遵循神经模态计算原则。


<details>
  <summary>Details</summary>
Motivation: 解决SNN性能落后于CNN的问题，主要因为刺刺数据信息容量有限，而现有用静态图像训练SNN的方法又违背了神经模态计算的原则。

Method: 提出神经元样编码方法，基于生物神经元的内在运作原理和功能生成刺刺数据，并加入人造光受器层以使刺刺数据包含颜色和亮度信息。

Result: 实验结果显示，这种受生物学启发的方法有效增加了刺刺信号的信息含量，提高了SNN的性能，同时遵循神经模态计算原则。

Conclusion: 这种方法有望克服神经模态计算的当前限制，促进SNN的更广泛应用。

Abstract: In recent years, neuromorphic computing and spiking neural networks (SNNs)
have ad-vanced rapidly through integration with deep learning. However, the
performance of SNNs still lags behind that of convolutional neural networks
(CNNs), primarily due to the limited information capacity of spike-based data.
Although some studies have attempted to improve SNN performance by training
them with non-spiking inputs such as static images, this approach deviates from
the original intent of neuromorphic computing, which emphasizes spike-based
information processing. To address this issue, we propose a Neuron-like
Encoding method that generates spike data based on the intrinsic operational
principles and functions of biological neurons. This method is further enhanced
by the incorporation of an artificial pho-toreceptor layer, enabling spike data
to carry both color and luminance information, thereby forming a complete
visual spike signal. Experimental results using the Integrate-and-Fire neuron
model demonstrate that this biologically inspired approach effectively
increases the information content of spike signals and improves SNN
performance, all while adhering to neuromorphic principles. We believe this
concept holds strong potential for future development and may contribute to
overcoming current limitations in neuro-morphic computing, facilitating broader
applications of SNNs.

</details>


### [3] [Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation](https://arxiv.org/abs/2508.13812)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Hyeongboo Baek,Brent ByungHoon Kang*

Main category: cs.CV

TL;DR: 这篇论文提出了时间步压缩攻击(TCA)框架，通过时间步级反向传播和对抗膜电位重用技术，在保持攻击成功率的同时将攻击延迟减少了超过56%，解决了现有攻击方法在脉冲神经网络中存在的实时性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于FGSM和PGD框架的梯度基础攻击方法在脉冲神经网络(SNNs)中存在显著的攻击延迟问题，这主要是因为它们直接扩展自ANN范式，没有利用SNN的关键特性，导致多时间步处理耗时过长，不适用于实际的实时应用。

Method: 提出时间步压缩攻击(TCA)框架，包含两个核心组件：1)时间步级反向传播(TLBP)，基于发现生成干扰的全局时间信息并非关键，支持按时间步评估并提前停止；2)对抗膜电位重用(A-MPR)，利用初始时间步主要用于累积膜电位的特点，将这个热身阶段预计算并重用。

Result: 在VGG-11和ResNet-17模型上使用CIFAR-10/100和CIFAR10-DVS数据集进行实验，TCA在白盒和黑盒设置下分别将所需攻击延迟减少了最高56.6%和57.1%，同时保持了相当的攻击成功率。

Conclusion: TCA框架通过充分利用SNN的特有性质，有效解决了现有攻击方法在实时应用中的延迟问题，为SNN安全研究提供了更高效的对抗攻击方案。

Abstract: State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural
networks (SNNs), which largely rely on extending FGSM and PGD frameworks, face
a critical limitation: substantial attack latency from multi-timestep
processing, rendering them infeasible for practical real-time applications.
This inefficiency stems from their design as direct extensions of ANN
paradigms, which fail to exploit key SNN properties. In this paper, we propose
the timestep-compressed attack (TCA), a novel framework that significantly
reduces attack latency. TCA introduces two components founded on key insights
into SNN behavior. First, timestep-level backpropagation (TLBP) is based on our
finding that global temporal information in backpropagation to generate
perturbations is not critical for an attack's success, enabling per-timestep
evaluation for early stopping. Second, adversarial membrane potential reuse
(A-MPR) is motivated by the observation that initial timesteps are
inefficiently spent accumulating membrane potential, a warm-up phase that can
be pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the
CIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the
required attack latency by up to 56.6% and 57.1% compared to SOTA methods in
white-box and black-box settings, respectively, while maintaining a comparable
attack success rate.

</details>


### [4] [Forecasting Smog Events Using ConvLSTM: A Spatio-Temporal Approach for Aerosol Index Prediction in South Asia](https://arxiv.org/abs/2508.13891)
*Taimur Khan*

Main category: cs.CV

TL;DR: 本研究使用Sentinel-5P卫星数据和ConvLSTM神经网络来预测南亚雾霾事件的悬浮颗粒物指数，实现了5天间隔的预测，误差较小但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 南亚雾霾事件对印度-恒河平原造成严重的社会经济影响，但缺乏区域尺度的实时预测系统。悬浮颗粒物指数与雾霾形成密切相关，是空气质量指数的重要组成部分。

Method: 使用2019-2023年Sentinel-5P卫星气溶胶数据，采用卷积长短期记忆(ConvLSTM)神经网络模型，利用340-380nm紫外气溶胶指数作为预测因子，捕捉时空相关性。

Result: 模型能够以5天间隔预测气溶胶指数，均方误差约0.0018，损失约0.3995，结构相似性指数约0.74，表现优于先前模型。

Conclusion: ConvLSTM模型能有效预测南亚雾霾事件，但可通过整合更多数据和优化架构来进一步提升预测性能。

Abstract: The South Asian Smog refers to the recurring annual air pollution events
marked by high contaminant levels, reduced visibility, and significant
socio-economic impacts, primarily affecting the Indo-Gangetic Plains (IGP) from
November to February. Over the past decade, increased air pollution sources
such as crop residue burning, motor vehicles, and changing weather patterns
have intensified these smog events. However, real-time forecasting systems for
increased particulate matter concentrations are still not established at
regional scale. The Aerosol Index, closely tied to smog formation and a key
component in calculating the Air Quality Index (AQI), reflects particulate
matter concentrations. This study forecasts aerosol events using Sentinel-5P
air constituent data (2019-2023) and a Convolutional Long-Short Term Memory
(ConvLSTM) neural network, which captures spatial and temporal correlations
more effectively than previous models. Using the Ultraviolet (UV) Aerosol Index
at 340-380 nm as the predictor, results show the Aerosol Index can be
forecasted at five-day intervals with a Mean Squared Error of ~0.0018, loss of
~0.3995, and Structural Similarity Index of ~0.74. While effective, the model
can be improved by integrating additional data and refining its architecture.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Deep Graph Neural Point Process For Learning Temporal Interactive Networks](https://arxiv.org/abs/2508.13219)
*Su Chen,Xiaohua Qi,Xixun Lin,Yanmin Shang,Xiaolin Xu,Yangxi Li*

Main category: cs.LG

TL;DR: 提出了DGNPP模型，结合图神经网络和点过程来学习时序交互网络，同时考虑网络拓扑结构和时间动态性，在事件预测和时间预测任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有的时序交互网络学习忽略了网络拓扑结构的影响，将问题简化为粗粒度的多序列预测

Method: DGNPP模型包含节点聚合层（捕获拓扑结构生成静态表示）和自注意力层（动态更新嵌入），将动态和静态嵌入结合到事件强度函数中，通过最大似然估计优化

Result: 在三个公开数据集上的实验表明，DGNPP在事件预测和时间预测任务上实现了优越性能和高效率，显著优于基线模型

Conclusion: DGNPP有效解决了先前方法的局限性，成功整合了网络拓扑结构和时间动态信息

Abstract: Learning temporal interaction networks(TIN) is previously regarded as a
coarse-grained multi-sequence prediction problem, ignoring the network topology
structure influence. This paper addresses this limitation and a Deep Graph
Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two
key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node
Aggregation Layer captures topological structures to generate static
representation for users and items, while the Self Attentive Layer dynamically
updates embeddings over time. By incorporating both dynamic and static
embeddings into the event intensity function and optimizing the model via
maximum likelihood estimation, DGNPP predicts events and occurrence time
effectively. Experimental evaluations on three public datasets demonstrate that
DGNPP achieves superior performance in event prediction and time prediction
tasks with high efficiency, significantly outperforming baseline models and
effectively mitigating the limitations of prior approaches.

</details>


### [6] [Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp](https://arxiv.org/abs/2508.13406)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 本研究提出了一种量化框架，通过时频分析chirp事件识别统计异常通道，并评估其与临床定义的疯痪起源区域的空间一致性。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种补充方法来准确定位疯痪起源区域，特别是在所测病人中验证手术成功的情况下。

Method: 采用两步方法：(1)无监督异常检测，使用局部异常因子(LOF)分析识别异常通道；(2)空间相关性分析，计算精确匹配指标和加权指数相似性。

Result: 方法在疯痪免病人群中表现最佳（精度均值0.903），成功所测病人也显示良好结果（精度均值0.865），而失败案例的一致性较低（精度均值0.460）。

Conclusion: chirp基础的异常检测结合加权空间指标，提供了一种有效的SOZ定位补充方法，特别适用于所测成功的病人。

Abstract: This study presents a quantitative framework for evaluating the spatial
concordance between clinically defined seizure onset zones (SOZs) and
statistically anomalous channels identified through time-frequency analysis of
chirp events. The proposed pipeline employs a two-step methodology: (1)
Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with
adaptive neighborhood selection identifies anomalous channels based on
spectro-temporal features of chirp (Onset frequency, offset frequency, and
temporal duration); and (2) Spatial Correlation Analysis, which computes both
exact co-occurrence metrics and weighted index similarity, incorporating
hemispheric congruence and electrode proximity. Key findings demonstrate that
the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects
outliers, with index matching (weighted by channel proximity) outperforming
exact matching in SOZ localization. Performance metrics (precision, recall, F1)
were highest for seizure-free patients (Index Precision mean: 0.903) and those
with successful surgical outcomes (Index Precision mean: 0.865), whereas
failure cases exhibited lower concordance (Index Precision mean: 0.460). The
key takeaway is that chirp-based outlier detection, combined with weighted
spatial metrics, provides a complementary method for SOZ localization,
particularly in patients with successful surgical outcomes.

</details>


### [7] [EventTSF: Event-Aware Non-Stationary Time Series Forecasting](https://arxiv.org/abs/2508.13434)
*Yunfeng Ge,Ming Jin,Yiji Zhao,Hongyan Li,Bo Du,Chang Xu,Shirui Pan*

Main category: cs.LG

TL;DR: EventTSF是一个创新的多模态时间序列预测框架，通过自回归扩散和流匹配技术，有效整合文本事件信息来提升非平稳时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法主要依赖单一模态数据，无法充分利用文本事件信息，导致在非平稳动态场景下预测性能受限。需要解决时间序列与文本事件之间的细粒度同步、时间不确定性以及模态对齐等挑战。

Method: 提出EventTSF框架，采用自回归扩散生成模型，在每个时间步使用流匹配技术捕捉时间-事件交互。通过事件语义信号自适应控制流匹配时间步，并使用多模态U形扩散transformer在不同分辨率下有效融合时间和文本模态。

Result: 在8个合成和真实数据集上的实验表明，EventTSF在12个基线方法中表现最优，预测准确率提升10.7%，训练效率提高1.13倍。

Conclusion: EventTSF成功解决了多模态时间序列预测中的关键挑战，证明了整合文本事件信息对提升非平稳时间序列预测性能的重要价值，为跨模态时间序列分析提供了有效解决方案。

Abstract: Time series forecasting plays a vital role in critical domains like energy
and transportation, where non-stationary dynamics are deeply intertwined with
events in other modalities such as texts. However, incorporating natural
language-based external events to improve non-stationary forecasting remains
largely unexplored, as most approaches still rely on a single modality,
resulting in limited contextual knowledge and model underperformance. Enabling
fine-grained multimodal interactions between temporal and textual data is
challenged by three fundamental issues: (1) the difficulty of fine-grained
synchronization between time-varying discrete textual events and continuous
time series; (2) the inherent temporal uncertainty introduced by textual
semantics; and (3) the misalignment between textual event embeddings and
multi-resolution temporal patterns. In this work, we address these challenges
by introducing event-aware non-stationary time series forecasting (EventTSF),
an autoregressive generation framework that integrates historical time series
with textual events to make subsequent forecasts. Specifically, EventTSF uses
autoregressive diffusion with flow matching at each step to capture nuanced
temporal-event interactions. To handle event-induced uncertainty, flow matching
timesteps are adaptively controlled according to event semantic signals. The
underlying denoiser employs a multimodal U-shaped diffusion transformer that
efficiently fuses temporal and textual modalities across different resolutions.
Extensive experiments on 8 synthetic and real-world datasets show that EventTSF
outperforms 12 baselines across diverse event-aware non-stationary time series
forecasting scenarios, achieving substantial improvements of 10.7% higher
forecasting accuracy and $1.13\times$ faster training efficiency.

</details>


### [8] [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
*Rituparna Datta,Jiaming Cui,Gregory R. Madden,Anil Vullikanti*

Main category: cs.LG

TL;DR: CALYPSO是一个混合框架，结合神经网络和机制性元种群模型来预测MRSA在医疗和社区环境中的传播动态，相比机器学习基线性能提升4.5%以上


<details>
  <summary>Details</summary>
Motivation: MRSA是医院和长期护理机构的重要公共卫生威胁，现有预测模型缺乏流行病学可解释性且性能有限，机制性流行病模型难以校准且无法有效整合多样化数据集

Method: 整合神经网络与机制性元种群模型，利用患者级保险索赔数据、通勤数据和医疗转移模式来学习区域和时间特异性参数，捕捉MRSA传播动态

Result: CALYPSO相比机器学习基线在全州范围预测性能提升超过4.5%，能够识别高风险区域和制定成本效益高的感染预防资源分配策略

Conclusion: 该混合框架能够提供准确、可解释的多空间分辨率预测，支持感染控制政策和暴发风险的反事实分析，为公共卫生决策提供有力工具

Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.

</details>


### [9] [Prediction of Hospital Associated Infections During Continuous Hospital Stays](https://arxiv.org/abs/2508.13561)
*Rituparna Datta,Methun Kamruzzaman,Eili Y. Klein,Gregory R Madden,Xinwei Deng,Anil Vullikanti,Parantapa Bhattacharya*

Main category: cs.LG

TL;DR: 提出GenHAI概率生成模型，用于建模住院患者MRSA检测结果序列，帮助医院管理者降低MRSA感染风险


<details>
  <summary>Details</summary>
Motivation: MRSA被CDC列为严重抗菌素耐药性威胁，住院患者因多种因素面临高风险，需要有效工具来理解和减轻感染风险

Method: 基于概率编程范式的生成概率模型，能够近似回答预测性、因果性和反事实性问题

Result: 在两个真实数据集上与判别式和生成式机器学习模型比较，证明了模型的有效性

Conclusion: GenHAI模型为医院管理者提供了一个强大的工具，可用于回答关于MRSA感染风险的各种重要问题

Abstract: The US Centers for Disease Control and Prevention (CDC), in 2019, designated
Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial
resistance threat. The risk of acquiring MRSA and suffering life-threatening
consequences due to it remains especially high for hospitalized patients due to
a unique combination of factors, including: co-morbid conditions, immuno
suppression, antibiotic use, and risk of contact with contaminated hospital
workers and equipment. In this paper, we present a novel generative
probabilistic model, GenHAI, for modeling sequences of MRSA test results
outcomes for patients during a single hospitalization. This model can be used
to answer many important questions from the perspectives of hospital
administrators for mitigating the risk of MRSA infections. Our model is based
on the probabilistic programming paradigm, and can be used to approximately
answer a variety of predictive, causal, and counterfactual questions. We
demonstrate the efficacy of our model by comparing it against discriminative
and generative machine learning models using two real-world datasets.

</details>


### [10] [Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models](https://arxiv.org/abs/2508.13625)
*Wenxuan Ye,Xueli An,Onur Ayan,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.LG

TL;DR: FedOL是一种联邦学习新方法，通过单轮通信实现服务器大模型构建，使用知识蒸馏而非参数共享，减少通信开销并支持异构模型架构。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中统一模型架构要求、多轮通信开销大、客户端计算资源受限等问题，同时保护数据隐私。

Method: 采用知识蒸馏技术，客户端仅交换在未标记公共数据集上的预测输出；引入专门的目标函数迭代优化伪标签和服务器模型；设计定制化的伪标签生成和知识蒸馏策略。

Result: 仿真结果显示FedOL显著优于现有基线方法，为移动网络提供了成本效益高的解决方案。

Conclusion: FedOL通过单轮通信和知识蒸馏有效解决了联邦学习中的通信开销和资源异构问题，同时保持了数据隐私保护。

Abstract: Large models, renowned for superior performance, outperform smaller ones even
without billion-parameter scales. While mobile network servers have ample
computational resources to support larger models than client devices, privacy
constraints prevent clients from directly sharing their raw data. Federated
Learning (FL) enables decentralized clients to collaboratively train a shared
model by exchanging model parameters instead of transmitting raw data. Yet, it
requires a uniform model architecture and multiple communication rounds, which
neglect resource heterogeneity, impose heavy computational demands on clients,
and increase communication overhead. To address these challenges, we propose
FedOL, to construct a larger and more comprehensive server model in one-shot
settings (i.e., in a single communication round). Instead of model parameter
sharing, FedOL employs knowledge distillation, where clients only exchange
model prediction outputs on an unlabeled public dataset. This reduces
communication overhead by transmitting compact predictions instead of full
model weights and enables model customization by allowing heterogeneous model
architectures. A key challenge in this setting is that client predictions may
be biased due to skewed local data distributions, and the lack of ground-truth
labels in the public dataset further complicates reliable learning. To mitigate
these issues, FedOL introduces a specialized objective function that
iteratively refines pseudo-labels and the server model, improving learning
reliability. To complement this, FedOL incorporates a tailored pseudo-label
generation and knowledge distillation strategy that effectively integrates
diverse knowledge. Simulation results show that FedOL significantly outperforms
existing baselines, offering a cost-effective solution for mobile networks
where clients possess valuable private data but limited computational
resources.

</details>


### [11] [Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management](https://arxiv.org/abs/2508.13905)
*Tianheng Ling,Vipin Singh,Chao Qian,Felix Biessmann,Gregor Schiele*

Main category: cs.LG

TL;DR: 这篇论文提出了一种用于预测合流制污水溢流池填充水位的端侧设备智能预测框架，通过轻量化Transformer和LSTM模型实现了高准确性和低能耗的平衡。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件加剧了老化合流污水系统的溢流风险，而现有AI预测方法依赖云计算在通信故障时可靠性低，需要能够在端侧设备上高效执行的解决方案。

Method: 提出了一种端到端的预测框架，整合轻量化Transformer和LSTM模型，通过整数采样压缩技术优化模型性能。使用自动化硬件感知部署流程在AMD Spartan-7 FPGA上寻找最优模型配置。

Result: 8位Transformer模型在24小时历史数据训练下实现了高准确性(MSE 0.0376)，每次推理能耗0.370mJ。8位LSTM模型能耗更低(0.009mJ)但准确性差14.89%，训练时间更长。

Conclusion: 该工作实现了本地化、能效高的预测能力，为合流制污水系统提供了更鲁棒的解决方案，根据部署优先级选择LSTM(极低能耗)或Transformer(高准确性)模型。

Abstract: Extreme weather events, intensified by climate change, increasingly challenge
aging combined sewer systems, raising the risk of untreated wastewater
overflow. Accurate forecasting of sewer overflow basin filling levels can
provide actionable insights for early intervention, helping mitigating
uncontrolled discharge. In recent years, AI-based forecasting methods have
offered scalable alternatives to traditional physics-based models, but their
reliance on cloud computing limits their reliability during communication
outages. To address this, we propose an end-to-end forecasting framework that
enables energy-efficient inference directly on edge devices. Our solution
integrates lightweight Transformer and Long Short-Term Memory (LSTM) models,
compressed via integer-only quantization for efficient on-device execution.
Moreover, an automated hardware-aware deployment pipeline is used to search for
optimal model configurations by jointly minimizing prediction error and energy
consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer
data, the selected 8-bit Transformer model, trained on 24 hours of historical
measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ
per inference. In contrast, the optimal 8-bit LSTM model requires significantly
less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE
0.0432) and much longer training time. This trade-off highlights the need to
align model selection with deployment priorities, favoring LSTM for ultra-low
energy consumption or Transformer for higher predictive accuracy. In general,
our work enables local, energy-efficient forecasting, contributing to more
resilient combined sewer systems. All code can be found in the GitHub
Repository (https://github.com/tianheng-ling/EdgeOverflowForecast).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 这篇论文探索了多模态结构化强化学习(MSRL)在图表到代码生成任务中的应用，通过多级奖励系统突破了监督学习的性能平台。


<details>
  <summary>Details</summary>
Motivation: 强化学习在需要深度理解信息丰富图像和生成结构化输出的任务中应用不足，监督学习单独使用往往不够有效，需要有效的强化学习策略来奖励结构化输出。

Method: 提出多模态结构化强化学习(MSRL)，构建了包含300万对图表-代码的最大训练语料库，使用多级奖励系统：文本层面的规则基奖励验证细粒度代码细节，视觉层面的模型基奖励通过渲染代码到图像并使用评估器模型评估结构相似性，采用两阶段课程进行训练稳定性控制。

Result: MSRL显著突破了监督学习的性能平台，在ChartMimic和ReachQA净高上分别提升6.2%和9.9%，达到了与先进闭源模型竞争的性能。

Conclusion: 多模态结构化强化学习是解决图表到代码生成这类需要复杂理解的多模态任务的有效方法，多级奖励系统能够有效提升结构化输出的质量。

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [13] [A Surveillance Based Interactive Robot](https://arxiv.org/abs/2508.13319)
*Kshitij Kavimandan,Pooja Mangal,Devanshi Mehta*

Main category: cs.RO

TL;DR: 基于树莓派Pi的移动监控机器人系统，支持实时视频流、语音控制和多语言交互，通过YOLOv3进行对象检测和室内导航


<details>
  <summary>Details</summary>
Motivation: 建立一个使用普通硬件和开源软件的易复现移动监控机器人系统，支持远程监控和语音控制

Method: 使用2台Raspberry Pi 4（前端和中央单元），采用FFmpeg传输视频，YOLOv3进行对象检测，Python语音库实现语音识别和翻译，Kinect RGB-D传感器提供视觉输入

Result: 室内测试中能够在CPU上以交互帧率检测常见物体，可靠识别命令并转换为动作，无需手动控制

Conclusion: 设计依靠市面硬件和开源软件，易于复现，并提出了传感器融合、GPU加速、人脸和文本识别等扩展可能性

Abstract: We build a mobile surveillance robot that streams video in real time and
responds to speech so a user can monitor and steer it from a phone or browser.
The system uses two Raspberry Pi 4 units: a front unit on a differential drive
base with camera, mic, and speaker, and a central unit that serves the live
feed and runs perception. Video is sent with FFmpeg. Objects in the scene are
detected using YOLOv3 to support navigation and event awareness. For voice
interaction, we use Python libraries for speech recognition, multilingual
translation, and text-to-speech, so the robot can take spoken commands and read
back responses in the requested language. A Kinect RGB-D sensor provides visual
input and obstacle cues. In indoor tests the robot detects common objects at
interactive frame rates on CPU, recognises commands reliably, and translates
them to actions without manual control. The design relies on off-the-shelf
hardware and open software, making it easy to reproduce. We discuss limits and
practical extensions, including sensor fusion with ultrasonic range data, GPU
acceleration, and adding face and text recognition.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [14] [Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray](https://arxiv.org/abs/2508.13236)
*Hyeonjin Choi,Jinse Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: 这篇论文提出了一种不确定性感知学习策略，通过同时学习医生的背景知识和肺部病变来提高医疗AI在胸部X光片诊断中的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 医生对医疗AI系统的不信任主要来自于对其诊断不确定性的担忧。临床诊断中医生利用了广泛的背景知识和经验，而医疗AI仅依赖对目标病变的重复学习，导致知识缺乏和诊断不确定性。

Method: 提出了不确定性感知学习策略，同时学习医生的背景知识和胸部X光片病变信息。使用了来自Ajou大学医院2,517张无病变图像和656张结节图像进行实验。

Result: 提出模型达到92%准确率（IoU 0.2 / FPPI 2），敏感度比基准模型提高10%，不确定性指标熬低了20%（熷增量减少0.2）。

Conclusion: 通过学习医生的背景知识，可以有效解决医疗AI的知识缺乏问题，提高诊断准确性和可靠性，有助于推进医疗AI在临床中的广泛采用。

Abstract: Early detection and rapid intervention of lung cancer are crucial.
Nonetheless, ensuring an accurate diagnosis is challenging, as physicians'
ability to interpret chest X-rays varies significantly depending on their
experience and degree of fatigue. Although medical AI has been rapidly
advancing to assist in diagnosis, physicians' trust in such systems remains
limited, preventing widespread clinical adoption. This skepticism fundamentally
stems from concerns about its diagnostic uncertainty. In clinical diagnosis,
physicians utilize extensive background knowledge and clinical experience. In
contrast, medical AI primarily relies on repetitive learning of the target
lesion to generate diagnoses based solely on that data. In other words, medical
AI does not possess sufficient knowledge to render a diagnosis, leading to
diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning
Policy that can address the issue of knowledge deficiency by learning the
physicians' background knowledge alongside the Chest X-ray lesion information.
We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou
University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a
10% enhancement in sensitivity compared to the baseline model while also
decreasing entropy as a measure of uncertainty by 0.2.

</details>
