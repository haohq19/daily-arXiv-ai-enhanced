<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 提出了一种基于事件法向流的运动分割与自运动估计框架，适用于神经形态视觉传感器，无需完整光流计算。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖光流或深度估计，而神经形态传感器的高时间分辨率事件数据未被充分利用。

Method: 利用几何约束，通过优化流程进行事件过分割、残差分析分离运动对象，并基于运动相似性和时间一致性进行层次聚类。

Result: 在EVIMO2v2数据集上验证了方法的准确性，尤其在物体边界表现优异。

Conclusion: 该方法在实时机器人及导航应用中具有显著潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [2] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 论文提出了一种相机引导系统，帮助摄影爱好者识别和去除照片中的杂乱元素，通过美学评估和图像修复算法提升照片质量。


<details>
  <summary>Details</summary>
Motivation: 照片中的杂乱元素会分散注意力，影响情感或故事的传达。业余摄影师常因疏忽或经验不足而拍摄杂乱场景，因此需要一种系统来指导他们优化构图。

Method: 系统结合了美学评估算法和基于生成对抗网络的图像修复算法，提供交互式杂乱识别和去除工具。

Result: 用户研究表明，该系统能帮助用户更高效地识别杂乱并拍摄更高质量的照片。

Conclusion: 该系统通过灵活界面和准确算法，有效提升了摄影爱好者的拍摄效率和照片质量。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [3] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: 提出MultiRetNet，结合视网膜成像、社会经济因素和共病资料，提高糖尿病视网膜病变分期准确性，并集成临床延迟系统。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球可预防失明的主要原因，低收入群体因筛查机会有限更易进展至晚期。共病条件加速病情发展。

Method: 采用三种多模态融合方法，全连接层融合效果最佳。通过对抗性低质量图像和对比学习训练延迟系统，识别需临床复查的异常样本。

Result: 系统在低质量图像上保持诊断准确性，整合关键健康数据，提高早期检测率，尤其对服务不足人群。

Conclusion: 该方法可降低医疗成本，提高早期检测率，减少医疗资源分配不均，促进医疗公平。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [4] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 论文提出了InterAct VideoQA数据集，用于评估和改进视频问答模型在交通监控任务中的表现，解决了现有模型在复杂交通场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答模型在复杂交通场景中表现不佳，需要领域特定数据集提升性能。

Method: 构建了包含8小时真实交通视频和25,000个问答对的InterAct VideoQA数据集，并评估了现有模型的性能。

Result: 现有模型在复杂交通场景中表现不足，但通过微调在InterAct VideoQA上性能显著提升。

Conclusion: InterAct VideoQA数据集为智能交通系统中的视频问答模型研究提供了重要基准。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [5] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过结合因果感知查询优化和细粒度视觉定位，解决了视频问答中关键帧稀疏和因果推理的挑战，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答方法存在任务无关采样和启发式检索的局限性，无法有效处理关键事件和因果推理。

Method: LeAdQA利用LLM优化问题-选项对，通过时间定位模型精确检索关键片段，并结合自适应融合机制和MLLM生成答案。

Result: 在NExT-QA、IntentQA和NExT-GQA数据集上，LeAdQA实现了最先进的性能，同时保持计算效率。

Conclusion: LeAdQA通过精确的视觉定位和因果推理优化，显著提升了视频问答的复杂推理能力。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [6] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出了一种新颖的时空多图表示方法，用于改进事件传感器的数据建模，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 事件传感器数据稀疏且异步，传统方法将其转换为密集张量会丧失其优势，而现有图表示方法对时空动态建模不足。

Method: 构建解耦的空间图（利用B样条基函数建模全局结构）和时间图（基于运动向量的注意力机制建模局部动态变化），替代计算昂贵的3D核。

Result: 在Gen1和eTraM数据集上，检测精度提升6%，速度提升5倍，参数减少且计算成本不变。

Conclusion: 结构化图建模在异步视觉任务中具有显著优势。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [7] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的视频压缩框架，通过生成模型从稀疏信号中重建视频，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在人类视觉感知对齐方面的优势，优化视频压缩的感知重建质量。

Method: 将视频压缩重构为条件生成任务，引入多粒度条件、紧凑表示和多条件训练模块。

Result: 在感知质量指标（如FVD和LPIPS）上显著优于传统和神经编解码器，尤其在高压缩比下表现突出。

Conclusion: 该方法通过条件扩散模型实现了感知优化的视频压缩，为高效传输提供了新思路。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [8] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 论文探讨了视觉语言模型（VLM）在生物识别系统中检测物理和数字攻击的应用，提出了一种上下文学习框架，实验表明其性能优于传统CNN。


<details>
  <summary>Details</summary>
Motivation: 随着生物识别系统检测技术的进步，攻击手段也日益复杂，传统深度学习模型难以适应多样化的攻击和环境条件，且数据收集面临隐私和多样性挑战。

Method: 提出了一种基于VLM的上下文学习框架，用于检测物理呈现攻击和数字变形攻击，并建立了首个系统化的定量评估框架。

Result: 实验表明，该框架在物理和数字攻击检测中表现优异，优于传统CNN，且无需资源密集型训练。

Conclusion: 该框架为提升攻击检测的泛化能力提供了有前景的工具。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 本文提出了一种半监督联邦学习框架SSFL-DCSL，通过双对比损失和软标签解决分布式客户端数据标签稀缺问题，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法需要大量标注数据且成本高，而不同客户端数据分布差异可能影响模型性能。

Method: 设计了基于拉普拉斯分布的样本加权函数、双对比损失（局部和全局对比损失），并通过加权平均和动量更新聚合本地原型。

Result: 在仅10%数据标注的最具挑战性任务中，SSFL-DCSL比现有方法准确率提升1.15%至7.85%。

Conclusion: SSFL-DCSL有效解决了数据标签稀缺和分布差异问题，提升了模型性能并保护了隐私。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [10] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究发现语言模型可以通过语义无关的数据传递行为特征，称为“潜意识学习”。即使数据过滤掉相关特征，学生模型仍能从教师模型生成的数据中学习到这些特征。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否能够通过看似无关的数据传递行为特征，揭示AI开发中潜在的风险。

Method: 通过教师模型生成数字序列、代码或推理痕迹的数据集，训练学生模型，观察其是否学习到教师模型的行为特征。

Result: 学生模型确实从过滤后的数据中学习到教师模型的行为特征，但仅当教师和学生模型的基础模型相同时有效。

Conclusion: 潜意识学习是一种普遍现象，可能成为AI开发中的潜在陷阱，即使数据过滤也无法完全避免。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [11] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR是一个基于Transformer的框架，通过时间感知注意力机制和预训练语言模型嵌入，解决了电子健康记录（EHR）中数据异质性和复杂时间模式的问题，显著提升了疾病进展预测的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含丰富的临床信息，但标准方法难以处理不规则时间间隔和复杂时间模式，需要更有效的建模方法。

Method: 提出TALE-EHR框架，结合时间感知注意力机制和预训练语言模型嵌入，显式建模连续时间间隔并增强语义理解。

Result: 在MIMIC-IV和PIC数据集上，TALE-EHR在疾病进展预测等任务中优于现有基线方法。

Conclusion: TALE-EHR通过结合显式时间建模和强语义表示，为EHR分析提供了高效解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [12] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 提出了一种基于共振隧穿二极管（RTD）的神经形态计算架构，用于物理储层计算（RC），并在图像识别任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向实时、边缘和资源受限环境扩展，需要硬件高效的计算模型。

Method: 理论构建并数值实现了基于RTD的RC系统，应用于手写数字分类和Fruit~360数据集的对象识别。

Result: 该架构在性能表现良好，同时遵循下一代RC原则，用确定性非线性变换取代随机连接。

Conclusion: RTD-based RC架构在资源受限环境中具有潜力，为高效计算提供了新思路。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [13] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化框架，用于安全高效的协作式大语言模型（LLM）微调，解决了传统联邦学习中的中心化漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习（FL）需要中心服务器，存在单点攻击和中毒攻击的风险，且在异构、无信任环境中微调70B参数模型是一个未解决的瓶颈。

Method: FLock结合区块链信任层和经济激励机制，替代中心聚合器，提供安全、可审计的协作协议。

Result: 实验表明，FLock能抵御后门中毒攻击，减少对抗攻击成功率>68%，并实现跨领域泛化优势。

Conclusion: FLock为去中心化LLM微调提供了安全高效的解决方案，显著提升了模型的安全性和泛化能力。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [14] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 本文探讨了住院时间（LOS）预测准确性与床位调度灵活性之间的关系，研究了在预测误差下最有效的患者调度策略以避免床位溢出并优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 下游资源（如床位）对计划选择性手术患者入院至关重要，但实际LOS与预测值可能存在较大差异，导致调度不可行。本文旨在通过研究LOS预测准确性与调度灵活性的关系，提出更有效的调度策略。

Method: 利用模拟机器学习方法评估数据驱动策略，分析不同纠正政策下LOS预测误差与调度灵活性的关系。

Result: 研究发现，更准确的LOS预测可以减少调度调整的影响，但训练高精度ML模型成本较高。

Conclusion: 通过优化调度策略，可以在LOS预测误差下有效避免床位溢出并提高资源利用率。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [15] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的替代模型，用于加速HEC-RAS洪水模拟，结合GRU和Geo-FNO，显著提升了计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统物理求解器（如HEC-RAS）计算成本高，难以实时决策，需在不牺牲精度的情况下加速模拟。

Method: 采用混合自回归架构，结合GRU捕捉短期时间动态和Geo-FNO建模长程空间依赖，从HEC-RAS文件中提取特征进行训练。

Result: 模型在密西西比河流域67个河段上验证，中位绝对水位误差为0.31英尺，计算时间从139分钟缩短至40分钟。

Conclusion: 数据驱动方法通过特征工程可替代传统水力模型，提升大规模洪水预报的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多代理框架，通过LLM代理实现材料发现的高通量、高保真模拟，减少对人类专家的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统误差处理困难的问题。

Method: 采用分层多代理框架，结合中央LLM规划代理和领域特定LLM代理，共享画布辅助协作。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，并通过贝叶斯采样验证FCC位点偏好。

Conclusion: DREAMS实现了L3级自动化，显著减少对人类专家的依赖，推动高通量材料发现的民主化。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [17] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，专为嵌入式系统优化的自主代理构建，解决了现有框架在资源受限环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有框架在动态环境和资源受限场景中表现不佳，依赖云端计算且缺乏持久自主性和环境感知能力。

Method: Amico采用Rust编写，支持通过WebAssembly在嵌入式平台和浏览器环境中高效运行，提供事件处理、状态管理和行为执行的抽象。

Result: Amico为构建适应有限计算和间歇性连接环境的弹性交互代理提供了统一基础设施。

Conclusion: Amico是一个高效、安全的框架，适用于资源受限环境中的自主代理开发。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [18] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在恶意合谋中的风险，通过模拟框架展示了去中心化系统在传播虚假信息和电商欺诈中的高效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的普及，多智能体系统的潜在危害尚未充分研究，尤其是在恶意合谋方面。

Method: 提出了一种模拟框架，支持集中式和去中心化协调结构，应用于虚假信息传播和电商欺诈场景。

Result: 去中心化系统在实施恶意行为时更高效，且能灵活调整策略以规避传统干预措施。

Conclusion: 研究强调了改进检测系统和应对措施的必要性，以应对恶意多智能体系统的威胁。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [19] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间，并在真实和合成事件日志上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测的准确性，解决信息提取和模型构建的挑战。

Method: 结合图注意力网络编码活动及其关系，使用LSTM处理时间依赖，构建端到端预测模型。

Result: 在真实和合成事件日志上表现出与现有先进方法竞争的性能。

Conclusion: 该模型有效解决了对象中心预测流程监控中的关键问题，具有实际应用潜力。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [20] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，使模型能够根据问题复杂度自适应调整推理深度，显著减少计算资源使用并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在推理过程中因统一策略导致的资源浪费问题，同时避免因效率优化而牺牲推理能力。

Method: 采用分层预算探索和差异化奖励机制，将样本分组并分配不同预算，使模型能够根据问题复杂度动态调整推理深度。

Result: 实验表明，HBPO在四个推理基准测试中平均减少60.6%的token使用，同时提升3.14%的准确率。

Conclusion: 推理效率和能力并非不可兼得，通过分层训练可以同时优化两者。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 研究构建了一个土耳其语自杀意念语料库，并提出了资源高效的标注框架，同时评估了标签可靠性和模型一致性，强调了心理健康NLP中语言包容性和透明性的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决自杀意念检测中语言覆盖不足和标注不可靠的问题，推动全球AI自杀预防的实现。

Method: 构建土耳其语语料库，引入三人类标注者和两大语言模型（LLMs）的标注框架，通过迁移学习评估标签可靠性和模型性能。

Result: 发现流行模型在零样本迁移学习中表现不佳，强调心理健康NLP需要更严格的标注和评估方法。

Conclusion: 呼吁在心理健康NLP中优先考虑数据和模型的可靠性，并提倡透明性。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [22] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 分析冲突叙事揭示公共领域极化与议题对齐的机制。


<details>
  <summary>Details</summary>
Motivation: 探索叙事如何作为解释政治现实的工具，并研究其对极化和议题对齐的影响。

Method: 从德国Twitter数据中提取对立意见群体的叙事信号，聚焦乌克兰战争、新冠疫情和气候变化等议题。

Result: 发现冲突叙事的两维度：角色分配差异和事件情节差异，并初步揭示叙事对齐模式。

Conclusion: 叙事分析为理解极化机制提供了新视角。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [23] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出“交互即智能”概念，重新定义人机关系，强调交互是智能的核心维度，而非单纯接口。通过Deep Cognition系统实现透明、可控的交互，显著提升研究任务的效率和协作效果。


<details>
  <summary>Details</summary>
Motivation: 传统人机交互模式（输入-等待-输出）导致错误累积、研究边界僵化及专家知识整合不足，亟需一种更智能的交互方式。

Method: 提出Deep Cognition系统，包含三大创新：透明可控的交互、细粒度双向对话、共享认知上下文，实现人类对AI思维过程的战略干预。

Result: 用户评估显示，该系统在透明度、细粒度交互、实时干预等六项指标上显著优于基线，研究任务性能提升31.8%至50.0%。

Conclusion: 交互是智能的核心，Deep Cognition通过认知监督模式，为人机协作研究任务提供了更高效的解决方案。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [24] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出了一种基于实时流场测量的四旋翼无人机闭环控制系统，用于在狭窄管道中悬停和横向移动。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼在狭窄管道中因气流扰动导致的悬停不稳定问题。

Method: 开发了低延迟的事件烟雾测速法，结合基于循环卷积神经网络的扰动估计器和强化学习训练的控制器。

Result: 系统能有效抵消瞬态气动效应，防止与管壁碰撞，首次实现了基于实时流场测量的闭环控制。

Conclusion: 为复杂气动环境中的飞行研究开辟了新方向，并揭示了管道飞行中的流场特性。

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [25] [CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions](https://arxiv.org/abs/2507.15499)
*Jongseok Lee,Timo Birr,Rudolph Triebel,Tamim Asfour*

Main category: cs.RO

TL;DR: CLEVER是一个基于深度神经网络的主动学习系统，通过在线获取人类支持并适应DNN，提升语义感知的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决DNN在流数据语义感知中的失败问题，通过人类干预和在线适应提升系统性能。

Method: 采用贝叶斯框架编码领域知识，设计了一个流式主动学习系统，并在真实机器人上实现。

Result: 通过用户验证和实验证明，CLEVER能有效提升DNN语义感知的鲁棒性。

Conclusion: CLEVER是首个在真实机器人上实现的流式主动学习系统，为DNN语义感知的实践改进提供了证据。

Abstract: We propose CLEVER, an active learning system for robust semantic perception
with Deep Neural Networks (DNNs). For data arriving in streams, our system
seeks human support when encountering failures and adapts DNNs online based on
human instructions. In this way, CLEVER can eventually accomplish the given
semantic perception tasks. Our main contribution is the design of a system that
meets several desiderata of realizing the aforementioned capabilities. The key
enabler herein is our Bayesian formulation that encodes domain knowledge
through priors. Empirically, we not only motivate CLEVER's design but further
demonstrate its capabilities with a user validation study as well as
experiments on humanoid and deformable objects. To our knowledge, we are the
first to realize stream-based active learning on a real robot, providing
evidence that the robustness of the DNN-based semantic perception can be
improved in practice. The project website can be accessed at
https://sites.google.com/view/thecleversystem.

</details>


### [26] [Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction](https://arxiv.org/abs/2507.15729)
*Jens V. Rüppel,Andrey Rudenko,Tim Schreiter,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 论文提出了一种基于大语言模型（LLM）的辅助机器人交互系统，通过多模态输入（如视线和语音）支持动态用户任务，并与传统脚本化系统进行了对比。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互系统在双向、多模态和上下文感知支持方面仍有不足，论文旨在解决这一问题。

Method: 设计了一个模块化、可转移的系统，结合多视觉输入和实时语言交互状态表示，支持动态任务。

Result: 实验表明，LLM方法提高了适应性和用户参与度，但可能产生冗余输出；脚本化系统更适合简单任务。

Conclusion: LLM方法在复杂任务中表现更优，但需优化冗余问题；脚本化系统在简单任务中更高效。

Abstract: The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.

</details>
