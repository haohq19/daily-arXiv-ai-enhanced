{"id": "2507.13455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13455", "abs": "https://arxiv.org/abs/2507.13455", "authors": ["Dean Chen", "Armin Pomeroy", "Brandon T. Peterson", "Will Flanagan", "He Kai Lim", "Alexandra Stavrakis", "Nelson F. SooHoo", "Jonathan B. Hopkins", "Tyler R. Clites"], "title": "Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms", "comment": "42 pages, 17 figures. Under review at ASME Journal of Mechanical\n  Design", "summary": "Compliant mechanisms have significant potential in precision applications due\nto their ability to guide motion without contact. However, an inherent\nvulnerability to fatigue and mechanical failure has hindered the translation of\ncompliant mechanisms to real-world applications. This is particularly\nchallenging in service environments where loading is complex and uncertain, and\nthe cost of failure is high. In such cases, mechanical hard stops are critical\nto prevent yielding and buckling. Conventional hard-stop designs, which rely on\nstacking single-DOF limits, must be overly restrictive in multi-DOF space to\nguarantee safety in the presence of unknown loads. In this study, we present a\nsystematic design synthesis method to guarantee overload protection in\ncompliant mechanisms by integrating coupled multi-DOF motion limits within a\nsingle pair of compact hard-stop surfaces. Specifically, we introduce a\ntheoretical and practical framework for optimizing the contact surface geometry\nto maximize the mechanisms multi-DOF working space while still ensuring that\nthe mechanism remains within its elastic regime. We apply this synthesis method\nto a case study of a caged-hinge mechanism for orthopaedic implants, and\nprovide numerical and experimental validation that the derived design offers\nreliable protection against fatigue, yielding, and buckling. This work\nestablishes a foundation for precision hard-stop design in compliant systems\noperating under uncertain loads, which is a crucial step toward enabling the\napplication of compliant mechanisms in real-world systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u81ea\u7531\u5ea6\u8fd0\u52a8\u9650\u5236\u7684\u7d27\u51d1\u786c\u505c\u6b62\u8868\u9762\uff0c\u4e3a\u67d4\u6027\u673a\u6784\u63d0\u4f9b\u8fc7\u8f7d\u4fdd\u62a4\u3002", "motivation": "\u67d4\u6027\u673a\u6784\u5728\u7cbe\u5bc6\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u75b2\u52b3\u548c\u673a\u68b0\u6545\u969c\u7684\u56fa\u6709\u8106\u5f31\u6027\u963b\u788d\u4e86\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u548c\u4e0d\u786e\u5b9a\u7684\u8d1f\u8f7d\u73af\u5883\u4e2d\u3002", "method": "\u5f15\u5165\u7406\u8bba\u548c\u5b9e\u8df5\u6846\u67b6\uff0c\u4f18\u5316\u63a5\u89e6\u8868\u9762\u51e0\u4f55\u5f62\u72b6\uff0c\u4ee5\u6700\u5927\u5316\u673a\u6784\u7684\u591a\u81ea\u7531\u5ea6\u5de5\u4f5c\u7a7a\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u5176\u5728\u5f39\u6027\u8303\u56f4\u5185\u3002", "result": "\u901a\u8fc7\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9632\u6b62\u75b2\u52b3\u3001\u5c48\u670d\u548c\u5c48\u66f2\u3002", "conclusion": "\u4e3a\u5728\u4e0d\u786e\u5b9a\u8d1f\u8f7d\u4e0b\u8fd0\u884c\u7684\u67d4\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cbe\u786e\u786c\u505c\u6b62\u8bbe\u8ba1\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u67d4\u6027\u673a\u6784\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.13558", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "title": "Why Isn't Relational Learning Taking Over the World?", "comment": "10 pages (6 pages + references + appendices)", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5e94\u5173\u6ce8\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\u5efa\u6a21\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\uff0c\u5e76\u5206\u6790\u4e86\u5173\u7cfb\u5b66\u4e60\u672a\u666e\u53ca\u7684\u539f\u56e0\u53ca\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5f53\u524dAI\u4e3b\u8981\u5efa\u6a21\u50cf\u7d20\u548c\u6587\u5b57\uff0c\u4f46\u4e16\u754c\u7531\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\u6784\u6210\uff0c\u5e94\u76f4\u63a5\u5efa\u6a21\u8fd9\u4e9b\u5b9e\u4f53\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5173\u7cfb\u5b66\u4e60\uff08\u5982\u7edf\u8ba1\u5173\u7cfbAI\uff09\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u5176\u672a\u666e\u53ca\u7684\u539f\u56e0\u3002", "result": "\u5173\u7cfb\u5b66\u4e60\u4ec5\u5728\u6709\u9650\u573a\u666f\u4e2d\u5e94\u7528\uff0c\u9700\u6539\u8fdb\u4ee5\u53d1\u6325\u5176\u6f5c\u529b\u3002", "conclusion": "\u5173\u7cfb\u5b66\u4e60\u9700\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u4ee5\u5728AI\u9886\u57df\u5360\u636e\u66f4\u91cd\u8981\u7684\u5730\u4f4d\u3002"}}
{"id": "2507.13468", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13468", "abs": "https://arxiv.org/abs/2507.13468", "authors": ["Shiye Cao", "Maia Stiber", "Amama Mahmood", "Maria Teresa Parreira", "Wendy Ju", "Micol Spitale", "Hatice Gunes", "Chien-Ming Huang"], "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations", "comment": null, "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.", "AI": {"tldr": "ERR@HRI 2.0\u6311\u6218\u8d5b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u5931\u8d25\uff0c\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u9ad8\u5931\u8d25\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u4ecd\u6613\u51fa\u9519\uff0c\u5982\u8bef\u89e3\u7528\u6237\u610f\u56fe\u6216\u4e2d\u65ad\u5bf9\u8bdd\uff0c\u9700\u68c0\u6d4b\u548c\u89e3\u51b3\u8fd9\u4e9b\u5931\u8d25\u4ee5\u7ef4\u6301\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u63d0\u4f9b16\u5c0f\u65f6\u7684\u4eba\u673a\u4ea4\u4e92\u591a\u6a21\u6001\u6570\u636e\uff08\u9762\u90e8\u3001\u8bed\u97f3\u3001\u5934\u90e8\u52a8\u4f5c\uff09\uff0c\u6807\u6ce8\u673a\u5668\u4eba\u9519\u8bef\u548c\u7528\u6237\u610f\u56fe\uff0c\u9f13\u52b1\u56e2\u961f\u5f00\u53d1\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u6311\u6218\u8d5b\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u8bef\u62a5\u7387\u7b49\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u662f\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u4e2d\u5931\u8d25\u68c0\u6d4b\u7684\u5173\u952e\u4e00\u6b65\uff0c\u5229\u7528\u793e\u4ea4\u4fe1\u53f7\u5206\u6790\u63d0\u5347\u673a\u5668\u4eba\u6027\u80fd\u3002"}}
{"id": "2507.13685", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13685", "abs": "https://arxiv.org/abs/2507.13685", "authors": ["Yue Yang", "Zihan Su", "Ying Zhang", "Chang Chuan Goh", "Yuxiang Lin", "Anthony Graham Bellotti", "Boon Giin Lee"], "title": "Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction", "comment": null, "summary": "This study addresses a critical challenge in time series anomaly detection:\nenhancing the predictive capability of loan default models more than three\nmonths in advance to enable early identification of default events, helping\nfinancial institutions implement preventive measures before risk events\nmaterialize. Existing methods have significant drawbacks, such as their lack of\naccuracy in early predictions and their dependence on training and testing\nwithin the same year and specific time frames. These issues limit their\npractical use, particularly with out-of-time data. To address these, the study\nintroduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge\nKolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long\nShort-Term Memory (LSTM) networks. The proposed models were evaluated against\nthe baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms\nof accuracy, precision, recall, F1 and AUC in different lengths of feature\nwindow, sample sizes, and early prediction intervals. The results demonstrate\nthat the proposed model achieves a prediction accuracy of over 92% three months\nin advance and over 88% eight months in advance, significantly outperforming\nexisting baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGRU-KAN\u548cLSTM-KAN\u4e24\u79cd\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u524d\u4e09\u4e2a\u6708\u4ee5\u4e0a\u9884\u6d4b\u8d37\u6b3e\u8fdd\u7ea6\uff0c\u51c6\u786e\u7387\u8d8592%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u6a21\u578b\u5728\u65e9\u671f\u9884\u6d4b\u4e2d\u51c6\u786e\u6027\u4e0d\u8db3\u53ca\u4f9d\u8d56\u7279\u5b9a\u65f6\u95f4\u6846\u67b6\u7684\u95ee\u9898\uff0c\u4ee5\u5e2e\u52a9\u91d1\u878d\u673a\u6784\u63d0\u524d\u91c7\u53d6\u9884\u9632\u63aa\u65bd\u3002", "method": "\u7ed3\u5408Kolmogorov-Arnold Networks (KAN)\u4e0eGRU\u548cLSTM\u7f51\u7edc\uff0c\u63d0\u51faGRU-KAN\u548cLSTM-KAN\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u7279\u5f81\u7a97\u53e3\u3001\u6837\u672c\u91cf\u548c\u9884\u6d4b\u95f4\u9694\u4e0b\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u65b0\u6a21\u578b\u5728\u63d0\u524d\u4e09\u4e2a\u6708\u9884\u6d4b\u4e2d\u51c6\u786e\u7387\u8d8592%\uff0c\u516b\u4e2a\u6708\u8d8588%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff08LSTM\u3001GRU\u7b49\uff09\u3002", "conclusion": "GRU-KAN\u548cLSTM-KAN\u6a21\u578b\u5728\u65e9\u671f\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13736", "categories": ["cs.LG", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13736", "abs": "https://arxiv.org/abs/2507.13736", "authors": ["Matthias Jobst", "Tim Langer", "Chen Liu", "Mehmet Alici", "Hector A. Gonzalez", "Christian Mayr"], "title": "An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC", "comment": "Poster at ACM ICONS 2025 - International Conference on Neuromorphic\n  Systems", "summary": "This work presents a multi-layer DNN scheduling framework as an extension of\nOctopuScheduler, providing an end-to-end flow from PyTorch models to inference\non a single SpiNNaker2 chip. Together with a front-end comprised of\nquantization and lowering steps, the proposed framework enables the edge-based\nexecution of large and complex DNNs up to transformer scale using the\nneuromorphic platform SpiNNaker2.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eOctopuScheduler\u7684\u591a\u5c42DNN\u8c03\u5ea6\u6846\u67b6\uff0c\u652f\u6301\u4ecePyTorch\u6a21\u578b\u5230SpiNNaker2\u82af\u7247\u7684\u7aef\u5230\u7aef\u63a8\u7406\u6d41\u7a0b\u3002", "motivation": "\u89e3\u51b3\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6267\u884c\u5927\u89c4\u6a21\u590d\u6742DNN\uff08\u5982Transformer\uff09\u7684\u6311\u6218\uff0c\u5229\u7528\u795e\u7ecf\u5f62\u6001\u5e73\u53f0SpiNNaker2\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u91cf\u5316\u548c\u964d\u9636\u6b65\u9aa4\u7684\u524d\u7aef\uff0c\u7ed3\u5408\u591a\u5c42DNN\u8c03\u5ea6\u6846\u67b6\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6d41\u7a0b\u3002", "result": "\u6210\u529f\u5728SpiNNaker2\u82af\u7247\u4e0a\u6267\u884c\u4e86\u5927\u89c4\u6a21\u590d\u6742DNN\u7684\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548DNN\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13805", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.13805", "abs": "https://arxiv.org/abs/2507.13805", "authors": ["Tim Rensmeyer", "Denis Kramer", "Oliver Niggemann"], "title": "On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach", "comment": null, "summary": "Due to the computational complexity of evaluating interatomic forces from\nfirst principles, the creation of interatomic machine learning force fields has\nbecome a highly active field of research. However, the generation of training\ndatasets of sufficient size and sample diversity itself comes with a\ncomputational burden that can make this approach impractical for modeling rare\nevents or systems with a large configuration space. Fine-tuning foundation\nmodels that have been pre-trained on large-scale material or molecular\ndatabases offers a promising opportunity to reduce the amount of training data\nnecessary to reach a desired level of accuracy. However, even if this approach\nrequires less training data overall, creating a suitable training dataset can\nstill be a very challenging problem, especially for systems with rare events\nand for end-users who don't have an extensive background in machine learning.\nIn on-the-fly learning, the creation of a training dataset can be largely\nautomated by using model uncertainty during the simulation to decide if the\nmodel is accurate enough or if a structure should be recalculated with\nclassical methods and used to update the model. A key challenge for applying\nthis form of active learning to the fine-tuning of foundation models is how to\nassess the uncertainty of those models during the fine-tuning process, even\nthough most foundation models lack any form of uncertainty quantification. In\nthis paper, we overcome this challenge by introducing a fine-tuning approach\nbased on Bayesian neural network methods and a subsequent on-the-fly workflow\nthat automatically fine-tunes the model while maintaining a pre-specified\naccuracy and can detect rare events such as transition states and sample them\nat an increased rate relative to their occurrence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u65f6\u5b66\u4e60\u5de5\u4f5c\u6d41\u81ea\u52a8\u4f18\u5316\u6a21\u578b\uff0c\u4ee5\u68c0\u6d4b\u548c\u91c7\u6837\u7a00\u6709\u4e8b\u4ef6\u3002", "motivation": "\u7531\u4e8e\u4ece\u5934\u8ba1\u7b97\u539f\u5b50\u95f4\u529b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u673a\u5668\u5b66\u4e60\u529b\u573a\u7684\u7814\u7a76\u6d3b\u8dc3\uff0c\u4f46\u751f\u6210\u8db3\u591f\u89c4\u6a21\u548c\u591a\u6837\u6027\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u5bf9\u7a00\u6709\u4e8b\u4ef6\u6216\u5927\u6784\u578b\u7a7a\u95f4\u7cfb\u7edf\u4e0d\u5b9e\u7528\u3002\u5fae\u8c03\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u53ef\u51cf\u5c11\u6240\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u662f\u6311\u6218\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9e\u65f6\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u81ea\u52a8\u5fae\u8c03\u6a21\u578b\u5e76\u4fdd\u6301\u9884\u8bbe\u7cbe\u5ea6\uff0c\u540c\u65f6\u68c0\u6d4b\u548c\u589e\u52a0\u7a00\u6709\u4e8b\u4ef6\uff08\u5982\u8fc7\u6e21\u6001\uff09\u7684\u91c7\u6837\u7387\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u514b\u670d\u4e86\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5fae\u8c03\u548c\u7a00\u6709\u4e8b\u4ef6\u7684\u9ad8\u6548\u68c0\u6d4b\u4e0e\u91c7\u6837\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u548c\u5b9e\u65f6\u5b66\u4e60\u5de5\u4f5c\u6d41\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u7a00\u6709\u4e8b\u4ef6\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.13659", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13659", "abs": "https://arxiv.org/abs/2507.13659", "authors": ["Xiao Wang", "Qian Zhu", "Shujuan Wu", "Bo Jiang", "Shiliang Zhang", "Yaowei Wang", "Yonghong Tian", "Bin Luo"], "title": "When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework", "comment": null, "summary": "Recent researchers have proposed using event cameras for person\nre-identification (ReID) due to their promising performance and better balance\nin terms of privacy protection, event camera-based person ReID has attracted\nsignificant attention. Currently, mainstream event-based person ReID algorithms\nprimarily focus on fusing visible light and event stream, as well as preserving\nprivacy. Although significant progress has been made, these methods are\ntypically trained and evaluated on small-scale or simulated event camera\ndatasets, making it difficult to assess their real identification performance\nand generalization ability. To address the issue of data scarcity, this paper\nintroduces a large-scale RGB-event based person ReID dataset, called EvReID.\nThe dataset contains 118,988 image pairs and covers 1200 pedestrian identities,\nwith data collected across multiple seasons, scenes, and lighting conditions.\nWe also evaluate 15 state-of-the-art person ReID algorithms, laying a solid\nfoundation for future research in terms of both data and benchmarking. Based on\nour newly constructed dataset, this paper further proposes a pedestrian\nattribute-guided contrastive learning framework to enhance feature learning for\nperson re-identification, termed TriPro-ReID. This framework not only\neffectively explores the visual features from both RGB frames and event\nstreams, but also fully utilizes pedestrian attributes as mid-level semantic\nfeatures. Extensive experiments on the EvReID dataset and MARS datasets fully\nvalidated the effectiveness of our proposed RGB-Event person ReID framework.\nThe benchmark dataset and source code will be released on\nhttps://github.com/Event-AHU/Neuromorphic_ReID", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u6570\u636e\u96c6EvReID\uff0c\u5e76\u63d0\u51fa\u4e86TriPro-ReID\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u7279\u5f81\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u6a21\u62df\u6027\u5f3a\u7684\u5c40\u9650\u6027\uff0c\u8bc4\u4f30\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u6784\u5efaEvReID\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u884c\u4eba\u5c5e\u6027\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6TriPro-ReID\u3002", "result": "\u5728EvReID\u548cMARS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EvReID\u6570\u636e\u96c6\u548cTriPro-ReID\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u548c\u57fa\u51c6\u652f\u6301\u3002"}}
{"id": "2507.13739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13739", "abs": "https://arxiv.org/abs/2507.13739", "authors": ["Junsu Kim", "Yunhoe Ku", "Seungryul Baek"], "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning", "comment": "6th CLVISION ICCV Workshop accepted", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely\nlimited training data; while aiming to reduce catastrophic forgetting and learn\nnew information. We propose Diffusion-FSCIL, a novel approach that employs a\ntext-to-image diffusion model as a frozen backbone. Our conjecture is that\nFSCIL can be tackled using a large generative model's capabilities benefiting\nfrom 1) generation ability via large-scale pre-training; 2) multi-scale\nrepresentation; 3) representational flexibility through the text encoder. To\nmaximize the representation capability, we propose to extract multiple\ncomplementary diffusion features to play roles as latent replay with slight\nsupport from feature distillation for preventing generative biases. Our\nframework realizes efficiency through 1) using a frozen backbone; 2) minimal\ntrainable components; 3) batch processing of multiple feature extractions.\nExtensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that\nDiffusion-FSCIL surpasses state-of-the-art methods, preserving performance on\npreviously learned classes and adapting effectively to new ones.", "AI": {"tldr": "Diffusion-FSCIL\u5229\u7528\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u89e3\u51b3\u5c0f\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u6f5c\u5728\u91cd\u653e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c0f\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u5e76\u7ed3\u5408\u6f5c\u5728\u91cd\u653e\u4e0e\u7279\u5f81\u84b8\u998f\uff0c\u51cf\u5c11\u751f\u6210\u504f\u5dee\u3002", "result": "\u5728CUB-200\u3001miniImageNet\u548cCIFAR-100\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u5e73\u8861\u65b0\u65e7\u7c7b\u522b\u7684\u5b66\u4e60\u3002", "conclusion": "Diffusion-FSCIL\u901a\u8fc7\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\u548c\u9ad8\u6548\u8bbe\u8ba1\uff0c\u4e3aFSCIL\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13753", "abs": "https://arxiv.org/abs/2507.13753", "authors": ["Tongtong Su", "Chengyu Wang", "Bingyan Liu", "Jun Huang", "Dongming Lu"], "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis", "comment": null, "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS.", "AI": {"tldr": "EVS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5c01\u88c5\u89c6\u9891\u5408\u6210\u5668\uff0c\u7ed3\u5408T2I\u548cT2V\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u3002", "motivation": "\u73b0\u6709T2V\u6a21\u578b\u5728\u751f\u6210\u89c6\u9891\u65f6\u5b58\u5728\u753b\u9762\u95ea\u70c1\u548c\u4f2a\u5f71\u95ee\u9898\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c\u6709\u6548\u8fd0\u52a8\u8868\u73b0\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684T2I\u6a21\u578b\u4f18\u5316\u4f4e\u8d28\u91cf\u89c6\u9891\u5e27\uff0c\u5e76\u901a\u8fc7T2V\u6a21\u578b\u786e\u4fdd\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u5c06T2V\u7684\u65f6\u5e8f\u5148\u9a8c\u5c01\u88c5\u5230T2I\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEVS\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.6x-4.5x\u3002", "conclusion": "EVS\u901a\u8fc7\u7ed3\u5408T2I\u548cT2V\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.13773", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13773", "abs": "https://arxiv.org/abs/2507.13773", "authors": ["Pu Jian", "Donglei Yu", "Wen Yang", "Shuo Ren", "Jiajun Zhang"], "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions", "comment": "ACL2025 Main", "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faClearVQA\u57fa\u51c6\uff0c\u89e3\u51b3\u89c6\u89c9\u95ee\u7b54\u4e2d\u7528\u6237\u6a21\u7cca\u95ee\u9898\u7684\u4ea4\u4e92\u5f0f\u6f84\u6e05\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u901a\u8fc7\u91cd\u8ff0\u95ee\u9898\u89e3\u51b3\u6a21\u7cca\u6027\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u53cd\u9988\u7684\u4ea4\u4e92\u6027\u3002", "method": "\u5f15\u5165ClearVQA\u57fa\u51c6\uff0c\u9488\u5bf9\u4e09\u7c7b\u5e38\u89c1\u6a21\u7cca\u95ee\u9898\uff0c\u6db5\u76d6\u591a\u79cdVQA\u573a\u666f\u3002", "result": "ClearVQA\u57fa\u51c6\u586b\u8865\u4e86\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u6f84\u6e05\u80fd\u529b\u7684\u7a7a\u767d\u3002", "conclusion": "ClearVQA\u4e3aVLM\u4ea4\u4e92\u5f0f\u6f84\u6e05\u80fd\u529b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.13934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13934", "abs": "https://arxiv.org/abs/2507.13934", "authors": ["Marzieh Gheisari", "Auguste Genovesio"], "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization", "comment": null, "summary": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.", "AI": {"tldr": "DiViD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u663e\u5f0f\u5206\u79bb\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u4fe1\u606f\u6cc4\u6f0f\u548c\u6a21\u7cca\u91cd\u5efa\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVAE\u548cGAN\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u4e2d\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u5185\u5bb9\u65f6\u5b58\u5728\u4fe1\u606f\u6cc4\u6f0f\u548c\u6a21\u7cca\u91cd\u5efa\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "DiViD\u901a\u8fc7\u5e8f\u5217\u7f16\u7801\u5668\u63d0\u53d6\u5168\u5c40\u9759\u6001\u6807\u8bb0\u548c\u9010\u5e27\u52a8\u6001\u6807\u8bb0\uff0c\u7ed3\u5408\u6761\u4ef6DDPM\u89e3\u7801\u5668\uff0c\u5229\u7528\u5171\u4eab\u566a\u58f0\u8ba1\u5212\u3001\u65f6\u95f4\u53d8\u5316\u7684KL\u74f6\u9888\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u6b63\u4ea4\u6b63\u5219\u5316\u5668\u3002", "result": "DiViD\u5728\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u4ea4\u6362\u8054\u5408\u51c6\u786e\u7387\uff0c\u4fdd\u6301\u4e86\u9759\u6001\u4fdd\u771f\u5ea6\u5e76\u6539\u5584\u4e86\u52a8\u6001\u4f20\u9012\uff0c\u51cf\u5c11\u4e86\u4ea4\u53c9\u6cc4\u6f0f\u3002", "conclusion": "DiViD\u9996\u6b21\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u9759\u6001-\u52a8\u6001\u663e\u5f0f\u5206\u89e3\uff0c\u4e3a\u89c6\u9891\u5185\u5bb9\u5206\u79bb\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
