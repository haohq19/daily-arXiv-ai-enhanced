{"id": "2506.18960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18960", "abs": "https://arxiv.org/abs/2506.18960", "authors": ["Siqi Shang", "Mingyo Seo", "Yuke Zhu", "Lilly Chin"], "title": "FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation", "comment": null, "summary": "Handling delicate and fragile objects remains a major challenge for robotic\nmanipulation, especially for rigid parallel grippers. While the simplicity and\nversatility of parallel grippers have led to widespread adoption, these\ngrippers are limited by their heavy reliance on visual feedback. Tactile\nsensing and soft robotics can add responsiveness and compliance. However,\nexisting methods typically involve high integration complexity or suffer from\nslow response times. In this work, we introduce FORTE, a tactile sensing system\nembedded in compliant gripper fingers. FORTE uses 3D-printed fin-ray grippers\nwith internal air channels to provide low-latency force and slip feedback.\nFORTE applies just enough force to grasp objects without damaging them, while\nremaining easy to fabricate and integrate. We find that FORTE can accurately\nestimate grasping forces from 0-8 N with an average error of 0.2 N, and detect\nslip events within 100 ms of occurring. We demonstrate FORTE's ability to grasp\na wide range of slippery, fragile, and deformable objects. In particular, FORTE\ngrasps fragile objects like raspberries and potato chips with a 98.6% success\nrate, and achieves 93% accuracy in detecting slip events. These results\nhighlight FORTE's potential as a robust and practical solution for enabling\ndelicate robotic manipulation. Project page: https://merge-lab.github.io/FORTE"}
{"id": "2506.18927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18927", "abs": "https://arxiv.org/abs/2506.18927", "authors": ["Shriyank Somvanshi", "Md Monzurul Islam", "Gaurab Chhetri", "Rohit Chakraborty", "Mahmuda Sultana Mimi", "Swagat Ahmed Shuvo", "Kazi Sifatul Islam", "Syed Aaqib Javed", "Sharif Ahmed Rafat", "Anandi Dutta", "Subasish Das"], "title": "From Tiny Machine Learning to Tiny Deep Learning: A Survey", "comment": null, "summary": "The rapid growth of edge devices has driven the demand for deploying\nartificial intelligence (AI) at the edge, giving rise to Tiny Machine Learning\n(TinyML) and its evolving counterpart, Tiny Deep Learning (TinyDL). While\nTinyML initially focused on enabling simple inference tasks on\nmicrocontrollers, the emergence of TinyDL marks a paradigm shift toward\ndeploying deep learning models on severely resource-constrained hardware. This\nsurvey presents a comprehensive overview of the transition from TinyML to\nTinyDL, encompassing architectural innovations, hardware platforms, model\noptimization techniques, and software toolchains. We analyze state-of-the-art\nmethods in quantization, pruning, and neural architecture search (NAS), and\nexamine hardware trends from MCUs to dedicated neural accelerators.\nFurthermore, we categorize software deployment frameworks, compilers, and\nAutoML tools enabling practical on-device learning. Applications across domains\nsuch as computer vision, audio recognition, healthcare, and industrial\nmonitoring are reviewed to illustrate the real-world impact of TinyDL. Finally,\nwe identify emerging directions including neuromorphic computing, federated\nTinyDL, edge-native foundation models, and domain-specific co-design\napproaches. This survey aims to serve as a foundational resource for\nresearchers and practitioners, offering a holistic view of the ecosystem and\nlaying the groundwork for future advancements in edge AI."}
{"id": "2506.19089", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19089", "abs": "https://arxiv.org/abs/2506.19089", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "comment": "14 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available."}
{"id": "2506.19279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19279", "abs": "https://arxiv.org/abs/2506.19279", "authors": ["Zhiyang Qi", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "title": "EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition", "comment": null, "summary": "The rising demand for mental health care has fueled interest in AI-driven\ncounseling systems. While large language models (LLMs) offer significant\npotential, current approaches face challenges, including limited understanding\nof clients' psychological states and counseling stages, reliance on\nhigh-quality training data, and privacy concerns associated with commercial\ndeployment. To address these issues, we propose EmoStage, a framework that\nenhances empathetic response generation by leveraging the inference\ncapabilities of open-source LLMs without additional training data. Our\nframework introduces perspective-taking to infer clients' psychological states\nand support needs, enabling the generation of emotionally resonant responses.\nIn addition, phase recognition is incorporated to ensure alignment with the\ncounseling process and to prevent contextually inappropriate or inopportune\nresponses. Experiments conducted in both Japanese and Chinese counseling\nsettings demonstrate that EmoStage improves the quality of responses generated\nby base models and performs competitively with data-driven methods."}
{"id": "2506.19174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19174", "abs": "https://arxiv.org/abs/2506.19174", "authors": ["Jialu Pi", "Juan Maria Farina", "Rimita Lahiri", "Jiwoong Jeong", "Archana Gurudu", "Hyung-Bok Park", "Chieh-Ju Chao", "Chadi Ayoub", "Reza Arsanjani", "Imon Banerjee"], "title": "MOSCARD -- Causal Reasoning and De-confounding for Multimodal Opportunistic Screening of Cardiovascular Adverse Events", "comment": null, "summary": "Major Adverse Cardiovascular Events (MACE) remain the leading cause of\nmortality globally, as reported in the Global Disease Burden Study 2021.\nOpportunistic screening leverages data collected from routine health check-ups\nand multimodal data can play a key role to identify at-risk individuals. Chest\nX-rays (CXR) provide insights into chronic conditions contributing to major\nadverse cardiovascular events (MACE), while 12-lead electrocardiogram (ECG)\ndirectly assesses cardiac electrical activity and structural abnormalities.\nIntegrating CXR and ECG could offer a more comprehensive risk assessment than\nconventional models, which rely on clinical scores, computed tomography (CT)\nmeasurements, or biomarkers, which may be limited by sampling bias and single\nmodality constraints. We propose a novel predictive modeling framework -\nMOSCARD, multimodal causal reasoning with co-attention to align two distinct\nmodalities and simultaneously mitigate bias and confounders in opportunistic\nrisk estimation. Primary technical contributions are - (i) multimodal alignment\nof CXR with ECG guidance; (ii) integration of causal reasoning; (iii) dual\nback-propagation graph for de-confounding. Evaluated on internal, shift data\nfrom emergency department (ED) and external MIMIC datasets, our model\noutperformed single modality and state-of-the-art foundational models - AUC:\n0.75, 0.83, 0.71 respectively. Proposed cost-effective opportunistic screening\nenables early intervention, improving patient outcomes and reducing\ndisparities."}
{"id": "2506.19483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19483", "abs": "https://arxiv.org/abs/2506.19483", "authors": ["Marcos Estecha-Garitagoitia", "Chen Zhang", "Mario Rodr√≠guez-Cantelar", "Luis Fernando D'Haro"], "title": "Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models", "comment": null, "summary": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems."}
{"id": "2506.19282", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.19282", "abs": "https://arxiv.org/abs/2506.19282", "authors": ["Yang Zhou", "Xiaoning Ren"], "title": "A Batch-Insensitive Dynamic GNN Approach to Address Temporal Discontinuity in Graph Streams", "comment": "8pages, 5figures", "summary": "In dynamic graphs, preserving temporal continuity is critical. However,\nMemory-based Dynamic Graph Neural Networks (MDGNNs) trained with large batches\noften disrupt event sequences, leading to temporal information loss. This\ndiscontinuity not only deteriorates temporal modeling but also hinders\noptimization by increasing the difficulty of parameter convergence. Our\ntheoretical study quantifies this through a Lipschitz upper bound, showing that\nlarge batch sizes enlarge the parameter search space. In response, we propose\nBADGNN, a novel batch-agnostic framework consisting of two core components: (1)\nTemporal Lipschitz Regularization (TLR) to control parameter search space\nexpansion, and (2) Adaptive Attention Adjustment (A3) to alleviate attention\ndistortion induced by both regularization and batching. Empirical results on\nthree benchmark datasets show that BADGNN maintains strong performance while\nenabling significantly larger batch sizes and faster training compared to TGN.\nOur code is available at Code:\nhttps://anonymous.4open.science/r/TGN_Lipichitz-C033/."}
{"id": "2506.19291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19291", "abs": "https://arxiv.org/abs/2506.19291", "authors": ["Xiaoyuan Wang", "Yizhou Zhao", "Botao Ye", "Xiaojun Shan", "Weijie Lyu", "Lu Qi", "Kelvin C. K. Chan", "Yinxiao Li", "Ming-Hsuan Yang"], "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis", "comment": null, "summary": "We propose HoliGS, a novel deformable Gaussian splatting framework that\naddresses embodied view synthesis from long monocular RGB videos. Unlike prior\n4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training\noverhead in minute-long captures, our method leverages invertible Gaussian\nSplatting deformation networks to reconstruct large-scale, dynamic environments\naccurately. Specifically, we decompose each scene into a static background plus\ntime-varying objects, each represented by learned Gaussian primitives\nundergoing global rigid transformations, skeleton-driven articulation, and\nsubtle non-rigid deformations via an invertible neural flow. This hierarchical\nwarping strategy enables robust free-viewpoint novel-view rendering from\nvarious embodied camera trajectories by attaching Gaussians to a complete\ncanonical foreground shape (\\eg, egocentric or third-person follow), which may\ninvolve substantial viewpoint changes and interactions between multiple actors.\nOur experiments demonstrate that \\ourmethod~ achieves superior reconstruction\nquality on challenging datasets while significantly reducing both training and\nrendering time compared to state-of-the-art monocular deformable NeRFs. These\nresults highlight a practical and scalable solution for EVS in real-world\nscenarios. The source code will be released."}
{"id": "2506.19548", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.19548", "abs": "https://arxiv.org/abs/2506.19548", "authors": ["Devesh Pant", "Rishi Raj Grandhe", "Vipin Samaria", "Mukul Paul", "Sudhir Kumar", "Saransh Khanna", "Jatin Agrawal", "Jushaan Singh Kalra", "Akhil VSSG", "Satish V Khalikar", "Vipin Garg", "Himanshu Chauhan", "Pranay Verma", "Neha Khandelwal", "Soma S Dhavala", "Minesh Mathew"], "title": "Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection", "comment": null, "summary": "Early detection of disease outbreaks is crucial to ensure timely intervention\nby the health authorities. Due to the challenges associated with traditional\nindicator-based surveillance, monitoring informal sources such as online media\nhas become increasingly popular. However, owing to the number of online\narticles getting published everyday, manual screening of the articles is\nimpractical. To address this, we propose Health Sentinel. It is a multi-stage\ninformation extraction pipeline that uses a combination of ML and non-ML\nmethods to extract events-structured information concerning disease outbreaks\nor other unusual health events-from online articles. The extracted events are\nmade available to the Media Scanning and Verification Cell (MSVC) at the\nNational Centre for Disease Control (NCDC), Delhi for analysis, interpretation\nand further dissemination to local agencies for timely intervention. From April\n2022 till date, Health Sentinel has processed over 300 million news articles\nand identified over 95,000 unique health events across India of which over\n3,500 events were shortlisted by the public health experts at NCDC as potential\noutbreaks."}
{"id": "2506.19416", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19416", "abs": "https://arxiv.org/abs/2506.19416", "authors": ["Yin Zhang", "Zian Ning", "Xiaoyu Zhang", "Shiliang Guo", "Peidong Liu", "Shiyu Zhao"], "title": "EvDetMAV: Generalized MAV Detection from Moving Event Cameras", "comment": "8 pages, 7 figures. This paper is accepted by IEEE Robotics and\n  Automation Letters", "summary": "Existing micro aerial vehicle (MAV) detection methods mainly rely on the\ntarget's appearance features in RGB images, whose diversity makes it difficult\nto achieve generalized MAV detection. We notice that different types of MAVs\nshare the same distinctive features in event streams due to their high-speed\nrotating propellers, which are hard to see in RGB images. This paper studies\nhow to detect different types of MAVs from an event camera by fully exploiting\nthe features of propellers in the original event stream. The proposed method\nconsists of three modules to extract the salient and spatio-temporal features\nof the propellers while filtering out noise from background objects and camera\nmotion. Since there are no existing event-based MAV datasets, we introduce a\nnovel MAV dataset for the community. This is the first event-based MAV dataset\ncomprising multiple scenarios and different types of MAVs. Without training,\nour method significantly outperforms state-of-the-art methods and can deal with\nchallenging scenarios, achieving a precision rate of 83.0\\% (+30.3\\%) and a\nrecall rate of 81.5\\% (+36.4\\%) on the proposed testing dataset. The dataset\nand code are available at: https://github.com/WindyLab/EvDetMAV."}
{"id": "2506.19342", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.19342", "abs": "https://arxiv.org/abs/2506.19342", "authors": ["Sudesh Bhagat", "Raghupathi Kandiboina", "Ibne Farabi Shihab", "Skylar Knickerbocker", "Neal Hawkins", "Anuj Sharma"], "title": "Unlocking Insights Addressing Alcohol Inference Mismatch through Database-Narrative Alignment", "comment": null, "summary": "Road traffic crashes are a significant global cause of fatalities,\nemphasizing the urgent need for accurate crash data to enhance prevention\nstrategies and inform policy development. This study addresses the challenge of\nalcohol inference mismatch (AIM) by employing database narrative alignment to\nidentify AIM in crash data. A framework was developed to improve data quality\nin crash management systems and reduce the percentage of AIM crashes. Utilizing\nthe BERT model, the analysis of 371,062 crash records from Iowa (2016-2022)\nrevealed 2,767 AIM incidents, resulting in an overall AIM percentage of 24.03%.\nStatistical tools, including the Probit Logit model, were used to explore the\ncrash characteristics affecting AIM patterns. The findings indicate that\nalcohol-related fatal crashes and nighttime incidents have a lower percentage\nof the mismatch, while crashes involving unknown vehicle types and older\ndrivers are more susceptible to mismatch. The geospatial cluster as part of\nthis study can identify the regions which have an increased need for education\nand training. These insights highlight the necessity for targeted training\nprograms and data management teams to improve the accuracy of crash reporting\nand support evidence-based policymaking."}
{"id": "2506.19750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19750", "abs": "https://arxiv.org/abs/2506.19750", "authors": ["Takashi Nishibayashi", "Seiji Kanazawa", "Kumpei Yamada"], "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach", "comment": null, "summary": "Background: Symptom Checkers (SCs) provide users with personalized medical\ninformation. To prevent performance degradation from algorithm updates, SC\ndevelopers must evaluate diagnostic performance changes for individual diseases\nbefore deployment. However, acquiring sufficient evaluation data for rare\ndiseases is difficult, and manually creating numerous clinical vignettes is\ncostly and impractical. Objective: This study proposes and validates a novel\nSynthetic Vignette Simulation Approach to evaluate diagnostic performance\nchanges for individual rare diseases following SC algorithm updates. Methods:\nWe used disease-phenotype annotations from the Human Phenotype Ontology (HPO),\na knowledge database for rare diseases, to generate synthetic vignettes. With\nthese, we simulated SC interviews to estimate the impact of algorithm updates\non real-world diagnostic performance. The method's effectiveness was evaluated\nretrospectively by comparing estimated values with actual metric changes using\nthe R 2(R-squared) coefficient. Results: The experiment included eight past SC\nalgorithm updates. For updates on diseases with frequency information in HPO\n(n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8\nchange, it was 0.78 (p=0.047), indicating the method can predict\npost-deployment performance. In contrast, large prediction errors occurred for\ndiseases without frequency information (n=3), highlighting its importance. The\nmanual effort to map HPO phenotypes to SC symptoms was approximately 2 hours\nper disease. Conclusions: Our method enables pre-deployment evaluation of SC\nalgorithm changes for individual rare diseases using a publicly available,\nexpert-created knowledge base. This transparent and low-cost approach allows\ndevelopers to efficiently improve diagnostic performance for rare diseases,\npotentially enhancing support for early diagnosis."}
{"id": "2506.19416", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19416", "abs": "https://arxiv.org/abs/2506.19416", "authors": ["Yin Zhang", "Zian Ning", "Xiaoyu Zhang", "Shiliang Guo", "Peidong Liu", "Shiyu Zhao"], "title": "EvDetMAV: Generalized MAV Detection from Moving Event Cameras", "comment": "8 pages, 7 figures. This paper is accepted by IEEE Robotics and\n  Automation Letters", "summary": "Existing micro aerial vehicle (MAV) detection methods mainly rely on the\ntarget's appearance features in RGB images, whose diversity makes it difficult\nto achieve generalized MAV detection. We notice that different types of MAVs\nshare the same distinctive features in event streams due to their high-speed\nrotating propellers, which are hard to see in RGB images. This paper studies\nhow to detect different types of MAVs from an event camera by fully exploiting\nthe features of propellers in the original event stream. The proposed method\nconsists of three modules to extract the salient and spatio-temporal features\nof the propellers while filtering out noise from background objects and camera\nmotion. Since there are no existing event-based MAV datasets, we introduce a\nnovel MAV dataset for the community. This is the first event-based MAV dataset\ncomprising multiple scenarios and different types of MAVs. Without training,\nour method significantly outperforms state-of-the-art methods and can deal with\nchallenging scenarios, achieving a precision rate of 83.0\\% (+30.3\\%) and a\nrecall rate of 81.5\\% (+36.4\\%) on the proposed testing dataset. The dataset\nand code are available at: https://github.com/WindyLab/EvDetMAV."}
{"id": "2506.19089", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19089", "abs": "https://arxiv.org/abs/2506.19089", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "comment": "14 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available."}
{"id": "2506.19697", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19697", "abs": "https://arxiv.org/abs/2506.19697", "authors": ["Jungwoo Park", "Taewhoo Lee", "Chanwoong Yoon", "Hyeon Hwang", "Jaewoo Kang"], "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models", "comment": null, "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."}
{"id": "2506.19680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19680", "abs": "https://arxiv.org/abs/2506.19680", "authors": ["Mihnea Ghitu", "Matthew Wicker", "Vihari Piratla"], "title": "Model Guidance via Robust Feature Attribution", "comment": null, "summary": "Controlling the patterns a model learns is essential to preventing reliance\non irrelevant or misleading features. Such reliance on irrelevant features,\noften called shortcut features, has been observed across domains, including\nmedical imaging and natural language processing, where it may lead to\nreal-world harms. A common mitigation strategy leverages annotations (provided\nby humans or machines) indicating which features are relevant or irrelevant.\nThese annotations are compared to model explanations, typically in the form of\nfeature salience, and used to guide the loss function during training.\nUnfortunately, recent works have demonstrated that feature salience methods are\nunreliable and therefore offer a poor signal to optimize. In this work, we\npropose a simplified objective that simultaneously optimizes for explanation\nrobustness and mitigation of shortcut learning. Unlike prior objectives with\nsimilar aims, we demonstrate theoretically why our approach ought to be more\neffective. Across a comprehensive series of experiments, we show that our\napproach consistently reduces test-time misclassifications by 20% compared to\nstate-of-the-art methods. We also extend prior experimental settings to include\nnatural language processing tasks. Additionally, we conduct novel ablations\nthat yield practical insights, including the relative importance of annotation\nquality over quantity. Code for our method and experiments is available at:\nhttps://github.com/Mihneaghitu/ModelGuidanceViaRobustFeatureAttribution."}
{"id": "2506.19847", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19847", "abs": "https://arxiv.org/abs/2506.19847", "authors": ["Zeju Qiu", "Weiyang Liu", "Adrian Weller", "Bernhard Sch√∂lkopf"], "title": "Orthogonal Finetuning Made Scalable", "comment": "Technical report (17 pages, 7 figures, project page:\n  https://spherelab.ai/oftv2/)", "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage."}
{"id": "2506.19697", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19697", "abs": "https://arxiv.org/abs/2506.19697", "authors": ["Jungwoo Park", "Taewhoo Lee", "Chanwoong Yoon", "Hyeon Hwang", "Jaewoo Kang"], "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models", "comment": null, "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."}
{"id": "2506.19279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19279", "abs": "https://arxiv.org/abs/2506.19279", "authors": ["Zhiyang Qi", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "title": "EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition", "comment": null, "summary": "The rising demand for mental health care has fueled interest in AI-driven\ncounseling systems. While large language models (LLMs) offer significant\npotential, current approaches face challenges, including limited understanding\nof clients' psychological states and counseling stages, reliance on\nhigh-quality training data, and privacy concerns associated with commercial\ndeployment. To address these issues, we propose EmoStage, a framework that\nenhances empathetic response generation by leveraging the inference\ncapabilities of open-source LLMs without additional training data. Our\nframework introduces perspective-taking to infer clients' psychological states\nand support needs, enabling the generation of emotionally resonant responses.\nIn addition, phase recognition is incorporated to ensure alignment with the\ncounseling process and to prevent contextually inappropriate or inopportune\nresponses. Experiments conducted in both Japanese and Chinese counseling\nsettings demonstrate that EmoStage improves the quality of responses generated\nby base models and performs competitively with data-driven methods."}
{"id": "2506.19342", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.19342", "abs": "https://arxiv.org/abs/2506.19342", "authors": ["Sudesh Bhagat", "Raghupathi Kandiboina", "Ibne Farabi Shihab", "Skylar Knickerbocker", "Neal Hawkins", "Anuj Sharma"], "title": "Unlocking Insights Addressing Alcohol Inference Mismatch through Database-Narrative Alignment", "comment": null, "summary": "Road traffic crashes are a significant global cause of fatalities,\nemphasizing the urgent need for accurate crash data to enhance prevention\nstrategies and inform policy development. This study addresses the challenge of\nalcohol inference mismatch (AIM) by employing database narrative alignment to\nidentify AIM in crash data. A framework was developed to improve data quality\nin crash management systems and reduce the percentage of AIM crashes. Utilizing\nthe BERT model, the analysis of 371,062 crash records from Iowa (2016-2022)\nrevealed 2,767 AIM incidents, resulting in an overall AIM percentage of 24.03%.\nStatistical tools, including the Probit Logit model, were used to explore the\ncrash characteristics affecting AIM patterns. The findings indicate that\nalcohol-related fatal crashes and nighttime incidents have a lower percentage\nof the mismatch, while crashes involving unknown vehicle types and older\ndrivers are more susceptible to mismatch. The geospatial cluster as part of\nthis study can identify the regions which have an increased need for education\nand training. These insights highlight the necessity for targeted training\nprograms and data management teams to improve the accuracy of crash reporting\nand support evidence-based policymaking."}
{"id": "2506.19847", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19847", "abs": "https://arxiv.org/abs/2506.19847", "authors": ["Zeju Qiu", "Weiyang Liu", "Adrian Weller", "Bernhard Sch√∂lkopf"], "title": "Orthogonal Finetuning Made Scalable", "comment": "Technical report (17 pages, 7 figures, project page:\n  https://spherelab.ai/oftv2/)", "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage."}
{"id": "2506.19847", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19847", "abs": "https://arxiv.org/abs/2506.19847", "authors": ["Zeju Qiu", "Weiyang Liu", "Adrian Weller", "Bernhard Sch√∂lkopf"], "title": "Orthogonal Finetuning Made Scalable", "comment": "Technical report (17 pages, 7 figures, project page:\n  https://spherelab.ai/oftv2/)", "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage."}
{"id": "2506.19697", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19697", "abs": "https://arxiv.org/abs/2506.19697", "authors": ["Jungwoo Park", "Taewhoo Lee", "Chanwoong Yoon", "Hyeon Hwang", "Jaewoo Kang"], "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models", "comment": null, "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."}
{"id": "2506.19847", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19847", "abs": "https://arxiv.org/abs/2506.19847", "authors": ["Zeju Qiu", "Weiyang Liu", "Adrian Weller", "Bernhard Sch√∂lkopf"], "title": "Orthogonal Finetuning Made Scalable", "comment": "Technical report (17 pages, 7 figures, project page:\n  https://spherelab.ai/oftv2/)", "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage."}
