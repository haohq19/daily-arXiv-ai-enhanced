{"id": "2506.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14900", "abs": "https://arxiv.org/abs/2506.14900", "authors": ["Imane Guellil", "Salom\u00e9 Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "comment": "Accepted and will be published at ACL2025 (main conference)", "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE) extraction from discharge summaries of elderly patients, a population often underrepresented in clinical NLP resources. The dataset includes 14 clinically significant AEs-such as falls, delirium, and intracranial haemorrhage, along with contextual attributes like negation, diagnosis type, and in-hospital occurrence. Uniquely, the annotation schema supports both discontinuous and overlapping entities, addressing challenges rarely tackled in prior work. We evaluate multiple models using FlairNLP across three annotation granularities: fine-grained, coarse-grained, and coarse-grained with negation. While transformer-based models (e.g., BERT-cased) achieve strong performance on document-level coarse-grained extraction (F1 = 0.943), performance drops notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly for rare events and complex attributes. These results demonstrate that despite high-level scores, significant challenges remain in detecting underrepresented AEs and capturing nuanced clinical language. Developed within a Trusted Research Environment (TRE), the dataset is available upon request via DataLoch and serves as a robust benchmark for evaluating AE extraction methods and supporting future cross-dataset generalisation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u8001\u5e74\u60a3\u8005\u51fa\u9662\u6458\u8981\u4e2d\u4e0d\u826f\u4e8b\u4ef6\uff08AE\uff09\u63d0\u53d6\u7684\u8bed\u6599\u5e93\uff0c\u586b\u8865\u4e86\u4e34\u5e8aNLP\u8d44\u6e90\u4e2d\u8001\u5e74\u4eba\u7fa4\u7684\u7a7a\u767d\u3002\u6570\u636e\u96c6\u5305\u542b14\u79cd\u4e34\u5e8a\u663e\u8457AE\u53ca\u4e0a\u4e0b\u6587\u5c5e\u6027\uff0c\u652f\u6301\u4e0d\u8fde\u7eed\u548c\u91cd\u53e0\u5b9e\u4f53\u6807\u6ce8\u3002\u8bc4\u4f30\u663e\u793a\uff0cBERT\u7b49\u6a21\u578b\u5728\u7c97\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u548c\u7f55\u89c1\u4e8b\u4ef6\u4e0a\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u8001\u5e74\u60a3\u8005\u5728\u4e34\u5e8aNLP\u8d44\u6e90\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5904\u7406\u4e0d\u8fde\u7eed\u548c\u91cd\u53e0\u5b9e\u4f53\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b14\u79cdAE\u53ca\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u652f\u6301\u590d\u6742\u5b9e\u4f53\u6807\u6ce8\u3002\u4f7f\u7528FlairNLP\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\u5728\u4e09\u4e2a\u7c92\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "result": "BERT\u5728\u6587\u6863\u7ea7\u7c97\u7c92\u5ea6\u4efb\u52a1\u4e0aF1=0.943\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff08\u5982F1=0.675\uff09\u548c\u7f55\u89c1\u4e8b\u4ef6\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5c3d\u7ba1\u6a21\u578b\u5728\u7c97\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7ec6\u7c92\u5ea6AE\u63d0\u53d6\u548c\u590d\u6742\u4e34\u5e8a\u8bed\u8a00\u4ecd\u5177\u6311\u6218\u6027\u3002\u6570\u636e\u96c6\u53ef\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002"}}
{"id": "2506.15030", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15030", "abs": "https://arxiv.org/abs/2506.15030", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "comment": "22 pages, 2 figures, 5 tables", "summary": "Social isolation and loneliness, which have been increasing in recent years strongly contribute toward suicide rates. Although social isolation and loneliness are not currently recorded within the US National Violent Death Reporting System's (NVDRS) structured variables, natural language processing (NLP) techniques can be used to identify these constructs in law enforcement and coroner medical examiner narratives. Using topic modeling to generate lexicon development and supervised learning classifiers, we developed high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over 300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic social isolation. Decedents had higher odds of chronic social isolation classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR = 3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001). We found significant predictors for other social isolation topics of recent or impending divorce, child custody loss, eviction or recent move, and break-up. Our methods can improve surveillance and prevention of social isolation and loneliness in the United States.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u6267\u6cd5\u548c\u6cd5\u533b\u53d9\u8ff0\uff0c\u8bc6\u522b\u793e\u4f1a\u9694\u79bb\u548c\u5b64\u72ec\u611f\u5bf9\u81ea\u6740\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u8d28\u91cf\u7684\u5206\u7c7b\u5668\u3002", "motivation": "\u793e\u4f1a\u9694\u79bb\u548c\u5b64\u72ec\u611f\u8fd1\u5e74\u663e\u8457\u589e\u52a0\uff0c\u4e14\u4e0e\u81ea\u6740\u7387\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u672a\u88ab\u7f8e\u56fd\u56fd\u5bb6\u66b4\u529b\u6b7b\u4ea1\u62a5\u544a\u7cfb\u7edf\u8bb0\u5f55\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4e3b\u9898\u5efa\u6a21\u751f\u6210\u8bcd\u5178\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5206\u67902002\u81f32020\u5e7430\u4e07\u4f8b\u81ea\u6740\u6848\u4f8b\u3002", "result": "\u5f00\u53d1\u51fa\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u5668\uff08F1\u5e73\u57470.86\uff0c\u51c6\u786e\u73870.82\uff09\uff0c\u8bc6\u522b\u51fa1,198\u4f8b\u6162\u6027\u793e\u4f1a\u9694\u79bb\u6848\u4f8b\uff0c\u5e76\u53d1\u73b0\u7537\u6027\u3001\u540c\u6027\u604b\u548c\u79bb\u5a5a\u8005\u98ce\u9669\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6539\u8fdb\u7f8e\u56fd\u5bf9\u793e\u4f1a\u9694\u79bb\u548c\u5b64\u72ec\u611f\u7684\u76d1\u6d4b\u4e0e\u9884\u9632\u3002"}}
{"id": "2506.14835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14835", "abs": "https://arxiv.org/abs/2506.14835", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "comment": null, "summary": "Precisely localizing 3D objects from a single image constitutes a central challenge in monocular 3D detection. While DETR-like architectures offer a powerful paradigm, their direct application in this domain encounters inherent limitations, preventing optimal performance. Our work addresses these challenges by introducing MonoVQD, a novel framework designed to fundamentally advance DETR-based monocular 3D detection. We propose three main contributions. First, we propose the Mask Separated Self-Attention mechanism that enables the integration of the denoising process into a DETR architecture. This improves the stability of Hungarian matching to achieve a consistent optimization objective. Second, we present the Variational Query Denoising technique to address the gradient vanishing problem of conventional denoising methods, which severely restricts the efficiency of the denoising process. This explicitly introduces stochastic properties to mitigate this fundamental limitation and unlock substantial performance gains. Finally, we introduce a sophisticated self-distillation strategy, leveraging insights from later decoder layers to synergistically improve query quality in earlier layers, thereby amplifying the iterative refinement process. Rigorous experimentation demonstrates that MonoVQD achieves superior performance on the challenging KITTI monocular benchmark. Highlighting its broad applicability, MonoVQD's core components seamlessly integrate into other architectures, delivering significant performance gains even in multi-view 3D detection scenarios on the nuScenes dataset and underscoring its robust generalization capabilities.", "AI": {"tldr": "MonoVQD\u662f\u4e00\u79cd\u57fa\u4e8eDETR\u7684\u5355\u76ee3D\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u63a9\u7801\u5206\u79bb\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u53d8\u5206\u67e5\u8be2\u53bb\u566a\u6280\u672f\u548c\u81ea\u84b8\u998f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3DETR\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u4f18\u5316\u6027\u80fd\u3002", "method": "1. \u63a9\u7801\u5206\u79bb\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff1b2. \u53d8\u5206\u67e5\u8be2\u53bb\u566a\u6280\u672f\uff1b3. \u81ea\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MonoVQD\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u573a\u666f\u3002"}}
{"id": "2506.15150", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15150", "abs": "https://arxiv.org/abs/2506.15150", "authors": ["Yuanlong Ji", "Xingbang Yang", "Ruoqi Zhao", "Qihan Ye", "Quan Zheng", "Yubo Fan"], "title": "Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation", "comment": null, "summary": "Gait phase estimation based on inertial measurement unit (IMU) signals facilitates precise adaptation of exoskeletons to individual gait variations. However, challenges remain in achieving high accuracy and robustness, particularly during periods of terrain changes. To address this, we develop a gait phase estimation neural network based on implicit modeling of human locomotion, which combines temporal convolution for feature extraction with transformer layers for multi-channel information fusion. A channel-wise masked reconstruction pre-training strategy is proposed, which first treats gait phase state vectors and IMU signals as joint observations of human locomotion, thus enhancing model generalization. Experimental results demonstrate that the proposed method outperforms existing baseline approaches, achieving a gait phase RMSE of $2.729 \\pm 1.071%$ and phase rate MAE of $0.037 \\pm 0.016%$ under stable terrain conditions with a look-back window of 2 seconds, and a phase RMSE of $3.215 \\pm 1.303%$ and rate MAE of $0.050 \\pm 0.023%$ under terrain transitions. Hardware validation on a hip exoskeleton further confirms that the algorithm can reliably identify gait cycles and key events, adapting to various continuous motion scenarios. This research paves the way for more intelligent and adaptive exoskeleton systems, enabling safer and more efficient human-robot interaction across diverse real-world environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIMU\u4fe1\u53f7\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u9690\u5f0f\u5efa\u6a21\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u5730\u5f62\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u53d8\u5316\u65f6\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5916\u9aa8\u9abc\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u667a\u80fd\u6027\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u5377\u79ef\u548cTransformer\u5c42\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u4e0e\u591a\u901a\u9053\u4fe1\u606f\u878d\u5408\uff0c\u5e76\u91c7\u7528\u901a\u9053\u63a9\u7801\u91cd\u5efa\u9884\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u7a33\u5b9a\u5730\u5f62\u4e0b\u6b65\u6001\u76f8\u4f4dRMSE\u4e3a2.729\u00b11.071%\uff0c\u76f8\u4f4d\u7387MAE\u4e3a0.037\u00b10.016%\uff1b\u5730\u5f62\u53d8\u5316\u65f6RMSE\u4e3a3.215\u00b11.303%\uff0cMAE\u4e3a0.050\u00b10.023%\u3002\u786c\u4ef6\u9a8c\u8bc1\u8868\u660e\u7b97\u6cd5\u80fd\u53ef\u9760\u8bc6\u522b\u6b65\u6001\u5468\u671f\u548c\u5173\u952e\u4e8b\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u667a\u80fd\u81ea\u9002\u5e94\u5916\u9aa8\u9abc\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.14830", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.14830", "abs": "https://arxiv.org/abs/2506.14830", "authors": ["Zhizhao Wen", "Ruoxin Zhang", "Chao Wang"], "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model", "comment": "Source code available; Accepted by 2025 6th International Conference on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures", "summary": "Aiming at the critical role of SSD health state prediction in data reliability assurance, this study proposes a hybrid BiGRU-MHA model that incorporates a multi-head attention mechanism to enhance the accuracy and stability of storage device health classification. The model innovatively integrates temporal feature extraction and key information focusing capabilities. Specifically, it leverages the bidirectional timing modeling advantages of the BiGRU network to capture both forward and backward dependencies of SSD degradation features. Simultaneously, the multi-head attention mechanism dynamically assigns feature weights, improving the model's sensitivity to critical health indicators. Experimental results show that the proposed model achieves classification accuracies of 92.70% on the training set and 92.44% on the test set, with a minimal performance gap of only 0.26%, demonstrating excellent generalization ability. Further analysis using the receiver operating characteristic (ROC) curve shows an area under the curve (AUC) of 0.94 on the test set, confirming the model's robust binary classification performance. This work not only presents a new technical approach for SSD health prediction but also addresses the generalization bottleneck of traditional models, offering a verifiable method with practical value for preventive maintenance of industrial-grade storage systems. The results show the model can significantly reduce data loss risks by providing early failure warnings and help optimize maintenance costs, supporting intelligent decision-making in building reliable storage systems for cloud computing data centers and edge storage environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u5411\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08BiGRU\uff09\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08MHA\uff09\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8eSSD\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "SSD\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\u5bf9\u6570\u636e\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u6a21\u578b\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u4e14\u7a33\u5b9a\u7684\u65b9\u6cd5\u3002", "method": "\u6a21\u578b\u7ed3\u5408BiGRU\u7684\u53cc\u5411\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\u548cMHA\u7684\u52a8\u6001\u7279\u5f81\u6743\u91cd\u5206\u914d\uff0c\u63d0\u53d6\u65f6\u95f4\u7279\u5f81\u5e76\u805a\u7126\u5173\u952e\u4fe1\u606f\u3002", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a92.70%\u548c92.44%\uff0cAUC\u4e3a0.94\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aSSD\u5065\u5eb7\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u6cdb\u5316\u74f6\u9888\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u53ef\u964d\u4f4e\u6570\u636e\u4e22\u5931\u98ce\u9669\u5e76\u4f18\u5316\u7ef4\u62a4\u6210\u672c\u3002"}}
{"id": "2506.14911", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.14911", "abs": "https://arxiv.org/abs/2506.14911", "authors": ["Ganyu Wang", "Boyu Wang", "Bin Gu", "Charles Ling"], "title": "Event-Driven Online Vertical Federated Learning", "comment": "Published as a conference paper at ICLR 2025", "summary": "Online learning is more adaptable to real-world scenarios in Vertical Federated Learning (VFL) compared to offline learning. However, integrating online learning into VFL presents challenges due to the unique nature of VFL, where clients possess non-intersecting feature sets for the same sample. In real-world scenarios, the clients may not receive data streaming for the disjoint features for the same entity synchronously. Instead, the data are typically generated by an \\emph{event} relevant to only a subset of clients. We are the first to identify these challenges in online VFL, which have been overlooked by previous research. To address these challenges, we proposed an event-driven online VFL framework. In this framework, only a subset of clients were activated during each event, while the remaining clients passively collaborated in the learning process. Furthermore, we incorporated \\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by online learning problems with non-convex models within a non-stationary environment. We conducted a comprehensive regret analysis of our proposed framework, specifically examining the DLR under non-convex conditions with event-driven online VFL. Extensive experiments demonstrated that our proposed framework was more stable than the existing online VFL framework under non-stationary data conditions while also significantly reducing communication and computation costs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebf\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6b65\u5230\u8fbe\u548c\u975e\u51f8\u6a21\u578b\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u5728VFL\u4e2d\u66f4\u5177\u9002\u5e94\u6027\uff0c\u4f46\u6570\u636e\u5f02\u6b65\u5230\u8fbe\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u7279\u6027\u5e26\u6765\u4e86\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\uff0c\u4ec5\u6fc0\u6d3b\u90e8\u5206\u5ba2\u6237\u7aef\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u5c40\u90e8\u9057\u61be\uff08DLR\uff09\u5904\u7406\u975e\u51f8\u548c\u975e\u5e73\u7a33\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u975e\u5e73\u7a33\u6570\u636e\u4e0b\u66f4\u7a33\u5b9a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebfVFL\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15201", "abs": "https://arxiv.org/abs/2506.15201", "authors": ["Xuelin Shen", "Jiayin Xu", "Kangsheng Yin", "Wenhan Yang"], "title": "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models", "comment": "11 pages, 6 figures, publised to ICML 2025", "summary": "The improved semantic understanding of vision-language pretrained (VLP) models has made it increasingly difficult to protect publicly posted images from being exploited by search engines and other similar tools. In this context, this paper seeks to protect users' privacy by implementing defenses at the image compression stage to prevent exploitation. Specifically, we propose a flexible coding method, termed Privacy-Shielded Image Compression (PSIC), that can produce bitstreams with multiple decoding options. By default, the bitstream is decoded to preserve satisfactory perceptual quality while preventing interpretation by VLP models. Our method also retains the original image compression functionality. With a customizable input condition, the proposed scheme can reconstruct the image that preserves its full semantic information. A Conditional Latent Trigger Generation (CLTG) module is proposed to produce bias information based on customizable conditions to guide the decoding process into different reconstructed versions, and an Uncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed to leverage the soft labels inferred from the target VLP model's uncertainty on the training data. This paper further incorporates an adaptive multi-objective optimization strategy to obtain improved encrypting performance and perceptual quality simultaneously within a unified training process. The proposed scheme is plug-and-play and can be seamlessly integrated into most existing Learned Image Compression (LIC) models. Extensive experiments across multiple downstream tasks have demonstrated the effectiveness of our design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPSIC\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89e3\u7801\u9009\u9879\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u7559\u56fe\u50cf\u538b\u7f29\u529f\u80fd\u3002", "motivation": "\u968f\u7740\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\uff0c\u516c\u5f00\u56fe\u50cf\u6613\u88ab\u641c\u7d22\u5f15\u64ce\u7b49\u5de5\u5177\u5229\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "method": "\u63d0\u51faPSIC\u65b9\u6cd5\uff0c\u901a\u8fc7Conditional Latent Trigger Generation\u6a21\u5757\u548cUncertainty-Aware Encryption-Oriented\u4f18\u5316\u51fd\u6570\uff0c\u5b9e\u73b0\u591a\u89e3\u7801\u9009\u9879\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePSIC\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u53c8\u80fd\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "PSIC\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u548c\u56fe\u50cf\u8d28\u91cf\u4f18\u5316\u3002"}}
{"id": "2506.15285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15285", "abs": "https://arxiv.org/abs/2506.15285", "authors": ["Mattia Nardon", "Stefano Messelodi", "Antonio Granata", "Fabio Poiesi", "Alberto Danese", "Davide Boscaini"], "title": "AI-driven visual monitoring of industrial assembly tasks", "comment": null, "summary": "Visual monitoring of industrial assembly tasks is critical for preventing equipment damage due to procedural errors and ensuring worker safety. Although commercial solutions exist, they typically require rigid workspace setups or the application of visual markers to simplify the problem. We introduce ViMAT, a novel AI-driven system for real-time visual monitoring of assembly tasks that operates without these constraints. ViMAT combines a perception module that extracts visual observations from multi-view video streams with a reasoning module that infers the most likely action being performed based on the observed assembly state and prior task knowledge. We validate ViMAT on two assembly tasks, involving the replacement of LEGO components and the reconfiguration of hydraulic press molds, demonstrating its effectiveness through quantitative and qualitative analysis in challenging real-world scenarios characterized by partial and uncertain visual observations. Project page: https://tev-fbk.github.io/ViMAT", "AI": {"tldr": "ViMAT\u662f\u4e00\u79cd\u65e0\u9700\u6807\u8bb0\u6216\u56fa\u5b9a\u5de5\u4f5c\u7a7a\u95f4\u8bbe\u7f6e\u7684AI\u9a71\u52a8\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9700\u8981\u56fa\u5b9a\u5de5\u4f5c\u7a7a\u95f4\u6216\u89c6\u89c9\u6807\u8bb0\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002ViMAT\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "ViMAT\u7ed3\u5408\u4e86\u4ece\u591a\u89c6\u89d2\u89c6\u9891\u6d41\u63d0\u53d6\u89c6\u89c9\u4fe1\u606f\u7684\u611f\u77e5\u6a21\u5757\u548c\u57fa\u4e8e\u89c2\u5bdf\u72b6\u6001\u4e0e\u4efb\u52a1\u77e5\u8bc6\u63a8\u7406\u52a8\u4f5c\u7684\u63a8\u7406\u6a21\u5757\u3002", "result": "\u5728LEGO\u7ec4\u4ef6\u66f4\u6362\u548c\u6db2\u538b\u6a21\u5177\u91cd\u7ec4\u4efb\u52a1\u4e2d\uff0cViMAT\u5728\u90e8\u5206\u548c\u4e0d\u786e\u5b9a\u89c6\u89c9\u89c2\u5bdf\u4e0b\u8868\u73b0\u6709\u6548\u3002", "conclusion": "ViMAT\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.15681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15681", "abs": "https://arxiv.org/abs/2506.15681", "authors": ["Byung-Kwan Lee", "Ryo Hachiuma", "Yong Man Ro", "Yu-Chiang Frank Wang", "Yueh-Hua Wu"], "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models", "comment": "Project page: https://byungkwanlee.github.io/GenRecal-page/", "summary": "Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.", "AI": {"tldr": "GenRecal\u662f\u4e00\u79cd\u901a\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u5b9e\u73b0\u8de8\u5f02\u6784VLM\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5927\u578bVLM\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u4e14\u4e0d\u540cVLM\u67b6\u6784\u5dee\u5f02\u5927\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u84b8\u998f\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGenRecal\u6846\u67b6\uff0c\u5305\u542bRecalibrator\u6a21\u5757\uff0c\u7528\u4e8e\u5bf9\u9f50\u5f02\u6784VLM\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGenRecal\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u53ca\u5927\u578b\u5f00\u6e90\u548c\u95ed\u6e90VLM\u3002", "conclusion": "GenRecal\u4e3a\u8de8\u5f02\u6784VLM\u7684\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
