{"id": "2510.14133", "categories": ["cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14133", "abs": "https://arxiv.org/abs/2510.14133", "authors": ["Edoardo Allegrini", "Ananth Shreekumar", "Z. Berkay Celik"], "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems", "comment": null, "summary": "Agentic AI systems, which leverage multiple autonomous agents and Large\nLanguage Models (LLMs), are increasingly used to address complex, multi-step\ntasks. The safety, security, and functionality of these systems are critical,\nespecially in high-stakes applications. However, the current ecosystem of\ninter-agent communication is fragmented, with protocols such as the Model\nContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol\nfor coordination being analyzed in isolation. This fragmentation creates a\nsemantic gap that prevents the rigorous analysis of system properties and\nintroduces risks such as architectural misalignment and exploitable\ncoordination issues. To address these challenges, we introduce a modeling\nframework for agentic AI systems composed of two foundational models. The\nfirst, the host agent model, formalizes the top-level entity that interacts\nwith the user, decomposes tasks, and orchestrates their execution by leveraging\nexternal agents and tools. The second, the task lifecycle model, details the\nstates and transitions of individual sub-tasks from creation to completion,\nproviding a fine-grained view of task management and error handling. Together,\nthese models provide a unified semantic framework for reasoning about the\nbehavior of multi-AI agent systems. Grounded in this framework, we define 17\nproperties for the host agent and 14 for the task lifecycle, categorized into\nliveness, safety, completeness, and fairness. Expressed in temporal logic,\nthese properties enable formal verification of system behavior, detection of\ncoordination edge cases, and prevention of deadlocks and security\nvulnerabilities. Through this effort, we introduce the first rigorously\ngrounded, domain-agnostic framework for the systematic analysis, design, and\ndeployment of correct, reliable, and robust agentic AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5efa\u6a21\u6846\u67b6\u6765\u7edf\u4e00\u5206\u6790\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u5305\u542b\u4e3b\u673a\u667a\u80fd\u4f53\u6a21\u578b\u548c\u4efb\u52a1\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u5b9a\u4e49\u4e8631\u4e2a\u7cfb\u7edf\u5c5e\u6027\u7528\u4e8e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u534f\u8bae\u788e\u7247\u5316\u5bfc\u81f4\u8bed\u4e49\u9e3f\u6c9f\uff0c\u65e0\u6cd5\u8fdb\u884c\u4e25\u683c\u7684\u7cfb\u7edf\u5c5e\u6027\u5206\u6790\uff0c\u5b58\u5728\u67b6\u6784\u9519\u4f4d\u548c\u53ef\u88ab\u5229\u7528\u7684\u534f\u8c03\u95ee\u9898\u7b49\u98ce\u9669\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u57fa\u7840\u6a21\u578b\uff1a\u4e3b\u673a\u667a\u80fd\u4f53\u6a21\u578b\uff08\u8d1f\u8d23\u4e0e\u7528\u6237\u4ea4\u4e92\u3001\u4efb\u52a1\u5206\u89e3\u548c\u534f\u8c03\u6267\u884c\uff09\u548c\u4efb\u52a1\u751f\u547d\u5468\u671f\u6a21\u578b\uff08\u8be6\u7ec6\u63cf\u8ff0\u5b50\u4efb\u52a1\u4ece\u521b\u5efa\u5230\u5b8c\u6210\u7684\u72b6\u6001\u8f6c\u6362\uff09\u3002", "result": "\u5b9a\u4e49\u4e8617\u4e2a\u4e3b\u673a\u667a\u80fd\u4f53\u5c5e\u6027\u548c14\u4e2a\u4efb\u52a1\u751f\u547d\u5468\u671f\u5c5e\u6027\uff0c\u5206\u4e3a\u6d3b\u8dc3\u6027\u3001\u5b89\u5168\u6027\u3001\u5b8c\u6574\u6027\u548c\u516c\u5e73\u6027\u56db\u7c7b\uff0c\u7528\u65f6\u5e8f\u903b\u8f91\u8868\u8fbe\uff0c\u652f\u6301\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4e25\u683c\u57fa\u7840\u3001\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u3001\u8bbe\u8ba1\u548c\u90e8\u7f72\u6b63\u786e\u3001\u53ef\u9760\u3001\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u3002"}}
{"id": "2510.14136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14136", "abs": "https://arxiv.org/abs/2510.14136", "authors": ["David Roqui", "Ad\u00e8le Cormier", "nistor Grozavu", "Ann Bourges"], "title": "A Multimodal Approach to Heritage Preservation in the Context of Climate Change", "comment": null, "summary": "Cultural heritage sites face accelerating degradation due to climate change,\nyet tradi- tional monitoring relies on unimodal analysis (visual inspection or\nenvironmental sen- sors alone) that fails to capture the complex interplay\nbetween environmental stres- sors and material deterioration. We propose a\nlightweight multimodal architecture that fuses sensor data (temperature,\nhumidity) with visual imagery to predict degradation severity at heritage\nsites. Our approach adapts PerceiverIO with two key innovations: (1) simplified\nencoders (64D latent space) that prevent overfitting on small datasets (n=37\ntraining samples), and (2) Adaptive Barlow Twins loss that encourages modality\ncomplementarity rather than redundancy. On data from Strasbourg Cathedral, our\nmodel achieves 76.9% accu- racy, a 43% improvement over standard multimodal\narchitectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.\nAblation studies reveal that sensor-only achieves 61.5% while image-only\nreaches 46.2%, confirming successful multimodal synergy. A systematic\nhyperparameter study identifies an optimal moderate correlation target ({\\tau}\n=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy\ncompared to other {\\tau} values ({\\tau} =0.1/0.5/0.7: 53.8%, {\\tau} =0.9:\n61.5%). This work demonstrates that architectural sim- plicity combined with\ncontrastive regularization enables effective multimodal learning in data-scarce\nheritage monitoring contexts, providing a foundation for AI-driven con-\nservation decision support systems.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u67b6\u6784\uff0c\u878d\u5408\u4f20\u611f\u5668\u6570\u636e\u548c\u89c6\u89c9\u56fe\u50cf\u9884\u6d4b\u6587\u5316\u9057\u4ea7\u9000\u5316\u7a0b\u5ea6\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5b9e\u73b076.9%\u51c6\u786e\u7387\uff0c\u6bd4\u6807\u51c6\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u534743%\u3002", "motivation": "\u6587\u5316\u9057\u4ea7\u5730\u56e0\u6c14\u5019\u53d8\u5316\u52a0\u901f\u9000\u5316\uff0c\u4f20\u7edf\u5355\u6a21\u6001\u76d1\u6d4b\u65e0\u6cd5\u6355\u6349\u73af\u5883\u5e94\u529b\u4e0e\u6750\u6599\u9000\u5316\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u57fa\u4e8ePerceiverIO\u7684\u8f7b\u91cf\u591a\u6a21\u6001\u67b6\u6784\uff0c\u91c7\u7528\u7b80\u5316\u7f16\u7801\u5668\uff0864D\u6f5c\u5728\u7a7a\u95f4\uff09\u9632\u6b62\u5c0f\u6570\u636e\u96c6\u8fc7\u62df\u5408\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94Barlow Twins\u635f\u5931\u4fc3\u8fdb\u6a21\u6001\u4e92\u8865\u6027\u3002", "result": "\u5728\u65af\u7279\u62c9\u65af\u5821\u5927\u6559\u5802\u6570\u636e\u4e0a\u8fbe\u523076.9%\u51c6\u786e\u7387\uff0c\u6bd4\u6807\u51c6\u591a\u6a21\u6001\u67b6\u6784\u63d0\u534743%\uff0c\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\uff08\u4f20\u611f\u566861.5%\uff0c\u56fe\u50cf46.2%\uff09\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u67b6\u6784\u7b80\u5316\u7ed3\u5408\u5bf9\u6bd4\u6b63\u5219\u5316\u53ef\u5728\u6570\u636e\u7a00\u7f3a\u7684\u9057\u4ea7\u76d1\u6d4b\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4e3aAI\u9a71\u52a8\u7684\u4fdd\u62a4\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.14245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14245", "abs": "https://arxiv.org/abs/2510.14245", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication", "comment": null, "summary": "Optical camera communication (OCC) represents a promising visible light\ncommunication technology. Nonetheless, typical OCC systems utilizing\nframe-based cameras are encumbered by limitations, including low bit rate and\nhigh processing load. To address these issues, OCC system utilizing an\nevent-based vision sensor (EVS) as receivers have been proposed. The EVS\nenables high-speed, low-latency, and robust communication due to its\nasynchronous operation and high dynamic range. In existing event-based OCC\nsystems, conventional modulation schemes such as on-off keying (OOK) and pulse\nposition modulation have been applied, however, to the best of our knowledge,\nno modulation method has been proposed that fully exploits the unique\ncharacteristics of the EVS. This paper proposes a novel modulation scheme,\ncalled the event interval modulation (EIM) scheme, specifically designed for\nevent-based OCC. EIM enables improvement in transmission speed by modulating\ninformation using the intervals between events. This paper proposes a\ntheoretical model of EIM and conducts a proof-of-concept experiment. First, the\nparameters of the EVS are tuned and customized to optimize the frequency\nresponse specifically for EIM. Then, the maximum modulation order usable in EIM\nis determined experimentally. We conduct transmission experiments based on the\nobtained parameters. Finally, we report successful transmission at 28 kbps over\n10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new\nbenchmark for bit rate in event-based OCC systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5149\u5b66\u901a\u4fe1\u8bbe\u8ba1\u7684\u65b0\u578b\u8c03\u5236\u65b9\u6848\u2014\u2014\u4e8b\u4ef6\u95f4\u9694\u8c03\u5236(EIM)\uff0c\u901a\u8fc7\u8c03\u5236\u4e8b\u4ef6\u95f4\u9694\u6765\u63d0\u5347\u4f20\u8f93\u901f\u5ea6\uff0c\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8628kbps@10\u7c73\u548c8.4kbps@50\u7c73\u7684\u4f20\u8f93\u901f\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e27\u7684\u76f8\u673aOCC\u7cfb\u7edf\u5b58\u5728\u6bd4\u7279\u7387\u4f4e\u548c\u5904\u7406\u8d1f\u8f7d\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u4e8b\u4ef6\u76f8\u673aOCC\u7cfb\u7edf\u4f7f\u7528\u7684\u4f20\u7edf\u8c03\u5236\u65b9\u6848\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u72ec\u7279\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u95f4\u9694\u8c03\u5236(EIM)\u65b9\u6848\uff0c\u901a\u8fc7\u8c03\u5236\u4e8b\u4ef6\u95f4\u9694\u6765\u7f16\u7801\u4fe1\u606f\uff1b\u4f18\u5316\u4e8b\u4ef6\u76f8\u673a\u53c2\u6570\u4ee5\u9002\u914dEIM\uff1b\u5b9e\u9a8c\u786e\u5b9a\u6700\u5927\u53ef\u7528\u8c03\u5236\u9636\u6570\uff1b\u57fa\u4e8e\u83b7\u5f97\u7684\u53c2\u6570\u8fdb\u884c\u4f20\u8f93\u5b9e\u9a8c\u3002", "result": "\u6210\u529f\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b028kbps@10\u7c73\u548c8.4kbps@50\u7c73\u7684\u4f20\u8f93\uff0c\u521b\u4e0b\u4e86\u4e8b\u4ef6\u76f8\u673aOCC\u7cfb\u7edf\u7684\u6bd4\u7279\u7387\u65b0\u7eaa\u5f55\u3002", "conclusion": "EIM\u65b9\u6848\u80fd\u591f\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u5f02\u6b65\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e8b\u4ef6\u76f8\u673aOCC\u7cfb\u7edf\u7684\u4f20\u8f93\u6027\u80fd\uff0c\u4e3a\u9ad8\u901f\u53ef\u89c1\u5149\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13843", "abs": "https://arxiv.org/abs/2510.13843", "authors": ["Zhirong Chou", "Quan Qin", "Shi Li"], "title": "Serialized EHR make for good text representations", "comment": null, "summary": "The emergence of foundation models in healthcare has opened new avenues for\nlearning generalizable representations from large scale clinical data. Yet,\nexisting approaches often struggle to reconcile the tabular and event based\nnature of Electronic Health Records (EHRs) with the sequential priors of\nnatural language models. This structural mismatch limits their ability to\ncapture longitudinal dependencies across patient encounters. We introduce\nSerialBEHRT, a domain aligned foundation model that extends SciBERT through\nadditional pretraining on structured EHR sequences. SerialBEHRT is designed to\nencode temporal and contextual relationships among clinical events, thereby\nproducing richer patient representations. We evaluate its effectiveness on the\ntask of antibiotic susceptibility prediction, a clinically meaningful problem\nin antibiotic stewardship. Through extensive benchmarking against state of the\nart EHR representation strategies, we demonstrate that SerialBEHRT achieves\nsuperior and more consistent performance, highlighting the importance of\ntemporal serialization in foundation model pretraining for healthcare.", "AI": {"tldr": "SerialBEHRT\u662f\u4e00\u4e2a\u9488\u5bf9\u533b\u7597\u9886\u57df\u7684\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5e8f\u5217\u5316\u6765\u66f4\u597d\u5730\u6355\u6349\u4e34\u5e8a\u4e8b\u4ef6\u95f4\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u6297\u751f\u7d20\u654f\u611f\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u534f\u8c03\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u8868\u683c\u548c\u4e8b\u4ef6\u6027\u8d28\u4e0e\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u5e8f\u5217\u5148\u9a8c\u4e4b\u95f4\u7684\u7ed3\u6784\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u6355\u6349\u60a3\u8005\u5c31\u8bca\u95f4\u7eb5\u5411\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u7ed3\u6784\u5316EHR\u5e8f\u5217\u7684\u989d\u5916\u9884\u8bad\u7ec3\u6269\u5c55SciBERT\uff0c\u8bbe\u8ba1SerialBEHRT\u6a21\u578b\u6765\u7f16\u7801\u4e34\u5e8a\u4e8b\u4ef6\u95f4\u7684\u65f6\u95f4\u548c\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002", "result": "\u5728\u6297\u751f\u7d20\u654f\u611f\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0cSerialBEHRT\u76f8\u6bd4\u6700\u5148\u8fdb\u7684EHR\u8868\u793a\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u548c\u66f4\u4e00\u81f4\u7684\u6027\u80fd\u3002", "conclusion": "\u533b\u7597\u9886\u57df\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u65f6\u5e8f\u5e8f\u5217\u5316\u7684\u91cd\u8981\u6027\u5f97\u5230\u9a8c\u8bc1\uff0cSerialBEHRT\u80fd\u591f\u4ea7\u751f\u66f4\u4e30\u5bcc\u7684\u60a3\u8005\u8868\u793a\u3002"}}
{"id": "2510.14255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14255", "abs": "https://arxiv.org/abs/2510.14255", "authors": ["Liao Shen", "Wentao Jiang", "Yiran Zhu", "Tiezheng Ge", "Zhiguo Cao", "Bo Zheng"], "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization", "comment": null, "summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable\nprogress in synthesizing high-quality, temporally coherent videos from static\nimages. Among all the applications of I2V, human-centric video generation\nincludes a large portion. However, existing I2V models encounter difficulties\nin maintaining identity consistency between the input human image and the\ngenerated video, especially when the person in the video exhibits significant\nexpression changes and movements. This issue becomes critical when the human\nface occupies merely a small fraction of the image. Since humans are highly\nsensitive to identity variations, this poses a critical yet under-explored\nchallenge in I2V generation. In this paper, we propose Identity-Preserving\nReward-guided Optimization (IPRO), a novel video diffusion framework based on\nreinforcement learning to enhance identity preservation. Instead of introducing\nauxiliary modules or altering model architectures, our approach introduces a\ndirect and effective tuning algorithm that optimizes diffusion models using a\nface identity scorer. To improve performance and accelerate convergence, our\nmethod backpropagates the reward signal through the last steps of the sampling\nchain, enabling richer gradient feedback. We also propose a novel facial\nscoring mechanism that treats faces in ground-truth videos as facial feature\npools, providing multi-angle facial information to enhance generalization. A\nKL-divergence regularization is further incorporated to stabilize training and\nprevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V\nmodel and our in-house I2V model demonstrate the effectiveness of our method.\nOur project and code are available at\n\\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.", "AI": {"tldr": "\u63d0\u51faIPRO\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4eba\u8138\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u4eba\u8138\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5f53\u4eba\u8138\u5728\u56fe\u50cf\u4e2d\u5360\u6bd4\u8f83\u5c0f\u4e14\u8868\u60c5\u52a8\u4f5c\u53d8\u5316\u5927\u65f6\uff0c\u4eba\u7c7b\u5bf9\u8eab\u4efd\u53d8\u5316\u9ad8\u5ea6\u654f\u611f\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u4f7f\u7528\u4eba\u8138\u8eab\u4efd\u8bc4\u5206\u5668\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u5956\u52b1\u4fe1\u53f7\u548cKL\u6563\u5ea6\u6b63\u5219\u5316\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728Wan 2.2 I2V\u6a21\u578b\u548c\u5185\u90e8I2V\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "IPRO\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4eba\u8138\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3002"}}
{"id": "2510.14266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14266", "abs": "https://arxiv.org/abs/2510.14266", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment", "comment": null, "summary": "We propose a robust demodulation scheme for optical camera communication\nsystems using an event-based vision sensor, combining OOK with toggle\ndemodulation and a digital phase-locked loop. This is the first report to\nachieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor\nexperiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u89c6\u89c9\u4f20\u611f\u5668\u7684\u5149\u5b66\u76f8\u673a\u901a\u4fe1\u7cfb\u7edf\u9c81\u68d2\u89e3\u8c03\u65b9\u6848\uff0c\u7ed3\u5408OOK\u4e0e\u5207\u6362\u89e3\u8c03\u53ca\u6570\u5b57\u9501\u76f8\u73af\uff0c\u5728\u5ba4\u5916\u5b9e\u9a8c\u4e2d\u9996\u6b21\u5b9e\u73b0\u4e86200\u7c73-60kbps\u548c400\u7c73-30kbps\u4e0bBER < 10^{-3}\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5149\u5b66\u76f8\u673a\u901a\u4fe1\u7cfb\u7edf\u5728\u957f\u8ddd\u79bb\u4f20\u8f93\u4e2d\u7684\u9c81\u68d2\u89e3\u8c03\u95ee\u9898\uff0c\u63d0\u9ad8\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4e8b\u4ef6\u89c6\u89c9\u4f20\u611f\u5668\uff0c\u7ed3\u5408OOK\u8c03\u5236\u4e0e\u5207\u6362\u89e3\u8c03\u6280\u672f\uff0c\u5e76\u91c7\u7528\u6570\u5b57\u9501\u76f8\u73af\u8fdb\u884c\u4fe1\u53f7\u540c\u6b65\u3002", "result": "\u5728\u5ba4\u5916\u5b9e\u9a8c\u4e2d\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86200\u7c73\u8ddd\u79bb60kbps\u548c400\u7c73\u8ddd\u79bb30kbps\u4f20\u8f93\u901f\u7387\u4e0b\uff0c\u8bef\u7801\u7387\u4f4e\u4e8e10^{-3}\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u5149\u5b66\u76f8\u673a\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u957f\u8ddd\u79bb\u9c81\u68d2\u89e3\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2510.14073", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14073", "abs": "https://arxiv.org/abs/2510.14073", "authors": ["Tommaso Mencattini", "Riccardo Cadei", "Francesco Locatello"], "title": "Exploratory Causal Inference in SAEnce", "comment": null, "summary": "Randomized Controlled Trials are one of the pillars of science; nevertheless,\nthey rely on hand-crafted hypotheses and expensive analysis. Such constraints\nprevent causal effect estimation at scale, potentially anchoring on popular yet\nincomplete hypotheses. We propose to discover the unknown effects of a\ntreatment directly from data. For this, we turn unstructured data from a trial\ninto meaningful representations via pretrained foundation models and interpret\nthem via a sparse autoencoder. However, discovering significant causal effects\nat the neural level is not trivial due to multiple-testing issues and effects\nentanglement. To address these challenges, we introduce Neural Effect Search, a\nnovel recursive procedure solving both issues by progressive stratification.\nAfter assessing the robustness of our algorithm on semi-synthetic experiments,\nwe showcase, in the context of experimental ecology, the first successful\nunsupervised causal effect identification on a real-world scientific trial.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeural Effect Search\u7684\u65e0\u76d1\u7763\u56e0\u679c\u6548\u5e94\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4ece\u8bd5\u9a8c\u6570\u636e\u4e2d\u81ea\u52a8\u53d1\u73b0\u672a\u77e5\u7684\u56e0\u679c\u6548\u5e94\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u4f9d\u8d56\u4eba\u5de5\u5047\u8bbe\u548c\u6602\u8d35\u5206\u6790\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u662f\u79d1\u5b66\u7684\u91cd\u8981\u652f\u67f1\uff0c\u4f46\u4f9d\u8d56\u4eba\u5de5\u5047\u8bbe\u548c\u6602\u8d35\u5206\u6790\uff0c\u9650\u5236\u4e86\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u89c4\u6a21\uff0c\u53ef\u80fd\u5bfc\u81f4\u56fa\u5b88\u6d41\u884c\u4f46\u4e0d\u5b8c\u6574\u7684\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5c06\u975e\u7ed3\u6784\u5316\u8bd5\u9a8c\u6570\u636e\u8f6c\u5316\u4e3a\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u89e3\u91ca\uff0c\u5e76\u5f15\u5165Neural Effect Search\u9012\u5f52\u7a0b\u5e8f\u901a\u8fc7\u6e10\u8fdb\u5206\u5c42\u89e3\u51b3\u591a\u91cd\u68c0\u9a8c\u548c\u6548\u5e94\u7ea0\u7f20\u95ee\u9898\u3002", "result": "\u5728\u534a\u5408\u6210\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u751f\u6001\u5b66\u80cc\u666f\u4e0b\u9996\u6b21\u6210\u529f\u5b9e\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u79d1\u5b66\u8bd5\u9a8c\u4e2d\u7684\u65e0\u76d1\u7763\u56e0\u679c\u6548\u5e94\u8bc6\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u672a\u77e5\u7684\u56e0\u679c\u6548\u5e94\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u7684\u9650\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.14538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14538", "abs": "https://arxiv.org/abs/2510.14538", "authors": ["Emanuele Marconato", "Samuele Bortolotti", "Emile van Krieken", "Paolo Morettin", "Elena Umili", "Antonio Vergari", "Efthymia Tsamoura", "Andrea Passerini", "Stefano Teso"], "title": "Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts", "comment": null, "summary": "Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose\npredictions comply with prior knowledge encoding, e.g. safety or structural\nconstraints. As such, it represents one of the most promising avenues for\nreliable and trustworthy AI. The core idea behind NeSy AI is to combine neural\nand symbolic steps: neural networks are typically responsible for mapping\nlow-level inputs into high-level symbolic concepts, while symbolic reasoning\ninfers predictions compatible with the extracted concepts and the prior\nknowledge. Despite their promise, it was recently shown that - whenever the\nconcepts are not supervised directly - NeSy models can be affected by Reasoning\nShortcuts (RSs). That is, they can achieve high label accuracy by grounding the\nconcepts incorrectly. RSs can compromise the interpretability of the model's\nexplanations, performance in out-of-distribution scenarios, and therefore\nreliability. At the same time, RSs are difficult to detect and prevent unless\nconcept supervision is available, which is typically not the case. However, the\nliterature on RSs is scattered, making it difficult for researchers and\npractitioners to understand and tackle this challenging problem. This overview\naddresses this issue by providing a gentle introduction to RSs, discussing\ntheir causes and consequences in intuitive terms. It also reviews and\nelucidates existing theoretical characterizations of this phenomenon. Finally,\nit details methods for dealing with RSs, including mitigation and awareness\nstrategies, and maps their benefits and limitations. By reformulating advanced\nmaterial in a digestible form, this overview aims to provide a unifying\nperspective on RSs to lower the bar to entry for tackling them. Ultimately, we\nhope this overview contributes to the development of reliable NeSy and\ntrustworthy AI models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5bf9\u795e\u7ecf\u7b26\u53f7AI\u4e2d\u63a8\u7406\u6377\u5f84\u95ee\u9898\u7684\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u5176\u6210\u56e0\u3001\u540e\u679c\uff0c\u5e76\u56de\u987e\u4e86\u73b0\u6709\u7684\u7406\u8bba\u7279\u5f81\u548c\u5e94\u5bf9\u65b9\u6cd5\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7AI\u6a21\u578b\u5728\u6982\u5ff5\u672a\u76f4\u63a5\u76d1\u7763\u65f6\u5bb9\u6613\u53d7\u5230\u63a8\u7406\u6377\u5f84\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u635f\u5bb3\u6a21\u578b\u89e3\u91ca\u6027\u3001\u5206\u5e03\u5916\u6027\u80fd\u53ca\u53ef\u9760\u6027\u3002\u73b0\u6709\u6587\u732e\u5206\u6563\uff0c\u9700\u8981\u7edf\u4e00\u89c6\u89d2\u6765\u5e2e\u52a9\u7814\u7a76\u8005\u7406\u89e3\u548c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6e29\u548c\u4ecb\u7ecd\u63a8\u7406\u6377\u5f84\uff0c\u76f4\u89c2\u8ba8\u8bba\u5176\u6210\u56e0\u548c\u540e\u679c\uff0c\u56de\u987e\u73b0\u6709\u7406\u8bba\u7279\u5f81\uff0c\u8be6\u7ec6\u9610\u8ff0\u5305\u62ec\u7f13\u89e3\u548c\u610f\u8bc6\u7b56\u7565\u5728\u5185\u7684\u5e94\u5bf9\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u4f18\u7f3a\u70b9\u3002", "result": "\u63d0\u4f9b\u4e86\u63a8\u7406\u6377\u5f84\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u964d\u4f4e\u4e86\u7814\u7a76\u8005\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u95e8\u69db\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7AI\u548c\u53ef\u4fe1AI\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u9ad8\u7ea7\u6750\u6599\u4f7f\u5176\u6613\u4e8e\u7406\u89e3\uff0c\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u63a8\u7406\u6377\u5f84\u63d0\u4f9b\u7edf\u4e00\u89c6\u89d2\uff0c\u4fc3\u8fdb\u53ef\u9760\u795e\u7ecf\u7b26\u53f7AI\u548c\u53ef\u4fe1AI\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.14560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14560", "abs": "https://arxiv.org/abs/2510.14560", "authors": ["Yulin Zhang", "Cheng Shi", "Yang Wang", "Sibei Yang"], "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video", "comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)", "summary": "Envision an AI capable of functioning in human-like settings, moving beyond\nmere observation to actively understand, anticipate, and proactively respond to\nunfolding events. Towards this vision, we focus on the innovative task where,\ngiven ego-streaming video input, an assistant proactively answers diverse,\nevolving questions at the opportune moment, while maintaining synchronized\nperception and reasoning. This task embodies three key properties: (1)\nProactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized\nEfficiency. To evaluate and address these properties, we first introduce\nESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a\nnovel framework designed for their rigorous assessment. Secondly, we propose a\ncomprehensive technical pipeline to enable models to tackle this challenging\ntask. This pipeline comprises: (1) a data engine, (2) a multi-stage training\nstrategy, and (3) a proactive dynamic compression technique. Our proposed model\neffectively addresses these critical properties while outperforming multiple\nbaselines across diverse online and offline benchmarks. Project\nPage:https://zhangyl4.github.io/publications/eyes-wide-open/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e3b\u52a8\u5f0fAI\u52a9\u624b\u4efb\u52a1\uff0c\u80fd\u591f\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u6d41\u4e2d\u7406\u89e3\u3001\u9884\u6d4b\u5e76\u4e3b\u52a8\u56de\u7b54\u52a8\u6001\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u611f\u77e5\u4e0e\u63a8\u7406\u7684\u540c\u6b65\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u4e3b\u52a8\u7406\u89e3\u548c\u54cd\u5e94\u4e8b\u4ef6\u7684AI\u7cfb\u7edf\uff0c\u8d85\u8d8a\u88ab\u52a8\u89c2\u5bdf\uff0c\u5b9e\u73b0\u4e3b\u52a8\u611f\u77e5\u548c\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u6570\u636e\u5f15\u64ce\u3001\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u4e3b\u52a8\u52a8\u6001\u538b\u7f29\u6280\u672f\u7684\u5b8c\u6574\u6280\u672f\u6d41\u7a0b\uff0c\u5e76\u5f15\u5165\u4e86ESTP-Bench\u57fa\u51c6\u548cESTP-F1\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u591a\u4e2a\u5728\u7ebf\u548c\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e3b\u52a8\u4e00\u81f4\u6027\u3001\u53ca\u65f6\u54cd\u5e94\u548c\u540c\u6b65\u6548\u7387\u7b49\u5173\u952e\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u73b0\u80fd\u591f\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u4e3b\u52a8\u8fd0\u4f5c\u7684AI\u7cfb\u7edf\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.14286", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14286", "abs": "https://arxiv.org/abs/2510.14286", "authors": ["Mayank Keoliya", "Seewon Choi", "Rajeev Alur", "Mayur Naik", "Eric Wong"], "title": "Stable Prediction of Adverse Events in Medical Time-Series Data", "comment": "18 pages, 3 Figures", "summary": "Early event prediction (EEP) systems continuously estimate a patient's\nimminent risk to support clinical decision-making. For bedside trust, risk\ntrajectories must be accurate and temporally stable, shifting only with new,\nrelevant evidence. However, current benchmarks (a) ignore stability of risk\nscores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior\nuntested. To address this gap, we introduce CAREBench, an EEP benchmark that\nevaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,\nand clinical text-and assesses temporal stability alongside predictive\naccuracy. We propose a stability metric that quantifies short-term variability\nin per-patient risk and penalizes abrupt oscillations based on local-Lipschitz\nconstants. CAREBench spans six prediction tasks such as sepsis onset and\ncompares classical learners, deep sequence models, and zero-shot LLMs. Across\ntasks, existing methods, especially LLMs, struggle to jointly optimize accuracy\nand stability, with notably poor recall at high-precision operating points.\nThese results highlight the need for models that produce evidence-aligned,\nstable trajectories to earn clinician trust in continuous monitoring settings.\n(Code: https://github.com/SeewonChoi/CAREBench.)", "AI": {"tldr": "CAREBench\u662f\u4e00\u4e2a\u65e9\u671f\u4e8b\u4ef6\u9884\u6d4b\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u65e9\u671f\u4e8b\u4ef6\u9884\u6d4b\u7cfb\u7edf\u5ffd\u89c6\u98ce\u9669\u8bc4\u5206\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u4e14\u4e3b\u8981\u57fa\u4e8e\u8868\u683c\u6570\u636e\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u98ce\u9669\u8f68\u8ff9\u884c\u4e3a\u7684\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faCAREBench\u57fa\u51c6\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u8f93\u5165\uff08\u8868\u683cEHR\u3001ECG\u6ce2\u5f62\u3001\u4e34\u5e8a\u6587\u672c\uff09\uff0c\u5f15\u5165\u57fa\u4e8e\u5c40\u90e8Lipschitz\u5e38\u6570\u7684\u7a33\u5b9a\u6027\u6307\u6807\u6765\u91cf\u5316\u77ed\u671f\u98ce\u9669\u53d8\u5f02\u6027\u3002", "result": "\u5728\u516d\u4e2a\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u7279\u522b\u662fLLMs\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u70b9\u53ec\u56de\u7387\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ea7\u751f\u8bc1\u636e\u5bf9\u9f50\u3001\u7a33\u5b9a\u8f68\u8ff9\u7684\u6a21\u578b\uff0c\u4ee5\u5728\u8fde\u7eed\u76d1\u6d4b\u73af\u5883\u4e2d\u8d62\u5f97\u4e34\u5e8a\u533b\u751f\u7684\u4fe1\u4efb\u3002"}}
{"id": "2510.14624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14624", "abs": "https://arxiv.org/abs/2510.14624", "authors": ["Natan Bagrov", "Eugene Khvedchenia", "Borys Tymchenko", "Shay Aharon", "Lior Kadoch", "Tomer Keren", "Ofri Masad", "Yonatan Geifman", "Ran Zilberstein", "Tuomas Rintamaki", "Matthieu Le", "Andrew Tao"], "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference", "comment": null, "summary": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality.", "AI": {"tldr": "EVS\u662f\u4e00\u79cd\u7b80\u5355\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u526a\u679d\u65f6\u95f4\u4e0a\u9759\u6001\u7684\u8865\u4e01\u6765\u51cf\u5c11\u89c6\u9891\u4e2d\u7684\u4ee4\u724c\u5197\u4f59\uff0c\u663e\u8457\u964d\u4f4e\u4ee4\u724c\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u9759\u6001\u56fe\u50cf\u7406\u89e3\u6269\u5c55\u5230\u89c6\u9891\u63a8\u7406\u65f6\uff0c\u5904\u7406\u5bc6\u96c6\u5e27\u5e8f\u5217\u7684\u4e8c\u6b21\u6210\u672c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u957f\u89c6\u9891\u7ecf\u5e38\u8d85\u51fa\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u7684\u4ee4\u724c\u9884\u7b97\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u5f15\u5165\u9ad8\u6548\u89c6\u9891\u91c7\u6837(EVS)\uff0c\u8bc6\u522b\u5e76\u526a\u679d\u65f6\u95f4\u4e0a\u9759\u6001\u7684\u8865\u4e01\u2014\u2014\u5728\u8fde\u7eed\u5e27\u95f4\u4fdd\u6301\u4e0d\u53d8\u7684\u7a7a\u95f4\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u4f4d\u7f6e\u8eab\u4efd\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u6216\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "EVS\u663e\u8457\u51cf\u5c11\u4ee4\u724c\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u4f7f\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\uff0c\u8f93\u5165\u5e8f\u5217\u66f4\u957f\u3002\u5728\u63a8\u7406\u65f6\u5e94\u7528\uff0cEVS\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9996\u4ee4\u724c\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe4\u500d\uff0c\u51c6\u786e\u7387\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "EVS\u6301\u7eed\u6539\u8fdb\u6548\u7387-\u51c6\u786e\u5ea6\u6743\u8861\uff0c\u5728\u4e0d\u727a\u7272\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u3002"}}
{"id": "2510.14386", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.14386", "abs": "https://arxiv.org/abs/2510.14386", "authors": ["Kartikay Agrawal", "Abhijeet Vikram", "Vedant Sharma", "Vaishnavi N.", "Ayon Borthakur"], "title": "SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences", "comment": null, "summary": "In recent years, with the emergence of large models, there has been a\nsignificant interest in spiking neural networks (SNNs) primarily due to their\nenergy efficiency, multiplication-free, and sparse event-based deep learning.\nSimilarly, state space models (SSMs) in varying designs have evolved as a\npowerful alternative to transformers for target modeling in long sequences,\nthereby overcoming the quadratic dependence on sequence length of a\ntransformer. Inspired by this progress, we here design SHaRe-SSM (Spiking\nHarmonic Resonate and Fire State Space Model), for target variable modeling\n(including both classification and regression) for very-long-range sequences.\nOur second-order spiking SSM, on average, performs better than transformers or\nfirst-order SSMs while circumventing multiplication operations, making it ideal\nfor resource-constrained applications. The proposed block consumes $73 \\times$\nless energy than second-order ANN-based SSMs for an 18k sequence, while\nretaining performance. To ensure learnability over the long-range sequences, we\npropose exploiting the stable and efficient implementation of the dynamical\nsystem using parallel scans. Moreover, for the first time, we propose a\nkernel-based spiking regressor using resonate and fire neurons for very\nlong-range sequences. Our network shows superior performance on even a 50k\nsequence while being significantly energy-efficient. In addition, we conducted\na systematic analysis of the impact of heterogeneity, dissipation, and\nconservation in resonate-and-fire SSMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86SHaRe-SSM\uff08\u5c16\u5cf0\u8c10\u6ce2\u5171\u632f\u653e\u7535\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u8d85\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u4e58\u6cd5\u81ea\u7531\u3001\u80fd\u91cf\u9ad8\u6548\u7684\u4e8c\u9636\u5c16\u5cf0\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "motivation": "\u7ed3\u5408\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u80fd\u91cf\u6548\u7387\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u4f18\u52bf\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u5f00\u53d1\u9ad8\u6548\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e8c\u9636\u5c16\u5cf0SSM\uff0c\u5229\u7528\u5e76\u884c\u626b\u63cf\u5b9e\u73b0\u52a8\u6001\u7cfb\u7edf\u7684\u7a33\u5b9a\u9ad8\u6548\u5b9e\u73b0\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u57fa\u4e8e\u6838\u7684\u5c16\u5cf0\u56de\u5f52\u5668\u7528\u4e8e\u8d85\u957f\u5e8f\u5217\u3002", "result": "\u572818k\u5e8f\u5217\u4e0a\u6bd4\u4e8c\u9636ANN-based SSMs\u8282\u770173\u500d\u80fd\u91cf\uff0c\u572850k\u5e8f\u5217\u4e0a\u4ecd\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\uff0c\u5e73\u5747\u8868\u73b0\u4f18\u4e8etransformer\u548c\u4e00\u9636SSMs\u3002", "conclusion": "SHaRe-SSM\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u662f\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u4e2d\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.13931", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13931", "abs": "https://arxiv.org/abs/2510.13931", "authors": ["Siying Liu", "Shisheng Zhang", "Indu Bala"], "title": "Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions", "comment": "Preprint of a paper accepted as a poster at the NeurIPS 2025 Workshop\n  on Generative AI for Health (GenAI4Health). The final camera-ready workshop\n  version may differ. Licensed under CC BY 4.0", "summary": "Large language models (LLMs) are increasingly applied in biomedical domains,\nyet their reliability in drug-safety prediction remains underexplored. In this\nwork, we investigate whether LLMs incorporate socio-demographic information\ninto adverse event (AE) predictions, despite such attributes being clinically\nirrelevant. Using structured data from the United States Food and Drug\nAdministration Adverse Event Reporting System (FAERS) and a persona-based\nevaluation framework, we assess two state-of-the-art models, ChatGPT-4o and\nBio-Medical-Llama-3.8B, across diverse personas defined by education, marital\nstatus, employment, insurance, language, housing stability, and religion. We\nfurther evaluate performance across three user roles (general practitioner,\nspecialist, patient) to reflect real-world deployment scenarios where\ncommercial systems often differentiate access by user type. Our results reveal\nsystematic disparities in AE prediction accuracy. Disadvantaged groups (e.g.,\nlow education, unstable housing) were frequently assigned higher predicted AE\nlikelihoods than more privileged groups (e.g., postgraduate-educated, privately\ninsured). Beyond outcome disparities, we identify two distinct modes of bias:\nexplicit bias, where incorrect predictions directly reference persona\nattributes in reasoning traces, and implicit bias, where predictions are\ninconsistent, yet personas are not explicitly mentioned. These findings expose\ncritical risks in applying LLMs to pharmacovigilance and highlight the urgent\nneed for fairness-aware evaluation protocols and mitigation strategies before\nclinical deployment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u836f\u7269\u5b89\u5168\u9884\u6d4b\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5f31\u52bf\u7fa4\u4f53\u88ab\u5206\u914d\u66f4\u9ad8\u7684\u4e0d\u826f\u4e8b\u4ef6\u9884\u6d4b\u6982\u7387\uff0c\u5b58\u5728\u663e\u6027\u548c\u9690\u6027\u4e24\u79cd\u504f\u89c1\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u5e94\u7528\u65f6\u662f\u5426\u4f1a\u5c06\u793e\u4f1a\u4eba\u53e3\u5b66\u4fe1\u606f\u7eb3\u5165\u4e34\u5e8a\u65e0\u5173\u7684\u4e0d\u826f\u4e8b\u4ef6\u9884\u6d4b\uff0c\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528FDA\u4e0d\u826f\u4e8b\u4ef6\u62a5\u544a\u7cfb\u7edf\u7ed3\u6784\u5316\u6570\u636e\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u8bd5ChatGPT-4o\u548cBio-Medical-Llama-3.8B\u6a21\u578b\u5728\u4e0d\u540c\u793e\u4f1a\u4eba\u53e3\u7279\u5f81\u548c\u7528\u6237\u89d2\u8272\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u9884\u6d4b\u51c6\u786e\u6027\u5dee\u5f02\uff0c\u5f31\u52bf\u7fa4\u4f53\u88ab\u5206\u914d\u66f4\u9ad8\u7684\u4e0d\u826f\u4e8b\u4ef6\u9884\u6d4b\u6982\u7387\uff0c\u8bc6\u522b\u51fa\u663e\u6027\u504f\u89c1\uff08\u63a8\u7406\u4e2d\u76f4\u63a5\u5f15\u7528\u89d2\u8272\u5c5e\u6027\uff09\u548c\u9690\u6027\u504f\u89c1\uff08\u9884\u6d4b\u4e0d\u4e00\u81f4\u4f46\u672a\u63d0\u53ca\u89d2\u8272\uff09\u3002", "conclusion": "LLMs\u5728\u836f\u7269\u8b66\u6212\u5e94\u7528\u4e2d\u5b58\u5728\u4e25\u91cd\u98ce\u9669\uff0c\u4e34\u5e8a\u90e8\u7f72\u524d\u8feb\u5207\u9700\u8981\u516c\u5e73\u6027\u8bc4\u4f30\u534f\u8bae\u548c\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2510.14545", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14545", "abs": "https://arxiv.org/abs/2510.14545", "authors": ["Guanting Dong", "Licheng Bao", "Zhongyuan Wang", "Kangzhi Zhao", "Xiaoxi Li", "Jiajie Jin", "Jinghan Yang", "Hangyu Mao", "Fuzheng Zhang", "Kun Gai", "Guorui Zhou", "Yutao Zhu", "Ji-Rong Wen", "Zhicheng Dou"], "title": "Agentic Entropy-Balanced Policy Optimization", "comment": "Working in progress", "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant\nprogress in incentivizing the multi-turn, long-horizon tool-use capabilities of\nweb agents. While mainstream agentic RL algorithms autonomously explore\nhigh-uncertainty tool-call steps under the guidance of entropy, excessive\nreliance on entropy signals can impose further constraints, leading to the\ntraining collapse. In this paper, we delve into the challenges caused by\nentropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an\nagentic RL algorithm designed to balance entropy in both the rollout and policy\nupdate phases. AEPO comprises two core components: (1) a dynamic\nentropy-balanced rollout mechanism that adaptively allocate global and branch\nsampling budget through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent over-branching issues;\nand (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient\noperation into the high-entropy clipping term to preserve and properly rescale\ngradients on high-entropy tokens, while incorporating entropy-aware advantage\nestimation to prioritize learning on high-uncertainty tokens. Results across 14\nchallenging datasets show that AEPO consistently outperforms 7 mainstream RL\nalgorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive\nresults: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker\nfor Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on\nWebWalker for Pass@5. Further analysis reveals that AEPO improves rollout\nsampling diversity while maintaining stable policy entropy, facilitating\nscalable web agent training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgentic Entropy-Balanced Policy Optimization (AEPO)\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u71b5\u5e73\u8861\u673a\u5236\u89e3\u51b3Agentic RL\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u71b5\u4fe1\u53f7\u5bfc\u81f4\u7684\u8bad\u7ec3\u5d29\u6e83\u95ee\u9898\uff0c\u572814\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e7\u79cd\u4e3b\u6d41RL\u7b97\u6cd5\u3002", "motivation": "\u4e3b\u6d41Agentic RL\u7b97\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u71b5\u4fe1\u53f7\u6765\u5f15\u5bfc\u63a2\u7d22\u9ad8\u4e0d\u786e\u5b9a\u6027\u5de5\u5177\u8c03\u7528\u6b65\u9aa4\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u71b5\u4f1a\u65bd\u52a0\u989d\u5916\u7ea6\u675f\uff0c\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\u3002", "method": "AEPO\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u52a8\u6001\u71b5\u5e73\u8861rollout\u673a\u5236\uff0c\u901a\u8fc7\u71b5\u9884\u76d1\u63a7\u81ea\u9002\u5e94\u5206\u914d\u5168\u5c40\u548c\u5206\u652f\u91c7\u6837\u9884\u7b97\uff0c\u5e76\u5bf9\u8fde\u7eed\u9ad8\u71b5\u5de5\u5177\u8c03\u7528\u6b65\u9aa4\u65bd\u52a0\u5206\u652f\u60e9\u7f5a\uff1b(2)\u71b5\u5e73\u8861\u7b56\u7565\u4f18\u5316\uff0c\u5728\u9ad8\u71b5\u88c1\u526a\u9879\u4e2d\u63d2\u5165\u505c\u6b62\u68af\u5ea6\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165\u71b5\u611f\u77e5\u4f18\u52bf\u4f30\u8ba1\u3002", "result": "\u572814\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0cAEPO\u4e00\u81f4\u4f18\u4e8e7\u79cd\u4e3b\u6d41RL\u7b97\u6cd5\u3002\u4ec5\u4f7f\u75281K RL\u6837\u672c\uff0cQwen3-14B\u6a21\u578b\u5728GAIA\u4e0a\u8fbe\u523047.6% Pass@1\u548c65.0% Pass@5\uff0c\u5728Humanity's Last Exam\u4e0a\u8fbe\u523011.2% Pass@1\u548c26.0% Pass@5\uff0c\u5728WebWalker\u4e0a\u8fbe\u523043.0% Pass@1\u548c70.0% Pass@5\u3002", "conclusion": "AEPO\u63d0\u9ad8\u4e86rollout\u91c7\u6837\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u7b56\u7565\u71b5\uff0c\u6709\u52a9\u4e8e\u53ef\u6269\u5c55\u7684web agent\u8bad\u7ec3\u3002"}}
{"id": "2510.14770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14770", "abs": "https://arxiv.org/abs/2510.14770", "authors": ["Zhang Nengbo", "Hann Woei Ho", "Ye Zhou"], "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks", "comment": null, "summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in\nenvironments, where conventional radio-based methods suffer from spectrum\ncongestion, jamming, and high power consumption. Inspired by the waggle dance\nof honeybees, which efficiently communicate the location of food sources\nwithout sound or contact, we propose a novel visual communication framework for\nMAV swarms using motion-based signaling. In this framework, MAVs convey\ninformation, such as heading and distance, through deliberate flight patterns,\nwhich are passively captured by event cameras and interpreted using a\npredefined visual codebook of four motion primitives: vertical (up/down),\nhorizontal (left/right), left-to-up-to-right, and left-to-down-to-right,\nrepresenting control symbols (``start'', ``end'', ``1'', ``0''). To decode\nthese signals, we design an event frame-based segmentation model and a\nlightweight Spiking Neural Network (SNN) for action recognition. An integrated\ndecoding algorithm then combines segmentation and classification to robustly\ninterpret MAV motion sequences. Experimental results validate the framework's\neffectiveness, which demonstrates accurate decoding and low power consumption,\nand highlights its potential as an energy-efficient alternative for MAV\ncommunication in constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8fd0\u52a8\u4fe1\u53f7\u7684MAV\u8702\u7fa4\u901a\u4fe1\u6846\u67b6\uff0c\u6a21\u4eff\u871c\u8702\u6447\u6446\u821e\uff0c\u4f7f\u7528\u56db\u79cd\u8fd0\u52a8\u57fa\u5143\u4f20\u9012\u4fe1\u606f\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u548cSNN\u8fdb\u884c\u89e3\u7801\uff0c\u5b9e\u73b0\u4f4e\u529f\u8017\u53ef\u9760\u901a\u4fe1\u3002", "motivation": "\u4f20\u7edf\u65e0\u7ebf\u7535\u901a\u4fe1\u5728MAV\u8702\u7fa4\u4e2d\u9762\u4e34\u9891\u8c31\u62e5\u585e\u3001\u5e72\u6270\u548c\u9ad8\u529f\u8017\u95ee\u9898\uff0c\u9700\u8981\u5bfb\u627e\u66ff\u4ee3\u901a\u4fe1\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u8fd0\u52a8\u57fa\u5143\uff08\u4e0a\u4e0b\u3001\u5de6\u53f3\u3001\u5de6-\u4e0a-\u53f3\u3001\u5de6-\u4e0b-\u53f3\uff09\u4f5c\u4e3a\u63a7\u5236\u7b26\u53f7\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u6355\u83b7\u8fd0\u52a8\u6a21\u5f0f\uff0c\u91c7\u7528\u4e8b\u4ef6\u5e27\u5206\u5272\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7SNN\u8fdb\u884c\u52a8\u4f5c\u8bc6\u522b\u548c\u89e3\u7801\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u89e3\u7801\u548c\u4f4e\u529f\u8017\uff0c\u5728\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u8be5\u89c6\u89c9\u901a\u4fe1\u6846\u67b6\u4e3aMAV\u8702\u7fa4\u63d0\u4f9b\u4e86\u4e00\u79cd\u8282\u80fd\u7684\u66ff\u4ee3\u901a\u4fe1\u65b9\u6848\uff0c\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.14688", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.14688", "abs": "https://arxiv.org/abs/2510.14688", "authors": ["Junya Shiraishi", "Jiechen Chen", "Osvaldo Simeone", "Petar Popovski"], "title": "Online Reliable Anomaly Detection via Neuromorphic Sensing and Communications", "comment": null, "summary": "This paper proposes a low-power online anomaly detection framework based on\nneuromorphic wireless sensor networks, encompassing possible use cases such as\nbrain-machine interfaces and remote environmental monitoring. In the considered\nsystem, a central reader node actively queries a subset of neuromorphic sensor\nnodes (neuro-SNs) at each time frame. The neuromorphic sensors are\nevent-driven, producing spikes in correspondence to relevant changes in the\nmonitored system. The queried neuro-SNs respond to the reader with impulse\nradio (IR) transmissions that directly encode the sensed local events. The\nreader processes these event-driven signals to determine whether the monitored\nenvironment is in a normal or anomalous state, while rigorously controlling the\nfalse discovery rate (FDR) of detections below a predefined threshold. The\nproposed approach employs an online hypothesis testing method with e-values to\nmaintain FDR control without requiring knowledge of the anomaly rate, and it\ndynamically optimizes the sensor querying strategy by casting it as a best-arm\nidentification problem in a multi-armed bandit framework. Extensive performance\nevaluation demonstrates that the proposed method can reliably detect anomalies\nunder stringent FDR requirements, while efficiently scheduling sensor\ncommunications and achieving low detection latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u7684\u4f4e\u529f\u8017\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7684\u795e\u7ecf\u5f62\u6001\u4f20\u611f\u5668\u548c\u8109\u51b2\u65e0\u7ebf\u7535\u4f20\u8f93\u5b9e\u73b0\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\uff0c\u540c\u65f6\u4e25\u683c\u63a7\u5236\u8bef\u62a5\u7387\u3002", "motivation": "\u9488\u5bf9\u8111\u673a\u63a5\u53e3\u548c\u8fdc\u7a0b\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u573a\u666f\uff0c\u9700\u8981\u4f4e\u529f\u8017\u3001\u9ad8\u6548\u7387\u7684\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u53ef\u9760\u5730\u68c0\u6d4b\u5f02\u5e38\u72b6\u6001\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u5047\u8bbe\u68c0\u9a8c\u65b9\u6cd5\uff0c\u4f7f\u7528e\u503c\u63a7\u5236\u8bef\u62a5\u7387\uff0c\u65e0\u9700\u5148\u9a8c\u5f02\u5e38\u7387\u77e5\u8bc6\uff1b\u5c06\u4f20\u611f\u5668\u67e5\u8be2\u7b56\u7565\u5efa\u6a21\u4e3a\u591a\u81c2\u8d4c\u535a\u673a\u6846\u67b6\u4e2d\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u52a8\u6001\u4f18\u5316\u67e5\u8be2\u7b56\u7565\u3002", "result": "\u6027\u80fd\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u4e25\u683c\u7684\u8bef\u62a5\u7387\u8981\u6c42\u4e0b\u53ef\u9760\u68c0\u6d4b\u5f02\u5e38\uff0c\u540c\u65f6\u9ad8\u6548\u8c03\u5ea6\u4f20\u611f\u5668\u901a\u4fe1\u5e76\u5b9e\u73b0\u4f4e\u68c0\u6d4b\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u795e\u7ecf\u5f62\u6001\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u4f4e\u529f\u8017\u7684\u540c\u65f6\u786e\u4fdd\u4e86\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u65f6\u6548\u6027\u3002"}}
{"id": "2510.14276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14276", "abs": "https://arxiv.org/abs/2510.14276", "authors": ["Haiquan Zhao", "Chenhan Yuan", "Fei Huang", "Xiaomeng Hu", "Yichang Zhang", "An Yang", "Bowen Yu", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin", "Baosong Yang", "Chen Cheng", "Jialong Tang", "Jiandong Jiang", "Jianwei Zhang", "Jijie Xu", "Ming Yan", "Minmin Sun", "Pei Zhang", "Pengjun Xie", "Qiaoyu Tang", "Qin Zhu", "Rong Zhang", "Shibin Wu", "Shuo Zhang", "Tao He", "Tianyi Tang", "Tingyu Xia", "Wei Liao", "Weizhou Shen", "Wenbiao Yin", "Wenmeng Zhou", "Wenyuan Yu", "Xiaobin Wang", "Xiaodong Deng", "Xiaodong Xu", "Xinyu Zhang", "Yang Liu", "Yeqiu Li", "Yi Zhang", "Yong Jiang", "Yu Wan", "Yuxin Zhou"], "title": "Qwen3Guard Technical Report", "comment": null, "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.", "AI": {"tldr": "\u63d0\u51fa\u4e86Qwen3Guard\u7cfb\u5217\u5b89\u5168\u62a4\u680f\u6a21\u578b\uff0c\u5305\u542b\u751f\u6210\u5f0f\u548c\u6d41\u5f0f\u4e24\u4e2a\u53d8\u4f53\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u7ec6\u7c92\u5ea6\u5b89\u5168\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u5b89\u5168\u7b56\u7565\u548c\u5b9e\u65f6\u76d1\u63a7\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u62a4\u680f\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u53ea\u80fd\u8f93\u51fa\u4e8c\u5143\u5b89\u5168\u6807\u7b7e\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u9886\u57df\u7684\u5b89\u5168\u5bb9\u5fcd\u5ea6\uff1b\u9700\u8981\u5b8c\u6574\u6a21\u578b\u8f93\u51fa\u624d\u80fd\u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c\u65e0\u6cd5\u4e0e\u6d41\u5f0f\u63a8\u7406\u517c\u5bb9\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u4e2a\u4e13\u95e8\u53d8\u4f53\uff1a\u751f\u6210\u5f0fQwen3Guard\u5c06\u5b89\u5168\u5206\u7c7b\u8f6c\u6362\u4e3a\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e09\u5206\u7c7b\uff1b\u6d41\u5f0fQwen3Guard\u5f15\u5165token\u7ea7\u5206\u7c7b\u5934\uff0c\u652f\u6301\u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u3002\u63d0\u4f9b\u4e09\u79cd\u89c4\u6a21\uff080.6B\u30014B\u30018B\uff09\uff0c\u652f\u6301119\u79cd\u8bed\u8a00\u3002", "result": "\u5728\u82f1\u8bed\u3001\u4e2d\u6587\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwen3Guard\u5728\u63d0\u793a\u548c\u54cd\u5e94\u5b89\u5168\u5206\u7c7b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Qwen3Guard\u4e3a\u5168\u7403LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5168\u9762\u3001\u53ef\u6269\u5c55\u548c\u4f4e\u5ef6\u8fdf\u7684\u5b89\u5168\u5ba1\u6838\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u6a21\u578b\u5747\u4ee5Apache 2.0\u8bb8\u53ef\u8bc1\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2510.14351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14351", "abs": "https://arxiv.org/abs/2510.14351", "authors": ["Perapard Ngokpol", "Kun Kerdthaisong", "Pasin Buakhaw", "Pitikorn Khlaisamniang", "Supasate Vorathammathorn", "Piyalitt Ittichaiwong", "Nutchanon Yongsatianchot"], "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts", "comment": null, "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86Beyond One World\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u89d2\u8272\u626e\u6f14\u4e2d\u5fe0\u5b9e\u5448\u73b0\u7279\u5b9a\u7248\u672c\u89d2\u8272\u7684\u80fd\u529b\uff0c\u5305\u542b30\u4e2a\u82f1\u96c4\u768490\u4e2a\u7248\u672c\uff0c\u6d4b\u8bd5\u4e8b\u5b9e\u56de\u5fc6\u548c\u9053\u5fb7\u56f0\u5883\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u5728\u89d2\u8272\u626e\u6f14\u4e2d\u96be\u4ee5\u5fe0\u5b9e\u3001\u4e00\u81f4\u5730\u5448\u73b0\u7248\u672c\u7279\u5b9a\u89d2\u8272\uff0c\u7279\u522b\u662f\u50cf\u6f2b\u5a01\u548cDC\u8fd9\u6837\u7684\u8d85\u7ea7\u82f1\u96c4\u5b87\u5b99\u4e2d\u4e0d\u540c\u7248\u672c\u7684\u89d2\u8272\u5177\u6709\u4e0d\u540c\u5386\u53f2\u3001\u4ef7\u503c\u89c2\u548c\u9053\u5fb7\u51c6\u5219\u3002", "method": "\u6784\u5efa\u5305\u542b30\u4e2a\u6807\u5fd7\u6027\u82f1\u96c4\u548c90\u4e2a\u7279\u5b9a\u7248\u672c\u89d2\u8272\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1aCanon Events\uff08\u4e8b\u5b9e\u56de\u5fc6\uff09\u548cMoral Dilemmas\uff08\u9053\u5fb7\u56f0\u5883\uff09\uff0c\u5e76\u63d0\u51fa\u4e86Think-Act Matching\u6307\u6807\u6765\u91cf\u5316\u63a8\u7406\u4e0e\u884c\u52a8\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(1)\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u6539\u5584\u8f83\u5f31\u6a21\u578b\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u4f46\u53ef\u80fd\u964d\u4f4e\u8f83\u5f3a\u6a21\u578b\u7684\u51c6\u786e\u6027\uff1b(2)\u89d2\u8272\u5185\u8de8\u7248\u672c\u6cdb\u5316\u4ecd\u662f\u4e3b\u8981\u969c\u788d\uff1b(3)\u6a21\u578b\u901a\u5e38\u5728\u601d\u8003\u6216\u884c\u52a8\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5f88\u5c11\u4e24\u8005\u517c\u5f97\u3002", "conclusion": "Beyond One World\u63ed\u793a\u4e86\u591a\u5b87\u5b99\u4e00\u81f4\u6027\u548c\u63a8\u7406\u5bf9\u9f50\u65b9\u9762\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u4e3a\u89d2\u8272\u626e\u6f14LLM\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2510.14832", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.14832", "abs": "https://arxiv.org/abs/2510.14832", "authors": ["Maria Lamprini A. Bartsioka", "Anastasios Giannopoulos", "Sotirios Spantideas"], "title": "Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks", "comment": "9 pages, 17 figures", "summary": "The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)\nnetworks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,\nrequires mobility decisions that remain reliable under fast channel dynamics,\ninterference, and heterogeneous coverage. Handover in multi-RAT deployments is\nstill highly reactive and event-triggered, relying on instantaneous\nmeasurements and threshold events. This work proposes a Machine Learning\n(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a\nmodel-driven and short-horizon signal quality forecasts. We present a\ngeneralized P-CHO sequence workflow orchestrated by a RAT Steering Controller,\nwhich standardizes data collection, parallel per-RAT predictions, decision\nlogic with hysteresis-based conditions, and CHO execution. Considering a\nrealistic multi-RAT environment, we train RAT-aware Long Short Term Memory\n(LSTM) networks to forecast the signal quality indicators of mobile users along\nrandomized trajectories. The proposed P-CHO models are trained and evaluated\nunder different channel models for cellular and IEEE 802.11 WiFi integrated\ncoverage. We study the impact of hyperparameter tuning of LSTM models under\ndifferent system settings, and compare direct multi-step versus recursive P-CHO\nvariants. Comparisons against baseline predictors are also carried out.\nFinally, the proposed P-CHO is tested under soft and hard handover settings,\nshowing that hysteresis-enabled P-CHO scheme is able to reduce handover\nfailures and ping-pong events. Overall, the proposed P-CHO framework can enable\naccurate, low-latency, and proactive handovers suitable for ML-assisted\nhandover steering in 6G multi-RAT deployments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u6027\u6761\u4ef6\u5207\u6362\u6846\u67b6\uff0c\u4f7f\u7528LSTM\u7f51\u7edc\u9884\u6d4b\u4fe1\u53f7\u8d28\u91cf\uff0c\u57286G\u591aRAT\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e3b\u52a8\u5207\u6362\u51b3\u7b56\uff0c\u51cf\u5c11\u5207\u6362\u5931\u8d25\u548c\u4e52\u4e53\u6548\u5e94\u3002", "motivation": "6G\u591aRAT\u7f51\u7edc\u4e2d\u4f20\u7edf\u5207\u6362\u673a\u5236\u53cd\u5e94\u5f0f\u3001\u4e8b\u4ef6\u89e6\u53d1\uff0c\u4f9d\u8d56\u77ac\u65f6\u6d4b\u91cf\uff0c\u5728\u5feb\u901f\u4fe1\u9053\u52a8\u6001\u3001\u5e72\u6270\u548c\u5f02\u6784\u8986\u76d6\u4e0b\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u9884\u6d4b\u6027\u5207\u6362\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u9a71\u52a8\u3001\u77ed\u65f6\u57df\u4fe1\u53f7\u8d28\u91cf\u9884\u6d4b\u7684ML\u8f85\u52a9\u9884\u6d4b\u6027\u6761\u4ef6\u5207\u6362\u6846\u67b6\uff0c\u4f7f\u7528RAT\u611f\u77e5LSTM\u7f51\u7edc\u9884\u6d4b\u79fb\u52a8\u7528\u6237\u4fe1\u53f7\u8d28\u91cf\u6307\u6807\uff0c\u91c7\u7528\u6ede\u540e\u6761\u4ef6\u51b3\u7b56\u903b\u8f91\uff0c\u652f\u6301\u8f6f\u786c\u5207\u6362\u8bbe\u7f6e\u3002", "result": "\u6ede\u540e\u542f\u7528\u7684P-CHO\u65b9\u6848\u80fd\u591f\u51cf\u5c11\u5207\u6362\u5931\u8d25\u548c\u4e52\u4e53\u4e8b\u4ef6\uff0c\u5728\u8f6f\u786c\u5207\u6362\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u826f\u597d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u9884\u6d4b\u5668\u6709\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684P-CHO\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u3001\u4f4e\u5ef6\u8fdf\u3001\u4e3b\u52a8\u7684\u5207\u6362\uff0c\u9002\u54086G\u591aRAT\u90e8\u7f72\u4e2d\u7684ML\u8f85\u52a9\u5207\u6362\u63a7\u5236\u3002"}}
{"id": "2510.14853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14853", "abs": "https://arxiv.org/abs/2510.14853", "authors": ["Guinan Su", "Yanwu Yang", "Li Shen", "Lu Yin", "Shiwei Liu", "Jonas Geiping"], "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models", "comment": null, "summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse\nexpert activation, but often suffer from suboptimal routing decisions due to\ndistribution shifts in deployment. While existing test-time adaptation methods\ncould potentially address these issues, they primarily focus on dense models\nand require access to external data, limiting their practical applicability to\nMoE architectures. However, we find that, instead of relying on reference data,\nwe can optimize MoE expert selection on-the-fly based only on input context. As\nsuch, we propose \\textit{a data-free, online test-time framework} that\ncontinuously adapts MoE routing decisions during text generation without\nexternal supervision or data. Our method cycles between two phases: During the\nprefill stage, and later in regular intervals, we optimize the routing\ndecisions of the model using self-supervision based on the already generated\nsequence. Then, we generate text as normal, maintaining the modified router\nuntil the next adaption. We implement this through lightweight additive vectors\nthat only update router logits in selected layers, maintaining computational\nefficiency while preventing over-adaptation. The experimental results show\nconsistent performance gains on challenging reasoning tasks while maintaining\nrobustness to context shifts. For example, our method achieves a 5.5\\%\nimprovement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play\nproperty, our method naturally complements existing test-time scaling\ntechniques, e.g., achieving 6\\% average gains when incorporated with\nself-consistency on DeepSeek-V2-Lite.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6570\u636e\u7684\u5728\u7ebf\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u4f18\u5316MoE\u6a21\u578b\u7684\u8def\u7531\u51b3\u7b56\uff0c\u5728\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u8fdb\u4e13\u5bb6\u9009\u62e9\u3002", "motivation": "MoE\u6a21\u578b\u5728\u90e8\u7f72\u4e2d\u5e38\u56e0\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u8def\u7531\u51b3\u7b56\u4e0d\u4f73\uff0c\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5bc6\u96c6\u6a21\u578b\u4e14\u9700\u8981\u5916\u90e8\u6570\u636e\uff0c\u96be\u4ee5\u5e94\u7528\u4e8eMoE\u67b6\u6784\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5faa\u73af\uff1a\u9884\u586b\u5145\u9636\u6bb5\u548c\u5b9a\u671f\u95f4\u9694\u65f6\uff0c\u57fa\u4e8e\u5df2\u751f\u6210\u5e8f\u5217\u901a\u8fc7\u81ea\u76d1\u7763\u4f18\u5316\u8def\u7531\u51b3\u7b56\uff1b\u6b63\u5e38\u751f\u6210\u6587\u672c\u65f6\u4fdd\u6301\u4fee\u6539\u540e\u7684\u8def\u7531\u5668\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u52a0\u6027\u5411\u91cf\u4ec5\u66f4\u65b0\u9009\u5b9a\u5c42\u7684\u8def\u7531\u5668logits\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u4f8b\u5982\u5728OLMoE\u4e0aHumanEval\u63d0\u53475.5%\uff1b\u4e0e\u81ea\u4e00\u81f4\u6027\u7b49\u6280\u672f\u7ed3\u5408\u65f6\u5728DeepSeek-V2-Lite\u4e0a\u5e73\u5747\u63d0\u53476%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316MoE\u8def\u7531\u51b3\u7b56\uff0c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u9632\u6b62\u8fc7\u5ea6\u9002\u5e94\uff0c\u4e14\u5177\u6709\u5373\u63d2\u5373\u7528\u7279\u6027\uff0c\u53ef\u4e0e\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\u81ea\u7136\u4e92\u8865\u3002"}}
