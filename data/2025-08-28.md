<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 13]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: 提出了Temporal Token Fusion (TTF)方法，通过整合历史视觉信息来增强VLA模型在机器人操作任务中的性能，无需重新训练即可实现跨模型的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型逐帧处理视觉输入，丢弃了机器人操作任务中宝贵的时间信息，使其容易受到视觉噪声影响并忽略连续帧之间的连贯性。

Method: TTF采用双维度检测（灰度像素差异分析和基于注意力的语义相关性评估），通过硬融合策略和关键帧锚定实现选择性时间token融合，防止错误累积。

Result: 在LIBERO上平均提升4.0个百分点（72.4% vs 68.4%基准），SimplerEnv上相对提升4.8%，真实机器人任务上相对提升8.7%，且证明具有模型无关性。

Conclusion: TTF方法有效提升了VLA模型的推理质量，选择性Query矩阵重用能够提升性能而非损害性能，为直接KQV矩阵重用策略提供了有前景的方向，在实现计算加速的同时提高任务成功率。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [2] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM是一个自监督框架，整合心脏磁共振成像和电子健康记录进行生存分析，在四个独立临床队列中超越传统方法和SOTA深度学习基线，发现了三个与心脏风险相关的成像特征。


<details>
  <summary>Details</summary>
Motivation: 准确预测主要不良心脏事件（MACE）是心血管预后的核心挑战，需要整合多模态数据来提升预测精度。

Method: 通过运动感知多视图蒸馏提取时间同步的成像特征，并使用医学信息文本提示进行调制，实现细粒度风险预测。

Result: 在四个独立临床队列中，PRISM在内部和外部验证中均超越经典生存预测模型和最先进的深度学习基线，发现了三个与MACE风险相关的成像特征。

Conclusion: PRISM整合成像和EHR表征为不同队列的心脏风险提供了有价值的见解，提示引导归因识别了高血压、糖尿病和吸烟是主要的临床和生理风险因素。

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [3] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench是首个专门评估多视频关系推理能力的基准测试，包含1000个QA对，涵盖三个层次：跨视频对象关联、事件关联和复杂推理。测试发现当前顶级MLLM模型在多视频推理任务上表现显著落后于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在单视频任务上表现良好，但在多视频推理能力方面研究不足，而这种能力对现实应用（如多摄像头监控、跨视频程序学习）至关重要。

Method: 构建CVBench基准测试，包含5个不同领域的视频集群，设计三个层次的推理任务：对象关联、事件关联和复杂推理。评估10+个领先MLLM模型在零样本和思维链提示下的表现。

Result: 主要模型如GPT-4o在因果推理任务上仅达到60%准确率，远低于人类的91%表现。分析揭示了当前MLLM架构的根本瓶颈：跨视频上下文保持能力不足和重叠实体消歧能力差。

Conclusion: CVBench为诊断和推进多视频推理提供了严谨框架，为下一代MLLM架构设计提供了重要见解。基准数据和评估代码已开源。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [4] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: 提出结合可训练编码器、原型引导重建和多样性感知对齐损失的统一框架，解决医学图像异常检测中的原型崩溃问题，显著提升表示质量和异常定位精度


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临标注有限和领域差异挑战。现有重建方法依赖冻结预训练编码器，限制了领域适应能力；原型学习方法存在原型崩溃问题，少数原型主导训练，影响多样性和泛化能力

Method: 使用可训练编码器（通过动量分支增强）进行领域自适应特征学习；轻量级原型提取器挖掘信息丰富的正常原型，通过注意力机制指导解码器进行精确重建；提出多样性感知对齐损失，通过多样性约束和逐原型归一化实现平衡的原型使用

Result: 在多个医学影像基准测试中显示出表示质量和异常定位的显著改进，优于现有方法；可视化和原型分配分析验证了抗崩溃机制的有效性和增强的可解释性

Conclusion: 所提出的统一框架有效解决了原型崩溃问题，通过领域自适应特征学习和多样性约束机制，在医学图像异常检测任务中取得了优异性能，同时提供了更好的可解释性

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [5] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种基于动态生成内核的迭代图像去噪方法，通过组合全局统计和局部相关特征来预测像素级变化的去噪内核，并在训练数据限的情况下实现了对未见噪声类型和级别的良好演续性。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像去噪方法对特定噪声分布依赖性强，容易过拟合且综合性能差。需要一种能够在训练数据有限的情况下保持良好演续性的高效去噪方法。

Method: 设计了包含特征提取模块、全局统计模块、局部相关模块和内核预测模块的系统。通过动态生成像素级变化的去噪内核，并迭代应用这些内核来进行图像恢复。

Result: 该简洁模型（约 0.04M 参数）仅在单一正态噪声上训练，但在多种噪声类型和级别上都表现优异，显示了良好的演续性能。

Conclusion: 迭代动态过滤方法为实际图像去噪应用提供了一种高效且强演续的解决方案，能够在训练数据有限的情况下实现良好的去噪效果。

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [6] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: 该研究对SNN在卫星位置估计任务中的能效进行了深入分析，发现硬件无关方法预测SNN比CNN节能50-60%，但硬件感知分析显示只有在神经形态硬件和高输入稀疏度下才能实现显著节能。


<details>
  <summary>Details</summary>
Motivation: 传统认为SNN具有固有能效优势，但近期研究表明数字实现中这种优势可能被高估，需要更透明的评估方法来公平比较神经网络能效。

Method: 使用LIF神经元的膜电位进行多输出回归训练，在逼真卫星数据集上与参考CNN比较MSE性能，并采用硬件无关和硬件感知两种能量估计方法。

Result: SNN达到与CNN相当的MSE性能，但能量分析显示SNN的节能优势高度依赖于硬件平台和输入数据稀疏度（暗像素比例）。

Conclusion: 需要透明的评估方法和明确披露底层假设，以确保神经网络能效比较的公平性，数据特性和硬件假设对能效评估有重要影响。

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [7] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种免训练的视频恢肌分割方法FreeVPS，通过结合IPS模型的空间上下文和SAM2的时间建模能力，解决了长期跟踪中的错误累积问题，在内外域场景中都取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频恢肌分割方法在空时间建模和域逆向性之间难以取得平衡，限制了在真实临床场景中的应用。特别是SAM2在长期跟踪中存在错误累积的雪球效应问题。

Method: 重构VPS任务为跟踪-检测范式，利用IPS模型的空间上下文和SAM2的时间建模能力。设计了两个免训练模块：内联合筛选模块消除空间不准确性，外联合精细模块防止错误传播，提高时间一致性。

Result: 方法在内域和外域场景中都达到了领先性能，并在长时间未剪连续细肠镜视频中展现了稳健的跟踪能力。

Conclusion: FreeVPS通过创新地重构问题和免训练模块设计，有效解决了视频恢肌分割中的空时间平衡和错误累积挑战，具有重要的临床应用价值。

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [8] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: SPLF-SAM是一个自提示光场分割模型，通过统一多尺度特征嵌入块和多尺度自适应滤波适配器，解决了现有方法忽略提示信息提取和频域信息分析的问题，在光场显著目标检测任务上优于10个SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型在光场显著目标检测任务中往往忽略提示信息的提取，同时传统方法忽视频域信息分析，导致小目标被噪声淹没。

Method: 提出SPLF-SAM模型，包含统一多尺度特征嵌入块(UMFEB)和多尺度自适应滤波适配器(MAFA)。UMFEB能识别不同大小的多个目标，MAFA通过学习频域特征有效防止小目标被噪声淹没。

Result: 大量实验证明该方法在10个最先进的光场显著目标检测方法中表现优越。

Conclusion: SPLF-SAM通过创新的多尺度特征嵌入和频域滤波技术，显著提升了光场显著目标检测的性能，特别是在处理小目标和噪声抑制方面表现出色。

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [9] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 提出了AIM方法来解决多模态学习中的优化偏差问题，通过自适应网络内调制实现平衡的多模态学习，不牺牲任何模态的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通过抑制主导模态来促进弱势模态，影响了整体多模态性能。研究发现根本原因是网络内部的优化偏差问题

Method: AIM方法将主导模态的未优化参数解耦到辅助块中，鼓励与弱势模态联合训练时依赖这些性能下降的块，同时根据网络深度自适应调整调制强度

Result: AIM在多个基准测试中优于最先进的不平衡模态学习方法，并在不同骨干网络、融合策略和优化器上表现出强泛化能力

Conclusion: AIM首次实现了不牺牲任何模态性能的平衡多模态学习，有效解决了网络内部优化偏差问题

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [10] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: 提出了CSSL框架，通过上下文感知阈值动态调节神经元激活，在事件相机视觉任务中实现高稀疏性和优异性能


<details>
  <summary>Details</summary>
Motivation: 现有事件相机深度学习方法未能充分利用事件数据的稀疏性，而脉冲神经网络在复杂任务中性能不足，且高激活稀疏性难以实现

Method: Context-aware Sparse Spatiotemporal Learning (CSSL)框架，引入上下文感知阈值机制，根据输入分布动态调节神经元激活，无需显式稀疏约束

Result: 在事件相机目标检测和光流估计任务中达到或超越最先进方法性能，同时保持极高的神经元稀疏性

Conclusion: CSSL为神经形态处理实现高效事件相机视觉提供了关键解决方案

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [11] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: 本文提出了PAUL框架来解决跨视角地理定位中的噪声对应问题，通过不确定性学习和选择性增强来处理GPS漂移导致的图像对未对齐问题


<details>
  <summary>Details</summary>
Motivation: 现有的跨视角地理定位方法通常假设训练图像对完美对齐，但实际应用中GPS漂移等因素导致系统性的对齐偏移，只有部分对应关系存在，这种噪声对应问题在当前研究中关注不足

Method: 提出PAUL框架，通过不确定性感知协同增强和证据协同训练来估计数据不确定性，基于此对训练数据进行分区和增强。选择性地增强具有高对应置信度的区域，并利用不确定性估计来细化特征学习

Result: 综合实验验证了PAUL各个组件的有效性，在各种噪声比例下都取得了优于其他竞争性噪声对应驱动方法的性能

Conclusion: PAUL框架有效解决了跨视角地理定位中的噪声对应问题，为理想化基准测试与实际应用之间的差距搭建了桥梁，提供了对噪声样本的鲁棒监督

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [12] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory是一个统一的文本到音频生成框架，通过结合大语言模型和TTA系统来生成长篇叙事音频，解决了现有方法在时序连贯性和组合推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到音频生成技术在合成短音频片段方面表现出色，但在生成长篇叙事音频时存在时序连贯性和组合推理的困难，需要一个新的框架来解决这些问题。

Method: 提出AudioStory框架，使用大语言模型将复杂叙事查询分解为时序有序的子任务，采用解耦的桥接机制（语义对齐桥接查询和连贯性保持残差查询）和端到端训练方法。

Result: 实验表明AudioStory在单音频生成和叙事音频生成方面均优于现有基线方法，在指令跟随能力和音频保真度方面都有显著提升。

Conclusion: AudioStory通过整合LLM和TTA系统，有效解决了长音频叙事的连贯性问题，建立了新的基准AudioStory-10K，为长格式音频生成提供了有效的解决方案。

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [13] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA是一个可训练的组合框架，通过两阶段训练流程整合通用规划器和专业执行器，在科学计算GUI任务中实现了卓越的执行精度和跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决GUI自主代理在科学计算领域面临的挑战：通用代理规划能力强但执行差，专业代理执行强但规划弱，现有组合框架静态不可训练无法从经验中学习。

Method: 提出CODA框架，包含通用规划器Cerebrum和专业执行器Cerebellum。采用两阶段训练：1)专业化阶段-使用decoupled GRPO方法为每个科学应用单独训练专家规划器；2)泛化阶段-聚合成功轨迹进行监督微调。

Result: 在ScienceBenchmark的四个挑战性应用上评估，CODA显著超越基线方法，在开源模型中建立了新的最先进水平。

Conclusion: CODA通过可训练的组合框架成功解决了科学计算GUI任务中的规划-执行权衡问题，实现了强大的执行能力和跨域泛化性能。

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级深度学习模型，使用RR间隔数据通过时序卷积网络和Mamba状态空间模型的组合，能够提前2小时预测房颤，且具有高准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 房颤是最常见的心律失常，特别是发作性房颤(PAF)因其突然发作和短暂持续时间而难以检测，但未被发现的PAF可能进展为持续性房颤，增加死亡风险。早期预测房颤可以通过预防性治疗减少疾病进展。

Method: 提出了一种轻量级深度学习模型，仅使用RR间隔(RRIs)数据，结合时序卷积网络(TCN)进行位置编码和Mamba选择性状态空间模型，实现高效并行序列建模。

Result: 在主体测试中，模型达到了敏感度0.908、特异度0.933、F1分数0.930、AUROC为0.972、AUPRC为0.932。模型仅有73.5千参数和38.3 MFLOPs计算量，能够仅使用30分钟输入数据预测未来2小时的房颤。

Conclusion: 该模型在准确性和模型紧凑性方面都超过了传统的CNN-RNN方法，能够提供充足的预警时间进行预防性干预，为早期房颤预测提供了高效、准确的解决方案。

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [15] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 基于LLaMA 3.2的视觉语言模型在高能物理中微子相互作用分类任务上表现优于传统CNN方法，支持多模态推理


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多模态推理方面的潜力，特别是在高能物理实验中对中微子相互作用的像素化探测器图像进行分类

Method: 使用基于LLaMA 3.2的视觉语言模型进行微调，与NOvA和DUNE实验中使用的CNN基线模型进行性能对比评估

Result: VLM不仅达到或超过CNN性能，还支持更丰富的推理能力和更好的辅助文本/语义上下文整合

Conclusion: 视觉语言模型为高能物理事件分类提供了有前景的通用骨干网络，为实验性中微子物理的多模态方法铺平了道路

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: 这篇论文提出了CASE（诈骗解释对话机器人）框架，通过AI对话机器调查潜在受害者，收集结构化诈骗情报，在Google Pay平台实现了21%的诈骗处置量提升。


<details>
  <summary>Details</summary>
Motivation: 数字支付平台的普及导致社工诈骗增多，但传统的用户和交易信号无法全面识别诈骗模式，需要新方法及时防范。

Method: 设计了主动对话机器机制，使用Google Gemini LLMs主动调查潜在受害者，将对话内容转换为结构化数据，用于自动化和手动执法机制。

Result: 在Google Pay印度平台实现后，诈骗处置量提升了21%，框架具有良好的可扩展性。

Conclusion: CASE框架通过主动收集诈骗情报，有效解决了诈骗防范的信息缺口问题，为其他敏感领域的AI驱动系统提供了蓝图。

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [17] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: 提出ARIS混合方法，结合自混合代理和判别式序列标注器，通过模型共识、置信度过滤和LLM反思推理来提升事件抽取性能，在三个基准数据集上超越现有最优方法


<details>
  <summary>Details</summary>
Motivation: 传统判别模型精度高但召回率低，特别是对细微或罕见事件；生成式LLM方法语义灵活性高但存在幻觉和不一致预测问题

Method: ARIS混合方法：自混合代理+判别式序列标注器，利用结构化模型共识、置信度过滤和LLM反思推理模块来可靠解决歧义；还研究了分解指令微调以增强LLM事件抽取理解

Result: 在三个基准数据集上的实验表明，该方法优于现有的最先进事件抽取方法

Conclusion: ARIS通过结合判别式和生成式方法的优势，有效解决了事件抽取中的精度-召回权衡问题，提高了整体事件预测质量

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [18] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: 本文讨论人工智能文本生成工具可能导致人类写作能力退化的风险，类似于古希腊黑暗时代文字能力的丧失


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的快速发展，人类可能过度依赖机器生成文本，导致自身写作能力退化，需要探讨这种历史重演的可能性

Method: 通过历史类比分析，将当前AI文本生成工具的普及与古希腊黑暗时代文字能力丧失的现象进行对比研究

Result: 识别出AI工具可能导致人类写作能力逐渐退化的潜在风险，这种模式与历史上其他文字能力丧失时期有相似之处

Conclusion: 需要警惕过度依赖AI文本生成工具可能带来的人类写作能力退化问题，保持人类自身的文字创作能力至关重要

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [19] [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
*Kun Peng,Cong Cao,Hao Peng,Guanlin Wu,Zhifeng Hao,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了未见情感对话识别(UERC)任务和ProEmoTrans原型迁移框架，解决了传统封闭域情感识别无法处理未知情感的问题，通过LLM增强描述、参数无关编码机制和改进的注意力维特比解码方法，在三个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前情感对话识别(ERC)研究基于封闭域假设，但心理学中情感分类没有明确共识，现实应用中模型难以识别未见过的情感类型，需要开发能够处理未知情感的新方法。

Method: 提出ProEmoTrans原型情感迁移框架：1）使用LLM增强描述解决隐式表达问题；2）参数无关机制处理长对话编码和过拟合；3）改进的注意力维特比解码(AVD)方法迁移已知情感转移模式到未知情感。

Result: 在三个数据集上的大量实验表明，该方法作为这一新领域的强基线，能够有效识别未见过的情感类型，证明了原型迁移方法的可行性。

Conclusion: 提出的UERC任务和ProEmoTrans框架为开放域情感识别提供了新的研究方向，通过原型迁移方法成功解决了未见情感识别问题，为实际应用中的情感分析提供了更实用的解决方案。

Abstract: Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

</details>


### [20] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: NEWSCOPE是一个两阶段新闻检索框架，通过句子级语义变化建模来提升事件报道的多样性，在保持相关性的同时显著提高检索结果的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有新闻检索系统主要关注文本相关性，导致结果冗余且视角有限，需要获取多样化的观点来更好地理解真实世界事件。

Method: 两阶段框架：第一阶段使用密集检索获取主题相关内容，第二阶段应用句子级聚类和多样性感知重排序来发现补充信息。

Result: NEWSCOPE在构建的两个段落级基准测试（LocalNews和DSGlobal）上始终优于强基线，实现了显著更高的多样性而不损害相关性。

Conclusion: 细粒度、可解释的建模在减少冗余和促进全面事件理解方面具有有效性，提出的三个可解释指标（平均成对距离、正聚类覆盖率和信息密度比）能有效评估检索多样性。

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [21] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: 本文提出选择性检索增强(SRA)方法，通过仅对训练集中低频标签样本进行检索增强，解决法律文本分类中长尾分布导致的罕见类别性能差的问题。


<details>
  <summary>Details</summary>
Motivation: 法律文本分类基准数据集通常存在长尾标签分布，许多标签样本不足导致模型在罕见类别上表现不佳，需要一种有效的数据增强方法来改善这种情况。

Method: 选择性检索增强(SRA)方法，仅对低频标签的训练样本进行检索增强，避免为充分表示的类别引入噪声，且无需改变模型架构，仅从训练数据中检索以避免信息泄露。

Result: 在LEDGAR(单标签)和UNFAIR-ToS(多标签)两个法律文本分类基准数据集上，SRA方法相比所有当前LexGLUE基线获得了更高的micro-F1和macro-F1分数。

Conclusion: SRA方法在法律文本长尾分类任务中取得了显著且一致的性能提升，为解决法律领域罕见类别分类问题提供了有效解决方案。

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

</details>
