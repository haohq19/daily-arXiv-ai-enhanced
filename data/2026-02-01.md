<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

TL;DR: 本文针对对话式图像生成中的非马尔可夫问题，提出数据构建、训练框架和优化方法，显著提升多轮一致性和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有对话式图像生成模型大多采用马尔可夫假设，仅依赖最近图像而忽略长程历史，无法处理用户回溯、撤销或跨轮引用等复杂交互场景。

Method: 提出三种方法：1) 非马尔可夫多轮数据构建策略（回滚式编辑和基于名称的多轮个性化）；2) 历史条件训练与推理框架（带令牌级缓存）；3) 高保真图像重建和可编辑个性化优化（基于重建的DiT解码器和多阶段微调课程）。

Result: 显式训练非马尔可夫交互显著提升了多轮一致性和指令遵循能力，同时保持了强大的单轮编辑和个性化性能。

Conclusion: 针对非马尔可夫场景的专门训练是提升对话式图像生成系统多轮交互能力的关键，为更自然、一致的多模态对话奠定了基础。

Abstract: Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

</details>


### [2] [LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models](https://arxiv.org/abs/2601.21220)
*Alvi Md Ishmam,Najibul Haque Sarker,Zaber Ibn Abdul Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: LAMP是一种针对多图像多模态大语言模型的黑盒攻击方法，通过学习通用对抗扰动，通过注意力约束和跨图像传染机制破坏模型的多图像信息聚合能力。


<details>
  <summary>Details</summary>
Motivation: 多图像MLLMs在视觉语言任务中表现出色，但其安全漏洞尚未被充分探索。现有对抗攻击主要针对单图像设置且多为白盒威胁模型，这在现实场景中不实用。因此需要研究针对多图像MLLMs的黑盒攻击方法。

Method: LAMP采用黑盒方法学习通用对抗扰动，包含三个关键技术：1）基于注意力的约束，防止模型有效聚合跨图像信息；2）跨图像传染约束，使扰动标记影响干净标记，无需修改所有输入；3）索引注意力抑制损失，实现鲁棒的位置不变攻击。

Result: 实验结果表明，LAMP在多个视觉语言任务和模型上均优于现有最先进的基线方法，取得了最高的攻击成功率。

Conclusion: LAMP成功展示了多图像MLLMs的安全漏洞，为黑盒场景下的对抗攻击提供了有效解决方案，强调了多模态模型安全评估的重要性。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable performance across vision-language tasks. Recent advancements allow these models to process multiple images as inputs. However, the vulnerabilities of multi-image MLLMs remain unexplored. Existing adversarial attacks focus on single-image settings and often assume a white-box threat model, which is impractical in many real-world scenarios. This paper introduces LAMP, a black-box method for learning Universal Adversarial Perturbations (UAPs) targeting multi-image MLLMs. LAMP applies an attention-based constraint that prevents the model from effectively aggregating information across images. LAMP also introduces a novel cross-image contagious constraint that forces perturbed tokens to influence clean tokens, spreading adversarial effects without requiring all inputs to be modified. Additionally, an index-attention suppression loss enables a robust position-invariant attack. Experimental results show that LAMP outperforms SOTA baselines and achieves the highest attack success rates across multiple vision-language tasks and models.

</details>


### [3] [Hypersolid: Emergent Vision Representations via Short-Range Repulsion](https://arxiv.org/abs/2601.21255)
*Esteban Rodríguez-Betancourt,Edgar Casasola-Murillo*

Main category: cs.CV

TL;DR: Hypersolid通过将表示学习重新解释为离散堆积问题，使用短程硬球排斥防止局部碰撞，避免表示崩溃


<details>
  <summary>Details</summary>
Motivation: 自监督学习中防止表示崩溃是一个常见挑战，现有方法通常依赖全局正则化，如最大化距离、解相关维度或强制特定分布。本文提出从不同角度解决这个问题

Method: 将表示学习重新解释为离散堆积问题，通过保持单射性来保留信息。Hypersolid方法使用短程硬球排斥来防止局部碰撞，形成高分离度的几何机制

Result: 该方法能够保持增强多样性，在细粒度和低分辨率分类任务上表现出色

Conclusion: 通过将表示学习视为离散堆积问题并使用局部排斥约束，Hypersolid提供了一种防止表示崩溃的有效方法，特别适用于需要保留细微差异的任务

Abstract: A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.

</details>


### [4] [Semantic-Guided Dynamic Sparsification for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2601.21345)
*Ruiqi Liu,Boyu Diao,Zijia An,Runjie Shao,Zhulin An,Fei Wang,Yongjun Xu*

Main category: cs.CV

TL;DR: SGDS通过语义引导的动态稀疏化，在激活空间中为相似类分配共享子空间、为不相似类分配非重叠子空间，有效缓解类增量学习中的干扰问题，同时保持模型可塑性。


<details>
  <summary>Details</summary>
Motivation: 现有CIL方法通常冻结预训练模型并使用轻量适配器，并通过参数正交化防止任务间干扰，但这种方法会损害模型的可塑性。需要一种既能防止干扰又不限制参数空间的方法。

Method: 提出语义引导的动态稀疏化(SGDS)，通过控制激活空间子空间的方向和秩来实现定向稀疏化。为相似类分配紧凑的共享激活子空间以促进知识迁移，为不相似类分配非重叠激活子空间以防止干扰。

Result: 在多个基准数据集上的广泛实验表明，SGDS取得了最先进的性能表现。

Conclusion: SGDS通过在激活空间中雕刻类特定的稀疏子空间，有效缓解了类增量学习中的干扰问题，同时避免了参数空间的刚性约束，实现了更好的可塑性和性能平衡。

Abstract: Class-Incremental Learning (CIL) requires a model to continually learn new classes without forgetting old ones. A common and efficient solution freezes a pre-trained model and employs lightweight adapters, whose parameters are often forced to be orthogonal to prevent inter-task interference. However, we argue that this parameter-constraining method is detrimental to plasticity. To this end, we propose Semantic-Guided Dynamic Sparsification (SGDS), a novel method that proactively guides the activation space by governing the orientation and rank of its subspaces through targeted sparsification. Specifically, SGDS promotes knowledge transfer by encouraging similar classes to share a compact activation subspace, while simultaneously preventing interference by assigning non-overlapping activation subspaces to dissimilar classes. By sculpting class-specific sparse subspaces in the activation space, SGDS effectively mitigates interference without imposing rigid constraints on the parameter space. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of SGDS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins](https://arxiv.org/abs/2601.20906)
*Nikita Makarov,Maria Bordukova,Lena Voith von Voithenberg,Estrella Pivel-Villanueva,Sabrina Mielke,Jonathan Wickes,Hanchen Wang,Mingyu Derek Ma,Keunwoo Choi,Kyunghyun Cho,Stephen Ra,Raul Rodriguez-Esteban,Fabian Schmich,Michael Menden*

Main category: cs.LG

TL;DR: TwinWeaver框架将患者纵向历史序列化为文本，使用大语言模型进行事件预测和预后，在癌症患者数据上显著优于传统时间序列方法。


<details>
  <summary>Details</summary>
Motivation: 精准肿瘤学需要预测临床事件和病程轨迹，但稀疏、多模态临床时间序列建模仍是一个关键挑战。

Method: 提出TwinWeaver开源框架，将纵向患者历史序列化为文本，使用大语言模型进行统一事件预测和预后，构建了基于93,054名20种癌症患者的Genie Digital Twin模型。

Result: GDT显著降低预测误差（MASE中位数0.87 vs 基线0.97），改善风险分层（平均C-index 0.703 vs 基线0.662），在分布外临床试验中零样本匹配基线，微调后超越基线。

Conclusion: TwinWeaver为纵向临床建模提供了可扩展且透明的基础，支持可解释的临床推理扩展。

Abstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.

</details>


### [6] [Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings](https://arxiv.org/abs/2601.20987)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 该研究开发了首个针对全球儿童发展的预训练编码器，使用UNICEF的357,709名儿童数据，在少量样本下显著提升发育迟缓监测性能


<details>
  <summary>Details</summary>
Motivation: 每年大量儿童经历可预防的发育迟缓，但机器学习在新国家的部署面临数据瓶颈：可靠模型需要数千样本，而新项目开始时通常少于100个样本

Method: 使用UNICEF调查数据，在44个国家的357,709名儿童上训练预训练编码器，采用迁移学习方法，应用迁移学习理论解释预训练多样性如何实现少样本泛化

Result: 仅用50个训练样本，预训练编码器平均AUC达0.65，比冷启动梯度提升的0.61提高8-12%；500个样本时AUC达0.73；零样本部署到未见国家AUC最高达0.84

Conclusion: 预训练编码器可以改变资源受限环境下机器学习用于SDG 4.2.1监测的可行性，为全球儿童发展监测提供有效解决方案

Abstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.

</details>


### [7] [SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model](https://arxiv.org/abs/2601.21031)
*Zongheng Guo,Tao Chen,Yang Jiao,Yi Pan,Xiao Hu,Manuela Ferrario*

Main category: cs.LG

TL;DR: SIGMA-PPG是一个基于统计先验的生成式PPG基础模型，通过对抗掩码机制和语义一致性约束，解决了PPG信号内在冗余和噪声问题，在12个下游任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前PPG信号基础模型面临信号内在冗余和噪声的挑战，标准掩码建模容易产生平凡解，而对比方法缺乏形态学精度，需要新的方法来提高PPG信号建模的鲁棒性和准确性。

Method: 提出SIGMA-PPG模型，包含：1）先验引导的对抗掩码机制，使用强化学习驱动的教师模型利用统计先验创建具有挑战性的学习路径；2）通过向量量化的语义一致性约束，确保生理相同波形映射到共享索引，提高码本语义密度并消除冗余特征结构。

Result: 在超过120,000小时数据上预训练后，SIGMA-PPG在12个多样化下游任务中平均性能优于5个最先进的基线方法。

Conclusion: SIGMA-PPG通过创新的对抗掩码机制和语义一致性约束，有效解决了PPG信号建模中的冗余和噪声问题，为PPG基础模型提供了新的解决方案。

Abstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.

</details>


### [8] [Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2601.21147)
*Kevin Han,Haolin Cong,Bowen Deng,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出动态截断半径方法，显著降低机器学习原子间势能（MLIPs）的内存消耗和推理时间，同时保持模拟稳定性


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势能（MLIPs）在分子动力学模拟中应用广泛，但面临推理时间和内存消耗两大瓶颈，阻碍其达到实际模拟规模。传统方法采用固定截断半径限制了性能优化。

Method: 挑战固定截断半径的传统观念，首次引入动态截断半径公式，通过针对每个原子设定特定邻居数量来诱导原子图稀疏化，显著减少内存和计算需求。在MACE、Nequip、Orbv3和TensorNet四种先进MLIPs上实现该方法。

Result: 动态截断方法使内存消耗减少2.26倍，推理时间加快2.04倍（具体取决于模型和原子系统）。误差分析显示与固定截断模型相比精度下降极小，在材料和分子数据集上均表现良好。

Conclusion: 动态截断半径方法有效解决了MLIPs的推理时间和内存消耗瓶颈，在保持模拟稳定性和精度的同时显著提升性能，所有模型实现和训练代码将完全开源。

Abstract: Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.

</details>


### [9] [The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset](https://arxiv.org/abs/2601.21170)
*Augusto Santos,Teresa Santos,Catarina Rodrigues,José M. F. Moura*

Main category: cs.LG

TL;DR: 提出一种基于协方差矩阵幂次变换的机器学习方法，用于早期检测复杂系统中的涌现现象（如癫痫发作、客户流失等），该方法能够揭示潜在因果结构并实现可解释的预测。


<details>
  <summary>Details</summary>
Motivation: 涌现现象（如癫痫发作、客户流失、疫情爆发）通常源于复杂系统中隐藏的因果相互作用。核心挑战在于：在数据生成过程未知且部分可观测的情况下，揭示并利用系统的潜在因果结构进行早期检测。

Method: 提出一种机器学习方法，从单参数估计器族（经验协方差矩阵或精度矩阵的幂次）中学习最优特征表示，通过调整参数来捕捉驱动关键事件涌现的底层结构。然后使用监督学习模块对学习到的表示进行分类。

Result: 证明了该估计器族的结构一致性，并在癫痫发作检测和客户流失预测任务上验证了方法的有效性，取得了有竞争力的结果。最优协方差幂次显示出良好的可识别性，同时捕捉到结构特征，实现了预测性能与可解释统计结构的统一。

Conclusion: 该方法不仅能够实现早期检测，还通过最优协方差幂次提供了良好的可解释性，在预测性能和统计结构解释之间取得了平衡，为复杂系统中涌现现象的检测提供了新的解决方案。

Abstract: Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.

</details>


### [10] [Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning](https://arxiv.org/abs/2601.21293)
*Changyu Li,Dingcheng Huang,Kexuan Yao,Xiaoya Ni,Lijuan Shen,Fei Luo*

Main category: cs.LG

TL;DR: PG-TMT：用于旋转机械可靠性预测的物理引导微型Mamba Transformer，通过三分支编码器捕获微瞬变、长程动态和跨通道共振，结合极值理论实现校准的早期预警。


<details>
  <summary>Details</summary>
Motivation: 旋转机械的可靠性预测需要能够在非平稳工况、领域偏移和严重类别不平衡下保持准确的早期预警信号，同时保持低且可预测的误报率。

Method: 提出物理引导微型Mamba Transformer（PG-TMT）：1）深度可分离卷积主干捕获微瞬变；2）Tiny-Mamba状态空间分支建模近线性长程动态；3）轻量级局部Transformer编码跨通道共振。结合解析的时频映射将注意力谱与经典轴承故障阶次带对齐，使用极值理论建模健康分数超限，采用双阈值滞后和最小保持时间抑制抖动。

Result: 在CWRU、Paderborn、XJTU-SY和工业试点数据集上，PG-TMT在无泄漏流式协议下获得更高的精确率-召回率AUC（主要针对不平衡问题），竞争性或更好的ROC AUC，以及在匹配误报强度下更短的平均检测时间，同时具备强大的跨领域迁移能力。

Conclusion: 通过将物理对齐表示与EVT校准的决策规则相结合，PG-TMT为可靠性中心的预测和健康管理提供了校准、可解释且可部署的早期预警系统。

Abstract: Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.

</details>


### [11] [Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21312)
*Xiaozhuang Li,Xindi Tang,Fang He*

Main category: cs.LG

TL;DR: 提出GAT-PEARL框架，结合图注意力网络和概率嵌入强化学习，解决动态充电网络下自动驾驶电动出租车车队管理问题


<details>
  <summary>Details</summary>
Motivation: 电动汽车和充电基础设施快速发展，但现有研究假设静态充电网络，与真实动态不确定的充电可用性存在差距，需要能适应动态充电网络布局的解决方案

Method: 提出GAT-PEARL元强化学习框架：1) 使用图注意力网络提取鲁棒空间表示并建模复杂时空关系；2) 采用概率嵌入的actor-critic强化学习实现快速推理适应，无需重新训练即可适应充电网络布局变化

Result: 在中国成都真实数据上进行广泛仿真，GAT-PEARL显著优于传统强化学习基线，对未见过的基础设施布局表现出优越的泛化能力，在动态环境中实现更高的整体运营效率

Conclusion: GAT-PEARL框架成功解决了动态充电网络下自动驾驶电动出租车车队管理的挑战，通过元强化学习实现了对充电网络布局变化的快速适应，提高了运营效率

Abstract: With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.

</details>


### [12] [Graph-Free Root Cause Analysis](https://arxiv.org/abs/2601.21359)
*Luan Pham*

Main category: cs.LG

TL;DR: PRISM是一个无需依赖图的根因分析框架，通过解决故障传播中的异常分数累积问题，显著提升诊断准确性


<details>
  <summary>Details</summary>
Motivation: 现有无依赖图的RCA方法通常假设根因具有最高异常分数，但在故障传播场景中，根因的小延迟可能在下游累积成更大异常，导致这一假设失效

Method: 提出PRISM框架，针对一类组件化系统，在无依赖图情况下进行根因分析，并提供理论保证

Result: 在9个真实数据集735个故障上，PRISM达到68%的Top-1准确率，比最佳基线提升258%，每次诊断仅需8毫秒

Conclusion: PRISM是一个简单高效的RCA框架，无需依赖图即可有效处理故障传播问题，在实际系统中表现优异

Abstract: Failures in complex systems demand rapid Root Cause Analysis (RCA) to prevent cascading damage. Existing RCA methods that operate without dependency graph typically assume that the root cause having the highest anomaly score. This assumption fails when faults propagate, as a small delay at the root cause can accumulate into a much larger anomaly downstream. In this paper, we propose PRISM, a simple and efficient framework for RCA when the dependency graph is absent. We formulate a class of component-based systems under which PRISM performs RCA with theoretical guarantees. On 735 failures across 9 real-world datasets, PRISM achieves 68% Top-1 accuracy, a 258% improvement over the best baseline, while requiring only 8ms per diagnosis.

</details>


### [13] [SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation](https://arxiv.org/abs/2601.21452)
*Yu Xie,Xing Kai Ren,Ying Qi,Hu Yao*

Main category: cs.LG

TL;DR: 本文提出SAGE优化框架，解决OneRec在推荐系统中依赖独立词汇表的问题，并改进其GBPO优化策略的对称保守主义缺陷，实现冷启动项目的高效更新和推荐多样性保持。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统（如OneRec）需要构建独立的词汇表，导致架构无法复用原生LLM词汇，维护成本高且扩展性差。同时，其GBPO优化策略存在"对称保守主义"问题，静态梯度边界抑制了冷启动项目的更新动量，在高噪声环境下无法防止多样性崩溃。

Method: 提出SAGE（Sequence-level Adaptive Gradient Evolution）统一优化框架，包含两个关键创新：1）序列级信号解耦：结合几何平均重要性比率和解耦多目标优势，消除令牌级方差，解决"奖励崩溃"问题；2）非对称自适应动态：构建动态梯度流形，对高潜力冷启动项目应用"提升因子"实现超线性更新，并采用"熵感知惩罚"打破信息茧房。

Result: 理论分析和实证结果表明，SAGE有效解锁了冷启动流量并维持了推荐多样性，同时保持了GBPO的数值稳定性。

Conclusion: SAGE框架成功解决了现有LLM推荐系统的词汇表依赖问题和优化策略缺陷，实现了高效复用开源LLM架构，同时通过自适应梯度机制改善了冷启动性能和推荐多样性。

Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.

</details>


### [14] [Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2601.22020)
*Chengyi Cai,Zesheng Ye,Peike Li,Bo Han,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 提出ViKeR方法，通过视觉引导的关键令牌正则化来改进多模态大语言模型的遗忘学习，重点关注答案中不同令牌的重要性差异。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM遗忘学习方法主要沿用LLM方法，将所有答案令牌同等对待，忽略了它们在遗忘过程中的重要性差异，且只关注语言模态而忽略了视觉线索。

Method: 提出视觉引导的关键令牌正则化(ViKeR)：1)利用无关视觉输入预测理想的遗忘后令牌级分布；2)用这些分布正则化遗忘过程，优先处理关键令牌；3)通过信息熵定义关键令牌；4)通过令牌级梯度重加权放大关键令牌的更新。

Result: 在MLLMU和CLEAR基准测试中，该方法能有效执行遗忘学习，同时减轻遗忘效应并保持回答连贯性。

Conclusion: ViKeR方法通过视觉引导和令牌级重要性区分，显著改进了多模态大语言模型的遗忘学习效果，解决了现有方法忽视令牌重要性差异和视觉线索的问题。

Abstract: Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.

</details>


### [15] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 论文揭示了强化学习中多模态任务下策略崩溃的根本原因在于期望回报目标本身的结构性缺陷，提出了逆概率缩放修正方法，能有效防止模式崩溃。


<details>
  <summary>Details</summary>
Motivation: 许多强化学习问题存在多个质量相当的最优解，但标准期望回报最大化训练的策略通常会崩溃到少数几个结果上。传统解释归因于探索不足或正则化不够，但作者认为这种解释不完整，需要从目标函数本身的结构性缺陷来理解。

Method: 作者分析了期望回报目标的数学结构，发现概率乘子会导致结果频率的指数级放大。提出了逆概率缩放修正方法，从学习信号中移除结果频率放大效应。具体实现为IPS-GRPO（Group Relative Policy Optimization的逆概率缩放版本），无需辅助模型或架构改变。

Result: 在推理和分子生成任务中，IPS-GRPO能显著减少结果级别的模式崩溃，同时匹配或超越基线性能。证明修正目标函数比添加探索启发式方法更能可靠地实现多模态策略优化。

Conclusion: 结果级别的模式崩溃是期望回报目标函数的结构性后果，而非探索不足。逆概率缩放修正能从根本上改变学习动态，产生与奖励成比例的终端分布，有效防止多模态设置下的崩溃。

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [16] [Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models](https://arxiv.org/abs/2601.21794)
*Yejin Kim,Dongjun Hwang,Sungmin Cha,Junsuk Choe*

Main category: cs.LG

TL;DR: KVW是一种无需训练的知识向量弱化方法，通过直接干预模型激活来高效实现大视觉语言模型的遗忘学习


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度优化的遗忘学习方法计算成本高，难以适用于大规模视觉语言模型，需要更高效的解决方案

Method: KVW识别模型在遗忘集上生成输出时激活的知识向量，并逐步弱化这些向量的贡献，避免使用梯度计算

Result: 在MLLMU和CLEAR基准测试中，KVW实现了稳定的遗忘-保留平衡，计算效率显著优于基于梯度和LoRA的方法

Conclusion: KVW提供了一种计算高效的训练免费遗忘学习方法，有效解决了大视觉语言模型中的隐私泄露和有害内容生成问题

Abstract: Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.

</details>


### [17] [Dependence of Equilibrium Propagation Training Success on Network Architecture](https://arxiv.org/abs/2601.21945)
*Qingshan Wang,Clara C. Wanjura,Florian Marquardt*

Main category: cs.LG

TL;DR: 稀疏局部连接网络通过平衡传播训练，在多种基准任务上能达到与密集网络相当的性能，为现实硬件实现提供指导。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展导致能耗不可持续增长，需要寻找数字神经网络替代方案。现有理论研究多关注全连接或密集层状网络，但这些架构在实验实现上存在连接性约束等挑战。

Method: 采用平衡传播这一物理基础训练方法，研究更现实的架构选择——局部连接晶格。训练XY模型，探索架构对各种基准任务的影响，跟踪训练过程中空间分布响应和耦合的演化。

Result: 稀疏网络仅通过局部连接就能实现与密集网络相当的性能表现。

Conclusion: 研究结果为在现实环境中基于平衡传播进一步扩展架构规模提供了指导原则。

Abstract: The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.

</details>


### [18] [Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2601.21978)
*Shiqi Fan,Quanming Yao,Hongyi Nie,Wentao Ma,Zhen Wang,Wen Hua*

Main category: cs.LG

TL;DR: IGETR是一个结合图神经网络和LLM的混合框架，用于时序知识图谱推理，通过三阶段流程提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法过于关注上下文而忽视结构关系，难以从动态图中提取相关子图，导致推理缺乏结构信息、容易产生幻觉，特别是存在时序不一致时。

Method: 提出三阶段框架：1) 使用时序GNN识别结构和时序一致的候选路径；2) LLM引导的路径编辑解决逻辑和语义不一致；3) 集成精炼的推理路径进行预测。

Result: 在标准TKG基准测试中达到SOTA性能，在ICEWS数据集上Hits@1相对提升5.6%，Hits@3提升8.1%。消融研究证实各组件有效性。

Conclusion: IGETR成功结合GNN的结构时序建模能力和LLM的上下文理解，解决了现有方法的结构信息不足和幻觉问题，实现了更准确可解释的时序推理。

Abstract: Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 提出EPDDL语言统一认知规划表示，解决现有DEL框架下规划器碎片化问题


<details>
  <summary>Details</summary>
Motivation: 现有认知规划器基于不同DEL片段，使用临时语言或没有统一表示，导致基准测试难以比较、重用和系统开发

Method: 1. 开发抽象事件模型作为认知动作的新表示；2. 基于DEL和抽象事件模型形式化定义EPDDL语法语义；3. 识别适合现有规划器的片段并展示EPDDL表示

Result: 提出EPDDL语言，能够完整捕获DEL语义，统一规范认知规划任务，促进互操作性、可复现评估和未来发展

Conclusion: EPDDL解决了认知规划领域的碎片化问题，为统一基准测试和规划器比较提供了标准化语言框架

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [20] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK框架通过建模查询不确定性，使用恢复假设和查询锚定聚合，在不牺牲鲁棒性的情况下提升非忠实查询下的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（有噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题，其中观察到的查询是从潜在目标项的噪声召回过程中提取的。

Method: 提出QUARK框架：1）通过恢复假设（即给定观察查询下潜在意图的多个合理解释）显式建模查询不确定性；2）引入查询锚定聚合来稳健地组合这些信号。原始查询作为语义锚点，恢复假设提供受控的辅助证据，防止语义漂移和假设劫持。

Result: 在受控模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，QUARK在稀疏和密集检索器上都提高了Recall、MRR和nDCG。消融实验表明QUARK对恢复假设数量具有鲁棒性，且锚定聚合优于非锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，并结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK是一个简单有效的免训练框架，能在不牺牲鲁棒性的情况下提升检索质量。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [21] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时序电子健康记录设计的检索增强框架，通过事件和时间感知的混合检索、自适应迭代检索以及双路径证据检索与推理，显著提升临床预测性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含丰富的纵向临床证据，对医疗决策至关重要。然而，长时序EHRs常常超出大语言模型的上下文限制，现有方法通常依赖截断或简单检索策略，这会丢弃临床相关事件和时间依赖性。

Method: 提出EHR-RAG框架，包含三个核心组件：1）事件和时间感知的混合EHR检索，以保留临床结构和时间动态；2）自适应迭代检索，逐步优化查询以扩大证据覆盖范围；3）双路径证据检索与推理，同时检索和推理事实性和反事实性证据。

Result: 在四个长时序EHR预测任务上的实验表明，EHR-RAG始终优于最强的基于LLM的基线方法，平均Macro-F1提升了10.76%。

Conclusion: 这项工作突显了检索增强的大语言模型在结构化EHR数据临床预测实践中的潜力，为解决长时序EHRs超出LLM上下文限制的问题提供了有效框架。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [22] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文提出了一种基于搜索的软件测试方法，用于在有限预算下发现企业级智能文档处理系统的多种故障机制，通过比较不同搜索策略发现它们具有互补性，需要组合使用才能有效发现所有风险。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域应用广泛，早期系统验证预算有限，需要发现多样化的故障机制而非单一最坏情况文档，这被形式化为搜索式软件测试问题。

Method: 在文档配置的组合空间中操作，渲染结构风险特征实例以诱导现实故障条件，比较了进化算法、群体智能、质量多样性、学习型和量子算法等多种搜索策略在相同预算约束下的表现。

Result: 不同求解器在可比预算下持续发现特定替代方法未发现的故障模式，跨时间分析显示所有评估预算中都存在求解器特定的持久发现，没有单一策略表现出绝对优势。

Conclusion: 虽然所有求解器的联合最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性延迟重要风险的发现，这证明了求解器的内在互补性，并推动了基于组合策略的工业IDP验证方法。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [23] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出ZSSD零样本统计降尺度框架，无需配对训练数据，通过物理一致气候先验和统一坐标引导解决GCM泛化问题


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据和与再分析数据的领域差距而难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题

Method: ZSSD框架包含：1) 从再分析数据学习的物理一致气候先验，结合地球物理边界和时序信息确保物理有效性；2) 统一坐标引导策略，解决DPS中的梯度消失问题并保证与大尺度场的一致性

Result: ZSSD在99百分位误差上显著优于现有零样本基线，能成功重建复杂天气事件（如热带气旋），并在异构GCMs上表现良好

Conclusion: ZSSD为零样本气候降尺度提供了有效解决方案，通过物理约束和鲁棒推理机制解决了现有方法的局限性，在GCM泛化方面表现出色

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [24] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 论文提出认知启发的两阶段训练框架CoMT+CCRL，通过分离抽象策略学习和具体任务适应，相比传统完整轨迹优化方法，在泛化能力和训练效率上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法通过SFT和基于结果的RL优化完整推理轨迹，但这与人类解决问题的认知过程不符。人类认知自然地将问题解决分解为两个阶段：先获取跨问题通用的抽象策略（元知识），然后将其适应到具体实例。现有方法将完整轨迹作为基本单元，本质上是问题中心的，将抽象策略与问题具体执行纠缠在一起。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过中间步骤的置信度感知奖励优化任务适应，防止过度自信的错误级联，提高执行可靠性。

Result: 在4个模型和8个基准测试上的实验显示，相比标准方法，在分布内性能提升2.19%，分布外性能提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅能带来更优的泛化能力，还能显著提升训练效率，证明了认知启发的训练框架的有效性。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [25] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: DeR2是一个用于评估大语言模型在科学推理能力上的受控测试框架，通过解耦证据访问与推理过程，隔离文档基础推理，解决了现有基准测试中检索、工具链选择和参数记忆等混杂因素的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法准确评估大语言模型对新颖科学信息的推理能力，因为端到端RAG管道中的推理与检索、工具链选择混杂，且受到参数记忆和开放网络波动性的干扰。

Method: DeR2采用受控深度研究沙箱，通过四种证据访问机制（仅指令、概念、仅相关文档、完整集）解耦证据访问与推理，使用两阶段验证防止参数泄漏，并提供冻结文档库和专家标注概念。

Result: 实验显示不同基础模型在科学推理能力上存在显著差异和提升空间：有些模型表现出模式切换脆弱性（完整集表现比仅指令更差），有些则存在结构性概念误用（能正确命名概念但无法作为程序执行）。

Conclusion: DeR2提供了一个精细化的评估框架，能够分离检索损失与推理损失，实现细粒度错误归因，揭示了当前大语言模型在科学推理方面的实质性局限和未来改进方向。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [26] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个针对足球事件流数据的系统性验证框架，通过状态转移模型检测和纠正逻辑不一致性，显著提高数据质量和下游分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析等领域至关重要，但现有数据存在逻辑不一致性（如事件顺序错误或缺失事件）等质量问题，限制了分析模型的可靠性。

Method: 提出VERSA验证框架，基于状态转移模型定义有效事件序列，能够自动检测和纠正事件流数据中的异常模式。

Result: 在K League 1（2024赛季）数据中发现18.81%的事件存在逻辑不一致；VERSA显著提高了跨数据提供商的一致性，并提升了VAEP（球员贡献评估）任务的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性，VERSA框架为足球事件数据提供了系统性的质量保障。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [27] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 提出液态接口(Liquid Interfaces)作为协调范式，将接口视为运行时通过意图表达和语义协商产生的短暂关系事件，而非持久技术制品。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持具有自适应、概率性和上下文相关推理能力的自主代理，而系统集成仍被静态接口和确定性契约所主导。

Method: 形式化液态接口模型，提出液态接口协议(LIP)来管理意图驱动交互、协商执行和在语义不确定性下强制执行短暂性，并描述参考架构。

Result: 液态接口为基于代理系统的自适应协调提供了原则性基础，展示了实际可行性。

Conclusion: 液态接口范式能够解决自主代理系统在自适应协调方面的挑战，通过运行时意图表达和语义协商实现更灵活的交互。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 提出一种新的预训练方法，使用强化学习在预训练阶段直接提升语言模型生成的质量、安全性和事实性，避免后续对齐的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全性和事实性主要依赖昂贵的数据收集和多阶段微调对齐，但这种方法无法纠正预训练阶段学习到的错误模式。需要在预训练阶段就解决这些问题，因为预训练塑造了模型的核心行为，防止不安全或幻觉输出被深度嵌入。

Method: 提出新的预训练方法：流式处理文档，使用强化学习改进每个步骤中生成的接下来K个token。使用一个经过后训练的强模型来评估候选生成（包括模型rollout、原始后缀和重写后缀）的质量、安全性和事实性。训练早期依赖原始和重写后缀，随着模型改进，RL奖励高质量的rollout。

Result: 相比标准预训练，在事实性方面获得36.2%的相对改进，在安全性方面获得18.5%的相对改进，在整体生成质量方面获得高达86.3%的胜率改进。

Conclusion: 该方法从基础层面构建了更高质量、更安全、更事实的模型，通过在预训练阶段直接集成质量、安全性和事实性评估，避免了传统对齐方法的局限性。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [29] [OVD: On-policy Verbal Distillation](https://arxiv.org/abs/2601.21968)
*Jing Xiong,Hui Shen,Shansan Gong,Yuxin Cheng,Jianghan Shen,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Ngai Wong*

Main category: cs.CL

TL;DR: OVD是一种基于轨迹匹配的在线知识蒸馏框架，使用教师模型的离散语言评分替代传统的token级概率匹配，显著降低内存消耗并提升学生模型探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有token级在线蒸馏方法需要学生模型与教师模型token级对齐，这限制了学生模型的探索能力，无法有效利用交互环境反馈，且在强化学习中存在严重内存瓶颈。

Method: 提出在线语言蒸馏(OVD)框架，用教师模型的离散语言评分(0-9)进行轨迹匹配，替代token级概率匹配，避免token级对齐要求，允许学生模型自由探索输出空间。

Result: 在Web问答和数学推理任务上，OVD显著优于现有方法：Web Q&A任务平均EM提升高达12.9%，数学基准提升高达25.7%(仅使用一个随机样本训练)，同时展现更优的训练效率。

Conclusion: OVD通过轨迹匹配和离散语言评分有效解决了传统token级蒸馏的内存瓶颈和探索限制问题，为高效知识蒸馏提供了新方向。

Abstract: Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [30] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: InspecSafe-V1是首个用于工业巡检安全评估的多模态基准数据集，包含真实工业环境中5种场景的5013个巡检实例，提供7种同步感知模态和像素级分割标注。


<details>
  <summary>Details</summary>
Motivation: 当前公开数据集存在模拟数据源、单模态感知或缺乏细粒度对象级标注等问题，限制了工业基础模型的鲁棒场景理解和多模态安全推理能力，阻碍了预测性维护和自主巡检的部署。

Method: 从真实工业环境中41个轮式和轨道式巡检机器人的2239个有效巡检点收集数据，涵盖隧道、电力设施、烧结设备、石油化工和煤炭输送栈桥五种场景，提供像素级分割标注、语义场景描述和安全等级标签。

Result: 构建了包含5013个巡检实例的InspecSafe-V1数据集，提供可见光图像、红外视频、音频、深度点云、雷达点云、气体测量、温湿度等7种同步感知模态，支持多模态异常识别、跨模态融合和综合安全评估。

Conclusion: InspecSafe-V1填补了工业巡检安全评估领域多模态基准数据集的空白，为开发工业基础模型、实现鲁棒场景理解和多模态安全推理提供了重要资源。

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [31] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 论文提出了一种分层主动调参框架HPTune，用于模型预测控制运动规划器的参数调优，通过评估未执行动作和结合多普勒激光雷达数据，实现更高效的参数更新和适应性运动规划。


<details>
  <summary>Details</summary>
Motivation: 现有MPC运动规划器的参数调优方法通常只评估已执行的动作，导致参数更新效率低下，因为失败事件（如障碍物接近或碰撞）较为稀疏。需要一种更全面的评估方法来提高调参效率。

Method: 提出分层主动调参框架HPTune：1）快速级调参采用预测接近速度和预测接近距离作为风险指标；2）慢速级调参利用扩展评估损失进行闭环反向传播。同时集成多普勒激光雷达提供障碍物速度信息，增强运动预测能力。

Result: 在高保真模拟器上的大量实验表明，HPTune实现了高效的MPC调参，在复杂环境中优于各种基线方案。该框架能够制定安全、敏捷的避碰策略，实现情境定制的运动规划。

Conclusion: HPTune通过评估未执行动作和结合多普勒激光雷达数据，显著提高了MPC运动规划器的参数调优效率和适应性，能够在复杂环境中实现更安全、更敏捷的碰撞避免策略。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [32] [Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning](https://arxiv.org/abs/2601.21548)
*Irene Ambrosini,Ingo Blakowski,Dmitrii Zendrikov,Cristiano Capone,Luna Gava,Giacomo Indiveri,Chiara De Luca,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 使用脉冲神经网络在神经形态处理器上实现空气曲棍球实时学习控制


<details>
  <summary>Details</summary>
Motivation: 空气曲棍球需要高速决策，传统方法难以应对。研究旨在将神经科学启发的硬件与真实世界机器人控制结合，展示脑启发方法能够处理快速交互任务并支持智能机器的持续学习。

Method: 采用混合信号模拟/数字神经形态处理器上的脉冲神经网络，通过硬件与学习算法的协同设计，使用固定随机连接捕获任务时间结构，在读出层采用局部e-prop学习规则，利用事件驱动活动实现快速高效学习。

Result: 通过强化学习在极少数试验中实现成功的冰球交互，展示了包含计算机和神经形态芯片的实时学习设置，能够实际训练用于机器人自主系统的脉冲神经网络。

Conclusion: 这项工作将神经科学启发的硬件与真实世界机器人控制连接起来，表明脑启发方法能够处理快速交互任务，同时支持智能机器的持续在线学习。

Abstract: Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.

</details>


### [33] [From Instruction to Event: Sound-Triggered Mobile Manipulation](https://arxiv.org/abs/2601.21667)
*Hao Ju,Shaofei Huang,Hongyu Li,Zihan Ding,Si Liu,Meng Wang,Zhedong Zheng*

Main category: cs.RO

TL;DR: 提出声音触发的移动操作新范式，让智能体主动感知声音源并交互，无需显式指令，通过Habitat-Echo平台和分层基线模型实现


<details>
  <summary>Details</summary>
Motivation: 当前移动操作研究主要依赖预定义文本指令，限制了智能体的自主性和对动态环境事件的响应能力，需要更主动的感知交互方式

Method: 开发Habitat-Echo数据平台整合声学渲染与物理交互，提出包含高层任务规划器和低层策略模型的分层基线框架

Result: 实验表明基线模型使智能体能主动检测和响应听觉事件，无需逐案指令；在挑战性双声源场景中能隔离主要声源并顺序操作多个对象

Conclusion: 声音触发移动操作是可行的新范式，提出的Habitat-Echo平台和分层基线为自主感知交互提供了有效解决方案，展示了在复杂声学环境中的鲁棒性

Abstract: Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.

</details>


### [34] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: mjlab是一个轻量级开源机器人学习框架，结合GPU加速仿真与可组合环境，安装简单，依赖少


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习框架通常设置复杂、依赖多，需要一个轻量级、易于安装、GPU加速的框架来降低入门门槛

Method: 采用Isaac Lab引入的manager-based API，用户可组合观测、奖励和事件的模块化构建块，结合MuJoCo Warp实现GPU加速物理仿真

Result: 创建了一个单命令安装的框架，依赖极少，提供对原生MuJoCo数据结构的直接访问，并包含速度跟踪、运动模仿和操作任务的参考实现

Conclusion: mjlab提供了一个轻量级、易于使用的机器人学习框架，降低了GPU加速仿真的入门门槛，有助于推动机器人学习研究

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>
