<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Factorized Learning for Temporally Grounded Video-Language Models](https://arxiv.org/abs/2512.24097)
*Wenzheng Zeng,Difei Gao,Mike Zheng Shou,Hwee Tou Ng*

Main category: cs.CV

TL;DR: D²VLM：通过解耦学习框架和因子化偏好优化算法，提升视频语言模型的事件级时间定位和文本响应能力


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在事件级感知的时间定位方面存在不足，时间定位和文本响应这两个关键任务通常耦合处理，缺乏清晰的逻辑层次结构，导致优化目标不理想

Method: 提出D²VLM框架，采用"先定位后回答并引用证据"范式，引入证据令牌进行证据定位；设计因子化偏好优化算法，将概率时间定位建模显式纳入优化目标；构建合成数据集支持因子化偏好学习

Result: 在多个任务上的实验表明该方法具有明显优势，代码已开源

Conclusion: 通过解耦学习和因子化偏好优化，D²VLM显著提升了视频语言模型的时间定位准确性和文本响应可靠性，为事件级视频理解提供了有效解决方案

Abstract: Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.

</details>


### [2] [Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design](https://arxiv.org/abs/2512.24120)
*Chandini Vysyaraju,Raghuvir Duvvuri,Avi Goyal,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 该论文提出两种关键贡献：Few-Shot Architecture Prompting (FSAP) 用于LLM视觉架构生成，发现n=3示例最佳；以及Whitespace-Normalized Hash Validation 用于去重，速度提升100倍。在7个视觉基准上生成1,900个独特架构。


<details>
  <summary>Details</summary>
Motivation: 自动化神经网络架构设计在计算机视觉中仍具挑战性。任务多样性和计算约束需要有效的架构和高效的搜索方法。LLMs作为NAS的替代方案很有前景，但在计算机视觉架构生成中的应用尚未系统研究，特别是在提示工程和验证策略方面。

Method: 基于任务无关的NNGPT/LEMUR框架，提出FSAP系统研究支持示例数量(n=1-6)对LLM架构生成的影响；引入Whitespace-Normalized Hash Validation轻量级去重方法；在7个计算机视觉基准上进行大规模实验；采用数据集平衡评估方法。

Result: 发现n=3示例在视觉任务中最佳平衡架构多样性和上下文聚焦；去重方法提供100倍加速；生成1,900个独特架构；建立严格的评估实践，使有限计算资源的研究者能更易进行自动化设计。

Conclusion: 为计算机视觉中基于LLM的架构搜索提供可操作指南，建立严谨评估实践，使自动化设计对计算资源有限的研究者更易访问。

Abstract: Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.

</details>


### [3] [Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning](https://arxiv.org/abs/2512.24146)
*Chubin Chen,Sujie Hu,Jiashu Zhu,Meiqi Wu,Jintao Chen,Yanxun Li,Nisha Huang,Chengyu Fang,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: 提出D²-Align框架解决文本到图像扩散模型在人类反馈强化学习中出现的偏好模式崩溃问题，通过方向性修正奖励信号来保持生成多样性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型通过人类反馈强化学习虽然能在自动化奖励指标上获得高分，但会导致偏好模式崩溃——模型收敛到狭窄的高分输出（如单一风格或过度曝光），严重损害生成多样性。

Method: 提出方向性解耦对齐框架：1）在奖励模型的嵌入空间中学习方向性修正（保持模型冻结）；2）在优化过程中应用修正后的奖励信号，防止模型崩溃到特定模式。

Result: 提出DivGenBench基准量化偏好模式崩溃现象；D²-Align框架在质量和多样性指标上均表现出色，实现了更好的人类偏好对齐。

Conclusion: 偏好模式崩溃由奖励模型固有偏见的过度优化驱动，D²-Align通过方向性修正奖励信号有效缓解此问题，在保持多样性的同时实现更好的人类偏好对齐。

Abstract: Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.

</details>


### [4] [MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation](https://arxiv.org/abs/2512.24243)
*Fuqiang Gu,Yuanke Li,Xianlei Long,Kangping Ji,Chao Chen,Qingyi Gu,Zhenliang Ni*

Main category: cs.CV

TL;DR: MambaSeg：使用并行Mamba编码器的双分支语义分割框架，结合RGB和事件数据，通过双维度交互模块进行时空融合，在DDD17和DSEC数据集上实现SOTA性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RGB方法在快速运动、低光照或高动态范围条件下性能下降，事件相机虽有高时间分辨率和低延迟优势但缺乏颜色和纹理信息。现有RGB-事件融合方法计算昂贵且主要关注空间融合，忽略了事件流的时间动态特性。

Method: 提出MambaSeg框架：1) 使用并行Mamba编码器分别建模RGB图像和事件流；2) 引入双维度交互模块(DDIM)，包含跨空间交互模块(CSIM)和跨时间交互模块(CTIM)，在空间和时间维度进行细粒度融合；3) 改善跨模态对齐，减少模糊性，利用各模态互补特性。

Result: 在DDD17和DSEC数据集上的广泛实验表明，MambaSeg实现了最先进的语义分割性能，同时显著降低了计算成本。

Conclusion: MambaSeg展示了高效、可扩展和鲁棒的多模态感知潜力，通过时空融合有效结合RGB和事件数据的优势，解决了现有方法的局限性。

Abstract: Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.

</details>


### [5] [DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model](https://arxiv.org/abs/2512.24408)
*Bohong Chen,Haiyang Liu*

Main category: cs.CV

TL;DR: DyStream：基于流匹配的自回归模型，用于实时生成双人对话头部视频，延迟低于100ms，实现高质量唇音同步


<details>
  <summary>Details</summary>
Motivation: 现有基于分块的方法需要完整的非因果上下文窗口，导致显著延迟，无法满足实时对话中非语言反馈的需求

Method: 采用流友好的自回归框架，结合流匹配头部进行概率建模；提出由前瞻模块增强的因果编码器，在保持低延迟的同时融入短期未来上下文（如60ms）

Result: 每帧生成时间34ms，系统总延迟低于100ms；在HDTF数据集上获得离线8.13和在线7.61的LipSync Confidence分数，达到最先进的唇音同步质量

Conclusion: DyStream通过创新的因果架构设计，实现了超低延迟的实时双人对话视频生成，在延迟和唇音同步质量方面均优于现有方法

Abstract: Generating realistic, dyadic talking head video requires ultra-low latency. Existing chunk-based methods require full non-causal context windows, introducing significant delays. This high latency critically prevents the immediate, non-verbal feedback required for a realistic listener. To address this, we present DyStream, a flow matching-based autoregressive model that could generate video in real-time from both speaker and listener audio. Our method contains two key designs: (1) we adopt a stream-friendly autoregressive framework with flow-matching heads for probabilistic modeling, and (2) We propose a causal encoder enhanced by a lookahead module to incorporate short future context (e.g., 60 ms) to improve quality while maintaining low latency. Our analysis shows this simple-and-effective method significantly surpass alternative causal strategies, including distillation and generative encoder. Extensive experiments show that DyStream could generate video within 34 ms per frame, guaranteeing the entire system latency remains under 100 ms. Besides, it achieves state-of-the-art lip-sync quality, with offline and online LipSync Confidence scores of 8.13 and 7.61 on HDTF, respectively. The model, weights and codes are available.

</details>


### [6] [From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation](https://arxiv.org/abs/2512.24639)
*Siyang Wang,Hanting Li,Wei Li,Jie Hu,Xinghao Chen,Feng Zhao*

Main category: cs.CV

TL;DR: RadAR：一种基于径向拓扑的并行自回归视觉生成框架，通过环状并行预测和嵌套注意力机制，在保持生成质量的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型在视觉生成中采用顺序解码机制，导致推理效率低下。视觉token具有强烈的局部依赖性和空间相关性，但标准光栅扫描解码顺序未能充分利用这一特性。

Method: 提出径向拓扑生成框架：1）选择初始token作为起点；2）根据空间距离将所有token分组为多个同心环；3）从内向外按环并行生成；4）引入嵌套注意力机制动态修正不一致预测，防止错误累积。

Result: RadAR在保持自回归模型表示能力的同时，显著提高了视觉生成的推理效率，通过并行化预测和动态修正机制实现了高效且高质量的生成。

Conclusion: RadAR通过径向并行预测和嵌套注意力机制，成功解决了传统自回归视觉生成的低效问题，为高效视觉生成提供了新范式。

Abstract: Inspired by the remarkable success of autoregressive models in language modeling, this paradigm has been widely adopted in visual generation. However, the sequential token-by-token decoding mechanism inherent in traditional autoregressive models leads to low inference efficiency.In this paper, we propose RadAR, an efficient and parallelizable framework designed to accelerate autoregressive visual generation while preserving its representational capacity. Our approach is motivated by the observation that visual tokens exhibit strong local dependencies and spatial correlations with their neighbors--a property not fully exploited in standard raster-scan decoding orders. Specifically, we organize the generation process around a radial topology: an initial token is selected as the starting point, and all other tokens are systematically grouped into multiple concentric rings according to their spatial distances from this center. Generation then proceeds in a ring-wise manner, from inner to outer regions, enabling the parallel prediction of all tokens within the same ring. This design not only preserves the structural locality and spatial coherence of visual scenes but also substantially increases parallelization. Furthermore, to address the risk of inconsistent predictions arising from simultaneous token generation with limited context, we introduce a nested attention mechanism. This mechanism dynamically refines implausible outputs during the forward pass, thereby mitigating error accumulation and preventing model collapse. By integrating radial parallel prediction with dynamic output correction, RadAR significantly improves generation efficiency.

</details>


### [7] [EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation](https://arxiv.org/abs/2512.24731)
*Bingxuan Li,Yiming Cui,Yicheng He,Yiwei Wang,Shu Zhang,Longyin Wen,Yulei Niu*

Main category: cs.CV

TL;DR: EchoFoley：视频声音生成新任务，通过符号化声音事件实现细粒度控制，提出EchoVidia框架在可控性和感知质量上显著超越现有方法


<details>
  <summary>Details</summary>
Motivation: 当前视频-文本到音频（VT2A）方法存在三个关键限制：1）视觉与文本条件不平衡导致视觉主导；2）缺乏细粒度可控生成的具体定义；3）指令理解和跟随能力弱，现有数据集依赖简短分类标签

Method: 提出EchoFoley任务，通过符号化声音事件表示（指定何时、什么、如何产生声音）实现事件级局部控制和分层语义控制；构建EchoFoley-6k大规模专家标注数据集；提出EchoVidia框架，采用慢-快思维策略的以声音事件为中心的智能生成框架

Result: EchoVidia在可控性上超越最近VT2A模型40.7%，在感知质量上提升12.5%

Conclusion: EchoFoley任务和EchoVidia框架有效解决了现有VT2A方法的局限性，通过符号化声音事件表示实现了细粒度可控的视频声音生成

Abstract: Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.

</details>


### [8] [Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection](https://arxiv.org/abs/2512.24922)
*Bartłomiej Olber,Jakub Winter,Paweł Wawrzyński,Andrii Gamalii,Daniel Górniak,Marcin Łojek,Robert Nowak,Krystian Radlak*

Main category: cs.CV

TL;DR: 提出基于神经元激活模式的LiDAR域自适应方法，仅需标注目标域中少量代表性样本即可达到SOTA性能，结合持续学习技术防止权重漂移。


<details>
  <summary>Details</summary>
Motivation: 3D物体检测器在自动驾驶感知系统中至关重要，但在不同地理区域（如美国、亚洲、欧洲）之间存在域泛化问题，模型在跨域时性能显著下降。

Method: 基于神经元激活模式选择目标域中少量代表性且多样化的样本进行标注，结合受持续学习启发的后训练技术防止权重漂移。

Result: 该方法在域自适应任务中优于线性探测和现有SOTA域自适应技术，且仅需极少的标注预算。

Conclusion: 通过神经元激活模式选择关键样本进行标注，结合持续学习技术，能够以低成本实现高效的LiDAR域自适应，提升3D检测器的跨域泛化能力。

Abstract: 3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [HINTS: Extraction of Human Insights from Time-Series Without External Sources](https://arxiv.org/abs/2512.23755)
*Sheo Yon Jhin,Noseong Park*

Main category: cs.LG

TL;DR: HINTS是一个自监督学习框架，无需外部数据，从时间序列残差中内生提取人类因素（如社会影响、记忆和偏见），通过FJ意见动力学模型作为结构归纳偏置，集成到骨干模型中提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 人类决策、情感和集体心理学是影响金融经济系统时间动态的复杂因素。现有方法依赖外部数据（新闻、社交媒体）来捕捉这些因素，但带来高昂的数据依赖成本（财务、计算和实际应用）。需要一种无需外部数据就能内生提取这些潜在因素的方法。

Method: 提出HINTS自监督学习框架：1）使用Friedkin-Johnsen意见动力学模型作为结构归纳偏置，建模演化中的社会影响、记忆和偏见模式；2）从时间序列残差中内生提取人类因素；3）将提取的因素作为注意力图集成到最先进的骨干预测模型中。

Result: 在9个真实世界和基准数据集上的实验表明，HINTS能持续提升预测准确性。案例研究和消融研究验证了模型的可解释性，显示提取的因素与现实世界事件有很强的语义对齐，证明了HINTS的实用价值。

Conclusion: HINTS框架成功实现了无需外部数据的内生人类因素提取，通过FJ意见动力学模型提供结构指导，不仅提升了预测性能，还提供了可解释的洞察，展示了在金融经济时间序列预测中的实际效用。

Abstract: Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.

</details>


### [10] [FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading](https://arxiv.org/abs/2512.23773)
*Molei Qin,Xinyu Cai,Yewen Li,Haochong Xia,Chuqiao Zong,Shuo Sun,Xinrun Wang,Bo An*

Main category: cs.LG

TL;DR: FineFT提出三阶段集成强化学习框架，通过选择性更新、盈利能力筛选和VAE引导的风险管理，解决加密货币期货交易中高杠杆带来的训练不稳定和风险控制问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法主要针对现货市场，无法直接应用于高杠杆的期货市场，面临两大挑战：1) 高杠杆放大奖励波动，导致训练不稳定难以收敛；2) 现有方法缺乏对能力边界的自我认知，面对新市场状态（如黑天鹅事件）时容易遭受重大损失。

Method: 提出三阶段集成强化学习框架：阶段I使用集成TD误差选择性更新Q学习器以提高收敛性；阶段II基于盈利能力筛选Q学习器，并训练VAE识别学习器的能力边界；阶段III根据训练好的VAE指导，从筛选后的集成策略和保守策略中选择，以保持盈利能力并降低新市场状态下的风险。

Result: 在高频交易环境下对加密货币期货进行实验（5倍杠杆），FineFT在6个金融指标上优于12个SOTA基线方法，风险降低超过40%，同时获得比第二名更优的盈利能力。可视化显示不同智能体专注于不同的市场动态，消融研究证实VAE路由有效降低最大回撤，选择性更新改善收敛和性能。

Conclusion: FineFT通过集成强化学习框架有效解决了高杠杆期货交易中的训练不稳定和风险控制问题，实现了在保持盈利能力的同时显著降低风险，为量化交易提供了新的解决方案。

Abstract: Futures are contracts obligating the exchange of an asset at a predetermined date and price, notable for their high leverage and liquidity and, therefore, thrive in the Crypto market. RL has been widely applied in various quantitative tasks. However, most methods focus on the spot and could not be directly applied to the futures market with high leverage because of 2 challenges. First, high leverage amplifies reward fluctuations, making training stochastic and difficult to converge. Second, prior works lacked self-awareness of capability boundaries, exposing them to the risk of significant loss when encountering new market state (e.g.,a black swan event like COVID-19). To tackle these challenges, we propose the Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading (FineFT), a novel three-stage ensemble RL framework with stable training and proper risk management. In stage I, ensemble Q learners are selectively updated by ensemble TD errors to improve convergence. In stage II, we filter the Q-learners based on their profitabilities and train VAEs on market states to identify the capability boundaries of the learners. In stage III, we choose from the filtered ensemble and a conservative policy, guided by trained VAEs, to maintain profitability and mitigate risk with new market states. Through extensive experiments on crypto futures in a high-frequency trading environment with high fidelity and 5x leverage, we demonstrate that FineFT outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by more than 40% while achieving superior profitability compared to the runner-up. Visualization of the selective update mechanism shows that different agents specialize in distinct market dynamics, and ablation studies certify routing with VAEs reduces maximum drawdown effectively, and selective update improves convergence and performance.

</details>


### [11] [GARDO: Reinforcing Diffusion Models without Reward Hacking](https://arxiv.org/abs/2512.24138)
*Haoran He,Yuxiao Ye,Jie Liu,Jiajun Liang,Zhiyong Wang,Ziyang Yuan,Xintao Wang,Hangyu Mao,Pengfei Wan,Ling Pan*

Main category: cs.LG

TL;DR: GARDO框架通过选择性正则化、自适应参考模型更新和多样性奖励增强，解决扩散模型在线RL微调中的奖励黑客、探索不足和模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在线强化学习微调常因代理奖励与真实目标不匹配导致奖励黑客问题——代理分数上升但真实图像质量下降、生成多样性崩溃。现有正则化方法使用次优参考策略，会牺牲样本效率并阻碍探索。

Method: 提出GARDO框架：1) 选择性正则化：仅对高不确定性样本施加惩罚；2) 自适应正则化：定期更新参考模型以匹配在线策略能力；3) 多样性感知优化：对高质量且高多样性的样本增强奖励。

Result: 在多种代理奖励和未见指标上的实验表明，GARDO能有效缓解奖励黑客问题，提升生成多样性，同时不牺牲样本效率或探索能力。

Conclusion: GARDO通过选择性正则化、自适应参考模型更新和多样性奖励增强，为扩散模型RL微调提供了平衡样本效率、探索和奖励黑客缓解的有效解决方案。

Abstract: Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.

</details>


### [12] [Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning](https://arxiv.org/abs/2512.24404)
*Soham Pahari,M. Srinivas*

Main category: cs.LG

TL;DR: 提出ViReLoc视觉推理框架，通过纯视觉表示进行规划和定位，无需依赖文本推理或实时GPS数据，在空间任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能系统主要依赖文本信息进行推理，限制了在视觉导航和地理定位等空间任务中的有效性。文本推理难以理解空间依赖和几何关系，需要探索纯视觉推理范式。

Method: 提出Geo-Consistent Visual Planning范式，开发ViReLoc框架：1) 学习空间依赖和几何关系；2) 在视觉域进行逐步推理编码；3) 使用强化学习目标优化；4) 集成对比学习和自适应特征交互来对齐跨视角并减少视点差异。

Result: 在多样化导航和定位场景实验中，ViReLoc在空间推理准确性和跨视角检索性能方面均取得一致改进，证明了视觉推理的有效性。

Conclusion: 视觉推理是导航和定位任务的强有力补充方法，可以在无需实时GPS数据的情况下执行空间任务，提供更安全的导航解决方案。

Abstract: Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.

</details>


### [13] [Scaling Open-Ended Reasoning to Predict the Future](https://arxiv.org/abs/2512.25070)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.LG

TL;DR: 训练语言模型进行开放式预测，使用新闻数据合成预测问题，开发OpenForecaster 8B模型，在准确度、校准和一致性方面匹配更大规模的专有模型


<details>
  <summary>Details</summary>
Motivation: 高风险决策需要在不确定性下对未来进行推理，需要开发能够进行开放式预测的语言模型，但现有训练数据有限且存在信息泄露风险

Method: 1) 从每日新闻中自动合成预测问题；2) 使用离线新闻语料防止信息泄露；3) 训练Qwen3思维模型；4) 结合检索机制；5) 改进强化学习奖励函数

Result: OpenForecaster 8B模型在2025年5-8月的测试中，在准确度、校准和一致性方面匹配更大规模的专有模型，且校准改进能泛化到其他基准测试

Conclusion: 通过自动化数据合成和专门训练，可以开发出高效的语言模型预测系统，开源模型、代码和数据将促进语言模型预测研究的普及

Abstract: High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自进化代理框架，通过将LLM集成到"计划-执行-总结"认知范式中，显著提高了进化效率，在算法发现和机器学习管道优化中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法在从静态LLM向自改进代理过渡时存在结构化推理不足的问题，现有方法在高维代码空间中容易过早收敛和探索效率低下。

Method: 提出LoongFlow框架，集成LLM到"计划-执行-总结"认知范式，采用混合进化记忆系统，结合多岛模型、MAP-Elites和自适应玻尔兹曼选择来平衡探索与利用。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow比OpenEvolve、ShinkaEvolve等基线方法进化效率提升高达60%，并能发现更优解决方案。

Conclusion: LoongFlow在自主科学发现领域迈出了重要一步，能够以更低的计算成本生成专家级解决方案，实现了从"盲目"突变到认知驱动进化的转变。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning](https://arxiv.org/abs/2512.23765)
*Tiancheng Su,Meicong Zhang,Guoxiu He*

Main category: cs.CL

TL;DR: EASD提出了一种无需训练的推测解码增强方法，通过动态熵惩罚机制，在模型不确定性高时拒绝候选token，让目标模型重新采样，从而可能超越目标模型自身性能。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码中草稿模型与目标模型过度对齐，限制了性能只能达到目标模型水平。需要一种方法既能加速推理，又能突破目标模型的性能上限。

Method: 在标准推测解码基础上，引入动态熵惩罚机制：使用采样分布的熵量化模型不确定性，当两个模型都表现出高熵且top-N预测重叠显著时，拒绝对应token并由目标LLM重新采样。

Result: 在多个推理基准测试中，EASD始终优于现有推测解码方法，并且在大多数情况下超越了目标LLM本身的性能。同时证明EASD的效率与标准推测解码相当。

Conclusion: EASD通过熵感知的动态惩罚机制，不仅保持了推测解码的效率优势，还实现了超越目标模型性能的可能性，为LLM推理加速提供了新的有效方法。

Abstract: Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.

</details>


### [16] [Modeling Language as a Sequence of Thoughts](https://arxiv.org/abs/2512.25026)
*Nasim Borazjanizadeh,James McClelland*

Main category: cs.CL

TL;DR: 提出Thought Gestalt模型，这是一种结合token和句子级"思想"状态的双层抽象Transformer，通过记忆机制提升语言建模的全局一致性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer主要依赖表层共现统计，缺乏对实体和事件的全局一致潜在表示，导致关系方向泛化能力弱、上下文错误和数据效率低。受认知科学启发，人类理解语言时会将其转换为紧凑的事件表示并保留在记忆中。

Method: 提出Thought Gestalt模型，一种循环Transformer，在token和句子级"思想"状态两个抽象层次上建模语言。模型逐句生成token，同时交叉关注先前句子表示的记忆。使用相同参数集生成token和句子表示，通过单一的下一个token交叉熵目标训练，梯度通过交叉注意力反向传播优化早期句子向量生成参数。

Result: 在扩展实验中，TG相比匹配的GPT-2基线持续提升效率，缩放拟合表明GPT-2需要多5-8%的数据和33-42%的参数才能达到TG的损失水平。TG在父子关系反转诅咒探针上减少了关系方向泛化错误。

Conclusion: Thought Gestalt模型通过结合token级和句子级表示，模仿人类认知中的记忆机制，有效提升了语言建模的全局一致性、数据效率和关系方向泛化能力。

Abstract: Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level "thought" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.

</details>
