{"id": "2601.02569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02569", "abs": "https://arxiv.org/abs/2601.02569", "authors": ["Hossein Rajabzadeh", "Maryam Dialameh", "Chul B. Park", "Il-Min Kim", "Hyock Ju Kwon"], "title": "LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference", "comment": null, "summary": "Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \\textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \\emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \\emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \\textbf{LLaMA2-7B}, \\textbf{LLaMA3-8B}, \\textbf{Qwen2.5-7B}, and \\textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \\textbf{2.6$\\times$ faster decoding} and \\textbf{45--55\\% KV-cache reduction} while staying within \\textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \\emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.", "AI": {"tldr": "LoRA-Drop\uff1a\u901a\u8fc7\u65f6\u95f4\u8ba1\u7b97\u8c03\u5ea6\u52a0\u901fLLM\u89e3\u7801\uff0c\u5728\u5927\u591a\u6570\u89e3\u7801\u6b65\u9aa4\u4e2d\u91cd\u7528\u524d\u4e00\u4e2atoken\u7684\u9690\u85cf\u72b6\u6001\u5e76\u5e94\u7528\u4f4e\u79e9LoRA\u6821\u6b63\uff0c\u5b9a\u671f\u5237\u65b0\u6267\u884c\u5b8c\u6574\u6a21\u578b\uff0c\u65e0\u9700\u8def\u7531\u7f51\u7edc\uff0c\u517c\u5bb9\u6807\u51c6KV\u7f13\u5b58\u3002", "motivation": "\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u987a\u5e8f\u89e3\u7801\uff0c\u6bcf\u4e2a\u65b0token\u901a\u5e38\u9700\u8981\u6267\u884c\u6240\u6709transformer\u5c42\u3002\u73b0\u6709\u52a8\u6001\u6df1\u5ea6\u548c\u5c42\u8df3\u8fc7\u65b9\u6cd5\u867d\u7136\u80fd\u51cf\u5c11\u6210\u672c\uff0c\u4f46\u4f9d\u8d56\u8f85\u52a9\u8def\u7531\u673a\u5236\u6216\u5728\u8df3\u8fc7\u5c42\u672a\u8865\u507f\u65f6\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51faLoRA-Drop\u6846\u67b6\uff0c\u5bf9\u56fa\u5b9a\u4e2d\u95f4\u5c42\u5b50\u96c6\u5e94\u7528\u65f6\u95f4\u8ba1\u7b97\u8c03\u5ea6\uff1a\u5927\u591a\u6570\u89e3\u7801\u6b65\u9aa4\u91cd\u7528\u524d\u4e00\u4e2atoken\u9690\u85cf\u72b6\u6001\u5e76\u5e94\u7528\u4f4e\u79e9LoRA\u6821\u6b63\uff0c\u5b9a\u671f\u5237\u65b0\u6b65\u9aa4\u6267\u884c\u5b8c\u6574\u6a21\u578b\u9632\u6b62\u6f02\u79fb\u3002\u65e0\u9700\u8def\u7531\u7f51\u7edc\uff0c\u517c\u5bb9\u6807\u51c6KV\u7f13\u5b58\uff0c\u53ef\u5728LoRA\u6b65\u9aa4\u4e2d\u8df3\u8fc7KV\u66f4\u65b0\u51cf\u5c11KV\u7f13\u5b58\u5360\u7528\u3002", "result": "\u5728LLaMA2-7B\u3001LLaMA3-8B\u3001Qwen2.5-7B\u548cQwen2.5-14B\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u8fbe2.6\u500d\u89e3\u7801\u52a0\u901f\u548c45-55% KV\u7f13\u5b58\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u7cbe\u5ea6\u76f8\u5dee\u57280.5\u4e2a\u767e\u5206\u70b9\u5185\u3002\u5728\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u957f\u4e0a\u4e0b\u6587/\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc6\u522b\u51fa\u4fdd\u6301\u8d28\u91cf\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u6548\u7387\u63d0\u5347\u7684\u8c03\u5ea6\u914d\u7f6e\u5b89\u5168\u533a\u3002", "conclusion": "LoRA-Drop\u4e3aLLM\u4e2d\u7684\u81ea\u9002\u5e94\u5bb9\u91cf\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u7b80\u5355\u8def\u5f84\uff0c\u901a\u8fc7\u65f6\u95f4\u8ba1\u7b97\u8c03\u5ea6\u5b9e\u73b0\u9ad8\u6548\u89e3\u7801\uff0c\u65e0\u9700\u590d\u6742\u8def\u7531\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2601.02578", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.02578", "abs": "https://arxiv.org/abs/2601.02578", "authors": ["Mengyi Sun"], "title": "DataParasite Enables Scalable and Repurposable Online Data Curation", "comment": null, "summary": "Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.", "AI": {"tldr": "DataParasite\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u6a21\u5757\u5316\u7684\u5728\u7ebf\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u53ef\u5c06\u8868\u683c\u6570\u636e\u6574\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u72ec\u7acb\u7684\u5b9e\u4f53\u7ea7\u641c\u7d22\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\uff0c\u4f7f\u7528\u5171\u4eab\u7684Python\u811a\u672c\u6267\u884c\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6210\u672c\u3002", "motivation": "\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u8bb8\u591a\u7814\u7a76\u4f9d\u8d56\u4e8e\u4ece\u5f02\u6784\u5728\u7ebf\u6765\u6e90\u7ec4\u88c5\u7684\u6570\u636e\u5e93\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u901a\u5e38\u52b3\u52a8\u5bc6\u96c6\u3001\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u590d\u73b0\u3002\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u641c\u7d22\u7cfb\u7edf\u5f80\u5f80\u4e0d\u900f\u660e\u3001\u4e0d\u7075\u6d3b\u6216\u4e0d\u9002\u5408\u79d1\u5b66\u6570\u636e\u6574\u7406\u3002", "method": "DataParasite\u5c06\u8868\u683c\u6574\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u72ec\u7acb\u7684\u5b9e\u4f53\u7ea7\u641c\u7d22\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\uff0c\u4f7f\u7528\u5171\u4eab\u7684\u4efb\u52a1\u65e0\u5173Python\u811a\u672c\u6267\u884c\u3002\u7ba1\u9053\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u91cd\u65b0\u7528\u4e8e\u65b0\u4efb\u52a1\uff0c\u5305\u62ec\u6ca1\u6709\u9884\u5b9a\u4e49\u5b9e\u4f53\u5217\u8868\u7684\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7ecf\u5178\u4efb\u52a1\uff08\u6559\u5e08\u62db\u8058\u5386\u53f2\u3001\u7cbe\u82f1\u6b7b\u4ea1\u4e8b\u4ef6\u3001\u653f\u6cbb\u751f\u6daf\u8f68\u8ff9\uff09\u4e0a\u8bc4\u4f30\uff0cDataParasite\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5c06\u6570\u636e\u6536\u96c6\u6210\u672c\u76f8\u5bf9\u4e8e\u4eba\u5de5\u6574\u7406\u964d\u4f4e\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "DataParasite\u901a\u8fc7\u964d\u4f4e\u5728\u7ebf\u6570\u636e\u7ec4\u88c5\u7684\u6280\u672f\u548c\u52b3\u52a8\u529b\u969c\u788d\uff0c\u4e3a\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u53ca\u5176\u4ed6\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u900f\u660e\u548c\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u6570\u636e\u6574\u7406\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2601.02663", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02663", "abs": "https://arxiv.org/abs/2601.02663", "authors": ["Subha Ghoshal", "Ali Al-Bustami"], "title": "When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark", "comment": null, "summary": "Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan-execute-replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\\% $\\rightarrow$ 67.5\\% for GPT-4o) while increasing latency by orders of magnitude ($\\sim$8s $\\rightarrow$ $\\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\\% at $\\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u89c4\u5212\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5728\u4e8b\u4ef6\u95ee\u7b54\u4efb\u52a1\u4e2d\u5de5\u5177\u589e\u5f3a\u80fd\u63d0\u5347\u51c6\u786e\u6027\u4f46\u5927\u5e45\u589e\u52a0\u5ef6\u8fdf\uff0c\u800c\u5728\u8bf4\u670d\u6027\u56de\u590d\u751f\u6210\u4e2d\u5355\u6b21\u63d0\u793a\u6548\u679c\u6700\u597d\uff0c\u5f3a\u8c03\u9700\u8981\u6839\u636e\u4efb\u52a1\u7279\u6027\u6743\u8861\u6a21\u578b\u5927\u5c0f\u4e0e\u5de5\u5177\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u63a8\u7406\u65f6\u89c4\u5212\u548c\u5916\u90e8\u5de5\u5177\u6765\u6539\u8fdb\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u6548\u679c\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u65b9\u9762\u7684\u6743\u8861\u3002", "method": "\u4f7f\u7528LangChain\u548cLangGraph\u6846\u67b6\uff0c\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u8fdb\u884c\u6bd4\u8f83\uff1a\u4e8b\u4ef6\u4e2d\u5fc3\u95ee\u7b54\uff08Event-QA\uff09\u548cReddit ChangeMyView\u8bf4\u670d\u6027\u56de\u590d\u751f\u6210\uff08CMV\uff09\u3002\u5bf9\u6bd4\u5355\u6b21\u63d0\u793a\u57fa\u7ebf\u4e0e\u914d\u5907\u4efb\u52a1\u7279\u5b9a\u5de5\u5177\uff08DBpedia SPARQL/\u67e5\u627e/\u6a21\u5f0f\u63a2\u7d22\u3001\u7ef4\u57fa\u767e\u79d1\u68c0\u7d22\u3001\u4e3b\u9898\u7f51\u7edc\u641c\u7d22\uff09\u7684\u89c4\u5212-\u6267\u884c-\u91cd\u65b0\u89c4\u5212\u667a\u80fd\u4f53\u3002\u8bc4\u4f30GPT-4o\u548cGPT-4o-mini\u5728\u76f8\u540c\u5de5\u4f5c\u6d41\u7a0b\u4e0b\u7684\u8868\u73b0\uff0c\u62a5\u544a\u51c6\u786e\u6027\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "result": "\u5728Event-QA\u4efb\u52a1\u4e2d\uff0c\u6700\u4f73\u5de5\u5177\u589e\u5f3a\u914d\u7f6e\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff08GPT-4o\u4ece47.5%\u63d0\u5347\u523067.5%\uff09\uff0c\u4f46\u5ef6\u8fdf\u5927\u5e45\u589e\u52a0\uff08\u4ece\u7ea68\u79d2\u589e\u52a0\u5230\u7ea6317\u79d2\uff09\u3002\u5728CMV\u4efb\u52a1\u4e2d\uff0c\u5355\u6b21\u63d0\u793a\u6548\u679c\u6700\u597d\uff08GPT-4o-mini\u8fbe\u523075%\u51c6\u786e\u7387\uff0c\u7ea66\u79d2\u5ef6\u8fdf\uff09\uff0c\u800c\u89c4\u5212+\u641c\u7d22\u5927\u5e45\u589e\u52a0\u5ef6\u8fdf\u4f46\u672a\u5e26\u6765\u4e00\u81f4\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002\u590d\u6742\u7684\u591a\u5de5\u5177\u7f16\u6392\u66b4\u9732\u4e86\u8f83\u5c0f\u6a21\u578b\u7684\u9000\u5316\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7279\u6027\u548c\u6210\u672c\u8003\u8651\uff0c\u8c28\u614e\u9009\u62e9\u6a21\u578b\u5927\u5c0f\u548c\u667a\u80fd\u4f53/\u5de5\u5177\u590d\u6742\u5ea6\u3002\u5de5\u5177\u589e\u5f3a\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u80fd\u63d0\u5347\u51c6\u786e\u6027\u4f46\u4ee3\u4ef7\u9ad8\u6602\uff0c\u800c\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u7b80\u5355\u65b9\u6cd5\u53ef\u80fd\u66f4\u4f18\uff0c\u5f3a\u8c03\u4e86\u4efb\u52a1\u7279\u5b9a\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.02669", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02669", "abs": "https://arxiv.org/abs/2601.02669", "authors": ["Hongzhan Lin", "Zixin Chen", "Zhiqi Shen", "Ziyang Luo", "Zhen Ye", "Jing Ma", "Tat-Seng Chua", "Guandong Xu"], "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking", "comment": "17 pages, 21 figures, 7 tables", "summary": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.", "AI": {"tldr": "FactArena\uff1a\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u7ade\u6280\u573a\u5f0f\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLM\u5728\u5b8c\u6574\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u58f0\u660e\u63d0\u53d6\u3001\u8bc1\u636e\u68c0\u7d22\u548c\u9a8c\u8bc1\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u9759\u6001\u58f0\u660e\u9a8c\u8bc1\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u58f0\u660e\u9a8c\u8bc1\uff0c\u5ffd\u7565\u4e86\u5b8c\u6574\u7684\u4e8b\u5b9e\u6838\u67e5\u5de5\u4f5c\u6d41\u7a0b\uff08\u5305\u62ec\u58f0\u660e\u63d0\u53d6\u548c\u8bc1\u636e\u68c0\u7d22\uff09\u3002\u8fd9\u79cd\u72ed\u7a84\u7684\u7126\u70b9\u4f7f\u5f97\u5f53\u524d\u57fa\u51c6\u65e0\u6cd5\u63ed\u793a\u73b0\u4ee3LLM\u7684\u7cfb\u7edf\u6027\u63a8\u7406\u5931\u8d25\u3001\u4e8b\u5b9e\u76f2\u70b9\u548c\u9c81\u68d2\u6027\u9650\u5236\u3002", "method": "FactArena\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) LLM\u9a71\u52a8\u7684\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\uff0c\u6807\u51c6\u5316\u58f0\u660e\u5206\u89e3\u3001\u5de5\u5177\u589e\u5f3a\u7684\u8bc1\u636e\u68c0\u7d22\u548c\u57fa\u4e8e\u7406\u7531\u7684\u88c1\u51b3\u9884\u6d4b\uff1b(2) \u7ade\u6280\u573a\u5f0f\u5224\u65ad\u673a\u5236\uff0c\u57fa\u4e8e\u7edf\u4e00\u53c2\u8003\u6307\u5357\u8fdb\u884c\u65e0\u504f\u89c1\u7684\u6210\u5bf9\u6bd4\u8f83\uff1b(3) \u7ade\u6280\u573a\u9a71\u52a8\u7684\u58f0\u660e\u6f14\u5316\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u751f\u6210\u66f4\u5177\u6311\u6218\u6027\u7684\u58f0\u660e\u4ee5\u6d4b\u8bd5LLM\u7684\u4e8b\u5b9e\u9c81\u68d2\u6027\u3002", "result": "\u572816\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff08\u6db5\u76d67\u4e2a\u6a21\u578b\u7cfb\u5217\uff09\u4e0a\uff0cFactArena\u4ea7\u751f\u4e86\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u6392\u540d\u3002\u5206\u6790\u663e\u793a\u9759\u6001\u58f0\u660e\u9a8c\u8bc1\u51c6\u786e\u6027\u4e0e\u7aef\u5230\u7aef\u4e8b\u5b9e\u6838\u67e5\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7a81\u663e\u4e86\u5168\u9762\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "FactArena\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u4fe1\u8d56\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u8bca\u65adLLM\u7684\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\uff0c\u6307\u5bfc\u672a\u6765\u6a21\u578b\u5f00\u53d1\uff0c\u5e76\u63a8\u8fdbLLM\u5728\u5b89\u5168\u5173\u952e\u7684\u4e8b\u5b9e\u6838\u67e5\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u3002"}}
{"id": "2601.02447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02447", "abs": "https://arxiv.org/abs/2601.02447", "authors": ["Bennet Kahrs", "Julia Andresen", "Fenja Falta", "Monty Santarossa", "Heinz Handels", "Timo Kepp"], "title": "Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis", "comment": "Extended journal version of the proceedings paper \"Bridging Gaps in Retinal Imaging: Fusing OCT and SLO Information with Implicit Neural Representations for Improved Interpolation and Segmentation\" from the German Conference on Medical Image Computing (BVM 2025; DOI:10.1007/978-3-658-47422-5_24). Under review for a MELBA Special Issue. Minor revision resubmitted; decision pending", "summary": "Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5404\u5411\u5f02\u6027\u7684\u89c6\u7f51\u819cOCT\u56fe\u50cf\uff0c\u5b9e\u73b0\u5bc6\u96c63D\u5206\u6790\u548c\u5206\u8fa8\u7387\u65e0\u5173\u7684\u89c6\u7f51\u819c\u56fe\u8c31", "motivation": "\u4e34\u5e8a\u89c6\u7f51\u819cOCT\u6210\u50cf\u901a\u5e38\u91c7\u7528\u5927\u5207\u7247\u95f4\u8ddd\uff0c\u5bfc\u81f4\u9ad8\u5ea6\u5404\u5411\u5f02\u6027\u7684\u56fe\u50cf\u548c\u7a00\u758f\u626b\u63cf\u3002\u73b0\u67092D\u65b9\u6cd5\u5b58\u5728\u76f8\u90bbB\u626b\u63cf\u7ed3\u679c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e14\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u5206\u8fa8\u7387\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u6210\u50cf\u534f\u8bae", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eINR\u7684\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u6574\u5408en-face\u6a21\u6001\u4fe1\u606f\u8fdb\u884cB\u626b\u63cf\u95f4\u63d2\u503c\uff0c\u4fdd\u7559\u7ed3\u6784\u4fe1\u606f\uff1b2\uff09\u521b\u5efa\u5206\u8fa8\u7387\u65e0\u5173\u7684\u89c6\u7f51\u819c\u56fe\u8c31\uff0c\u5b9e\u73b0\u901a\u7528\u5206\u6790\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u5229\u7528\u53ef\u6cdb\u5316\u7684INR\uff0c\u901a\u8fc7\u7fa4\u4f53\u8bad\u7ec3\u6539\u8fdb\u89c6\u7f51\u819c\u5f62\u72b6\u8868\u793a", "result": "\u5f00\u53d1\u51fa\u5206\u8fa8\u7387\u72ec\u7acb\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u5927B\u626b\u63cf\u95f4\u8ddd\u7684OCT\u56fe\u50cf\uff0c\u4e3a\u89c6\u7f51\u819c\u7ed3\u6784\u548c\u75c5\u7406\u7684\u5bb9\u79ef\u8bc4\u4f30\u5f00\u8f9f\u53ef\u80fd\u6027", "conclusion": "\u57fa\u4e8eINR\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5404\u5411\u5f02\u6027OCT\u6570\u636e\uff0c\u5b9e\u73b0\u5bc6\u96c63D\u5206\u6790\uff0c\u4e3a\u4e34\u5e8a\u7a00\u758f\u626b\u63cf\u89c6\u7f51\u819c\u56fe\u50cf\u7684\u5bb9\u79ef\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177"}}
{"id": "2601.02671", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02671", "abs": "https://arxiv.org/abs/2601.02671", "authors": ["Ahmed Ahmed", "A. Feder Cooper", "Sanmi Koyejo", "Percy Liang"], "title": "Extracting books from production language models", "comment": "We ran experiments from mid-August to mid-September 2025, notified affected providers shortly after, and now make our findings public after a 90-day disclosure window", "summary": "Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u751f\u4ea7\u7ea7LLMs\uff08\u5982Claude\u3001GPT-4\u3001Gemini\u3001Grok\uff09\u4ecd\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u98ce\u9669\uff0c\u5373\u4f7f\u6709\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u90e8\u5206\u6a21\u578b\u53ef\u63d0\u53d6\u5927\u91cf\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6587\u672c\u5185\u5bb9\u3002", "motivation": "\u89e3\u51b3\u5173\u4e8eLLMs\u662f\u5426\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4ee5\u53ca\u80fd\u5426\u63d0\u53d6\u53d7\u7248\u6743\u5185\u5bb9\u7684\u6cd5\u5f8b\u4e89\u8bae\uff0c\u7279\u522b\u5173\u6ce8\u751f\u4ea7\u7ea7LLMs\u5728\u5b89\u5168\u63aa\u65bd\u4e0b\u7684\u6570\u636e\u63d0\u53d6\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u521d\u59cb\u63a2\u6d4b\uff08\u6709\u65f6\u4f7f\u7528Best-of-N\u8d8a\u72f1\uff09\u6d4b\u8bd5\u63d0\u53d6\u53ef\u884c\u6027\uff1b2) \u8fed\u4ee3\u8fde\u7eed\u63d0\u793a\u5c1d\u8bd5\u63d0\u53d6\u4e66\u7c4d\u5185\u5bb9\u3002\u4f7f\u7528\u57fa\u4e8e\u5757\u7684\u6700\u957f\u516c\u5171\u5b50\u4e32\u8fd1\u4f3c\uff08nv-recall\uff09\u8bc4\u4f30\u63d0\u53d6\u6210\u529f\u7387\u3002", "result": "\u4e0d\u540cLLMs\u8868\u73b0\u5404\u5f02\uff1aGemini 2.5 Pro\u548cGrok 3\u65e0\u9700\u8d8a\u72f1\u5373\u53ef\u63d0\u53d6\u5927\u91cf\u6587\u672c\uff08nv-recall\u8fbe76.8%\u548c70.3%\uff09\uff1bClaude 3.7 Sonnet\u8d8a\u72f1\u540e\u53ef\u8fd1\u4e4e\u9010\u5b57\u8f93\u51fa\u6574\u672c\u4e66\uff08nv-recall=95.8%\uff09\uff1bGPT-4.1\u9700\u8981\u66f4\u591a\u8d8a\u72f1\u5c1d\u8bd5\u4e14\u6700\u7ec8\u62d2\u7edd\u7ee7\u7eed\uff08nv-recall=4.0%\uff09\u3002", "conclusion": "\u5373\u4f7f\u6709\u6a21\u578b\u548c\u7cfb\u7edf\u7ea7\u5b89\u5168\u9632\u62a4\uff0c\u751f\u4ea7\u7ea7LLMs\u4ecd\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u98ce\u9669\uff0c\u8fd9\u5bf9\u7248\u6743\u4fdd\u62a4\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.02609", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.02609", "abs": "https://arxiv.org/abs/2601.02609", "authors": ["Arjun S. Nair"], "title": "Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth", "comment": "61 pages, 25 figures, open-source framework available at https://github.com/Ajwebdevs/Chronicals and pip install chronicals", "summary": "Large language model fine-tuning is bottlenecked by memory: a 7B parameter model requires 84GB--14GB for weights, 14GB for gradients, and 56GB for FP32 optimizer states--exceeding even A100-40GB capacity. We present Chronicals, an open-source training framework achieving 3.51x speedup over Unsloth through four synergistic optimizations: (1) fused Triton kernels eliminating 75% of memory traffic via RMSNorm (7x), SwiGLU (5x), and QK-RoPE (2.3x) fusion; (2) Cut Cross-Entropy reducing logit memory from 5GB to 135MB through online softmax computation; (3) LoRA+ with theoretically-derived 16x differential learning rates between adapter matrices; and (4) Best-Fit Decreasing sequence packing recovering 60-75% of compute wasted on padding.\n  On Qwen2.5-0.5B with A100-40GB, Chronicals achieves 41,184 tokens/second for full fine-tuning versus Unsloth's 11,736 tokens/second (3.51x). For LoRA at rank 32, we reach 11,699 tokens/second versus Unsloth MAX's 2,857 tokens/second (4.10x). Critically, we discovered that Unsloth's reported 46,000 tokens/second benchmark exhibited zero gradient norms--the model was not training.\n  We provide complete mathematical foundations: online softmax correctness proofs, FlashAttention IO complexity bounds O(N^2 d^2 M^{-1}), LoRA+ learning rate derivations from gradient magnitude analysis, and bin-packing approximation guarantees. All implementations, benchmarks, and proofs are available at https://github.com/Ajwebdevs/Chronicals with pip installation via https://pypi.org/project/chronicals/.", "AI": {"tldr": "Chronicals\u662f\u4e00\u4e2a\u5f00\u6e90\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u79cd\u4f18\u5316\u6280\u672f\u5b9e\u73b03.51\u500d\u52a0\u901f\uff1a\u878d\u5408Triton\u5185\u6838\u51cf\u5c1175%\u5185\u5b58\u6d41\u91cf\u3001Cut Cross-Entropy\u5c06logit\u5185\u5b58\u4ece5GB\u964d\u81f3135MB\u3001LoRA+\u4f7f\u752816\u500d\u5dee\u5f02\u5b66\u4e60\u7387\u3001\u5e8f\u5217\u6253\u5305\u6062\u590d60-75%\u8ba1\u7b97\u6d6a\u8d39\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u53d7\u9650\u4e8e\u5185\u5b58\u74f6\u9888\uff1a7B\u53c2\u6570\u6a21\u578b\u9700\u898184GB\u5185\u5b58\uff08\u6743\u91cd14GB\u3001\u68af\u5ea614GB\u3001\u4f18\u5316\u5668\u72b6\u600156GB\uff09\uff0c\u8d85\u8fc7A100-40GB\u7684\u5bb9\u91cf\u3002\u73b0\u6709\u8bad\u7ec3\u6846\u67b6\u5982Unsloth\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u751a\u81f3\u5b58\u5728\u68af\u5ea6\u4e3a\u96f6\u7684\u865a\u5047\u8bad\u7ec3\u60c5\u51b5\u3002", "method": "1) \u878d\u5408Triton\u5185\u6838\uff1a\u901a\u8fc7RMSNorm(7x)\u3001SwiGLU(5x)\u3001QK-RoPE(2.3x)\u878d\u5408\u51cf\u5c1175%\u5185\u5b58\u6d41\u91cf\uff1b2) Cut Cross-Entropy\uff1a\u901a\u8fc7\u5728\u7ebfsoftmax\u8ba1\u7b97\u5c06logit\u5185\u5b58\u4ece5GB\u964d\u81f3135MB\uff1b3) LoRA+\uff1a\u7406\u8bba\u63a8\u5bfc\u51fa\u9002\u914d\u5668\u77e9\u9635\u95f416\u500d\u5dee\u5f02\u5b66\u4e60\u7387\uff1b4) Best-Fit Decreasing\u5e8f\u5217\u6253\u5305\uff1a\u6062\u590d60-75%\u56e0\u586b\u5145\u6d6a\u8d39\u7684\u8ba1\u7b97\u3002", "result": "\u5728Qwen2.5-0.5B\u548cA100-40GB\u4e0a\uff0cChronicals\u5168\u5fae\u8c03\u8fbe\u523041,184 tokens/\u79d2\uff0c\u6bd4Unsloth\u768411,736 tokens/\u79d2\u5feb3.51\u500d\uff1bLoRA rank 32\u8fbe\u523011,699 tokens/\u79d2\uff0c\u6bd4Unsloth MAX\u76842,857 tokens/\u79d2\u5feb4.10\u500d\u3002\u53d1\u73b0Unsloth\u62a5\u544a\u768446,000 tokens/\u79d2\u57fa\u51c6\u6d4b\u8bd5\u68af\u5ea6\u4e3a\u96f6\uff0c\u6a21\u578b\u672a\u771f\u6b63\u8bad\u7ec3\u3002", "conclusion": "Chronicals\u901a\u8fc7\u56db\u79cd\u534f\u540c\u4f18\u5316\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u6548\u7387\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u6570\u5b66\u57fa\u7840\u8bc1\u660e\uff0c\u5305\u62ec\u5728\u7ebfsoftmax\u6b63\u786e\u6027\u8bc1\u660e\u3001FlashAttention IO\u590d\u6742\u5ea6\u8fb9\u754c\u3001LoRA+\u5b66\u4e60\u7387\u63a8\u5bfc\u548c\u88c5\u7bb1\u95ee\u9898\u8fd1\u4f3c\u4fdd\u8bc1\u3002\u6240\u6709\u5b9e\u73b0\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc1\u660e\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.02777", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02777", "abs": "https://arxiv.org/abs/2601.02777", "authors": ["Jingcheng Cao", "Chaoran Xiong", "Jianmin Song", "Shang Yan", "Jiachen Liu", "Ling Pei"], "title": "M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination", "comment": "6 pages, 7 figures", "summary": "Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments.", "AI": {"tldr": "M-SEVIQ\u662f\u4e00\u4e2a\u591a\u6ce2\u6bb5\u7acb\u4f53\u4e8b\u4ef6\u89c6\u89c9\u548c\u60ef\u6027\u56db\u8db3\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u654f\u6377\u8fd0\u52a8\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u6311\u6218", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u76f8\u673a\u5728\u56db\u8db3\u673a\u5668\u4eba\u654f\u6377\u8fd0\u52a8\u4e2d\u5bb9\u6613\u4ea7\u751f\u6a21\u7cca\u56fe\u50cf\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u5bbd\u52a8\u6001\u8303\u56f4\u7684\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\u5728\u7acb\u4f53\u914d\u7f6e\u548c\u591a\u6ce2\u6bb5\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u4f7f\u7528Unitree Go2\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a\u3001\u5e27\u5f0f\u76f8\u673a\u3001IMU\u548c\u5173\u8282\u7f16\u7801\u5668\uff0c\u6536\u96c6\u4e86\u8d85\u8fc730\u4e2a\u771f\u5b9e\u4e16\u754c\u5e8f\u5217\uff0c\u6db5\u76d6\u4e0d\u540c\u901f\u5ea6\u6c34\u5e73\u3001\u7167\u660e\u6ce2\u957f\u548c\u5149\u7167\u6761\u4ef6\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u6821\u51c6\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86M-SEVIQ\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6ce2\u6bb5\u7acb\u4f53\u4e8b\u4ef6\u89c6\u89c9\u548c\u60ef\u6027\u6570\u636e\uff0c\u652f\u6301\u4f20\u611f\u5668\u878d\u5408\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u654f\u6377\u673a\u5668\u4eba\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u8d44\u6e90\u3002", "conclusion": "M-SEVIQ\u6570\u636e\u96c6\u80fd\u591f\u652f\u6301\u654f\u6377\u673a\u5668\u4eba\u611f\u77e5\u3001\u4f20\u611f\u5668\u878d\u5408\u3001\u8bed\u4e49\u5206\u5272\u548c\u591a\u6a21\u6001\u89c6\u89c9\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.02536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02536", "abs": "https://arxiv.org/abs/2601.02536", "authors": ["Shaden Shaar", "Bradon Thymes", "Sirawut Chaixanien", "Claire Cardie", "Bharath Hariharan"], "title": "MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark", "comment": null, "summary": "Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary \"facts\" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MovieRecapsQA\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7535\u5f71\u89e3\u8bf4\u89c6\u9891\u7684\u5f00\u6e90\u591a\u6a21\u6001\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b\u7ea68.2K\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u652f\u6301\u65e0\u53c2\u8003\u8bc4\u4f30\uff0c\u7528\u4e8e\u6d4b\u8bd5\u6a21\u578b\u6574\u5408\u89c6\u89c9\u548c\u5bf9\u8bdd\u7ebf\u7d22\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VideoQA\u57fa\u51c6\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u89c6\u9891\uff08\u5982\u7535\u5f71\uff09\u6240\u9700\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5927\u591a\u4e0d\u662f\u5f00\u653e\u5f0f\u95ee\u7b54\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u56de\u7b54\u590d\u6742\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528YouTube\u7535\u5f71\u89e3\u8bf4\u89c6\u9891\u521b\u5efa\u57fa\u51c6\uff0c\u901a\u8fc7\u89e3\u8bf4\u6458\u8981\u751f\u6210\u7ea68.2K\u4e2a\u4e0e\u7535\u5f71\u5b57\u5e55\u5bf9\u9f50\u7684QA\u5bf9\uff0c\u63d0\u4f9b\u9a8c\u8bc1\u7b54\u6848\u6240\u9700\u7684\"\u4e8b\u5b9e\"\u4fe1\u606f\uff0c\u652f\u6301\u65e0\u53c2\u8003\u8bc4\u4f30\u3002\u57fa\u51c6\u5305\u542b\u4e0d\u540c\u957f\u5ea6\u89c6\u9891\u7247\u6bb5\u548c\u6309\u6a21\u6001/\u7c7b\u578b\u5206\u7c7b\u7684\u95ee\u9898\u3002", "result": "\u8bc4\u4f30\u4e867\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u53d1\u73b0\uff1a1\uff09\u7eaf\u89c6\u89c9\u95ee\u9898\u6700\u5177\u6311\u6218\u6027\uff1b2\uff09\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u6587\u672c\u8f93\u5165\uff1b3\uff09\u4ece\u89c6\u9891\u5185\u5bb9\u63d0\u53d6\u51c6\u786e\u4e8b\u5b9e\u4fe1\u606f\u4ecd\u7136\u56f0\u96be\uff1b4\uff09\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u5728\u89c6\u9891\u4f9d\u8d56\u95ee\u9898\u4e0a\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "MovieRecapsQA\u662f\u9996\u4e2a\u63d0\u4f9b\u663e\u5f0f\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u5f00\u6e90VideoQA\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2601.02818", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.02818", "abs": "https://arxiv.org/abs/2601.02818", "authors": ["Muzhen Zhang", "Yujie Cheng", "Zhanxiang Lei"], "title": "Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs", "comment": "22 pages, 7 figures", "summary": "Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u589e\u5f3a\u7684\u957f\u77ed\u671f\u8bb0\u5fc6\u6ce8\u610f\u529b\u6a21\u578b(QLSTMA)\uff0c\u9996\u6b21\u5c06\u53d8\u5206\u91cf\u5b50\u7535\u8def\u878d\u5165\u5faa\u73af\u5355\u5143\uff0c\u663e\u8457\u63d0\u5347\u50a8\u5c42\u6e17\u900f\u7387\u9884\u6d4b\u7cbe\u5ea6\uff0c8-qubit\u72ec\u7acb\u95e8\u7ed3\u6784\u6bd4\u4f20\u7edfLSTMA\u964d\u4f4eMAE 19%\u3001RMSE 20%\u3002", "motivation": "\u50a8\u5c42\u53c2\u6570\uff08\u7279\u522b\u662f\u6e17\u900f\u7387\uff09\u7684\u7a7a\u95f4\u9884\u6d4b\u5bf9\u6cb9\u6c14\u52d8\u63a2\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6e17\u900f\u7387\u53d8\u5316\u8303\u56f4\u5927\u3001\u53d8\u5f02\u6027\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u53ef\u9760\u9884\u6d4b\u3002\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u7ea0\u7f20\u548c\u53e0\u52a0\u539f\u7406\u6709\u671b\u63d0\u5347\u590d\u6742\u5730\u8d28\u53c2\u6570\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51faQLSTMA\u6a21\u578b\uff0c\u5c06\u53d8\u5206\u91cf\u5b50\u7535\u8def(VQC)\u96c6\u6210\u5230\u5faa\u73af\u5355\u5143\u4e2d\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u91cf\u5b50\u5316\u7ed3\u6784\uff1a\u5171\u4eab\u95e8\u7ed3\u6784(QLSTMA-SG)\u548c\u72ec\u7acb\u95e8\u7ed3\u6784(QLSTMA-IG)\uff0c\u7814\u7a76\u91cf\u5b50\u7ed3\u6784\u914d\u7f6e\u548c\u91cf\u5b50\u6bd4\u7279\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "8-qubit QLSTMA-IG\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edfLSTMA\uff0cMAE\u964d\u4f4e19%\uff0cRMSE\u964d\u4f4e20%\uff0c\u5728\u590d\u6742\u6d4b\u4e95\u6570\u636e\u533a\u57df\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002\u589e\u52a0\u91cf\u5b50\u6bd4\u7279\u6570\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u5c3d\u7ba1\u76ee\u524d\u4f9d\u8d56\u7ecf\u5178\u6a21\u62df\u3002", "conclusion": "\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u5728\u50a8\u5c42\u9884\u6d4b\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5728\u771f\u5b9e\u91cf\u5b50\u786c\u4ef6\u4e0a\u90e8\u7f72\u6b64\u7c7b\u6a21\u578b\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u53ef\u6269\u5c55\u5230\u77f3\u6cb9\u5de5\u7a0b\u548c\u5730\u7403\u79d1\u5b66\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2601.02799", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02799", "abs": "https://arxiv.org/abs/2601.02799", "authors": ["Seunghwan Jang", "SooJean Han"], "title": "Stratified Hazard Sampling: Minimal-Variance Event Scheduling for CTMC/DTMC Discrete Diffusion and Flow Models", "comment": "Work in progress. Feedback welcome", "summary": "CTMC/DTMC-based discrete generative models, including uniform-noise discrete diffusion (e.g., D3PM/CTDD) and discrete flow matching, enable non-autoregressive sequence generation by repeatedly replacing tokens through a time-inhomogeneous Markov process. Inference is typically implemented with step-based simulation: each token decides to jump via independent Bernoulli (or categorical) draws at every discretization step. Under uniform-noise initialization, where self-correction requires multiple edits per position, these independent decisions induce substantial variance in both the number and timing of edits, leading to characteristic failure modes such as under-editing (residual noise) or over-editing (cascading unnecessary substitutions), decreasing reproducibility.\n  We propose Stratified Hazard Sampling (SHS), a drop-in and hyperparameter-free inference principle for any sampler that admits a stay-vs.-replace decomposition. SHS models per-token edits as events driven by cumulative hazard (CTMC) or cumulative jump mass (DTMC) and places events by stratifying this cumulative quantity: with a single random phase per position, a token jumps whenever its accumulated hazard crosses unit-spaced thresholds. This preserves the expected number of jumps while achieving the minimum possible variance among unbiased integer estimators (bounded by 1/4), without altering per-jump destination sampling and thus retaining multimodality. We also introduce a phase-allocation variant for blacklist-style lexical constraints that prioritizes early edits at high-risk positions to mitigate late-masking artifacts.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u5371\u9669\u91c7\u6837(SHS)\uff0c\u4e00\u79cd\u7528\u4e8e\u79bb\u6563\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7d2f\u79ef\u5371\u9669\u503c\u6765\u51cf\u5c11\u7f16\u8f91\u65b9\u5dee\uff0c\u89e3\u51b3\u4f20\u7edf\u72ec\u7acb\u4f2f\u52aa\u5229\u91c7\u6837\u5bfc\u81f4\u7684\u6b20\u7f16\u8f91\u548c\u8fc7\u7f16\u8f91\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eCTMC/DTMC\u7684\u79bb\u6563\u751f\u6210\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u72ec\u7acb\u4f2f\u52aa\u5229\u91c7\u6837\u51b3\u5b9atoken\u662f\u5426\u8df3\u8f6c\uff0c\u5bfc\u81f4\u7f16\u8f91\u6b21\u6570\u548c\u65f6\u95f4\u65b9\u5dee\u8fc7\u5927\uff0c\u4ea7\u751f\u6b20\u7f16\u8f91\uff08\u6b8b\u7559\u566a\u58f0\uff09\u548c\u8fc7\u7f16\u8f91\uff08\u4e0d\u5fc5\u8981\u66ff\u6362\uff09\u7b49\u5931\u8d25\u6a21\u5f0f\uff0c\u964d\u4f4e\u53ef\u91cd\u590d\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5371\u9669\u91c7\u6837(SHS)\uff1a\u5c06\u6bcf\u4e2atoken\u7684\u7f16\u8f91\u5efa\u6a21\u4e3a\u7d2f\u79ef\u5371\u9669\uff08CTMC\uff09\u6216\u7d2f\u79ef\u8df3\u8f6c\u8d28\u91cf\uff08DTMC\uff09\u9a71\u52a8\u7684\u4e8b\u4ef6\uff0c\u901a\u8fc7\u5206\u5c42\u7d2f\u79ef\u91cf\u653e\u7f6e\u7f16\u8f91\u4e8b\u4ef6\u2014\u2014\u6bcf\u4e2a\u4f4d\u7f6e\u4f7f\u7528\u5355\u4e00\u968f\u673a\u76f8\u4f4d\uff0c\u5f53\u7d2f\u79ef\u5371\u9669\u8de8\u8d8a\u5355\u4f4d\u95f4\u9694\u9608\u503c\u65f6token\u8df3\u8f6c\u3002\u8fd8\u63d0\u51fa\u76f8\u4f4d\u5206\u914d\u53d8\u4f53\u7528\u4e8e\u9ed1\u540d\u5355\u5f0f\u8bcd\u6c47\u7ea6\u675f\uff0c\u4f18\u5148\u5728\u9ad8\u98ce\u9669\u4f4d\u7f6e\u65e9\u671f\u7f16\u8f91\u4ee5\u51cf\u8f7b\u540e\u671f\u63a9\u7801\u4f2a\u5f71\u3002", "result": "SHS\u5728\u4fdd\u6301\u671f\u671b\u8df3\u8f6c\u6b21\u6570\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u65e0\u504f\u6574\u6570\u4f30\u8ba1\u5668\u7684\u6700\u5c0f\u53ef\u80fd\u65b9\u5dee\uff08\u6709\u754c\u4e8e1/4\uff09\uff0c\u4e0d\u6539\u53d8\u6bcf\u6b21\u8df3\u8f6c\u7684\u76ee\u6807\u91c7\u6837\u4ece\u800c\u4fdd\u7559\u591a\u6a21\u6001\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u3001\u65e0\u9700\u8d85\u53c2\u6570\u7684\u63a8\u7406\u539f\u5219\u5e94\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u505c\u7559vs\u66ff\u6362\u5206\u89e3\u7684\u91c7\u6837\u5668\u3002", "conclusion": "\u5206\u5c42\u5371\u9669\u91c7\u6837(SHS)\u901a\u8fc7\u51cf\u5c11\u7f16\u8f91\u65b9\u5dee\u663e\u8457\u6539\u5584\u4e86\u79bb\u6563\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u72ec\u7acb\u91c7\u6837\u65b9\u6cd5\u7684\u56fa\u6709\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6a21\u6001\u6027\u548c\u671f\u671b\u7f16\u8f91\u6b21\u6570\uff0c\u4e3a\u7ea6\u675f\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u53d8\u4f53\u3002"}}
{"id": "2601.03055", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03055", "abs": "https://arxiv.org/abs/2601.03055", "authors": ["Shiying Dong", "Zhipeng Shen", "Rudolf Reiter", "Hailong Huang", "Bingzhao Gao", "Hong Chen", "Wen-Hua Chen"], "title": "A Fast Semidefinite Convex Relaxation for Optimal Control Problems With Spatio-Temporal Constraints", "comment": "9 pages, 6 figures", "summary": "Solving optimal control problems (OCPs) of autonomous agents operating under spatial and temporal constraints fast and accurately is essential in applications ranging from eco-driving of autonomous vehicles to quadrotor navigation. However, the nonlinear programs approximating the OCPs are inherently nonconvex due to the coupling between the dynamics and the event timing, and therefore, they are challenging to solve. Most approaches address this challenge by predefining waypoint times or just using nonconvex trajectory optimization, which simplifies the problem but often yields suboptimal solutions. To significantly improve the numerical properties, we propose a formulation with a time-scaling direct multiple shooting scheme that partitions the prediction horizon into segments aligned with characteristic time constraints. Moreover, we develop a fast semidefinite-programming-based convex relaxation that exploits the sparsity pattern of the lifted formulation. Comprehensive simulation studies demonstrate the solution optimality and computational efficiency. Furthermore, real-world experiments on a quadrotor waypoint flight task with constrained open time windows validate the practical applicability of the approach in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65f6\u95f4\u7f29\u653e\u76f4\u63a5\u591a\u91cd\u5c04\u51fb\u65b9\u6848\u548c\u5feb\u901f\u534a\u5b9a\u89c4\u5212\u51f8\u677e\u5f1b\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u65f6\u7a7a\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u663e\u8457\u6539\u5584\u6570\u503c\u7279\u6027\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u65f6\u7a7a\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u8282\u80fd\u9a7e\u9a76\u3001\u56db\u65cb\u7ffc\u5bfc\u822a\uff09\u901a\u5e38\u901a\u8fc7\u975e\u7ebf\u6027\u89c4\u5212\u8fd1\u4f3c\uff0c\u4f46\u7531\u4e8e\u52a8\u529b\u5b66\u4e0e\u4e8b\u4ef6\u65f6\u5e8f\u7684\u8026\u5408\u5bfc\u81f4\u975e\u51f8\u6027\uff0c\u6c42\u89e3\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9884\u8bbe\u822a\u70b9\u65f6\u95f4\u6216\u4f7f\u7528\u975e\u51f8\u8f68\u8ff9\u4f18\u5316\uff0c\u8fd9\u7b80\u5316\u4e86\u95ee\u9898\u4f46\u5f80\u5f80\u5f97\u5230\u6b21\u4f18\u89e3\u3002", "method": "1. \u63d0\u51fa\u65f6\u95f4\u7f29\u653e\u76f4\u63a5\u591a\u91cd\u5c04\u51fb\u65b9\u6848\uff0c\u5c06\u9884\u6d4b\u65f6\u57df\u5212\u5206\u4e3a\u4e0e\u7279\u5f81\u65f6\u95f4\u7ea6\u675f\u5bf9\u9f50\u7684\u6bb5\uff1b2. \u5f00\u53d1\u57fa\u4e8e\u534a\u5b9a\u89c4\u5212\u7684\u5feb\u901f\u51f8\u677e\u5f1b\u65b9\u6cd5\uff0c\u5229\u7528\u63d0\u5347\u516c\u5f0f\u7684\u7a00\u758f\u6a21\u5f0f\u3002", "result": "\u7efc\u5408\u4eff\u771f\u7814\u7a76\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89e3\u7684\u6700\u4f18\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u5728\u56db\u65cb\u7ffc\u822a\u70b9\u98de\u884c\u4efb\u52a1\uff08\u5177\u6709\u7ea6\u675f\u5f00\u653e\u65f6\u95f4\u7a97\u53e3\uff09\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65f6\u95f4\u7f29\u653e\u76f4\u63a5\u591a\u91cd\u5c04\u51fb\u65b9\u6848\u548c\u534a\u5b9a\u89c4\u5212\u51f8\u677e\u5f1b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5177\u6709\u65f6\u7a7a\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6700\u4f18\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63a7\u5236\u3002"}}
{"id": "2601.02884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02884", "abs": "https://arxiv.org/abs/2601.02884", "authors": ["Hana Yahia", "Bruno Figliuzzi", "Florent Di Meglio", "Laurent Gerbaud", "Stephane Menand", "Mohamed Mahjoub"], "title": "Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction", "comment": null, "summary": "This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u94bb\u4e95\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u6280\u672f\uff0c\u7528\u4e8e\u9884\u6d4b\u8fde\u7eed\u7684\u7c98\u6ed1\u6307\u6570\uff08SSI\uff09\u3002ADG\u548cIRM\u6a21\u578b\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u534710%\u548c8%\uff0c\u5176\u4e2dADG\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4e25\u91cd\u4e8b\u4ef6\u68c0\u6d4b\u7387\u4ece20%\u63d0\u5347\u81f360%\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u4e0d\u540c\u94bb\u4e95\u4e95\u573a\u95f4\u6cdb\u5316\u7684\u9c81\u68d2\u56de\u5f52\u6a21\u578b\uff0c\u9884\u6d4b\u5173\u952e\u7684\u7c98\u6ed1\u6307\u6570\uff08SSI\uff09\uff0c\u4ee5\u8bc4\u4f30\u94bb\u5934\u7684\u626d\u8f6c\u632f\u52a8\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u65b0\u4e95\u573a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u752860\u79d2\u6807\u8bb0\u76841Hz\u5730\u9762\u94bb\u4e95\u6570\u636e\u5e8f\u5217\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\uff0c\u91c7\u7528\u7f51\u683c\u641c\u7d22\u4f18\u5316\u8d85\u53c2\u6570\u3002\u6bd4\u8f83\u4e86\u5bf9\u6297\u9886\u57df\u6cdb\u5316\uff08ADG\uff09\u3001\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\uff08IRM\uff09\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u7684\u6548\u679c\u3002\u6a21\u578b\u5728\u8bad\u7ec3\u4e95\u573a\u4ee5\u5916\u7684\u4e95\u573a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "ADG\u548cIRM\u6a21\u578b\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u534710%\u548c8%\u3002\u4e25\u91cd\u4e8b\u4ef6\u68c0\u6d4b\u7387\u4ece\u57fa\u7ebf\u6a21\u578b\u768420%\u5927\u5e45\u63d0\u5347\u81f360%\u3002ADG\u6a21\u578b\u7565\u4f18\u4e8eIRM\u6a21\u578b\uff0c\u8fc1\u79fb\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u5728\u94bb\u4e95\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0cADG\u662f\u6700\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u65b0\u4e95\u573a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e25\u91cd\u4e8b\u4ef6\u68c0\u6d4b\u80fd\u529b\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2601.02997", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02997", "abs": "https://arxiv.org/abs/2601.02997", "authors": ["Waleed Khalid", "Dmitry Ignatov", "Radu Timofte"], "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures", "comment": null, "summary": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.", "AI": {"tldr": "LLM\u901a\u8fc7\u95ed\u73af\u5408\u6210\u6846\u67b6\u81ea\u4e3b\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ecf\u8fc722\u8f6e\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u6027\u80fd\u3001\u7ed3\u6784\u65b0\u9896\u7684\u5377\u79ef\u7f51\u7edc\uff0c\u9a8c\u8bc1\u4e86LLM\u53ef\u4ee5\u901a\u8fc7\u6267\u884c\u53cd\u9988\u5b66\u4e60\u7ecf\u9a8c\u6027\u67b6\u6784\u5148\u9a8c\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u7a0b\u5e8f\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u65b9\u9762\u7684\u81ea\u4e3b\u80fd\u529b\u2014\u2014\u5e73\u8861\u8bed\u6cd5\u53ef\u9760\u6027\u3001\u6027\u80fd\u548c\u7ed3\u6784\u65b0\u9896\u6027\u2014\u2014\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22LLM\u662f\u5426\u80fd\u591f\u901a\u8fc7\u6267\u884c\u53cd\u9988\u5b66\u4e60\u7ecf\u9a8c\u6027\u67b6\u6784\u5148\u9a8c\uff0c\u8d85\u8d8a\u8bad\u7ec3\u6570\u636e\u9650\u5236\u3002", "method": "\u5c06\u4ee3\u7801\u5bfc\u5411\u7684LLM\u7f6e\u4e8e\u95ed\u73af\u5408\u6210\u6846\u67b6\u4e2d\uff0c\u7ecf\u8fc722\u8f6e\u76d1\u7763\u5fae\u8c03\u5faa\u73af\u3002\u6a21\u578b\u751f\u6210PyTorch\u5377\u79ef\u7f51\u7edc\uff0c\u901a\u8fc7\u9a8c\u8bc1\u3001\u4f4e\u4fdd\u771f\u6027\u80fd\u4fe1\u53f7\uff08\u5355\u8f6e\u51c6\u786e\u7387\uff09\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528MinHash-Jaccard\u51c6\u5219\u8fc7\u6ee4\u7ed3\u6784\u5197\u4f59\u3002\u9ad8\u6027\u80fd\u65b0\u9896\u67b6\u6784\u88ab\u8f6c\u6362\u4e3a\u63d0\u793a-\u4ee3\u7801\u5bf9\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684LoRA\u9002\u914d\u8fdb\u884c\u8fed\u4ee3\u5fae\u8c03\uff0c\u4eceLEMUR\u6570\u636e\u96c6\u521d\u59cb\u5316\u3002", "result": "\u6a21\u578b\u5185\u5316\u4e86\u7ecf\u9a8c\u6027\u67b6\u6784\u5148\u9a8c\uff0c\u6210\u4e3a\u5f3a\u5927\u7684\u751f\u6210\u5668\u3002\u6709\u6548\u751f\u6210\u7387\u7a33\u5b9a\u572850.6%\uff08\u5cf0\u503c74.5%\uff09\uff0c\u5e73\u5747\u9996\u8f6e\u51c6\u786e\u7387\u4ece28.06%\u63d0\u5347\u81f350.99%\uff0c\u8d85\u8fc740%\u51c6\u786e\u7387\u7684\u5019\u9009\u6bd4\u4f8b\u4ece2.04%\u589e\u957f\u523096.81%\u3002\u6a21\u578b\u751f\u6210\u4e86455\u4e2a\u539f\u59cb\u8bed\u6599\u4e2d\u4e0d\u5b58\u5728\u7684\u9ad8\u6027\u80fd\u67b6\u6784\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4ee3\u7801\u5408\u6210\u4e0e\u6267\u884c\u53cd\u9988\u76f8\u7ed3\u5408\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u968f\u673a\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u81ea\u4e3b\u3001\u6027\u80fd\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u5668\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u84dd\u56fe\uff0c\u8bc1\u660eLLM\u80fd\u591f\u5185\u5316\u7ecf\u9a8c\u6027\u3001\u975e\u6587\u672c\u5956\u52b1\u6765\u8d85\u8d8a\u5176\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2601.02908", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02908", "abs": "https://arxiv.org/abs/2601.02908", "authors": ["Wei-Yuan Cheng", "Kai-Po Chang", "Chi-Pin Huang", "Fu-En Yang", "Yu-Chiang Frank Wang"], "title": "TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors", "comment": "8 pages for main paper (exclude citation pages), 6 pages for appendix, totally 10 figures 7 tables and 2 algorithms. The paper is accepted by WACV 2026", "summary": "Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.", "AI": {"tldr": "\u63d0\u51faTA-Prompting\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u951a\u70b9\u589e\u5f3aVideoLLMs\uff0c\u63d0\u9ad8\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4e2d\u4e8b\u4ef6\u8fb9\u754c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u4e8b\u4ef6\u8fde\u8d2f\u6027\u91c7\u6837\u7b56\u7565\u4f18\u5316\u591a\u4e8b\u4ef6\u63cf\u8ff0\u751f\u6210\u3002", "motivation": "\u73b0\u6709VideoLLMs\u5728\u672a\u4fee\u526a\u89c6\u9891\u4e2d\u96be\u4ee5\u51c6\u786e\u5b9a\u4f4d\u4e8b\u4ef6\u8fb9\u754c\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u63cf\u8ff0\u7f3a\u4e4f\u65f6\u95f4\u57fa\u7840\u3002\u9700\u8981\u6539\u8fdb\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51faTA-Prompting\u65b9\u6cd5\uff1a1) \u65f6\u95f4\u951a\u70b9\u5b66\u4e60\u7cbe\u786e\u5b9a\u4f4d\u4e8b\u4ef6\u8fb9\u754c\uff1b2) \u63d0\u793aVideoLLMs\u8fdb\u884c\u65f6\u95f4\u611f\u77e5\u7684\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\uff1b3) \u63a8\u7406\u65f6\u5f15\u5165\u4e8b\u4ef6\u8fde\u8d2f\u6027\u91c7\u6837\u7b56\u7565\uff0c\u9009\u62e9\u8de8\u65f6\u95f4\u4e8b\u4ef6\u8fde\u8d2f\u4e14\u4e0e\u89c6\u9891\u8de8\u6a21\u6001\u76f8\u4f3c\u7684\u4e8b\u4ef6\u63cf\u8ff0\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTA-Prompting\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684VideoLLMs\uff0c\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u3001\u65f6\u523b\u68c0\u7d22\u548cTemporalQA\u7b49\u65f6\u95f4\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "TA-Prompting\u901a\u8fc7\u65f6\u95f4\u951a\u70b9\u548c\u4e8b\u4ef6\u8fde\u8d2f\u6027\u91c7\u6837\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86VideoLLMs\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4e2d\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u548c\u63cf\u8ff0\u8d28\u91cf\uff0c\u4e3a\u89c6\u9891\u4e8b\u4ef6\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.02918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02918", "abs": "https://arxiv.org/abs/2601.02918", "authors": ["Guoqiang Liang", "Jianyi Wang", "Zhonghua Wu", "Shangchen Zhou"], "title": "Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning", "comment": "Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage", "summary": "Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.", "AI": {"tldr": "Zoom-IQA\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u884c\u4e3a\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u3001\u533a\u57df\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\uff09\u6765\u540c\u65f6\u751f\u6210\u8d28\u91cf\u63cf\u8ff0\u548c\u5206\u6570\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u53ea\u9884\u6d4b\u6570\u503c\u5206\u6570\u800c\u4e0d\u63d0\u4f9b\u89e3\u91ca\uff0c\u8981\u4e48\u63d0\u4f9b\u4f4e\u5c42\u6b21\u63cf\u8ff0\u4f46\u7f3a\u4e4f\u7cbe\u786e\u5206\u6570\u3002\u867d\u7136\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u540c\u65f6\u751f\u6210\u63cf\u8ff0\u548c\u5206\u6570\uff0c\u4f46\u73b0\u6709VLM\u65b9\u6cd5\u5728\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faZoom-IQA\u6a21\u578b\uff0c\u6a21\u62df\u4e09\u4e2a\u5173\u952e\u8ba4\u77e5\u884c\u4e3a\uff1a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u3001\u533a\u57df\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a1) \u5728Grounded-Rationale-IQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u8ba9\u6a21\u578b\u5c06\u8bc4\u4f30\u57fa\u4e8e\u5173\u952e\u533a\u57df\uff1b2) \u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u52a8\u6001\u7b56\u7565\u63a2\u7d22\uff0c\u4f7f\u7528KL-Coverage\u6b63\u5219\u5316\u9632\u6b62\u63a8\u7406\u548c\u8bc4\u5206\u591a\u6837\u6027\u5d29\u6e83\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u91cd\u91c7\u6837\u7b56\u7565\u7f13\u89e3\u6807\u6ce8\u504f\u5dee\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eZoom-IQA\u5728\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u6709\u63d0\u5347\u3002\u5728\u56fe\u50cf\u4fee\u590d\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86Zoom-IQA\u7684\u6709\u6548\u6027\u3002", "conclusion": "Zoom-IQA\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u63a8\u7406\u548c\u66f4\u5168\u9762\u7684\u8d28\u91cf\u5206\u6790\u3002"}}
{"id": "2601.02957", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02957", "abs": "https://arxiv.org/abs/2601.02957", "authors": ["Fabian Lukassen", "Christoph Weisser", "Michael Schlee", "Manish Kumar", "Anton Thielmann", "Benjamin Saefken", "Thomas Kneib"], "title": "LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation", "comment": null, "summary": "This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u96c6\u6210\u7edf\u8ba1\u65b9\u6cd5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u53d8\u70b9\u68c0\u6d4b\u6846\u67b6\uff0c\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u5355\u4e2a\u68c0\u6d4b\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u7279\u5f81\u4e0b\u8868\u73b0\u4e92\u8865\uff0c\u65b9\u6cd5\u9009\u62e9\u56f0\u96be\u4e14\u5bb9\u6613\u5f97\u5230\u6b21\u4f18\u7ed3\u679c\uff1b2) \u7f3a\u4e4f\u5bf9\u68c0\u6d4b\u5230\u7684\u53d8\u5316\u7684\u81ea\u52a8\u5316\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u89e3\u91ca", "method": "\u96c6\u6210\u5341\u79cd\u4e0d\u540c\u7684\u53d8\u70b9\u68c0\u6d4b\u7b97\u6cd5\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u89e3\u91ca\u7ba1\u9053\u81ea\u52a8\u751f\u6210\u4e0a\u4e0b\u6587\u53d9\u4e8b\uff0c\u5bf9\u4e8e\u79c1\u6709\u6216\u9886\u57df\u7279\u5b9a\u6570\u636e\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u65b9\u6848", "result": "\u76f8\u6bd4\u5355\u4e2a\u65b9\u6cd5\uff0c\u96c6\u6210\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5c06\u68c0\u6d4b\u5230\u7684\u53d8\u70b9\u4e0e\u6f5c\u5728\u7684\u73b0\u5b9e\u4e16\u754c\u5386\u53f2\u4e8b\u4ef6\u8054\u7cfb\u8d77\u6765", "conclusion": "\u5f00\u6e90Python\u6846\u67b6\u5728\u91d1\u878d\u3001\u653f\u6cbb\u79d1\u5b66\u548c\u73af\u5883\u79d1\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u5c55\u793a\u4e86\u5b9e\u7528\u4ef7\u503c\uff0c\u5c06\u539f\u59cb\u7edf\u8ba1\u8f93\u51fa\u8f6c\u5316\u4e3a\u5206\u6790\u5e08\u548c\u51b3\u7b56\u8005\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3"}}
{"id": "2601.03085", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03085", "abs": "https://arxiv.org/abs/2601.03085", "authors": ["Mahsa Raeiszadeh", "Amin Ebrahimzadeh", "Roch H. Glitho", "Johan Eker", "Raquel A. F. Mini"], "title": "Real-Time Adaptive Anomaly Detection in Industrial IoT Environments", "comment": null, "summary": "To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in today's industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5de5\u4e1a\u7269\u8054\u7f51\u6d41\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u6e90\u9884\u6d4b\u6a21\u578b\u548c\u6982\u5ff5\u6f02\u79fb\u9002\u5e94\u6280\u672f\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u7f51\u7edc\u9700\u8981\u81ea\u52a8\u5316\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u6765\u5904\u7406\u591a\u7ef4\u5f02\u6784\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u5de5\u4e1a\u7269\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u5bf9\u4e8e\u9884\u9632\u6545\u969c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5de5\u4e1a\u7269\u8054\u7f51\u591a\u7ef4\u6570\u636e\u6d41\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5c06\u9884\u6d4b\u6a21\u578b\u4e0e\u65b0\u9896\u7684\u6982\u5ff5\u6f02\u79fb\u9002\u5e94\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5904\u7406\u5de5\u4e1a\u7269\u8054\u7f51\u6d41\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u6e90\u9884\u6d4b\u6a21\u578b\u6765\u5e94\u5bf9\u6570\u636e\u590d\u6742\u6027\uff0c\u5e76\u901a\u8fc7\u6f02\u79fb\u9002\u5e94\u673a\u5236\u5904\u7406\u6570\u636e\u52a8\u6001\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u8f68\u8ff9\u9a71\u52a8\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728AUC\u6307\u6807\u4e0a\u8fbe\u523089.71%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u540c\u65f6\u6ee1\u8db3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u8981\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u7269\u8054\u7f51\u6d41\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u7ef4\u5f02\u6784\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u53d8\u5316\u3002"}}
{"id": "2601.03149", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03149", "abs": "https://arxiv.org/abs/2601.03149", "authors": ["Dehao Yuan", "Tyler Farnan", "Stefan Tesliuc", "Doron L Bergman", "Yulun Wu", "Xiaoyu Liu", "Minghui Liu", "James Montgomery", "Nam H Nguyen", "C. Bayan Bruss", "Furong Huang"], "title": "PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback", "comment": null, "summary": "Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware \"nextprompt\" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.", "AI": {"tldr": "PersonaLedger\uff1a\u57fa\u4e8eLLM\u548c\u89c4\u5219\u5f15\u64ce\u7684\u5408\u6210\u91d1\u878d\u4ea4\u6613\u6570\u636e\u751f\u6210\u7cfb\u7edf\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u884c\u4e3a\u591a\u6837\u6027\u548c\u903b\u8f91\u6b63\u786e\u6027", "motivation": "\u4e25\u683c\u7684\u9690\u79c1\u6cd5\u89c4\u9650\u5236\u4e86\u771f\u5b9e\u4ea4\u6613\u6570\u636e\u7684\u83b7\u53d6\uff0c\u963b\u788d\u4e86\u91d1\u878dAI\u7684\u5f00\u653e\u7814\u7a76\u3002\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u884c\u4e3a\u591a\u6837\u6027\u548c\u903b\u8f91\u6b63\u786e\u6027\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u7f3a\u4e4f\u4eba\u7c7b\u884c\u4e3a\u7684\u4e30\u5bcc\u6027\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u5982GAN\uff09\u7ecf\u5e38\u8fdd\u53cd\u91d1\u878d\u7ea6\u675f\u4e14\u4ecd\u9700\u79c1\u6709\u6570\u636e\u8bad\u7ec3\u3002", "method": "\u63d0\u51faPersonaLedger\u751f\u6210\u5f15\u64ce\uff1a1\uff09\u4f7f\u7528\u57fa\u4e8e\u4e30\u5bcc\u7528\u6237\u89d2\u8272\u7684\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u4ea4\u6613\u6d41\uff1b2\uff09\u7ed3\u5408\u4e13\u5bb6\u53ef\u914d\u7f6e\u7684\u7a0b\u5e8f\u5316\u5f15\u64ce\u786e\u4fdd\u6b63\u786e\u6027\u3002LLM\u548c\u5f15\u64ce\u5f62\u6210\u95ed\u73af\u4ea4\u4e92\uff1a\u6bcf\u6b21\u4e8b\u4ef6\u540e\uff0c\u5f15\u64ce\u66f4\u65b0\u7528\u6237\u72b6\u6001\u3001\u5f3a\u5236\u6267\u884c\u91d1\u878d\u89c4\u5219\uff0c\u5e76\u8fd4\u56de\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\"nextprompt\"\u5f15\u5bfcLLM\u751f\u6210\u53ef\u884c\u7684\u4e0b\u4e00\u6b65\u884c\u52a8\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b23,000\u4e2a\u7528\u6237\u76843,000\u4e07\u7b14\u4ea4\u6613\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u5305\u542b\u6d41\u52a8\u6027\u4e0d\u8db3\u5206\u7c7b\u548c\u8eab\u4efd\u76d7\u7a83\u5206\u5272\u4e24\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002PersonaLedger\u63d0\u4f9b\u4e86\u73b0\u5b9e\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u8d44\u6e90\uff0c\u652f\u6301\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u4e25\u683c\u8bc4\u4f30\u3002", "conclusion": "PersonaLedger\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u4e30\u5bcc\u3001\u73b0\u5b9e\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u8d44\u6e90\uff08\u5305\u542b\u4ee3\u7801\u3001\u89c4\u5219\u548c\u751f\u6210\u65e5\u5fd7\uff09\uff0c\u52a0\u901f\u91d1\u878dAI\u521b\u65b0\u5e76\u652f\u6301\u4e25\u683c\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u3002"}}
{"id": "2601.03166", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03166", "abs": "https://arxiv.org/abs/2601.03166", "authors": ["Daphne Theodorakopoulos", "Marcel Wever", "Marius Lindauer"], "title": "Dynamic Hyperparameter Importance for Efficient Multi-Objective Optimization", "comment": "Submitted to IJCAI 2026", "summary": "Choosing a suitable ML model is a complex task that can depend on several objectives, e.g., accuracy, model size, fairness, inference time, or energy consumption. In practice, this requires trading off multiple, often competing, objectives through multi-objective optimization (MOO). However, existing MOO methods typically treat all hyperparameters as equally important, overlooking that hyperparameter importance (HPI) can vary significantly depending on the trade-off between objectives. We propose a novel dynamic optimization approach that prioritizes the most influential hyperparameters based on varying objective trade-offs during the search process, which accelerates empirical convergence and leads to better solutions. Building on prior work on HPI for MOO post-analysis, we now integrate HPI, calculated with HyperSHAP, into the optimization. For this, we leverage the objective weightings naturally produced by the MOO algorithm ParEGO and adapt the configuration space by fixing the unimportant hyperparameters, allowing the search to focus on the important ones. Eventually, we validate our method with diverse tasks from PyMOO and YAHPO-Gym. Empirical results demonstrate improvements in convergence speed and Pareto front quality compared to baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u4f18\u5316\u65b9\u6cd5\uff0c\u6839\u636e\u76ee\u6807\u6743\u8861\u52a8\u6001\u8c03\u6574\u8d85\u53c2\u6570\u91cd\u8981\u6027\uff0c\u52a0\u901f\u6536\u655b\u5e76\u63d0\u5347\u5e15\u7d2f\u6258\u524d\u6cbf\u8d28\u91cf", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u5c06\u6240\u6709\u8d85\u53c2\u6570\u89c6\u4e3a\u540c\u7b49\u91cd\u8981\uff0c\u5ffd\u7565\u4e86\u8d85\u53c2\u6570\u91cd\u8981\u6027\u4f1a\u968f\u76ee\u6807\u6743\u8861\u53d8\u5316\u800c\u663e\u8457\u4e0d\u540c\uff0c\u8fd9\u9650\u5236\u4e86\u4f18\u5316\u6548\u7387", "method": "\u96c6\u6210HyperSHAP\u8ba1\u7b97\u8d85\u53c2\u6570\u91cd\u8981\u6027\uff0c\u5229\u7528ParEGO\u7b97\u6cd5\u4ea7\u751f\u7684\u76ee\u6807\u6743\u91cd\u52a8\u6001\u8c03\u6574\u914d\u7f6e\u7a7a\u95f4\uff0c\u56fa\u5b9a\u4e0d\u91cd\u8981\u8d85\u53c2\u6570\u4ee5\u805a\u7126\u91cd\u8981\u8d85\u53c2\u6570\u641c\u7d22", "result": "\u5728PyMOO\u548cYAHPO-Gym\u7684\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u5e15\u7d2f\u6258\u524d\u6cbf\u8d28\u91cf\u65b9\u9762\u5747\u6709\u6539\u8fdb", "conclusion": "\u52a8\u6001\u8003\u8651\u8d85\u53c2\u6570\u91cd\u8981\u6027\u7684\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u52a0\u901f\u591a\u76ee\u6807\u4f18\u5316\u6536\u655b\u5e76\u4ea7\u751f\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.03051", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03051", "abs": "https://arxiv.org/abs/2601.03051", "authors": ["Vidhi Rathore", "Sambu Aneesh", "Himanshu Singh"], "title": "Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation", "comment": null, "summary": "Hallucinations can be produced by conversational AI systems, particularly in multi-turn conversations where context changes and contradictions may eventually surface. By representing the entire conversation as a temporal graph, we present a novel graph-based method for detecting dialogue-level hallucinations. Our framework models each dialogue as a node, encoding it using a sentence transformer. We explore two different ways of connectivity: i) shared-entity edges, which connect turns that refer to the same entities; ii) temporal edges, which connect contiguous turns in the conversation. Message-passing is used to update the node embeddings, allowing flow of information between related nodes. The context-aware node embeddings are then combined using attention pooling into a single vector, which is then passed on to a classifier to determine the presence and type of hallucinations. We demonstrate that our method offers slightly improved performance over existing methods. Further, we show the attention mechanism can be used to justify the decision making process. The code and model weights are made available at: https://github.com/sambuaneesh/anlp-project.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u8bdd\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c06\u5bf9\u8bdd\u5efa\u6a21\u4e3a\u65f6\u5e8f\u56fe\uff0c\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u548c\u6ce8\u610f\u529b\u6c60\u5316\u8bc6\u522b\u5e7b\u89c9", "motivation": "\u5bf9\u8bddAI\u7cfb\u7edf\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u7279\u522b\u662f\u5728\u4e0a\u4e0b\u6587\u53d8\u5316\u548c\u51fa\u73b0\u77db\u76fe\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u5bf9\u8bdd\u7ea7\u522b\u7684\u5e7b\u89c9\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u5efa\u6a21\u5bf9\u8bdd\u7ed3\u6784\u548c\u65f6\u5e8f\u5173\u7cfb\u3002", "method": "\u5c06\u6574\u4e2a\u5bf9\u8bdd\u8868\u793a\u4e3a\u65f6\u5e8f\u56fe\uff0c\u6bcf\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u4f5c\u4e3a\u8282\u70b9\uff0c\u4f7f\u7528\u53e5\u5b50\u7f16\u7801\u5668\u7f16\u7801\u3002\u6784\u5efa\u4e24\u79cd\u8fb9\u8fde\u63a5\uff1a1) \u5171\u4eab\u5b9e\u4f53\u8fb9 - \u8fde\u63a5\u5f15\u7528\u76f8\u540c\u5b9e\u4f53\u7684\u8f6e\u6b21\uff1b2) \u65f6\u5e8f\u8fb9 - \u8fde\u63a5\u8fde\u7eed\u5bf9\u8bdd\u8f6e\u6b21\u3002\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u66f4\u65b0\u8282\u70b9\u5d4c\u5165\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u6c60\u5316\u5c06\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8282\u70b9\u5d4c\u5165\u7ec4\u5408\u4e3a\u5355\u4e2a\u5411\u91cf\uff0c\u6700\u540e\u901a\u8fc7\u5206\u7c7b\u5668\u68c0\u6d4b\u5e7b\u89c9\u7c7b\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u8f7b\u5fae\u6027\u80fd\u63d0\u5347\uff0c\u6ce8\u610f\u529b\u673a\u5236\u53ef\u7528\u4e8e\u89e3\u91ca\u51b3\u7b56\u8fc7\u7a0b\u3002\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5df2\u5f00\u6e90\u3002", "conclusion": "\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u5bf9\u8bdd\u7ed3\u6784\u548c\u65f6\u5e8f\u5173\u7cfb\uff0c\u4e3a\u5bf9\u8bdd\u7ea7\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4e14\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.03237", "categories": ["cs.LG", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.03237", "abs": "https://arxiv.org/abs/2601.03237", "authors": ["Javier Salazar Cavazos"], "title": "PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters", "comment": null, "summary": "Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.", "AI": {"tldr": "PET-TURTLE\u6539\u8fdb\u4e86TURTLE\u6df1\u5ea6\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5e42\u5f8b\u5148\u9a8c\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u5206\u5e03\uff0c\u5e76\u52a0\u5165\u7a00\u758flogits\u7b80\u5316\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u6570\u636e\u96c6\u7684\u805a\u7c7b\u7cbe\u5ea6\u3002", "motivation": "TURTLE\u7b97\u6cd5\u5047\u8bbe\u6570\u636e\u7c07\u662f\u5e73\u8861\u7684\uff0c\u4f46\u5728\u5b9e\u9645\u4e0d\u5e73\u8861\u6570\u636e\u5206\u5e03\u4e0b\u4f1a\u4ea7\u751f\u4e0d\u7406\u60f3\u7684\u8d85\u5e73\u9762\uff0c\u5bfc\u81f4\u66f4\u9ad8\u7684\u805a\u7c7b\u9519\u8bef\u3002\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u3002", "method": "1) \u901a\u8fc7\u5e42\u5f8b\u5148\u9a8c\u63a8\u5e7f\u6210\u672c\u51fd\u6570\u4ee5\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u5206\u5e03\uff1b2) \u5728\u6807\u7b7e\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7a00\u758flogits\uff0c\u4f18\u5316\u66f4\u7b80\u5355\u7684\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPET-TURTLE\u63d0\u9ad8\u4e86\u4e0d\u5e73\u8861\u6570\u636e\u6e90\u7684\u51c6\u786e\u6027\uff0c\u9632\u6b62\u5bf9\u5c11\u6570\u7c07\u7684\u8fc7\u5ea6\u9884\u6d4b\uff0c\u5e76\u589e\u5f3a\u4e86\u6574\u4f53\u805a\u7c7b\u6027\u80fd\u3002", "conclusion": "PET-TURTLE\u6210\u529f\u89e3\u51b3\u4e86TURTLE\u5728\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5e42\u5f8b\u5148\u9a8c\u548c\u7a00\u758flogits\u6539\u8fdb\uff0c\u5728\u5e73\u8861\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u90fd\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u805a\u7c7b\u6027\u80fd\u3002"}}
