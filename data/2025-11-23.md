<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EvoVLA: Self-Evolving Vision-Language-Action Model](https://arxiv.org/abs/2511.16166)
*Zeting Liu,Zida Yang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: EvoVLA是一个自监督的视觉语言动作模型框架，通过阶段对齐奖励、基于姿态的对象探索和长时程记忆三个组件，解决了长时程机器人操作中的阶段幻觉问题，在模拟和真实环境中都取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在长时程机器人操作中存在阶段幻觉问题，即智能体利用粗略评估信号来走捷径完成多步任务，报告高进度但未真正完成任务。

Method: 1. 阶段对齐奖励(SAR)：使用三元组对比学习和Gemini生成的困难负样本来防止视觉捷径；2. 基于姿态的对象探索(POE)：基于相对对象-夹爪姿态而非原始像素来建立好奇心；3. 长时程记忆：使用选择性上下文保留和门控融合来稳定扩展部署过程中的内在塑造。

Result: 在Discoverse-L长时程操作基准测试中，EvoVLA比最强基线(OpenVLA-OFT)平均任务成功率提高10.2个百分点，达到69.2%。样本效率提高1.5倍，阶段幻觉从38.5%降低到14.8%。真实世界部署在物理机器人上达到54.6%的平均成功率，比OpenVLA-OFT高11个百分点。

Conclusion: EvoVLA有效解决了VLA模型中的阶段幻觉问题，实现了从模拟到真实世界的有效迁移和强泛化能力，在长时程机器人操作任务中表现出色。

Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

</details>


### [2] [Graph Neural Networks for Surgical Scene Segmentation](https://arxiv.org/abs/2511.16430)
*Yihan Li,Nikhil Churamani,Maria Robu,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 提出两种结合Vision Transformer和Graph Neural Networks的图分割模型，用于腹腔镜胆囊切除术中的肝脏解剖结构分割，在遮挡、长距离依赖和精细几何结构识别方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在手术场景分析中难以处理遮挡、长距离依赖和罕见结构的精细几何特征，需要增强空间和语义理解能力。

Method: 提出两种分割模型：1) 静态k-NN图结合GCNII实现稳定的长距离信息传播；2) 动态可微分图生成器结合GAT支持自适应拓扑学习。在Endoscapes-Seg50和CholecSeg8k基准上评估。

Result: 相比最先进基线，mIoU提升7-8%，mDice提升6%，在薄壁、罕见和安全关键结构上产生解剖学一致的预测。

Conclusion: 图分割方法提升了手术场景分割的性能和解剖一致性，结合ViT的全局上下文和图的关系推理，提高了模型的可解释性和可靠性，为更安全的腹腔镜和机器人辅助手术铺平道路。

Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.

</details>


### [3] [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669)
*Junhao Cheng,Liang Hou,Xin Tao,Jing Liao*

Main category: cs.CV

TL;DR: 本文提出了Video-Next-Event Prediction (VNEP)任务，将传统文本回答的下一事件预测扩展为视频回答，并开发了VANS模型，通过强化学习对齐视觉语言模型和视频扩散模型，在程序性和预测性任务上取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 视频具有展示物理世界信息的能力，而语言模型在视频生成方面主要局限于娱乐应用。作者发现将视频作为下一事件预测的新回答形式，能够提供更直观和定制化的答案，用于程序学习和创造性探索。

Method: 引入VANS模型，使用提出的Joint-GRPO强化学习框架，协调视觉语言模型(VLM)和视频扩散模型(VDM)协同工作。通过共享奖励机制优化VLM生成准确且易于可视化的描述，同时指导VDM生成忠实于描述和输入视觉上下文的视频。

Result: 在程序性和预测性基准测试中，VANS在视频事件预测和可视化方面都达到了最先进的性能水平。

Conclusion: VNEP任务为视频生成开辟了新的应用领域，VANS模型通过强化学习成功解决了多模态理解、指令条件推理和视觉语义一致性等挑战，证明了视频作为回答形式的潜力。

Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty](https://arxiv.org/abs/2511.16225)
*Victor Croisfelt,João Henrique Inacio de Souza,Shashi Raj Pandey,Beatriz Soret,Petar Popovski*

Main category: cs.LG

TL;DR: 提出了一种神经启发的非阻塞推理范式，使用自适应时间整合窗口来动态适应异构数据流的随机延迟模式，无需参考模态要求，在通信延迟下实现鲁棒的实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有非阻塞推理方法依赖参考模态范式，需要一个模态输入完全接收后才能处理，且需要昂贵的离线分析，无法有效应对跨数据流的不确定通信延迟。

Method: 采用自适应时间整合窗口(TWIs)动态调整异构数据流的随机延迟模式，放松参考模态要求，构建通信延迟感知框架。

Result: 在音频-视觉事件定位任务上的实验表明，相比现有方法，该方法对网络动态具有更优越的适应性。

Conclusion: 所提出的神经启发非阻塞推理范式能够实现更精细的精度-延迟权衡控制，在通信延迟下提供鲁棒的实时推理性能。

Abstract: Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.

</details>


### [5] [Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning](https://arxiv.org/abs/2511.16333)
*Mohammad Areeb Qazi,Maryam Nadeem,Mohammad Yaqub*

Main category: cs.LG

TL;DR: 本文综述了医疗领域的世界模型，这些模型学习预测动态以实现多步推演、反事实评估和规划。文章调查了医学影像、电子健康记录疾病进展建模和机器人手术三个领域的近期工作，并提出了L1-L4能力评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型缺乏物理基础和时序推理能力，无法满足临床决策支持需求。世界模型能够学习多模态、时序一致且动作条件的表征，反映医疗护理的物理和因果结构。

Method: 通过文献综述方法，分析医疗世界模型在三个领域的应用：(1)医学影像和诊断，(2)电子健康记录疾病进展建模，(3)机器人手术和手术规划。提出L1-L4能力评估框架。

Result: 大多数系统达到L1-L2能力（时序预测和动作条件预测），较少系统实现L3（反事实推演决策支持），极少数达到L4（规划/控制）。识别出限制临床可靠性的关键差距。

Conclusion: 提出了将生成主干网络（Transformer、扩散模型、VAE）与因果/机械基础整合的研究议程，以开发临床稳健的预测优先世界模型，为医疗安全决策支持提供基础。

Abstract: Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.

</details>


### [6] [Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin](https://arxiv.org/abs/2511.16523)
*Ming-Lun Lee,Fu-Shiang Yang,Cheng-Kuan Lin,Yan-Ann Chen,Chih-Yu Lin,Yu-Chee Tseng*

Main category: cs.LG

TL;DR: 提出了首个专门用于动态客户端参与联邦学习(DPFL)的开源基准框架，揭示了动态参与导致的性能显著下降，并提出了KPFL解决方案来缓解知识损失和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究大多假设客户端持续参与，忽略了实际中客户端动态加入或离开的动态参与场景，且缺乏专门针对DPFL挑战的系统性基准框架。

Method: 开发了支持可配置数据分布、参与模式和评估指标的DPFL基准框架，并提出了KPFL方法，通过维护共享知识池、双年龄和数据偏差加权以及生成知识蒸馏来应对动态参与挑战。

Result: 基准测试显示动态参与导致FL模型性能显著下降，KPFL能有效提高模型鲁棒性和泛化能力。

Conclusion: 动态参与对联邦学习性能有重大影响，KPFL提供了一种有效的通用解决方案来应对这一挑战。

Abstract: Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.

</details>


### [7] [Toward Valid Generative Clinical Trial Data with Survival Endpoints](https://arxiv.org/abs/2511.16551)
*Perrine Chassat,Van Tuan Nguyen,Lucas Ducrot,Emilie Lanoy,Agathe Guilloux*

Main category: cs.LG

TL;DR: 提出了一种变分自编码器方法，用于生成包含混合类型协变量和生存结果的合成控制臂，解决了传统GAN方法在数据稀缺、不稳定性和独立删失假设方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 临床试验面临患者群体碎片化、入组缓慢和成本不可持续的问题，特别是在肿瘤学和罕见病领域。现有的外部控制臂方法依赖真实世界数据，而生成式AI提供了合成控制臂的替代方案，但时间-事件结果的生成在删失和小样本情况下具有挑战性。

Method: 使用变分自编码器(VAE)在统一的潜变量框架中联合生成混合类型协变量和生存结果，不假设独立删失。

Result: 在合成和真实试验数据集上的评估显示，该方法在保真度、实用性和隐私指标上优于GAN基线方法，但揭示了I类错误和功效的系统性误校准问题。

Conclusion: 提出了一种后生成选择程序来改进校准，突显了生成生存建模的进展和开放挑战。

Abstract: Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation](https://arxiv.org/abs/2511.15825)
*Tuan-Anh Le,Anh Mai Vu,David Yang,Akash Awasthi,Hien Van Nguyen*

Main category: cs.AI

TL;DR: IMACT-CXR是一个交互式多代理对话导师，通过整合空间标注、视线分析、知识检索和图像推理来帮助学员解读胸部X光片，使用AutoGen工作流实现实时教学反馈。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时处理学员的边界框标注、视线样本和自由文本观察的智能导师系统，提升胸部X光片解读的培训效果，为放射科住院医师提供个性化教学。

Method: 基于AutoGen的多代理工作流，包含定位质量评估、苏格拉底式辅导、PubMed证据检索、相似病例推荐和视觉语言推理等专业代理，结合贝叶斯知识追踪和肺叶分割模块。

Result: 系统展示了响应式教学流程、可控的答案泄露机制和可扩展性，初步评估显示在定位和诊断推理方面优于基线方法。

Conclusion: IMACT-CXR成功整合了多种AI技术，为胸部X光片解读培训提供了有效的交互式教学解决方案，具备临床部署潜力。

Abstract: IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.

</details>


### [9] [FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos](https://arxiv.org/abs/2511.16183)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.AI

TL;DR: 本文介绍了FOOTPASS数据集，这是首个用于足球比赛全场逐场动作识别的基准数据集，结合了计算机视觉输出和足球战术知识，旨在实现更自动化和可靠的逐场数据提取。


<details>
  <summary>Details</summary>
Motivation: 当前足球视频理解方法在构建可靠的逐场数据方面仍不足，通常只能辅助而非完全自动化标注。同时战术建模、轨迹预测等研究需要基于比赛状态和逐场数据，这促使利用战术知识作为先验来支持基于计算机视觉的预测。

Method: 引入FOOTPASS数据集，支持在多模态、多智能体战术背景下进行球员中心的动作识别，结合计算机视觉任务输出（如跟踪、识别）和足球战术规律知识。

Result: 创建了首个足球比赛全场逐场动作识别基准数据集，为开发能够生成可靠逐场数据流的方法提供了基础。

Conclusion: FOOTPASS数据集为数据驱动的体育分析提供了重要输入，通过结合计算机视觉和战术知识，能够实现更自动化和可靠的足球比赛逐场数据分析。

Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.

</details>


### [10] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: MedBayes-Lite是一个轻量级贝叶斯增强框架，用于transformer临床语言模型，通过不确定性量化提高预测可靠性，无需重新训练或架构修改。


<details>
  <summary>Details</summary>
Motivation: transformer模型在临床决策支持中容易过度自信，特别是在模糊的医疗案例中，需要校准的不确定性来确保可靠性。

Method: 集成三个组件：贝叶斯嵌入校准（使用蒙特卡洛dropout）、不确定性加权注意力（对token可靠性进行边际化）、置信度引导决策塑造（受临床风险最小化启发）。

Result: 在生物医学QA和临床预测基准测试中，持续改善校准和可信度，减少过度自信32-48%，在模拟临床环境中可防止高达41%的诊断错误。

Conclusion: 该框架能有效实现可靠的不确定性传播，提高医疗AI系统的可解释性。

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [11] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: SEG框架通过将支持结构优化集成到3D生成流程中，直接在模型生成阶段减少支撑材料使用，显著降低3D打印的材料浪费和生产时间。


<details>
  <summary>Details</summary>
Motivation: 当前3D打印切片技术主要关注后处理优化，而非在模型生成阶段解决支撑结构效率问题，导致材料浪费和打印时间增加。

Method: 提出SEG框架，将带偏移的直接偏好优化(ODPO)集成到3D生成流程中，在训练过程中融入支撑结构模拟，鼓励生成需要较少支撑的几何形状。

Result: 在Thingi10k-Val和GPT-3DP-Val数据集上的实验表明，SEG在支撑体积减少和可打印性方面显著优于TRELLIS、DPO和DRO等基线模型。

Conclusion: SEG通过在生成过程中直接优化模型，为更可持续和高效的数字制造实践铺平了道路，同时保持对输入提示的高保真度。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>
