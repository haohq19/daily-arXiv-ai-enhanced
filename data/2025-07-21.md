<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: 论文提出了一种新的大规模RGB-事件数据集EvReID，并提出了TriPro-ReID框架，用于提升行人重识别的特征学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有事件相机数据集规模小、模拟性强的局限性，评估真实场景下的行人重识别性能。

Method: 构建EvReID数据集，并提出基于行人属性的对比学习框架TriPro-ReID。

Result: 在EvReID和MARS数据集上的实验验证了框架的有效性。

Conclusion: EvReID数据集和TriPro-ReID框架为未来研究提供了数据和基准支持。

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [2] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: Diffusion-FSCIL利用冻结的文本到图像扩散模型解决小样本类增量学习问题，通过多尺度特征提取和潜在重放提升性能。


<details>
  <summary>Details</summary>
Motivation: 小样本类增量学习（FSCIL）面临数据稀缺和灾难性遗忘的挑战，需要一种高效且灵活的方法。

Method: 采用冻结的扩散模型作为骨干，提取多尺度特征并结合潜在重放与特征蒸馏，减少生成偏差。

Result: 在CUB-200、miniImageNet和CIFAR-100上表现优于现有方法，有效平衡新旧类别的学习。

Conclusion: Diffusion-FSCIL通过生成模型的能力和高效设计，为FSCIL提供了可行的解决方案。

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [3] [Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis](https://arxiv.org/abs/2507.13753)
*Tongtong Su,Chengyu Wang,Bingyan Liu,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: EVS是一种无需训练的封装视频合成器，结合T2I和T2V模型，提升生成视频的视觉质量和运动平滑性。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型在生成视频时存在画面闪烁和伪影问题，难以同时保证高质量图像和有效运动表现。

Method: 利用预训练的T2I模型优化低质量视频帧，并通过T2V模型确保运动一致性，将T2V的时序先验封装到T2I生成过程中。

Result: 实验证明EVS在视觉质量和运动平滑性上优于现有方法，推理速度提升1.6x-4.5x。

Conclusion: EVS通过结合T2I和T2V模型的优势，显著提升了视频生成的质量和效率。

Abstract: In recent years, large text-to-video (T2V) synthesis models have garnered
considerable attention for their abilities to generate videos from textual
descriptions. However, achieving both high imaging quality and effective motion
representation remains a significant challenge for these T2V models. Existing
approaches often adapt pre-trained text-to-image (T2I) models to refine video
frames, leading to issues such as flickering and artifacts due to
inconsistencies across frames. In this paper, we introduce EVS, a training-free
Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both
visual fidelity and motion smoothness of generated videos. Our approach
utilizes a well-trained diffusion-based T2I model to refine low-quality video
frames by treating them as out-of-distribution samples, effectively optimizing
them with noising and denoising steps. Meanwhile, we employ T2V backbones to
ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior
into the T2I generation process, EVS successfully leverages the strengths of
both types of models, resulting in videos of improved imaging and motion
quality. Experimental results validate the effectiveness of our approach
compared to previous approaches. Our composition process also leads to a
significant improvement of 1.6x-4.5x speedup in inference time. Source codes:
https://github.com/Tonniia/EVS.

</details>


### [4] [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
*Pu Jian,Donglei Yu,Wen Yang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: 论文提出ClearVQA基准，解决视觉问答中用户模糊问题的交互式澄清问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过重述问题解决模糊性，忽略了用户反馈的交互性。

Method: 引入ClearVQA基准，针对三类常见模糊问题，涵盖多种VQA场景。

Result: ClearVQA基准填补了评估交互式澄清能力的空白。

Conclusion: ClearVQA为VLM交互式澄清能力的研究提供了新方向。

Abstract: In visual question answering (VQA) context, users often pose ambiguous
questions to visual language models (VLMs) due to varying expression habits.
Existing research addresses such ambiguities primarily by rephrasing questions.
These approaches neglect the inherently interactive nature of user interactions
with VLMs, where ambiguities can be clarified through user feedback. However,
research on interactive clarification faces two major challenges: (1)
Benchmarks are absent to assess VLMs' capacity for resolving ambiguities
through interaction; (2) VLMs are trained to prefer answering rather than
asking, preventing them from seeking clarification. To overcome these
challenges, we introduce \textbf{ClearVQA} benchmark, which targets three
common categories of ambiguity in VQA context, and encompasses various VQA
scenarios.

</details>


### [5] [DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization](https://arxiv.org/abs/2507.13934)
*Marzieh Gheisari,Auguste Genovesio*

Main category: cs.CV

TL;DR: DiViD是一个端到端的视频扩散框架，用于显式分离静态外观和动态运动，解决了现有方法中的信息泄漏和模糊重建问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE和GAN的方法在视频中分离静态和动态内容时存在信息泄漏和模糊重建问题，需要更有效的解决方案。

Method: DiViD通过序列编码器提取全局静态标记和逐帧动态标记，结合条件DDPM解码器，利用共享噪声计划、时间变化的KL瓶颈和交叉注意力机制，以及正交正则化器。

Result: DiViD在真实基准测试中表现优于现有方法，实现了最高的交换联合准确率，保持了静态保真度并改善了动态传递，减少了交叉泄漏。

Conclusion: DiViD首次实现了端到端的静态-动态显式分解，为视频内容分离提供了高效且性能优越的解决方案。

Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video
remains a fundamental challenge, often hindered by information leakage and
blurry reconstructions in existing VAE- and GAN-based approaches. We introduce
DiViD, the first end-to-end video diffusion framework for explicit
static-dynamic factorization. DiViD's sequence encoder extracts a global static
token from the first frame and per-frame dynamic tokens, explicitly removing
static content from the motion code. Its conditional DDPM decoder incorporates
three key inductive biases: a shared-noise schedule for temporal consistency, a
time-varying KL-based bottleneck that tightens at early timesteps (compressing
static information) and relaxes later (enriching dynamics), and cross-attention
that routes the global static token to all frames while keeping dynamic tokens
frame-specific. An orthogonality regularizer further prevents residual
static-dynamic leakage. We evaluate DiViD on real-world benchmarks using
swap-based accuracy and cross-leakage metrics. DiViD outperforms
state-of-the-art sequential disentanglement methods: it achieves the highest
swap-based joint accuracy, preserves static fidelity while improving dynamic
transfer, and reduces average cross-leakage.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 论文提出GRU-KAN和LSTM-KAN两种新架构，用于提前三个月以上预测贷款违约，准确率超92%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有贷款违约预测模型在早期预测中准确性不足及依赖特定时间框架的问题，以帮助金融机构提前采取预防措施。

Method: 结合Kolmogorov-Arnold Networks (KAN)与GRU和LSTM网络，提出GRU-KAN和LSTM-KAN模型，并在不同特征窗口、样本量和预测间隔下评估性能。

Result: 新模型在提前三个月预测中准确率超92%，八个月超88%，显著优于基线模型（LSTM、GRU等）。

Conclusion: GRU-KAN和LSTM-KAN模型在早期贷款违约预测中表现出色，具有实际应用潜力。

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [7] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: 介绍了一种基于OctopuScheduler的多层DNN调度框架，支持从PyTorch模型到SpiNNaker2芯片的端到端推理流程。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上执行大规模复杂DNN（如Transformer）的挑战，利用神经形态平台SpiNNaker2实现高效推理。

Method: 提出一个包含量化和降阶步骤的前端，结合多层DNN调度框架，实现端到端流程。

Result: 成功在SpiNNaker2芯片上执行了大规模复杂DNN的推理任务。

Conclusion: 该框架为边缘设备上的高效DNN推理提供了可行解决方案。

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [8] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯神经网络的方法，用于微调预训练的基础模型，并通过实时学习工作流自动优化模型，以检测和采样稀有事件。


<details>
  <summary>Details</summary>
Motivation: 由于从头计算原子间力的计算复杂度高，机器学习力场的研究活跃，但生成足够规模和多样性的训练数据集计算成本高，尤其对稀有事件或大构型空间系统不实用。微调预训练基础模型可减少所需训练数据，但缺乏不确定性量化是挑战。

Method: 引入基于贝叶斯神经网络的方法，结合实时学习工作流，自动微调模型并保持预设精度，同时检测和增加稀有事件（如过渡态）的采样率。

Result: 方法成功克服了基础模型缺乏不确定性量化的问题，实现了自动微调和稀有事件的高效检测与采样。

Conclusion: 提出的贝叶斯神经网络方法和实时学习工作流有效解决了基础模型微调中的不确定性量化问题，为稀有事件建模提供了实用工具。

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了AI应关注实体及其关系建模，而非仅关注文本和图像数据，并分析了关系学习未普及的原因及改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要建模像素和文字，但世界由实体及其关系构成，应直接建模这些实体。

Method: 通过分析关系学习（如统计关系AI）的现状，指出其未普及的原因。

Result: 关系学习仅在有限场景中应用，需改进以发挥其潜力。

Conclusion: 关系学习需进一步发展，以在AI领域占据更重要的地位。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [10] [Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms](https://arxiv.org/abs/2507.13455)
*Dean Chen,Armin Pomeroy,Brandon T. Peterson,Will Flanagan,He Kai Lim,Alexandra Stavrakis,Nelson F. SooHoo,Jonathan B. Hopkins,Tyler R. Clites*

Main category: cs.RO

TL;DR: 提出了一种系统性设计方法，通过集成多自由度运动限制的紧凑硬停止表面，为柔性机构提供过载保护。


<details>
  <summary>Details</summary>
Motivation: 柔性机构在精密应用中潜力巨大，但疲劳和机械故障的固有脆弱性阻碍了其实际应用，尤其是在复杂和不确定的负载环境中。

Method: 引入理论和实践框架，优化接触表面几何形状，以最大化机构的多自由度工作空间，同时确保其在弹性范围内。

Result: 通过数值和实验验证，证明该方法能有效防止疲劳、屈服和屈曲。

Conclusion: 为在不确定负载下运行的柔性系统提供了精确硬停止设计的基础，推动了柔性机构在实际系统中的应用。

Abstract: Compliant mechanisms have significant potential in precision applications due
to their ability to guide motion without contact. However, an inherent
vulnerability to fatigue and mechanical failure has hindered the translation of
compliant mechanisms to real-world applications. This is particularly
challenging in service environments where loading is complex and uncertain, and
the cost of failure is high. In such cases, mechanical hard stops are critical
to prevent yielding and buckling. Conventional hard-stop designs, which rely on
stacking single-DOF limits, must be overly restrictive in multi-DOF space to
guarantee safety in the presence of unknown loads. In this study, we present a
systematic design synthesis method to guarantee overload protection in
compliant mechanisms by integrating coupled multi-DOF motion limits within a
single pair of compact hard-stop surfaces. Specifically, we introduce a
theoretical and practical framework for optimizing the contact surface geometry
to maximize the mechanisms multi-DOF working space while still ensuring that
the mechanism remains within its elastic regime. We apply this synthesis method
to a case study of a caged-hinge mechanism for orthopaedic implants, and
provide numerical and experimental validation that the derived design offers
reliable protection against fatigue, yielding, and buckling. This work
establishes a foundation for precision hard-stop design in compliant systems
operating under uncertain loads, which is a crucial step toward enabling the
application of compliant mechanisms in real-world systems.

</details>


### [11] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: ERR@HRI 2.0挑战赛提供了一个多模态数据集，用于检测LLM驱动的对话机器人失败，旨在通过机器学习模型提高失败检测能力。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的对话机器人仍易出错，如误解用户意图或中断对话，需检测和解决这些失败以维持用户信任。

Method: 提供16小时的人机交互多模态数据（面部、语音、头部动作），标注机器人错误和用户意图，鼓励团队开发检测模型。

Result: 挑战赛通过多模态数据分析，评估模型在检测准确率和误报率等指标上的表现。

Conclusion: 该挑战赛是改进人机交互中失败检测的关键一步，利用社交信号分析提升机器人性能。

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>
