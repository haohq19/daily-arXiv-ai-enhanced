{"id": "2508.11680", "categories": ["cs.LG", "cs.AI", "es: 62M10 (primary), 62P20, 68T05, 91B72 (secondary)"], "pdf": "https://arxiv.org/pdf/2508.11680", "abs": "https://arxiv.org/abs/2508.11680", "authors": ["Aditya Akella", "Jonathan Farah"], "title": "Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics", "comment": "6 pages, 4 figures, 3 tables", "summary": "Demographic shifts, influenced by globalization, economic conditions,\ngeopolitical events, and environmental factors, pose significant challenges for\npolicymakers and researchers. Accurate demographic forecasting is essential for\ninformed decision-making in areas such as urban planning, healthcare, and\neconomic policy. This study explores the application of time series foundation\nmodels to predict demographic changes in the United States using datasets from\nthe U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate\nthe performance of the Time Series Foundation Model (TimesFM) against\ntraditional baselines including Long Short-Term Memory (LSTM) networks,\nAutoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our\nexperiments across six demographically diverse states demonstrate that TimesFM\nachieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with\nparticularly strong performance on minority populations with sparse historical\ndata. These findings highlight the potential of pre-trained foundation models\nto enhance demographic analysis and inform proactive policy interventions\nwithout requiring extensive task-specific fine-tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86TimesFM\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u7f8e\u56fd\u4eba\u53e3\u7edf\u8ba1\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u572886.67%\u7684\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f4e\u7684MSE\uff0c\u7279\u522b\u5728\u5386\u53f2\u6570\u636e\u7a00\u758f\u7684\u5c11\u6570\u65cf\u88d4\u7fa4\u4f53\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4eba\u53e3\u7edf\u8ba1\u53d8\u5316\u53d7\u5168\u7403\u5316\u3001\u7ecf\u6d4e\u6761\u4ef6\u3001\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\u548c\u73af\u5883\u56e0\u7d20\u5f71\u54cd\uff0c\u5bf9\u653f\u7b56\u5236\u5b9a\u8005\u548c\u7814\u7a76\u8005\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u51c6\u786e\u7684\u4eba\u53e3\u9884\u6d4b\u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u3001\u533b\u7597\u4fdd\u5065\u548c\u7ecf\u6d4e\u653f\u7b56\u7b49\u9886\u57df\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5\u5c40\u548cFRED\u7684\u6570\u636e\u96c6\uff0c\u5c06TimesFM\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0eLSTM\u3001ARIMA\u548c\u7ebf\u6027\u56de\u5f52\u7b49\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5728\u516d\u4e2a\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u7684\u5dde\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "TimesFM\u572886.67%\u7684\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u7279\u522b\u662f\u5728\u5386\u53f2\u6570\u636e\u7a00\u758f\u7684\u5c11\u6570\u65cf\u88d4\u4eba\u53e3\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u6709\u6f5c\u529b\u589e\u5f3a\u4eba\u53e3\u7edf\u8ba1\u5206\u6790\uff0c\u4e3a\u4e3b\u52a8\u653f\u7b56\u5e72\u9884\u63d0\u4f9b\u4fe1\u606f\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u3002"}}
{"id": "2508.11727", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11727", "abs": "https://arxiv.org/abs/2508.11727", "authors": ["Songyao Jin", "Biwei Huang"], "title": "Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks", "comment": null, "summary": "Multivariate Hawkes process provides a powerful framework for modeling\ntemporal dependencies and event-driven interactions in complex systems. While\nexisting methods primarily focus on uncovering causal structures among observed\nsubprocesses, real-world systems are often only partially observed, with latent\nsubprocesses posing significant challenges. In this paper, we show that\ncontinuous-time event sequences can be represented by a discrete-time model as\nthe time interval shrinks, and we leverage this insight to establish necessary\nand sufficient conditions for identifying latent subprocesses and the causal\ninfluences. Accordingly, we propose a two-phase iterative algorithm that\nalternates between inferring causal relationships among discovered subprocesses\nand uncovering new latent subprocesses, guided by path-based conditions that\nguarantee identifiability. Experiments on both synthetic and real-world\ndatasets show that our method effectively recovers causal structures despite\nthe presence of latent subprocesses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc6\u522b\u591a\u5143\u970d\u514b\u65af\u8fc7\u7a0b\u4e2d\u6f5c\u5728\u5b50\u8fc7\u7a0b\u548c\u56e0\u679c\u5f71\u54cd\u7684\u4e24\u9636\u6bb5\u8fed\u4ee3\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5b58\u5728\u6f5c\u5728\u5b50\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u6062\u590d\u56e0\u679c\u7ed3\u6784\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u5f80\u5f80\u53ea\u6709\u90e8\u5206\u88ab\u89c2\u6d4b\uff0c\u5b58\u5728\u6f5c\u5728\u5b50\u8fc7\u7a0b\u5bf9\u73b0\u6709\u65b9\u6cd5\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5df2\u89c2\u6d4b\u5b50\u8fc7\u7a0b\u95f4\u7684\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u3002", "method": "\u5c06\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u8868\u793a\u4e3a\u79bb\u6563\u65f6\u95f4\u6a21\u578b\uff0c\u5efa\u7acb\u8bc6\u522b\u6f5c\u5728\u5b50\u8fc7\u7a0b\u548c\u56e0\u679c\u5f71\u54cd\u7684\u5145\u8981\u6761\u4ef6\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8fed\u4ee3\u7b97\u6cd5\uff0c\u4ea4\u66ff\u63a8\u65ad\u5df2\u53d1\u73b0\u5b50\u8fc7\u7a0b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u548c\u53d1\u73b0\u65b0\u7684\u6f5c\u5728\u5b50\u8fc7\u7a0b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b58\u5728\u6f5c\u5728\u5b50\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u80fd\u6709\u6548\u6062\u590d\u56e0\u679c\u7ed3\u6784\u3002", "conclusion": "\u901a\u8fc7\u79bb\u6563\u65f6\u95f4\u6a21\u578b\u8868\u793a\u548c\u8def\u5f84\u6761\u4ef6\u6307\u5bfc\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u5143\u970d\u514b\u65af\u8fc7\u7a0b\u4e2d\u6f5c\u5728\u5b50\u8fc7\u7a0b\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12075", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12075", "abs": "https://arxiv.org/abs/2508.12075", "authors": ["Shaul Ashkenazi", "Gabriel Skantze", "Jane Stuart-Smith", "Mary Ellen Foster"], "title": "Into the Wild: When Robots Are Not Welcome", "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025 (paper PubRob-Fails/2025/4)", "summary": "Social robots are increasingly being deployed in public spaces, where they\nface not only technological difficulties and unexpected user utterances, but\nalso objections from stakeholders who may not be comfortable with introducing a\nrobot into those spaces. We describe our difficulties with deploying a social\nrobot in two different public settings: 1) Student services center; 2) Refugees\nand asylum seekers drop-in service. Although this is a failure report, in each\nuse case we eventually managed to earn the trust of the staff and form a\nrelationship with them, allowing us to deploy our robot and conduct our\nstudies.", "AI": {"tldr": "\u793e\u4ea4\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u90e8\u7f72\u9047\u5230\u7684\u56f0\u96be\u548c\u53cd\u5bf9\u58f0\u97f3\uff0c\u4f46\u6700\u7ec8\u901a\u8fc7\u5efa\u7acb\u4fe1\u4efb\u5173\u7cfb\u6210\u529f\u5b8c\u6210\u90e8\u7f72", "motivation": "\u7814\u7a76\u793e\u4ea4\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u6280\u672f\u56f0\u96be\u3001\u610f\u5916\u7528\u6237\u8bed\u8a00\u4ee5\u53ca\u5229\u76ca\u76f8\u5173\u8005\u53cd\u5bf9\u7684\u6311\u6218", "method": "\u5728\u4e24\u4e2a\u4e0d\u540c\u516c\u5171\u573a\u666f\u4e2d\u90e8\u7f72\u793e\u4ea4\u673a\u5668\u4eba\uff1a1\uff09\u5b66\u751f\u670d\u52a1\u4e2d\u5fc3\uff1b2\uff09\u96be\u6c11\u548c\u5bfb\u6c42\u5bb5\u62a4\u8005\u4e34\u65f6\u670d\u52a1\u70b9", "result": "\u867d\u7136\u9047\u5230\u5931\u8d25\u548c\u56f0\u96be\uff0c\u4f46\u6700\u7ec8\u6210\u529f\u83b7\u5f97\u5458\u5de5\u4fe1\u4efb\u5e76\u5efa\u7acb\u826f\u597d\u5173\u7cfb\uff0c\u5b8c\u6210\u4e86\u673a\u5668\u4eba\u90e8\u7f72\u548c\u7814\u7a76\u5de5\u4f5c", "conclusion": "\u901a\u8fc7\u5efa\u7acb\u4fe1\u4efb\u548c\u826f\u597d\u5173\u7cfb\uff0c\u53ef\u4ee5\u514b\u670d\u793e\u4ea4\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u90e8\u7f72\u65f6\u9047\u5230\u7684\u5404\u79cd\u6311\u6218\uff0c\u5b9e\u73b0\u6210\u529f\u90e8\u7f72"}}
{"id": "2508.11943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11943", "abs": "https://arxiv.org/abs/2508.11943", "authors": ["Sishun Liu", "Ke Deng", "Xiuzhen Zhang", "Yan Wang"], "title": "Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning", "comment": "ECAI 2025 full version", "summary": "Neural network-based Marked Temporal Point Process (MTPP) models have been\nwidely adopted to model event sequences in high-stakes applications, raising\nconcerns about the trustworthiness of outputs from these models. This study\nfocuses on Explanation for MTPP, aiming to identify the minimal and rational\nexplanation, that is, the minimum subset of events in history, based on which\nthe prediction accuracy of MTPP matches that based on full history to a great\nextent and better than that based on the complement of the subset. This study\nfinds that directly defining Explanation for MTPP as counterfactual explanation\nor factual explanation can result in irrational explanations. To address this\nissue, we define Explanation for MTPP as a combination of counterfactual\nexplanation and factual explanation. This study proposes Counterfactual and\nFactual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of\ndeliberately designed techniques. Experiments demonstrate the correctness and\nsuperiority of CFF over baselines regarding explanation quality and processing\nefficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CFF\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u548c\u4e8b\u5b9e\u89e3\u91ca\u6765\u4e3a\u6807\u8bb0\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u63d0\u4f9b\u6700\u5c0f\u4e14\u5408\u7406\u7684\u89e3\u91ca\u5b50\u96c6\uff0c\u63d0\u9ad8\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u6807\u8bb0\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u5b58\u5728\u62c5\u5fe7\uff0c\u9700\u8981\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u6765\u8bc6\u522b\u5386\u53f2\u4e8b\u4ef6\u4e2d\u7684\u6700\u5c0f\u89e3\u91ca\u5b50\u96c6\u3002", "method": "\u63d0\u51faCFF\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u89e3\u91ca\u548c\u4e8b\u5b9e\u89e3\u91ca\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6280\u672f\u6765\u89e3\u51b3MTPP\u7684\u89e3\u91ca\u95ee\u9898\uff0c\u907f\u514d\u5355\u4e00\u89e3\u91ca\u65b9\u5f0f\u7684\u4e0d\u5408\u7406\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCFF\u5728\u89e3\u91ca\u8d28\u91cf\u548c\u5904\u7406\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u6b63\u786e\u4e14\u4f18\u8d8a\u7684\u89e3\u91ca\u7ed3\u679c\u3002", "conclusion": "CFF\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u53cd\u4e8b\u5b9e\u548c\u4e8b\u5b9e\u89e3\u91ca\uff0c\u6709\u6548\u89e3\u51b3\u4e86MTPP\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u6700\u5c0f\u4e14\u5408\u7406\u7684\u89e3\u91ca\u5b50\u96c6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2508.12274", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12274", "abs": "https://arxiv.org/abs/2508.12274", "authors": ["Jian Zhao", "Yunlong Lian", "Andy M Tyrrell", "Michael Gienger", "Jihong Zhu"], "title": "Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments", "comment": "8 pages, 41 figures", "summary": "Robot-assisted dressing is a popular but challenging topic in the field of\nrobotic manipulation, offering significant potential to improve the quality of\nlife for individuals with mobility limitations. Currently, the majority of\nresearch on robot-assisted dressing focuses on how to put on loose-fitting\nclothing, with little attention paid to tight garments. For the former, since\nthe armscye is larger, a single robotic arm can usually complete the dressing\ntask successfully. However, for the latter, dressing with a single robotic arm\noften fails due to the narrower armscye and the property of diminishing\nrigidity in the armscye, which eventually causes the armscye to get stuck. This\npaper proposes a bimanual dressing strategy suitable for dressing tight-fitting\nclothing. To facilitate the encoding of dressing trajectories that adapt to\ndifferent human arm postures, a spherical coordinate system for dressing is\nestablished. We uses the azimuthal angle of the spherical coordinate system as\na task-relevant feature for bimanual manipulation. Based on this new\ncoordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture\nRegression (GMR) for imitation learning of bimanual dressing trajectories,\ngenerating dressing strategies that adapt to different human arm postures. The\neffectiveness of the proposed method is validated through various experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u7a7a\u95f4\u5750\u6807\u7cfb\u7684\u53cc\u624b\u7a7f\u8863\u7b56\u7565\uff0c\u901a\u8fc7GMM/GMR\u6a21\u62df\u5b66\u4e60\u6765\u9002\u5e94\u4e0d\u540c\u4eba\u4f53\u624b\u81c2\u59ff\u52bf\uff0c\u89e3\u51b3\u7d27\u8eab\u670d\u88c5\u7a7f\u6234\u96be\u9898", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8f85\u52a9\u7a7f\u8863\u7814\u7a76\u591a\u5173\u6ce8\u677e\u8d34\u670d\u88c5\uff0c\u5bf9\u7d27\u8eab\u670d\u88c5\u5173\u6ce8\u5c11\uff0c\u5355\u624b\u673a\u5668\u4eba\u7a7f\u6234\u7d27\u8eab\u670d\u88c5\u5bb9\u6613\u5931\u8d25", "method": "\u5efa\u7acb\u7403\u5750\u6807\u7cfb\u7f16\u7801\u7a7f\u8863\u8f68\u8ff9\uff0c\u4f7f\u7528\u65b9\u4f4d\u89d2\u4f5c\u4e3a\u53cc\u624b\u64cd\u4f5c\u7684\u4efb\u52a1\u7279\u5f81\uff0c\u91c7\u7528GMM\u548cGMR\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60", "result": "\u901a\u8fc7\u591a\u79cd\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u53cc\u624b\u7a7f\u8863\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7d27\u8eab\u670d\u88c5\u7a7f\u6234\u96be\u9898\uff0c\u9002\u5e94\u4e0d\u540c\u4eba\u4f53\u624b\u81c2\u59ff\u52bf"}}
{"id": "2508.11988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11988", "abs": "https://arxiv.org/abs/2508.11988", "authors": ["Nicolas Mastropasqua", "Ignacio Bugueno-Cordova", "Rodrigo Verschae", "Daniel Acevedo", "Pablo Negri", "Maria E. Buemi"], "title": "Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis", "comment": null, "summary": "Micro-expression analysis has applications in domains such as Human-Robot\nInteraction and Driver Monitoring Systems. Accurately capturing subtle and fast\nfacial movements remains difficult when relying solely on RGB cameras, due to\nlimitations in temporal resolution and sensitivity to motion blur. Event\ncameras offer an alternative, with microsecond-level precision, high dynamic\nrange, and low latency. However, public datasets featuring event-based\nrecordings of Action Units are still scarce. In this work, we introduce a\nnovel, preliminary multi-resolution and multi-modal micro-expression dataset\nrecorded with synchronized RGB and event cameras under variable lighting\nconditions. Two baseline tasks are evaluated to explore the spatial-temporal\ndynamics of micro-expressions: Action Unit classification using Spiking Neural\nNetworks (51.23\\% accuracy with events vs. 23.12\\% with RGB), and frame\nreconstruction using Conditional Variational Autoencoders, achieving SSIM =\n0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising\nresults show that event-based data can be used for micro-expression recognition\nand frame reconstruction.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u5206\u8fa8\u7387\u591a\u6a21\u6001\u5fae\u8868\u60c5\u6570\u636e\u96c6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u6536\u96c6\u540c\u6b65RGB\u548c\u4e8b\u4ef6\u6570\u636e\uff0c\u5728\u53d8\u5316\u7167\u660e\u6761\u4ef6\u4e0b\u5f55\u5236\u5fae\u8868\u60c5\u3002\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u884c\u4e3a\u5355\u5143\u5206\u7c7b\u548c\u5e27\u91cd\u5efa\u4efb\u52a1\u90fd\u53d6\u5f97\u4e86\u6bd4\u4f20\u7edfRGB\u65b9\u6cd5\u66f4\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u5fae\u8868\u60c5\u5206\u6790\u5728\u4eba\u673a\u4ea4\u4e92\u548c\u9a7e\u9a76\u5458\u76d1\u6d4b\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u4f20\u7edfRGB\u76f8\u673a\u56e0\u4e3a\u65f6\u95f4\u5206\u8fa8\u7387\u9650\u5236\u548c\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u5f88\u96be\u51c6\u786e\u6355\u6349\u7ec6\u5fae\u800c\u5feb\u901f\u7684\u9762\u90e8\u8fd0\u52a8\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u7cbe\u5ea6\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7684\u4f18\u52bf\uff0c\u4f46\u76ee\u524d\u516c\u5f00\u7684\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u884c\u4e3a\u5355\u5143\u6570\u636e\u96c6\u4ecd\u7136\u7a00\u7f3a\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u5206\u8fa8\u7387\u591a\u6a21\u6001\u5fae\u8868\u60c5\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u540c\u6b65\u7684RGB\u548c\u4e8b\u4ef6\u76f8\u673a\u5728\u53d8\u5316\u7167\u660e\u6761\u4ef6\u4e0b\u8fdb\u884c\u5f55\u5236\u3002\u8bc4\u4f30\u4e86\u4e24\u4e2a\u57fa\u51c6\u4efb\u52a1\uff1a1\uff09\u4f7f\u7528\u810f\u7535\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u884c\u4e3a\u5355\u5143\u5206\u7c7b\uff1b2\uff09\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u5e27\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u4f7f\u7528\u4e8b\u4ef6\u6570\u636e\u7684\u884c\u4e3a\u5355\u5143\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523051.23%\uff08RGB\u6570\u636e\u4ec5\u4e3a23.12%\uff09\uff1b\u9ad8\u5206\u8fa8\u7387\u4e8b\u4ef6\u8f93\u5165\u7684\u5e27\u91cd\u5efa\u4efb\u52a1\u8fbe\u5230SSIM=0.8513\u548cPSNR=26.89dB\u7684\u9ad8\u8d28\u91cf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e8b\u4ef6\u57fa\u6570\u636e\u53ef\u4ee5\u6709\u6548\u7528\u4e8e\u5fae\u8868\u60c5\u8bc6\u522b\u548c\u5e27\u91cd\u5efa\uff0c\u4e3a\u5fae\u8868\u60c5\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.12104", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12104", "abs": "https://arxiv.org/abs/2508.12104", "authors": ["Shane Waxler", "Paul Blazek", "Davis White", "Daniel Sneider", "Kevin Chung", "Mani Nagarathnam", "Patrick Williams", "Hank Voeller", "Karen Wong", "Matthew Swanhorst", "Sheng Zhang", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon", "Andrew Loza", "Daniella Meeker", "Seth Hain", "Rahul Shah"], "title": "Generative Medical Event Models Improve with Scale", "comment": null, "summary": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes.", "AI": {"tldr": "CoMET\u662f\u4e00\u4e2a\u57fa\u4e8e160\u4ebf\u6b21\u533b\u7597\u4e8b\u4ef6\u8bad\u7ec3\u7684\u533b\u7597\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u533b\u7597\u4e8b\u4ef6\u6765\u6a21\u62df\u60a3\u8005\u5065\u5eb7\u65f6\u95f4\u7ebf\uff0c\u572878\u4e2a\u533b\u7597\u4efb\u52a1\u4e2d\u65e0\u9700\u5fae\u8c03\u5c31\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4e13\u7528\u76d1\u7763\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u5b9e\u73b0\u89c4\u6a21\u5316\u4e2a\u6027\u5316\u533b\u7597\u9700\u8981\u4ece\u7eb5\u5411\u60a3\u8005\u65c5\u7a0b\u4e2d\u63d0\u53d6\u6d1e\u5bdf\uff0c\u5927\u89c4\u6a21\u533b\u7597\u4e8b\u4ef6\u6570\u636e\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u6709\u671b\u6269\u5c55\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u751f\u6210\u5e76\u6cdb\u5316\u5230\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u4f7f\u7528Epic Cosmos\u6570\u636e\u96c6\uff08163\u4ebf\u6b21\u5c31\u8bca\u30013\u4ebf\u60a3\u8005\u8bb0\u5f55\uff09\uff0c\u8bad\u7ec3\u4e86CoMET\u7cfb\u5217\u89e3\u7801\u5668Transformer\u6a21\u578b\uff0c\u8fdb\u884c\u4e86\u6700\u5927\u7684\u533b\u7597\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u7814\u7a76\uff0c\u9884\u8bad\u7ec3\u4e86\u8ba1\u7b97\u6700\u4f18\u6a21\u578b\uff08\u6700\u591a10\u4ebf\u53c2\u6570\uff09\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u4e0b\u4e00\u4e2a\u533b\u7597\u4e8b\u4ef6\u6765\u6a21\u62df\u60a3\u8005\u65f6\u95f4\u7ebf\u3002", "result": "\u572878\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff08\u8bca\u65ad\u9884\u6d4b\u3001\u75be\u75c5\u9884\u540e\u3001\u533b\u7597\u8fd0\u8425\uff09\u4e2d\uff0cCoMET\u901a\u5e38\u4f18\u4e8e\u6216\u5339\u914d\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u6a21\u578b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u6216\u5c11\u6837\u672c\u793a\u4f8b\uff0c\u9884\u6d4b\u80fd\u529b\u968f\u6a21\u578b\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u89c4\u6a21\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "CoMET\u4f5c\u4e3a\u751f\u6210\u5f0f\u533b\u7597\u4e8b\u4ef6\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u4e34\u5e8a\u52a8\u6001\uff0c\u4e3a\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3001\u7b80\u5316\u533b\u7597\u8fd0\u8425\u548c\u6539\u5584\u60a3\u8005\u7ed3\u5c40\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u7684\u6846\u67b6\u3002"}}
{"id": "2508.12500", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12500", "abs": "https://arxiv.org/abs/2508.12500", "authors": ["Rahmat K. Adesunkanmi", "Ashfaq Khokhar", "Goce Trajcevski", "Sohail Murad"], "title": "Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models", "comment": "Submitted to ACM", "summary": "Molecular dynamics simulations (MDS) face challenges, including\nresource-heavy computations and the need to manually scan outputs to detect\n\"interesting events,\" such as the formation and persistence of hydrogen bonds\nbetween atoms of different molecules. A critical research gap lies in\nidentifying the underlying causes of hydrogen bond formation and separation\n-understanding which interactions or prior events contribute to their emergence\nover time. With this challenge in mind, we propose leveraging spatio-temporal\ndata analytics and machine learning models to enhance the detection of these\nphenomena. In this paper, our approach is inspired by causal modeling and aims\nto identify the root cause variables of hydrogen bond formation and separation\nevents. Specifically, we treat the separation of hydrogen bonds as an\n\"intervention\" occurring and represent the causal structure of the bonding and\nseparation events in the MDS as graphical causal models. These causal models\nare built using a variational autoencoder-inspired architecture that enables us\nto infer causal relationships across samples with diverse underlying causal\ngraphs while leveraging shared dynamic information. We further include a step\nto infer the root causes of changes in the joint distribution of the causal\nmodels. By constructing causal models that capture shifts in the conditional\ndistributions of molecular interactions during bond formation or separation,\nthis framework provides a novel perspective on root cause analysis in molecular\ndynamic systems. We validate the efficacy of our model empirically on the\natomic trajectories that used MDS for chiral separation, demonstrating that we\ncan predict many steps in the future and also find the variables driving the\nobserved changes in the system.", "AI": {"tldr": "\u5229\u7528\u56e0\u679c\u6a21\u578b\u548c\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u6765\u5206\u6790\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u6c22\u952e\u5f62\u6210\u548c\u89e3\u79bb\u7684\u6839\u672c\u539f\u56e0", "motivation": "\u89e3\u51b3\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u8d44\u6e90\u6d89\u53ca\u8ba1\u7b97\u91cd\u3001\u9700\u624b\u52a8\u626b\u63cf\u8f93\u51fa\u4ee5\u53d1\u73b0\u5173\u952e\u4e8b\u4ef6\u7684\u6311\u6218\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u6c22\u952e\u5f62\u6210\u548c\u89e3\u79bb\u7684\u6839\u672c\u539f\u56e0", "method": "\u91c7\u7528\u56e0\u679c\u6a21\u578b\u6280\u672f\uff0c\u5c06\u6c22\u952e\u89e3\u79bb\u89c6\u4e3a\"\u5e72\u9884\"\u4e8b\u4ef6\uff0c\u6784\u5efa\u56fe\u5f62\u56e0\u679c\u6a21\u578b\u3002\u4f7f\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u63a8\u65ad\u6837\u672c\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u5305\u542b\u805a\u5408\u5206\u5e03\u53d8\u5316\u7684\u6839\u56e0\u63a8\u65ad\u6b65\u9aa4", "result": "\u5728\u624b\u6027\u5206\u79bb\u7684\u539f\u5b50\u8f68\u8ff9\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u6709\u6548\u6027\uff0c\u80fd\u591f\u9884\u6d4b\u591a\u6b65\u672a\u6765\u53d8\u5316\uff0c\u5e76\u53d1\u73b0\u9a71\u52a8\u7cfb\u7edf\u53d8\u5316\u7684\u5173\u952e\u53d8\u91cf", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u5b50\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u5efa\u6a21\u6355\u6349\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u5728\u952e\u5408\u5f62\u6210\u6216\u89e3\u79bb\u8fc7\u7a0b\u4e2d\u6761\u4ef6\u5206\u5e03\u7684\u53d8\u5316"}}
{"id": "2508.12564", "categories": ["cs.RO", "cs.CV", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.12564", "abs": "https://arxiv.org/abs/2508.12564", "authors": ["Jiayao Mai", "Xiuyuan Lu", "Kuan Dai", "Shaojie Shen", "Yi Zhou"], "title": "Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems", "comment": "8 pages, 5 figures", "summary": "Event cameras generate asynchronous signals in response to pixel-level\nbrightness changes, offering a sensing paradigm with theoretically\nmicrosecond-scale latency that can significantly enhance the performance of\nmulti-sensor systems. Extrinsic calibration is a critical prerequisite for\neffective sensor fusion; however, the configuration that involves event cameras\nremains an understudied topic. In this paper, we propose a motion-based\ntemporal and rotational calibration framework tailored for event-centric\nmulti-sensor systems, eliminating the need for dedicated calibration targets.\nOur method uses as input the rotational motion estimates obtained from event\ncameras and other heterogeneous sensors, respectively. Different from\nconventional approaches that rely on event-to-frame conversion, our method\nefficiently estimates angular velocity from normal flow observations, which are\nderived from the spatio-temporal profile of event data. The overall calibration\npipeline adopts a two-step approach: it first initializes the temporal offset\nand rotational extrinsics by exploiting kinematic correlations in the spirit of\nCanonical Correlation Analysis (CCA), and then refines both temporal and\nrotational parameters through a joint non-linear optimization using a\ncontinuous-time parametrization in SO(3). Extensive evaluations on both\npublicly available and self-collected datasets validate that the proposed\nmethod achieves calibration accuracy comparable to target-based methods, while\nexhibiting superior stability over purely CCA-based methods, and highlighting\nits precision, robustness and flexibility. To facilitate future research, our\nimplementation will be made open-source. Code:\nhttps://github.com/NAIL-HNU/EvMultiCalib.", "AI": {"tldr": "\u4e8b\u4ef6\u76f8\u673a\u5916\u90e8\u6807\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u4f30\u8ba1\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u65e0\u9700\u4e13\u95e8\u6807\u5b9a\u7269\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u76f8\u673a\u4e0e\u5176\u4ed6\u4f20\u611f\u5668\u7684\u65f6\u95f4\u504f\u79fb\u548c\u65cb\u8f6c\u5916\u53c2\u7684\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u4f18\u52bf\uff0c\u4f46\u5728\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u4e2d\u7684\u5916\u90e8\u6807\u5b9a\u95ee\u9898\u7814\u7a76\u8f83\u5c11\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8b\u4ef6\u5230\u5e27\u8f6c\u6362\u548c\u4e13\u95e8\u6807\u5b9a\u7269\uff0c\u5b58\u5728\u9650\u5236\u3002", "method": "\u4f7f\u7528\u6cd5\u5411\u6d41\u89c2\u6d4b\u4f30\u89d2\u901f\u5ea6\uff0c\u91c7\u7528\u4e24\u6b65\u6d41\u7a0b\uff1a\u5148\u901a\u8fc7\u5170\u8054\u5206\u6790\u521d\u59cb\u5316\u65f6\u95f4\u504f\u79fb\u548c\u65cb\u8f6c\u5916\u53c2\uff0c\u7136\u540e\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4SO(3)\u53c2\u6570\u5316\u8fdb\u884c\u975e\u7ebf\u6027\u4f18\u5316\u7cbe\u7ec6\u8c03\u6574\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u6536\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0e\u57fa\u4e8e\u6807\u5b9a\u7269\u65b9\u6cd5\u76f8\u5f53\u7684\u6807\u5b9a\u7cbe\u5ea6\uff0c\u4e14\u6bd4\u7eaf\u7cb9CCA\u65b9\u6cd5\u66f4\u7a33\u5b9a\uff0c\u663e\u793a\u4e86\u9ad8\u7cbe\u5ea6\u3001\u7a33\u5065\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u76f8\u673a\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4e13\u95e8\u6807\u5b9a\u7269\u7684\u9ad8\u6548\u5916\u90e8\u6807\u5b9a\u65b9\u6848\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5b9e\u7528\u4ef7\u503c\u548c\u63a8\u5e7f\u6f5c\u529b\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2508.12162", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12162", "abs": "https://arxiv.org/abs/2508.12162", "authors": ["J. M. I. H. Jayakody", "A. M. H. H. Alahakoon", "C. R. M. Perera", "R. M. L. C. Srimal", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis", "comment": null, "summary": "The paradigm of electrocardiogram (ECG) analysis has evolved into real-time\ndigital analysis, facilitated by artificial intelligence (AI) and machine\nlearning (ML), which has improved the diagnostic precision and predictive\ncapacity of cardiac diseases. This work proposes a novel deep learning (DL)\narchitecture called the attention-integrated convolutional residual network\n(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,\nthe QRS duration, the heart rate, the peak amplitude of the R wave, and the\namplitude of the T wave for interpretable ECG analysis. Our architecture is\nspecially designed with spatial and channel attention-related mechanisms to\naddress the type and spatial location of the ECG features for regression. The\nmodels employ a convolutional residual network to address vanishing and\nexploding gradient problems. The designed system addresses traditional analysis\nchallenges, such as loss of focus due to human errors, and facilitates the fast\nand easy detection of cardiac events, thereby reducing the manual efforts\nrequired to solve analysis tasks. AICRN models outperform existing models in\nparameter regression with higher precision. This work demonstrates that DL can\nplay a crucial role in the interpretability and precision of ECG analysis,\nopening up new clinical applications for cardiac monitoring and management.", "AI": {"tldr": "\u901a\u8fc7\u6ce8\u610f\u529b\u96c6\u6210\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc(AICRN)\u6a21\u578b\uff0c\u5b9e\u73b0\u5fc3\u7535\u56fe\u5173\u952e\u53c2\u6570\u7684\u9ad8\u7cbe\u5ea6\u56de\u5f52\u9884\u6d4b\uff0c\u63d0\u5347\u5fc3\u7535\u56fe\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5fc3\u7535\u56fe\u5206\u6790\u4e2d\u7684\u4eba\u4e3a\u9519\u8bef\u3001\u5173\u952e\u7279\u5f81\u5931\u7126\u7b49\u95ee\u9898\uff0c\u901a\u8fc7AI/ML\u6280\u672f\u63d0\u5347\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u7684\u7cbe\u786e\u6027\u548c\u9884\u6d4b\u80fd\u529b", "method": "\u8bbe\u8ba1\u4e86\u5177\u6709\u7a7a\u95f4\u548c\u901a\u9053\u6ce8\u610f\u673a\u5236\u7684AICRN\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u91c7\u7528\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc\u89e3\u51b3\u6838\u5fc3\u95ee\u9898\uff0c\u5b9e\u73b0\u5fc3\u7535\u56fe\u5173\u952e\u53c2\u6570(PR\u95f4\u671f\u3001QT\u95f4\u671f\u7b49)\u7684\u56de\u5f52\u9884\u6d4b", "result": "AICRN\u6a21\u578b\u5728\u53c2\u6570\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u8d85\u8fc7\u73b0\u6709\u6a21\u578b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u5fc3\u810f\u4e8b\u4ef6\u5e76\u51cf\u5c11\u624b\u52a8\u5206\u6790\u5de5\u4f5c\u91cf", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5fc3\u7535\u56fe\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7cbe\u786e\u6027\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5fc3\u810f\u76d1\u6d4b\u548c\u7ba1\u7406\u5f00\u542f\u4e86\u65b0\u7684\u4e34\u5e8a\u5e94\u7528\u524d\u666f"}}
{"id": "2508.12084", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12084", "abs": "https://arxiv.org/abs/2508.12084", "authors": ["Jaejun Hwang", "Dayoung Gong", "Manjin Kim", "Minsu Cho"], "title": "Generic Event Boundary Detection via Denoising Diffusion", "comment": "Accepted to ICCV 2025", "summary": "Generic event boundary detection (GEBD) aims to identify natural boundaries\nin a video, segmenting it into distinct and meaningful chunks. Despite the\ninherent subjectivity of event boundaries, previous methods have focused on\ndeterministic predictions, overlooking the diversity of plausible solutions. In\nthis paper, we introduce a novel diffusion-based boundary detection model,\ndubbed DiffGEBD, that tackles the problem of GEBD from a generative\nperspective. The proposed model encodes relevant changes across adjacent frames\nvia temporal self-similarity and then iteratively decodes random noise into\nplausible event boundaries being conditioned on the encoded features.\nClassifier-free guidance allows the degree of diversity to be controlled in\ndenoising diffusion. In addition, we introduce a new evaluation metric to\nassess the quality of predictions considering both diversity and fidelity.\nExperiments show that our method achieves strong performance on two standard\nbenchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event\nboundaries.", "AI": {"tldr": "DiffGEBD\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u89c6\u89d2\u89e3\u51b3\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u95ee\u9898\uff0c\u80fd\u591f\u4ea7\u751f\u591a\u6837\u5316\u7684\u8fb9\u754c\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u4e8b\u4ef6\u8fb9\u754c\u7684\u4e3b\u89c2\u6027\u548c\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u7684\u8fb9\u754c\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u81ea\u76f8\u4f3c\u6027\u7f16\u7801\u76f8\u90bb\u5e27\u7684\u76f8\u5173\u53d8\u5316\uff0c\u7136\u540e\u8fed\u4ee3\u5730\u5c06\u968f\u673a\u566a\u58f0\u89e3\u7801\u4e3a\u5408\u7406\u7684\u4e8b\u4ef6\u8fb9\u754c\uff0c\u4f7f\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u63a7\u5236\u591a\u6837\u6027\u3002", "result": "\u5728Kinetics-GEBD\u548cTAPOS\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u5408\u7406\u7684\u4e8b\u4ef6\u8fb9\u754c\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u901a\u7528\u4e8b\u4ef6\u8fb9\u754c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4e8b\u4ef6\u8fb9\u754c\u7684\u4e3b\u89c2\u591a\u6837\u6027\u95ee\u9898\u3002"}}
{"id": "2508.12132", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12132", "abs": "https://arxiv.org/abs/2508.12132", "authors": ["Amira Guesmi", "Bassem Ouni", "Muhammad Shafique"], "title": "TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks", "comment": null, "summary": "Quantized Neural Networks (QNNs) are increasingly deployed in edge and\nresource-constrained environments due to their efficiency in computation and\nmemory usage. While shown to distort the gradient landscape and weaken\nconventional pixel-level attacks, it provides limited robustness against\npatch-based adversarial attacks-localized, high-saliency perturbations that\nremain surprisingly transferable across bit-widths. Existing defenses either\noverfit to fixed quantization settings or fail to address this cross-bit\ngeneralization vulnerability. We introduce \\textbf{TriQDef}, a tri-level\nquantization-aware defense framework designed to disrupt the transferability of\npatch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature\nDisalignment Penalty (FDP) that enforces semantic inconsistency by penalizing\nperceptual similarity in intermediate representations; (2) a Gradient\nPerceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients\nacross bit-widths by minimizing structural and directional agreement via Edge\nIoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training\nProtocol that unifies these penalties within a shared-weight training scheme\nacross multiple quantization levels. Extensive experiments on CIFAR-10 and\nImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over\n40\\% on unseen patch and quantization combinations, while preserving high clean\naccuracy. Our findings underscore the importance of disrupting both semantic\nand perceptual gradient alignment to mitigate patch transferability in QNNs.", "AI": {"tldr": "TriQDef\u662f\u4e00\u4e2a\u4e09\u5c42\u6b21\u91cf\u5316\u611f\u77e5\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u4e0d\u5bf9\u9f50\u60e9\u7f5a\u3001\u68af\u5ea6\u611f\u77e5\u5931\u8c10\u60e9\u7f5a\u548c\u8054\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u534f\u8bae\uff0c\u6709\u6548\u62b5\u5fa1QNN\u4e2d\u7684\u8865\u4e01\u5f0f\u5bf9\u6297\u653b\u51fb\u8de8\u6bd4\u7279\u5bbd\u5ea6\u8fc1\u79fb\u6027", "motivation": "\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u8bbe\u5907\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u73b0\u6709\u9632\u5fa1\u8981\u4e48\u8fc7\u62df\u5408\u56fa\u5b9a\u91cf\u5316\u8bbe\u7f6e\uff0c\u8981\u4e48\u65e0\u6cd5\u89e3\u51b3\u8de8\u6bd4\u7279\u6cdb\u5316\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5bf9\u5c40\u90e8\u9ad8\u663e\u8457\u6027\u8865\u4e01\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u9632\u5fa1\u4e0d\u8db3", "method": "TriQDef\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u7279\u5f81\u4e0d\u5bf9\u9f50\u60e9\u7f5a\uff08FDP\uff09\u901a\u8fc7\u60e9\u7f5a\u4e2d\u95f4\u8868\u793a\u7684\u611f\u77e5\u76f8\u4f3c\u6027\u6765\u5f3a\u5236\u8bed\u4e49\u4e0d\u4e00\u81f4\uff1b2\uff09\u68af\u5ea6\u611f\u77e5\u5931\u8c10\u60e9\u7f5a\uff08GPDP\uff09\u901a\u8fc7\u8fb9\u7f18IoU\u548cHOG\u4f59\u5f26\u5ea6\u91cf\u6700\u5c0f\u5316\u7ed3\u6784\u6027\u548c\u65b9\u5411\u6027\u4e00\u81f4\u6027\u6765\u663e\u5f0f\u9519\u4f4d\u8f93\u5165\u68af\u5ea6\uff1b3\uff09\u8054\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u534f\u8bae\u5728\u591a\u4e2a\u91cf\u5316\u7ea7\u522b\u4e0a\u7edf\u4e00\u8fd9\u4e9b\u60e9\u7f5a", "result": "\u5728CIFAR-10\u548cImageNet\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTriQDef\u5728\u672a\u89c1\u8fc7\u7684\u8865\u4e01\u548c\u91cf\u5316\u7ec4\u5408\u4e0a\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u4e8640%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6e05\u6d01\u51c6\u786e\u7387", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7834\u574f\u8bed\u4e49\u548c\u611f\u77e5\u68af\u5ea6\u5bf9\u9f50\u5bf9\u4e8e\u51cf\u8f7bQNN\u4e2d\u8865\u4e01\u8fc1\u79fb\u6027\u7684\u91cd\u8981\u6027\uff0cTriQDef\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6bd4\u7279\u6cdb\u5316\u6f0f\u6d1e"}}
{"id": "2508.12535", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12535", "abs": "https://arxiv.org/abs/2508.12535", "authors": ["Seonglae Cho", "Zekun Wu", "Adriano Koshiyama"], "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection", "comment": "42 pages, 9 tables", "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.", "AI": {"tldr": "CorrSteer\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u63a8\u7406\u65f6\u6fc0\u6d3b\u6765\u63d0\u53d6\u76f8\u5173\u7279\u5f81\uff0c\u907f\u514d\u4e86\u865a\u5047\u76f8\u5173\u6027\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u89e3\u51b3\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u4e0b\u6e38\u63a7\u5236\u4efb\u52a1\u4e2d\u9700\u8981\u5bf9\u6bd4\u6570\u636e\u96c6\u6216\u5927\u91cf\u6fc0\u6d3b\u5b58\u50a8\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7279\u5f81\u9009\u62e9\u548c\u81ea\u52a8\u5316\u63a7\u5236", "method": "\u901a\u8fc7\u5c06\u6837\u672c\u6b63\u786e\u6027\u4e0e\u63a8\u7406\u65f6\u751f\u6210\u7684token\u7684SAE\u6fc0\u6d3b\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u6765\u9009\u62e9\u7279\u5f81\uff0c\u4f7f\u7528\u5e73\u5747\u6fc0\u6d3b\u83b7\u53d6\u63a7\u5236\u7cfb\u6570\uff0c\u5b9e\u73b0\u5168\u81ea\u52a8\u5316\u6d41\u7a0b", "result": "\u5728Gemma 2 2B\u548cLLaMA 3.1 8B\u4e0a\uff0cQA\u3001\u504f\u89c1\u7f13\u89e3\u3001\u8d8a\u72f1\u9884\u9632\u548c\u63a8\u7406\u57fa\u51c6\u4efb\u52a1\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cMMLU\u6027\u80fd\u63d0\u9ad84.1%\uff0cHarmBench\u63d0\u534722.9%\uff08\u4ec5\u97004000\u6837\u672c\uff09", "conclusion": "\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u7279\u5f81\u9009\u62e9\u662f\u81ea\u52a8\u5316SAE\u63a7\u5236\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u6240\u9009\u7279\u5f81\u5c55\u73b0\u51fa\u4e0e\u4efb\u52a1\u9700\u6c42\u4e00\u81f4\u7684\u8bed\u4e49\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u9a71\u52a8\u6027\u80fd\u7684\u5e95\u5c42\u80fd\u529b"}}
{"id": "2508.12271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12271", "abs": "https://arxiv.org/abs/2508.12271", "authors": ["Ronghua Xu", "Jin Xie", "Jing Nie", "Jiale Cao", "Yanwei Pang"], "title": "SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration", "comment": "11 pages", "summary": "Spiking Neural Networks (SNNs), characterized by discrete binary activations,\noffer high computational efficiency and low energy consumption, making them\nwell-suited for computation-intensive tasks such as stereo image restoration.\nIn this work, we propose SNNSIR, a simple yet effective Spiking Neural Network\nfor Stereo Image Restoration, specifically designed under the spike-driven\nparadigm where neurons transmit information through sparse, event-based binary\nspikes. In contrast to existing hybrid SNN-ANN models that still rely on\noperations such as floating-point matrix division or exponentiation, which are\nincompatible with the binary and event-driven nature of SNNs, our proposed\nSNNSIR adopts a fully spike-driven architecture to achieve low-power and\nhardware-friendly computation. To address the expressiveness limitations of\nbinary spiking neurons, we first introduce a lightweight Spike Residual Basic\nBlock (SRBB) to enhance information flow via spike-compatible residual\nlearning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)\nmodule introduces simplified nonlinearity through element-wise multiplication\nand highlights noise-sensitive regions via cross-view-aware modulation.\nComplementing this, the Spike Stereo Cross-Attention (SSCA) module further\nimproves stereo correspondence by enabling efficient bidirectional feature\ninteraction across views within a spike-compatible framework. Extensive\nexperiments on diverse stereo image restoration tasks, including rain streak\nremoval, raindrop removal, low-light enhancement, and super-resolution\ndemonstrate that our model achieves competitive restoration performance while\nsignificantly reducing computational overhead. These results highlight the\npotential for real-time, low-power stereo vision applications. The code will be\navailable after the article is accepted.", "AI": {"tldr": "\u63d0\u51faSNNSIR\uff0c\u4e00\u79cd\u5168\u805a\u5408\u5237\u65b0\u7f51\u7edc\u7ed3\u6784\uff0c\u901a\u8fc7\u5237\u65b0\u6b8b\u5dee\u57fa\u7840\u5757\u3001\u5237\u65b0\u53cc\u76ee\u5377\u79ef\u8c03\u5236\u548c\u5237\u65b0\u53cc\u76ee\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u4f4e\u529f\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u80fd\u7684\u53cc\u76ee\u56fe\u50cf\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u7684\u6df7\u5408SNN-ANN\u6a21\u578b\u4ecd\u4f9d\u8d56\u6d6e\u70b9\u8fd0\u7b97\uff0c\u4e0eSNN\u7684\u4e8c\u8fdb\u5236\u548c\u4e8b\u4ef6\u9a71\u52a8\u6027\u8d28\u4e0d\u517c\u5bb9\u3002\u9700\u8981\u4e00\u79cd\u5168\u5237\u65b0\u9a71\u52a8\u7684\u7b80\u5355\u6709\u6548\u7ed3\u6784\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u529f\u8017\u786c\u4ef6\u53cb\u597d\u7684\u53cc\u76ee\u56fe\u50cf\u6062\u590d\u3002", "method": "1. \u8f7b\u91cf\u5237\u65b0\u6b8b\u5dee\u57fa\u7840\u5757(SRBB)\u589e\u5f3a\u4fe1\u606f\u6d41 2. \u5237\u65b0\u53cc\u76ee\u5377\u79ef\u8c03\u5236(SSCM)\u6a21\u5757\u901a\u8fc7\u5143\u7d20\u4e58\u6cd5\u5b9e\u73b0\u7b80\u5316\u975e\u7ebf\u6027 3. \u5237\u65b0\u53cc\u76ee\u4ea4\u53c9\u6ce8\u610f\u529b(SSCA)\u6a21\u5757\u652f\u6301\u53cc\u5411\u7279\u5f81\u4ea4\u4e92", "result": "\u5728\u96e8\u7eb9\u53bb\u9664\u3001\u96e8\u6ef4\u53bb\u9664\u3001\u4f4e\u5149\u589e\u5f3a\u3001\u8d85\u5206\u8fa8\u7387\u7b49\u591a\u79cd\u53cc\u76ee\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6062\u590d\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "SNNSIR\u6f14\u793a\u4e86\u5237\u65b0\u795e\u7ecf\u7f51\u7edc\u5728\u53cc\u76ee\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u65f6\u4f4e\u529f\u8017\u53cc\u76ee\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12569", "categories": ["cs.LG", "cs.CE", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.12569", "abs": "https://arxiv.org/abs/2508.12569", "authors": ["Quercus Hernandez", "Max Win", "Thomas C. O'Connor", "Paulo E. Arratia", "Nathaniel Trask"], "title": "Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems", "comment": "34 pages, 12 figures", "summary": "Multiscale systems are ubiquitous in science and technology, but are\nnotoriously challenging to simulate as short spatiotemporal scales must be\nappropriately linked to emergent bulk physics. When expensive high-dimensional\ndynamical systems are coarse-grained into low-dimensional models, the entropic\nloss of information leads to emergent physics which are dissipative,\nhistory-dependent, and stochastic. To machine learn coarse-grained dynamics\nfrom time-series observations of particle trajectories, we propose a framework\nusing the metriplectic bracket formalism that preserves these properties by\nconstruction; most notably, the framework guarantees discrete notions of the\nfirst and second laws of thermodynamics, conservation of momentum, and a\ndiscrete fluctuation-dissipation balance crucial for capturing non-equilibrium\nstatistics. We introduce the mathematical framework abstractly before\nspecializing to a particle discretization. As labels are generally unavailable\nfor entropic state variables, we introduce a novel self-supervised learning\nstrategy to identify emergent structural variables. We validate the method on\nbenchmark systems and demonstrate its utility on two challenging examples: (1)\ncoarse-graining star polymers at challenging levels of coarse-graining while\npreserving non-equilibrium statistics, and (2) learning models from high-speed\nvideo of colloidal suspensions that capture coupling between local\nrearrangement events and emergent stochastic dynamics. We provide open-source\nimplementations in both PyTorch and LAMMPS, enabling large-scale inference and\nextensibility to diverse particle-based systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5ea6\u91cf-\u8f9b\u62ec\u53f7\u5f62\u5f0f\u7684\u591a\u5c3a\u5ea6\u7cfb\u7edf\u7c97\u7c92\u5316\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u4fdd\u6301\u70ed\u529b\u5b66\u5b9a\u5f8b\u3001\u52a8\u91cf\u5b88\u6052\u548c\u6da8\u843d-\u8017\u6563\u5e73\u8861\uff0c\u7528\u4e8e\u4ece\u7c92\u5b50\u8f68\u8ff9\u65f6\u95f4\u5e8f\u5217\u4e2d\u5b66\u4e60\u7c97\u7c92\u5316\u52a8\u529b\u5b66\u3002", "motivation": "\u591a\u5c3a\u5ea6\u7cfb\u7edf\u6a21\u62df\u5177\u6709\u6311\u6218\u6027\uff0c\u7c97\u7c92\u5316\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u71b5\u635f\u5931\u5bfc\u81f4\u51fa\u73b0\u8017\u6563\u6027\u3001\u5386\u53f2\u4f9d\u8d56\u6027\u548c\u968f\u673a\u6027\u7b49\u6d8c\u73b0\u7269\u7406\u73b0\u8c61\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u8fd9\u4e9b\u7279\u6027\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5ea6\u91cf-\u8f9b\u62ec\u53f7\u5f62\u5f0f\u4e3b\u4e49\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u8bc6\u522b\u6d8c\u73b0\u7ed3\u6784\u53d8\u91cf\uff0c\u5728\u7c92\u5b50\u79bb\u6563\u5316\u4e2d\u5b9e\u73b0\uff0c\u4fdd\u8bc1\u70ed\u529b\u5b66\u7b2c\u4e00\u548c\u7b2c\u4e8c\u5b9a\u5f8b\u3001\u52a8\u91cf\u5b88\u6052\u4ee5\u53ca\u79bb\u6563\u6da8\u843d-\u8017\u6563\u5e73\u8861\u3002", "result": "\u5728\u57fa\u51c6\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u661f\u5f62\u805a\u5408\u7269\u7684\u6311\u6218\u6027\u7c97\u7c92\u5316\u6c34\u5e73\uff0c\u5e76\u80fd\u591f\u4ece\u80f6\u4f53\u60ac\u6d6e\u6db2\u7684\u9ad8\u901f\u89c6\u9891\u4e2d\u5b66\u4e60\u8026\u5408\u5c40\u90e8\u91cd\u6392\u4e8b\u4ef6\u4e0e\u6d8c\u73b0\u968f\u673a\u52a8\u529b\u5b66\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u5c3a\u5ea6\u7cfb\u7edf\u7684\u7c97\u7c92\u5316\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u6301\u5173\u952e\u7269\u7406\u7279\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86PyTorch\u548cLAMMPS\u7684\u5f00\u6e90\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u7c92\u5b50\u7cfb\u7edf\u3002"}}
{"id": "2508.12381", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12381", "abs": "https://arxiv.org/abs/2508.12381", "authors": ["Guo Tang", "Songhan Jiang", "Jinpeng Lu", "Linghan Cai", "Yongbing Zhang"], "title": "IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis", "comment": "13 pages, 5 figures", "summary": "Pathological images play an essential role in cancer prognosis, while\nsurvival analysis, which integrates computational techniques, can predict\ncritical clinical events such as patient mortality or disease recurrence from\nwhole-slide images (WSIs). Recent advancements in multiple instance learning\nhave significantly improved the efficiency of survival analysis. However,\nexisting methods often struggle to balance the modeling of long-range spatial\nrelationships with local contextual dependencies and typically lack inherent\ninterpretability, limiting their clinical utility. To address these challenges,\nwe propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel\nframework that captures the characteristics of the tumor microenvironment and\nmodels their spatial dependencies across the tissue. IPGPhormer uniquely\nprovides interpretability at both tissue and cellular levels without requiring\npost-hoc manual annotations, enabling detailed analyses of individual WSIs and\ncross-cohort assessments. Comprehensive evaluations on four public benchmark\ndatasets demonstrate that IPGPhormer outperforms state-of-the-art methods in\nboth predictive accuracy and interpretability. In summary, our method,\nIPGPhormer, offers a promising tool for cancer prognosis assessment, paving the\nway for more reliable and interpretable decision-support systems in pathology.\nThe code is publicly available at\nhttps://anonymous.4open.science/r/IPGPhormer-6EEB.", "AI": {"tldr": "\u63d0\u51faIPGPhormer\u6846\u67b6\uff0c\u901a\u8fc7\u56fe-Transformer\u7ed3\u6784\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u751f\u5b58\u5206\u6790\u4e2d\u957f\u8ddd\u79bb\u7a7a\u95f4\u5173\u7cfb\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u7ec4\u7ec7\u7ea7\u548c\u7ec6\u80de\u7ea7\u7684\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u957f\u8ddd\u79bb\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u4f9d\u8d56\uff0c\u4e14\u7f3a\u4e4f\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027", "method": "Interpretable Pathology Graph-Transformer (IPGPhormer)\u6846\u67b6\uff0c\u6355\u83b7\u80bf\u7624\u5fae\u73af\u5883\u7279\u5f81\u5e76\u5efa\u6a21\u7ec4\u7ec7\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u540e\u5904\u7406\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u63d0\u4f9b\u591a\u7ea7\u53ef\u89e3\u91ca\u6027", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5168\u9762\u8bc4\u4f30\uff0cIPGPhormer\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "IPGPhormer\u4e3a\u764c\u75c7\u9884\u540e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u4e3a\u75c5\u7406\u5b66\u4e2d\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2508.12673", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12673", "abs": "https://arxiv.org/abs/2508.12673", "authors": ["Yuhao Zhou", "Jindi Lv", "Yuxin Tian", "Dan Si", "Qing Ye", "Jiancheng Lv"], "title": "Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach", "comment": "17 pages", "summary": "Federated Learning (FL) has emerged as a promising paradigm for\nprivacy-preserving collaborative learning, yet data heterogeneity remains a\ncritical challenge. While existing methods achieve progress in addressing data\nheterogeneity for participating clients, they fail to generalize to\nnon-participating clients with in-domain distribution shifts and resource\nconstraints. To mitigate this issue, we present HyperFedZero, a novel method\nthat dynamically generates specialized models via a hypernetwork conditioned on\ndistribution-aware embeddings. Our approach explicitly incorporates\ndistribution-aware inductive biases into the model's forward pass, extracting\nrobust distribution embeddings using a NoisyEmbed-enhanced extractor with a\nBalancing Penalty, effectively preventing feature collapse. The hypernetwork\nthen leverages these embeddings to generate specialized models chunk-by-chunk\nfor non-participating clients, ensuring adaptability to their unique data\ndistributions. Extensive experiments on multiple datasets and models\ndemonstrate HyperFedZero's remarkable performance, surpassing competing methods\nconsistently with minimal computational, storage, and communication overhead.\nMoreover, ablation studies and visualizations further validate the necessity of\neach component, confirming meaningful adaptations and validating the\neffectiveness of HyperFedZero.", "AI": {"tldr": "HyperFedZero\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u9488\u5bf9\u975e\u53c2\u4e0e\u5ba2\u6237\u7aef\u7684\u4e13\u7528\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6784\u6027\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u5177\u6709\u57df\u5185\u5206\u5e03\u504f\u79fb\u548c\u8d44\u6e90\u7ea6\u675f\u7684\u975e\u53c2\u4e0e\u5ba2\u6237\u7aef", "method": "\u4f7f\u7528\u57fa\u4e8e\u5206\u5e03\u611f\u77e5\u5d4c\u5165\u7684\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u4e13\u7528\u6a21\u578b\uff0c\u91c7\u7528NoisyEmbed\u589e\u5f3a\u7684\u63d0\u53d6\u5668\u548c\u5e73\u8861\u60e9\u7f5a\u6765\u9632\u6b62\u7279\u5f81\u5d29\u6e83\uff0c\u5206\u5757\u751f\u6210\u6a21\u578b\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5206\u5e03", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHyperFedZero\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0c\u8ba1\u7b97\u3001\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u6700\u5c0f", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u975e\u53c2\u4e0e\u5ba2\u6237\u7aef\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u6d88\u878d\u7814\u7a76\u548c\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\uff0c\u8bc1\u660e\u4e86HyperFedZero\u7684\u6709\u6548\u6027"}}
{"id": "2508.13152", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13152", "abs": "https://arxiv.org/abs/2508.13152", "authors": ["Xin Chen", "Junchao Wu", "Shu Yang", "Runzhe Zhan", "Zeyu Wu", "Ziyang Luo", "Di Wang", "Min Yang", "Lidia S. Chao", "Derek F. Wong"], "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns", "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version", "summary": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard", "AI": {"tldr": "\u57fa\u4e8eLLM\u5185\u90e8\u8868\u5f81\u7684\u7edf\u8ba1\u68c0\u6d4b\u65b9\u6cd5RepreGuard\uff0c\u5728\u5185\u90e8\u548c\u5916\u90e8\u5206\u5e03\u573a\u666f\u4e0b\u90fd\u80fd\u9ad8\u6548\u8bc6\u522bAI\u751f\u6210\u6587\u672c\uff0c\u5e73\u5747AUROC\u8fbe94.92%", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u5916\u90e8\u5206\u5e03\u573a\u666f\u4e0b\u7a33\u5065\u6027\u4e0d\u8db3\uff0cLLM\u5185\u90e8\u8868\u5f81\u5305\u542b\u66f4\u5168\u9762\u7684\u7edf\u8ba1\u7279\u5f81\u80fd\u66f4\u6709\u6548\u533a\u5206AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u6587\u672c", "method": "\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u6536\u96c6LGT\u548cHWT\u7684\u8868\u5f81\uff0c\u63d0\u53d6\u7279\u5f02\u6fc0\u6d3b\u7279\u5f81\uff0c\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u8868\u5f81\u5728\u8be5\u7279\u5f81\u65b9\u5411\u4e0a\u7684\u6295\u5f71\u5f97\u5206\u5e76\u4e0e\u9884\u8ba1\u7b97\u9608\u503c\u6bd4\u8f83\u6765\u5206\u7c7b", "result": "\u5728\u5185\u90e8\u548c\u5916\u90e8\u5206\u5e03\u573a\u666f\u4e0b\u90fd\u8d85\u8fc7\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747AUROC\u8fbe94.92%\uff0c\u5bf9\u4e0d\u540c\u6587\u672c\u957f\u5ea6\u548c\u4e3b\u6d41\u653b\u51fb\u90fd\u4f53\u73b0\u51fa\u7a33\u5065\u8010\u53d7\u6027", "conclusion": "LLM\u5185\u90e8\u8868\u5f81\u786e\u5b9e\u5305\u542b\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u7279\u5f81\uff0cRepreGuard\u65b9\u6cd5\u901a\u8fc7\u7edf\u8ba1\u6295\u5f71\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7a33\u5065\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b"}}
{"id": "2508.12885", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12885", "abs": "https://arxiv.org/abs/2508.12885", "authors": ["Aleksei Liuliakov", "Alexander Schulz", "Luca Hermes", "Barbara Hammer"], "title": "One-Class Intrusion Detection with Dynamic Graphs", "comment": null, "summary": "With the growing digitalization all over the globe, the relevance of network\nsecurity becomes increasingly important. Machine learning-based intrusion\ndetection constitutes a promising approach for improving security, but it bears\nseveral challenges. These include the requirement to detect novel and unseen\nnetwork events, as well as specific data properties, such as events over time\ntogether with the inherent graph structure of network communication. In this\nwork, we propose a novel intrusion detection method, TGN-SVDD, which builds\nupon modern dynamic graph modelling and deep anomaly detection. We demonstrate\nits superiority over several baselines for realistic intrusion detection data\nand suggest a more challenging variant of the latter.", "AI": {"tldr": "\u63d0\u51faTGN-SVDD\u65b9\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u56fe\u5efa\u6a21\u548c\u6df1\u5ea6\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u5165\u4fb5\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u968f\u7740\u5168\u7403\u6570\u5b57\u5316\u53d1\u5c55\uff0c\u7f51\u7edc\u5b89\u5168\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u673a\u5668\u5b66\u4e60\u5165\u4fb5\u68c0\u6d4b\u9762\u4e34\u68c0\u6d4b\u65b0\u578b\u672a\u77e5\u7f51\u7edc\u4e8b\u4ef6\u3001\u5904\u7406\u65f6\u5e8f\u6570\u636e\u548c\u7f51\u7edc\u901a\u4fe1\u56fe\u7ed3\u6784\u7b49\u6311\u6218", "method": "\u57fa\u4e8e\u73b0\u4ee3\u52a8\u6001\u56fe\u5efa\u6a21\u548c\u6df1\u5ea6\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\uff0c\u63d0\u51faTGN-SVDD\u65b9\u6cd5", "result": "\u5728\u771f\u5b9e\u5165\u4fb5\u68c0\u6d4b\u6570\u636e\u4e0a\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u591a\u4e2a\u57fa\u7ebf\u7684\u4f18\u8d8a\u6027", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5177\u6311\u6218\u6027\u7684\u5165\u4fb5\u68c0\u6d4b\u6570\u636e\u53d8\u4f53\uff0cTGN-SVDD\u65b9\u6cd5\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f"}}
{"id": "2508.13099", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.13099", "abs": "https://arxiv.org/abs/2508.13099", "authors": ["Mingyu Kim", "Daniel Stilwell", "Jorge Jimenez"], "title": "Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network", "comment": "IEEE OCEANS", "summary": "This paper presents a framework for classifying and detecting spatial\ncommission outliers in maritime environments using seabed acoustic sensor\nnetworks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as\na mixture of normal and outlier processes, we estimate the probability that a\nnewly observed event is an outlier. We propose a second-order approximation of\nthis probability that incorporates both the mean and variance of the normal\nintensity function, providing improved classification accuracy compared to\nmean-only approaches. We analytically show that our method yields a tighter\nbound to the true probability using Jensen's inequality. To enhance detection,\nwe integrate a real-time, near-optimal sensor placement strategy that\ndynamically adjusts sensor locations based on the evolving outlier intensity.\nThe proposed framework is validated using real ship traffic data near Norfolk,\nVirginia, where numerical results demonstrate the effectiveness of our approach\nin improving both classification performance and outlier detection through\nsensor deployment.", "AI": {"tldr": "\u57fa\u4e8e\u6d77\u5e95\u58f0\u5b66\u4f20\u611f\u5668\u7f51\u7edc\u548cLGCP\u6a21\u578b\u7684\u6d77\u4e0a\u7a7a\u95f4\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e8c\u9636\u6982\u7387\u8fd1\u4f3c\u548c\u52a8\u6001\u4f20\u611f\u5668\u90e8\u7f72\u63d0\u9ad8\u5f02\u5e38\u5206\u7c7b\u548c\u68c0\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5f3a\u5ea6\u51fd\u6570\u5747\u503c\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u7cbe\u5ea6\u6709\u9650\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8003\u8651\u5747\u503c\u548c\u65b9\u5dee\u7684\u66f4\u7cbe\u786e\u5f02\u5e38\u6982\u7387\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u4f18\u5316\u4f20\u611f\u5668\u90e8\u7f72\u4ee5\u63d0\u9ad8\u5b9e\u65f6\u68c0\u6d4b\u80fd\u529b", "method": "\u4f7f\u7528\u5bf9\u6570\u9ad8\u65afCox\u8fc7\u7a0b(LGCP)\u5bf9\u76ee\u6807\u5230\u8fbe\u8fdb\u884c\u5efa\u6a21\uff0c\u5c06\u6b63\u5e38\u548c\u5f02\u5e38\u8fc7\u7a0b\u6df7\u5408\u3002\u63d0\u51fa\u4e8c\u9636\u6982\u7387\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u6b63\u5e38\u5f3a\u5ea6\u51fd\u6570\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002\u7ed3\u5408\u5b9e\u65f6\u8fd1\u6700\u4f18\u4f20\u611f\u5668\u90e8\u7f72\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u4f4d\u7f6e", "result": "\u5728\u5f17\u5409\u5c3c\u4e9a\u5dde\u8bfa\u798f\u514b\u771f\u5b9e\u8239\u8236\u4ea4\u901a\u6570\u636e\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5747\u503c\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7684\u4f20\u611f\u5668\u90e8\u7f72\u589e\u5f3a\u4e86\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b", "conclusion": "\u63d0\u51fa\u7684\u4e8c\u9636\u6982\u7387\u8fd1\u4f3c\u6846\u67b6\u4e3a\u6d77\u4e0a\u7a7a\u95f4\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u52a8\u6001\u4f20\u611f\u5668\u90e8\u7f72\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5b9e\u65f6\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u6d77\u6d0b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2508.12784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12784", "abs": "https://arxiv.org/abs/2508.12784", "authors": ["Dan Ruta", "Abdelaziz Djelouah", "Raphael Ortiz", "Christopher Schroers"], "title": "Leveraging Diffusion Models for Stylization using Multiple Style Images", "comment": null, "summary": "Recent advances in latent diffusion models have enabled exciting progress in\nimage style transfer. However, several key issues remain. For example, existing\nmethods still struggle to accurately match styles. They are often limited in\nthe number of style images that can be used. Furthermore, they tend to entangle\ncontent and style in undesired ways. To address this, we propose leveraging\nmultiple style images which helps better represent style features and prevent\ncontent leaking from the style images. We design a method that leverages both\nimage prompt adapters and statistical alignment of the features during the\ndenoising process. With this, our approach is designed such that it can\nintervene both at the cross-attention and the self-attention layers of the\ndenoising UNet. For the statistical alignment, we employ clustering to distill\na small representative set of attention features from the large number of\nattention values extracted from the style samples. As demonstrated in our\nexperimental section, the resulting method achieves state-of-the-art results\nfor stylization.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6837\u5f0f\u56fe\u50cf\u7ed3\u5408\u56fe\u50cf\u63d0\u793a\u9002\u914d\u5668\u548c\u7edf\u8ba1\u5bf9\u9f50\u6280\u672f\uff0c\u5728\u53bb\u566aUNet\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u5e72\u9884\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u6548\u679c", "motivation": "\u89e3\u51b3\u73b0\u6709\u6f5c\u5723\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u4e2d\u5b58\u5728\u7684\u98ce\u683c\u5339\u914d\u4e0d\u51c6\u3001\u53ef\u7528\u98ce\u683c\u56fe\u50cf\u6570\u91cf\u6709\u9650\u3001\u5185\u5bb9\u4e0e\u98ce\u683c\u6052\u7ed3\u7b49\u95ee\u9898", "method": "\u5229\u7528\u591a\u5f20\u98ce\u683c\u56fe\u50cf\u6765\u66f4\u597d\u8868\u5f81\u98ce\u683c\u7279\u5f81\uff0c\u907f\u514d\u98ce\u683c\u56fe\u50cf\u4e2d\u7684\u5185\u5bb9\u6cc4\u6f0f\uff1b\u8bbe\u8ba1\u4e86\u7ed3\u5408\u56fe\u50cf\u63d0\u793a\u9002\u914d\u5668\u548c\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7279\u5f81\u7edf\u8ba1\u5bf9\u9f50\u7684\u65b9\u6cd5\uff1b\u5728\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u5e72\u9884\uff1b\u91c7\u7528\u805a\u7c7b\u6280\u672f\u4ece\u5927\u91cf\u98ce\u683c\u6837\u672c\u5173\u6ce8\u7279\u5f81\u4e2d\u7cbe\u70bc\u51fa\u5c0f\u89c4\u6a21\u4ee3\u8868\u6027\u96c6\u5408", "result": "\u8be5\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u76ee\u524d\u6700\u597d\u7684\u7ed3\u679c", "conclusion": "\u901a\u8fc7\u591a\u98ce\u683c\u56fe\u50cf\u7ed3\u5408\u548c\u591a\u5c42\u6b21\u5e72\u9884\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u98ce\u683c\u63a7\u5236"}}
{"id": "2508.12813", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12813", "abs": "https://arxiv.org/abs/2508.12813", "authors": ["Friedhelm Hamann", "Emil Mededovic", "Fabian G\u00fclhan", "Yuli Wu", "Johannes Stegmaier", "Jing He", "Yiqing Wang", "Kexin Zhang", "Lingling Li", "Licheng Jiao", "Mengru Ma", "Hongxiang Huang", "Yuhao Yan", "Hongwei Ren", "Xiaopeng Lin", "Yulong Huang", "Bojun Cheng", "Se Hyun Lee", "Gyu Sung Ham", "Kanghan Oh", "Gi Hyun Lim", "Boxuan Yang", "Bowen Du", "Guillermo Gallego"], "title": "SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop", "comment": "13 pages, 7 figures, 7 tables", "summary": "We present an overview of the Spatio-temporal Instance Segmentation (SIS)\nchallenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.\nThe task is to predict accurate pixel-level segmentation masks of defined\nobject classes from spatio-temporally aligned event camera and grayscale camera\ndata. We provide an overview of the task, dataset, challenge details and\nresults. Furthermore, we describe the methods used by the top-5 ranking teams\nin the challenge. More resources and code of the participants' methods are\navailable here:\nhttps://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md", "AI": {"tldr": "CVPR 2025\u4e8b\u4ef6\u89c6\u89c9\u7814\u8ba8\u4f1a\u4e3e\u529e\u7684\u65f6\u7a7a\u5b9e\u4f8b\u5206\u5272\u6311\u6218\u8d5b\u6982\u8ff0\uff0c\u5305\u542b\u4efb\u52a1\u63cf\u8ff0\u3001\u6570\u636e\u96c6\u3001\u6311\u6218\u7ec6\u8282\u548c\u7ed3\u679c\u5206\u6790\uff0c\u4ee5\u53ca\u524d5\u540d\u56e2\u961f\u7684\u65b9\u6cd5\u4ecb\u7ecd", "motivation": "\u63a8\u52a8\u4e8b\u4ef6\u76f8\u673a\u548c\u7070\u5ea6\u76f8\u673a\u6570\u636e\u878d\u5408\u7684\u65f6\u7a7a\u5b9e\u4f8b\u5206\u5272\u6280\u672f\u53d1\u5c55\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u793e\u533a\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0", "method": "\u7ec4\u7ec7\u6311\u6218\u8d5b\u5e76\u63d0\u4f9b\u6570\u636e\u96c6\uff0c\u6536\u96c6\u53c2\u4e0e\u8005\u63d0\u4ea4\u7684\u50cf\u7d20\u7ea7\u5206\u5272\u65b9\u6cd5\uff0c\u5bf9\u524d5\u540d\u56e2\u961f\u7684\u6280\u672f\u65b9\u6848\u8fdb\u884c\u5206\u6790\u603b\u7ed3", "result": "\u6210\u529f\u4e3e\u529e\u4e86\u65f6\u7a7a\u5b9e\u4f8b\u5206\u5272\u6311\u6218\u8d5b\uff0c\u6536\u96c6\u4e86\u591a\u4e2a\u56e2\u961f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e0e\u7070\u5ea6\u76f8\u673a\u6570\u636e\u878d\u5408\u5728\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u7684\u65f6\u7a7a\u5b9e\u4f8b\u5206\u5272\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u63a8\u52a8\u529b\uff0c\u524d5\u540d\u56e2\u961f\u7684\u65b9\u6cd5\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003"}}
{"id": "2508.12932", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12932", "abs": "https://arxiv.org/abs/2508.12932", "authors": ["Hongyang Chen", "Shaoling Pu", "Lingyu Zheng", "Zhongwu Sun"], "title": "SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory", "comment": "Accepted by ICONIP2025", "summary": "In incremental learning, enhancing the generality of knowledge is crucial for\nadapting to dynamic data inputs. It can develop generalized representations or\nmore balanced decision boundaries, preventing the degradation of long-term\nknowledge over time and thus mitigating catastrophic forgetting. Some emerging\nincremental learning methods adopt an encoder-decoder architecture and have\nachieved promising results. In the encoder-decoder achitecture, improving the\ngeneralization capabilities of both the encoder and decoder is critical, as it\nhelps preserve previously learned knowledge while ensuring adaptability and\nrobustness to new, diverse data inputs. However, many existing continual\nmethods focus solely on enhancing one of the two components, which limits their\neffectiveness in mitigating catastrophic forgetting. And these methods perform\neven worse in small-memory scenarios, where only a limited number of historical\nsamples can be stored. To mitigate this limitation, we introduces SEDEG, a\ntwo-stage training framework for vision transformers (ViT), focusing on\nsequentially improving the generality of both Decoder and Encoder. Initially,\nSEDEG trains an ensembled encoder through feature boosting to learn generalized\nrepresentations, which subsequently enhance the decoder's generality and\nbalance the classifier. The next stage involves using knowledge distillation\n(KD) strategies to compress the ensembled encoder and develop a new, more\ngeneralized encoder. This involves using a balanced KD approach and feature KD\nfor effective knowledge transfer. Extensive experiments on three benchmark\ndatasets show SEDEG's superior performance, and ablation studies confirm the\nefficacy of its components. The code is available at\nhttps://github.com/ShaolingPu/CIL.", "AI": {"tldr": "SEDEG\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u5347\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6cdb\u5316\u80fd\u529b\u6765\u7f13\u89e3\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7279\u522b\u5728\u5c0f\u5185\u5b58\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\u4e2d\u7684\u4e00\u4e2a\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5c0f\u5185\u5b58\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u5dee", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7279\u5f81\u589e\u5f3a\u8bad\u7ec3\u96c6\u6210\u7f16\u7801\u5668\u5b66\u4e60\u6cdb\u5316\u8868\u793a\uff0c\u63d0\u5347\u89e3\u7801\u5668\u6cdb\u5316\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u538b\u7f29\u96c6\u6210\u7f16\u7801\u5668\uff0c\u5f00\u53d1\u65b0\u7684\u6cdb\u5316\u7f16\u7801\u5668", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793aSEDEG\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027", "conclusion": "SEDEG\u901a\u8fc7\u987a\u5e8f\u63d0\u5347\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7279\u522b\u9002\u5408\u5c0f\u5185\u5b58\u573a\u666f"}}
{"id": "2508.13000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13000", "abs": "https://arxiv.org/abs/2508.13000", "authors": ["Zhangyong Tang", "Tianyang Xu", "Xuefeng Zhu", "Hui Li", "Shaochuan Zhao", "Tao Zhou", "Chunyang Cheng", "Xiaojun Wu", "Josef Kittler"], "title": "Omni Survey for Multimodality Analysis in Visual Object Tracking", "comment": "The first comprehensive survey for multi-modal visual object\n  tracking; 6 multi-modal tasks; 338 references", "summary": "The development of smart cities has led to the generation of massive amounts\nof multi-modal data in the context of a range of tasks that enable a\ncomprehensive monitoring of the smart city infrastructure and services. This\npaper surveys one of the most critical tasks, multi-modal visual object\ntracking (MMVOT), from the perspective of multimodality analysis. Generally,\nMMVOT differs from single-modal tracking in four key aspects, data collection,\nmodality alignment and annotation, model designing, and evaluation.\nAccordingly, we begin with an introduction to the relevant data modalities,\nlaying the groundwork for their integration. This naturally leads to a\ndiscussion of challenges of multi-modal data collection, alignment, and\nannotation. Subsequently, existing MMVOT methods are categorised, based on\ndifferent ways to deal with visible (RGB) and X modalities: programming the\nauxiliary X branch with replicated or non-replicated experimental\nconfigurations from the RGB branch. Here X can be thermal infrared (T), depth\n(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part\nof the paper addresses evaluation and benchmarking. In summary, we undertake an\nomni survey of all aspects of multi-modal visual object tracking (VOT),\ncovering six MMVOT tasks and featuring 338 references in total. In addition, we\ndiscuss the fundamental rhetorical question: Is multi-modal tracking always\nguaranteed to provide a superior solution to unimodal tracking with the help of\ninformation fusion, and if not, in what circumstances its application is\nbeneficial. Furthermore, for the first time in this field, we analyse the\ndistributions of the object categories in the existing MMVOT datasets,\nrevealing their pronounced long-tail nature and a noticeable lack of animal\ncategories when compared with RGB datasets.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u4e2a\u5173\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a(MMVOT)\u7684\u5168\u9762\u8c03\u7814\u62a5\u544a\uff0c\u6db5\u76d6\u4e86\u6570\u636e\u91c7\u96c6\u3001\u6a21\u578b\u8bbe\u8ba1\u3001\u8bc4\u4f30\u6807\u51c6\u7b49\u6240\u6709\u65b9\u9762\uff0c\u5305\u542b6\u4e2aMMVOT\u4efb\u52a1\u548c338\u7bc7\u53c2\u8003\u6587\u732e\u3002", "motivation": "\u667a\u6167\u57ce\u5e02\u4ea7\u751f\u5927\u91cf\u591a\u6a21\u6001\u6570\u636e\uff0c\u9700\u8981\u7efc\u5408\u76d1\u63a7\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u548c\u670d\u52a1\u3002\u591a\u6a21\u6001\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u4f5c\u4e3a\u5173\u952e\u4efb\u52a1\uff0c\u9700\u8981\u4ece\u591a\u6a21\u6001\u89d2\u5ea6\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u4ece\u56db\u4e2a\u5173\u952e\u65b9\u9762\u5bf9\u6bd4\u591a\u6a21\u6001\u4e0e\u5355\u6a21\u6001\u8ddf\u8e2a\u7684\u5dee\u5f02\uff1a\u6570\u636e\u91c7\u96c6\u3001\u6a21\u6001\u5bf9\u9f50\u548c\u6807\u6ce8\u3001\u6a21\u578b\u8bbe\u8ba1\u3001\u8bc4\u4f30\u3002\u5bf9\u73b0\u6709\u65b9\u6cd5\u6309\u5904\u7406\u53ef\u89c1\u5149(RGB)\u548c\u5176\u4ed6\u6a21\u6001(X)\u7684\u4e0d\u540c\u65b9\u5f0f\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709MMVOT\u6570\u636e\u96c6\u4e2d\u5bf9\u8c61\u7c7b\u522b\u7684\u660e\u663e\u957f\u5c3e\u5206\u5e03\u7279\u5f81\uff0c\u4ee5\u53ca\u4e0eRGB\u6570\u636e\u96c6\u76f8\u6bd4\u52a8\u7269\u7c7b\u522b\u663e\u8457\u7f3a\u4e4f\u7684\u95ee\u9898\u3002\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u8ddf\u8e2a\u662f\u5426\u603b\u662f\u66f4\u4f18\u4e8e\u5355\u6a21\u6001\u8ddf\u8e2a\u7684\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u9886\u57df\u7684\u5168\u9762\u8c03\u7814\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u77e5\u8bc6\u57fa\u7840\u548c\u7814\u7a76\u65b9\u5411\u3002\u6301\u7eed\u7684\u6570\u636e\u96c6\u504f\u5dee\u95ee\u9898\u9700\u8981\u5f97\u5230\u89e3\u51b3\u3002"}}
