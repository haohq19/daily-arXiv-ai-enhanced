<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models](https://arxiv.org/abs/2601.03369)
*Sha Luo,Yogesh Prabhu,Tim Ossowski,Kaiping Chen,Junjie Hu*

Main category: cs.CV

TL;DR: 提出RiskCueBench视频风险预测基准，要求模型从风险信号片段（而非完整视频）预测潜在安全事件，揭示现有系统在早期风险识别上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频风险评估研究通常让模型访问包含事故本身的完整视频序列，这大大降低了任务难度，无法反映真实世界条件。需要更贴近实际应用的评估基准。

Method: 引入RiskCueBench视频理解基准，对视频进行精细标注以识别风险信号片段——即指示潜在安全关注的最早时刻，要求模型基于早期视觉信号预测未来风险事件。

Result: 实验结果显示当前系统在解释演变情况和基于早期视觉信号预测未来风险事件方面存在显著能力差距，突显了视频风险预测模型实际部署的重要挑战。

Conclusion: RiskCueBench基准更好地反映了真实世界条件，揭示了现有视频风险预测系统在早期风险识别方面的不足，为实际部署提出了重要挑战。

Abstract: With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.

</details>


### [2] [Semantic Belief-State World Model for 3D Human Motion Prediction](https://arxiv.org/abs/2601.03517)
*Sarim Chaudhry*

Main category: cs.CV

TL;DR: SBWM将人体运动预测重构为人体流形上的潜在动力学模拟，通过信念状态演化而非直接预测姿态，实现稳定长时程推演


<details>
  <summary>Details</summary>
Motivation: 传统运动预测方法存在复合漂移、平均姿态塌陷和不确定性校准差的问题，因为它们将观测重建与动力学建模混在一起，缺乏对运动潜在原因的显式表示

Method: 提出语义信念状态世界模型(SBWM)，在人体流形上进行潜在动力学模拟。模型维护循环概率信念状态，其演化独立于姿态重建学习，并与SMPL-X解剖参数化对齐，形成结构信息瓶颈

Result: SBWM实现了连贯的长时程推演，在显著降低计算成本的同时保持竞争力精度，优于基于RSSM、transformer和扩散的方法

Conclusion: 将人体视为世界模型状态空间的一部分而非输出，从根本上改变了运动模拟和预测的方式，为稳定长时程预测提供了新范式

Abstract: Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.

</details>


### [3] [Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution](https://arxiv.org/abs/2601.03526)
*Zhicheng Zhao,Fengjiao Peng,Jinquan Yan,Wei Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: PCNet提出了一种用于热成像无人机图像超分辨率的物理约束跨模态增强方法，通过跨分辨率互增强模块和热传导物理约束，解决了现有方法中高频信息丢失和物理不一致伪影的问题。


<details>
  <summary>Details</summary>
Motivation: 现有光学引导的热成像超分辨率方法通常压缩光学特征以匹配热成像特征维度，这会导致对超分辨率有益的高频信息丢失，并且由于忽略模态间成像物理差异而引入纹理扭曲和边缘模糊等物理不一致伪影。

Method: 提出PCNet框架：1) 跨分辨率互增强模块(CRME)联合优化热成像超分辨率和光学到热成像模态转换，实现双向特征交互同时保留高频光学先验；2) 物理驱动热传导模块(PDTM)将二维热传导融入光学引导，建模空间变化的热传导特性；3) 温度一致性损失确保生成的热成像符合真实热辐射原理。

Result: 在VGTSR2.0和DroneVehicle数据集上的大量实验表明，PCNet在重建质量和下游任务（包括语义分割和目标检测）方面显著优于现有最先进方法。

Conclusion: PCNet通过物理约束的跨模态增强方法，实现了鲁棒的热成像无人机图像超分辨率，解决了现有方法中的高频信息丢失和物理不一致伪影问题，在重建质量和下游任务性能上均有显著提升。

Abstract: Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.

</details>


### [4] [MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction](https://arxiv.org/abs/2601.03781)
*Xiaokun Sun,Zezhong Wu,Zewen Ding,Linli Xu*

Main category: cs.CV

TL;DR: 提出MVP（Masked Video Prediction）后训练目标，通过重建被遮蔽的连续视频片段来增强VideoLLMs的时序推理和因果理解能力，使用GRPO和细粒度奖励函数进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的VideoLLMs后训练方法主要关注整体内容理解，缺乏对内在时序连贯性和帧间相关性的显式监督，限制了模型捕捉复杂动态和细粒度视觉因果关系的能力。

Method: 提出MVP后训练目标：要求模型从一组具有挑战性的干扰项中重建被遮蔽的连续视频片段，强制模型关注事件的序列逻辑和时序上下文。引入可扩展的数据合成管道，将任意视频语料库转换为MVP训练样本，并采用带有细粒度奖励函数的GRPO来增强模型对视频上下文和时序属性的理解。

Result: 综合评估表明，MVP通过直接强化时序推理和因果理解，显著提升了视频推理能力。

Conclusion: MVP作为一种新颖的后训练目标，通过显式监督时序连贯性和帧间相关性，有效增强了VideoLLMs的时序推理和因果理解能力，弥补了现有方法在捕捉复杂动态方面的不足。

Abstract: Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.

</details>


### [5] [ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.03955)
*Xu Zhang,Cheng Da,Huan Yang,Kun Gai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: ResTok是一种1D视觉分词器，通过构建分层残差表示来恢复视觉的层次化特性，显著提升自回归图像生成性能，在ImageNet-256上仅需9步采样即可达到2.34 gFID。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言建模设计的1D视觉分词器将视觉数据视为平坦的序列化token流，忽略了视觉的层次化和残差网络设计特性，而这些特性对视觉模型的收敛和效率至关重要。

Method: 提出ResTok，一种构建图像token和潜在token分层残差的1D视觉分词器。通过渐进合并获得分层表示，实现每层的跨层级特征融合；语义残差防止信息重叠，产生更集中的潜在分布。同时引入分层AR生成器，通过一次预测整个层级的潜在token来减少采样步骤。

Result: 在ImageNet-256上仅用9个采样步骤就达到了2.34 gFID，显著提升了自回归图像生成性能。跨层级绑定自然涌现，无需显式约束。

Conclusion: 在视觉分词中恢复分层残差先验能显著改善自回归图像生成，ResTok通过将"视觉"特性重新引入视觉建模，取得了优异性能。

Abstract: Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.

</details>


### [6] [Klear: Unified Multi-Task Audio-Video Joint Generation](https://arxiv.org/abs/2601.04151)
*Jun Wang,Chunyu Qiang,Yuxin Guo,Yiran Wang,Xijuan Zeng,Chen Zhang,Pengfei Wan*

Main category: cs.CV

TL;DR: Klear是一个统一的音频-视频联合生成系统，通过创新的单塔架构、渐进式多任务训练策略和大规模高质量数据集，解决了现有方法中的音频-视觉异步、唇语对齐差和单模态退化等问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视频联合生成方法存在三个主要问题：音频-视觉异步、唇语对齐差、单模态退化。这些问题源于音频-视觉对应建模弱、泛化能力有限、高质量密集标注数据稀缺。

Method: 1. 架构：采用单塔设计，统一DiT块和Omni-Full Attention机制；2. 训练：渐进式多任务策略（随机模态掩码到联合优化）和多阶段课程学习；3. 数据：构建首个大规模带密集标注的音频-视频数据集，使用自动化流水线筛选高质量对齐的三元组。

Result: Klear在各项任务中大幅超越先前方法，性能与Veo 3相当，能够生成高保真、语义和时间对齐、遵循指令的音频-视频内容，在联合和单模态设置下都表现优异，并能鲁棒地泛化到分布外场景。

Conclusion: Klear通过统一的架构、训练策略和高质量数据，为下一代音频-视频合成提供了一个可扩展的路径，解决了现有方法的核心问题，实现了高质量的音频-视觉对齐生成。

Abstract: Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Extreme-value forest fire prediction A study of the Loss Function in an Ordinality Scheme](https://arxiv.org/abs/2601.03327)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: 提出首个用于法国野火严重程度预测的序数分类框架，比较不同损失函数对罕见高严重度火灾预测的影响，发现序数监督显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 野火在空间和严重程度上高度不平衡，预测极端事件具有挑战性。需要开发与法国运营决策直接对齐的野火严重程度预测框架，特别关注罕见但关键的高严重度火灾预测

Method: 引入首个野火严重程度序数分类框架，比较标准交叉熵与多种序数感知目标函数，包括提出的基于截断离散指数广义帕累托分布的概率TDeGPD损失。在多种架构和真实运营数据上进行广泛基准测试

Result: 序数监督显著优于传统方法。加权Kappa损失（WKLoss）表现最佳，在最极端严重度类别上获得超过+0.1 IoU增益，同时保持有竞争力的校准质量。但由于数据集中罕见事件代表性极低，对最罕见事件的预测性能仍然有限

Conclusion: 研究强调了将严重程度排序、数据不平衡考虑和季节性风险整合到野火预测系统中的重要性。未来工作将专注于将季节动态和不确定性信息纳入训练，进一步提高极端事件预测的可靠性

Abstract: Wildfires are highly imbalanced natural hazards in both space and severity, making the prediction of extreme events particularly challenging. In this work, we introduce the first ordinal classification framework for forecasting wildfire severity levels directly aligned with operational decision-making in France. Our study investigates the influence of loss-function design on the ability of neural models to predict rare yet critical high-severity fire occurrences. We compare standard cross-entropy with several ordinal-aware objectives, including the proposed probabilistic TDeGPD loss derived from a truncated discrete exponentiated Generalized Pareto Distribution. Through extensive benchmarking over multiple architectures and real operational data, we show that ordinal supervision substantially improves model performance over conventional approaches. In particular, the Weighted Kappa Loss (WKLoss) achieves the best overall results, with more than +0.1 IoU gain on the most extreme severity classes while maintaining competitive calibration quality. However, performance remains limited for the rarest events due to their extremely low representation in the dataset. These findings highlight the importance of integrating both severity ordering, data imbalance considerations, and seasonality risk into wildfire forecasting systems. Future work will focus on incorporating seasonal dynamics and uncertainty information into training to further improve the reliability of extreme-event prediction.

</details>


### [8] [VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding](https://arxiv.org/abs/2601.03434)
*Zibo Liu,Muyang Li,Zhe Jiang,Shigang Chen*

Main category: cs.LG

TL;DR: VNU-Bench：首个多源跨视频新闻理解基准，包含429个新闻组、1,405个视频和2,501个高质量问题，挑战现有多模态大语言模型


<details>
  <summary>Details</summary>
Motivation: 现有新闻视频理解基准主要关注单源、视频内推理，而现实新闻消费本质上是多源的——同一事件由不同媒体以互补细节、不同叙事选择和有时相互矛盾的说法报道。稳健的新闻理解需要模型能够比较不同来源的观点、对齐跨来源的多模态证据并综合多源信息。

Method: 1. 设计新型问题类型，从多个角度测试模型理解多源多模态新闻的能力
2. 提出混合人机QA生成流程，解决跨源新闻理解大规模数据集构建中的可扩展性和质量控制问题
3. 构建包含429个新闻组、1,405个视频和2,501个高质量问题的数据集

Result: 对闭源和开源多模态模型的综合评估显示，VNU-Bench对当前MLLMs构成了实质性挑战，表明现有模型在多源跨视频新闻理解方面存在显著不足。

Conclusion: VNU-Bench填补了多源跨视频新闻理解基准的空白，为评估模型在现实新闻消费场景中的能力提供了重要工具，揭示了当前多模态大语言模型在处理多源信息整合方面的局限性。

Abstract: News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.

</details>


### [9] [A Proposed Paradigm for Imputing Missing Multi-Sensor Data in the Healthcare Domain](https://arxiv.org/abs/2601.03565)
*Vaibhav Gupta,Florian Grensing,Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 该论文综述了连续健康监测中多传感器数据缺失值插补技术，针对低血糖预测，提出基于特征特性和缺失时长定制插补策略的系统范式。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病如糖尿病的管理面临挑战，特别是低血糖等并发症需要及时检测和干预。虽然可穿戴传感器的连续健康监测为血糖事件早期预测提供了解决方案，但多传感器数据存在信号噪声和频繁缺失值的问题，阻碍了有效利用。

Method: 1. 分析现有数据集的局限性，强调与低血糖预测相关的关键特征的时间特性；2. 对现有研究中使用的插补技术进行全面分析；3. 评估机器学习和深度学习在其他医疗场景中应用的插补方法处理长时间序列数据缺失的潜力；4. 提出系统范式，根据特定特征的性质和缺失间隔的持续时间定制插补策略。

Result: 研究提出了一个系统性的插补范式，强调需要根据特征特性和缺失时长采用定制化的插补策略，而不是单一方法。该范式能够更有效地处理数据中固有的异质时间模式。

Conclusion: 综述强调研究个体特征的时间动态性和实施多个特征特定插补技术的重要性，以有效处理数据中固有的异质时间模式，从而提高低血糖预测的准确性和可靠性。

Abstract: Chronic diseases such as diabetes pose significant management challenges, particularly due to the risk of complications like hypoglycemia, which require timely detection and intervention. Continuous health monitoring through wearable sensors offers a promising solution for early prediction of glycemic events. However, effective use of multisensor data is hindered by issues such as signal noise and frequent missing values. This study examines the limitations of existing datasets and emphasizes the temporal characteristics of key features relevant to hypoglycemia prediction. A comprehensive analysis of imputation techniques is conducted, focusing on those employed in state-of-the-art studies. Furthermore, imputation methods derived from machine learning and deep learning applications in other healthcare contexts are evaluated for their potential to address longer gaps in time-series data. Based on this analysis, a systematic paradigm is proposed, wherein imputation strategies are tailored to the nature of specific features and the duration of missing intervals. The review concludes by emphasizing the importance of investigating the temporal dynamics of individual features and the implementation of multiple, feature-specific imputation techniques to effectively address heterogeneous temporal patterns inherent in the data.

</details>


### [10] [Group and Exclusive Sparse Regularization-based Continual Learning of CNNs](https://arxiv.org/abs/2601.03658)
*Basile Tousside,Janis Mohr,Jörg Frochte*

Main category: cs.LG

TL;DR: GESCL是一种基于正则化的持续学习方法，通过稳定性和可塑性正则化项防止灾难性遗忘，同时利用CNN的过参数化特性进行稀疏化，相比动态扩展网络或记忆数据的方法参数更少。


<details>
  <summary>Details</summary>
Motivation: 解决卷积神经网络在持续学习多个任务时出现的灾难性遗忘问题，同时避免现有方法需要动态扩展网络或记忆过去任务数据带来的参数和计算开销。

Method: 提出GESCL方法，包含两个正则化项：稳定性正则化防止对过去任务重要的滤波器参数变化过大；可塑性正则化利用CNN的过参数化特性，稀疏化网络并调整不重要滤波器使其适应未来任务。

Result: 在流行的持续学习视觉基准测试中，GESCL在整体分类准确率和避免灾难性遗忘方面显著优于现有最先进方法。

Conclusion: GESCL通过结合稳定性和可塑性正则化，在固定容量CNN中有效解决了持续学习的灾难性遗忘问题，同时减少了参数和计算需求。

Abstract: We present a regularization-based approach for continual learning (CL) of fixed capacity convolutional neural networks (CNN) that does not suffer from the problem of catastrophic forgetting when learning multiple tasks sequentially. This method referred to as Group and Exclusive Sparsity based Continual Learning (GESCL) avoids forgetting of previous tasks by ensuring the stability of the CNN via a stability regularization term, which prevents filters detected as important for past tasks to deviate too much when learning a new task. On top of that, GESCL makes the network plastic via a plasticity regularization term that leverage the over-parameterization of CNNs to efficiently sparsify the network and tunes unimportant filters making them relevant for future tasks. Doing so, GESCL deals with significantly less parameters and computation compared to CL approaches that either dynamically expand the network or memorize past tasks' data. Experiments on popular CL vision benchmarks show that GESCL leads to significant improvements over state-of-the-art method in terms of overall CL performance, as measured by classification accuracy as well as in terms of avoiding catastrophic forgetting.

</details>


### [11] [Clinical Data Goes MEDS? Let's OWL make sense of it](https://arxiv.org/abs/2601.04164)
*Alberto Marfoglia,Jong Ho Jhee,Adrien Coulet*

Main category: cs.LG

TL;DR: MEDS-OWL是一个轻量级OWL本体，将医疗事件数据标准（MEDS）与语义网生态系统连接，通过RDF图表示MEDS数据集，并提供了Python转换工具meds2rdf。


<details>
  <summary>Details</summary>
Motivation: 医疗数据缺乏标准化和语义明确的表示，导致跨数据集和实验的互操作性和可重复性有限。MEDS虽然提供了事件中心的数据模型，但未与语义网生态系统集成。

Method: 开发MEDS-OWL本体（13个类、10个对象属性、20个数据属性、24个OWL公理），并实现meds2rdf Python库将MEDS事件转换为符合本体的RDF图。在颅内动脉瘤破裂的合成临床数据集上演示，使用SHACL约束验证结果图。

Result: MEDS-OWL首个版本成功实现，结合meds2rdf能够将数据转换为符合FAIR原则的数据集，支持溯源感知发布和基于事件的临床数据互操作。验证了合成数据集上的应用可行性。

Conclusion: 通过将MEDS与语义网连接，这项工作为基于事件的临床数据提供了可重用的语义层，为后续基于图的分析建立了坚实基础，促进了医疗数据的互操作性和可重复性。

Abstract: The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.

</details>


### [12] [Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition](https://arxiv.org/abs/2601.04181)
*Nia Touko,Matthew O A Ellis,Cristiano Capone,Alessio Burrello,Elisa Donati,Luca Manneschi*

Main category: cs.LG

TL;DR: 提出轻量级测试时适应框架，使用TCN骨干网络，通过因果自适应批归一化、GMM对齐与经验回放、元学习三种策略，显著提升表面肌电信号长期解码的跨会话稳定性。


<details>
  <summary>Details</summary>
Motivation: 表面肌电信号长期解码受电极偏移、肌肉疲劳和姿势变化引起的信号漂移影响，现有模型在会话间性能急剧下降，且现有解决方案需要大数据集或高计算量，不适用于能效穿戴设备。

Method: 提出轻量级测试时适应框架，使用TCN骨干网络，包含三种部署就绪策略：1) 因果自适应批归一化实现实时统计对齐；2) GMM对齐与经验回放防止遗忘；3) 元学习实现快速少样本校准。

Result: 在NinaPro DB6多会话数据集上评估，显著缩小会话间准确率差距，计算开销最小。经验回放更新在有限数据下提供优越稳定性，元学习在少量样本下达到竞争性能。

Conclusion: 为长期假肢使用的稳健"即插即用"肌电控制建立了可行路径，实现了轻量级、高效的自适应框架。

Abstract: Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, "plug-and-play" myoelectric control for long-term prosthetic use.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 大型基础模型(LFMs)在医疗AI中应用广泛，但无法替代N-of-1试验进行个性化治疗推荐，两者应互补结合


<details>
  <summary>Details</summary>
Motivation: 探讨大型基础模型在个性化医疗中的局限性，特别是其预测能力与因果推断需求之间的差距，以及医疗AI中存在的多个悖论（泛化性悖论、隐私-性能悖论等）

Method: 提出混合框架：LFMs利用多模态数据从群体模式中快速生成假设和干预候选方案，N-of-1试验（交叉自我实验）为个体提供因果验证，两者结合实现个性化医疗

Result: LFMs无法替代N-of-1试验，但两者可以互补：LFMs擅长基于群体数据的假设生成，N-of-1试验擅长个体因果推断，结合框架能解决医疗AI中的多个悖论

Conclusion: 明确预测与因果的边界，通过LFMs与N-of-1试验的互补结合，能够负责任地将AI整合到个性化医疗中，解决现有悖论并实现真正的个性化治疗推荐

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [14] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: Sandwich Reasoning (SandwichR) 提出了一种 Answer-Reasoning-Answer 范式，通过一致性感知强化学习策略，在保持推理精度的同时显著降低查询纠正的延迟。


<details>
  <summary>Details</summary>
Motivation: 查询纠正是现代搜索流水线的关键入口，需要在实时延迟约束下保持高精度。虽然思维链（CoT）推理能提高精度，但其延迟过高无法满足实时需求。提前输出答案虽然能降低延迟，但答案与后续推理无关，无法利用推理能力提升精度。

Method: 提出 Sandwich Reasoning (SandwichR) 方法，采用 Answer-Reasoning-Answer 范式：先生成初始纠正，再进行显式推理，最后生成最终精炼纠正。通过一致性感知强化学习策略，包括一致性奖励来对齐初始和最终纠正，以及基于边界的拒绝采样来优先处理推理影响最大的边界样本。

Result: 实验结果表明，SandwichR 在达到与标准 CoT 相当的 SOTA 精度的同时，实现了 40-70% 的延迟降低，解决了在线搜索中的延迟-精度权衡问题。同时构建了高质量的查询纠正数据集。

Conclusion: SandwichR 通过将快速初始答案与事后推理对齐，实现了低延迟查询纠正而不牺牲推理感知的精度，解决了 CoT 推理在实时应用中的延迟瓶颈问题。

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [OpenAI GPT-5 System Card](https://arxiv.org/abs/2601.03267)
*Aaditya Singh,Adam Fry,Adam Perelman,Adam Tart,Adi Ganesh,Ahmed El-Kishky,Aidan McLaughlin,Aiden Low,AJ Ostrow,Akhila Ananthram,Akshay Nathan,Alan Luo,Alec Helyar,Aleksander Madry,Aleksandr Efremov,Aleksandra Spyra,Alex Baker-Whitcomb,Alex Beutel,Alex Karpenko,Alex Makelov,Alex Neitz,Alex Wei,Alexandra Barr,Alexandre Kirchmeyer,Alexey Ivanov,Alexi Christakis,Alistair Gillespie,Allison Tam,Ally Bennett,Alvin Wan,Alyssa Huang,Amy McDonald Sandjideh,Amy Yang,Ananya Kumar,Andre Saraiva,Andrea Vallone,Andrei Gheorghe,Andres Garcia Garcia,Andrew Braunstein,Andrew Liu,Andrew Schmidt,Andrey Mereskin,Andrey Mishchenko,Andy Applebaum,Andy Rogerson,Ann Rajan,Annie Wei,Anoop Kotha,Anubha Srivastava,Anushree Agrawal,Arun Vijayvergiya,Ashley Tyra,Ashvin Nair,Avi Nayak,Ben Eggers,Bessie Ji,Beth Hoover,Bill Chen,Blair Chen,Boaz Barak,Borys Minaiev,Botao Hao,Bowen Baker,Brad Lightcap,Brandon McKinzie,Brandon Wang,Brendan Quinn,Brian Fioca,Brian Hsu,Brian Yang,Brian Yu,Brian Zhang,Brittany Brenner,Callie Riggins Zetino,Cameron Raymond,Camillo Lugaresi,Carolina Paz,Cary Hudson,Cedric Whitney,Chak Li,Charles Chen,Charlotte Cole,Chelsea Voss,Chen Ding,Chen Shen,Chengdu Huang,Chris Colby,Chris Hallacy,Chris Koch,Chris Lu,Christina Kaplan,Christina Kim,CJ Minott-Henriques,Cliff Frey,Cody Yu,Coley Czarnecki,Colin Reid,Colin Wei,Cory Decareaux,Cristina Scheau,Cyril Zhang,Cyrus Forbes,Da Tang,Dakota Goldberg,Dan Roberts,Dana Palmie,Daniel Kappler,Daniel Levine,Daniel Wright,Dave Leo,David Lin,David Robinson,Declan Grabb,Derek Chen,Derek Lim,Derek Salama,Dibya Bhattacharjee,Dimitris Tsipras,Dinghua Li,Dingli Yu,DJ Strouse,Drew Williams,Dylan Hunn,Ed Bayes,Edwin Arbus,Ekin Akyurek,Elaine Ya Le,Elana Widmann,Eli Yani,Elizabeth Proehl,Enis Sert,Enoch Cheung,Eri Schwartz,Eric Han,Eric Jiang,Eric Mitchell,Eric Sigler,Eric Wallace,Erik Ritter,Erin Kavanaugh,Evan Mays,Evgenii Nikishin,Fangyuan Li,Felipe Petroski Such,Filipe de Avila Belbute Peres,Filippo Raso,Florent Bekerman,Foivos Tsimpourlas,Fotis Chantzis,Francis Song,Francis Zhang,Gaby Raila,Garrett McGrath,Gary Briggs,Gary Yang,Giambattista Parascandolo,Gildas Chabot,Grace Kim,Grace Zhao,Gregory Valiant,Guillaume Leclerc,Hadi Salman,Hanson Wang,Hao Sheng,Haoming Jiang,Haoyu Wang,Haozhun Jin,Harshit Sikchi,Heather Schmidt,Henry Aspegren,Honglin Chen,Huida Qiu,Hunter Lightman,Ian Covert,Ian Kivlichan,Ian Silber,Ian Sohl,Ibrahim Hammoud,Ignasi Clavera,Ikai Lan,Ilge Akkaya,Ilya Kostrikov,Irina Kofman,Isak Etinger,Ishaan Singal,Jackie Hehir,Jacob Huh,Jacqueline Pan,Jake Wilczynski,Jakub Pachocki,James Lee,James Quinn,Jamie Kiros,Janvi Kalra,Jasmyn Samaroo,Jason Wang,Jason Wolfe,Jay Chen,Jay Wang,Jean Harb,Jeffrey Han,Jeffrey Wang,Jennifer Zhao,Jeremy Chen,Jerene Yang,Jerry Tworek,Jesse Chand,Jessica Landon,Jessica Liang,Ji Lin,Jiancheng Liu,Jianfeng Wang,Jie Tang,Jihan Yin,Joanne Jang,Joel Morris,Joey Flynn,Johannes Ferstad,Johannes Heidecke,John Fishbein,John Hallman,Jonah Grant,Jonathan Chien,Jonathan Gordon,Jongsoo Park,Jordan Liss,Jos Kraaijeveld,Joseph Guay,Joseph Mo,Josh Lawson,Josh McGrath,Joshua Vendrow,Joy Jiao,Julian Lee,Julie Steele,Julie Wang,Junhua Mao,Kai Chen,Kai Hayashi,Kai Xiao,Kamyar Salahi,Kan Wu,Karan Sekhri,Karan Sharma,Karan Singhal,Karen Li,Kenny Nguyen,Keren Gu-Lemberg,Kevin King,Kevin Liu,Kevin Stone,Kevin Yu,Kristen Ying,Kristian Georgiev,Kristie Lim,Kushal Tirumala,Kyle Miller,Lama Ahmad,Larry Lv,Laura Clare,Laurance Fauconnet,Lauren Itow,Lauren Yang,Laurentia Romaniuk,Leah Anise,Lee Byron,Leher Pathak,Leon Maksin,Leyan Lo,Leyton Ho,Li Jing,Liang Wu,Liang Xiong,Lien Mamitsuka,Lin Yang,Lindsay McCallum,Lindsey Held,Liz Bourgeois,Logan Engstrom,Lorenz Kuhn,Louis Feuvrier,Lu Zhang,Lucas Switzer,Lukas Kondraciuk,Lukasz Kaiser,Manas Joglekar,Mandeep Singh,Mandip Shah,Manuka Stratta,Marcus Williams,Mark Chen,Mark Sun,Marselus Cayton,Martin Li,Marvin Zhang,Marwan Aljubeh,Matt Nichols,Matthew Haines,Max Schwarzer,Mayank Gupta,Meghan Shah,Melody Huang,Meng Dong,Mengqing Wang,Mia Glaese,Micah Carroll,Michael Lampe,Michael Malek,Michael Sharman,Michael Zhang,Michele Wang,Michelle Pokrass,Mihai Florian,Mikhail Pavlov,Miles Wang,Ming Chen,Mingxuan Wang,Minnia Feng,Mo Bavarian,Molly Lin,Moose Abdool,Mostafa Rohaninejad,Nacho Soto,Natalie Staudacher,Natan LaFontaine,Nathan Marwell,Nelson Liu,Nick Preston,Nick Turley,Nicklas Ansman,Nicole Blades,Nikil Pancha,Nikita Mikhaylin,Niko Felix,Nikunj Handa,Nishant Rai,Nitish Keskar,Noam Brown,Ofir Nachum,Oleg Boiko,Oleg Murk,Olivia Watkins,Oona Gleeson,Pamela Mishkin,Patryk Lesiewicz,Paul Baltescu,Pavel Belov,Peter Zhokhov,Philip Pronin,Phillip Guo,Phoebe Thacker,Qi Liu,Qiming Yuan,Qinghua Liu,Rachel Dias,Rachel Puckett,Rahul Arora,Ravi Teja Mullapudi,Raz Gaon,Reah Miyara,Rennie Song,Rishabh Aggarwal,RJ Marsan,Robel Yemiru,Robert Xiong,Rohan Kshirsagar,Rohan Nuttall,Roman Tsiupa,Ronen Eldan,Rose Wang,Roshan James,Roy Ziv,Rui Shu,Ruslan Nigmatullin,Saachi Jain,Saam Talaie,Sam Altman,Sam Arnesen,Sam Toizer,Sam Toyer,Samuel Miserendino,Sandhini Agarwal,Sarah Yoo,Savannah Heon,Scott Ethersmith,Sean Grove,Sean Taylor,Sebastien Bubeck,Sever Banesiu,Shaokyi Amdo,Shengjia Zhao,Sherwin Wu,Shibani Santurkar,Shiyu Zhao,Shraman Ray Chaudhuri,Shreyas Krishnaswamy,Shuaiqi,Xia,Shuyang Cheng,Shyamal Anadkat,Simón Posada Fishman,Simon Tobin,Siyuan Fu,Somay Jain,Song Mei,Sonya Egoian,Spencer Kim,Spug Golden,SQ Mah,Steph Lin,Stephen Imm,Steve Sharpe,Steve Yadlowsky,Sulman Choudhry,Sungwon Eum,Suvansh Sanjeev,Tabarak Khan,Tal Stramer,Tao Wang,Tao Xin,Tarun Gogineni,Taya Christianson,Ted Sanders,Tejal Patwardhan,Thomas Degry,Thomas Shadwell,Tianfu Fu,Tianshi Gao,Timur Garipov,Tina Sriskandarajah,Toki Sherbakov,Tomer Kaftan,Tomo Hiratsuka,Tongzhou Wang,Tony Song,Tony Zhao,Troy Peterson,Val Kharitonov,Victoria Chernova,Vineet Kosaraju,Vishal Kuo,Vitchyr Pong,Vivek Verma,Vlad Petrov,Wanning Jiang,Weixing Zhang,Wenda Zhou,Wenlei Xie,Wenting Zhan,Wes McCabe,Will DePue,Will Ellsworth,Wulfie Bain,Wyatt Thompson,Xiangning Chen,Xiangyu Qi,Xin Xiang,Xinwei Shi,Yann Dubois,Yaodong Yu,Yara Khakbaz,Yifan Wu,Yilei Qian,Yin Tat Lee,Yinbo Chen,Yizhen Zhang,Yizhong Xiong,Yonglong Tian,Young Cha,Yu Bai,Yu Yang,Yuan Yuan,Yuanzhi Li,Yufeng Zhang,Yuguang Yang,Yujia Jin,Yun Jiang,Yunyun Wang,Yushi Wang,Yutian Liu,Zach Stubenvoll,Zehao Dou,Zheng Wu,Zhigang Wang*

Main category: cs.CL

TL;DR: GPT-5是一个统一系统，包含智能快速模型、深度推理模型和实时路由器，通过安全训练减少幻觉和奉承，在写作、编程和健康领域表现提升，并对生物化学领域采取预防性安全措施。


<details>
  <summary>Details</summary>
Motivation: 开发一个更高效、更实用的AI系统，通过统一架构解决不同复杂度的问题，同时提升安全性、减少幻觉，并在关键应用领域（写作、编程、健康）提供更好的用户体验。

Method: 采用统一系统架构：1) 智能快速模型处理大多数问题；2) 深度推理模型解决复杂问题；3) 实时路由器根据对话类型、复杂度、工具需求和用户意图动态选择模型；4) 路由器通过用户切换行为、偏好评分和正确性测量进行持续训练；5) 使用安全完成技术防止禁止内容；6) 对生物化学领域采取高级别安全防护措施。

Result: GPT-5在基准测试中超越先前模型，响应速度更快，对真实世界查询更有用；在减少幻觉、改进指令遵循、减少奉承方面取得显著进展；在写作、编程和健康这三个ChatGPT最常用领域表现提升；建立了生物化学领域的高级安全防护框架。

Conclusion: GPT-5通过创新的统一架构和持续学习机制，在性能、实用性和安全性方面实现了显著提升，特别是在关键应用领域表现出色，同时采取预防性安全措施应对潜在风险，代表了AI系统设计的重要进步。

Abstract: This is the system card published alongside the OpenAI GPT-5 launch, August 2025.
  GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say 'think hard about this' in the prompt). The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time. Once usage limits are reached, a mini version of each model handles remaining queries.
  This system card focuses primarily on gpt-5-thinking and gpt-5-main, while evaluations for other models are available in the appendix. The GPT-5 system not only outperforms previous models on benchmarks and answers questions more quickly, but -- more importantly -- is more useful for real-world queries. We've made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, and have leveled up GPT-5's performance in three of ChatGPT's most common uses: writing, coding, and health. All of the GPT-5 models additionally feature safe-completions, our latest approach to safety training to prevent disallowed content.
  Similarly to ChatGPT agent, we have decided to treat gpt-5-thinking as High capability in the Biological and Chemical domain under our Preparedness Framework, activating the associated safeguards. While we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm -- our defined threshold for High capability -- we have chosen to take a precautionary approach.

</details>


### [16] [Rendering Data Unlearnable by Exploiting LLM Alignment Mechanisms](https://arxiv.org/abs/2601.03401)
*Ruihan Zhang,Jun Sun*

Main category: cs.CL

TL;DR: 提出Disclaimer Injection方法，通过注入精心设计的对齐触发免责声明，使文本对LLM不可学习，利用模型自身的对齐机制实现数据保护


<details>
  <summary>Details</summary>
Motivation: LLM在训练中大量使用异构文本语料，引发对未经授权使用专有或个人数据的担忧，需要在不访问或修改训练流程的情况下保护数据不被模型学习

Method: 提出Disclaimer Injection防御方法，在文本中注入精心设计的对齐触发免责声明，利用LLM自身的对齐机制阻止有效学习。通过层级分析发现，在受保护数据上微调会导致对齐相关层持续激活

Result: 在受保护数据上训练的模型相比标准微调表现出显著且系统的性能下降，对齐约束会覆盖任务学习，即使对常见输入也是如此

Conclusion: 对齐行为是数据保护中一个先前未被探索的杠杆，该方法是在不访问或修改训练流程的情况下，在LLM规模上限制数据可学习性的首个实用方法

Abstract: Large language models (LLMs) are increasingly trained on massive, heterogeneous text corpora, raising serious concerns about the unauthorised use of proprietary or personal data during model training. In this work, we address the problem of data protection against unwanted model learning in a realistic black-box setting. We propose Disclaimer Injection, a novel data-level defence that renders text unlearnable to LLMs. Rather than relying on model-side controls or explicit data removal, our approach exploits the models' own alignment mechanisms: by injecting carefully designed alignment-triggering disclaimers to prevent effective learning. Through layer-wise analysis, we find that fine-tuning on such protected data induces persistent activation of alignment-related layers, causing alignment constraints to override task learning even on common inputs. Consequently, models trained on such data exhibit substantial and systematic performance degradation compared to standard fine-tuning. Our results identify alignment behaviour as a previously unexplored lever for data protection and, to our knowledge, present the first practical method for restricting data learnability at LLM scale without requiring access to or modification of the training pipeline.

</details>


### [17] [SyncThink: A Training-Free Strategy to Align Inference Termination with Reasoning Saturation](https://arxiv.org/abs/2601.03649)
*Gengyang Li,Wang Cai,Yifeng Gao,Yunfang Wu*

Main category: cs.CL

TL;DR: SyncThink是一种无需训练、即插即用的解码方法，通过监控模型自身的推理转换信号来提前终止推理，显著减少CoT推理开销。


<details>
  <summary>Details</summary>
Motivation: CoT提示虽然能提升推理能力，但会产生冗长的推理轨迹，大幅增加推理成本。需要一种方法在不修改模型权重的情况下减少CoT开销。

Method: 基于发现答案token对早期推理关注较弱而聚焦于"/think"特殊token的现象，SyncThink监控模型自身的推理转换信号，在适当时机终止推理过程。

Result: 在GSM8K、MMLU、GPQA和BBH数据集上，SyncThink平均Top-1准确率达到62.00%，仅需656个token和28.68秒延迟，相比完整CoT解码的61.22%、2141个token和92.01秒有明显优势。在GPQA等长视野任务中，SyncThink还能通过防止"过度思考"获得最高+8.1的绝对准确率提升。

Conclusion: SyncThink通过监控模型内部信号实现推理提前终止，有效减少CoT开销，在保持甚至提升准确率的同时显著降低计算成本。

Abstract: Chain-of-Thought (CoT) prompting improves reasoning but often produces long and redundant traces that substantially increase inference cost. We present SyncThink, a training-free and plug-and-play decoding method that reduces CoT overhead without modifying model weights. We find that answer tokens attend weakly to early reasoning and instead focus on the special token "/think", indicating an information bottleneck. Building on this observation, SyncThink monitors the model's own reasoning-transition signal and terminates reasoning. Experiments on GSM8K, MMLU, GPQA, and BBH across three DeepSeek-R1 distilled models show that SyncThink achieves 62.00 percent average Top-1 accuracy using 656 generated tokens and 28.68 s latency, compared to 61.22 percent, 2141 tokens, and 92.01 s for full CoT decoding. On long-horizon tasks such as GPQA, SyncThink can further yield up to +8.1 absolute accuracy by preventing over-thinking.

</details>


### [18] [Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents](https://arxiv.org/abs/2601.03785)
*Dehao Tao,Guoliang Ma,Yongfeng Huang,Minghu Jiang*

Main category: cs.CL

TL;DR: Membox是一个基于主题连续性的分层记忆架构，通过Topic Loom将对话按主题分组存储，Trace Weaver恢复长程事件时间线，显著提升LLM代理的时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆系统遵循"碎片化-补偿"范式，将对话流分解为孤立语句存储，然后通过嵌入检索恢复连贯性，这破坏了叙事和因果流，并使检索偏向词汇相似性。需要一种能保持主题连续性的记忆架构。

Method: 提出membox分层记忆架构：1) Topic Loom以滑动窗口方式持续监控对话，将连续相同主题的轮次分组为连贯的"记忆盒"；2) Trace Weaver将密封的记忆盒链接成长程事件时间线轨迹，恢复跨不连续性的宏观主题重现。

Result: 在LoCoMo数据集上的实验显示，Membox在时序推理任务上实现高达68%的F1提升，优于Mem0、A-MEM等基线方法。同时仅使用现有方法所需上下文令牌的一小部分，在效率和效果间取得优越平衡。

Conclusion: 通过显式建模主题连续性，Membox提供了一种认知激励的机制，显著增强了LLM代理的连贯性和效率，为对话记忆系统提供了新方向。

Abstract: Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent "memory boxes" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.

</details>


### [19] [AI Generated Text Detection](https://arxiv.org/abs/2601.03812)
*Adilkhan Alikhanov,Aidar Amangeldi,Diar Demeubay,Dilnaz Akhmetzhan,Nurbek Moldakhmetov,Omar Polat,Galymzhan Zharas*

Main category: cs.CL

TL;DR: 本文评估了AI文本检测方法，发现深度学习模型（特别是DistilBERT）在检测AI生成文本方面表现最佳，准确率达88.11%，ROC-AUC达0.96，优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，学生越来越多地将AI生成内容当作自己的作业提交，这违反了学术诚信。因此需要评估有效的AI文本检测方法来应对这一问题。

Method: 使用HC3和DAIGT v2两个数据集构建统一基准，采用基于主题的数据分割防止信息泄露。评估了传统机器学习（TF-IDF逻辑回归）和深度学习模型（BiLSTM和DistilBERT）。

Result: TF-IDF逻辑回归达到82.87%的准确率；BiLSTM达到88.86%准确率；DistilBERT达到88.11%准确率且ROC-AUC最高（0.96）。深度学习模型明显优于传统方法，上下文语义建模比词汇特征更有效。

Conclusion: 深度学习模型在AI文本检测中表现最佳，特别是DistilBERT。研究强调了通过适当评估协议减轻主题记忆的重要性。未来将扩展数据集多样性，使用参数高效微调方法，并探索更小的模型和硬件优化策略。

Abstract: The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.

</details>


### [20] [VotIE: Information Extraction from Meeting Minutes](https://arxiv.org/abs/2601.03997)
*José Pedro Evans,Luís Filipe Cunha,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: 提出VotIE任务用于从市政会议纪要中提取结构化投票信息，建立葡萄牙市政纪要基准，发现微调编码器在域内表现最佳，但跨市泛化时few-shot LLM更鲁棒，不过计算成本限制其实际应用。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要记录地方民主决策，但不同于标准化议会记录，它们以高度异构的自由叙述文本编码投票结果，各市格式差异大，给自动化提取带来重大挑战。

Method: 引入VotIE（投票信息提取）任务，使用葡萄牙市政纪要建立首个基准（基于CitiLink语料库），比较微调编码器（XLM-R-CRF）与生成式方法，并在跨市设置中评估模型泛化能力。

Result: 1. 域内评估：微调编码器（XLM-R-CRF）表现最强，达到93.2%宏F1，优于生成式方法；2. 跨市评估：微调模型性能大幅下降，而few-shot LLM展现更强鲁棒性，性能下降显著较小。

Conclusion: 尽管生成式模型在泛化方面有优势，但其高计算成本限制实际应用，轻量级微调编码器仍是大规模实际部署的更实用选择。公开发布基准、训练模型和评估框架以支持行政NLP研究。

Abstract: Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.

</details>


### [21] [Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients](https://arxiv.org/abs/2601.04055)
*Prith Sharma,Austin Z. Henley*

Main category: cs.CL

TL;DR: MPO是一种基于模式的提示优化框架，将提示视为结构化对象，通过局部文本梯度独立优化各个语义部分，提高小规模开源语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法将提示视为整体文本块，难以定位错误、保留关键指令或防止提示无限增长。特别是对于依赖显式结构的小型开源指令调优模型，需要更精细的优化方法。

Method: MPO将提示分解为固定语义部分（系统角色、上下文、任务描述、约束、输出格式），应用由批评语言模型生成的局部文本梯度独立优化每个部分，通过去重整合更新以减少冗余和组件间干扰。

Result: 在ARC-Challenge和MMLU两个推理基准测试中，使用LLaMA-3 8B-Instruct和Mistral-7B-Instruct作为求解模型，MPO始终优于未调优的结构化提示和TextGrad基线，在不修改模型参数或改变提示结构的情况下实现了显著的准确率提升。

Conclusion: 保持固定提示模式的同时应用局部、分节优化是提高小型开源语言模型推理性能的有效实用方法，提供可解释且鲁棒的优化过程。

Abstract: Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [22] [Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware](https://arxiv.org/abs/2601.03904)
*Korbinian Moller,Glenn Johannes Tungka,Lucas Jürgens,Johannes Betz*

Main category: cs.RO

TL;DR: 论文提出了一种用于自动驾驶的主动安全扩展方案，在嵌入式实时操作系统上部署轻量级采样轨迹规划器，为故障操作提供确定性实时规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶安全验证方法（如在线验证）只能检测不可行规划输出，缺乏主动机制在主规划器故障时确保安全操作。需要为故障操作自动驾驶系统开发主动安全扩展。

Method: 在汽车级嵌入式平台（运行实时操作系统）上部署轻量级采样轨迹规划器，在受限计算资源下持续计算轨迹，为未来紧急规划架构奠定基础。

Result: 实验结果显示具有确定性时序行为，延迟有界且抖动最小，验证了在安全可认证硬件上进行轨迹规划的可行性。

Conclusion: 该研究展示了将主动后备机制整合到下一代安全框架中的潜力，同时指出了剩余挑战，为故障操作自动驾驶的主动安全扩展迈出了第一步。

Abstract: Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning

</details>


### [23] [An Event-Based Opto-Tactile Skin](https://arxiv.org/abs/2601.03907)
*Mohammadreza Koolani,Simeon Bamford,Petr Trunin,Simon F. Müller-Cleve,Matteo Lo Preti,Fulvio Mastrogiovanni,Lucia Beccai,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 基于DVS和柔性光学波导的神经形态触觉传感系统，通过双目视觉和DBSCAN聚类实现大面积软皮肤上的按压定位，即使在极端数据缩减下仍保持功能


<details>
  <summary>Details</summary>
Motivation: 开发大面积柔性触觉传感器用于软机器人和交互环境，传统嵌入式光接收器需要重复扫描，而DVS事件驱动方式能提供更高效的传感方案

Method: 将两个DVS相机侧向观察柔性硅胶光学波导皮肤，通过亮度变化产生事件，使用DBSCAN聚类分析接触事件，通过三角测量估计2D皮肤表面的按压位置

Result: 在4620 mm²探测区域内，95%可见按压的定位RMSE为4.66 mm；即使事件数据缩减至1/1024，平均定位误差仅增至9.33 mm，85%试验仍能有效定位；检测延迟分布特征宽度为31 ms

Conclusion: 该事件驱动触觉传感系统在大面积柔性皮肤上表现出良好的定位精度和鲁棒性，即使在极端数据稀疏化下仍能工作，为低功耗、低计算负载的软机器人触觉传感提供了有前景的解决方案

Abstract: This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.

</details>
