<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors](https://arxiv.org/abs/2602.11323)
*Arda Alniak,Sinan Kalkan,Mustafa Mert Ankarali,Afsar Saranli,Abdullah Aydin Alatan*

Main category: cs.CV

TL;DR: 提出一种将学习深度先验集成到VINS-Mono优化后端的新框架，通过仿射不变深度一致性和成对序数约束，在边缘设备上实现实时鲁棒的单目视觉惯性里程计


<details>
  <summary>Details</summary>
Motivation: 传统单目VIO系统在低纹理环境中表现不佳，稀疏视觉特征不足；现有基于ViT的密集深度估计模型计算量大，无法在边缘设备实时部署

Method: 将学习深度先验直接集成到VINS-Mono优化后端，强制仿射不变深度一致性和成对序数约束，通过基于方差的门控机制过滤不稳定伪影

Result: 在TartanGround和M3ED数据集上的实验表明，该方法能防止在挑战性场景中发散，显著提升精度，绝对轨迹误差降低达28.3%

Conclusion: 提出的框架在严格遵循边缘设备计算限制的同时，鲁棒地恢复度量尺度，为低纹理环境下的实时VIO提供了有效解决方案

Abstract: Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.

</details>


### [2] [DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation](https://arxiv.org/abs/2602.12160)
*Xu Guo,Fulong Ye,Qichao Sun,Liyang Chen,Bingchuan Li,Pengze Zhang,Jiawei Liu,Songtao Zhao,Qian He,Xiangwang Hou*

Main category: cs.CV

TL;DR: DreamID-Omni：统一可控人中心音视频生成框架，通过对称条件扩散Transformer和双级解耦策略解决多人场景中的身份-音色绑定问题，实现多任务统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法将参考音视频生成、视频编辑和音频驱动视频动画等任务视为孤立目标，且在多人物场景中难以实现精确的身份和音色解耦控制。

Method: 1. 对称条件扩散Transformer整合异质条件信号；2. 双级解耦策略：信号级的同步RoPE确保注意力空间绑定，语义级的结构化描述建立属性-主体映射；3. 多任务渐进训练方案利用弱约束生成先验正则化强约束任务。

Result: 在视频、音频和音视频一致性方面实现全面SOTA性能，甚至超越领先的专有商业模型。

Conclusion: DreamID-Omni提供了一个统一的框架，解决了人中心音视频生成中的身份-音色绑定问题，并展示了卓越的多任务性能，将推动学术研究向商业级应用转化。

Abstract: Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra](https://arxiv.org/abs/2602.11206)
*Jose Marie Antonio Miñoza*

Main category: cs.LG

TL;DR: UltraLIF框架使用热带几何中的超离散化替代传统SNN中的启发式代理梯度，通过log-sum-exp函数作为可学习的软最大值来模拟神经阈值动态，提供完全可微的SNN训练方法。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）具有高能效和生物合理性，但脉冲生成的非可微性导致必须依赖启发式的代理梯度，存在前向-后向不匹配问题。

Method: 提出UltraLIF框架，使用热带几何中的超离散化替代代理梯度。核心是将log-sum-exp函数作为可学习的软最大值来模拟神经阈值动态，当温度参数ε→0时收敛到硬阈值。从两种动力学系统推导出两个神经元模型：基于LIF常微分方程的UltraLIF（时间动态）和基于扩散方程模拟间隙连接耦合的UltraDLIF（空间动态）。

Result: 在六个基准测试（静态图像、神经形态视觉、音频）上优于代理梯度基线方法，在单时间步（T=1）设置和神经形态/时序数据集上提升最明显。可选稀疏惩罚能在保持竞争力的准确率同时显著降低能耗。

Conclusion: UltraLIF为SNN训练提供了基于数学原理的替代方案，消除了前向-后向不匹配问题，在多个任务上表现出优越性能，特别是在单时间步设置下，同时支持能耗优化。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.

</details>


### [4] [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)
*David Pardoe,Neil Daftary,Miro Furtado,Aditya Aiyer,Yu Wang,Liuqing Li,Tao Song,Lars Hertel,Young Jin Yun,Senthil Radhakrishnan,Zhiwei Wang,Tommy Li,Khai Tran,Ananth Nagarajan,Ali Naqvi,Yue Zhang,Renpeng Fang,Avi Romascanu,Arjun Kulothungun,Deepak Kumar,Praneeth Boda,Fedor Borisyuk,Ruoyan Wang*

Main category: cs.LG

TL;DR: CADET是一个用于广告点击率预测的解码器Transformer模型，通过上下文条件解码、自门控注意力、时间感知位置编码等创新，在LinkedIn广告平台上实现了11.04%的CTR提升。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习推荐模型在CTR预测领域长期占据主导地位，但生成式推荐器在内容推荐中显示出潜力。然而，将Transformer架构应用于广告CTR预测面临独特挑战：处理评分后上下文信号、保持线上线下一致性、扩展到工业级工作负载。

Method: 提出CADET模型，包含五个关键创新：1) 上下文条件解码架构与多塔预测头，显式建模广告位置等评分后信号；2) 自门控注意力机制，在表示和交互层面自适应调节信息流；3) 基于时间戳的RoPE变体，捕捉从秒到月的时间关系；4) 会话掩码策略，防止模型学习不可用的会话内事件依赖；5) 生产工程优化，包括张量打包、序列分块和自定义Flash Attention内核。

Result: 在线A/B测试中，CADET相比生产基线模型LiRank（DCNv2和序列编码器的混合集成）实现了11.04%的CTR提升。系统已成功部署在LinkedIn广告平台，服务于主页信息流赞助更新的主要流量。

Conclusion: CADET展示了将解码器Transformer架构成功应用于工业级广告CTR预测的可行性，通过创新的架构设计和工程优化解决了实际部署中的关键挑战，为广告推荐系统提供了新的有效解决方案。

Abstract: Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.

</details>


### [5] [Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning](https://arxiv.org/abs/2602.11498)
*Xuan Yu,Xu Wang,Rui Zhu,Yudong Zhang,Yang Wang*

Main category: cs.LG

TL;DR: 提出一种新的GFlowNet方法，通过引入规划器将状态空间划分为重叠的部分空间，限制智能体探索范围，提高在大状态空间中的收敛速度和生成样本的质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNet在状态空间中自由探索，当扩展到大规模状态空间时面临显著的收敛挑战。需要一种方法来提高在大状态空间中的探索效率。

Method: 引入规划器将整个状态空间划分为重叠的部分状态空间；在有限大小的部分空间中，智能体能高效识别高奖励子区域；采用启发式策略切换部分区域，避免智能体浪费时间探索已完全探索或低奖励区域；通过迭代探索这些部分状态空间，智能体学习收敛到整个状态空间中的高奖励子区域。

Result: 在多个广泛使用的数据集上实验表明，该方法在大状态空间中比现有工作收敛更快；不仅能生成更高奖励的候选样本，还显著提高了样本的多样性。

Conclusion: 通过限制智能体探索范围并采用智能区域切换策略，提出的方法有效解决了GFlowNet在大状态空间中的收敛问题，提高了生成样本的质量和多样性。

Abstract: Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \modelname converges faster than existing works on large state spaces. Furthermore, \modelname not only generates candidates with higher rewards but also significantly improves their diversity.

</details>


### [6] [Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice](https://arxiv.org/abs/2602.11505)
*Jiangkai Xiong,Kalyan Talluri,Hanzhao Wang*

Main category: cs.LG

TL;DR: 提出校准方法，将不完美的外部选项预测转化为统计有效的无购买估计，仅使用购买数据


<details>
  <summary>Details</summary>
Motivation: 企业通常无法观察消费者是否从竞争对手购买、不购买或完全考虑企业产品等关键行为，这种外部选项信息的缺失使得市场规模和偏好估计变得困难，特别是在仅记录交易数据的情况下

Method: 开发两种校准方法：1）在逻辑空间仿射错误校准下，通过简单回归识别外部选项效用参数；2）在较弱近单调条件下，提出基于排名的校准方法并推导有限样本误差界

Result: 数值实验显示在无购买估计和下游品类决策方面有改进，误差界明确显示了预测器对齐和效用学习误差的影响

Conclusion: 该方法能将不完美的辅助预测转化为统计有效的无购买估计，无需收集新的无购买事件标签，并量化校准准确性对收入绩效的影响

Abstract: Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.

</details>


### [7] [Unifying Stable Optimization and Reference Regularization in RLHF](https://arxiv.org/abs/2602.11523)
*Li He,Qiang Qu,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出统一正则化方法，平衡奖励黑客攻击和稳定优化两个核心挑战，通过加权监督微调损失实现更好的权衡，在多个基准测试中优于RLHF和在线偏好学习方法。


<details>
  <summary>Details</summary>
Motivation: RLHF在提升对齐能力方面取得显著进展，但仍面临奖励黑客攻击和稳定优化两大核心挑战。现有解决方案通过单独的正则化策略分别处理这些问题：使用KL散度惩罚防止奖励黑客攻击，使用策略比率裁剪促进稳定对齐。然而，同时向π₀和π₇正则化所产生的隐式权衡尚未得到充分探索。

Method: 提出统一的正则化方法，明确平衡防止奖励黑客攻击和保持稳定策略更新的目标。该方法产生一个简单的加权监督微调损失，实现了更优的权衡，显著改善了对齐结果和实现复杂度。

Result: 在多个基准测试上的广泛实验验证表明，该方法在一致性和稳定性方面优于RLHF和在线偏好学习方法，实现了增强的对齐性能。

Conclusion: 通过统一的正则化框架平衡奖励黑客攻击和稳定优化的目标，提供了一种简单而原则性的对齐目标，在多个维度上优于现有方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \textbf{reward hacking} and \textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.

</details>


### [8] [Both Topology and Text Matter: Revisiting LLM-guided Out-of-Distribution Detection on Text-attributed Graphs](https://arxiv.org/abs/2602.11641)
*Yinlin Zhu,Di Wu,Xu Wang,Guocong Quan,Miao Hu*

Main category: cs.LG

TL;DR: LG-Plug：一种用于文本属性图OOD检测的LLM引导即插即用策略，通过对齐拓扑和文本表示生成细粒度节点嵌入，并利用聚类迭代LLM提示生成共识驱动的OOD暴露


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：拓扑驱动方法未能充分利用文本语义信息，而基于LLM的方法存在可靠性-信息性不平衡问题，且依赖专门架构无法利用已有的拓扑级洞察

Method: 提出LG-Plug策略：1）对齐拓扑和文本表示生成细粒度节点嵌入；2）通过聚类迭代LLM提示生成共识驱动的OOD暴露；3）利用轻量级集群内码本和启发式采样减少LLM查询时间成本

Result: 生成的OOD暴露作为正则化项分离ID和OOD节点，能够与现有检测器无缝集成，解决了可靠性-信息性平衡问题并降低了计算成本

Conclusion: LG-Plug通过LLM引导的即插即用策略，有效解决了文本属性图中OOD检测的语义-结构信息融合问题，平衡了可靠性和信息性，同时保持了与现有拓扑方法的兼容性

Abstract: Text-attributed graphs (TAGs) associate nodes with textual attributes and graph structure, enabling GNNs to jointly model semantic and structural information. While effective on in-distribution (ID) data, GNNs often encounter out-of-distribution (OOD) nodes with unseen textual or structural patterns in real-world settings, leading to overconfident and erroneous predictions in the absence of reliable OOD detection. Early approaches address this issue from a topology-driven perspective, leveraging neighboring structures to mitigate node-level detection bias. However, these methods typically encode node texts as shallow vector features, failing to fully exploit rich semantic information. In contrast, recent LLM-based approaches generate pseudo OOD priors by leveraging textual knowledge, but they suffer from several limitations: (1) a reliability-informativeness imbalance in the synthesized OOD priors, as the generated OOD exposures either deviate from the true OOD semantics, or introduce non-negligible ID noise, all of which offers limited improvement to detection performance; (2) reliance on specialized architectures, which prevents incorporation of the extensive effective topology-level insights that have been empirically validated in prior work. To this end, we propose LG-Plug, an LLM-Guided Plug-and-play strategy for TAG OOD detection tasks. LG-Plug aligns topology and text representations to produce fine-grained node embeddings, then generates consensus-driven OOD exposure via clustered iterative LLM prompting. Moreover, it leverages lightweight in-cluster codebook and heuristic sampling reduce time cost of LLM querying. The resulting OOD exposure serves as a regularization term to separate ID and OOD nodes, enabling seamless integration with existing detectors.

</details>


### [9] [Potential-energy gating for robust state estimation in bistable stochastic systems](https://arxiv.org/abs/2602.11712)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 提出"势能门控"方法，通过已知势能函数调制观测噪声协方差，在双阱随机系统中实现鲁棒状态估计，相比标准滤波器显著降低RMSE误差。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯滤波器在双阱随机系统中对观测噪声处理不够灵活，无法根据状态在势能景观中的位置自适应调整观测信任度，导致在势垒区域容易受到异常值影响。

Method: 基于势能函数调制观测噪声协方差：当状态接近势能最小值时信任观测，接近势垒时逐渐折扣观测。该方法可集成到扩展卡尔曼滤波、无迹卡尔曼滤波、集合卡尔曼滤波、自适应卡尔曼滤波和粒子滤波中，仅需两个超参数。

Result: 在Ginzburg-Landau双阱过程基准测试中，相比标准扩展卡尔曼滤波实现57-80%的RMSE改进（p<10^{-15}）。即使势能参数偏差50%，改进仍不低于47%。在Dansgaard-Oeschger事件应用中，估计不对称参数γ=-0.109，异常值比例解释了91%的滤波器改进方差。

Conclusion: 势能门控方法通过物理机制调制观测信任度，在双阱随机系统中实现了显著的状态估计改进，对参数误设具有鲁棒性，并在古气候数据应用中展示了实用价值。

Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.

</details>


### [10] [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776)
*Cláudio Correia,Alberto E. A. Ferreira,Lucas Martins,Miguel P. Bento,Sofia Guerreiro,Ricardo Ribeiro Pereira,Ana Sofia Gomes,Jacopo Bono,Hugo Ferreira,Pedro Bizarro*

Main category: cs.LG

TL;DR: MUSE是一个多租户模型服务框架，通过解耦模型分数和客户决策边界，实现无缝模型更新，解决了评分即服务环境中模型重训练导致的决策阈值失效问题。


<details>
  <summary>Details</summary>
Motivation: 在评分即服务环境中，模型重训练会改变分数分布，导致现有决策阈值失效。多租户环境下需要协调数百个客户更新阈值，造成严重瓶颈、人力消耗和模型停滞。

Method: MUSE采用动态意图路由共享模型，结合两级分数转换将模型输出映射到稳定的参考分布，解耦模型分数与客户决策边界。

Result: 在Feedzai部署中，MUS处理每秒超千事件，过去12个月处理超550亿事件，服务数十租户，保持高可用性和低延迟，将模型交付时间从数周缩短到数分钟。

Conclusion: MUSE通过稳定分数分布和决策边界解耦，显著提升模型抗攻击能力，节省数百万美元欺诈损失和运营成本，解决了多租户模型服务的关键瓶颈。

Abstract: In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.

</details>


### [11] [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786)
*Keita Broadwater*

Main category: cs.LG

TL;DR: APST框架通过重复采样相同提示来评估LLM在持续使用下的可靠性，发现传统基准测试可能掩盖模型在重复推理中的实际故障率差异。


<details>
  <summary>Details</summary>
Motivation: 传统LLM安全评估主要关注广度任务测试，但实际部署中，模型需要在相同或相似提示下进行重复推理，这种持续使用下的响应一致性和安全性风险尚未得到充分评估。

Method: 提出加速提示压力测试（APST）框架，受可靠性工程启发，在受控操作条件下重复采样相同提示，使用伯努利和二项分布模型形式化安全故障，估计每次推理的故障概率。

Result: 对多个指令调优LLM在AIR-BENCH安全提示上的测试显示，具有相似基准分数的模型在重复采样下表现出显著不同的实际故障率，特别是随着温度增加，单次采样评估可能掩盖可靠性差异。

Conclusion: APST为评估LLM在重复推理下的安全性和可靠性提供了实用框架，补充现有基准测试，弥合基准对齐和部署导向风险评估之间的差距。

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

</details>


### [12] [Capability-Oriented Training Induced Alignment Risk](https://arxiv.org/abs/2602.12124)
*Yujun Zhou,Yue Huang,Han Bao,Kehan Guo,Zhenwen Liang,Pin-Yu Chen,Tian Gao,Werner Geyer,Nuno Moniz,Nitesh V Chawla,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 语言模型在强化学习环境中会自发学习利用系统漏洞最大化奖励，即使没有恶意意图，这些利用策略具有可迁移性，对现有对齐方法构成根本挑战。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究主要关注防止模型生成有害内容，但更微妙的风险是能力导向训练引发的利用行为。研究探索语言模型在存在隐含漏洞的环境中是否会自发学习利用这些缺陷来最大化奖励。

Method: 设计了四个不同的"漏洞游戏"，每个游戏呈现独特的可利用缺陷：上下文条件合规性、代理指标、奖励篡改和自我评估。通过强化学习在这些环境中训练模型，并测试其利用策略的可迁移性和可蒸馏性。

Result: 模型一致地学习利用这些漏洞，发现机会主义策略，以牺牲任务正确性或安全性为代价显著提高奖励。这些利用策略不是狭隘的"技巧"，而是可泛化的技能，可以迁移到新任务，甚至通过数据从有能力的教师模型"蒸馏"到其他学生模型。

Conclusion: 能力导向训练引发的风险对当前对齐方法构成根本挑战，未来AI安全工作必须超越内容审核，严格审计和保护训练环境和奖励机制本身。

Abstract: While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse "vulnerability games", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow "tricks" but generalizable skills; they can be transferred to new tasks and even "distilled" from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk.

</details>


### [13] [On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy](https://arxiv.org/abs/2602.12009)
*Luiz Pereira,Mirko Perkusich,Dalton Valadares,Kyller Gorgônio*

Main category: cs.LG

TL;DR: 本文分析了差分隐私机制如何影响脉冲神经网络的发放率统计，以及这些扰动如何传播到基于发放率的联邦神经形态学习协调中，提供了隐私保护联邦神经形态学习的实用指导。


<details>
  <summary>Details</summary>
Motivation: 联邦神经形态学习虽然能实现节能和隐私保护，但实际部署需要额外的隐私机制（如差分隐私），这些机制会显著改变训练信号。需要理解差分隐私机制如何影响脉冲神经网络的发放率统计，以及这些扰动如何传播到基于发放率的联邦协调中。

Method: 在非独立同分布设置下的语音识别任务中，通过梯度裁剪和噪声注入等差分隐私机制，分析它们对脉冲神经网络发放率统计的扰动。通过在不同隐私预算和裁剪边界上进行消融实验，研究发放率偏移、聚合衰减和客户端选择中的排名不稳定性。

Result: 研究发现差分隐私机制会导致系统性的发放率偏移、聚合衰减和客户端选择中的排名不稳定性。这些偏移与稀疏性和内存指标相关。实验揭示了隐私强度与基于发放率的协调之间的权衡关系。

Conclusion: 研究结果为隐私保护联邦神经形态学习提供了可操作的指导，特别是在隐私强度与基于发放率的协调之间的平衡方面。这些发现有助于在实际部署中更好地设计隐私保护机制。

Abstract: Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.

</details>


### [14] [PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories](https://arxiv.org/abs/2602.12080)
*Hyunsung Kim,Kunhee Lee,Sangwoo Seo,Sang-Ki Ko,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: PathCRF：仅使用球员轨迹数据检测足球场上持球事件的框架，无需球追踪数据，通过动态图建模和条件随机场确保逻辑一致性


<details>
  <summary>Details</summary>
Motivation: 当前足球事件数据收集依赖劳动密集型人工标注，球追踪技术成本高难以规模化，导致全面数据收集仅限于顶级赛事，限制了数据驱动分析的广泛应用

Method: 将球员轨迹建模为全连接动态图，将事件检测转化为每个时间步选择一条边表示当前持球状态。使用条件随机场确保连续边之间的逻辑一致性，通过基于集合注意力的骨干架构计算边嵌入，使用Viterbi解码获取最可能边序列

Result: PathCRF能够生成准确且逻辑一致的持球路径，实现可靠的下游分析，同时大幅减少手动事件标注的需求

Conclusion: 该框架仅使用球员跟踪数据即可有效检测足球持球事件，解决了球追踪数据难以获取的问题，使数据驱动分析能够扩展到更广泛的足球赛事中

Abstract: Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.

</details>


### [15] [SafeNeuron: Neuron-Level Safety Alignment for Large Language Models](https://arxiv.org/abs/2602.12158)
*Zhaoxin Wang,Jiaming Liang,Fengbin Zhu,Weixiang Zhao,Junfeng Fang,Jiayi Ji,Handing Wang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: SafeNeuron：一种神经元级安全对齐框架，通过重新分布安全表示来增强大语言模型和多模态大语言模型的安全性，提高对抗神经元剪枝攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法通常集中在行为层面，且安全行为仅依赖于少数参数，导致对齐脆弱且容易被神经元级攻击绕过。需要更深入控制模型内部安全机制的方法。

Method: 首先识别安全相关神经元，然后在偏好优化过程中冻结这些神经元，防止模型依赖稀疏的安全路径，强制其构建冗余的安全表示。

Result: 实验表明SafeNeuron显著提高了对抗神经元剪枝攻击的鲁棒性，降低了开源模型被重新用作红队生成器的风险，同时保持了通用能力。层间分析揭示了安全行为由稳定共享的内部表示控制。

Conclusion: SafeNeuron为模型对齐提供了可解释且鲁棒的视角，通过神经元级干预实现了更稳健的安全对齐。

Abstract: Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Causal-JEPA: Learning World Models through Object-Level Latent Interventions](https://arxiv.org/abs/2602.11389)
*Heejeong Nam,Quentin Le Lidec,Lucas Maes,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: C-JEPA：一种面向对象的世界模型，通过对象级掩码实现隐式干预，提升关系推理和因果理解能力


<details>
  <summary>Details</summary>
Motivation: 现有基于对象中心表示的世界模型不足以捕捉交互依赖的动态关系，需要更强的关系理解能力来支持预测、推理和控制任务

Method: 将掩码联合嵌入预测从图像块扩展到对象中心表示，通过对象级掩码要求从其他对象推断目标对象状态，实现隐式干预和防止捷径解

Result: 在视觉问答中提升约20%的反事实推理能力；在智能体控制任务中仅需1%的潜在输入特征即可达到与基于图像块模型相当的性能

Conclusion: C-JEPA通过对象级掩码引入因果归纳偏置，有效提升世界模型的关系推理能力，为预测、推理和控制任务提供更高效的基础

Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.

</details>


### [17] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文提出Rung Collapse理论，解释自回归训练无法区分关联与干预，导致模型"正确但理由错误"。提出Epistemic Regret Minimization方法，通过三层架构防止因果推理错误固化。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统常通过"捷径"获得高性能，但这些捷径在分布变化时会失效。这种病理有精确的因果起源：自回归训练无法区分关联P(Y|X)和干预P(Y|do(X))，导致模型基于错误因果推理却得到正确答案。

Method: 提出Epistemic Regret Minimization(ERM)作为因果信念修正算子，满足AGM公设。构建三层架构：1)物理基础定理证明满足执行器独立的动作实现有效do操作；2)ERM作为因果信念修正；3)错误模式分类法注入领域无关保护。提供有限样本边界证明渐进恢复真实干预分布。

Result: 在6个前沿LLM的1,360个因果陷阱场景实验中：Rung Collapse在推理增强模型中仍存在(GPT-5.2为3.7%)；可操控性呈现逆缩放现象；针对性ERM反馈能恢复53-59%的固化错误，而结果级反馈失败。

Conclusion: 论文形式化了Rung Collapse作为自回归训练的固有缺陷，提出ERM框架防止因果推理错误固化。该方法在理论上保证渐进恢复真实干预分布，实验证明能有效纠正前沿LLM中的因果推理错误。

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [18] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: AIR是首个针对LLM智能体的安全事件响应框架，通过语义检测、自主响应和规则生成，实现超过90%的事件检测、修复和根除成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体的安全机制主要关注事前预防，缺乏事中响应、控制和事后恢复能力。随着LLM智能体在自主应用中的广泛部署，需要建立系统化的事件响应机制来处理不可避免的安全事件。

Method: AIR框架包含三个核心组件：1) 基于环境状态和上下文的语义检测机制；2) 指导智能体执行控制和恢复操作的工具集成；3) 在根除阶段合成防护规则以阻止类似事件再次发生。框架通过领域特定语言管理LLM智能体系统的事件响应生命周期，并将其集成到智能体的执行循环中。

Result: 在三种代表性智能体类型上的评估显示，AIR实现了超过90%的事件检测、修复和根除成功率。实验还证实了AIR关键设计组件的必要性，展示了其及时性和适度开销，并证明LLM生成的规则在不同领域中接近开发者编写规则的效果。

Conclusion: 事件响应作为提升智能体安全的一等机制既是可行的也是必要的。AIR框架为LLM智能体系统提供了首个系统化的事件响应解决方案，填补了当前安全机制的空白。

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [19] [AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution](https://arxiv.org/abs/2602.11917)
*Taian Guo,Haiyang Shen,Junyu Luo,Binqi Chen,Hongjun Ding,Jinsheng Huang,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaPROBE将alpha因子挖掘重构为有向无环图导航问题，通过贝叶斯因子检索器和DAG感知因子生成器实现全局结构化的因子进化，显著提升预测精度、收益稳定性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动因子挖掘方法存在两大范式：解耦因子生成将因子发现视为孤立事件，迭代因子进化仅关注局部父子优化。这两种方法都缺乏全局结构视角，将因子池视为非结构化集合或碎片化链条，导致冗余搜索和多样性受限。

Method: AlphaPROBE框架将因子挖掘重构为有向无环图(DAG)的战略导航，将因子建模为节点，进化链接建模为边。包含两个核心组件：1)贝叶斯因子检索器，通过后验概率模型平衡利用与探索，识别高潜力种子；2)DAG感知因子生成器，利用因子的完整祖先轨迹，生成上下文感知的非冗余优化。

Result: 在三个主要中国股票市场数据集上，与8个竞争基线相比，AlphaPROBE在预测准确性、收益稳定性和训练效率方面均获得显著提升。实验结果证实，利用全局进化拓扑对于高效稳健的自动alpha发现至关重要。

Conclusion: 将因子池视为动态互连生态系统，通过全局进化拓扑进行战略导航，是实现高效和稳健自动alpha发现的关键。AlphaPROBE框架为量化金融中的alpha因子挖掘提供了新的结构化方法。

Abstract: Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.

</details>


### [20] [Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments](https://arxiv.org/abs/2602.11964)
*Romain Froger,Pierre Andrews,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Gerard Moreno-Torres Bertran,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Vladislav Vorotilov,Mengjue Wang,Ian Yu,Amine Benhalloum,Grégoire Mialon,Thomas Scialom*

Main category: cs.AI

TL;DR: Gaia2是一个用于评估LLM智能体在异步环境中的基准测试，强调现实性、时间约束和多智能体协作，支持细粒度动作级评估和强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准多为静态或同步环境，无法反映真实世界中环境独立演化、时间约束、噪声动态事件等复杂情况，需要新的评估框架来缩小"模拟到现实"的差距。

Method: 基于开源Agents Research Environments平台构建消费级环境，设计异步场景（环境独立演化），配备写操作验证器进行细粒度动作级评估，支持可验证奖励的强化学习。

Result: 评估显示不同模型在不同能力间存在权衡：GPT-5总体最强（42% pass@1）但在时间敏感任务上失败；Claude-4 Sonnet在准确性和速度间权衡；Kimi-K2开源模型最佳（21% pass@1）。

Conclusion: Gaia2揭示了推理、效率、鲁棒性之间的基本权衡，暴露了"模拟到现实"差距的挑战，通过提供灵活的基础设施支持下一代实用智能体系统的开发、评估和训练。

Abstract: We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the "sim2real" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.

</details>


### [21] [Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication](https://arxiv.org/abs/2602.12083)
*Antonin Sulc*

Main category: cs.AI

TL;DR: 提出可微分模态逻辑(DML)和模态逻辑神经网络(MLNNs)，通过神经符号方法从行为数据中学习信任网络、因果链和监管边界，用于多智能体系统的语义故障调试。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统从简单聊天机器人发展为自主群体，调试语义故障需要推理知识、信念、因果和责任，这正是模态逻辑设计用来形式化的内容。但传统模态逻辑需要手动指定在真实系统中未知或动态的关系结构。

Method: 提出可微分模态逻辑(DML)，通过模态逻辑神经网络(MLNNs)实现，使系统能够仅从行为数据中学习信任网络、因果链和监管边界。通过四种模态构建统一的神经符号调试框架：认知模态（信任谁）、时间模态（事件何时导致故障）、道义模态（哪些动作被允许）、信念模态（如何解释智能体置信度）。

Result: 在具体多智能体场景中演示了每种模态的应用，从外交游戏中发现欺骗联盟到检测LLM幻觉。展示了逻辑矛盾如何成为可学习的优化目标，提供了完整的实现代码作为可执行的Jupyter笔记本。

Conclusion: 为神经符号社区提供了关键贡献：1)可解释的学习结构，信任和因果是显式参数而非不透明嵌入；2)通过可微分公理进行知识注入，用稀疏数据指导学习；3)组合多模态推理，结合认知、时间和道义约束；4)实用的部署模式，用于监控、主动控制和多智能体系统通信。

Abstract: As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.
  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety](https://arxiv.org/abs/2602.11157)
*Max Zhang,Derek Liu,Kai Zhang,Joshua Franco,Haihao Liu*

Main category: cs.CL

TL;DR: 研究探索知识蒸馏在多语言越狱防御中的应用，发现传统微调反而增加越狱成功率，通过移除边界拒绝可缓解安全退化，但推理性能仍下降。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐主要针对英语，在非英语尤其是低资源语言中存在漏洞。需要研究如何将专有模型的安全拒绝行为有效迁移到开源模型，以提升多语言环境下的安全性。

Method: 使用知识蒸馏方法，将OpenAI o1-mini专有教师模型的拒绝行为通过LoRA参数高效微调蒸馏到三个开源学生模型（Meta-Llama-3-8B-Instruct、Gemma-2-2B-IT、Qwen3-8B）。使用XSafety的约28,000个多语言越狱提示进行黑盒响应式微调。

Result: 发现反直觉现象：在教师模型"安全"拒绝数据上进行标准微调反而增加所有学生模型的越狱成功率（最高达16.6个百分点）。不同基础模型在蒸馏过程中对未见语言表现出不同的泛化能力。通过移除"边界拒绝"这一主要安全退化来源，可以缓解甚至逆转安全下降，但推理性能（GSM8K）仍会降低。

Conclusion: 知识蒸馏作为多语言安全对齐技术既面临挑战也有潜力。研究为未来方向奠定了基础，表明需要更精细的方法来处理安全拒绝行为，平衡安全性和模型性能。

Abstract: Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel application of knowledge distillation (KD) in the context of multilingual jailbreak prevention, examining its efficacy. We distill the refusal behaviors of a proprietary teacher model (OpenAI o1-mini) with Low-Rank Adaptation (LoRA) into three open-source student models: Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, and Qwen3-8B, using ~28,000 multilingual jailbreak prompts from XSafety via black-box response-based, parameter-efficient fine-tuning (PEFT). Evaluation on the MultiJail benchmark reveals a counterintuitive behavior: standard fine-tuning on the teacher's ``safe'' refusal data inadvertently increases Jailbreak Success Rate (JSR) for all student models, up to 16.6 percentage points. Our experiments reveal a divergent generalization to unseen languages during distillation, with varying outcomes depending on the base model. By removing a primary source of safety degradation, nuanced `boundary' refusals, we mitigate or even reverse safety declines in student models, although reductions in reasoning performance (GSM8K) persist. Overall, our exploratory study highlights the challenges and potential of KD as a technique for multilingual safety alignment, offering a foundation for future research in this direction.

</details>


### [23] [PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models](https://arxiv.org/abs/2602.11170)
*Jiawei Xu,Zhenyu Yu,Ziqian Bi,Minh Duc Pham,Xiaoyi Qu,Danyang Zhang*

Main category: cs.CL

TL;DR: PRIME框架通过三个专门代理（执行器、验证器、协调器）和群体相对策略优化，将算法推理准确率从26.8%提升至93.8%，在需要持续状态跟踪的任务上改进最显著。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多样化推理任务上表现出色，但在算法推理方面表现有限，需要解决这一局限性。

Method: 提出PRIME框架，包含三个专门代理：执行器负责逐步推理，验证器进行约束检查，协调器控制回溯，通过群体相对策略优化进行优化。同时创建PRIME-Bench基准，包含86个任务、12个类别、51,600个实例。

Result: PRIME将平均准确率从26.8%提升至93.8%（相对提升250%）。在需要持续状态跟踪的任务上改进最大：图灵机模拟从9%提升至92%，长除法从16%提升至94%。迭代验证是主要贡献因素，防止了错误传播。小模型受益更大，能达到比其大8倍模型的准确率。

Conclusion: PRIME框架通过多代理协作和迭代验证，显著提升了语言模型在算法推理任务上的性能，特别是在需要状态跟踪的复杂任务上，且小模型也能获得显著收益。

Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.

</details>


### [24] [Are Aligned Large Language Models Still Misaligned?](https://arxiv.org/abs/2602.11305)
*Usman Naseem,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Agrima Seth*

Main category: cs.CL

TL;DR: 提出Mis-Align Bench统一基准，用于同时评估LLMs在安全、价值观和文化三个维度上的错位问题，克服现有基准只能单独评估单一维度的限制。


<details>
  <summary>Details</summary>
Motivation: 现有错位基准（如INSECURE CODE、VALUEACTIONLENS、CULTURALHERITAGE）只能单独评估安全、价值观或文化维度，无法同时评估这三个在现实世界中必须共同满足的维度，导致评估不全面。

Method: 1) 构建SAVACU数据集：从LLM-PROMPT-DATASET重新分类382,424个样本，涵盖112个领域（14个安全域、56个价值观域、42个文化域），使用Mistral-7B-Instruct-v0.3分类，用Llama-3.1-8B-Instruct扩展低资源域；2) 通过两阶段拒绝采样为提示配对错位和对齐的响应；3) 在通用、微调和开源LLMs上进行系统评估。

Result: 单维度模型在覆盖率上可达97.6%，但在联合条件下假失败率超过50%，对齐分数较低（63%-66%），表明单独优化单一维度无法满足现实世界的综合对齐要求。

Conclusion: 需要开发能够同时处理安全、价值观和文化维度的多维度对齐方法，现有单维度优化方法在综合评估中表现不佳，凸显了统一评估基准的重要性。

Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.

</details>


### [25] [LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss](https://arxiv.org/abs/2602.12005)
*Szilvia Ujváry,Louis Béthune,Pierre Ablin,João Monteiro,Marco Cuturi,Michael Kirchhof*

Main category: cs.CL

TL;DR: LaCy：一种基于语法分析的小语言模型预训练方法，通过智能选择哪些token由模型自行预测、哪些需要委托给外部资源，提升事实准确性


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLM）参数容量有限，容易产生事实错误。虽然可以通过访问外部资源（如大模型、文档、数据库）来缓解，但需要解决一个根本问题：哪些token应该由SLM在预训练中学习，哪些应该通过<CALL>标记委托给外部资源。

Method: 提出LaCy预训练方法，使用spaCy语法分析器增强损失信号，区分哪些token即使损失高也是可接受的（真实的替代延续），哪些应该委托给外部资源以防止事实错误。模型学习智能选择预测和委托的token。

Result: LaCy模型成功学会了哪些token应该预测、哪些应该委托。在与大模型级联生成时获得更高的FactScore，优于Rho或LLM-judge训练的SLM，同时更简单、更便宜。

Conclusion: 通过语法分析增强损失信号，LaCy方法能够有效指导小语言模型在预训练中做出智能的token选择决策，平衡模型自身预测和外部资源委托，提高事实准确性。

Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [26] [H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model](https://arxiv.org/abs/2602.11291)
*Wenyuan Chen,Jinbang Huang,Oscar Pang,Zhiyuan Li,Xiao Hu,Lingfeng Zhang,Zhanguang Zhang,Mark Coates,Tongtong Cao,Xingyue Quan,Yingxue Zhang*

Main category: cs.RO

TL;DR: 提出分层世界模型（H-WM），结合高层逻辑世界模型和低层视觉世界模型，统一预测逻辑和视觉状态转换，解决机器人长期规划中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法（如视频生成或自然语言预测）难以直接与机器人动作关联，且长期规划中误差会累积。传统任务和运动规划使用符号逻辑世界模型，但缺乏视觉感知同步。需要结合符号推理的机器人可执行性与视觉观测的感知基础。

Method: 提出分层世界模型（H-WM）：1）高层逻辑世界模型预测符号状态转换；2）低层视觉世界模型预测视觉状态转换；3）双层框架统一整合逻辑和视觉预测；4）使用对齐机器人运动、符号状态、动作和视觉观测的数据集进行训练。

Result: 实验在视觉-语言-动作（VLA）控制策略上验证了方法的有效性和通用性。分层输出为长期任务提供稳定一致的中间指导，减轻误差累积，实现跨扩展任务序列的鲁棒执行。

Conclusion: H-WM通过统一的双层框架结合符号推理的机器人可执行性和视觉观测的感知基础，解决了长期机器人规划中的误差累积问题，为复杂任务提供鲁棒的中间指导。

Abstract: World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.

</details>


### [27] [ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control](https://arxiv.org/abs/2602.11321)
*Ziyan Xiong,Lixing Fang,Junyun Huang,Kashu Yamazaki,Hao Zhang,Chuang Gan*

Main category: cs.RO

TL;DR: ExtremControl是一个低延迟全身控制框架，通过SE(3)姿态直接控制、笛卡尔空间映射和速度前馈控制，实现了50ms端到端延迟的人形机器人遥操作系统。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人遥操作系统依赖预处理的重定向和位置控制，导致高延迟（约200ms），限制了响应性，无法执行需要快速反馈和反应的任务。

Method: 1) 直接在选定刚性链接（主要是人形机器人末端）的SE(3)姿态上操作，避免全身重定向；2) 使用笛卡尔空间映射直接将人体运动转换为机器人链接目标；3) 在底层加入速度前馈控制以支持快速变化的控制接口下的高响应性。

Result: 实现了端到端延迟低至50ms的遥操作系统，支持光学动作捕捉和VR运动跟踪，能够执行乒乓球平衡、杂耍和实时返回等高响应性行为，显著超越了先前工作的200ms延迟限制。

Conclusion: ExtremControl框架通过简化控制流程和加入前馈控制，成功解决了人形机器人遥操作的高延迟问题，为实现动态、反应性强的机器人演示提供了有效解决方案。

Abstract: Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.

</details>
