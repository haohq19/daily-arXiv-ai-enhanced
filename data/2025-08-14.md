<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 11]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.CL](#cs.CL) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 论文提出了一种基于优化GAN和知识蒸馏的合成深度人脸生成框架，结合遗传算法提升多样性和质量，在情感分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 情感计算面临高质量、多样化深度面部数据集的缺乏问题，尤其是识别细微情感表达时。

Method: 使用优化的GAN和知识蒸馏（EMA教师模型）稳定训练，结合遗传算法优化潜在向量，提取多特征用于分类。

Result: 在多样性和质量上优于GAN、VAE、GMM和KDE，分类准确率达94%和96%，评估指标（FID、IS、SSIM、PSNR）优于现有方法。

Conclusion: 提出的框架在合成深度人脸生成和情感分类任务中表现卓越，为情感计算提供了高质量数据支持。

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [2] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: 论文提出了一种四轴评估框架和递归重写策略（BSD），用于更准确地评估和提升多模态大语言模型（MLLMs）对抗性提示的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有评估标准可能高估对抗性提示的攻击效果，且安全机制无法有效阻止有害输出。

Method: 引入四轴评估框架（输入相关性、输入OOD强度、输出有害性、输出拒绝率），并开发BSD策略，通过语义对齐子任务和OOD信号提升攻击效果。

Result: BSD在13个MLLMs中显著提高了攻击成功率和输出有害性，成功率提升67%，有害性提升21%。

Conclusion: 当前多模态安全系统存在未被充分认识的弱点，BSD策略揭示了平衡相关性和新颖性的对抗性提示更易绕过检测。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [3] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS提出了一种基于梯度方向感知的自适应密度控制框架，解决了3D高斯泼溅中的过重建和过密集问题，显著降低了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在复杂场景中存在过重建和过密集问题，导致内存开销增加和渲染质量下降。

Method: GDAGS通过梯度一致性比率（GCR）和非线性动态加权机制，实现梯度方向感知的密度控制。

Result: GDAGS在多种真实场景基准测试中表现出色，减少了50%的内存消耗，同时提升了渲染质量。

Conclusion: GDAGS有效解决了3D高斯泼溅中的关键问题，为实时逼真渲染提供了更高效的解决方案。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [4] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出了一种新型输入自适应导航方法，通过三种算法提升视觉与语言导航（VLN）模型的效率，显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有输入自适应机制在减少计算量时会导致性能显著下降，因此需要更高效的解决方案。

Method: 1. 选择性处理全景视图；2. 基于重要性的自适应阈值提前退出；3. 缓存机制避免重复处理视图。

Result: 在七个VLN基准测试中，计算量减少超过2倍，性能未显著下降。

Conclusion: 提出的方法有效提升了VLN模型的效率，适用于计算资源有限的实际场景。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [5] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: SkyShield是一个事件驱动的端到端框架，用于检测亚毫米级障碍物，采用轻量级U-Net架构和创新的Dice-Contour正则化损失，实现了高精度和低延迟。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中无人机面临的亚毫米级障碍物（如钢丝、风筝线）难以被传统传感器检测，需要新的感知方法。

Method: 基于事件流的独特特征，使用轻量级U-Net架构和Dice-Contour正则化损失进行精确检测。

Result: 实验结果显示，该方法平均F1分数为0.7088，延迟仅为21.2毫秒，适合边缘和移动平台部署。

Conclusion: SkyShield框架在亚毫米级障碍物检测中表现出色，具有实际应用潜力。

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [6] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: 提出了一种基于模型信息优化的平面检测框架，通过信息最小化选择最可能的地面真实模型，减少误检并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决RANSAC方法在复杂场景中因阈值模糊导致的误检问题，尤其是当真实平面数量未知时。

Method: 将深度数据视为离散随机变量，通过随机子采样生成候选平面模型，结合传感器物理和噪声模型计算信息量，选择信息最少的模型作为真实平面。

Result: 在合成数据实验中，算法比Open3D RANSAC更准确地估计平面参数，并通过神经网络分割加速处理。

Conclusion: 提出的框架能有效减少误检并提高平面检测的准确性，适用于复杂场景。

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [7] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经形态计算范式的高效能量鲁棒拟合方法，使用新型脉冲神经网络在Intel Loihi 2硬件上实现，能耗仅为传统CPU方法的15%。


<details>
  <summary>Details</summary>
Motivation: 随着AI能耗问题日益突出，研究能量高效的鲁棒拟合方法变得至关重要。

Method: 设计了新型脉冲神经网络，并开发了事件驱动的模型估计方法，以适应Loihi 2硬件的独特架构。

Result: 在相同精度下，神经形态鲁棒拟合的能耗仅为传统CPU方法的15%。

Conclusion: 神经形态计算为能量高效的鲁棒拟合提供了可行方案，具有实际应用潜力。

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [8] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM框架通过模拟人类情景记忆，解决了Video-LLMs在长视频理解中的关键帧冗余和时空关系缺失问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将长视频简化为静态关键帧匹配，忽略了时空关系和场景连续性，导致信息冗余和准确性下降。

Method: Video-EM将关键帧建模为时序性情景事件，结合空间关系和时间动态，并利用LLMs的链式思维迭代筛选信息。

Result: 在多个基准测试中，Video-EM性能提升4-9%，且使用更少帧数。

Conclusion: Video-EM通过情景记忆和链式思维优化关键帧选择，显著提升了长视频理解的准确性和效率。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [9] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: BAAS是一个基于贝叶斯跟踪和融合的雷达检测标注框架，用于自动驾驶中的扩展目标跟踪，提供精确的轨迹和形状估计，并支持多级监督标注。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中雷达检测的精确标注和跟踪问题，支持闭环持续改进。

Method: 利用贝叶斯跟踪、平滑和融合方法，结合多级监督标注，独立或联合分析各模块。

Result: 在复杂城市场景中验证了跟踪性能和标注误差，适用于不同动态目标和类别。

Conclusion: BAAS框架在雷达检测标注和跟踪中表现出色，支持闭环改进，适用于多种场景。

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [10] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: TOTNet是一种用于体育视频分析中遮挡情况下球体跟踪的时序遮挡跟踪网络，通过3D卷积、可见性加权损失和遮挡增强技术显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 体育视频分析中，遮挡情况下的球体跟踪是关键挑战，影响事件检测和裁判决策。

Method: TOTNet采用3D卷积、可见性加权损失和遮挡增强技术，并引入新的遮挡丰富的乒乓球数据集TTA。

Result: 在四个数据集上评估，TOTNet显著优于现有方法，RMSE从37.30降至7.19，全遮挡帧准确率从0.63提升至0.80。

Conclusion: TOTNet在快速场景下的离线体育分析中表现出色，代码和数据已开源。

Abstract: Robust ball tracking under occlusion remains a key challenge in sports video
analysis, affecting tasks like event detection and officiating. We present
TOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,
visibility-weighted loss, and occlusion augmentation to improve performance
under partial and full occlusions. Developed in collaboration with Paralympics
Australia, TOTNet is designed for real-world sports analytics. We introduce
TTA, a new occlusion-rich table tennis dataset collected from
professional-level Paralympic matches, comprising 9,159 samples with 1,996
occlusion cases. Evaluated on four datasets across tennis, badminton, and table
tennis, TOTNet significantly outperforms prior state-of-the-art methods,
reducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded
frames from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for
offline sports analytics in fast-paced scenarios. Code and data
access:\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.

</details>


### [11] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: E-4DGS是一种基于事件相机的动态高斯泼溅方法，用于多视角事件流的新视角合成，解决了传统RGB相机在光照不足、运动模糊和动态范围有限等问题。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在场景重建中存在光照依赖、运动模糊和动态范围限制等问题，事件相机因其低功耗、高时间分辨率和宽动态范围，为高速运动和低光场景重建提供了新思路。

Method: 提出事件驱动的动态高斯泼溅方法（E-4DGS），包括事件初始化方案、事件自适应切片泼溅、强度重要性剪枝和自适应对比度阈值优化。

Result: E-4DGS在合成多视角事件流数据集上表现优于纯事件和事件-RGB融合基线方法。

Conclusion: E-4DGS为多视角事件流重建提供了新方法，推动了快速场景捕捉的探索。

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive提出了一种多目标强化学习框架，通过对抗优化实现轨迹生成与评估的闭环协同进化，解决了现有方法中的标量化偏差和迭代优化不足问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要实现类似人类的迭代决策，但现有方法在轨迹生成与评估之间存在隔离，且强化学习将多维偏好压缩为标量奖励，导致关键权衡被掩盖。

Method: EvaDrive采用分层生成器（结合自回归意图建模和扩散细化）和多目标评估器，通过对抗游戏实现多轮迭代优化，并利用帕累托前沿选择机制。

Result: 在NAVSIM和Bench2Drive基准测试中表现优异，分别达到94.9 PDMS和64.96 Driving Score，优于现有方法。

Conclusion: EvaDrive通过动态权重生成多样驾驶风格，无需外部偏好数据，为标量化自由的轨迹优化提供了新思路。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [13] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 该论文整合了15个数据集，创建了一个包含2510名受试者的大规模数据库，用于研究1型糖尿病和低血糖事件。通过机器学习模型预测血糖水平，并分析了血糖与心率数据的关系。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病（T1D）患者依赖外部胰岛素注射，但胰岛素可能导致低血糖，带来严重后果。现有研究因数据不足受限，因此需要整合大规模数据集以改进糖尿病护理。

Method: 系统整合15个数据集，形成包含2510名受试者的数据库，记录每5分钟的血糖测量值（共1.49亿次）。提取两个子数据库（人口统计数据和心率数据），并评估数据质量。

Result: 数据库包含4%的低血糖范围数据，性别和年龄分布均衡。数据不平衡和缺失值是主要挑战。血糖与心率数据在低血糖前15至55分钟存在相关性。

Conclusion: 整合的数据集为糖尿病研究提供了丰富资源，但需解决数据质量问题。血糖与心率的相关性为低血糖预警提供了新思路。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [14] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: scAGC是一种单细胞聚类方法，通过对比学习指导自适应细胞图学习，解决了传统方法在高维稀疏数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据的高维性和稀疏性使传统聚类方法面临挑战，现有图神经网络方法依赖静态图结构，难以捕捉长尾分布。

Method: scAGC结合拓扑自适应图自编码器和ZINB损失函数，动态优化图结构和特征表示，并引入对比学习目标增强稳定性。

Result: 在9个真实数据集上，scAGC在NMI和ARI指标上均优于现有方法。

Conclusion: scAGC通过自适应图学习和对比指导，显著提升了单细胞聚类性能。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [15] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 论文介绍了Construction Safety Dataset (CSDataset)，一个多层次的建筑安全数据集，结合了结构化和非结构化数据，用于机器学习和语言模型分析。初步分析显示投诉驱动的检查能减少17.3%的事故概率。


<details>
  <summary>Details</summary>
Motivation: 现有建筑安全数据集规模小且多样性不足，限制了深入分析。

Method: 引入CSDataset，整合OSHA的事故、检查和违规记录，结合结构化和非结构化数据。

Result: 投诉驱动的检查与后续事故概率降低17.3%相关。

Conclusion: CSDataset为建筑安全研究提供了新工具，支持未来更深入的分析和改进。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [16] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Hi-Vec提出了一种分层自适应网络，通过动态选择层和权重合并机制，提升了预训练模型在测试时适应分布变化的能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法因单维线性分类层无法处理复杂分布变化的问题。

Method: 采用分层组织层、动态层选择、权重合并机制和线性层一致性门控。

Result: 在多种目标数据集上验证了Hi-Vec的鲁棒性，提升了不确定性处理和异常值适应能力。

Conclusion: Hi-Vec显著提升了测试时适应能力，适用于复杂分布变化场景。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [17] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: NEXICA算法通过分析高速公路速度数据，识别导致交通拥堵的因果关系，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决交通拥堵问题，通过聚焦拥堵原因提高资源利用效率。

Method: 开发新算法，基于时间序列事件、概率模型和二元分类器识别因果关系。

Result: 在洛杉矶地区195个传感器数据上测试，表现优于现有基线。

Conclusion: NEXICA在准确性和计算速度上优于现有方法，有效识别拥堵原因。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [18] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 提出了一种在联邦持续学习（FCL）中结合轻量级本地模型与基础模型（FMs）的协作框架，解决基础模型在本地任务中的性能不足和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型在本地任务中表现不佳且难以避免遗忘，而小型模型在资源受限条件下更易训练。

Method: 通过轻量级本地模型动态适应新任务，并结合小型模型持续微调和一对一蒸馏技术。

Result: 实验证明该框架在异构本地模型下仍表现优异。

Conclusion: 该框架有效解决了基础模型在联邦持续学习中的挑战。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [19] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种统一的、可解释的GNN框架，用于预测业务流程监控中的未来事件，解决了现有GNN模型在时间相关性和转移语义上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在预测业务流程监控中未能充分利用时间相关性和转移语义，导致性能不足。

Method: 结合前缀GCN和全局GAT，引入时间衰减注意力机制和转移类型语义嵌入，构建动态预测窗口。

Result: 在五个基准测试中，模型表现优异，无需针对数据集调整即可获得高准确率。

Conclusion: 该框架为PBPM中的下一事件预测提供了鲁棒、通用且可解释的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [20] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: SYNAPSE-G利用LLMs生成合成数据解决罕见事件分类中的冷启动问题，通过半监督标签传播扩展数据集，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 标记数据稀缺，尤其是罕见事件，阻碍了有效机器学习模型的训练。

Method: 提出SYNAPSE-G，利用LLMs生成合成数据，通过半监督标签传播扩展数据集，并结合人工或LLM标注。

Result: 在SST2和MHS数据集上，SYNAPSE-G在发现正标签方面优于基线方法。

Conclusion: SYNAPSE-G通过合成数据生成和标签传播有效解决了罕见事件分类问题。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [21] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 论文提出了一个基于算子理论的框架，用于在嵌入空间中进行时间锚定，通过漂移映射和事件索引块的交替实现，最终通过仿射投影完成。


<details>
  <summary>Details</summary>
Motivation: 为时间锚定问题提供一个严格的数学框架，并验证其在实际计算中的适用性。

Method: 使用漂移映射与事件索引块交替的模型，结合仿射投影，并证明了相关的收敛定理和鲁棒性变体。

Result: 证明了软注意力层的Lipschitz性质，并推导了层收缩的充分条件（正交/非正交头）。

Conclusion: 提出的框架在理论和计算上均具有严格性，适用于注意力层等实际场景。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [22] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 该研究开发了一种机器学习流程，通过纵向生物标志物数据预测年龄，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 预测个体衰老轨迹是预防医学和生物信息学的核心挑战，但现有模型难以捕捉衰老的动态纵向特性。

Method: 利用2019-2022年的纵向数据，设计捕捉生物标志物变化率的新特征，构建LightGBM模型。

Result: 模型在预测未来时间点年龄时表现优异（男性R²=0.515，女性R²=0.498），优于传统方法。

Conclusion: 动态健康轨迹比静态健康快照更能预测生物年龄，为临床工具开发奠定基础。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [23] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 该研究利用机器学习模型分析专家提出的生物力学特征，识别影响跳远比赛顶级表现的关键特征，并探索其组合效应。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以明确分析生物力学特征与运动员表现的关系，现代机器学习和统计方法为运动分析提供了新工具。

Method: 使用分位数回归建模生物力学特征与有效距离的关系，结合SHAP、PDP和ICE图解释模型。

Result: 发现除速度相关特征外，特定技术特征（如男性运动员支撑腿膝盖角度、女性运动员着陆姿势和助跑技术）对顶级表现至关重要。

Conclusion: 研究为分析运动表现中各种特征的影响提供了框架，尤其关注顶级表现事件。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 论文提出了一种名为N-Gram Coverage Attack的成员推理攻击方法，仅依赖目标模型的文本输出，适用于黑盒模型。该方法通过计算生成文本与候选成员的后缀相似性来判断成员资格，性能优于其他黑盒方法，甚至媲美白盒攻击。实验表明，攻击成功率随计算预算增加而提升，并发现GPT-4o等新模型对成员推理的鲁棒性增强。


<details>
  <summary>Details</summary>
Motivation: 当前许多先进的成员推理攻击需要访问模型的隐藏状态或概率分布，限制了其在仅提供API访问的模型（如GPT-4）上的应用。因此，需要一种仅依赖文本输出的攻击方法。

Method: N-Gram Coverage Attack通过获取目标模型在候选成员前缀条件下的多个生成文本，利用n-gram重叠度量计算这些输出与真实后缀的相似性，高相似性表明可能是成员。

Result: 该方法在多种基准测试中优于其他黑盒方法，甚至媲美白盒攻击。攻击成功率随生成序列数量增加而提升。实验还发现GPT-4o等新模型对成员推理的鲁棒性更强。

Conclusion: N-Gram Coverage Attack是一种高效的黑盒成员推理方法，适用于广泛使用的API模型。同时，新模型在隐私保护方面表现出进步。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [25] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 本文综述了自然语言处理（NLP）在心理健康领域中的应用，特别是认知扭曲（CDs）的自动检测与分类研究。通过分析38项研究，总结了数据集、建模方法和评估策略，并提出统一的CD分类参考。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲是心理健康治疗中的重要问题，但目前研究领域存在分类、任务定义和评估方法的不一致性，需要系统梳理以推动更一致和可重复的研究。

Method: 通过综述过去20年的38项研究，分析了数据集、建模方法和评估策略，并提出了统一的CD分类参考。

Result: 总结了CD研究的现状，提供了统一的分类框架，并指出了当前研究中的开放性问题。

Conclusion: 本文为CD研究提供了系统综述和分类参考，有助于推动该领域的更一致和可重复发展。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [26] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 论文提出一种通过添加合理上下文来增强事件描述的方法，以提高情感标注的一致性。


<details>
  <summary>Details</summary>
Motivation: 情感分析具有模糊性，传统方法忽略上下文缺失可能是模糊性的根源。

Method: 通过自动生成基于不同情感的事件链，结合短故事生成技术，创建上下文丰富的叙述。

Result: 上下文叙述增强了特定情感的解释，并提高了标注的一致性。

Conclusion: 上下文信息有助于提升情感标注的可靠性。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>
