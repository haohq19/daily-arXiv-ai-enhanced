<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn,Yura Choi,Hyeonbeom Choi,Seongwon Cho,San Kim,Jonghyun Choi*

Main category: cs.CV

TL;DR: 论文提出VECTOR基准测试VLMMs的事件时序理解能力，发现现有模型表现不佳，提出MECOT方法通过事件级指令微调和思维链提升时序理解，在VECTOR和现有基准上均取得改进。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型在视频理解上表现良好，但其对多个事件时序顺序的准确捕捉能力尚未充分探索。研究发现即使视频帧被打乱，模型在现有基准上仍表现良好，表明模型可能依赖场景先验知识而非准确的时序处理。

Method: 提出VECTOR基准专门评估模型的事件时序识别能力；提出MECOT方法：1）在详细的事件级视频描述上进行指令微调；2）推理时使用思维链提示增强时序意识。

Result: 多种VLMMs在VECTOR基准上经常无法理解事件顺序；MECOT方法在VECTOR基准上优于先前方法，同时在现有视频基准上也有性能提升，表明时序理解的有效性。

Conclusion: VLMMs的时序理解能力存在不足，需要专门评估；MECOT通过事件级微调和思维链能有效提升模型的时序理解能力，为视频理解提供了新方向。

Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.

</details>


### [2] [3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization](https://arxiv.org/abs/2512.08987)
*Yuze Hao,Linchao Zhu,Yi Yang*

Main category: cs.CV

TL;DR: 提出3DID框架，通过连续潜在表示与物理感知优化策略直接探索3D设计空间，实现高质量3D几何生成


<details>
  <summary>Details</summary>
Motivation: 现有逆设计方法在3D领域存在局限：网格搜索不可行，现有深度学习方法多使用2D投影或微调现有3D形状，牺牲体积细节并限制设计探索，无法实现真正的从零开始3D设计

Method: 1. 学习统一物理-几何嵌入，在连续潜在空间中紧凑捕捉形状和物理场数据；2. 两阶段物理感知优化：第一阶段使用梯度引导扩散采样器探索全局潜在流形，第二阶段进行目标驱动、拓扑保持的细化雕刻

Result: 3DID能够生成高保真3D几何，在解决方案质量和设计多样性方面均优于现有方法

Conclusion: 提出的3DID框架通过直接导航3D设计空间，实现了真正的从零开始3D逆设计，克服了现有方法的局限性

Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.

</details>


### [3] [Learning to Remove Lens Flare in Event Camera](https://arxiv.org/abs/2512.09016)
*Haiqian Han,Lingdong Kong,Jianing Li,Ao Liang,Chengtao Zhu,Jiacheng Lyu,Lai Xing Ng,Xiangyang Ji,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: E-Deflare：首个系统性的去除事件相机镜头光晕框架，包含物理模型、大规模数据集和SOTA恢复网络


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率和动态范围的优势，但容易受到镜头光晕的影响，这种光学伪影在事件流中形成复杂的时空失真，目前尚未得到充分研究

Method: 1. 建立物理基础：推导基于物理的非线性抑制机制前向模型；2. 创建E-Deflare Benchmark：包含大规模模拟训练集E-Flare-2.7K和首个配对真实世界测试集E-Flare-R；3. 设计E-DeflareNet实现SOTA恢复性能

Result: E-DeflareNet在去除镜头光晕方面达到最先进的恢复性能，实验验证了方法的有效性，并证明对下游任务有明显益处

Conclusion: E-Deflare是首个系统性的去除事件相机镜头光晕框架，通过物理模型、基准数据集和专用网络解决了这一长期被忽视的问题，代码和数据集已公开

Abstract: Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.

</details>


### [4] [Prompt-Based Continual Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.09172)
*Sauda Maryam,Sara Nadeem,Faisal Qureshi,Mohsen Ali*

Main category: cs.CV

TL;DR: 提出了首个基于提示的持续组合零样本学习框架PromptCCZSL，通过多教师蒸馏和会话感知提示解决视觉语言模型在组合属性学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习假设类别互斥，但组合零样本学习中属性和对象可能在不同会话中重复出现，而组合保持唯一，这带来了更复杂的灾难性遗忘挑战。需要开发专门方法在适应新知识的同时保留先前知识。

Method: 基于冻结的VLM骨干，提出PromptCCZSL框架：1) 通过最近加权多教师蒸馏保留先前知识；2) 使用会话感知组合提示融合多模态特征；3) 通过会话无关融合学习属性和对象提示以保持全局语义一致性；4) 余弦锚点损失稳定先前知识；5) 正交投影损失确保新嵌入与先前嵌入区分；6) 会话内多样性损失促进当前会话嵌入的多样性。

Result: 在UT-Zappos和C-GQA基准测试中，PromptCCZSL显著优于现有的VLM和非VLM基线方法，为封闭世界设置下的CCZSL设立了新的基准。

Conclusion: 该工作首次解决了组合零样本学习中的持续适应问题，提出的PromptCCZSL框架通过创新的提示设计和损失函数，有效平衡了新知识学习和先前知识保留，为视觉语言模型的持续组合学习提供了有效解决方案。

Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.

</details>


### [5] [CS3D: An Efficient Facial Expression Recognition via Event Vision](https://arxiv.org/abs/2512.09592)
*Zhe Wang,Qijin Song,Yucen Peng,Weibang Bai*

Main category: cs.CV

TL;DR: CS3D框架通过分解3D卷积降低计算复杂度和能耗，结合软脉冲神经元和时空注意力机制，在事件相机上进行高效准确的面部表情识别。


<details>
  <summary>Details</summary>
Motivation: 事件相机在面部表情识别中具有高时间分辨率、低延迟等优势，但传统深度学习方法能耗高，难以部署在边缘设备上，限制了事件视觉方法的实际应用。

Method: 提出CS3D框架：1) 分解Convolutional 3D方法降低计算复杂度和能耗；2) 使用软脉冲神经元增强信息保留能力；3) 引入时空注意力机制提升检测精度。

Result: CS3D在多个数据集上比RNN、Transformer和C3D等架构获得更高准确率，同时能耗仅为原始C3D的21.97%。

Conclusion: CS3D框架有效解决了事件相机面部表情识别中的能耗问题，实现了高效准确的人机交互，适合边缘设备部署。

Abstract: Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.

</details>


### [6] [ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning](https://arxiv.org/abs/2512.09924)
*Xinyu Liu,Hangjie Yuan,Yujie Wei,Jiazheng Xing,Yujin Han,Jiahao Pan,Yanbiao Ma,Chi-Min Chan,Kang Zhao,Shiwei Zhang,Wenhan Luo,Yike Guo*

Main category: cs.CV

TL;DR: 提出RVE任务和ReViSE框架，通过自反思推理统一生成与评估，显著提升视频编辑的推理准确性和视觉质量


<details>
  <summary>Details</summary>
Motivation: 现有视频统一模型在理解和生成方面表现强大，但在推理感知的视频编辑方面存在不足。这主要由于两个原因：1) 缺乏专门用于训练和评估推理感知视频编辑的数据集；2) 模型推理能力与编辑能力之间存在脱节，丰富的理解无法有效指导编辑过程。

Method: 1) 提出推理感知视频编辑(RVE)任务，要求编辑时考虑物理合理性和因果动态；2) 构建RVE-Bench基准，包含推理感知视频编辑和上下文视频生成两个互补子集；3) 提出ReViSE框架，采用自反思推理(SRF)架构，将生成与评估统一在单一架构中，利用内部VLM提供内在反馈来优化生成器的推理行为。

Result: 在RVE-Bench上的大量实验表明，ReViSE显著提升了编辑准确性和视觉保真度，在推理感知视频编辑子集上相比最先进方法实现了32%的总体分数提升。

Conclusion: 通过引入RVE任务、构建系统评估基准以及提出统一的ReViSE框架，成功弥合了视频模型推理能力与编辑能力之间的鸿沟，为推理感知的视频编辑提供了有效的解决方案。

Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [LUMOS: Large User MOdels for User Behavior Prediction](https://arxiv.org/abs/2512.08957)
*Dhruv Nigam*

Main category: cs.LG

TL;DR: LUMOS是一个基于Transformer的大规模用户行为预测模型，通过联合学习多个任务并利用原始用户活动数据，消除了传统任务特定模型和手动特征工程的需求。


<details>
  <summary>Details</summary>
Motivation: 在线B2C平台面临大规模用户行为预测的挑战，传统方法依赖任务特定模型和领域特定特征工程，这既耗时又计算昂贵，且需要领域专业知识，难以扩展。

Method: LUMOS采用Transformer架构，引入新颖的交叉注意力机制，将预测条件化于未来已知事件（如节假日、促销等），并使用多模态标记化技术，将用户交易、事件上下文和静态用户人口统计属性通过专门的嵌入路径处理。

Result: 在包含2750亿用户活动标记和2.5亿用户的生产数据集上，LUMOS在5个任务中相比传统基线模型，二分类任务ROC-AUC平均提升0.025，回归任务MAPE降低4.6%。在线A/B测试显示每日活跃用户增加3.15%。

Conclusion: LUMOS通过消除任务特定模型和手动特征工程，实现了可扩展的大规模用户行为预测，显著提升了预测性能并带来了可衡量的业务影响。

Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.

</details>


### [8] [Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction](https://arxiv.org/abs/2512.09074)
*Shangqing Xu,Zhiyuan Zhao,Megha Sharma,José María Martín-Olalla,Alexander Rodríguez,Gregory A. Wellenius,B. Aditya Prakash*

Main category: cs.LG

TL;DR: DeepTherm是一个模块化的致命热浪早期预警系统，无需热相关死亡率历史数据，通过深度学习分离基线死亡率与热浪影响，实现准确预警。


<details>
  <summary>Details</summary>
Motivation: 城市严重热浪对公共健康构成重大威胁，需要建立早期预警策略。现有方法难以预测即将到来的致命热浪，主要因为热相关死亡率难以定义和估计，且早期预警系统需要满足数据可用性、时空鲁棒性和决策成本等额外要求。

Method: 提出DeepTherm模块化早期预警系统，采用深度学习双预测管道，从全因死亡率中分离出无热浪和其他不规则事件时的基线死亡率，无需热相关死亡率历史数据。

Result: 在西班牙真实数据上评估显示，DeepTherm在不同地区、时间段和人群群体中表现一致、鲁棒且准确，同时允许在漏报和误报之间进行权衡。

Conclusion: DeepTherm成功解决了预测致命热浪的挑战，提供了一个灵活、鲁棒的早期预警系统，能够在没有历史死亡率数据的情况下准确预测致命热浪，为公共卫生决策提供支持。

Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.

</details>


### [9] [Goal inference with Rao-Blackwellized Particle Filters](https://arxiv.org/abs/2512.09269)
*Yixuan Wang,Dan P. Guralnik,Warren E. Dixon*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Rao-Blackwellized粒子滤波的意图推断方法，用于从噪声轨迹观测中推断移动代理的最终目标，并引入信息论泄露度量来量化推断效果。


<details>
  <summary>Details</summary>
Motivation: 从噪声轨迹观测中推断移动代理的最终目标是一个基本估计问题。现有方法在样本效率方面存在不足，需要开发更有效的意图推断技术，同时量化推断效果。

Method: 使用Rao-Blackwellized粒子滤波变体，利用代理的闭环行为假设和闭式动力学模型，解析地边缘化线性高斯子结构，仅更新粒子权重。引入两种差分估计器：基于RBPF权重的高斯混合模型和限制在有效样本上的简化版本。

Result: 实验表明该方法能快速准确地恢复合规代理的意图。通过高斯混合KL界限提供了真实意图分布与RBPF估计之间KL散度的可计算下界，并证明简化估计器的性能几乎与完整版本相当。

Conclusion: 该方法在意图推断方面表现出色，为设计意图混淆控制器提供了理论基础，未来可进一步研究如何设计能隐藏意图的控制策略。

Abstract: Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.

</details>


### [10] [Improved Physics-Driven Neural Network to Solve Inverse Scattering Problems](https://arxiv.org/abs/2512.09333)
*Yutong Du,Zicheng Liu,Bo Wu,Jingwei Kou,Hang Li,Changyou Li,Yali Zong,Bo Qi*

Main category: cs.LG

TL;DR: 提出改进的物理驱动神经网络框架IPDNN，用于电磁逆散射问题求解，引入新的激活函数GLOW和动态散射子区域识别策略，结合迁移学习提升实用性和效率。


<details>
  <summary>Details</summary>
Motivation: 电磁逆散射问题在医学成像、无损检测等领域有重要应用，但传统方法存在收敛不稳定、计算成本高、难以实时应用等问题。需要结合物理模型的可解释性和神经网络的高效推理能力。

Method: 1. 提出GLOW激活函数（高斯局部化振荡抑制窗口）稳定收敛并实现轻量级网络架构；2. 开发动态散射子区域识别策略，自适应细化计算域；3. 结合迁移学习扩展求解器适用性；4. 将迭代算法的物理可解释性与神经网络的实时推理能力相结合。

Result: 数值模拟和实验结果表明，所提出的求解器在重建精度、鲁棒性和效率方面均优于现有最先进方法，实现了更准确、更稳定、更高效的重建效果。

Conclusion: IPDNN框架成功解决了电磁逆散射问题中的关键挑战，通过创新的激活函数、自适应区域识别和迁移学习策略，实现了物理可解释性与实时推理能力的有效结合，为实际应用提供了高效可靠的解决方案。

Abstract: This paper presents an improved physics-driven neural network (IPDNN) framework for solving electromagnetic inverse scattering problems (ISPs). A new Gaussian-localized oscillation-suppressing window (GLOW) activation function is introduced to stabilize convergence and enable a lightweight yet accurate network architecture. A dynamic scatter subregion identification strategy is further developed to adaptively refine the computational domain, preventing missed detections and reducing computational cost. Moreover, transfer learning is incorporated to extend the solver's applicability to practical scenarios, integrating the physical interpretability of iterative algorithms with the real-time inference capability of neural networks. Numerical simulations and experimental results demonstrate that the proposed solver achieves superior reconstruction accuracy, robustness, and efficiency compared with existing state-of-the-art methods.

</details>


### [11] [CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning](https://arxiv.org/abs/2512.09368)
*Mingyuan Li,Chunyu Liu,Zhuojun Li,Xiao Liu,Guangsheng Yu,Bo Du,Jun Shen,Qiang Wu*

Main category: cs.LG

TL;DR: 提出CFLight框架，通过反事实学习增强交通信号控制中的安全性，在保持效率的同时显著减少碰撞事故


<details>
  <summary>Details</summary>
Motivation: 交通信号控制中强化学习方法通常优先考虑效率而忽视安全性，且缺乏可解释性，需要平衡安全与效率并提高可解释性

Method: 提出基于反事实学习的框架，构建结构因果模型预测不同行动结果，集成CF模块和"X"模块，开发CFLight算法实现近零碰撞控制策略

Result: 在真实世界和合成数据集上的实验表明，CFLight相比传统RL方法和近期安全RL模型，能减少碰撞并提升整体交通性能

Conclusion: CFLight提供了一个通用且安全的RL框架，能有效处理交通安全事件，为其他领域应用提供了可能性

Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.

</details>


### [12] [Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning](https://arxiv.org/abs/2512.09706)
*Kaichen He,Zihao Wang,Muyao Li,Anji Liu,Yitao Liang*

Main category: cs.LG

TL;DR: CrossAgent是一个统一智能体模型，能够掌握异构动作空间并自主选择最优交互接口，在Minecraft环境中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常局限于静态预定义动作空间（如API、GUI事件或机器人命令），这种刚性限制了它们在动态环境中的适应性，因为最优交互粒度会随上下文变化

Method: 提出统一智能体模型CrossAgent，采用包含冷启动监督微调和多轮组相对策略优化（GRPO）算法的综合训练流程，使智能体能够学习自适应动作切换，无需人工指定规则

Result: 在开放世界Minecraft环境的800多个任务上进行广泛实验，CrossAgent实现了最先进的性能，通过动态利用不同动作空间的优势，显著优于固定动作基线，在长时程推理中表现出卓越的泛化能力和效率

Conclusion: CrossAgent通过掌握异构动作空间和自主选择最优接口，解决了现有智能体在动态环境中适应性受限的问题，为智能体AI从工程化复杂工作流向后训练原生模型的范式转变提供了有效方案

Abstract: The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni是一个全模态大语言模型，专注于增强音频视频的显式和隐式时间定位能力，通过时间戳标记和强化学习实现跨模态时间建模，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对视觉语言场景，专注于显式时间定位问题，但对音频模态利用不足，且忽略了跨模态的隐式时间关系（如视觉内容与语音的对应关系），而这些在现实场景中很常见。

Method: 1. 在每个时间单元中将基于文本的时间戳标记与视觉和音频表示交错，实现跨模态的统一时间建模；2. 通过专门设计的奖励函数结合强化学习，强制正确的时间顺序并增强细粒度时间推理；3. 构建ChronusAV数据集，支持音频视频时间定位任务的训练和评估。

Result: ChronusOmni在ChronusAV数据集上实现了超过30%的性能提升，并在其他时间定位基准测试的大多数指标上达到最佳结果，同时保持了通用的视频和音频理解能力。

Conclusion: ChronusOmni展示了强大的跨模态时间感知能力，能够处理显式和隐式的时间定位问题，为全模态大语言模型的时间理解提供了有效解决方案。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>
