{"id": "2511.14010", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14010", "abs": "https://arxiv.org/abs/2511.14010", "authors": ["Chenchen Kuai", "Zihao Li", "Braden Rosen", "Stephanie Paan", "Navid Jafari", "Jean-Louis Briaud", "Yunlong Zhang", "Youssef M. A. Hashash", "Yang Zhou"], "title": "Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports", "comment": "17 pages, 5 figures", "summary": "Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.", "AI": {"tldr": "MoRA-RAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u68c0\u7d22\u673a\u5236\u548c\u4ee3\u7406\u5206\u5757\u6280\u672f\uff0c\u5c06\u707e\u540e\u52d8\u5bdf\u62a5\u544a\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u7528\u4e8e\u591a\u707e\u5bb3\u63a8\u7406\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe94.5%\u3002", "motivation": "\u707e\u540e\u52d8\u5bdf\u62a5\u544a\u5305\u542b\u7406\u89e3\u591a\u707e\u5bb3\u76f8\u4e92\u4f5c\u7528\u7684\u5173\u952e\u8bc1\u636e\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u53d9\u8ff0\u4f7f\u5f97\u7cfb\u7edf\u77e5\u8bc6\u4f20\u9012\u56f0\u96be\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f3a\u4e4f\u9886\u57df\u57fa\u7840\u65f6\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9760\u6216\u5e7b\u89c9\u8f93\u51fa\u3002", "method": "\u5f00\u53d1\u4e86MoRA-RAG\u6846\u67b6\uff0c\u5305\u542b\u6df7\u5408\u68c0\u7d22\u673a\u5236\u52a8\u6001\u8def\u7531\u8de8\u707e\u5bb3\u7279\u5b9a\u6570\u636e\u5e93\u7684\u67e5\u8be2\uff0c\u4f7f\u7528\u4ee3\u7406\u5206\u5757\u4fdd\u6301\u68c0\u7d22\u4e2d\u7684\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff0c\u5e76\u5305\u542b\u9a8c\u8bc1\u5faa\u73af\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u3001\u4f18\u5316\u67e5\u8be2\u5e76\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u65f6\u542f\u52a8\u9488\u5bf9\u6027\u641c\u7d22\u3002", "result": "MoRA-RAG\u5728HazardRecQA\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.5%\u51c6\u786e\u7387\uff0c\u6bd4\u96f6\u6837\u672cLLM\u63d0\u9ad830%\uff0c\u6bd4\u6700\u5148\u8fdbRAG\u7cfb\u7edf\u63d0\u9ad810%\uff0c\u540c\u65f6\u5728\u4e0d\u540cLLM\u67b6\u6784\u4e2d\u51cf\u5c11\u5e7b\u89c9\u3002\u8fd8\u4f7f\u5f00\u6e90LLM\u8fbe\u5230\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "MoRA-RAG\u4e3a\u5c06\u707e\u540e\u6587\u6863\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u3001\u53ef\u4fe1\u8d56\u7684\u707e\u5bb3\u97e7\u6027\u60c5\u62a5\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.13892", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13892", "abs": "https://arxiv.org/abs/2511.13892", "authors": ["Badhan Chandra Das", "Md Tasnim Jawad", "Md Jueal Mia", "M. Hadi Amini", "Yanzhao Wu"], "title": "Jailbreaking Large Vision Language Models in Intelligent Transportation Systems", "comment": null, "summary": "Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8d8a\u72f1\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u50cf\u6392\u7248\u64cd\u7eb5\u548c\u591a\u8f6e\u63d0\u793a\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u5c42\u54cd\u5e94\u8fc7\u6ee4\u9632\u5fa1\u6280\u672f\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u6781\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5176\u8106\u5f31\u6027\u5e76\u5f00\u53d1\u6709\u6548\u9632\u5fa1\u63aa\u65bd\u3002", "method": "\u6784\u5efa\u4ea4\u901a\u76f8\u5173\u6709\u5bb3\u67e5\u8be2\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u6392\u7248\u64cd\u7eb5\u548c\u591a\u8f6e\u63d0\u793a\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\uff1b\u5f00\u53d1\u591a\u5c42\u54cd\u5e94\u8fc7\u6ee4\u9632\u5fa1\u6280\u672f\uff1b\u4f7f\u7528GPT-4\u5224\u65ad\u548c\u4eba\u5de5\u9a8c\u8bc1\u8bc4\u4f30\u653b\u51fb\u548c\u9632\u5fa1\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u63d0\u51fa\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90LVLMs\u5747\u6709\u6548\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5177\u6709\u66f4\u5f3a\u7684\u653b\u51fb\u80fd\u529b\uff0c\u540c\u65f6\u9632\u5fa1\u6280\u672f\u80fd\u591f\u6709\u6548\u963b\u6b62\u6a21\u578b\u751f\u6210\u4e0d\u5f53\u54cd\u5e94\u3002", "conclusion": "LVLMs\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u56fe\u50cf\u6392\u7248\u64cd\u7eb5\u548c\u591a\u8f6e\u63d0\u793a\u7684\u8d8a\u72f1\u653b\u51fb\u5a01\u80c1\u5de8\u5927\uff0c\u9700\u8981\u591a\u5c42\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u969c\u7cfb\u7edf\u5b89\u5168\u3002"}}
{"id": "2511.14106", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14106", "abs": "https://arxiv.org/abs/2511.14106", "authors": ["Le Yu", "Zhengyue Zhao", "Yawen Zheng", "Yunhao Liu"], "title": "Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT", "comment": "10 pages, 7 figures", "summary": "Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \\textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \\textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \\textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \\textcolor{red}{\\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStealth Fine-Tuning\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6bb5\u5e72\u6270\u548c\u81ea\u751f\u6210\u76d1\u7763\u6570\u636e\uff0c\u4ec5\u7528499\u4e2a\u6837\u672c\u57283\u5c0f\u65f6\u5185\u5c31\u80fd\u6709\u6548\u7a81\u7834RVLMs\u7684\u5b89\u5168\u5bf9\u9f50\u9632\u5fa1\uff0c\u653b\u51fb\u6210\u529f\u7387\u6bd4IDEATOR\u63d0\u9ad838.52%", "motivation": "\u867d\u7136RVLMs\u4f9d\u8d56\u5b89\u5168\u5bf9\u9f50\u6765\u9632\u6b62\u6709\u5bb3\u884c\u4e3a\uff0c\u4f46\u5176\u66b4\u9732\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u5b89\u5168\u5bf9\u9f50\u5bb9\u6613\u88ab\u7a81\u7834", "method": "\u4f7f\u7528\u5206\u6bb5\u5e72\u6270\u5f15\u53d1\u6709\u5bb3\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u5c06\u81ea\u751f\u6210\u8f93\u51fa\u4f5c\u4e3a\u76d1\u7763\u5fae\u8c03\u6570\u636e\uff0c\u91c7\u7528\u8f6e\u6b21\u52a0\u6743\u635f\u5931\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u3001\u5206\u5e03\u4e00\u81f4\u7684\u5fae\u8c03\u65b9\u6cd5", "result": "\u5728AdvBench\u548c\u591a\u4e2a\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStealth Fine-Tuning\u4ee5\u4f4e\u6210\u672c\u9ad8\u6548\u5730\u7ed5\u8fc7\u5bf9\u9f50\u9632\u5fa1\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u63a8\u7406\u80fd\u529b", "conclusion": "Stealth Fine-Tuning\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u7a81\u7834RVLMs\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u539f\u59cb\u8868\u793a\u5206\u5e03"}}
{"id": "2511.13853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13853", "abs": "https://arxiv.org/abs/2511.13853", "authors": ["Xinxin Liu", "Zhaopan Xu", "Kai Wang", "Yong Jae Lee", "Yuzhang Shang"], "title": "Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark", "comment": "10 pages", "summary": "While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.", "AI": {"tldr": "Gen-ViRe\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u94fe\u5f0f\u5e27\u63a8\u7406\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u4fdd\u771f\u5ea6\u6216\u5bf9\u9f50\u5ea6\uff0c\u65e0\u6cd5\u8bc4\u4f30\u94fe\u5f0f\u5e27\u63a8\u7406\u7684\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff0c\u5982\u591a\u6b65\u89c4\u5212\u3001\u7b97\u6cd5\u903b\u8f91\u548c\u62bd\u8c61\u6a21\u5f0f\u63a8\u65ad\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u7cfb\u7edf\u7406\u89e3\u548c\u6539\u8fdb\u6307\u5bfc\u3002", "method": "\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u548c\u5b9e\u9645AI\u5e94\u7528\uff0c\u5c06\u94fe\u5f0f\u5e27\u63a8\u7406\u5206\u89e3\u4e3a6\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\u548c24\u4e2a\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6e90\u6570\u636e\u6574\u7406\u3001\u6700\u5c0f\u63d0\u793a\u534f\u8bae\u548c\u6df7\u5408VLM\u8f85\u52a9\u8bc4\u4f30\u6765\u91cf\u5316\u89c6\u9891\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7cfb\u7edf\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u89c6\u89c9\u8d28\u91cf\u4e0e\u63a8\u7406\u6df1\u5ea6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e3a\u771f\u6b63\u7684\u4e16\u754c\u6a21\u62df\u5668\u5efa\u7acb\u4e86\u57fa\u51c6\u548c\u8bca\u65ad\u5de5\u5177\u3002", "conclusion": "Gen-ViRe\u63d0\u4f9b\u4e86\u9996\u4e2a\u5bf9\u89c6\u9891\u6a21\u578b\u4f5c\u4e3a\u63a8\u7406\u5668\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u771f\u6b63\u5177\u6709\u8ba4\u77e5\u80fd\u529b\u7684\u4e16\u754c\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.13756", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13756", "abs": "https://arxiv.org/abs/2511.13756", "authors": ["Niklas Erdmann", "Lars Bentsen", "Roy Stenbro", "Heine Nygard Riise", "Narada Dilp Warakagoda", "Paal E. Engelstad"], "title": "Multi-Horizon Time Series Forecasting of non-parametric CDFs with Deep Lattice Networks", "comment": null, "summary": "Probabilistic forecasting is not only a way to add more information to a prediction of the future, but it also builds on weaknesses in point prediction. Sudden changes in a time series can still be captured by a cumulative distribution function (CDF), while a point prediction is likely to miss it entirely. The modeling of CDFs within forecasts has historically been limited to parametric approaches, but due to recent advances, this no longer has to be the case. We aim to advance the fields of probabilistic forecasting and monotonic networks by connecting them and propose an approach that permits the forecasting of implicit, complete, and nonparametric CDFs. For this purpose, we propose an adaptation to deep lattice networks (DLN) for monotonically constrained simultaneous/implicit quantile regression in time series forecasting. Quantile regression usually produces quantile crossovers, which need to be prevented to achieve a legitimate CDF. By leveraging long short term memory units (LSTM) as the embedding layer, and spreading quantile inputs to all sub-lattices of a DLN with an extended output size, we can produce a multi-horizon forecast of an implicit CDF due to the monotonic constraintability of DLNs that prevent quantile crossovers. We compare and evaluate our approach's performance to relevant state of the art within the context of a highly relevant application of time series forecasting: Day-ahead, hourly forecasts of solar irradiance observations. Our experiments show that the adaptation of a DLN performs just as well or even better than an unconstrained approach. Further comparison of the adapted DLN against a scalable monotonic neural network shows that our approach performs better. With this adaptation of DLNs, we intend to create more interest and crossover investigations in techniques of monotonic neural networks and probabilistic forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u683c\u7f51\u7edc(DLN)\u7684\u5355\u8c03\u7ea6\u675f\u9690\u5f0f\u5206\u4f4d\u6570\u56de\u5f52\u65b9\u6cd5\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u6982\u7387\u9884\u6d4b\uff0c\u80fd\u591f\u751f\u6210\u5b8c\u6574\u3001\u975e\u53c2\u6570\u5316\u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570(CDF)\uff0c\u907f\u514d\u4e86\u5206\u4f4d\u6570\u4ea4\u53c9\u95ee\u9898\u3002", "motivation": "\u6982\u7387\u9884\u6d4b\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u66f4\u591a\u672a\u6765\u4fe1\u606f\uff0c\u8fd8\u80fd\u5f25\u8865\u70b9\u9884\u6d4b\u7684\u5f31\u70b9\u3002\u4f20\u7edfCDF\u5efa\u6a21\u4e3b\u8981\u9650\u4e8e\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u672c\u6587\u65e8\u5728\u8fde\u63a5\u6982\u7387\u9884\u6d4b\u548c\u5355\u8c03\u7f51\u7edc\u9886\u57df\uff0c\u5b9e\u73b0\u9690\u5f0f\u3001\u5b8c\u6574\u3001\u975e\u53c2\u6570\u5316\u7684CDF\u9884\u6d4b\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u683c\u7f51\u7edc(DLN)\u8fdb\u884c\u5355\u8c03\u7ea6\u675f\u7684\u9690\u5f0f\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u4f7f\u7528LSTM\u4f5c\u4e3a\u5d4c\u5165\u5c42\uff0c\u5c06\u5206\u4f4d\u6570\u8f93\u5165\u6269\u5c55\u5230\u6240\u6709\u5b50\u683c\uff0c\u5229\u7528DLN\u7684\u5355\u8c03\u7ea6\u675f\u80fd\u529b\u9632\u6b62\u5206\u4f4d\u6570\u4ea4\u53c9\uff0c\u5b9e\u73b0\u591a\u65f6\u95f4\u6b65\u957f\u7684\u9690\u5f0fCDF\u9884\u6d4b\u3002", "result": "\u5728\u592a\u9633\u80fd\u8f90\u7167\u5ea6\u9884\u6d4b\u5b9e\u9a8c\u4e2d\uff0c\u6539\u8fdb\u7684DLN\u65b9\u6cd5\u6027\u80fd\u4e0e\u65e0\u7ea6\u675f\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u4e14\u4f18\u4e8e\u53ef\u6269\u5c55\u5355\u8c03\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "\u901a\u8fc7DLN\u7684\u6539\u8fdb\uff0c\u65e8\u5728\u4fc3\u8fdb\u5355\u8c03\u795e\u7ecf\u7f51\u7edc\u548c\u6982\u7387\u9884\u6d4b\u6280\u672f\u4e4b\u95f4\u7684\u4ea4\u53c9\u7814\u7a76\u5174\u8da3\u3002"}}
{"id": "2511.13863", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.13863", "abs": "https://arxiv.org/abs/2511.13863", "authors": ["Kranti Kumar Parida", "Omar Emara", "Hazel Doughty", "Dima Damen"], "title": "Segmenting Collision Sound Sources in Egocentric Videos", "comment": "Under Review. Webpage: https://krantiparida.github.io/projects/cs3.html", "summary": "Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief.\n  To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\\times$ and $4.7\\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.", "AI": {"tldr": "\u63d0\u51fa\u78b0\u649e\u58f0\u6e90\u5206\u5272\u4efb\u52a1(CS3)\uff0c\u901a\u8fc7\u97f3\u9891\u6761\u4ef6\u4ece\u89c6\u9891\u4e2d\u5206\u5272\u4ea7\u751f\u78b0\u649e\u58f0\u97f3\u7684\u7269\u4f53\uff0c\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u7279\u522b\u6709\u6548\u3002", "motivation": "\u4eba\u7c7b\u64c5\u957f\u591a\u611f\u5b98\u611f\u77e5\uff0c\u80fd\u4ece\u4ea4\u4e92\u58f0\u97f3\u4e2d\u8bc6\u522b\u7269\u4f53\u5c5e\u6027\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u78b0\u649e\u58f0\u97f3\u6765\u5206\u5272\u89c6\u89c9\u8f93\u5165\u4e2d\u4ea7\u751f\u58f0\u97f3\u7684\u7269\u4f53\u3002", "method": "\u4f7f\u7528\u5f31\u76d1\u7763\u7684\u97f3\u9891\u6761\u4ef6\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b(CLIP\u548cSAM2)\uff0c\u5e76\u6574\u5408\u81ea\u6211\u4e2d\u5fc3\u7ebf\u7d22(\u5982\u624b\u4e2d\u7269\u4f53)\u6765\u8bc6\u522b\u53ef\u80fd\u7684\u78b0\u649e\u58f0\u6e90\u3002", "result": "\u5728\u4e24\u4e2a\u65b0\u57fa\u51c6EPIC-CS3\u548cEgo4D-CS3\u4e0a\uff0c\u65b9\u6cd5\u6027\u80fd\u5206\u522b\u6bd4\u57fa\u7ebf\u9ad83\u500d\u548c4.7\u500d(mIoU)\u3002", "conclusion": "\u63d0\u51fa\u7684CS3\u4efb\u52a1\u548c\u65b9\u6cd5\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u78b0\u649e\u58f0\u6e90\u5206\u5272\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.14366", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14366", "abs": "https://arxiv.org/abs/2511.14366", "authors": ["Hongwei Liu", "Junnan Liu", "Shudong Liu", "Haodong Duan", "Yuqiang Li", "Mao Su", "Xiaohong Liu", "Guangtao Zhai", "Xinyu Fang", "Qianhong Ma", "Taolin Zhang", "Zihan Ma", "Yufeng Zhao", "Peiheng Zhou", "Linchen Xiao", "Wenlong Zhang", "Shijie Zhou", "Xingjian Ma", "Siqi Sun", "Jiaye Ge", "Meng Li", "Yuhong Liu", "Jianxin Dong", "Jiaying Li", "Hui Wu", "Hanwen Liang", "Jintai Lin", "Yanting Wang", "Jie Dong", "Tong Zhu", "Tianfan Fu", "Conghui He", "Qi Zhang", "Songyang Zhang", "Lei Bai", "Kai Chen"], "title": "ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning", "comment": "39 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable \"ruler\" for progress toward Artificial General Intelligence.", "AI": {"tldr": "ATLAS\u662f\u4e00\u4e2a\u9762\u5411AGI\u7684\u5927\u89c4\u6a21\u3001\u9ad8\u96be\u5ea6\u8de8\u5b66\u79d1\u79d1\u5b66\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542b\u7ea6800\u4e2a\u539f\u521b\u95ee\u9898\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u533a\u5206\u524d\u6cbf\u6a21\u578b\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6027\u80fd\u9971\u548c\u3001\u5b66\u79d1\u8303\u56f4\u72ed\u7a84\u3001\u7b54\u6848\u683c\u5f0f\u7b80\u5316\u3001\u6570\u636e\u6c61\u67d3\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u79d1\u5b66\u63a2\u7a76\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u7531\u9886\u57df\u4e13\u5bb6\u5f00\u53d1\uff0c\u6db5\u76d67\u4e2a\u6838\u5fc3\u79d1\u5b66\u9886\u57df\uff1b\u91c7\u7528\u539f\u521b\u95ee\u9898\u9632\u6b62\u6570\u636e\u6cc4\u9732\uff1b\u5f3a\u8c03\u8de8\u5b66\u79d1\u77e5\u8bc6\u6574\u5408\uff1b\u4f7f\u7528\u590d\u6742\u5f00\u653e\u5f0f\u7b54\u6848\u683c\u5f0f\uff1b\u5b9e\u65bd\u591a\u9636\u6bb5\u4e13\u5bb6\u8bc4\u5ba1\u548c\u8d28\u91cf\u63a7\u5236\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793aATLAS\u80fd\u6709\u6548\u533a\u5206\u9886\u5148\u6a21\u578b\u7684\u5148\u8fdb\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ATLAS\u5c06\u53d1\u5c55\u6210\u957f\u671f\u5f00\u653e\u7684\u793e\u533a\u9a71\u52a8\u5e73\u53f0\uff0c\u4e3aAGI\u8fdb\u5c55\u63d0\u4f9b\u53ef\u9760\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2511.13944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13944", "abs": "https://arxiv.org/abs/2511.13944", "authors": ["Noam Glazner", "Noam Tsfaty", "Sharon Shalev", "Avishai Weizman"], "title": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets", "comment": "1 figure, 1 table", "summary": "We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u7684\u5e27\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u89c6\u89c9\u76f8\u4f3c\u6027\u5206\u7ec4\u6765\u7f13\u89e3\u89c6\u9891\u5e27\u6570\u636e\u96c6\u4e2d\u7684\u4fe1\u606f\u6cc4\u9732\u95ee\u9898", "motivation": "\u89e3\u51b3\u89c6\u9891\u5e27\u6570\u636e\u96c6\u4e2d\u7531\u4e8e\u65f6\u95f4\u8fde\u7eed\u6027\u5bfc\u81f4\u7684\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\uff0c\u786e\u4fdd\u6570\u636e\u96c6\u5212\u5206\u66f4\u5177\u4ee3\u8868\u6027", "method": "\u5728\u5212\u5206\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e4b\u524d\uff0c\u5148\u5bf9\u89c6\u89c9\u76f8\u4f3c\u7684\u5e27\u8fdb\u884c\u805a\u7c7b\u5206\u7ec4", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u5177\u4ee3\u8868\u6027\u3001\u5e73\u8861\u6027\u548c\u53ef\u9760\u6027\u7684\u6570\u636e\u96c6\u5212\u5206", "conclusion": "\u57fa\u4e8e\u805a\u7c7b\u7684\u5e27\u9009\u62e9\u7b56\u7565\u662f\u7f13\u89e3\u89c6\u9891\u5e27\u6570\u636e\u96c6\u4e2d\u4fe1\u606f\u6cc4\u9732\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2511.14423", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14423", "abs": "https://arxiv.org/abs/2511.14423", "authors": ["Xin Yi", "Yue Li", "Dongsheng Shi", "Linlin Wang", "Xiaoling Wang", "Liang He"], "title": "Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduHarm\u57fa\u51c6\u548cTSSF\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u9632\u5fa1\u6559\u80b2\u573a\u666f\u4e2dLLM\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5305\u62ec\u8d8a\u72f1\u653b\u51fb\u548c\u5fae\u8c03\u653b\u51fb\u3002", "motivation": "LLM\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u9762\u4e34\u72ec\u7279\u7684\u5b89\u5168\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u5b89\u5168\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6559\u80b2\u573a\u666f\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u6784\u5efaEduHarm\u57fa\u51c6\uff0c\u63d0\u51fa\u4e09\u9636\u6bb5\u9632\u62a4\u6846\u67b6TSSF\uff1a\u5b89\u5168\u611f\u77e5\u6ce8\u610f\u529b\u91cd\u5bf9\u9f50\u3001\u5c42\u7ea7\u5b89\u5168\u5224\u65ad\u3001\u9632\u5fa1\u9a71\u52a8\u53cc\u8def\u7531\u3002", "result": "\u57288\u79cd\u8d8a\u72f1\u653b\u51fb\u7b56\u7565\u4e0a\u6709\u6548\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u57283\u4e2a\u5fae\u8c03\u653b\u51fb\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7a33\u5065\u9632\u5fa1\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u67e5\u8be2\u7684\u6548\u7528\u3002", "conclusion": "TSSF\u6846\u67b6\u80fd\u591f\u540c\u65f6\u7f13\u89e3\u8d8a\u72f1\u548c\u5fae\u8c03\u653b\u51fb\uff0c\u4e3a\u6559\u80b2LLM\u63d0\u4f9b\u6709\u6548\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2511.14056", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.DG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.14056", "abs": "https://arxiv.org/abs/2511.14056", "authors": ["Marios Papamichals", "Regina Ruane"], "title": "Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds", "comment": "This is the first version of the paper", "summary": "Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.", "AI": {"tldr": "\u63d0\u51fa\u4e86Radial Compensation (RC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u5207\u7a7a\u95f4\u4e2d\u7684\u57fa\u7840\u5bc6\u5ea6\uff0c\u4f7f\u4f3c\u7136\u4ec5\u4f9d\u8d56\u4e8e\u6d4b\u5730\u8ddd\u79bb\uff0c\u89e3\u8026\u53c2\u6570\u8bed\u4e49\u4e0e\u66f2\u7387\uff0c\u4e3a\u6d41\u5f62\u4e0a\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u9ed8\u8ba4\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u6d41\u5f62\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6307\u6570\u6620\u5c04\u548c\u4f53\u79ef\u4fdd\u6301\u56fe\u8868\u5206\u522b\u5b58\u5728\u96c5\u53ef\u6bd4\u77e9\u9635\u521a\u6027\u548c\u6d4b\u5730\u8ddd\u79bb\u626d\u66f2\u95ee\u9898\uff0c\u4e14\u90fd\u5c06\u66f2\u7387\u4e0e\u6a21\u578b\u53c2\u6570\u7ea0\u7f20\uff0c\u5bfc\u81f4\u68af\u5ea6\u65b9\u5dee\u589e\u5927\u3002\u5728\u9ad8\u7ef4\u6f5c\u5728\u5f52\u4e00\u5316\u6d41\u4e2d\uff0c\u5305\u88f9\u6307\u6570\u5148\u9a8c\u4f1a\u4f7f\u534a\u5f84\u62c9\u4f38\u8d85\u51fa\u66f2\u7387\u5c3a\u5ea6\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u4f3c\u7136\u5dee\u548c\u6c42\u89e3\u5668\u521a\u6027\u3002", "method": "\u5f15\u5165\u5f84\u5411\u8865\u507f(RC)\u65b9\u6cd5\uff0c\u5728\u5207\u7a7a\u95f4\u4e2d\u9009\u62e9\u57fa\u7840\u5bc6\u5ea6\uff0c\u4f7f\u4f3c\u7136\u4ec5\u4f9d\u8d56\u4e8e\u6d4b\u5730\u8ddd\u79bb\uff0c\u89e3\u8026\u53c2\u6570\u8bed\u4e49\u4e0e\u66f2\u7387\u3002\u63a8\u5bfc\u4e86Balanced-Exponential (bExp)\u56fe\u8868\u65cf\uff0c\u5e73\u8861\u4f53\u79ef\u626d\u66f2\u548c\u6d4b\u5730\u8bef\u5dee\u3002", "result": "RC\u5728\u5bc6\u5ea6\u3001VAE\u3001\u56fe\u50cf\u548c\u56fe\u7684\u6d41\u4ee5\u53ca\u86cb\u767d\u8d28\u6a21\u578b\u4e2d\u5747\u4ea7\u751f\u7a33\u5b9a\u7684\u751f\u6210\u6a21\u578b\u3002RC\u63d0\u9ad8\u4e86\u4f3c\u7136\uff0c\u6062\u590d\u4e86\u6e05\u6670\u7684\u6d4b\u5730\u534a\u5f84\uff0c\u5e76\u9632\u6b62\u4e86\u9ad8\u7ef4\u6d41\u4e2d\u7684\u534a\u5f84\u7206\u70b8\u3002", "conclusion": "RC-bExp\u6210\u4e3a\u6d41\u5f62\u4e0a\u4f3c\u7136\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u7a33\u5065\u9ed8\u8ba4\u65b9\u6848\uff0c\u901a\u8fc7\u89e3\u8026\u66f2\u7387\u4e0e\u53c2\u6570\u8bed\u4e49\uff0c\u6539\u5584\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6570\u503c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.14075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14075", "abs": "https://arxiv.org/abs/2511.14075", "authors": ["Nakkyu Yang", "Yechan Lee", "SooJean Han"], "title": "CFG-EC: Error Correction Classifier-Free Guidance", "comment": null, "summary": "Classifier-Free Guidance (CFG) has become a mainstream approach for simultaneously improving prompt fidelity and generation quality in conditional generative models. During training, CFG stochastically alternates between conditional and null prompts to enable both conditional and unconditional generation. However, during sampling, CFG outputs both null and conditional prompts simultaneously, leading to inconsistent noise estimates between the training and sampling processes. To reduce this error, we propose CFG-EC, a versatile correction scheme augmentable to any CFG-based method by refining the unconditional noise predictions. CFG-EC actively realigns the unconditional noise error component to be orthogonal to the conditional error component. This corrective maneuver prevents interference between the two guidance components, thereby constraining the sampling error's upper bound and establishing more reliable guidance trajectories for high-fidelity image generation. Our numerical experiments show that CFG-EC handles the unconditional component more effectively than CFG and CFG++, delivering a marked performance increase in the low guidance sampling regime and consistently higher prompt alignment across the board.", "AI": {"tldr": "\u63d0\u51fa\u4e86CFG-EC\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63\u65e0\u6761\u4ef6\u566a\u58f0\u9884\u6d4b\u6765\u51cf\u5c11CFG\u5728\u8bad\u7ec3\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u5ea6\u3002", "motivation": "CFG\u5728\u8bad\u7ec3\u65f6\u4ea4\u66ff\u4f7f\u7528\u6761\u4ef6\u63d0\u793a\u548c\u65e0\u63d0\u793a\uff0c\u4f46\u5728\u91c7\u6837\u65f6\u540c\u65f6\u8f93\u51fa\u4e24\u8005\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u4f30\u8ba1\u4e0d\u4e00\u81f4\uff0c\u4ea7\u751f\u8bef\u5dee\u3002", "method": "CFG-EC\u901a\u8fc7\u5c06\u65e0\u6761\u4ef6\u566a\u58f0\u8bef\u5dee\u5206\u91cf\u91cd\u65b0\u5bf9\u9f50\u4e3a\u4e0e\u6761\u4ef6\u8bef\u5dee\u5206\u91cf\u6b63\u4ea4\uff0c\u9632\u6b62\u4e24\u4e2a\u5f15\u5bfc\u5206\u91cf\u4e4b\u95f4\u7684\u5e72\u6270\uff0c\u4ece\u800c\u7ea6\u675f\u91c7\u6837\u8bef\u5dee\u7684\u4e0a\u754c\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660eCFG-EC\u6bd4CFG\u548cCFG++\u66f4\u6709\u6548\u5730\u5904\u7406\u65e0\u6761\u4ef6\u5206\u91cf\uff0c\u5728\u4f4e\u5f15\u5bfc\u91c7\u6837\u673a\u5236\u4e0b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6574\u4f53\u63d0\u793a\u5bf9\u9f50\u5ea6\u66f4\u9ad8\u3002", "conclusion": "CFG-EC\u662f\u4e00\u79cd\u53ef\u589e\u5f3a\u4efb\u4f55\u57fa\u4e8eCFG\u65b9\u6cd5\u7684\u901a\u7528\u6821\u6b63\u65b9\u6848\uff0c\u901a\u8fc7\u6539\u8fdb\u65e0\u6761\u4ef6\u566a\u58f0\u9884\u6d4b\u6765\u5efa\u7acb\u66f4\u53ef\u9760\u7684\u5f15\u5bfc\u8f68\u8ff9\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2511.14133", "categories": ["cs.LG", "econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.14133", "abs": "https://arxiv.org/abs/2511.14133", "authors": ["Jessy Xinyi Han", "Devavrat Shah"], "title": "Synthetic Survival Control: Extending Synthetic Controls for \"When-If\" Decision", "comment": null, "summary": "Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such \"when-if\" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.", "AI": {"tldr": "\u63d0\u51faSynthetic Survival Control (SSC)\u65b9\u6cd5\uff0c\u5728\u9762\u677f\u6570\u636e\u8bbe\u7f6e\u4e2d\u4f30\u8ba1\u53cd\u4e8b\u5b9e\u98ce\u9669\u8f68\u8ff9\uff0c\u89e3\u51b3\u89c2\u5bdf\u6570\u636e\u4e2d\u56e0\u679c\u63a8\u65ad\u7684\u6311\u6218", "motivation": "\u89c2\u5bdf\u6570\u636e\u4e2d\u65f6\u95f4\u5230\u4e8b\u4ef6\u7ed3\u679c\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u9762\u4e34\u5220\u5931\u3001\u6837\u672c\u91cf\u6709\u9650\u548c\u975e\u968f\u673a\u6cbb\u7597\u5206\u914d\u7684\u6311\u6218\uff0c\u9700\u8981\u56de\u7b54\"\u5982\u679c-\u4f55\u65f6\"\u95ee\u9898", "method": "SSC\u5c06\u76ee\u6807\u5355\u5143\u7684\u53cd\u4e8b\u5b9e\u98ce\u9669\u8f68\u8ff9\u4f30\u8ba1\u4e3a\u5176\u4ed6\u5355\u5143\u89c2\u5bdf\u8f68\u8ff9\u7684\u52a0\u6743\u7ec4\u5408\uff0c\u5f15\u5165\u5177\u6709\u4f4e\u79e9\u7ed3\u6784\u7684\u9762\u677f\u6846\u67b6\u8fdb\u884c\u56e0\u679c\u751f\u5b58\u5206\u6790", "result": "\u5728\u764c\u75c7\u6cbb\u7597\u7ed3\u679c\u7684\u591a\u56fd\u4e34\u5e8a\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0c\u53d1\u73b0\u65b0\u578b\u6cbb\u7597\u4e0e\u6539\u5584\u751f\u5b58\u76f8\u5173\uff0c\u5e72\u9884\u540e\u98ce\u9669\u8f68\u8ff9\u4f4e\u4e8e\u5408\u6210\u5bf9\u5e94\u7269", "conclusion": "SSC\u4e3a\u4f7f\u7528\u89c2\u5bdf\u6570\u636e\u8fdb\u884c\u53cd\u4e8b\u5b9e\u751f\u5b58\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u5728\u533b\u5b66\u3001\u7ecf\u6d4e\u5b66\u548c\u516c\u5171\u653f\u7b56\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u4ef7\u503c"}}
{"id": "2511.14113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14113", "abs": "https://arxiv.org/abs/2511.14113", "authors": ["Ziyao Zeng", "Jingcheng Ni", "Ruyi Liu", "Alex Wong"], "title": "Coffee: Controllable Diffusion Fine-tuning", "comment": null, "summary": "Text-to-image diffusion models can generate diverse content with flexible prompts, which makes them well-suited for customization through fine-tuning with a small amount of user-provided data. However, controllable fine-tuning that prevents models from learning undesired concepts present in the fine-tuning data, and from entangling those concepts with user prompts, remains an open challenge. It is crucial for downstream tasks like bias mitigation, preventing malicious adaptation, attribute disentanglement, and generalizable fine-tuning of diffusion policy. We propose Coffee that allows using language to specify undesired concepts to regularize the adaptation process. The crux of our method lies in keeping the embeddings of the user prompt from aligning with undesired concepts. Crucially, Coffee requires no additional training and enables flexible modification of undesired concepts by modifying textual descriptions. We evaluate Coffee by fine-tuning on images associated with user prompts paired with undesired concepts. Experimental results demonstrate that Coffee can prevent text-to-image models from learning specified undesired concepts during fine-tuning and outperforms existing methods. Code will be released upon acceptance.", "AI": {"tldr": "Coffee\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u6765\u6307\u5b9a\u4e0d\u5e0c\u671b\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u9632\u6b62\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4e0d\u9700\u8981\u7684\u6982\u5ff5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5fae\u8c03\u65f6\u5bb9\u6613\u5b66\u4e60\u5230\u4e0d\u9700\u8981\u7684\u6982\u5ff5\uff0c\u5e76\u4e0e\u7528\u6237\u63d0\u793a\u4ea7\u751f\u7ea0\u7f20\uff0c\u8fd9\u5728\u504f\u89c1\u7f13\u89e3\u3001\u9632\u6b62\u6076\u610f\u9002\u5e94\u3001\u5c5e\u6027\u89e3\u8026\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4fdd\u6301\u7528\u6237\u63d0\u793a\u7684\u5d4c\u5165\u4e0e\u4e0d\u5e0c\u671b\u5b66\u4e60\u7684\u6982\u5ff5\u4e0d\u5bf9\u9f50\uff0c\u4f7f\u7528\u8bed\u8a00\u63cf\u8ff0\u6765\u89c4\u8303\u9002\u5e94\u8fc7\u7a0b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u4e14\u53ef\u4ee5\u7075\u6d3b\u4fee\u6539\u4e0d\u5e0c\u671b\u5b66\u4e60\u7684\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoffee\u80fd\u591f\u6709\u6548\u9632\u6b62\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u6307\u5b9a\u7684\u4e0d\u5e0c\u671b\u6982\u5ff5\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Coffee\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53ef\u63a7\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u6765\u9632\u6b62\u6a21\u578b\u5b66\u4e60\u4e0d\u9700\u8981\u7684\u6982\u5ff5\uff0c\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.14120", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14120", "abs": "https://arxiv.org/abs/2511.14120", "authors": ["Hao Zhen", "Yunxiang Yang", "Jidong J. Yang"], "title": "Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models", "comment": "23 pages, 4 figures, 3 tables", "summary": "Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86MP-PVIR\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u89c6\u9891\u5206\u6790\u548c\u884c\u4eba\u884c\u4e3a\u9636\u6bb5\u5206\u5272\uff0c\u5c06\u884c\u4eba-\u8f66\u8f86\u4e8b\u6545\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8bca\u65ad\u62a5\u544a\uff0c\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7cfb\u7edf\u53ea\u80fd\u68c0\u6d4b\u4e8b\u6545\u662f\u5426\u53d1\u751f\uff0c\u4f46\u65e0\u6cd5\u5206\u6790\u4e8b\u6545\u5728\u4e0d\u540c\u8ba4\u77e5\u9636\u6bb5\u7684\u53d1\u5c55\u8fc7\u7a0b\uff0c\u4e14\u7f3a\u4e4f\u591a\u89c6\u89d2\u96c6\u6210\u548c\u65f6\u95f4\u7ed3\u6784\u5316\u5904\u7406\u3002", "method": "\u56db\u9636\u6bb5\u6846\u67b6\uff1a\u4e8b\u4ef6\u89e6\u53d1\u591a\u89c6\u89d2\u89c6\u9891\u91c7\u96c6\u3001\u884c\u4eba\u884c\u4e3a\u9636\u6bb5\u5206\u5272\u3001\u9636\u6bb5\u7279\u5b9a\u591a\u89c6\u89d2\u63a8\u7406\u3001\u5206\u5c42\u5408\u6210\u548c\u8bca\u65ad\u63a8\u7406\u3002\u4f7f\u7528\u4e24\u4e2a\u4e13\u7528VLM\u6a21\u578b\u8fdb\u884c\u9636\u6bb5\u5206\u5272\u548c\u591a\u89c6\u89d2\u5206\u6790\u3002", "result": "TG-VLM\u9636\u6bb5\u5206\u5272mIoU\u8fbe0.4881\uff0cPhaVR-VLM\u5b57\u5e55\u8bc4\u520633.063\uff0c\u95ee\u7b54\u51c6\u786e\u7387\u6700\u9ad8\u8fbe64.70%\u3002\u5728Woven Traffic Safety\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "MP-PVIR\u6210\u529f\u5c06\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u63a8\u8fdb\u4e86\u8f66\u8f86-\u57fa\u7840\u8bbe\u65bd\u534f\u540c\u7cfb\u7edf\u7684AI\u9a71\u52a8\u4ea4\u901a\u5b89\u5168\u5206\u6790\u3002"}}
{"id": "2511.14220", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14220", "abs": "https://arxiv.org/abs/2511.14220", "authors": ["Yaniv Oren", "Joery A. de Vries", "Pascal R. van der Vaart", "Matthijs T. J. Spaan", "Wendelin B\u00f6hmer"], "title": "Parallelizing Tree Search with Twice Sequential Monte Carlo", "comment": null, "summary": "Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.", "AI": {"tldr": "\u63d0\u51fa\u4e86TSMCTS\u7b97\u6cd5\u6765\u89e3\u51b3SMC\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u9ad8\u65b9\u5dee\u548c\u8def\u5f84\u9000\u5316\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\u90fd\u4f18\u4e8eSMC\u57fa\u7ebf\u548c\u73b0\u4ee3MCTS\u7248\u672c\u3002", "motivation": "\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0cSMC\u867d\u7136\u6bd4MCTS\u66f4\u5bb9\u6613\u5e76\u884c\u5316\u548cGPU\u52a0\u901f\uff0c\u4f46\u5b58\u5728\u9ad8\u65b9\u5dee\u548c\u8def\u5f84\u9000\u5316\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u6df1\u5ea6\u641c\u7d22\u4e2d\u7684\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86TSMCTS\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b9\u5dee\u51cf\u5c11\u548c\u8def\u5f84\u9000\u5316\u7f13\u89e3\u6280\u672f\uff0c\u5728\u4fdd\u6301SMC\u5e76\u884c\u5316\u4f18\u52bf\u7684\u540c\u65f6\u6539\u5584\u5176\u6269\u5c55\u6027\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\uff0cTSMCTS\u90fd\u4f18\u4e8eSMC\u57fa\u7ebf\u548c\u73b0\u4ee3MCTS\u7248\u672c\uff0c\u80fd\u591f\u66f4\u597d\u5730\u968f\u987a\u5e8f\u8ba1\u7b97\u6269\u5c55\u3002", "conclusion": "TSMCTS\u901a\u8fc7\u89e3\u51b3SMC\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5e76\u884c\u5316\u4f18\u52bf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u6269\u5c55\uff0c\u4e3a\u57fa\u4e8e\u641c\u7d22\u7684\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14186", "abs": "https://arxiv.org/abs/2511.14186", "authors": ["Zhaoyu Liu", "Kan Jiang", "Murong Ma", "Zhe Hou", "Yun Lin", "Jin Song Dong"], "title": "Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation", "comment": "The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at https://github.com/LZYAndy/UMEG-Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86UMEG-Net\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u5b9e\u4f53\u56fe\u7f51\u7edc\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u7cbe\u786e\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u901a\u8fc7\u6574\u5408\u4eba\u4f53\u9aa8\u67b6\u548c\u8fd0\u52a8\u7279\u5b9a\u7269\u4f53\u5173\u952e\u70b9\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7cbe\u786e\u4e8b\u4ef6\u68c0\u6d4b\u5728\u4f53\u80b2\u5206\u6790\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5feb\u901f\u8fde\u7eed\u52a8\u4f5c\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u7ec6\u5fae\u89c6\u89c9\u5dee\u5f02\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u7279\u5b9a\u9886\u57df\u8bad\u7ec3\uff0c\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "UMEG-Net\u5c06\u4eba\u4f53\u9aa8\u67b6\u548c\u8fd0\u52a8\u7279\u5b9a\u7269\u4f53\u5173\u952e\u70b9\u6574\u5408\u5230\u7edf\u4e00\u56fe\u4e2d\uff0c\u91c7\u7528\u5148\u8fdb\u7684GCN\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f4d\u79fb\u8fdb\u884c\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u6001\u84b8\u998f\u5c06\u77e5\u8bc6\u4ece\u5173\u952e\u70b9\u56fe\u8fc1\u79fb\u5230\u89c6\u89c9\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u6027\u80fd\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "UMEG-Net\u4e3a\u5c11\u6837\u672c\u7cbe\u786e\u4e8b\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14416", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14416", "abs": "https://arxiv.org/abs/2511.14416", "authors": ["Haobin Li", "Mouxing Yang", "Xi Peng"], "title": "Toward Robust and Harmonious Adaptation for Cross-modal Retrieval", "comment": "19 pages, 6 figures", "summary": "Recently, the general-to-customized paradigm has emerged as the dominant approach for Cross-Modal Retrieval (CMR), which reconciles the distribution shift problem between the source domain and the target domain. However, existing general-to-customized CMR methods typically assume that the entire target-domain data is available, which is easily violated in real-world scenarios and thus inevitably suffer from the query shift (QS) problem. Specifically, query shift embraces the following two characteristics and thus poses new challenges to CMR. i) Online Shift: real-world queries always arrive in an online manner, rendering it impractical to access the entire query set beforehand for customization approaches; ii) Diverse Shift: even with domain customization, the CMR models struggle to satisfy queries from diverse users or scenarios, leaving an urgent need to accommodate diverse queries. In this paper, we observe that QS would not only undermine the well-structured common space inherited from the source model, but also steer the model toward forgetting the indispensable general knowledge for CMR. Inspired by the observations, we propose a novel method for achieving online and harmonious adaptation against QS, dubbed Robust adaptation with quEry ShifT (REST). To deal with online shift, REST first refines the retrieval results to formulate the query predictions and accordingly designs a QS-robust objective function on these predictions to preserve the well-established common space in an online manner. As for tackling the more challenging diverse shift, REST employs a gradient decoupling module to dexterously manipulate the gradients during the adaptation process, thus preventing the CMR model from forgetting the general knowledge. Extensive experiments on 20 benchmarks across three CMR tasks verify the effectiveness of our method against QS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faREST\u65b9\u6cd5\u89e3\u51b3\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u67e5\u8be2\u504f\u79fb\u95ee\u9898\uff0c\u5305\u62ec\u5728\u7ebf\u504f\u79fb\u548c\u591a\u6837\u504f\u79fb\uff0c\u901a\u8fc7\u67e5\u8be2\u9884\u6d4b\u4f18\u5316\u548c\u68af\u5ea6\u89e3\u8026\u6a21\u5757\u5b9e\u73b0\u9c81\u68d2\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u5230\u5b9a\u5236\u5316\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\u5047\u8bbe\u6574\u4e2a\u76ee\u6807\u57df\u6570\u636e\u53ef\u7528\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u67e5\u8be2\u4ee5\u5728\u7ebf\u65b9\u5f0f\u5230\u8fbe\u4e14\u5177\u6709\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u67e5\u8be2\u504f\u79fb\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "REST\u65b9\u6cd5\u901a\u8fc7\u67e5\u8be2\u9884\u6d4b\u5236\u5b9aQS\u9c81\u68d2\u76ee\u6807\u51fd\u6570\u4fdd\u62a4\u516c\u5171\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u68af\u5ea6\u89e3\u8026\u6a21\u5757\u9632\u6b62\u6a21\u578b\u9057\u5fd8\u901a\u7528\u77e5\u8bc6\u3002", "result": "\u57283\u4e2a\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u768420\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "REST\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u67e5\u8be2\u504f\u79fb\u95ee\u9898\uff0c\u5728\u5728\u7ebf\u548c\u591a\u6837\u5316\u573a\u666f\u4e0b\u4fdd\u6301\u8de8\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.14682", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.14682", "abs": "https://arxiv.org/abs/2511.14682", "authors": ["Vaskar Chakma", "MD Jaheid Hasan Nerab", "Abdur Rouf", "Abu Sayed", "Hossem MD Saim", "Md. Nournabi Khan"], "title": "Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk", "comment": "This paper has been officially accepted for publication in the Journal of Intelligent Medicine and Healthcare. Once the final published version is available online, this document will be updated accordingly", "summary": "Smoking continues to be a major preventable cause of death worldwide, affecting millions through damage to the heart, metabolism, liver, and kidneys. However, current medical screening methods often miss the early warning signs of smoking-related health problems, leading to late-stage diagnoses when treatment options become limited. This study presents a systematic comparative evaluation of machine learning approaches for smoking-related health risk assessment, emphasizing clinical interpretability and practical deployment over algorithmic innovation. We analyzed health screening data from 55,691 individuals, examining various health indicators, including body measurements, blood tests, and demographic information. We tested three advanced prediction algorithms - Random Forest, XGBoost, and LightGBM - to determine which could most accurately identify people at high risk. This study employed a cross-sectional design to classify current smoking status based on health screening biomarkers, not to predict future disease development. Our Random Forest model performed best, achieving an Area Under the Curve (AUC) of 0.926, meaning it could reliably distinguish between high-risk and lower-risk individuals. Using SHAP (SHapley Additive exPlanations) analysis to understand what the model was detecting, we found that key health markers played crucial roles in prediction: blood pressure levels, triglyceride concentrations, liver enzyme readings, and kidney function indicators (serum creatinine) were the strongest signals of declining health in smokers.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5438\u70df\u76f8\u5173\u5065\u5eb7\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08AUC=0.926\uff09\uff0c\u5e76\u901a\u8fc7SHAP\u5206\u6790\u8bc6\u522b\u51fa\u8840\u538b\u3001\u7518\u6cb9\u4e09\u916f\u3001\u809d\u9176\u548c\u80be\u529f\u80fd\u6307\u6807\u662f\u5438\u70df\u8005\u5065\u5eb7\u98ce\u9669\u7684\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002", "motivation": "\u5438\u70df\u662f\u5168\u7403\u4e3b\u8981\u7684\u53ef\u9884\u9632\u6b7b\u56e0\uff0c\u4f46\u73b0\u6709\u533b\u7597\u7b5b\u67e5\u65b9\u6cd5\u5e38\u9519\u8fc7\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\uff0c\u5bfc\u81f4\u665a\u671f\u8bca\u65ad\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u8bc6\u522b\u5438\u70df\u76f8\u5173\u5065\u5eb7\u98ce\u9669\u3002", "method": "\u4f7f\u752855,691\u4eba\u7684\u5065\u5eb7\u7b5b\u67e5\u6570\u636e\uff0c\u6bd4\u8f83\u968f\u673a\u68ee\u6797\u3001XGBoost\u548cLightGBM\u4e09\u79cd\u7b97\u6cd5\uff0c\u91c7\u7528\u6a2a\u65ad\u9762\u8bbe\u8ba1\u57fa\u4e8e\u751f\u7269\u6807\u5fd7\u7269\u5206\u7c7b\u5f53\u524d\u5438\u70df\u72b6\u6001\uff0c\u5e76\u4f7f\u7528SHAP\u5206\u6790\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cAUC\u8fbe\u52300.926\u3002SHAP\u5206\u6790\u663e\u793a\u8840\u538b\u3001\u7518\u6cb9\u4e09\u916f\u3001\u809d\u9176\u548c\u8840\u6e05\u808c\u9150\u662f\u6700\u91cd\u8981\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u5438\u70df\u76f8\u5173\u5065\u5eb7\u98ce\u9669\uff0c\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u4e3a\u4e34\u5e8a\u7b5b\u67e5\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u5e72\u9884\u3002"}}
{"id": "2511.14738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14738", "abs": "https://arxiv.org/abs/2511.14738", "authors": ["Tzu-Hsuan Chou", "Chun-Nan Chou"], "title": "LAUD: Integrating Large Language Models with Active Learning for Unlabeled Data", "comment": "7 pages and one figure", "summary": "Large language models (LLMs) have shown a remarkable ability to generalize beyond their pre-training data, and fine-tuning LLMs can elevate performance to human-level and beyond. However, in real-world scenarios, lacking labeled data often prevents practitioners from obtaining well-performing models, thereby forcing practitioners to highly rely on prompt-based approaches that are often tedious, inefficient, and driven by trial and error. To alleviate this issue of lacking labeled data, we present a learning framework integrating LLMs with active learning for unlabeled dataset (LAUD). LAUD mitigates the cold-start problem by constructing an initial label set with zero-shot learning. Experimental results show that LLMs derived from LAUD outperform LLMs with zero-shot or few-shot learning on commodity name classification tasks, demonstrating the effectiveness of LAUD.", "AI": {"tldr": "\u63d0\u51fa\u4e86LAUD\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u5b66\u4e60\u6784\u5efa\u521d\u59cb\u6807\u7b7e\u96c6\uff0c\u89e3\u51b3\u65e0\u6807\u7b7e\u6570\u636e\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u963b\u788d\u4e86\u4ece\u4e1a\u8005\u83b7\u5f97\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u8feb\u4f7f\u4f9d\u8d56\u7e41\u7410\u3001\u4f4e\u6548\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "method": "LAUD\u6846\u67b6\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4e3b\u52a8\u5b66\u4e60\uff0c\u4f7f\u7528\u96f6\u6837\u672c\u5b66\u4e60\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u6784\u5efa\u521d\u59cb\u6807\u7b7e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLAUD\u6846\u67b6\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5546\u54c1\u540d\u79f0\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u5b66\u4e60\u3002", "conclusion": "LAUD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u6807\u7b7e\u6570\u636e\u573a\u666f\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.14398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14398", "abs": "https://arxiv.org/abs/2511.14398", "authors": ["Saksham Kumar", "D Sridhar Aditya", "T Likhil Kumar", "Thulasi Bikku", "Srinivasarao Thota", "Chandan Kumar"], "title": "Stage Aware Diagnosis of Diabetic Retinopathy via Ordinal Regression", "comment": "Submitted to Confluence 2026, Amity University", "summary": "Diabetic Retinopathy (DR) has emerged as a major cause of preventable blindness in recent times. With timely screening and intervention, the condition can be prevented from causing irreversible damage. The work introduces a state-of-the-art Ordinal Regression-based DR Detection framework that uses the APTOS-2019 fundus image dataset. A widely accepted combination of preprocessing methods: Green Channel (GC) Extraction, Noise Masking, and CLAHE, was used to isolate the most relevant features for DR classification. Model performance was evaluated using the Quadratic Weighted Kappa, with a focus on agreement between results and clinical grading. Our Ordinal Regression approach attained a QWK score of 0.8992, setting a new benchmark on the APTOS dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e8f\u6570\u56de\u5f52\u7684\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\u6846\u67b6\uff0c\u5728APTOS-2019\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.8992\u7684QWK\u5206\u6570\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5df2\u6210\u4e3a\u53ef\u9884\u9632\u6027\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u901a\u8fc7\u53ca\u65f6\u7b5b\u67e5\u548c\u5e72\u9884\u53ef\u4ee5\u9632\u6b62\u4e0d\u53ef\u9006\u635f\u4f24", "method": "\u4f7f\u7528\u5e8f\u6570\u56de\u5f52\u6846\u67b6\uff0c\u7ed3\u5408\u7eff\u901a\u9053\u63d0\u53d6\u3001\u566a\u58f0\u63a9\u853d\u548cCLAHE\u7b49\u9884\u5904\u7406\u65b9\u6cd5\u63d0\u53d6\u76f8\u5173\u7279\u5f81", "result": "\u5728APTOS\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e860.8992\u7684QWK\u5206\u6570\uff0c\u521b\u4e0b\u4e86\u65b0\u7684\u57fa\u51c6", "conclusion": "\u8be5\u5e8f\u6570\u56de\u5f52\u65b9\u6cd5\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e34\u5e8a\u5206\u7ea7\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u4e00\u81f4\u7684\u68c0\u6d4b\u7ed3\u679c"}}
{"id": "2511.14469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14469", "abs": "https://arxiv.org/abs/2511.14469", "authors": ["Mingchen Zhong", "Xin Lu", "Dong Li", "Senyan Xu", "Ruixuan Jiang", "Xueyang Fu", "Baocai Yin"], "title": "CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring", "comment": null, "summary": "Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.", "AI": {"tldr": "CompEvent\u662f\u4e00\u4e2a\u57fa\u4e8e\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u7684\u4f4e\u5149\u89c6\u9891\u53bb\u6a21\u7cca\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u6d41\u7a0b\u878d\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB\u5e27\uff0c\u5b9e\u73b0\u4e86\u65f6\u7a7a\u5bf9\u9f50\u548c\u6df1\u5ea6\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u89c6\u9891\u53bb\u6a21\u7cca\u6027\u80fd\u3002", "motivation": "\u4f4e\u5149\u89c6\u9891\u53bb\u6a21\u7cca\u5728\u591c\u95f4\u76d1\u63a7\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u7684\u4e8b\u4ef6\u76f8\u673a\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5206\u9636\u6bb5\u7b56\u7565\uff0c\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u4f4e\u5149\u548c\u8fd0\u52a8\u6a21\u7cca\u7684\u8054\u5408\u9000\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faCompEvent\u6846\u67b6\uff0c\u5305\u542b\u590d\u6570\u65f6\u5e8f\u5bf9\u9f50GRU\uff08\u4f7f\u7528\u590d\u6570\u5377\u79ef\u548cGRU\u8fed\u4ee3\u5904\u7406\u89c6\u9891\u548c\u4e8b\u4ef6\u6d41\uff09\u548c\u590d\u6570\u7a7a\u95f4-\u9891\u7387\u5b66\u4e60\u6a21\u5757\uff08\u5728\u7a7a\u95f4\u548c\u9891\u57df\u8fdb\u884c\u7edf\u4e00\u590d\u6570\u4fe1\u53f7\u5904\u7406\uff09\uff0c\u5b9e\u73b0\u5168\u6d41\u7a0b\u65f6\u7a7a\u878d\u5408\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCompEvent\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u7684\u5168\u6d41\u7a0b\u8868\u793a\u80fd\u529b\uff0cCompEvent\u5b9e\u73b0\u4e86\u6a21\u6001\u95f4\u7684\u4e92\u8865\u5b66\u4e60\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u4f4e\u5149\u89c6\u9891\u53bb\u6a21\u7cca\u80fd\u529b\u3002"}}
{"id": "2511.14698", "categories": ["cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.14698", "abs": "https://arxiv.org/abs/2511.14698", "authors": ["Sriram Srinivasan", "Srinivasan Aruchamy", "Siva Ram Krisha Vadali"], "title": "HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring", "comment": "Multi-label seismic signal classification using novel attention-based feature fusion. Submitting to cs.CV due to relevance to general pattern recognition and time-frequency (spectrogram) analysis", "summary": "Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.", "AI": {"tldr": "\u63d0\u51faHyMAD\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u9891\u8c31\u7279\u5f81\u548c\u65f6\u5e8f\u4f9d\u8d56\u6765\u68c0\u6d4b\u548c\u533a\u5206\u540c\u65f6\u53d1\u751f\u7684\u5730\u9707\u6d3b\u52a8\uff08\u4eba\u7c7b\u5165\u4fb5\u3001\u52a8\u7269\u79fb\u52a8\u3001\u8f66\u8f86\u884c\u9a76\uff09\uff0c\u89e3\u51b3\u8fb9\u5883\u76d1\u63a7\u4e2d\u91cd\u53e0\u6d3b\u52a8\u8bc6\u522b\u7684\u6311\u6218\u3002", "motivation": "\u5730\u9707\u4f20\u611f\u5668\u5728\u8fb9\u5883\u76d1\u63a7\u4e2d\u5177\u6709\u9690\u853d\u6027\u4f18\u52bf\uff0c\u4f46\u590d\u6742\u5608\u6742\u7684\u5730\u9707\u4fe1\u53f7\u4f7f\u5f97\u51c6\u786e\u68c0\u6d4b\u548c\u533a\u5206\u540c\u65f6\u53d1\u751f\u7684\u6d3b\u52a8\uff08\u5982\u4eba\u7c7b\u3001\u52a8\u7269\u3001\u8f66\u8f86\uff09\u6210\u4e3a\u4e3b\u8981\u6311\u6218\uff0c\u9519\u8bef\u5206\u7c7b\u4f1a\u964d\u4f4e\u76d1\u63a7\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684HyMAD\u67b6\u6784\uff0c\u96c6\u6210SincNet\u63d0\u53d6\u7684\u9891\u8c31\u7279\u5f81\u548cRNN\u5efa\u6a21\u7684\u65f6\u5e8f\u4f9d\u8d56\uff0c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u5c42\u589e\u5f3a\u6a21\u6001\u5185\u8868\u793a\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\u5b9e\u73b0\u7a33\u5065\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5728\u771f\u5b9e\u8fb9\u5883\u76d1\u63a7\u573a\u666f\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u540c\u65f6\u6d3b\u52a8\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "HyMAD\u4e3a\u73b0\u5b9e\u4e16\u754c\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u5730\u9707\u6d3b\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u540c\u65f6\u53d1\u751f\u7684\u591a\u6d3b\u52a8\u68c0\u6d4b\u95ee\u9898\u3002"}}
{"id": "2511.14639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14639", "abs": "https://arxiv.org/abs/2511.14639", "authors": ["Marco Acerbis", "Swarnadip Chatterjee", "Christophe Avenel", "Joakim Lindblad"], "title": "SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology", "comment": "5 pages, 2 figures, Submitted to ISBI2026", "summary": "Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86SLAM-AGS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u89e3\u51b3\u8ba1\u7b97\u7ec6\u80de\u5b66\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u548c\u9633\u6027\u6837\u672c\u7387\u6781\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u4f4e\u9633\u6027\u7387\u4e0b\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u8ba1\u7b97\u7ec6\u80de\u5b66\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u5b9e\u4f8b\u7ea7\u6807\u7b7e\u4e0d\u53ef\u9760\u4e14\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u9633\u6027\u6837\u672c\u7387\u6781\u4f4e", "method": "SLAM-AGS\u6846\u67b6\u8054\u5408\u4f18\u5316\u5f31\u76d1\u7763\u76f8\u4f3c\u6027\u76ee\u6807\uff08\u5728\u9634\u6027\u6837\u672c\u4e0a\uff09\u548c\u81ea\u76d1\u7763\u5bf9\u6bd4\u76ee\u6807\uff08\u5728\u9633\u6027\u6837\u672c\u4e0a\uff09\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u68af\u5ea6\u624b\u672f\u89e3\u51b3\u4efb\u52a1\u68af\u5ea6\u51b2\u7a81\uff0c\u5e76\u96c6\u6210\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u805a\u5408\u5668\u4e2d", "result": "\u5728\u516c\u5f00\u9aa8\u9ad3\u7ec6\u80de\u5b66\u6570\u636e\u96c6\u4e0a\uff0c\u5f53\u9633\u6027\u7387\u4ece10%\u964d\u81f30.5%\u65f6\uff0cSLAM-AGS\u5728\u888b\u7ea7F1\u5206\u6570\u548cTop 400\u9633\u6027\u7ec6\u80de\u68c0\u7d22\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4f4e\u9633\u6027\u7387\u4e0b\u589e\u76ca\u6700\u5927", "conclusion": "\u89e3\u51b3\u68af\u5ea6\u5e72\u6270\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u7684\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd"}}
{"id": "2511.14716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14716", "abs": "https://arxiv.org/abs/2511.14716", "authors": ["Xiyuan Wang", "Muhan Zhang"], "title": "Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model", "comment": "Tech Report. 10 pages", "summary": "Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.", "AI": {"tldr": "\u63d0\u51faDSD\u6846\u67b6\uff0c\u5c06\u6807\u51c6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u4e09\u4e2a\u7ec4\u4ef6\u7edf\u4e00\u4e3a\u5355\u4e00\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u6f5c\u5728\u5d29\u6e83\u95ee\u9898\uff0c\u5728ImageNet 256\u00d7256\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u4e09\u90e8\u5206\u67b6\u6784\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u6027\u80fd\u6b21\u4f18\uff0c\u4e14\u963b\u788d\u4e86\u4e0e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u5355\u7f51\u7edc\u67b6\u6784\u7684\u7edf\u4e00\u3002\u76ee\u6807\u662f\u7edf\u4e00\u7f16\u7801\u5668\u3001\u89e3\u7801\u5668\u548c\u6269\u6563\u7f51\u7edc\u4e3a\u5355\u4e00\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u6269\u6563\u4f5c\u4e3a\u81ea\u84b8\u998f(DSD)\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u76ee\u6807\u7684\u4fee\u6539\u6765\u7a33\u5b9a\u6f5c\u5728\u7a7a\u95f4\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u4e00\u7f51\u7edc\u7684\u7a33\u5b9a\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u540c\u65f6\u5b66\u4e60\u7f16\u7801\u3001\u89e3\u7801\u548c\u6269\u6563\u3002", "result": "\u5728ImageNet 256\u00d7256\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97FID=13.44/6.38/4.25\uff0c\u4ec5\u4f7f\u752842M/118M/205M\u53c2\u6570\u548c50\u4e2a\u8bad\u7ec3\u5468\u671f\uff0c\u4e14\u4e0d\u4f7f\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u3002", "conclusion": "DSD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6f5c\u5728\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5355\u7f51\u7edc\u7edf\u4e00\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
