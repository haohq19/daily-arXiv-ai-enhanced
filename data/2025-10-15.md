<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis](https://arxiv.org/abs/2510.11907)
*Blessing Agyei Kyem,Neema Jakisa Owor,Andrews Danyo,Joshua Kofi Asamoah,Eugene Denteh,Tanner Muturi,Anthony Dontoh,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: 提出双模型框架，分别优化VideoLLaMA和Qwen2.5-VL在交通视频分析中的captioning和VQA任务，通过分离训练避免任务干扰，在AI City Challenge Track 2中获得第10名。


<details>
  <summary>Details</summary>
Motivation: 交通视频安全分析需要细粒度行为模式识别和全面描述生成，现有方法存在任务干扰问题。

Method: 采用双模型框架，分别训练VideoLLaMA负责captioning任务，Qwen2.5-VL负责VQA任务，利用各自优势实现任务专业化。

Result: VideoLLaMA在时间推理上CIDEr得分1.1001，Qwen2.5-VL在视觉理解上VQA准确率60.80%，在WTS数据集上S2得分45.7572，排名第10。

Conclusion: 分离训练策略比联合训练在VQA准确率上提升8.6%，同时保持captioning质量，证明了任务专业化的重要性。

Abstract: Traffic safety analysis requires complex video understanding to capture
fine-grained behavioral patterns and generate comprehensive descriptions for
accident prevention. In this work, we present a unique dual-model framework
that strategically utilizes the complementary strengths of VideoLLaMA and
Qwen2.5-VL through task-specific optimization to address this issue. The core
insight behind our approach is that separating training for captioning and
visual question answering (VQA) tasks minimizes task interference and allows
each model to specialize more effectively. Experimental results demonstrate
that VideoLLaMA is particularly effective in temporal reasoning, achieving a
CIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a
VQA accuracy of 60.80\%. Through extensive experiments on the WTS dataset, our
method achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,
placing 10th on the challenge leaderboard. Ablation studies validate that our
separate training strategy outperforms joint training by 8.6\% in VQA accuracy
while maintaining captioning quality.

</details>


### [2] [Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos](https://arxiv.org/abs/2510.12190)
*Shingo Yokoi,Kento Sasaki,Yu Yamaguchi*

Main category: cs.CV

TL;DR: 提出了一种用于从行车记录仪视频生成事故报告的分层推理框架，结合帧级描述、事故帧检测和视觉语言模型的细粒度推理，在2COOOL挑战中排名第2。


<details>
  <summary>Details</summary>
Motivation: 解决端到端自动驾驶模型在分布外场景中的表现不佳问题，特别是生成人类可理解的事故报告。

Method: 分层推理框架，包括帧级描述、事故帧检测、视觉语言模型的细粒度推理，以及模型集成和盲A/B评分选择协议。

Result: 在官方2COOOL开放排行榜中排名第2（共29个团队），获得最佳CIDEr-D分数，生成准确连贯的事故叙述。

Conclusion: 基于视觉语言模型的分层推理是事故分析和安全关键交通事件理解的可行方向。

Abstract: Recent advances in end-to-end (E2E) autonomous driving have been enabled by
training on diverse large-scale driving datasets, yet autonomous driving models
still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark
targets this gap by encouraging hazard understanding beyond closed taxonomies,
and the 2COOOL challenge extends it to generating human-interpretable incident
reports. We present a hierarchical reasoning framework for incident report
generation from dashcam videos that integrates frame-level captioning, incident
frame detection, and fine-grained reasoning within vision-language models
(VLMs). We further improve factual accuracy and readability through model
ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL
open leaderboard, our method ranks 2nd among 29 teams and achieves the best
CIDEr-D score, producing accurate and coherent incident narratives. These
results indicate that hierarchical reasoning with VLMs is a promising direction
for accident analysis and for broader understanding of safety-critical traffic
events. The implementation and code are available at
https://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.

</details>


### [3] [CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving](https://arxiv.org/abs/2510.12560)
*Xiaoji Zheng,Ziyuan Yang,Yanhao Chen,Yuhang Peng,Yuanrong Tang,Gengyuan Liu,Bokui Chen,Jiangtao Gong*

Main category: cs.CV

TL;DR: 提出CoIRL-AD框架，通过竞争机制结合模仿学习和强化学习，提升自动驾驶模型的泛化能力和性能


<details>
  <summary>Details</summary>
Motivation: 模仿学习泛化能力差，强化学习样本效率低且收敛不稳定，需要结合两者优势

Method: 竞争式双策略框架，让IL和RL智能体在训练中交互，通过竞争机制促进知识交换并避免梯度冲突

Result: 在nuScenes数据集上碰撞率降低18%，在长尾场景中表现更好，泛化能力更强

Conclusion: CoIRL-AD框架有效结合IL和RL优势，显著提升自动驾驶性能

Abstract: End-to-end autonomous driving models trained solely with imitation learning
(IL) often suffer from poor generalization. In contrast, reinforcement learning
(RL) promotes exploration through reward maximization but faces challenges such
as sample inefficiency and unstable convergence. A natural solution is to
combine IL and RL. Moving beyond the conventional two-stage paradigm (IL
pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive
dual-policy framework that enables IL and RL agents to interact during
training. CoIRL-AD introduces a competition-based mechanism that facilitates
knowledge exchange while preventing gradient conflicts. Experiments on the
nuScenes dataset show an 18% reduction in collision rate compared to baselines,
along with stronger generalization and improved performance on long-tail
scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.

</details>


### [4] [VideoLucy: Deep Memory Backtracking for Long Video Understanding](https://arxiv.org/abs/2510.12422)
*Jialong Zuo,Yongtai Deng,Lingdong Kong,Jingkang Yang,Rui Jin,Yiwei Zhang,Nong Sang,Liang Pan,Ziwei Liu,Changxin Gao*

Main category: cs.CV

TL;DR: VideoLucy是一个基于深度记忆回溯的框架，用于解决长视频理解中时间上下文丢失和关键信息遗漏的问题。通过分层记忆结构和迭代回溯机制，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代理系统在长视频理解中存在两个主要问题：1) 在单个帧上进行建模和推理，难以捕捉连续帧的时间上下文；2) 为降低密集帧级标注成本而采用稀疏帧采样，可能丢失关键信息。

Method: 提出VideoLucy框架，受人类从粗到细的回忆过程启发，采用分层记忆结构，在不同层次深度定义记忆的细节级别和时间范围。通过基于代理的迭代回溯机制，系统挖掘视频范围内与问题相关的深度记忆，直到收集足够信息提供可靠答案。

Result: 在多个长视频理解基准测试中，基于开源模型的VideoLucy显著优于最先进方法，性能甚至超过GPT-4o等最新专有模型。同时提出了EgoMem新基准来全面评估模型对复杂时间事件的理解能力。

Conclusion: VideoLucy通过分层记忆结构和迭代回溯机制，有效解决了长视频理解中的时间上下文捕捉和关键信息保留问题，在多个基准测试中表现出优越性能。

Abstract: Recent studies have shown that agent-based systems leveraging large language
models (LLMs) for key information retrieval and integration have emerged as a
promising approach for long video understanding. However, these systems face
two major challenges. First, they typically perform modeling and reasoning on
individual frames, struggling to capture the temporal context of consecutive
frames. Second, to reduce the cost of dense frame-level captioning, they adopt
sparse frame sampling, which risks discarding crucial information. To overcome
these limitations, we propose VideoLucy, a deep memory backtracking framework
for long video understanding. Inspired by the human recollection process from
coarse to fine, VideoLucy employs a hierarchical memory structure with
progressive granularity. This structure explicitly defines the detail level and
temporal scope of memory at different hierarchical depths. Through an
agent-based iterative backtracking mechanism, VideoLucy systematically mines
video-wide, question-relevant deep memories until sufficient information is
gathered to provide a confident answer. This design enables effective temporal
understanding of consecutive frames while preserving critical details. In
addition, we introduce EgoMem, a new benchmark for long video understanding.
EgoMem is designed to comprehensively evaluate a model's ability to understand
complex events that unfold over time and capture fine-grained details in
extremely long videos. Extensive experiments demonstrate the superiority of
VideoLucy. Built on open-source models, VideoLucy significantly outperforms
state-of-the-art methods on multiple long video understanding benchmarks,
achieving performance even surpassing the latest proprietary models such as
GPT-4o. Our code and dataset will be made publicly at
https://videolucy.github.io

</details>


### [5] [BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring](https://arxiv.org/abs/2510.12493)
*An Zhao,Piaopiao Yu,Zhe Zhu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出双阶段3D高斯泼溅框架，解决从运动模糊图像重建3D场景的挑战，通过相机位姿优化和全局刚性变换来校正运动模糊失真。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS去模糊方法性能受限，对相机位姿精度依赖过高，且无法有效控制运动模糊导致的错误高斯基元密集化问题。

Method: 双阶段框架：第一阶段粗略优化相机位姿减少运动失真；第二阶段固定粗略位姿，通过全局刚性变换进一步校正模糊失真。采用子帧梯度聚合策略和时空双阶段优化策略。

Result: 综合实验验证了该去模糊方法的有效性，并显示出优于现有技术的性能。

Conclusion: 提出的双阶段3D高斯泼溅框架能够从运动模糊图像中准确重建3D场景，解决了现有方法的局限性。

Abstract: 3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene
reconstruction.However, reconstructing high-quality 3D scenes from
motion-blurred images caused by camera motion poses a significant challenge.The
performance of existing 3DGS-based deblurring methods are limited due to their
inherent mechanisms, such as extreme dependence on the accuracy of camera poses
and inability to effectively control erroneous Gaussian primitives
densification caused by motion blur.To solve these problems, we introduce a
novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D
scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose
Refinement roughly optimizes camera poses to reduce motion-induced distortions.
Second, with fixed rough camera poses, Global RigidTransformation further
corrects motion-induced blur distortions.To alleviate multi-subframe gradient
conflicts, we propose a subframe gradient aggregation strategy to optimize both
stages.Furthermore, a space-time bi-stage optimization strategy is introduced
to dynamically adjust primitive densification thresholds and prevent premature
noisy Gaussian generation in blurred regions. Comprehensive experiments verify
the effectiveness of our proposed deblurring method and show its superiority
over the state of the arts.

</details>


### [6] [E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization](https://arxiv.org/abs/2510.12753)
*Wenpu Li,Bangyan Liao,Yi Zhou,Qi Xu,Pian Wan,Peidong Liu*

Main category: cs.CV

TL;DR: 提出了一种名为E-MoFlow的无监督框架，通过隐式时空和几何正则化联合优化自运动和光流估计，避免了显式变分正则化或结构-运动参数化带来的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将光流和6-DoF自运动估计作为独立任务处理，但在事件相机中由于缺乏鲁棒的数据关联，单独解决这两个问题变得不适定。现有方法要么引入偏差和计算开销，要么容易陷入局部最优。

Method: 将相机自运动建模为连续样条，光流建模为隐式神经表示，通过归纳偏置嵌入时空一致性；结合微分几何约束引入结构-运动先验，避免显式深度估计但保持几何一致性。

Result: 实验表明该方法适用于一般6-DoF运动场景，在无监督方法中达到最先进性能，甚至与有监督方法竞争。

Conclusion: E-MoFlow框架通过隐式正则化在完全无监督范式下统一了自运动和光流估计，解决了事件相机中的不适定问题。

Abstract: The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in
3D vision, has typically been addressed independently. For neuromorphic vision
(e.g., event cameras), however, the lack of robust data association makes
solving the two problems separately an ill-posed challenge, especially in the
absence of supervision via ground truth. Existing works mitigate this
ill-posedness by either enforcing the smoothness of the flow field via an
explicit variational regularizer or leveraging explicit structure-and-motion
priors in the parametrization to improve event alignment. The former notably
introduces bias in results and computational overhead, while the latter, which
parametrizes the optical flow in terms of the scene depth and the camera
motion, often converges to suboptimal local minima. To address these issues, we
propose an unsupervised framework that jointly optimizes egomotion and optical
flow via implicit spatial-temporal and geometric regularization. First, by
modeling camera's egomotion as a continuous spline and optical flow as an
implicit neural representation, our method inherently embeds spatial-temporal
coherence through inductive biases. Second, we incorporate structure-and-motion
priors through differential geometric constraints, bypassing explicit depth
estimation while maintaining rigorous geometric consistency. As a result, our
framework (called E-MoFlow) unifies egomotion and optical flow estimation via
implicit regularization under a fully unsupervised paradigm. Experiments
demonstrate its versatility to general 6-DoF motion scenarios, achieving
state-of-the-art performance among unsupervised methods and competitive even
with supervised approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Actor-Enriched Time Series Forecasting of Process Performance](https://arxiv.org/abs/2510.11856)
*Aurelie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 本研究探讨将参与者行为信息建模为时间序列是否能提高吞吐时间预测模型的性能。通过构建包含参与者行为特征的多变量时间序列，研究发现加入参与者行为信息的模型在预测性能上显著优于仅使用吞吐时间特征的基线模型。


<details>
  <summary>Details</summary>
Motivation: 在预测性流程监控中，虽然现有研究已考虑参与者行为，但其作为时变信号的作用仍有限。由于流程通常由资源驱动，理解和整合参与者行为对预测至关重要。

Method: 使用真实事件日志构建多变量时间序列，包含吞吐时间及参与者相关特征（参与程度、继续/中断/交接行为的频率和持续时间）。训练并比较多个模型，研究添加参与者行为的好处。

Result: 结果显示，加入参与者行为的模型在RMSE、MAE和R2指标上持续优于仅包含吞吐时间特征的基线模型。

Conclusion: 将参与者行为建模为时间序列并整合到预测模型中，能够显著提升性能指标预测的准确性。

Abstract: Predictive Process Monitoring (PPM) is a key task in Process Mining that aims
to predict future behavior, outcomes, or performance indicators. Accurate
prediction of the latter is critical for proactive decision-making. Given that
processes are often resource-driven, understanding and incorporating actor
behavior in forecasting is crucial. Although existing research has incorporated
aspects of actor behavior, its role as a time-varying signal in PPM remains
limited. This study investigates whether incorporating actor behavior
information, modeled as time series, can improve the predictive performance of
throughput time (TT) forecasting models. Using real-life event logs, we
construct multivariate time series that include TT alongside actor-centric
features, i.e., actor involvement, the frequency of continuation, interruption,
and handover behaviors, and the duration of these behaviors. We train and
compare several models to study the benefits of adding actor behavior. The
results show that actor-enriched models consistently outperform baseline
models, which only include TT features, in terms of RMSE, MAE, and R2. These
findings demonstrate that modeling actor behavior over time and incorporating
this information into forecasting models enhances performance indicator
predictions.

</details>


### [8] [Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks](https://arxiv.org/abs/2510.11903)
*Rizal Fathony,Igor Melnyk,Owen Reinert,Nam H. Nguyen,Daniele Rosa,C. Bayan Bruss*

Main category: cs.LG

TL;DR: 提出了一个统一框架来同时建模个人事件和关系事件，并发布了相关数据集，实证表明结合两种事件类型能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中用户事件包含个人事件和关系事件，但现有方法通常将它们分开建模，缺乏统一的处理框架和数据集。

Method: 引入包含两种事件类型的数据集集合，提出统一的形式化方法，并实证验证结合两种事件类型的优势。

Result: 实验结果表明，同时考虑个人事件和关系事件的模型性能更好，且当前方法仍有显著改进空间。

Conclusion: 需要进一步发展统一用户事件建模方法，发布的数据集和框架将支持这一研究方向。

Abstract: User event modeling plays a central role in many machine learning
applications, with use cases spanning e-commerce, social media, finance,
cybersecurity, and other domains. User events can be broadly categorized into
personal events, which involve individual actions, and relational events, which
involve interactions between two users. These two types of events are typically
modeled separately, using sequence-based methods for personal events and
graph-based methods for relational events. Despite the need to capture both
event types in real-world systems, prior work has rarely considered them
together. This is often due to the convenient simplification that user behavior
can be adequately represented by a single formalization, either as a sequence
or a graph. To address this gap, there is a need for public datasets and
prediction tasks that explicitly incorporate both personal and relational
events. In this work, we introduce a collection of such datasets, propose a
unified formalization, and empirically show that models benefit from
incorporating both event types. Our results also indicate that current methods
leave a notable room for improvements. We release these resources to support
further research in unified user event modeling and encourage progress in this
direction.

</details>


### [9] [Budget-constrained Active Learning to Effectively De-censor Survival Data](https://arxiv.org/abs/2510.12144)
*Ali Parsaee,Bei Jiang,Zachary Friggstad,Russell Greiner*

Main category: cs.LG

TL;DR: 该论文研究了在生存数据集的预算学习问题，探索了如何在包含右删失实例的数据集中，通过预算来获取部分标签信息以改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准监督学习需要完整标注的数据集，但在现实世界中，生存数据往往包含右删失实例，只能获得事件时间的下界。预算学习允许在有限预算下获取更多标签信息，模拟了现实世界中随访患者数据收集的场景。

Method: 提出了将最先进的预算学习算法应用于生存数据的方法，允许支付预算来部分标注删失实例，比如从(3年，删失)更新为(7.2年，非删失)或其他变体。

Result: 实验和理论分析表明，该方法在多个生存任务基准测试中优于其他潜在方法，提供了与标准主动学习方法BatchBALD相当的边界和时间复杂度。

Conclusion: 该研究为生存数据集的预算学习提供了有效的方法，能够在不完全信息下优化模型性能，具有重要的现实应用价值。

Abstract: Standard supervised learners attempt to learn a model from a labeled dataset.
Given a small set of labeled instances, and a pool of unlabeled instances, a
budgeted learner can use its given budget to pay to acquire the labels of some
unlabeled instances, which it can then use to produce a model. Here, we explore
budgeted learning in the context of survival datasets, which include (right)
censored instances, where we know only a lower bound on an instance's
time-to-event. Here, that learner can pay to (partially) label a censored
instance -- e.g., to acquire the actual time for an instance [perhaps go from
(3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about
one more year, so go from (3 yr, censored) to either (4 yr, censored) or
perhaps (3.2 yr, uncensored)]. This serves as a model of real world data
collection, where follow-up with censored patients does not always lead to
uncensoring, and how much information is given to the learner model during data
collection is a function of the budget and the nature of the data itself. We
provide both experimental and theoretical results for how to apply
state-of-the-art budgeted learning algorithms to survival data and the
respective limitations that exist in doing so. Our approach provides bounds and
time complexity asymptotically equivalent to the standard active learning
method BatchBALD. Moreover, empirical analysis on several survival tasks show
that our model performs better than other potential approaches on several
benchmarks.

</details>


### [10] [Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand](https://arxiv.org/abs/2510.12328)
*Kiattikun Chobtham,Kanoksri Sarinnapakorn,Kritanai Torsri,Prattana Deeprasertkul,Jirawan Kamma*

Main category: cs.LG

TL;DR: 提出结合物理信息图神经网络和极值分析技术的新方法，用于改进泰国雨量站的降雨预测，特别针对极端事件。


<details>
  <summary>Details</summary>
Motivation: 准确预测降雨特别是极端事件在气候学和地球系统中仍具挑战性，需要改进传统模型的局限性。

Method: 使用图注意力网络与LSTM结合，构建雨量站图结构捕捉时空模式，采用基于地形降水物理的边缘特征，并通过空间季节感知GPD方法进行极值分析。

Result: 该方法在大多数区域包括极端易发区优于基准模型，与SEAS5操作预报系统相比改进了极端事件预测，提供支持长期水资源管理的精细分辨率地图。

Conclusion: 提出的物理信息图神经网络结合极值分析技术能够有效提升降雨预测精度，特别是对极端事件的预测能力，为水资源管理决策提供实用工具。

Abstract: Accurate rainfall forecasting, particularly for extreme events, remains a
significant challenge in climatology and the Earth system. This paper presents
novel physics-informed Graph Neural Networks (GNNs) combined with extreme-value
analysis techniques to improve gauge-station rainfall predictions across
Thailand. The model leverages a graph-structured representation of gauge
stations to capture complex spatiotemporal patterns, and it offers
explainability through teleconnections. We preprocess relevant climate indices
that potentially influence regional rainfall. The proposed Graph Attention
Network with Long Short-Term Memory (Attention-LSTM) applies the attention
mechanism using initial edge features derived from simple
orographic-precipitation physics formulation. The embeddings are subsequently
processed by LSTM layers. To address extremes, we perform Peak-Over-Threshold
(POT) mapping using the novel Spatial Season-aware Generalized Pareto
Distribution (GPD) method, which overcomes limitations of traditional
machine-learning models. Experiments demonstrate that our method outperforms
well-established baselines across most regions, including areas prone to
extremes, and remains strongly competitive with the state of the art. Compared
with the operational forecasting system SEAS5, our real-world application
improves extreme-event prediction and offers a practical enhancement to produce
fine-resolution maps that support decision-making in long-term water
management.

</details>


### [11] [Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models](https://arxiv.org/abs/2510.12343)
*Donghwan Rho,Sieun Seo,Hyewon Sung,Chohong Min,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了一种基于TSP的令牌重排序策略和后续处理步骤，用于解决加密文本生成的挑战，特别是下一令牌预测问题，从而推进实用且保护隐私的LLM推理。


<details>
  <summary>Details</summary>
Motivation: 随着用户越来越多地使用私有信息与大型语言模型交互，安全加密通信变得至关重要。同态加密虽然提供了在加密数据上直接计算的原理性解决方案，但文本生成特别是下一令牌预测问题在HE下的研究有限，是实用加密交互的主要障碍。

Method: 采用基于旅行商问题（TSP）的令牌重排序策略来处理加密文本生成的困难，并结合一个后续处理步骤来进一步减少近似误差。

Result: 理论分析和实验结果表明，该方法防止了崩溃，提高了生成文本的连贯性，并在整个过程中保持了数据隐私。

Conclusion: 这项工作推进了实用且保护隐私的LLM推理的可行性，为加密环境下的文本生成提供了有效的解决方案。

Abstract: As users increasingly interact with large language models (LLMs) using
private information, secure and encrypted communication becomes essential.
Homomorphic encryption (HE) provides a principled solution by enabling
computation directly on encrypted data. Although prior work has explored
aspects of running LLMs under HE, the challenge of text generation,
particularly next-token prediction, has received limited attention and remains
a key obstacle to practical encrypted interaction. In this work, we propose a
TSP-based token reordering strategy to address the difficulties of encrypted
text generation, together with a post-processing step that further reduces
approximation error. Theoretical analysis and experimental results demonstrate
that our method prevents collapse, improves coherence in generated text, and
preserves data privacy throughout. Overall, our contributions advance the
feasibility of practical and privacy-preserving LLM inference.

</details>


### [12] [On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery](https://arxiv.org/abs/2510.12640)
*David Berghaus,Patrick Seifner,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 提出了一种基于基础模型的事件序列分析方法，无需为每个新数据集重新训练模型，通过少量示例即可快速分析新数据


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要为每个新数据集从头构建和训练模型，过程缓慢且成本高昂，限制了科学发现的速度

Method: 训练一个基础模型，在数百万个模拟事件序列上学习事件数据的底层模式，获得对事件如何展开的通用理解

Result: 该模型能够即时分析新的科学数据而无需重新训练，只需查看数据集中的少量示例，并且可以快速微调以获得更高精度

Conclusion: 这种方法使复杂的事件分析更加易于获取，并加速了科学发现的步伐

Abstract: Many scientific fields, from medicine to seismology, rely on analyzing
sequences of events over time to understand complex systems. Traditionally,
machine learning models must be built and trained from scratch for each new
dataset, which is a slow and costly process. We introduce a new approach: a
single, powerful model that learns the underlying patterns of event data in
context. We trained this "foundation model" on millions of simulated event
sequences, teaching it a general-purpose understanding of how events can
unfold. As a result, our model can analyze new scientific data instantly,
without retraining, simply by looking at a few examples from the dataset. It
can also be quickly fine-tuned for even higher accuracy. This approach makes
sophisticated event analysis more accessible and accelerates the pace of
scientific discovery.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 提出了一个地理空间感知层(GAL)，将LLM智能体与结构化地球数据相结合，通过整合基础设施、人口、地形和天气信息来改进灾害响应决策。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法缺乏语义上下文、跨事件泛化能力差且可解释性有限，而LLM虽然具有少样本泛化能力，但受限于文本且对地理信息不敏感。

Method: 从原始野火检测开始，GAL自动从外部地理数据库中检索并整合基础设施、人口统计、地形和天气信息，将其组装成简洁的带单位标注的感知脚本。

Result: 在真实野火场景中评估表明，地理空间接地的智能体能够超越基线模型，产生基于证据的资源分配建议。

Conclusion: 该框架可以推广到洪水、飓风等其他灾害类型，为灾害响应提供了更有效的决策支持。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 该论文探讨了将离散推理空间的推理技术（如多样本生成和PRM/ORM重排序）应用于连续空间推理的可行性，发现虽然能生成多样化的推理路径，但由于连续思维表示缺乏关键归纳偏置，导致PRM/ORM模型难以有效区分正确与错误推理。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以将离散推理空间中证明有效的推理技术（多样本生成和PRM/ORM重排序）成功适配到连续空间推理中，以提升连续推理语言模型的性能。

Method: 使用COCONUT连续空间推理LM作为骨干模型，通过基于dropout的采样生成多样化推理路径，并尝试在连续思维空间中训练PRM和ORM模型进行重排序。

Result: Pass@N分析显示生成样本具有显著性能提升的潜力，但实际在连续空间中PRM和ORM模型仅带来边际改进。研究发现连续思维表示缺乏关键归纳偏置，导致难以有效区分正确与错误推理。

Conclusion: 连续推理LM的训练框架不仅需要优化准确性，还应明确纳入可在推理时用于区分正确与错误思维的归纳偏置。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [15] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文首次系统研究大型音频语言模型中的时间偏差问题，发现模型在预测事件时间戳时存在系统性偏差，偏差随音频长度增加而累积，并提出Temporal Bias Index量化这一现象。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在音频理解和多模态推理中应用日益广泛，但其定位事件发生时间的能力尚未得到充分探索，特别是时间戳预测中的系统性偏差问题。

Method: 通过在带时间戳的数据集上进行控制实验，量化分析时间偏差现象，提出Temporal Bias Index(TBI)来测量预测事件时间与真实时间的系统性偏差，并开发可视化框架。

Result: 研究发现时间偏差(i)在各类数据集和模型中普遍存在，(ii)随音频长度增加而累积，在长录音中可达数十秒，(iii)在不同事件类型和位置间存在差异。

Conclusion: 当前大型音频语言模型存在基本的时间定位能力限制，需要开发具有时间鲁棒性的架构来解决这一问题。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>
