{"id": "2507.16951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16951", "abs": "https://arxiv.org/abs/2507.16951", "authors": ["Shuyuan Lin", "Lei Duan", "Philip Hughes", "Yuxuan Sheng"], "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs", "comment": null, "summary": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SALU\uff08Self-Aware LLM for Unanswerability\uff09\uff0c\u4e00\u79cd\u80fd\u591f\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u68c0\u6d4b\u65e0\u6cd5\u56de\u7b54\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u8bdd\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u5bf9\u8bdd\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5728\u5904\u7406\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u65f6\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u5bb9\u6613\u4ea7\u751f\u8bef\u5bfc\u6027\u6216\u5e7b\u89c9\u5185\u5bb9\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\uff0c\u53ef\u80fd\u4e0e\u6838\u5fc3\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faSALU\u65b9\u6cd5\uff0c\u5c06\u65e0\u6cd5\u56de\u7b54\u68c0\u6d4b\u76f4\u63a5\u96c6\u6210\u5230LLM\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u540c\u65f6\u8fdb\u884c\u6807\u51c6\u95ee\u7b54\u548c\u663e\u5f0f\u62d2\u7edd\u751f\u6210\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u5f15\u5bfc\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60(RLHF)\u9636\u6bb5\uff0c\u660e\u786e\u60e9\u7f5a\u5e7b\u89c9\u56de\u7b54\u5e76\u5956\u52b1\u9002\u5f53\u7684\u62d2\u7edd\u56de\u7b54\u3002", "result": "\u5728\u81ea\u5efa\u7684C-IR_Answerability\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSALU\u5728\u6b63\u786e\u56de\u7b54\u6216\u62d2\u7edd\u95ee\u9898\u7684\u6574\u4f53\u51c6\u786e\u6027\u65b9\u9762consistently\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u6df7\u5408LLM-\u5206\u7c7b\u5668\u7cfb\u7edf\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u786e\u8ba4\u4e86SALU\u5728\u4e8b\u5b9e\u6027\u3001\u9002\u5f53\u62d2\u7edd\u548c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u7684\u4f18\u8d8a\u53ef\u9760\u6027\u3002", "conclusion": "SALU\u6210\u529f\u5b9e\u73b0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u81ea\u6211\u610f\u8bc6\uff0c\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\u5e76\u9002\u5f53\u5730\"\u77e5\u9053\u4f55\u65f6\u8bf4'\u6211\u4e0d\u77e5\u9053'\"\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u5bf9\u8bdd\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16841", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16841", "abs": "https://arxiv.org/abs/2507.16841", "authors": ["Waseem Akram", "Muhayy Ud Din", "Abdelhaleem Saad", "Irfan Hussain"], "title": "AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens", "comment": null, "summary": "Inspection of aquaculture net pens is essential for maintaining the\nstructural integrity, biosecurity, and operational efficiency of fish farming\nsystems. Traditional inspection approaches rely on pre-programmed missions or\nmanual control, offering limited adaptability to dynamic underwater conditions\nand user-specific demands. In this study, we propose AquaChat, a novel Remotely\nOperated Vehicle (ROV) framework that integrates Large Language Models (LLMs)\nfor intelligent and adaptive net pen inspection. The system features a\nmulti-layered architecture: (1) a high-level planning layer that interprets\nnatural language user commands using an LLM to generate symbolic task plans;\n(2) a mid-level task manager that translates plans into ROV control sequences;\nand (3) a low-level motion control layer that executes navigation and\ninspection tasks with precision. Real-time feedback and event-triggered\nreplanning enhance robustness in challenging aquaculture environments. The\nframework is validated through experiments in both simulated and controlled\naquatic environments representative of aquaculture net pens. Results\ndemonstrate improved task flexibility, inspection accuracy, and operational\nefficiency. AquaChat illustrates the potential of integrating language-based AI\nwith marine robotics to enable intelligent, user-interactive inspection systems\nfor sustainable aquaculture operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86AquaChat\u6846\u67b6\uff0c\u4e00\u4e2a\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u667a\u80fd\u5316\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5b9e\u73b0\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u3002", "motivation": "\u4f20\u7edf\u7684\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\u65b9\u6cd5\u4f9d\u8d56\u9884\u7f16\u7a0b\u4efb\u52a1\u6216\u4eba\u5de5\u63a7\u5236\uff0c\u5728\u52a8\u6001\u6c34\u4e0b\u73af\u5883\u548c\u7528\u6237\u7279\u5b9a\u9700\u6c42\u9762\u524d\u9002\u5e94\u6027\u6709\u9650\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u66f4\u7075\u6d3b\u7684\u68c0\u67e5\u7cfb\u7edf\u6765\u7ef4\u62a4\u517b\u6b96\u7cfb\u7edf\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3001\u751f\u7269\u5b89\u5168\u6027\u548c\u8fd0\u8425\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u4e86AquaChat\u591a\u5c42\u67b6\u6784\u6846\u67b6\uff1a(1)\u9ad8\u5c42\u89c4\u5212\u5c42\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u7528\u6237\u6307\u4ee4\u5e76\u751f\u6210\u7b26\u53f7\u5316\u4efb\u52a1\u8ba1\u5212\uff1b(2)\u4e2d\u5c42\u4efb\u52a1\u7ba1\u7406\u5668\u5c06\u8ba1\u5212\u8f6c\u6362\u4e3aROV\u63a7\u5236\u5e8f\u5217\uff1b(3)\u4f4e\u5c42\u8fd0\u52a8\u63a7\u5236\u5c42\u7cbe\u786e\u6267\u884c\u5bfc\u822a\u548c\u68c0\u67e5\u4efb\u52a1\u3002\u7cfb\u7edf\u8fd8\u96c6\u6210\u4e86\u5b9e\u65f6\u53cd\u9988\u548c\u4e8b\u4ef6\u89e6\u53d1\u7684\u91cd\u65b0\u89c4\u5212\u673a\u5236\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u53d7\u63a7\u6c34\u751f\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4efb\u52a1\u7075\u6d3b\u6027\u3001\u68c0\u67e5\u7cbe\u5ea6\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u5584\uff0c\u80fd\u591f\u6709\u6548\u9002\u5e94\u5177\u6709\u6311\u6218\u6027\u7684\u6c34\u4ea7\u517b\u6b96\u73af\u5883\u3002", "conclusion": "AquaChat\u5c55\u793a\u4e86\u8bed\u8a00AI\u4e0e\u6d77\u6d0b\u673a\u5668\u4eba\u6280\u672f\u96c6\u6210\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u53ef\u6301\u7eed\u6c34\u4ea7\u517b\u6b96\u4f5c\u4e1a\u63d0\u4f9b\u667a\u80fd\u5316\u3001\u7528\u6237\u4ea4\u4e92\u5f0f\u7684\u68c0\u67e5\u7cfb\u7edf\uff0c\u4e3a\u6c34\u4ea7\u517b\u6b96\u4e1a\u7684\u81ea\u52a8\u5316\u548c\u667a\u80fd\u5316\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2507.16878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16878", "abs": "https://arxiv.org/abs/2507.16878", "authors": ["Xuchen Li", "Xuzhao Li", "Shiyu Hu", "Kaiqi Huang", "Wentao Zhang"], "title": "CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos", "comment": "Preprint, Under review", "summary": "Recent advances in large language models (LLMs) have improved reasoning in\ntext and image domains, yet achieving robust video reasoning remains a\nsignificant challenge. Existing video benchmarks mainly assess shallow\nunderstanding and reasoning and allow models to exploit global context, failing\nto rigorously evaluate true causal and stepwise reasoning. We present\nCausalStep, a benchmark designed for explicit stepwise causal reasoning in\nvideos. CausalStep segments videos into causally linked units and enforces a\nstrict stepwise question-answer (QA) protocol, requiring sequential answers and\npreventing shortcut solutions. Each question includes carefully constructed\ndistractors based on error type taxonomy to ensure diagnostic value. The\nbenchmark features 100 videos across six categories and 1,852 multiple-choice\nQA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,\nenabling precise diagnosis of causal reasoning capabilities. Experiments with\nleading proprietary and open-source models, as well as human baselines, reveal\na significant gap between current models and human-level stepwise reasoning.\nCausalStep provides a rigorous benchmark to drive progress in robust and\ninterpretable video reasoning.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86CausalStep\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u4e2d\u7684\u9010\u6b65\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u6d45\u5c42\u7406\u89e3\u548c\u63a8\u7406\uff0c\u5141\u8bb8\u6a21\u578b\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u4e25\u683c\u8bc4\u4f30\u771f\u6b63\u7684\u56e0\u679c\u548c\u9010\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218", "method": "\u6784\u5efaCausalStep\u57fa\u51c6\u6d4b\u8bd5\uff1a\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u56e0\u679c\u5173\u8054\u7684\u5355\u5143\uff0c\u6267\u884c\u4e25\u683c\u7684\u9010\u6b65\u95ee\u7b54\u534f\u8bae\uff0c\u8981\u6c42\u987a\u5e8f\u56de\u7b54\u5e76\u9632\u6b62\u6377\u5f84\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u542b100\u4e2a\u89c6\u9891\u30016\u4e2a\u7c7b\u522b\u30011852\u4e2a\u591a\u9009\u9898\uff0c\u5e76\u5f15\u51657\u4e2a\u8bca\u65ad\u6307\u6807\u8fdb\u884c\u5168\u9762\u8bc4\u4f30", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u7684\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u4ee5\u53ca\u4eba\u7c7b\u57fa\u7ebf\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "conclusion": "CausalStep\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u52a8\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u9891\u63a8\u7406\u8fdb\u5c55\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177"}}
{"id": "2507.17125", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17125", "abs": "https://arxiv.org/abs/2507.17125", "authors": ["Jacob M. Delgado-L\u00f3pez", "Andrea P. Seda-Hernandez", "Juan D. Guadalupe-Rosado", "Luis E. Fernandez Ramirez", "Miguel Giboyeaux-Camilo", "Wilfredo E. Lugo-Beauchamp"], "title": "Model Compression Engine for Wearable Devices Skin Cancer Diagnosis", "comment": null, "summary": "Skin cancer is one of the most prevalent and preventable types of cancer, yet\nits early detection remains a challenge, particularly in resource-limited\nsettings where access to specialized healthcare is scarce. This study proposes\nan AI-driven diagnostic tool optimized for embedded systems to address this\ngap. Using transfer learning with the MobileNetV2 architecture, the model was\nadapted for binary classification of skin lesions into \"Skin Cancer\" and\n\"Other.\" The TensorRT framework was employed to compress and optimize the model\nfor deployment on the NVIDIA Jetson Orin Nano, balancing performance with\nenergy efficiency. Comprehensive evaluations were conducted across multiple\nbenchmarks, including model size, inference speed, throughput, and power\nconsumption. The optimized models maintained their performance, achieving an\nF1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.\nPost-compression results showed reductions in model size of up to 0.41, along\nwith improvements in inference speed and throughput, and a decrease in energy\nconsumption of up to 0.93 in INT8 precision. These findings validate the\nfeasibility of deploying high-performing, energy-efficient diagnostic tools on\nresource-constrained edge devices. Beyond skin cancer detection, the\nmethodologies applied in this research have broader applications in other\nmedical diagnostics and domains requiring accessible, efficient AI solutions.\nThis study underscores the potential of optimized AI systems to revolutionize\nhealthcare diagnostics, thereby bridging the divide between advanced technology\nand underserved regions.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eMobileNetV2\u548cTensorRT\u4f18\u5316\u7684AI\u76ae\u80a4\u764c\u8bca\u65ad\u5de5\u5177\uff0c\u53ef\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u5b9e\u73b0\u4e8687.18%\u7684F1\u5206\u6570\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u80fd\u8017\u3002", "motivation": "\u76ae\u80a4\u764c\u662f\u6700\u5e38\u89c1\u4e14\u53ef\u9884\u9632\u7684\u764c\u75c7\u7c7b\u578b\u4e4b\u4e00\uff0c\u4f46\u65e9\u671f\u68c0\u6d4b\u4ecd\u7136\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u6709\u9650\u3001\u7f3a\u4e4f\u4e13\u4e1a\u533b\u7597\u670d\u52a1\u7684\u5730\u533a\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684AI\u8bca\u65ad\u5de5\u5177\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u4f7f\u7528MobileNetV2\u67b6\u6784\u8fdb\u884c\u76ae\u80a4\u75c5\u53d8\u7684\u4e8c\u5206\u7c7b\uff08\"\u76ae\u80a4\u764c\"\u548c\"\u5176\u4ed6\"\uff09\uff1b\u4f7f\u7528TensorRT\u6846\u67b6\u5bf9\u6a21\u578b\u8fdb\u884c\u538b\u7f29\u548c\u4f18\u5316\uff0c\u4ee5\u4fbf\u5728NVIDIA Jetson Orin Nano\u4e0a\u90e8\u7f72\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u80fd\u6548\u3002", "result": "\u4f18\u5316\u540e\u7684\u6a21\u578b\u4fdd\u6301\u4e86\u826f\u597d\u6027\u80fd\uff0cF1\u5206\u6570\u8fbe\u523087.18%\uff0c\u7cbe\u786e\u738793.18%\uff0c\u53ec\u56de\u738781.91%\uff1b\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u81f30.41\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u548c\u541e\u5410\u91cf\u63d0\u5347\uff0cINT8\u7cbe\u5ea6\u4e0b\u80fd\u8017\u964d\u4f4e\u81f30.93\u500d\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9ad8\u6027\u80fd\u3001\u9ad8\u80fd\u6548\u8bca\u65ad\u5de5\u5177\u7684\u53ef\u884c\u6027\uff1b\u8be5\u7814\u7a76\u65b9\u6cd5\u5728\u5176\u4ed6\u533b\u7597\u8bca\u65ad\u548c\u9700\u8981\u53ef\u8bbf\u95ee\u3001\u9ad8\u6548AI\u89e3\u51b3\u65b9\u6848\u7684\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff1b\u4f18\u5316\u7684AI\u7cfb\u7edf\u6709\u6f5c\u529b\u9769\u547d\u6027\u5730\u6539\u53d8\u533b\u7597\u8bca\u65ad\uff0c\u7f29\u5c0f\u5148\u8fdb\u6280\u672f\u4e0e\u670d\u52a1\u4e0d\u8db3\u5730\u533a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.17161", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17161", "abs": "https://arxiv.org/abs/2507.17161", "authors": ["Vinura Galwaduge", "Jagath Samarabandu"], "title": "Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection", "comment": null, "summary": "Modern network intrusion detection systems (NIDS) frequently utilize the\npredictive power of complex deep learning models. However, the \"black-box\"\nnature of such deep learning methods adds a layer of opaqueness that hinders\nthe proper understanding of detection decisions, trust in the decisions and\nprevent timely countermeasures against such attacks. Explainable AI (XAI)\nmethods provide a solution to this problem by providing insights into the\ncauses of the predictions. The majority of the existing XAI methods provide\nexplanations which are not convenient to convert into actionable\ncountermeasures. In this work, we propose a novel diffusion-based\ncounterfactual explanation framework that can provide actionable explanations\nfor network intrusion attacks. We evaluated our proposed algorithm against\nseveral other publicly available counterfactual explanation algorithms on 3\nmodern network intrusion datasets. To the best of our knowledge, this work also\npresents the first comparative analysis of existing counterfactual explanation\nalgorithms within the context of network intrusion detection systems. Our\nproposed method provide minimal, diverse counterfactual explanations out of the\ntested counterfactual explanation algorithms in a more efficient manner by\nreducing the time to generate explanations. We also demonstrate how\ncounterfactual explanations can provide actionable explanations by summarizing\nthem to create a set of global rules. These rules are actionable not only at\ninstance level but also at the global level for intrusion attacks. These global\ncounterfactual rules show the ability to effectively filter out incoming attack\nqueries which is crucial for efficient intrusion detection and defense\nmechanisms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u53ef\u89e3\u91caAI\uff0c\u80fd\u591f\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u89e3\u91ca\u5e76\u8f6c\u5316\u4e3a\u5168\u5c40\u9632\u5fa1\u89c4\u5219\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\"\u9ed1\u76d2\"\u7279\u6027\u963b\u788d\u4e86\u5bf9\u68c0\u6d4b\u51b3\u7b56\u7684\u7406\u89e3\u548c\u4fe1\u4efb\uff0c\u73b0\u6709\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\u96be\u4ee5\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5bf9\u6297\u63aa\u65bd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6700\u5c0f\u5316\u3001\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e76\u5c06\u8fd9\u4e9b\u89e3\u91ca\u603b\u7ed3\u4e3a\u5168\u5c40\u89c4\u5219\u96c6\uff0c\u5b9e\u73b0\u4ece\u5b9e\u4f8b\u7ea7\u5230\u5168\u5c40\u7ea7\u7684\u53ef\u64cd\u4f5c\u89e3\u91ca\u3002", "result": "\u57283\u4e2a\u73b0\u4ee3\u7f51\u7edc\u5165\u4fb5\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4\u5176\u4ed6\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7b97\u6cd5\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u751f\u6210\u6700\u5c0f\u5316\u3001\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u51cf\u5c11\u4e86\u89e3\u91ca\u751f\u6210\u65f6\u95f4\uff0c\u5e76\u6210\u529f\u521b\u5efa\u4e86\u80fd\u591f\u6709\u6548\u8fc7\u6ee4\u653b\u51fb\u67e5\u8be2\u7684\u5168\u5c40\u53cd\u4e8b\u5b9e\u89c4\u5219\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7b97\u6cd5\u7684\u9996\u6b21\u6bd4\u8f83\u5206\u6790\uff0c\u8fd8\u8bc1\u660e\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u53ef\u4ee5\u8f6c\u5316\u4e3a\u5b9e\u4f8b\u7ea7\u548c\u5168\u5c40\u7ea7\u7684\u53ef\u64cd\u4f5c\u89e3\u91ca\uff0c\u8fd9\u4e9b\u5168\u5c40\u89c4\u5219\u5bf9\u4e8e\u9ad8\u6548\u7684\u5165\u4fb5\u68c0\u6d4b\u548c\u9632\u5fa1\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.17189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17189", "abs": "https://arxiv.org/abs/2507.17189", "authors": ["Shaohan Li", "Hao Yang", "Min Chen", "Xiaolin Qin"], "title": "Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems", "comment": null, "summary": "The increasing frequency of extreme weather events due to global climate\nchange urges accurate weather prediction. Recently, great advances have been\nmade by the \\textbf{end-to-end methods}, thanks to deep learning techniques,\nbut they face limitations of \\textit{representation inconsistency} in\nmultivariable integration and struggle to effectively capture the dependency\nbetween variables, which is required in complex weather systems. Treating\ndifferent variables as distinct modalities and applying a \\textbf{two-stage\ntraining approach} from multimodal models can partially alleviate this issue,\nbut due to the inconformity in training tasks between the two stages, the\nresults are often suboptimal. To address these challenges, we propose an\nimplicit two-stage training method, configuring separate encoders and decoders\nfor each variable. In detailed, in the first stage, the Translator is frozen\nwhile the Encoders and Decoders learn a shared latent space, in the second\nstage, the Encoders and Decoders are frozen, and the Translator captures\ninter-variable interactions for prediction. Besides, by introducing a\nself-attention mechanism for multivariable fusion in the latent space, the\nperformance achieves further improvements. Empirically, extensive experiments\nshow the state-of-the-art performance of our method. Specifically, it reduces\nthe MSE for near-surface air temperature and relative humidity predictions by\n28.82\\% and 23.39\\%, respectively. The source code is available at\nhttps://github.com/ShremG/Met2Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5Met2Net\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u53d8\u91cf\u914d\u7f6e\u72ec\u7acb\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u5e76\u5f15\u5165\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u591a\u53d8\u91cf\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u7684\u51c6\u786e\u6027", "motivation": "\u5168\u7403\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u9891\u53d1\uff0c\u8feb\u5207\u9700\u8981\u51c6\u786e\u7684\u5929\u6c14\u9884\u6d4b\u3002\u73b0\u6709\u7aef\u5230\u7aef\u65b9\u6cd5\u5728\u591a\u53d8\u91cf\u96c6\u6210\u4e2d\u5b58\u5728\u8868\u793a\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u96be\u4ee5\u6709\u6548\u6355\u83b7\u590d\u6742\u5929\u6c14\u7cfb\u7edf\u4e2d\u53d8\u91cf\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u4f20\u7edf\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u7531\u4e8e\u4e24\u4e2a\u9636\u6bb5\u8bad\u7ec3\u4efb\u52a1\u4e0d\u4e00\u81f4\uff0c\u6548\u679c\u5f80\u5f80\u4e0d\u7406\u60f3", "method": "\u63d0\u51fa\u9690\u5f0f\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u51bb\u7ed3Translator\uff0c\u8ba9\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5b66\u4e60\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u51bb\u7ed3\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u8ba9Translator\u6355\u83b7\u53d8\u91cf\u95f4\u4ea4\u4e92\u8fdb\u884c\u9884\u6d4b\u3002\u540c\u65f6\u5f15\u5165\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u591a\u53d8\u91cf\u878d\u5408", "result": "\u5728\u8fd1\u5730\u8868\u6c14\u6e29\u548c\u76f8\u5bf9\u6e7f\u5ea6\u9884\u6d4b\u4e2d\uff0cMSE\u5206\u522b\u964d\u4f4e\u4e8628.82%\u548c23.39%\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u53d8\u91cf\u5929\u6c14\u9884\u6d4b\u4e2d\u7684\u8868\u793a\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6781\u7aef\u5929\u6c14\u9884\u6d4b\u7684\u51c6\u786e\u6027"}}
{"id": "2507.17649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17649", "abs": "https://arxiv.org/abs/2507.17649", "authors": ["J. D. Clark", "P. Ellison"], "title": "Event Detection for Active Lower Limb Prosthesis", "comment": null, "summary": "Accurate event detection is key to the successful design of semi-passive and\npowered prosthetics. Kinematically, the natural knee is complex, with\ntranslation and rotation components that have a substantial impact on gait\ncharacteristics. When simplified to a pin joint, some of this behaviour is\nlost. This study investigates the role of cruciate ligament stretch in event\ndetection. A bicondylar knee design was used, constrained by analogues of the\nanterior and posterior cruciate ligaments. This offers the ability to\ncharacterize knee kinematics by the stretch of the ligaments. The ligament\nstretch was recorded using LVDTs parallel to the ligaments of the Russell knee\non a bent knee crutch. Which was used to capture data on a treadmill at 3\nspeeds. This study finds speed dependence within the stretch of the cruciate\nligaments, prominently around 5\\% and 80\\% of the gait cycle for the posterior\nand anterior. The cycle profile remains consistent with speed; therefore, other\nstatic events such as the turning point feature at around 90\\% and 95\\% of the\ncycle, for the posterior and anterior, respectively, could be used as a\npredictive precursor for initial contact. Likewise at 90\\% and 95\\%, another\npair of turning points that in this case could be used to predict foot flat.\nThis concludes that the use of a bicondylar knee design could improve the\ndetection of events during the gait cycle, and therefore could increase the\naccuracy of subsequent controllers for powered prosthetics.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\u4e2d\u7684\u5341\u5b57\u97e7\u5e26\u62c9\u4f38\u6765\u6539\u8fdb\u6b65\u6001\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u53d1\u73b0\u97e7\u5e26\u62c9\u4f38\u6a21\u5f0f\u53ef\u7528\u4e8e\u9884\u6d4b\u6b65\u6001\u5468\u671f\u4e2d\u7684\u5173\u952e\u4e8b\u4ef6\uff0c\u4ece\u800c\u63d0\u9ad8\u52a8\u529b\u5047\u80a2\u63a7\u5236\u5668\u7684\u51c6\u786e\u6027", "motivation": "\u4f20\u7edf\u7684\u9500\u8f74\u5173\u8282\u7b80\u5316\u8bbe\u8ba1\u4f1a\u4e22\u5931\u81ea\u7136\u819d\u5173\u8282\u7684\u590d\u6742\u8fd0\u52a8\u5b66\u7279\u6027\uff0c\u5f71\u54cd\u6b65\u6001\u7279\u5f81\u3002\u51c6\u786e\u7684\u4e8b\u4ef6\u68c0\u6d4b\u5bf9\u534a\u88ab\u52a8\u548c\u52a8\u529b\u5047\u80a2\u7684\u6210\u529f\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u597d\u7684\u819d\u5173\u8282\u8bbe\u8ba1\u6765\u6539\u5584\u4e8b\u4ef6\u68c0\u6d4b\u80fd\u529b", "method": "\u91c7\u7528\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\uff0c\u901a\u8fc7\u524d\u540e\u5341\u5b57\u97e7\u5e26\u7c7b\u4f3c\u7269\u8fdb\u884c\u7ea6\u675f\u3002\u4f7f\u7528\u4e0eRussell\u819d\u5173\u8282\u97e7\u5e26\u5e73\u884c\u7684LVDT\u8bb0\u5f55\u97e7\u5e26\u62c9\u4f38\u60c5\u51b5\u3002\u5728\u5f2f\u819d\u62d0\u6756\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728\u8dd1\u6b65\u673a\u4e0a\u4ee53\u79cd\u4e0d\u540c\u901f\u5ea6\u91c7\u96c6\u6570\u636e\uff0c\u5206\u6790\u97e7\u5e26\u62c9\u4f38\u4e0e\u6b65\u6001\u5468\u671f\u7684\u5173\u7cfb", "result": "\u53d1\u73b0\u5341\u5b57\u97e7\u5e26\u62c9\u4f38\u5b58\u5728\u901f\u5ea6\u4f9d\u8d56\u6027\uff0c\u4e3b\u8981\u51fa\u73b0\u5728\u6b65\u6001\u5468\u671f\u76845%\u548c80%\u5904\uff08\u540e\u5341\u5b57\u97e7\u5e26\u548c\u524d\u5341\u5b57\u97e7\u5e26\uff09\u3002\u5468\u671f\u8f6e\u5ed3\u968f\u901f\u5ea6\u4fdd\u6301\u4e00\u81f4\uff0c\u572890%\u548c95%\u5904\u51fa\u73b0\u8f6c\u6298\u70b9\u7279\u5f81\uff0c\u53ef\u5206\u522b\u7528\u4e8e\u9884\u6d4b\u521d\u59cb\u63a5\u89e6\u548c\u8db3\u5e73\u653e\u7f6e\u4e8b\u4ef6", "conclusion": "\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\u80fd\u591f\u6539\u5584\u6b65\u6001\u5468\u671f\u4e2d\u7684\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u901a\u8fc7\u97e7\u5e26\u62c9\u4f38\u6a21\u5f0f\u53ef\u4ee5\u9884\u6d4b\u5173\u952e\u6b65\u6001\u4e8b\u4ef6\uff0c\u4ece\u800c\u63d0\u9ad8\u52a8\u529b\u5047\u80a2\u540e\u7eed\u63a7\u5236\u5668\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5047\u80a2\u63a7\u5236\u7cfb\u7edf\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def"}}
{"id": "2507.17273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17273", "abs": "https://arxiv.org/abs/2507.17273", "authors": ["Rishi Parekh", "Saisubramaniam Gopalakrishnan", "Zishan Ahmad", "Anirudh Deodhar"], "title": "Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance", "comment": "12 pages, 2 figures", "summary": "Analyzing large, complex output datasets from Discrete Event Simulations\n(DES) of warehouse operations to identify bottlenecks and inefficiencies is a\ncritical yet challenging task, often demanding significant manual effort or\nspecialized analytical tools. Our framework integrates Knowledge Graphs (KGs)\nand Large Language Model (LLM)-based agents to analyze complex Discrete Event\nSimulation (DES) output data from warehouse operations. It transforms raw DES\ndata into a semantically rich KG, capturing relationships between simulation\nevents and entities. An LLM-based agent uses iterative reasoning, generating\ninterdependent sub-questions. For each sub-question, it creates Cypher queries\nfor KG interaction, extracts information, and self-reflects to correct errors.\nThis adaptive, iterative, and self-correcting process identifies operational\nissues mimicking human analysis. Our DES approach for warehouse bottleneck\nidentification, tested with equipment breakdowns and process irregularities,\noutperforms baseline methods. For operational questions, it achieves\nnear-perfect pass rates in pinpointing inefficiencies. For complex\ninvestigative questions, we demonstrate its superior diagnostic ability to\nuncover subtle, interconnected issues. This work bridges simulation modeling\nand AI (KG+LLM), offering a more intuitive method for actionable insights,\nreducing time-to-insight, and enabling automated warehouse inefficiency\nevaluation and diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31(KG)\u548c\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4ed3\u5e93\u8fd0\u8425\u7684\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f(DES)\u8f93\u51fa\u6570\u636e\uff0c\u81ea\u52a8\u8bc6\u522b\u74f6\u9888\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5206\u6790\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5206\u6790\u4ed3\u5e93\u8fd0\u8425\u7684\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u4ea7\u751f\u7684\u5927\u578b\u590d\u6742\u8f93\u51fa\u6570\u636e\u96c6\u4ee5\u8bc6\u522b\u74f6\u9888\u548c\u4f4e\u6548\u95ee\u9898\u662f\u4e00\u9879\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u4eba\u5de5\u5de5\u4f5c\u6216\u4e13\u95e8\u7684\u5206\u6790\u5de5\u5177\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u81ea\u52a8\u5316\u548c\u667a\u80fd\u5316\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u96c6\u6210\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u6846\u67b6\uff1a1\uff09\u5c06\u539f\u59cbDES\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u6355\u83b7\u4eff\u771f\u4e8b\u4ef6\u548c\u5b9e\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b2\uff09\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\uff0c\u751f\u6210\u76f8\u4e92\u4f9d\u8d56\u7684\u5b50\u95ee\u9898\uff1b3\uff09\u4e3a\u6bcf\u4e2a\u5b50\u95ee\u9898\u521b\u5efaCypher\u67e5\u8be2\u4e0e\u77e5\u8bc6\u56fe\u8c31\u4ea4\u4e92\uff0c\u63d0\u53d6\u4fe1\u606f\u5e76\u8fdb\u884c\u81ea\u6211\u53cd\u601d\u4ee5\u7ea0\u6b63\u9519\u8bef\uff1b4\uff09\u901a\u8fc7\u8fd9\u79cd\u81ea\u9002\u5e94\u3001\u8fed\u4ee3\u548c\u81ea\u6211\u7ea0\u9519\u7684\u8fc7\u7a0b\u6765\u8bc6\u522b\u8fd0\u8425\u95ee\u9898\u3002", "result": "\u5728\u8bbe\u5907\u6545\u969c\u548c\u6d41\u7a0b\u5f02\u5e38\u7684\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4ed3\u5e93\u74f6\u9888\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5bf9\u4e8e\u8fd0\u8425\u95ee\u9898\uff0c\u5728\u8bc6\u522b\u4f4e\u6548\u95ee\u9898\u65b9\u9762\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u901a\u8fc7\u7387\u3002\u5bf9\u4e8e\u590d\u6742\u7684\u8c03\u67e5\u6027\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u53d1\u73b0\u5fae\u5999\u3001\u76f8\u4e92\u5173\u8054\u95ee\u9898\u65b9\u9762\u7684\u5353\u8d8a\u8bca\u65ad\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6210\u529f\u8fde\u63a5\u4e86\u4eff\u771f\u5efa\u6a21\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff08\u77e5\u8bc6\u56fe\u8c31+\u5927\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u7684\u65b9\u6cd5\u6765\u83b7\u5f97\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\uff0c\u51cf\u5c11\u4e86\u83b7\u5f97\u6d1e\u5bdf\u6240\u9700\u7684\u65f6\u95f4\uff0c\u5e76\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u7684\u4ed3\u5e93\u4f4e\u6548\u8bc4\u4f30\u548c\u8bca\u65ad\u3002"}}
{"id": "2507.17268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17268", "abs": "https://arxiv.org/abs/2507.17268", "authors": ["Kailong Zhang", "Youwei Lyu", "Heng Guo", "Si Li", "Zhanyu Ma", "Boxin Shi"], "title": "PolarAnything: Diffusion-based Polarimetric Image Synthesis", "comment": "11 pages", "summary": "Polarization images facilitate image enhancement and 3D reconstruction tasks,\nbut the limited accessibility of polarization cameras hinders their broader\napplication. This gap drives the need for synthesizing photorealistic\npolarization images.The existing polarization simulator Mitsuba relies on a\nparametric polarization image formation model and requires extensive 3D assets\ncovering shape and PBR materials, preventing it from generating large-scale\nphotorealistic images. To address this problem, we propose PolarAnything,\ncapable of synthesizing polarization images from a single RGB input with both\nphotorealism and physical accuracy, eliminating the dependency on 3D asset\ncollections. Drawing inspiration from the zero-shot performance of pretrained\ndiffusion models, we introduce a diffusion-based generative framework with an\neffective representation strategy that preserves the fidelity of polarization\nproperties. Experiments show that our model generates high-quality polarization\nimages and supports downstream tasks like shape from polarization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPolarAnything\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u5177\u6709\u771f\u5b9e\u611f\u548c\u7269\u7406\u51c6\u786e\u6027\u7684\u504f\u632f\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u504f\u632f\u76f8\u673a\u53ef\u53ca\u6027\u6709\u9650\u548c\u4f20\u7edf\u504f\u632f\u6a21\u62df\u5668\u9700\u8981\u5927\u91cf3D\u8d44\u4ea7\u7684\u95ee\u9898\u3002", "motivation": "\u504f\u632f\u56fe\u50cf\u5728\u56fe\u50cf\u589e\u5f3a\u548c3D\u91cd\u5efa\u4efb\u52a1\u4e2d\u5f88\u6709\u7528\uff0c\u4f46\u504f\u632f\u76f8\u673a\u7684\u53ef\u53ca\u6027\u6709\u9650\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684\u504f\u632f\u6a21\u62df\u5668Mitsuba\u4f9d\u8d56\u53c2\u6570\u5316\u504f\u632f\u56fe\u50cf\u5f62\u6210\u6a21\u578b\uff0c\u9700\u8981\u5927\u91cf\u5305\u542b\u5f62\u72b6\u548cPBR\u6750\u8d28\u76843D\u8d44\u4ea7\uff0c\u65e0\u6cd5\u751f\u6210\u5927\u89c4\u6a21\u7684\u771f\u5b9e\u611f\u56fe\u50cf\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5408\u6210\u771f\u5b9e\u611f\u504f\u632f\u56fe\u50cf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faPolarAnything\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u53d7\u5230\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u96f6\u6837\u672c\u6027\u80fd\u7684\u542f\u53d1\uff0c\u91c7\u7528\u6709\u6548\u7684\u8868\u793a\u7b56\u7565\u6765\u4fdd\u6301\u504f\u632f\u7279\u6027\u7684\u4fdd\u771f\u5ea6\uff0c\u80fd\u591f\u4ece\u5355\u5f20RGB\u8f93\u5165\u751f\u6210\u504f\u632f\u56fe\u50cf\uff0c\u6d88\u9664\u4e86\u5bf93D\u8d44\u4ea7\u96c6\u5408\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u504f\u632f\u56fe\u50cf\uff0c\u540c\u65f6\u5177\u6709\u771f\u5b9e\u611f\u548c\u7269\u7406\u51c6\u786e\u6027\u3002\u751f\u6210\u7684\u504f\u632f\u56fe\u50cf\u8fd8\u80fd\u591f\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\uff0c\u5982\u57fa\u4e8e\u504f\u632f\u7684\u5f62\u72b6\u6062\u590d\uff08shape from polarization\uff09\u3002", "conclusion": "PolarAnything\u6210\u529f\u89e3\u51b3\u4e86\u504f\u632f\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20RGB\u56fe\u50cf\u5230\u504f\u632f\u56fe\u50cf\u7684\u8f6c\u6362\uff0c\u65e2\u4fdd\u8bc1\u4e86\u89c6\u89c9\u771f\u5b9e\u611f\u53c8\u7ef4\u6301\u4e86\u7269\u7406\u51c6\u786e\u6027\uff0c\u4e3a\u504f\u632f\u6210\u50cf\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.17281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17281", "abs": "https://arxiv.org/abs/2507.17281", "authors": ["Huanli Zhuo", "Leilei Ma", "Haifeng Zhao", "Shiwei Zhou", "Dengdi Sun", "Yanping Fu"], "title": "Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation", "comment": "This manuscript has been accepted for presentation at the IEEE\n  International Conference on Systems, Man, and Cybernetics (IEEE SMC 2025) and\n  is copyrighted by IEEE", "summary": "Although SAM-based single-source domain generalization models for medical\nimage segmentation can mitigate the impact of domain shift on the model in\ncross-domain scenarios, these models still face two major challenges. First,\nthe segmentation of SAM is highly dependent on domain-specific expert-annotated\nprompts, which prevents SAM from achieving fully automated medical image\nsegmentation and therefore limits its application in clinical settings. Second,\nproviding poor prompts (such as bounding boxes that are too small or too large)\nto the SAM prompt encoder can mislead SAM into generating incorrect mask\nresults. Therefore, we propose the FA-SAM, a single-source domain\ngeneralization framework for medical image segmentation that achieves fully\nautomated SAM. FA-SAM introduces two key innovations: an Auto-prompted\nGeneration Model (AGM) branch equipped with a Shallow Feature Uncertainty\nModeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module\nintegrated into the SAM mask decoder. Specifically, AGM models the uncertainty\ndistribution of shallow features through the SUFM module to generate bounding\nbox prompts for the target domain, enabling fully automated segmentation with\nSAM. The IPEF module integrates multiscale information from SAM image\nembeddings and prompt embeddings to capture global and local details of the\ntarget object, enabling SAM to mitigate the impact of poor prompts. Extensive\nexperiments on publicly available prostate and fundus vessel datasets validate\nthe effectiveness of FA-SAM and highlight its potential to address the above\nchallenges.", "AI": {"tldr": "\u63d0\u51fa\u4e86FA-SAM\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u63d0\u793a\u751f\u6210\u6a21\u578b(AGM)\u548c\u56fe\u50cf-\u63d0\u793a\u5d4c\u5165\u878d\u5408(IPEF)\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u5168\u81ea\u52a8\u5316SAM\uff0c\u89e3\u51b3\u4e86SAM\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u63d0\u793a\u548c\u9519\u8bef\u63d0\u793a\u5bfc\u81f4\u5206\u5272\u5931\u8d25\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8eSAM\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1)SAM\u9ad8\u5ea6\u4f9d\u8d56\u9886\u57df\u4e13\u5bb6\u6807\u6ce8\u7684\u63d0\u793a\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5168\u81ea\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u9650\u5236\u4e34\u5e8a\u5e94\u7528\uff1b2)\u9519\u8bef\u7684\u63d0\u793a(\u5982\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684\u8fb9\u754c\u6846)\u4f1a\u8bef\u5bfcSAM\u751f\u6210\u9519\u8bef\u7684\u63a9\u7801\u7ed3\u679c", "method": "\u63d0\u51faFA-SAM\u5355\u6e90\u57df\u6cdb\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1)\u914d\u5907\u6d45\u5c42\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21(SUFM)\u6a21\u5757\u7684\u81ea\u52a8\u63d0\u793a\u751f\u6210\u6a21\u578b(AGM)\u5206\u652f\uff1b2)\u96c6\u6210\u5230SAM\u63a9\u7801\u89e3\u7801\u5668\u4e2d\u7684\u56fe\u50cf-\u63d0\u793a\u5d4c\u5165\u878d\u5408(IPEF)\u6a21\u5757\u3002AGM\u901a\u8fc7SUFM\u6a21\u5757\u5efa\u6a21\u6d45\u5c42\u7279\u5f81\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u5e03\u6765\u751f\u6210\u8fb9\u754c\u6846\u63d0\u793a\uff0cIPEF\u6a21\u5757\u6574\u5408SAM\u56fe\u50cf\u5d4c\u5165\u548c\u63d0\u793a\u5d4c\u5165\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\u6765\u6355\u83b7\u76ee\u6807\u5bf9\u8c61\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7ec6\u8282", "result": "\u5728\u516c\u5f00\u7684\u524d\u5217\u817a\u548c\u773c\u5e95\u8840\u7ba1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FA-SAM\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\u5e76\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u6f5c\u529b", "conclusion": "FA-SAM\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u63d0\u793a\u751f\u6210\u548c\u56fe\u50cf-\u63d0\u793a\u5d4c\u5165\u878d\u5408\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u5168\u81ea\u52a8\u5316SAM\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u9519\u8bef\u63d0\u793a\u7684\u5f71\u54cd\uff0c\u4e3aSAM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17346", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17346", "abs": "https://arxiv.org/abs/2507.17346", "authors": ["Rongwei Lu", "Jingyan Jiang", "Chunyang Li", "Haotian Dong", "Xingguang Wei", "Delin Cai", "Zhi Wang"], "title": "DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression Ratio for Distributed SGD", "comment": null, "summary": "Distributed machine learning in high end-to-end latency and low, varying\nbandwidth network environments undergoes severe throughput degradation. Due to\nits low communication requirements, distributed SGD (D-SGD) remains the\nmainstream optimizer in such challenging networks, but it still suffers from\nsignificant throughput reduction. To mitigate these limitations, existing\napproaches typically employ gradient compression and delayed aggregation to\nalleviate low bandwidth and high latency, respectively. To address both\nchallenges simultaneously, these strategies are often combined, introducing a\ncomplex three-way trade-off among compression ratio, staleness (delayed\nsynchronization steps), and model convergence rate. To achieve the balance\nunder varying bandwidth conditions, an adaptive policy is required to\ndynamically adjust these parameters. Unfortunately, existing works rely on\nstatic heuristic strategies due to the lack of theoretical guidance, which\nprevents them from achieving this goal. This study fills in this theoretical\ngap by introducing a new theoretical tool, decomposing the joint optimization\nproblem into a traditional convergence rate analysis with multiple analyzable\nnoise terms. We are the first to reveal that staleness exponentially amplifies\nthe negative impact of gradient compression on training performance, filling a\ncritical gap in understanding how compressed and delayed gradients affect\ntraining. Furthermore, by integrating the convergence rate with a network-aware\ntime minimization condition, we propose DeCo-SGD, which dynamically adjusts the\ncompression ratio and staleness based on the real-time network condition and\ntraining task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and\nstatic strategy in high-latency and low, varying bandwidth networks,\nrespectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DeCo-SGD\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u68af\u5ea6\u538b\u7f29\u6bd4\u548c\u5ef6\u8fdf\u540c\u6b65\u6b65\u6570\u6765\u89e3\u51b3\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5728\u9ad8\u5ef6\u8fdf\u3001\u4f4e\u5e26\u5bbd\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u541e\u5410\u91cf\u4e0b\u964d\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad85.07\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5728\u9ad8\u5ef6\u8fdf\u3001\u4f4e\u5e26\u5bbd\u4e14\u53d8\u5316\u7684\u7f51\u7edc\u73af\u5883\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u541e\u5410\u91cf\u4e0b\u964d\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u7ed3\u5408\u68af\u5ea6\u538b\u7f29\u548c\u5ef6\u8fdf\u805a\u5408\u6765\u5e94\u5bf9\u4f4e\u5e26\u5bbd\u548c\u9ad8\u5ef6\u8fdf\uff0c\u4f46\u8fd9\u5f15\u5165\u4e86\u538b\u7f29\u6bd4\u3001\u5ef6\u8fdf\u6b65\u6570\u548c\u6a21\u578b\u6536\u655b\u7387\u4e4b\u95f4\u590d\u6742\u7684\u4e09\u65b9\u6743\u8861\u3002\u7531\u4e8e\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\uff0c\u73b0\u6709\u5de5\u4f5c\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5728\u53d8\u5316\u5e26\u5bbd\u6761\u4ef6\u4e0b\u7684\u52a8\u6001\u5e73\u8861\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5c06\u8054\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u4f20\u7edf\u6536\u655b\u7387\u5206\u6790\u4e0e\u591a\u4e2a\u53ef\u5206\u6790\u566a\u58f0\u9879\u7684\u7ec4\u5408\u3002\u901a\u8fc7\u5c06\u6536\u655b\u7387\u4e0e\u7f51\u7edc\u611f\u77e5\u7684\u65f6\u95f4\u6700\u5c0f\u5316\u6761\u4ef6\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86DeCo-SGD\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6839\u636e\u5b9e\u65f6\u7f51\u7edc\u6761\u4ef6\u548c\u8bad\u7ec3\u4efb\u52a1\u52a8\u6001\u8c03\u6574\u538b\u7f29\u6bd4\u548c\u5ef6\u8fdf\u6b65\u6570\u3002", "result": "DeCo-SGD\u5728\u9ad8\u5ef6\u8fdf\u548c\u4f4e\u53d8\u5316\u5e26\u5bbd\u7f51\u7edc\u4e2d\u5206\u522b\u5b9e\u73b0\u4e86\u76f8\u6bd4D-SGD\u6700\u9ad85.07\u500d\u548c\u76f8\u6bd4\u9759\u6001\u7b56\u75651.37\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002\u7814\u7a76\u9996\u6b21\u63ed\u793a\u4e86\u5ef6\u8fdf\u4f1a\u6307\u6570\u7ea7\u653e\u5927\u68af\u5ea6\u538b\u7f29\u5bf9\u8bad\u7ec3\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u538b\u7f29\u548c\u5ef6\u8fdf\u68af\u5ea6\u5982\u4f55\u5f71\u54cd\u8bad\u7ec3\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u52a8\u6001\u8c03\u6574\u7b56\u7565\u7684\u7406\u8bba\u57fa\u7840\u3002DeCo-SGD\u7b97\u6cd5\u901a\u8fc7\u5b9e\u65f6\u9002\u5e94\u7f51\u7edc\u6761\u4ef6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5728\u5177\u6709\u6311\u6218\u6027\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u4e3a\u5206\u5e03\u5f0f\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17368", "abs": "https://arxiv.org/abs/2507.17368", "authors": ["Hao Dai", "Chong Tang", "Jagmohan Chauhan"], "title": "ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning", "comment": "6 pages, 2 figures", "summary": "Continual learning (CL) with long-tailed data distributions remains a\ncritical challenge for real-world AI systems, where models must sequentially\nadapt to new classes while retaining knowledge of old ones, despite severe\nclass imbalance. Existing methods struggle to balance stability and plasticity,\noften collapsing under extreme sample scarcity. To address this, we propose\nViRN, a novel CL framework that integrates variational inference (VI) with\ndistributional trilateration for robust long-tailed learning. First, we model\nclass-conditional distributions via a Variational Autoencoder to mitigate bias\ntoward head classes. Second, we reconstruct tail-class distributions via\nWasserstein distance-based neighborhood retrieval and geometric fusion,\nenabling sample-efficient alignment of tail-class representations. Evaluated on\nsix long-tailed classification benchmarks, including speech (e.g., rare\nacoustic events, accents) and image tasks, ViRN achieves a 10.24% average\naccuracy gain over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86ViRN\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u548c\u5206\u5e03\u4e09\u89d2\u6d4b\u91cf\u6280\u672f\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u957f\u5c3e\u6570\u636e\u5206\u5e03\u7684\u6311\u6218\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534710.24%", "motivation": "\u73b0\u5b9e\u4e16\u754cAI\u7cfb\u7edf\u9762\u4e34\u6301\u7eed\u5b66\u4e60\u4e2d\u957f\u5c3e\u6570\u636e\u5206\u5e03\u7684\u5173\u952e\u6311\u6218\uff0c\u6a21\u578b\u9700\u8981\u5728\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u987a\u5e8f\u9002\u5e94\u65b0\u7c7b\u522b\u540c\u65f6\u4fdd\u6301\u5bf9\u65e7\u7c7b\u522b\u7684\u77e5\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u7ecf\u5e38\u5728\u6781\u7aef\u6837\u672c\u7a00\u7f3a\u4e0b\u5d29\u6e83", "method": "\u63d0\u51faViRN\u6846\u67b6\uff0c\u96c6\u6210\u53d8\u5206\u63a8\u7406\u548c\u5206\u5e03\u4e09\u89d2\u6d4b\u91cf\uff1a1\uff09\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u7c7b\u6761\u4ef6\u5206\u5e03\u4ee5\u51cf\u8f7b\u5bf9\u5934\u90e8\u7c7b\u522b\u7684\u504f\u5dee\uff1b2\uff09\u901a\u8fc7\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u90bb\u57df\u68c0\u7d22\u548c\u51e0\u4f55\u878d\u5408\u91cd\u6784\u5c3e\u90e8\u7c7b\u522b\u5206\u5e03\uff0c\u5b9e\u73b0\u5c3e\u90e8\u7c7b\u522b\u8868\u793a\u7684\u6837\u672c\u9ad8\u6548\u5bf9\u9f50", "result": "\u5728\u516d\u4e2a\u957f\u5c3e\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u8bed\u97f3\u4efb\u52a1\uff08\u5982\u7a00\u6709\u58f0\u5b66\u4e8b\u4ef6\u3001\u53e3\u97f3\uff09\u548c\u56fe\u50cf\u4efb\u52a1\uff0cViRN\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534710.24%", "conclusion": "ViRN\u6846\u67b6\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u548c\u5206\u5e03\u4e09\u89d2\u6d4b\u91cf\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u957f\u5c3e\u6570\u636e\u5206\u5e03\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17664", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17664", "abs": "https://arxiv.org/abs/2507.17664", "authors": ["Lingdong Kong", "Dongyue Lu", "Ao Liang", "Rong Li", "Yuhao Dong", "Tianshuai Hu", "Lai Xing Ng", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras", "comment": "Preprint; 42 pages, 17 figures, 16 tables; Project Page at\n  https://talk2event.github.io", "summary": "Event cameras offer microsecond-level latency and robustness to motion blur,\nmaking them ideal for understanding dynamic environments. Yet, connecting these\nasynchronous streams to human language remains an open challenge. We introduce\nTalk2Event, the first large-scale benchmark for language-driven object\ngrounding in event-based perception. Built from real-world driving data, we\nprovide over 30,000 validated referring expressions, each enriched with four\ngrounding attributes -- appearance, status, relation to viewer, and relation to\nother objects -- bridging spatial, temporal, and relational reasoning. To fully\nexploit these cues, we propose EventRefer, an attribute-aware grounding\nframework that dynamically fuses multi-attribute representations through a\nMixture of Event-Attribute Experts (MoEE). Our method adapts to different\nmodalities and scene dynamics, achieving consistent gains over state-of-the-art\nbaselines in event-only, frame-only, and event-frame fusion settings. We hope\nour dataset and approach will establish a foundation for advancing multimodal,\ntemporally-aware, and language-driven perception in real-world robotics and\nautonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Talk2Event\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u9a71\u52a8\u76ee\u6807\u5b9a\u4f4d\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc730,000\u4e2a\u9a8c\u8bc1\u7684\u6307\u79f0\u8868\u8fbe\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86EventRefer\u6846\u67b6\u6765\u5904\u7406\u591a\u5c5e\u6027\u878d\u5408\u7684\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u548c\u8fd0\u52a8\u6a21\u7cca\u9c81\u68d2\u6027\uff0c\u9002\u5408\u7406\u89e3\u52a8\u6001\u73af\u5883\uff0c\u4f46\u5c06\u8fd9\u4e9b\u5f02\u6b65\u6d41\u4e0e\u4eba\u7c7b\u8bed\u8a00\u8fde\u63a5\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u8bed\u8a00\u9a71\u52a8\u76ee\u6807\u5b9a\u4f4d\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86Talk2Event\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc730,000\u4e2a\u6765\u81ea\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u7684\u9a8c\u8bc1\u6307\u79f0\u8868\u8fbe\u5f0f\uff0c\u6bcf\u4e2a\u8868\u8fbe\u5f0f\u90fd\u5305\u542b\u56db\u4e2a\u5b9a\u4f4d\u5c5e\u6027\uff08\u5916\u89c2\u3001\u72b6\u6001\u3001\u4e0e\u89c2\u5bdf\u8005\u7684\u5173\u7cfb\u3001\u4e0e\u5176\u4ed6\u7269\u4f53\u7684\u5173\u7cfb\uff09\u3002\u63d0\u51faEventRefer\u6846\u67b6\uff0c\u4f7f\u7528\u4e8b\u4ef6-\u5c5e\u6027\u4e13\u5bb6\u6df7\u5408\u6a21\u578b(MoEE)\u52a8\u6001\u878d\u5408\u591a\u5c5e\u6027\u8868\u793a\u3002", "result": "EventRefer\u65b9\u6cd5\u5728\u4ec5\u4e8b\u4ef6\u3001\u4ec5\u5e27\u548c\u4e8b\u4ef6-\u5e27\u878d\u5408\u4e09\u79cd\u8bbe\u7f6e\u4e0b\u90fd\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u6a21\u6001\u548c\u573a\u666f\u52a8\u6001\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u3001\u65f6\u95f4\u611f\u77e5\u548c\u8bed\u8a00\u9a71\u52a8\u7684\u611f\u77e5\u6280\u672f\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.17343", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.17343", "abs": "https://arxiv.org/abs/2507.17343", "authors": ["Xiaohao Liu", "Xiaobo Xia", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Principled Multimodal Representation Learning", "comment": "32 pages, 9 figures, 10 tables", "summary": "Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PMRL\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8868\u793a\u77e9\u9635\u7684\u4e3b\u5947\u5f02\u503c\u6765\u5b9e\u73b0\u591a\u6a21\u6001\u540c\u65f6\u5bf9\u9f50\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u951a\u70b9\u6a21\u6001\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8esoftmax\u7684\u635f\u5931\u51fd\u6570\u548c\u5b9e\u4f8b\u7ea7\u5bf9\u6bd4\u6b63\u5219\u5316\u63d0\u9ad8\u4e86\u5bf9\u9f50\u7684\u7a33\u5b9a\u6027\u548c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6210\u5bf9\u5bf9\u6bd4\u5b66\u4e60\u548c\u9884\u5b9a\u4e49\u951a\u70b9\u6a21\u6001\uff0c\u9650\u5236\u4e86\u6240\u6709\u6a21\u6001\u95f4\u7684\u5bf9\u9f50\u6548\u679c\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u540c\u65f6\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u56fa\u5b9a\u951a\u70b9\u9650\u5236\u548c\u5947\u5f02\u503c\u4e58\u79ef\u4f18\u5316\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u65e0\u951a\u70b9\u4f9d\u8d56\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPMRL\uff08\u539f\u5219\u6027\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u5b8c\u5168\u5bf9\u9f50\u5bf9\u5e94rank-1\u683c\u62c9\u59c6\u77e9\u9635\u7684\u7406\u8bba\u6d1e\u5bdf\uff0c\u901a\u8fc7\u4f18\u5316\u8868\u793a\u77e9\u9635\u7684\u4e3b\u5947\u5f02\u503c\u6765\u6cbf\u5171\u4eab\u4e3b\u65b9\u5411\u5bf9\u9f50\u6a21\u6001\u3002\u4f7f\u7528\u57fa\u4e8esoftmax\u7684\u635f\u5931\u51fd\u6570\u5c06\u5947\u5f02\u503c\u4f5c\u4e3alogits\u6765\u4f18\u5148\u8003\u8651\u6700\u5927\u5947\u5f02\u503c\uff0c\u5e76\u91c7\u7528\u4e3b\u7279\u5f81\u5411\u91cf\u4e0a\u7684\u5b9e\u4f8b\u7ea7\u5bf9\u6bd4\u6b63\u5219\u5316\u6765\u7ef4\u6301\u5b9e\u4f8b\u95f4\u53ef\u5206\u6027\u5e76\u9632\u6b62\u8868\u793a\u5d29\u584c\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660ePMRL\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "PMRL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u5947\u5f02\u503c\u4f18\u5316\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u65e0\u951a\u70b9\u591a\u6a21\u6001\u540c\u65f6\u5bf9\u9f50\uff0c\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17373", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17373", "abs": "https://arxiv.org/abs/2507.17373", "authors": ["Keon-Hee Park", "Seun-An Choe", "Gyeong-Moon Park"], "title": "SFUOD: Source-Free Unknown Object Detection", "comment": "This paper has been accepted by ICCV 2025", "summary": "Source-free object detection adapts a detector pre-trained on a source domain\nto an unlabeled target domain without requiring access to labeled source data.\nWhile this setting is practical as it eliminates the need for the source\ndataset during domain adaptation, it operates under the restrictive assumption\nthat only pre-defined objects from the source domain exist in the target\ndomain. This closed-set setting prevents the detector from detecting undefined\nobjects. To ease this assumption, we propose Source-Free Unknown Object\nDetection (SFUOD), a novel scenario which enables the detector to not only\nrecognize known objects but also detect undefined objects as unknown objects.\nTo this end, we propose CollaPAUL (Collaborative tuning and Principal\nAxis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning\nenhances knowledge adaptation by integrating target-dependent knowledge from\nthe auxiliary encoder with source-dependent knowledge from the pre-trained\ndetector through a cross-domain attention mechanism. Additionally, principal\naxes-based unknown labeling assigns pseudo-labels to unknown objects by\nestimating objectness via principal axes projection and confidence scores from\nmodel predictions. The proposed CollaPAUL achieves state-of-the-art\nperformances on SFUOD benchmarks, and extensive experiments validate its\neffectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86Source-Free Unknown Object Detection (SFUOD)\u573a\u666f\uff0c\u901a\u8fc7CollaPAUL\u6846\u67b6\u5728\u65e0\u9700\u6e90\u57df\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u65e2\u80fd\u68c0\u6d4b\u5df2\u77e5\u76ee\u6807\u4e5f\u80fd\u53d1\u73b0\u672a\u5b9a\u4e49\u7684\u672a\u77e5\u76ee\u6807", "motivation": "\u4f20\u7edf\u7684\u65e0\u6e90\u57df\u76ee\u6807\u68c0\u6d4b\u5047\u8bbe\u76ee\u6807\u57df\u4e2d\u53ea\u5b58\u5728\u6e90\u57df\u9884\u5b9a\u4e49\u7684\u76ee\u6807\uff0c\u8fd9\u79cd\u5c01\u95ed\u96c6\u8bbe\u5b9a\u9650\u5236\u4e86\u68c0\u6d4b\u5668\u53d1\u73b0\u672a\u5b9a\u4e49\u76ee\u6807\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u653e\u5bbd\u8fd9\u4e00\u5047\u8bbe", "method": "\u63d0\u51faCollaPAUL\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u534f\u4f5c\u8c03\u4f18\u901a\u8fc7\u8de8\u57df\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u8f85\u52a9\u7f16\u7801\u5668\u7684\u76ee\u6807\u76f8\u5173\u77e5\u8bc6\u548c\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u7684\u6e90\u57df\u77e5\u8bc6\uff1b2\uff09\u57fa\u4e8e\u4e3b\u8f74\u7684\u672a\u77e5\u6807\u6ce8\u901a\u8fc7\u4e3b\u8f74\u6295\u5f71\u4f30\u8ba1\u76ee\u6807\u6027\u5e76\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e3a\u672a\u77e5\u76ee\u6807\u5206\u914d\u4f2a\u6807\u7b7e", "result": "\u5728SFUOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "CollaPAUL\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u6e90\u57df\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u672a\u77e5\u76ee\u6807\u53d1\u73b0\u95ee\u9898\uff0c\u901a\u8fc7\u534f\u4f5c\u8c03\u4f18\u548c\u4e3b\u8f74\u672a\u77e5\u6807\u6ce8\u5b9e\u73b0\u4e86\u5bf9\u5df2\u77e5\u548c\u672a\u77e5\u76ee\u6807\u7684\u6709\u6548\u68c0\u6d4b"}}
{"id": "2507.17436", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17436", "abs": "https://arxiv.org/abs/2507.17436", "authors": ["Yehao Lu", "Minghe Weng", "Zekang Xiao", "Rui Jiang", "Wei Su", "Guangcong Zheng", "Ping Lu", "Xi Li"], "title": "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection", "comment": "Accepted by ICCV 2025", "summary": "The Mixture of Experts (MoE) architecture has excelled in Large\nVision-Language Models (LVLMs), yet its potential in real-time open-vocabulary\nobject detectors, which also leverage large-scale vision-language datasets but\nsmaller models, remains unexplored. This work investigates this domain,\nrevealing intriguing insights. In the shallow layers, experts tend to cooperate\nwith diverse peers to expand the search space. While in the deeper layers,\nfixed collaborative structures emerge, where each expert maintains 2-3 fixed\npartners and distinct expert combinations are specialized in processing\nspecific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding\nDINO 1.5 Edge from a dense model to a dynamic inference framework via an\nefficient MoE-Tuning strategy. Additionally, we design a granularity\ndecomposition mechanism to decompose the Feed-Forward Network (FFN) of base\nmodel into multiple smaller expert networks, expanding the subnet search space.\nTo prevent performance degradation at the start of fine-tuning, we further\npropose a pre-trained weight allocation strategy for the experts, coupled with\na specific router initialization. During inference, only the input-relevant\nexperts are activated to form a compact subnet. Experiments show that,\npretrained with merely 1.56M open-source data, Dynamic-DINO outperforms\nGrounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDynamic-DINO\uff0c\u5c06Grounding DINO 1.5 Edge\u4ece\u5bc6\u96c6\u6a21\u578b\u6269\u5c55\u4e3a\u52a8\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7MoE\u67b6\u6784\u5728\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u4ec5\u7528156\u4e07\u5f00\u6e90\u6570\u636e\u5c31\u8d85\u8d8a\u4e86\u5728\u79c1\u67092000\u4e07\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u5728\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u8fd9\u4e9b\u68c0\u6d4b\u5668\u540c\u6837\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u96c6\u4f46\u4f7f\u7528\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7814\u7a76MoE\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faDynamic-DINO\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7684MoE-Tuning\u7b56\u7565\u5c06Grounding DINO 1.5 Edge\u6269\u5c55\u4e3a\u52a8\u6001\u63a8\u7406\u6a21\u578b\u3002\u8bbe\u8ba1\u7c92\u5ea6\u5206\u89e3\u673a\u5236\u5c06\u57fa\u7840\u6a21\u578b\u7684\u524d\u9988\u7f51\u7edc\u5206\u89e3\u4e3a\u591a\u4e2a\u5c0f\u578b\u4e13\u5bb6\u7f51\u7edc\uff0c\u6269\u5c55\u5b50\u7f51\u641c\u7d22\u7a7a\u95f4\u3002\u63d0\u51fa\u9884\u8bad\u7ec3\u6743\u91cd\u5206\u914d\u7b56\u7565\u548c\u7279\u5b9a\u7684\u8def\u7531\u5668\u521d\u59cb\u5316\u65b9\u6cd5\u9632\u6b62\u5fae\u8c03\u521d\u671f\u6027\u80fd\u4e0b\u964d\u3002\u63a8\u7406\u65f6\u53ea\u6fc0\u6d3b\u4e0e\u8f93\u5165\u76f8\u5173\u7684\u4e13\u5bb6\u5f62\u6210\u7d27\u51d1\u5b50\u7f51\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528156\u4e07\u5f00\u6e90\u6570\u636e\u9884\u8bad\u7ec3\u7684Dynamic-DINO\u8d85\u8d8a\u4e86\u5728\u79c1\u6709Grounding20M\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684Grounding DINO 1.5 Edge\u3002\u53d1\u73b0\u6d45\u5c42\u4e13\u5bb6\u503e\u5411\u4e8e\u4e0e\u591a\u6837\u5316\u4f19\u4f34\u5408\u4f5c\u6269\u5c55\u641c\u7d22\u7a7a\u95f4\uff0c\u6df1\u5c42\u5219\u5f62\u6210\u56fa\u5b9a\u534f\u4f5c\u7ed3\u6784\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u7ef4\u63012-3\u4e2a\u56fa\u5b9a\u4f19\u4f34\uff0c\u4e0d\u540c\u4e13\u5bb6\u7ec4\u5408\u4e13\u95e8\u5904\u7406\u7279\u5b9a\u6a21\u5f0f\u3002", "conclusion": "Dynamic-DINO\u6210\u529f\u8bc1\u660e\u4e86MoE\u67b6\u6784\u5728\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u548c\u9ad8\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4f7f\u7528\u66f4\u5c11\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.17511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17511", "abs": "https://arxiv.org/abs/2507.17511", "authors": ["Jiajun Luo", "Yicheng Xiao", "Jianru Xu", "Yangxiu You", "Rongwei Lu", "Chen Tang", "Jingyan Jiang", "Zhi Wang"], "title": "Accelerating Parallel Diffusion Model Serving with Residual Compression", "comment": null, "summary": "Diffusion models produce realistic images and videos but require substantial\ncomputational resources, necessitating multi-accelerator parallelism for\nreal-time deployment. However, parallel inference introduces significant\ncommunication overhead from exchanging large activations between devices,\nlimiting efficiency and scalability. We present CompactFusion, a compression\nframework that significantly reduces communication while preserving generation\nquality. Our key observation is that diffusion activations exhibit strong\ntemporal redundancy-adjacent steps produce highly similar activations,\nsaturating bandwidth with near-duplicate data carrying little new information.\nTo address this inefficiency, we seek a more compact representation that\nencodes only the essential information. CompactFusion achieves this via\nResidual Compression that transmits only compressed residuals (step-wise\nactivation differences). Based on empirical analysis and theoretical\njustification, we show that it effectively removes redundant data, enabling\nsubstantial data reduction while maintaining high fidelity. We also integrate\nlightweight error feedback to prevent error accumulation. CompactFusion\nestablishes a new paradigm for parallel diffusion inference, delivering lower\nlatency and significantly higher generation quality than prior methods. On\n4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also\nuniquely supports communication-heavy strategies like sequence parallelism on\nslow networks, achieving 6.7x speedup over prior overlap-based method.\nCompactFusion applies broadly across diffusion models and parallel settings,\nand integrates easily without requiring pipeline rework. Portable\nimplementation demonstrated on xDiT is publicly available at\nhttps://github.com/Cobalt-27/CompactFusion", "AI": {"tldr": "CompactFusion\u662f\u4e00\u4e2a\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u6fc0\u6d3b\u7684\u65f6\u95f4\u5197\u4f59\u6027\uff0c\u4f7f\u7528\u6b8b\u5dee\u538b\u7f29\u6280\u672f\u663e\u8457\u51cf\u5c11\u5e76\u884c\u63a8\u7406\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b03.0-6.7\u500d\u7684\u901f\u5ea6\u63d0\u5347", "motivation": "\u6269\u6563\u6a21\u578b\u9700\u8981\u591a\u52a0\u901f\u5668\u5e76\u884c\u63a8\u7406\u6765\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\uff0c\u4f46\u5e76\u884c\u63a8\u7406\u4e2d\u8bbe\u5907\u95f4\u4ea4\u6362\u5927\u91cf\u6fc0\u6d3b\u6570\u636e\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u901a\u4fe1\u7684\u540c\u65f6\u5f80\u5f80\u727a\u7272\u751f\u6210\u8d28\u91cf", "method": "\u63d0\u51faCompactFusion\u538b\u7f29\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u6b8b\u5dee\u538b\u7f29(Residual Compression)\u6280\u672f\uff1a1)\u89c2\u5bdf\u5230\u6269\u6563\u6fc0\u6d3b\u5177\u6709\u5f3a\u65f6\u95f4\u5197\u4f59\u6027-\u76f8\u90bb\u6b65\u9aa4\u4ea7\u751f\u9ad8\u5ea6\u76f8\u4f3c\u7684\u6fc0\u6d3b\uff1b2)\u4ec5\u4f20\u8f93\u538b\u7f29\u7684\u6b8b\u5dee(\u6b65\u9aa4\u95f4\u6fc0\u6d3b\u5dee\u5f02)\u800c\u975e\u5b8c\u6574\u6fc0\u6d3b\uff1b3)\u96c6\u6210\u8f7b\u91cf\u7ea7\u9519\u8bef\u53cd\u9988\u673a\u5236\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\uff1b4)\u57fa\u4e8e\u7ecf\u9a8c\u5206\u6790\u548c\u7406\u8bba\u8bc1\u660e\u6765\u79fb\u9664\u5197\u4f59\u6570\u636e", "result": "\u57284xL20\u4e0a\u5b9e\u73b03.0\u500d\u52a0\u901f\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4fdd\u771f\u5ea6\uff1b\u5728\u6162\u901f\u7f51\u7edc\u4e0a\u652f\u6301\u5e8f\u5217\u5e76\u884c\u7b49\u901a\u4fe1\u5bc6\u96c6\u7b56\u7565\uff0c\u76f8\u6bd4\u5148\u524d\u91cd\u53e0\u65b9\u6cd5\u5b9e\u73b06.7\u500d\u52a0\u901f\uff1b\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5404\u79cd\u6269\u6563\u6a21\u578b\u548c\u5e76\u884c\u8bbe\u7f6e\uff0c\u6613\u4e8e\u96c6\u6210\u4e14\u65e0\u9700\u91cd\u6784\u6d41\u6c34\u7ebf", "conclusion": "CompactFusion\u4e3a\u5e76\u884c\u6269\u6563\u63a8\u7406\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u65f6\u95f4\u5197\u4f59\u6027\u5b9e\u73b0\u4e86\u901a\u4fe1\u538b\u7f29\u4e0e\u751f\u6210\u8d28\u91cf\u7684\u5e73\u8861\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u8fd8\u652f\u6301\u66f4\u7075\u6d3b\u7684\u5e76\u884c\u7b56\u7565\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17533", "abs": "https://arxiv.org/abs/2507.17533", "authors": ["Liwen Liu", "Weidong Yang", "Lipeng Ma", "Ben Fei"], "title": "Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding", "comment": null, "summary": "Recent advances in multi-modal pre-training methods have shown promising\neffectiveness in learning 3D representations by aligning multi-modal features\nbetween 3D shapes and their corresponding 2D counterparts. However, existing\nmulti-modal pre-training frameworks primarily rely on a single pre-training\ntask to gather multi-modal data in 3D applications. This limitation prevents\nthe models from obtaining the abundant information provided by other relevant\ntasks, which can hinder their performance in downstream tasks, particularly in\ncomplex and diverse domains. In order to tackle this issue, we propose MMPT, a\nMulti-modal Multi-task Pre-training framework designed to enhance point cloud\nunderstanding. Specifically, three pre-training tasks are devised: (i)\nToken-level reconstruction (TLR) aims to recover masked point tokens, endowing\nthe model with representative learning abilities. (ii) Point-level\nreconstruction (PLR) is integrated to predict the masked point positions\ndirectly, and the reconstructed point cloud can be considered as a transformed\npoint cloud used in the subsequent task. (iii) Multi-modal contrastive learning\n(MCL) combines feature correspondences within and across modalities, thus\nassembling a rich learning signal from both 3D point cloud and 2D image\nmodalities in a self-supervised manner. Moreover, this framework operates\nwithout requiring any 3D annotations, making it scalable for use with large\ndatasets. The trained encoder can be effectively transferred to various\ndownstream tasks. To demonstrate its effectiveness, we evaluated its\nperformance compared to state-of-the-art methods in various discriminant and\ngenerative applications under widely-used benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMMPT\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1\uff08\u6807\u8bb0\u7ea7\u91cd\u5efa\u3001\u70b9\u7ea7\u91cd\u5efa\u3001\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\uff09\u6765\u589e\u5f3a\u70b9\u4e91\u7406\u89e3\uff0c\u65e0\u97003D\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u9884\u8bad\u7ec3\u4efb\u52a1\u6765\u6536\u96c63D\u5e94\u7528\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u8fd9\u79cd\u9650\u5236\u963b\u6b62\u4e86\u6a21\u578b\u83b7\u53d6\u5176\u4ed6\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u591a\u6837\u7684\u9886\u57df\u4e2d\u4f1a\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd", "method": "\u63d0\u51faMMPT\u591a\u6a21\u6001\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1\uff1a(1)\u6807\u8bb0\u7ea7\u91cd\u5efa(TLR)\u6062\u590d\u88ab\u906e\u6321\u7684\u70b9\u6807\u8bb0\uff1b(2)\u70b9\u7ea7\u91cd\u5efa(PLR)\u76f4\u63a5\u9884\u6d4b\u88ab\u906e\u6321\u70b9\u7684\u4f4d\u7f6e\uff1b(3)\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60(MCL)\u7ed3\u5408\u6a21\u6001\u5185\u548c\u8de8\u6a21\u6001\u7684\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u4ece3D\u70b9\u4e91\u548c2D\u56fe\u50cf\u4e2d\u83b7\u53d6\u4e30\u5bcc\u5b66\u4e60\u4fe1\u53f7", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u5224\u522b\u548c\u751f\u6210\u5e94\u7528\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u53ef\u4ee5\u6709\u6548\u8fc1\u79fb\u5230\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1", "conclusion": "MMPT\u6846\u67b6\u901a\u8fc7\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u548c\u591a\u6a21\u6001\u5b66\u4e60\u6709\u6548\u589e\u5f3a\u4e86\u70b9\u4e91\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u97003D\u6807\u6ce8\u4f7f\u5176\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd"}}
{"id": "2507.17659", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17659", "abs": "https://arxiv.org/abs/2507.17659", "authors": ["Junjie Wang", "Yunhan Tang", "Yijie Wang", "Zhihao Yuan", "Huan Wang", "Yangfan He", "Bin Li"], "title": "See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86Synergos-VQA\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u6574\u4f53\u8bc1\u636e\u3001\u7ed3\u6784\u8bc1\u636e\u548c\u56e0\u679c\u8bc1\u636e\u4e09\u79cd\u4e92\u8865\u8bc1\u636e\u6d41\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u578b\u89c6\u89c9\u95ee\u7b54\u4e2d\u4f9d\u8d56\u5355\u4e00\u7ef4\u5ea6\u8bc1\u636e\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u578b\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b58\u5728\u6839\u672c\u6027\u74f6\u9888\uff0c\u5373\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4e00\u7ef4\u5ea6\u7684\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\uff0c\u91c7\u7528\"\u53ea\u89c1\u6811\u6728\u4e0d\u89c1\u68ee\u6797\"\u7684\u65b9\u6cd5\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7a33\u5065\u7684\u591a\u9762\u7406\u89e3\u3002", "method": "\u63d0\u51faSynergos-VQA\u534f\u540c\u63a8\u7406\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u5e76\u53d1\u751f\u6210\u5e76\u878d\u5408\u4e09\u79cd\u4e92\u8865\u8bc1\u636e\u6d41\uff1a(1)\u6574\u4f53\u8bc1\u636e-\u611f\u77e5\u6574\u4e2a\u573a\u666f(\"\u68ee\u6797\")\uff1b(2)\u7ed3\u6784\u8bc1\u636e-\u901a\u8fc7\u539f\u578b\u9a71\u52a8\u6a21\u5757\u8bc6\u522b\u5173\u952e\u5bf9\u8c61(\"\u6811\u6728\")\uff1b(3)\u56e0\u679c\u8bc1\u636e-\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a2\u6d4b\u786e\u4fdd\u63a8\u7406\u7a33\u5065\u63a5\u5730\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5(\u5305\u62ecOK-VQA\u548cA-OKVQA)\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u8bb0\u5f55\u3002\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5373\u63d2\u5373\u7528\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cd\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4f18\u79c0\u7684\u65b9\u6cd5\u8bbe\u8ba1\u53ef\u4ee5\u8d85\u8d8a\u7eaf\u7cb9\u7684\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u3002", "conclusion": "\u901a\u8fc7\u534f\u540c\u878d\u5408\u591a\u9762\u8bc1\u636e\uff0cSynergos-VQA\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u53ef\u9760\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u77e5\u8bc6\u578b\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u7a81\u7834\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u8bba\u521b\u65b0\u7684\u91cd\u8981\u6027\u8d85\u8d8a\u5355\u7eaf\u7684\u6a21\u578b\u89c4\u6a21\u63d0\u5347\u3002"}}
