{"id": "2512.20631", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20631", "abs": "https://arxiv.org/abs/2512.20631", "authors": ["Aayam Bansal", "Ishaan Gangwani"], "title": "Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams", "comment": "ICML NewInML", "summary": "We present a comprehensive zero-training temporal drift analysis of transformer-based sentiment models validated on authentic social media data from major real-world events. Through systematic evaluation across three transformer architectures and rigorous statistical validation on 12,279 authentic social media posts, we demonstrate significant model instability with accuracy drops reaching 23.4% during event-driven periods. Our analysis reveals maximum confidence drops of 13.0% (Bootstrap 95% CI: [9.1%, 16.5%]) with strong correlation to actual performance degradation. We introduce four novel drift metrics that outperform embedding-based baselines while maintaining computational efficiency suitable for production deployment. Statistical validation across multiple events confirms robust detection capabilities with practical significance exceeding industry monitoring thresholds. This zero-training methodology enables immediate deployment for real-time sentiment monitoring systems and provides new insights into transformer model behavior during dynamic content periods.", "AI": {"tldr": "\u65e0\u9700\u8bad\u7ec3\u7684\u65f6\u5e8f\u6f02\u79fb\u5206\u6790\u663e\u793a\uff0c\u57fa\u4e8eTransformer\u7684\u60c5\u611f\u6a21\u578b\u5728\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u4e8b\u4ef6\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u7a33\u5b9a\u6027\uff0c\u51c6\u786e\u7387\u6700\u5927\u4e0b\u964d23.4%\uff0c\u540c\u65f6\u63d0\u51fa\u56db\u79cd\u4f18\u4e8e\u57fa\u7ebf\u7684\u65b0\u6f02\u79fb\u6307\u6807\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5206\u6790Transformer\u60c5\u611f\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u4e2d\u7684\u65f6\u5e8f\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u52a8\u6001\u53d8\u5316\u65f6\u671f\uff0c\u73b0\u6709\u76d1\u63a7\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u9000\u5316\u3002", "method": "\u91c7\u7528\u96f6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5bf9\u4e09\u79cdTransformer\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u4f7f\u752812,279\u4e2a\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u8fdb\u884c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u63d0\u51fa\u56db\u79cd\u65b0\u7684\u6f02\u79fb\u6307\u6807\uff0c\u5e76\u4e0e\u57fa\u4e8e\u5d4c\u5165\u7684\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u4e8b\u4ef6\u9a71\u52a8\u671f\u95f4\u663e\u8457\u4e0d\u7a33\u5b9a\uff0c\u51c6\u786e\u7387\u6700\u5927\u4e0b\u964d23.4%\uff0c\u7f6e\u4fe1\u5ea6\u6700\u5927\u4e0b\u964d13.0%\uff0c\u65b0\u6f02\u79fb\u6307\u6807\u4f18\u4e8e\u57fa\u7ebf\u4e14\u8ba1\u7b97\u6548\u7387\u9002\u5408\u751f\u4ea7\u90e8\u7f72\uff0c\u7edf\u8ba1\u9a8c\u8bc1\u8bc1\u5b9e\u68c0\u6d4b\u80fd\u529b\u8d85\u8fc7\u884c\u4e1a\u76d1\u63a7\u9608\u503c\u3002", "conclusion": "\u96f6\u8bad\u7ec3\u65b9\u6cd5\u53ef\u76f4\u63a5\u90e8\u7f72\u4e8e\u5b9e\u65f6\u60c5\u611f\u76d1\u63a7\u7cfb\u7edf\uff0c\u4e3aTransformer\u6a21\u578b\u5728\u52a8\u6001\u5185\u5bb9\u671f\u95f4\u7684\u884c\u4e3a\u63d0\u4f9b\u65b0\u89c1\u89e3\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.21043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21043", "abs": "https://arxiv.org/abs/2512.21043", "authors": ["Cheng-Yu Kuo", "Hirofumi Shin", "Takamitsu Matsubara"], "title": "Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction", "comment": "8 pages. Accepted by IEEE Robotics and Automation Letters (RA-L)", "summary": "Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u80fd\u91cf\u62bd\u8c61\u7684\u65b9\u6cd5\uff0c\u8ba9\u673a\u5668\u4eba\u624b\u901a\u8fc7\u89e6\u89c9\u5feb\u901f\u5b66\u4e60\u6293\u63e1\u529b\u63a7\u5236\uff0c\u65e0\u9700\u5916\u90e8\u4f20\u611f\u6216\u7269\u4f53\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u51e0\u5206\u949f\u5185\u5c31\u80fd\u5b66\u4f1a\u51cf\u5c11\u6ed1\u52a8", "motivation": "\u5728\u52a8\u6001\u7269\u4f53\u4ea4\u4e92\u4e2d\u8c03\u8282\u6293\u63e1\u529b\u4ee5\u51cf\u5c11\u6ed1\u52a8\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u7269\u4f53\u88ab\u591a\u4e2a\u6eda\u52a8\u63a5\u89e6\u64cd\u7eb5\u3001\u5177\u6709\u672a\u77e5\u5c5e\u6027\uff08\u5982\u8d28\u91cf\u6216\u8868\u9762\u6761\u4ef6\uff09\u4e14\u5916\u90e8\u4f20\u611f\u4e0d\u53ef\u9760\u65f6\u3002\u800c\u4eba\u7c7b\u5373\u4f7f\u6ca1\u6709\u89c6\u89c9\u7ebf\u7d22\u4e5f\u80fd\u901a\u8fc7\u89e6\u89c9\u5feb\u901f\u8c03\u8282\u6293\u63e1\u529b\uff0c\u53d7\u6b64\u542f\u53d1\uff0c\u5e0c\u671b\u8ba9\u673a\u5668\u4eba\u624b\u4e5f\u80fd\u5feb\u901f\u63a2\u7d22\u7269\u4f53\u5e76\u5b66\u4e60\u89e6\u89c9\u9a71\u52a8\u7684\u6293\u63e1\u529b\u63a7\u5236", "method": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u80fd\u91cf\u62bd\u8c61\uff0c\u5c06\u7269\u4f53\u5efa\u6a21\u4e3a\u865a\u62df\u80fd\u91cf\u5bb9\u5668\u3002\u624b\u6307\u65bd\u52a0\u7684\u529f\u7387\u4e0e\u7269\u4f53\u4fdd\u7559\u80fd\u91cf\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u63a8\u65ad\u6ed1\u52a8\u611f\u77e5\u7a33\u5b9a\u6027\u7684\u7269\u7406\u57fa\u7840\u4fe1\u53f7\u3002\u57fa\u4e8e\u6b64\u62bd\u8c61\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u548c\u89c4\u5212\uff0c\u4ece\u89e6\u89c9\u4f20\u611f\u9ad8\u6548\u5efa\u6a21\u80fd\u91cf\u52a8\u529b\u5b66\uff0c\u5e76\u6267\u884c\u5b9e\u65f6\u6293\u63e1\u529b\u4f18\u5316", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u51e0\u5206\u949f\u5185\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u6293\u63e1\u529b\u63a7\u5236\uff0c\u6709\u6548\u51cf\u5c11\u6ed1\u52a8\uff0c\u5e76\u5728\u5404\u79cd\u8fd0\u52a8-\u7269\u4f53\u5bf9\u4e2d\u5ef6\u957f\u6293\u63e1\u6301\u7eed\u65f6\u95f4\uff0c\u4e14\u4e0d\u4f9d\u8d56\u5916\u90e8\u4f20\u611f\u6216\u7269\u4f53\u5148\u9a8c\u77e5\u8bc6", "conclusion": "\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u80fd\u91cf\u62bd\u8c61\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u624b\u5feb\u901f\u5b66\u4e60\u89e6\u89c9\u9a71\u52a8\u7684\u6293\u63e1\u529b\u63a7\u5236\uff0c\u4e3a\u89e3\u51b3\u52a8\u6001\u7269\u4f53\u4ea4\u4e92\u4e2d\u7684\u6ed1\u52a8\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5"}}
{"id": "2512.20761", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20761", "abs": "https://arxiv.org/abs/2512.20761", "authors": ["Marcel Meyer", "Sascha Kaltenpoth", "Kevin Zalipski", "Henrik Albers", "Oliver M\u00fcller"], "title": "TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform", "comment": null, "summary": "While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.", "AI": {"tldr": "TS-Arena\u5e73\u53f0\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u5371\u673a\uff0c\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u6d41\u9884\u6ce8\u518c\u673a\u5236\u786e\u4fdd\u6d4b\u8bd5\u6570\u636e\u5728\u63a8\u7406\u65f6\u4e0d\u5b58\u5728\uff0c\u9632\u6b62\u5386\u53f2\u6570\u636e\u6c61\u67d3", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5b58\u5728\u8bc4\u4f30\u5371\u673a\uff0c\u4e3b\u8981\u6e90\u4e8e\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u91cd\u53e0\u5bfc\u81f4\u7684\u4fe1\u606f\u6cc4\u9732\uff0c\u4ee5\u53ca\u5168\u5c40\u6a21\u5f0f\u5411\u6d4b\u8bd5\u6570\u636e\u7684\u975e\u6cd5\u8f6c\u79fb\uff0c\u8fdd\u53cd\u4e86\u6709\u6548\u57fa\u51c6\u6d4b\u8bd5\u6240\u9700\u7684\u72ec\u7acb\u6027\u8981\u6c42", "method": "\u5f15\u5165TS-Arena\u5e73\u53f0\uff0c\u91c7\u7528\u5b9e\u65f6\u6570\u636e\u6d41\u9884\u6ce8\u518c\u673a\u5236\uff0c\u5c06\u771f\u6b63\u672a\u77e5\u7684\u672a\u6765\u4f5c\u4e3a\u6d4b\u8bd5\u73af\u5883\uff0c\u5b9e\u65bd\u4e25\u683c\u7684\u5168\u5c40\u65f6\u95f4\u5206\u5272\uff0c\u9632\u6b62\u5386\u53f2\u6570\u636e\u6c61\u67d3", "result": "\u5728\u80fd\u6e90\u9886\u57df\u521d\u6b65\u5e94\u7528\uff0c\u5efa\u7acb\u4e86\u53ef\u6301\u7eed\u7684\u57fa\u7840\u6a21\u578b\u6bd4\u8f83\u57fa\u7840\u8bbe\u65bd\uff0c\u786e\u4fdd\u8bc4\u4f30\u76ee\u6807\u5728\u63a8\u7406\u65f6\u7269\u7406\u4e0a\u4e0d\u5b58\u5728\uff0c\u63d0\u4f9b\u771f\u5b9e\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30", "conclusion": "TS-Arena\u901a\u8fc7\u79fb\u52a8\u65f6\u95f4\u8fb9\u754c\u673a\u5236\u6062\u590d\u4e86\u9884\u6d4b\u7684\u64cd\u4f5c\u5b8c\u6574\u6027\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u6709\u6548\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2512.20985", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20985", "abs": "https://arxiv.org/abs/2512.20985", "authors": ["Salman Jan", "Hassan Ali Razzaqi", "Ali Akarma", "Mohammad Riyaz Belgaum"], "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines", "comment": "This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain", "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408LangChain\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u8bb8\u53ef\u533a\u5757\u94fe\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u4fdd\u969c\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u76d1\u63a7\u3001\u7b56\u7565\u6267\u884c\u548c\u4e0d\u53ef\u7be1\u6539\u5ba1\u8ba1", "motivation": "\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u533b\u7597\u3001\u667a\u6167\u57ce\u5e02\u3001\u6570\u5b57\u53d6\u8bc1\u548c\u4f9b\u5e94\u94fe\u7ba1\u7406\u7b49\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u5b58\u5728\u4fe1\u4efb\u3001\u76d1\u7763\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u786e\u4fdd\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u5ba1\u8ba1\u6027\u548c\u5b89\u5168\u6027", "method": "\u8bbe\u8ba1\u5355\u4e00\u67b6\u6784\u6a21\u578b\uff0c\u5c06LangChain\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u8bb8\u53ef\u533a\u5757\u94fe\u7ed3\u5408\uff0c\u5c06\u611f\u77e5-\u6982\u5ff5\u5316-\u884c\u52a8\u5468\u671f\u4e0e\u533a\u5757\u94fe\u6cbb\u7406\u5c42\u5173\u8054\uff0c\u9a8c\u8bc1\u8f93\u5165\u3001\u8bc4\u4f30\u5efa\u8bae\u884c\u52a8\u5e76\u8bb0\u5f55\u6267\u884c\u7ed3\u679c\u3002\u5177\u4f53\u5b9e\u73b0\u5305\u62ecHyperledger Fabric\u7cfb\u7edf\u3001MCP\u96c6\u6210\u884c\u52a8\u6267\u884c\u5668\u548cLangChain\u667a\u80fd\u4f53", "result": "\u5728\u667a\u80fd\u5e93\u5b58\u7ba1\u7406\u3001\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u548c\u533b\u7597\u76d1\u63a7\u7b49\u5b9e\u9a8c\u4e2d\uff0c\u533a\u5757\u94fe\u5b89\u5168\u9a8c\u8bc1\u80fd\u6709\u6548\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u64cd\u4f5c\uff0c\u63d0\u4f9b\u5168\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u8ffd\u6eaf\u6027\uff0c\u5e76\u5c06\u64cd\u4f5c\u5ef6\u8fdf\u4fdd\u6301\u5728\u5408\u7406\u8303\u56f4\u5185", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u65bd\u9ad8\u5f71\u54cd\u529b\u7684\u81ea\u4e3bAI\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7cfb\u7edf\uff0c\u65e2\u80fd\u4fdd\u6301\u81ea\u4e3b\u6027\u53c8\u80fd\u786e\u4fdd\u8d23\u4efb\u6027"}}
{"id": "2512.21329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21329", "abs": "https://arxiv.org/abs/2512.21329", "authors": ["Xinhe Wang", "Jin Huang", "Xingjian Zhang", "Tianhao Wang", "Jiaqi W. Ma"], "title": "Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks", "comment": null, "summary": "Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.\n  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.", "AI": {"tldr": "ARC\u7c7b\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\u5dee\u8ddd\u4e3b\u8981\u6e90\u4e8e\u89c6\u89c9\u611f\u77e5\u7f3a\u9677\u800c\u975e\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u901a\u8fc7\u5206\u79bb\u611f\u77e5\u4e0e\u63a8\u7406\u7684\u4e24\u9636\u6bb5\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u5bf9ARC\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u5dee\u8ddd\u7684\u4e3b\u6d41\u89e3\u91ca\uff08\u5f52\u56e0\u4e8e\u673a\u5668\u63a8\u7406\u7f3a\u9677\uff09\uff0c\u63d0\u51fa\u5047\u8bbe\uff1a\u6027\u80fd\u5dee\u8ddd\u4e3b\u8981\u6e90\u4e8e\u89c6\u89c9\u611f\u77e5\u9650\u5236\u800c\u975e\u5f52\u7eb3\u63a8\u7406\u7f3a\u9677\u3002", "method": "\u5f15\u5165\u4e24\u9636\u6bb5\u5b9e\u9a8c\u6d41\u7a0b\uff1a1) \u611f\u77e5\u9636\u6bb5\uff1a\u5c06\u6bcf\u4e2a\u56fe\u50cf\u72ec\u7acb\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b2) \u63a8\u7406\u9636\u6bb5\uff1a\u6a21\u578b\u4f7f\u7528\u8fd9\u4e9b\u63cf\u8ff0\u8fdb\u884c\u5f52\u7eb3\u548c\u5e94\u7528\u89c4\u5219\u3002\u5728Mini-ARC\u3001ACRE\u548cBongard-LOGO\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e24\u9636\u6bb5\u6d41\u7a0b\u4e0e\u6807\u51c6\u7aef\u5230\u7aef\u8bc4\u4f30\u3002", "result": "\u5728\u4e09\u4e2aARC\u98ce\u683c\u6570\u636e\u96c6\u4e0a\uff0c\u611f\u77e5\u80fd\u529b\u662f\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u5dee\u8ddd\u7684\u4e3b\u5bfc\u56e0\u7d20\uff1b\u5bf9VLM\u8f93\u51fa\u7684\u63a8\u7406\u8f68\u8ff9\u624b\u52a8\u68c0\u67e5\u663e\u793a\u7ea680%\u7684\u6a21\u578b\u5931\u8d25\u6e90\u4e8e\u611f\u77e5\u9519\u8bef\u3002", "conclusion": "ARC\u98ce\u683c\u57fa\u51c6\u6d4b\u8bd5\u6df7\u6dc6\u4e86\u611f\u77e5\u548c\u63a8\u7406\u6311\u6218\uff0c\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u5dee\u8ddd\u53ef\u80fd\u5938\u5927\u4e86\u673a\u5668\u63a8\u7406\u7684\u7f3a\u9677\u3002\u8bc4\u4f30\u673a\u5668\u667a\u80fd\u8fdb\u5c55\u65f6\u9700\u8981\u5206\u79bb\u611f\u77e5\u548c\u63a8\u7406\u7684\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2512.21053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21053", "abs": "https://arxiv.org/abs/2512.21053", "authors": ["Zibin Liu", "Banglei Guan", "Yang Shang", "Shunkun Liang", "Zhenbao Yu", "Qifeng Yu"], "title": "Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera", "comment": "9 pages, 5 figures. In Proceedings of the 32nd ACM International Conference on Multimedia (MM '24)", "summary": "Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5149\u6d41\u5f15\u5bfc6DoF\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc72D-3D\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u548c\u5149\u6d41\u5173\u8054\u5b9e\u73b0\u7cbe\u786e\u8ddf\u8e2a", "motivation": "\u4f20\u7edf\u76f8\u673a\u5728\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u4e2d\u9762\u4e34\u8fd0\u52a8\u6a21\u7cca\u3001\u4f20\u611f\u5668\u566a\u58f0\u3001\u90e8\u5206\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u7684\u4f18\u52bf\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u91c7\u75282D-3D\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u68c0\u6d4b\u4e8b\u4ef6\u548c\u7269\u4f53\u6a21\u578b\u7684\u89d2\u70b9\u548c\u8fb9\u7f18\uff1b\u901a\u8fc7\u6700\u5927\u5316\u65f6\u7a7a\u7a97\u53e3\u5185\u4e8b\u4ef6\u5173\u8054\u6982\u7387\u641c\u7d22\u89d2\u70b9\u5149\u6d41\uff1b\u5efa\u7acb\u5149\u6d41\u5f15\u5bfc\u7684\u89d2\u70b9-\u8fb9\u7f18\u5173\u8054\uff1b\u901a\u8fc7\u6700\u5c0f\u5316\u89d2\u70b9\u4e0e\u8fb9\u7f18\u8ddd\u79bb\u8fed\u4ee3\u4f18\u53166DoF\u59ff\u6001", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u5149\u6d41\u5f15\u5bfc6DoF\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u4f18\u52bf\uff0c\u5728\u590d\u6742\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u9c81\u68d2\u7684\u59ff\u6001\u8ddf\u8e2a"}}
{"id": "2512.21118", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21118", "abs": "https://arxiv.org/abs/2512.21118", "authors": ["Shi Quan Foo", "Chi-Ho Wong", "Zhihan Gao", "Dit-Yan Yeung", "Ka-Hing Wong", "Wai-Kin Wong"], "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting", "comment": "Accepted by TMLR. Camera-ready submission", "summary": "Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.", "AI": {"tldr": "STLDM\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\u7ed3\u5408\u786e\u5b9a\u6027\u9884\u6d4b\u548c\u751f\u6210\u589e\u5f3a\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u5bf9\u9884\u9632\u6781\u7aef\u5929\u6c14\u707e\u5bb3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff1a\u786e\u5b9a\u6027\u6a21\u578b\u9884\u6d4b\u6a21\u7cca\uff0c\u751f\u6210\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u51c6\u786e\u6027\u53c8\u80fd\u751f\u6210\u6e05\u6670\u9884\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTLDM\u6a21\u578b\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u8868\u793a\u3002\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u786e\u5b9a\u6027\u9884\u62a5\u9636\u6bb5\uff08\u7531\u6761\u4ef6\u7f51\u7edc\u5904\u7406\uff09\uff1b2) \u589e\u5f3a\u9636\u6bb5\uff08\u7531\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6267\u884c\uff09\u3002", "result": "\u5728\u591a\u4e2a\u96f7\u8fbe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTLDM\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "STLDM\u901a\u8fc7\u4e24\u9636\u6bb5\u5206\u89e3\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u786e\u5b9a\u6027\u6a21\u578b\u6a21\u7cca\u548c\u751f\u6210\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u53cc\u91cd\u63d0\u5347\u3002"}}
{"id": "2512.21221", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21221", "abs": "https://arxiv.org/abs/2512.21221", "authors": ["Dao Sy Duy Minh", "Huynh Trung Kiet", "Nguyen Lam Phu Quy", "Phu-Hoa Pham", "Tran Chi Nguyen"], "title": "Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval", "comment": "System description paper for EVENTA Grand Challenge Track 2 at ACM Multimedia 2025 (MM '25). Ranked 4th place. 6 pages, 1 figure, 2 tables", "summary": "Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u4e24\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff0c\u7ed3\u5408\u4e8b\u4ef6\u5b9e\u4f53\u63d0\u53d6\u4e0e\u591a\u6a21\u6001\u8bed\u4e49\u5efa\u6a21\uff0c\u5728OpenEvents v1\u57fa\u51c6\u4e0a\u53d6\u5f970.559 mAP\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u9762\u4e34\u6a21\u7cca\u67e5\u8be2\u3001\u8bed\u5883\u4f9d\u8d56\u3001\u8bed\u8a00\u53d8\u5f02\u6027\u548c\u53ef\u6269\u5c55\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u590d\u6742\u771f\u5b9e\u573a\u666f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u4e8b\u4ef6\u4e2d\u5fc3\u5b9e\u4f53\u63d0\u53d6\uff0c\u4f7f\u7528BM25\u8fdb\u884c\u9ad8\u6548\u5019\u9009\u8fc7\u6ee4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528BEiT-3\u6a21\u578b\u6355\u6349\u6df1\u5ea6\u591a\u6a21\u6001\u8bed\u4e49\u5e76\u8fdb\u884c\u91cd\u6392\u5e8f\u3002", "result": "\u5728OpenEvents v1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.559\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff0c\u5927\u5e45\u8d85\u8d8a\u5148\u524d\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4e8b\u4ef6\u5f15\u5bfc\u8fc7\u6ee4\u4e0e\u957f\u6587\u672c\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e8b\u4ef6\u5f15\u5bfc\u8fc7\u6ee4\u4e0e\u6df1\u5ea6\u591a\u6a21\u6001\u8bed\u4e49\u5efa\u6a21\u7684\u7ed3\u5408\u4e3a\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u51c6\u786e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4e8b\u4ef6\u4e2d\u5fc3\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.21165", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21165", "abs": "https://arxiv.org/abs/2512.21165", "authors": ["Qizhi Wang"], "title": "BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft", "comment": "15 pages, 22 tables, 11 figures", "summary": "Randomized election timeouts are a simple and effective liveness heuristic for Raft, but they become brittle under long-tail latency, jitter, and partition recovery, where repeated split votes can inflate unavailability. This paper presents BALLAST, a lightweight online adaptation mechanism that replaces static timeout heuristics with contextual bandits. BALLAST selects from a discrete set of timeout \"arms\" using efficient linear contextual bandits (LinUCB variants), and augments learning with safe exploration to cap risk during unstable periods. We evaluate BALLAST on a reproducible discrete-event simulation with long-tail delay, loss, correlated bursts, node heterogeneity, and partition/recovery turbulence. Across challenging WAN regimes, BALLAST substantially reduces recovery time and unwritable time compared to standard randomized timeouts and common heuristics, while remaining competitive on stable LAN/WAN settings.", "AI": {"tldr": "BALLAST\u4f7f\u7528\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u66ff\u4ee3Raft\u4e2d\u9759\u6001\u7684\u968f\u673a\u9009\u4e3e\u8d85\u65f6\u673a\u5236\uff0c\u901a\u8fc7\u5b89\u5168\u63a2\u7d22\u5728\u7ebf\u9002\u5e94\u7f51\u7edc\u5ef6\u8fdf\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u957f\u5c3e\u5ef6\u8fdf\u3001\u6296\u52a8\u548c\u5206\u533a\u6062\u590d\u7b49\u6311\u6218\u6027\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u53ef\u7528\u6027\u3002", "motivation": "Raft\u534f\u8bae\u4e2d\u7684\u968f\u673a\u9009\u4e3e\u8d85\u65f6\u673a\u5236\u5728\u957f\u5c3e\u5ef6\u8fdf\u3001\u7f51\u7edc\u6296\u52a8\u548c\u5206\u533a\u6062\u590d\u7b49\u60c5\u51b5\u4e0b\u53d8\u5f97\u8106\u5f31\uff0c\u91cd\u590d\u7684\u5206\u7968\u4f1a\u5bfc\u81f4\u53ef\u7528\u6027\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u9002\u5e94\u673a\u5236\u6765\u5e94\u5bf9\u52a8\u6001\u7f51\u7edc\u73af\u5883\u3002", "method": "BALLAST\u91c7\u7528\u7ebf\u6027\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\uff08LinUCB\u53d8\u4f53\uff09\uff0c\u4ece\u79bb\u6563\u7684\u8d85\u65f6\u9009\u9879\u4e2d\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u5b89\u5168\u63a2\u7d22\u673a\u5236\u4ee5\u5728\u4e0d\u7a33\u5b9a\u671f\u95f4\u9650\u5236\u98ce\u9669\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u3002", "result": "\u5728\u5305\u542b\u957f\u5c3e\u5ef6\u8fdf\u3001\u4e22\u5305\u3001\u76f8\u5173\u7a81\u53d1\u3001\u8282\u70b9\u5f02\u6784\u6027\u548c\u5206\u533a/\u6062\u590d\u52a8\u8361\u7684\u53ef\u91cd\u590d\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u4e2d\uff0cBALLAST\u76f8\u6bd4\u6807\u51c6\u968f\u673a\u8d85\u65f6\u548c\u5e38\u89c1\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u6311\u6218\u6027WAN\u73af\u5883\u4e0b\u663e\u8457\u51cf\u5c11\u4e86\u6062\u590d\u65f6\u95f4\u548c\u4e0d\u53ef\u5199\u65f6\u95f4\u3002", "conclusion": "BALLAST\u901a\u8fc7\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u5b9e\u73b0\u4e86Raft\u9009\u4e3e\u8d85\u65f6\u7684\u667a\u80fd\u81ea\u9002\u5e94\uff0c\u5728\u4fdd\u6301\u7a33\u5b9aLAN/WAN\u73af\u5883\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u6311\u6218\u6027\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u53ef\u7528\u6027\u548c\u6062\u590d\u6027\u80fd\u3002"}}
{"id": "2512.21284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21284", "abs": "https://arxiv.org/abs/2512.21284", "authors": ["Shihao Zou", "Jingjing Li", "Wei Ji", "Jincai Huang", "Kai Wang", "Guo Dan", "Weixin Si", "Yi Pan"], "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential", "comment": null, "summary": "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \\textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\\times$. Notably, it delivers over $20\\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.", "AI": {"tldr": "\u63d0\u51faSpikeSurgSeg\uff0c\u9996\u4e2a\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u89c6\u9891Transformer\u6846\u67b6\uff0c\u7528\u4e8e\u624b\u672f\u573a\u666f\u5206\u5272\uff0c\u5728\u975eGPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u624b\u672f\u7cfb\u7edf\u4f9d\u8d56\u667a\u80fd\u573a\u666f\u7406\u89e3\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u5927\u6a21\u578b\uff09\u8ba1\u7b97\u9700\u6c42\u5927\u3001\u529f\u8017\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u624b\u672f\u73af\u5883\u4e2d\u5b9e\u65f6\u90e8\u7f72\u3002\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4f5c\u4e3a\u9ad8\u6548\u8ba1\u7b97\u8303\u5f0f\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u624b\u672f\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u624b\u672f\u89c6\u9891\u8868\u793a\u7a00\u758f\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faSpikeSurgSeg\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u624b\u672f\u573a\u666f\u63a9\u7801\u81ea\u7f16\u7801\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u5c42\u7ba1\u72b6\u63a9\u7801\u5b9e\u73b0\u9c81\u68d2\u7684\u65f6\u7a7a\u8868\u793a\u5b66\u4e60\uff1b2\uff09\u91c7\u7528\u8f7b\u91cf\u7ea7\u8109\u51b2\u9a71\u52a8\u5206\u5272\u5934\uff0c\u5728\u4fdd\u6301SNN\u4f4e\u5ef6\u8fdf\u7279\u6027\u7684\u540c\u65f6\u4ea7\u751f\u65f6\u95f4\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "result": "\u5728EndoVis18\u548c\u5185\u90e8SurgBleed\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSpikeSurgSeg\u8fbe\u5230\u4e0eSOTA ANN\u6a21\u578b\u76f8\u5f53\u7684mIoU\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u81f3\u5c118\u500d\uff0c\u76f8\u6bd4\u5927\u591a\u6570\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\u52a0\u901f\u8d85\u8fc720\u500d\u3002", "conclusion": "SpikeSurgSeg\u5c55\u793a\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u65f6\u95f4\u5173\u952e\u578b\u624b\u672f\u573a\u666f\u5206\u5272\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u975eGPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u624b\u672f\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21311", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21311", "abs": "https://arxiv.org/abs/2512.21311", "authors": ["Lilian Welschinger", "Yilin Liu", "Zican Wang", "Niloy Mitra"], "title": "Learning to Solve PDEs on Neural Shape Representations", "comment": "Article webpage link: https://welschinger.github.io/Learning-to-Solve-PDEs-on-Neural-Shape-Representations/", "summary": "Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u7f51\u683c\u7684\u795e\u7ecf\u8868\u9762PDE\u6c42\u89e3\u65b9\u6cd5\uff0c\u53ef\u76f4\u63a5\u5728\u795e\u7ecf\u8868\u793a\u4e0a\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u65e0\u9700\u663e\u5f0f\u7f51\u683c\u63d0\u53d6\u6216\u9010\u5b9e\u4f8b\u4f18\u5316\u3002", "motivation": "\u73b0\u4ee33D\u8d44\u4ea7\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u795e\u7ecf\u8868\u793a\uff0c\u4f46\u73b0\u6709PDE\u6c42\u89e3\u5668\u4e3b\u8981\u9488\u5bf9\u591a\u8fb9\u5f62/\u4e09\u89d2\u5f62\u7f51\u683c\uff0c\u5bfc\u81f4\u5728\u795e\u7ecf\u57df\u4e2d\u65e0\u6cd5\u76f4\u63a5\u6c42\u89e3\u8868\u9762PDE\uff0c\u9700\u8981\u663e\u5f0f\u7f51\u683c\u63d0\u53d6\u6216\u9010\u5b9e\u4f8b\u6b8b\u5dee\u8bad\u7ec3\uff0c\u963b\u788d\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5c40\u90e8\u5f62\u72b6\u5c5e\u6027\u7684\u5c40\u90e8\u66f4\u65b0\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u7b97\u5b50\u4e0e\u4e3b\u6d41\u795e\u7ecf\u8868\u9762\u8868\u793a\u81ea\u7136\u96c6\u6210\uff0c\u53ea\u9700\u5728\u5355\u4e2a\u4ee3\u8868\u6027\u5f62\u72b6\u4e0a\u8bad\u7ec3\u4e00\u6b21\uff0c\u5373\u53ef\u6cdb\u5316\u5230\u4e0d\u540c\u5f62\u72b6\u548c\u62d3\u6251\u53d8\u5316\u3002", "result": "\u5728\u89e3\u6790\u57fa\u51c6\u6d4b\u8bd5\uff08\u7403\u4f53\u4e0a\u7684\u70ed\u65b9\u7a0b\u548c\u6cca\u677e\u65b9\u7a0b\u6c42\u89e3\uff09\u548c\u4e0d\u540c\u8868\u793a\u7684\u771f\u5b9e\u795e\u7ecf\u8d44\u4ea7\u4e0a\uff0c\u65b9\u6cd5\u7565\u4f18\u4e8eCPM\uff0c\u4e0eFEM\u4fdd\u6301\u5408\u7406\u63a5\u8fd1\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u795e\u7ecf\u548c\u7ecf\u5178\u8868\u9762\u8868\u793a\u4e0a\u6c42\u89e3\u8868\u9762PDE\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u795e\u7ecf\u8868\u793a\u4e0a\u76f4\u63a5\u6c42\u89e3\u8868\u9762PDE\uff0c\u65e0\u9700\u663e\u5f0f\u7f51\u683c\u5316\u6216\u9010\u5b9e\u4f8b\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5fae\u6027\uff0c\u4e3a\u795e\u7ecf\u5f62\u72b6\u5206\u6790\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21334", "abs": "https://arxiv.org/abs/2512.21334", "authors": ["Jiaer Xia", "Peixian Chen", "Mengdan Zhang", "Xing Sun", "Kaiyang Zhou"], "title": "Streaming Video Instruction Tuning", "comment": null, "summary": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.", "AI": {"tldr": "Streamo\u662f\u4e00\u4e2a\u5b9e\u65f6\u6d41\u89c6\u9891LLM\uff0c\u4f5c\u4e3a\u901a\u7528\u4ea4\u4e92\u52a9\u624b\uff0c\u80fd\u591f\u6267\u884c\u591a\u79cd\u6d41\u89c6\u9891\u4efb\u52a1\uff0c\u5305\u62ec\u5b9e\u65f6\u89e3\u8bf4\u3001\u52a8\u4f5c\u7406\u89e3\u3001\u4e8b\u4ef6\u63cf\u8ff0\u3001\u65f6\u95f4\u4e8b\u4ef6\u5b9a\u4f4d\u548c\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u89c6\u9891\u6a21\u578b\u4e3b\u8981\u4e13\u6ce8\u4e8e\u95ee\u7b54\u6216\u5b57\u5e55\u751f\u6210\u7b49\u72ed\u7a84\u4efb\u52a1\uff0c\u7f3a\u4e4f\u80fd\u591f\u5904\u7406\u591a\u79cd\u6d41\u89c6\u9891\u4efb\u52a1\u7684\u901a\u7528\u4ea4\u4e92\u52a9\u624b\u3002\u9700\u8981\u586b\u8865\u79bb\u7ebf\u89c6\u9891\u611f\u77e5\u6a21\u578b\u548c\u5b9e\u65f6\u591a\u6a21\u6001\u52a9\u624b\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86Streamo-Instruct-465K\u5927\u89c4\u6a21\u6307\u4ee4\u8ddf\u968f\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u591a\u4efb\u52a1\u76d1\u7763\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u6d41\u7a0b\u7edf\u4e00\u8bad\u7ec3\u5f02\u6784\u6d41\u89c6\u9891\u4efb\u52a1\u3002", "result": "Streamo\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3001\u54cd\u5e94\u5f0f\u4ea4\u4e92\u80fd\u529b\u4ee5\u53ca\u5728\u591a\u79cd\u6d41\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6d41\u89c6\u9891\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Streamo\u586b\u8865\u4e86\u79bb\u7ebf\u89c6\u9891\u611f\u77e5\u6a21\u578b\u548c\u5b9e\u65f6\u591a\u6a21\u6001\u52a9\u624b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5411\u7edf\u4e00\u3001\u667a\u80fd\u7684\u8fde\u7eed\u89c6\u9891\u6d41\u7406\u89e3\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
