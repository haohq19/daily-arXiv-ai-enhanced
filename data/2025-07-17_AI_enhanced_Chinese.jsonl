{"id": "2507.11621", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11621", "abs": "https://arxiv.org/abs/2507.11621", "authors": ["Tianyi Wang", "Yangyang Wang", "Jie Pan", "Junfeng Jiao", "Christian Claudel"], "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways", "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "Highway on-ramp merging areas are common bottlenecks to traffic congestion\nand accidents. Currently, a cooperative control strategy based on connected and\nautomated vehicles (CAVs) is a fundamental solution to this problem. While CAVs\nare not fully widespread, it is necessary to propose a hierarchical cooperative\non-ramp merging control (HCOMC) framework for heterogeneous traffic flow on\ntwo-lane highways to address this gap. This paper extends longitudinal\ncar-following models based on the intelligent driver model and lateral\nlane-changing models using the quintic polynomial curve to account for\nhuman-driven vehicles (HDVs) and CAVs, comprehensively considering human\nfactors and cooperative adaptive cruise control. Besides, this paper proposes a\nHCOMC framework, consisting of a hierarchical cooperative planning model based\non the modified virtual vehicle model, a discretionary lane-changing model\nbased on game theory, and a multi-objective optimization model using the\nelitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and\nefficient merging process. Then, the performance of our HCOMC is analyzed under\ndifferent traffic densities and CAV penetration rates through simulation. The\nfindings underscore our HCOMC's pronounced comprehensive advantages in\nenhancing the safety of group vehicles, stabilizing and expediting merging\nprocess, optimizing traffic efficiency, and economizing fuel consumption\ncompared with benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u534f\u4f5c\u5f0f\u531d\u9053\u5408\u6d41\u63a7\u5236\uff08HCOMC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6df7\u5408\u4ea4\u901a\u6d41\u4e2d\u7684\u5408\u6d41\u95ee\u9898\uff0c\u7ed3\u5408\u7eb5\u5411\u8ddf\u9a70\u6a21\u578b\u548c\u6a2a\u5411\u6362\u9053\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u531d\u9053\u5408\u6d41\u533a\u662f\u4ea4\u901a\u62e5\u5835\u548c\u4e8b\u6545\u7684\u5e38\u89c1\u74f6\u9888\uff0c\u800c\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAVs\uff09\u5c1a\u672a\u666e\u53ca\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9002\u7528\u4e8e\u6df7\u5408\u4ea4\u901a\u6d41\u7684\u534f\u4f5c\u63a7\u5236\u7b56\u7565\u3002", "method": "\u6269\u5c55\u4e86\u57fa\u4e8e\u667a\u80fd\u9a7e\u9a76\u5458\u6a21\u578b\u7684\u7eb5\u5411\u8ddf\u9a70\u6a21\u578b\u548c\u57fa\u4e8e\u4e94\u6b21\u591a\u9879\u5f0f\u66f2\u7ebf\u7684\u6a2a\u5411\u6362\u9053\u6a21\u578b\uff0c\u63d0\u51fa\u4e86HCOMC\u6846\u67b6\uff0c\u5305\u62ec\u5206\u5c42\u534f\u4f5c\u89c4\u5212\u6a21\u578b\u3001\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u81ea\u7531\u6362\u9053\u6a21\u578b\u548c\u591a\u76ee\u6807\u4f18\u5316\u6a21\u578b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cHCOMC\u5728\u63d0\u5347\u8f66\u8f86\u7fa4\u5b89\u5168\u6027\u3001\u7a33\u5b9a\u548c\u52a0\u901f\u5408\u6d41\u8fc7\u7a0b\u3001\u4f18\u5316\u4ea4\u901a\u6548\u7387\u548c\u8282\u7ea6\u71c3\u6cb9\u6d88\u8017\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "HCOMC\u6846\u67b6\u4e3a\u6df7\u5408\u4ea4\u901a\u6d41\u4e2d\u7684\u531d\u9053\u5408\u6d41\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728CAV\u666e\u53ca\u7387\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.11570", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11570", "abs": "https://arxiv.org/abs/2507.11570", "authors": ["Ha Na Cho", "Sairam Sutari", "Alexander Lopez", "Hansen Bow", "Kai Zheng"], "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery", "comment": null, "summary": "Objective: To develop and evaluate machine learning (ML) models for\npredicting length of stay (LOS) in elective spine surgery, with a focus on the\nbenefits of temporal modeling and model interpretability. Materials and\nMethods: We compared traditional ML models (e.g., linear regression, random\nforest, support vector machine (SVM), and XGBoost) with our developed model,\nSurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an\nattention, using structured perioperative electronic health records (EHR) data.\nPerformance was evaluated using the coefficient of determination (R2), and key\npredictors were identified using explainable AI. Results: SurgeryLSTM achieved\nthe highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)\nand baseline models. The attention mechanism improved interpretability by\ndynamically identifying influential temporal segments within preoperative\nclinical sequences, allowing clinicians to trace which events or features most\ncontributed to each LOS prediction. Key predictors of LOS included bone\ndisorder, chronic kidney disease, and lumbar fusion identified as the most\nimpactful predictors of LOS. Discussion: Temporal modeling with attention\nmechanisms significantly improves LOS prediction by capturing the sequential\nnature of patient data. Unlike static models, SurgeryLSTM provides both higher\naccuracy and greater interpretability, which are critical for clinical\nadoption. These results highlight the potential of integrating attention-based\ntemporal models into hospital planning workflows. Conclusion: SurgeryLSTM\npresents an effective and interpretable AI solution for LOS prediction in\nelective spine surgery. Our findings support the integration of temporal,\nexplainable ML approaches into clinical decision support systems to enhance\ndischarge readiness and individualized patient care.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u7528\u4e8e\u9884\u6d4b\u810a\u67f1\u624b\u672f\u4f4f\u9662\u65f6\u95f4\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u65f6\u95f4\u5efa\u6a21\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u4f18\u52bf\u3002", "motivation": "\u63d0\u9ad8\u810a\u67f1\u624b\u672f\u4f4f\u9662\u65f6\u95f4\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u548c\u533b\u9662\u89c4\u5212\u3002", "method": "\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001SVM\u548cXGBoost\uff09\u4e0e\u5f00\u53d1\u7684SurgeryLSTM\u6a21\u578b\uff08\u5e26\u6ce8\u610f\u529b\u673a\u5236\u7684BiLSTM\uff09\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u3002", "result": "SurgeryLSTM\u9884\u6d4b\u51c6\u786e\u7387\u6700\u9ad8\uff08R2=0.86\uff09\uff0c\u4f18\u4e8eXGBoost\uff08R2=0.85\uff09\u548c\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002\u6ce8\u610f\u529b\u673a\u5236\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u52a8\u6001\u8bc6\u522b\u672f\u524d\u4e34\u5e8a\u5e8f\u5217\u4e2d\u7684\u5173\u952e\u65f6\u95f4\u7247\u6bb5\u3002", "conclusion": "SurgeryLSTM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5c06\u65f6\u95f4\u5efa\u6a21\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6574\u5408\u5230\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2507.11571", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11571", "abs": "https://arxiv.org/abs/2507.11571", "authors": ["Varun Velankar"], "title": "Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation", "comment": null, "summary": "Estimating a person's age from their gait has important applications in\nhealthcare, security and human-computer interaction. In this work, we review\nfifty-nine studies involving over seventy-five thousand subjects recorded with\nvideo, wearable and radar sensors. We observe that convolutional neural\nnetworks produce an average error of about 4.2 years, inertial-sensor models\nabout 4.5 years and multi-sensor fusion as low as 3.4 years, with notable\ndifferences between lab and real-world data. We then analyse sixty-three\nthousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population\ndataset to quantify correlations between age and five key metrics: stride\nlength, walking speed, step cadence, step-time variability and joint-angle\nentropy, with correlation coefficients of at least 0.27. Next, we fine-tune a\nResNet34 model and apply Grad-CAM to reveal that the network attends to the\nknee and pelvic regions, consistent with known age-related gait changes.\nFinally, on a one hundred thousand sample subset of the VersatileGait database,\nwe compare support vector machines, decision trees, random forests, multilayer\nperceptrons and convolutional neural networks, finding that deep networks\nachieve up to 96 percent accuracy while processing each sample in under 0.1\nseconds. By combining a broad meta-analysis with new large-scale experiments\nand interpretable visualizations, we establish solid performance baselines and\npractical guidelines for reducing gait-age error below three years in\nreal-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5143\u5206\u6790\u548c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u6b65\u6001\u8bc6\u522b\u5e74\u9f84\u7684\u6280\u672f\uff0c\u53d1\u73b0\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bef\u5dee\u6700\u4f4e\uff083.4\u5e74\uff09\uff0c\u5e76\u5206\u6790\u4e86\u6b65\u6001\u7279\u5f81\u4e0e\u5e74\u9f84\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u6b65\u6001\u8bc6\u522b\u5e74\u9f84\u5728\u533b\u7597\u3001\u5b89\u5168\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u6280\u672f\u8bef\u5dee\u8f83\u5927\uff0c\u9700\u5efa\u7acb\u66f4\u51c6\u786e\u7684\u57fa\u7ebf\u548c\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5143\u5206\u6790\uff0859\u9879\u7814\u7a76\uff09\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08OU-ISIR\u548cVersatileGait\uff09\u548c\u591a\u79cd\u6a21\u578b\uff08CNN\u3001SVM\u7b49\uff09\uff0c\u91cf\u5316\u6b65\u6001\u7279\u5f81\u4e0e\u5e74\u9f84\u7684\u5173\u7cfb\u3002", "result": "CNN\u8bef\u5dee\u6700\u4f4e\uff083.4\u5e74\uff09\uff0c\u6b65\u6001\u7279\u5f81\u4e0e\u5e74\u9f84\u76f8\u5173\u6027\u663e\u8457\uff08\u76f8\u5173\u7cfb\u6570\u22650.27\uff09\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u7387\u8fbe96%\u3002", "conclusion": "\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u53ef\u5c06\u6b65\u6001\u5e74\u9f84\u8bef\u5dee\u964d\u81f33\u5e74\u4ee5\u4e0b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u57fa\u7ebf\u3002"}}
{"id": "2507.11620", "categories": ["cs.LG", "astro-ph.HE", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11620", "abs": "https://arxiv.org/abs/2507.11620", "authors": ["Steven Dillmann", "Juan Rafael Mart\u00ednez-Galarza"], "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification", "comment": "Accepted at the 2025 ICML Workshop on Machine Learning for\n  Astrophysics, Code available at:\n  https://github.com/StevenDillmann/ml-xraytransients-mnras", "summary": "Event time series are sequences of discrete events occurring at irregular\ntime intervals, each associated with a domain-specific observational modality.\nThey are common in domains such as high-energy astrophysics, computational\nsocial science, cybersecurity, finance, healthcare, neuroscience, and\nseismology. Their unstructured and irregular structure poses significant\nchallenges for extracting meaningful patterns and identifying salient phenomena\nusing conventional techniques. We propose novel two- and three-dimensional\ntensor representations for event time series, coupled with sparse autoencoders\nthat learn physically meaningful latent representations. These embeddings\nsupport a variety of downstream tasks, including anomaly detection,\nsimilarity-based retrieval, semantic clustering, and unsupervised\nclassification. We demonstrate our approach on a real-world dataset from X-ray\nastronomy, showing that these representations successfully capture temporal and\nspectral signatures and isolate diverse classes of X-ray transients. Our\nframework offers a flexible, scalable, and generalizable solution for analyzing\ncomplex, irregular event time series across scientific and industrial domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u8868\u793a\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u5728X\u5c04\u7ebf\u5929\u6587\u5b66\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5176\u4e0d\u89c4\u5219\u6027\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u4e8c\u7ef4\u548c\u4e09\u7ef4\u5f20\u91cf\u8868\u793a\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\uff0c\u7ed3\u5408\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7269\u7406\u610f\u4e49\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u6355\u83b7\u4e86X\u5c04\u7ebf\u77ac\u53d8\u7684\u65f6\u95f4\u548c\u9891\u8c31\u7279\u5f81\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u9886\u57df\u7684\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11642", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11642", "abs": "https://arxiv.org/abs/2507.11642", "authors": ["Abhishek Jaiswal", "Nisheeth Srivastava"], "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment", "comment": null, "summary": "Posture-based mental state inference has significant potential in diagnosing\nfatigue, preventing injury, and enhancing performance across various domains.\nSuch tools must be research-validated with large datasets before being\ntranslated into practice. Unfortunately, such vision diagnosis faces serious\nchallenges due to the sensitivity of human subject data. To address this, we\nidentify sports settings as a viable alternative for accumulating data from\nhuman subjects experiencing diverse emotional states. We test our hypothesis in\nthe game of cricket and present a posture-based solution to identify human\nintent from activity videos. Our method achieves over 75\\% F1 score and over\n80\\% AUC-ROC in discriminating aggressive and defensive shot intent through\nmotion analysis. These findings indicate that posture leaks out strong signals\nfor intent inference, even with inherent noise in the data pipeline.\nFurthermore, we utilize existing data statistics as weak supervision to\nvalidate our findings, offering a potential solution for overcoming data\nlabelling limitations. This research contributes to generalizable techniques\nfor sports analytics and also opens possibilities for applying human behavior\nanalysis across various fields.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u59ff\u52bf\u7684\u5fc3\u7406\u72b6\u6001\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u677f\u7403\u8fd0\u52a8\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0cF1\u5206\u6570\u8d85\u8fc775%\uff0cAUC-ROC\u8d85\u8fc780%\u3002", "motivation": "\u59ff\u52bf\u63a8\u65ad\u5728\u75b2\u52b3\u8bca\u65ad\u3001\u4f24\u5bb3\u9884\u9632\u548c\u6027\u80fd\u63d0\u5347\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u6570\u636e\u654f\u611f\u6027\u6311\u6218\u3002\u4f53\u80b2\u573a\u666f\u4e3a\u591a\u6837\u5316\u60c5\u7eea\u6570\u636e\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u677f\u7403\u6bd4\u8d5b\u4e2d\u7684\u59ff\u52bf\u5206\u6790\uff0c\u7ed3\u5408\u8fd0\u52a8\u89c6\u9891\u8bc6\u522b\u51fb\u7403\u610f\u56fe\uff0c\u5e76\u5229\u7528\u73b0\u6709\u6570\u636e\u7edf\u8ba1\u4f5c\u4e3a\u5f31\u76d1\u7763\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u5728\u533a\u5206\u653b\u51fb\u6027\u548c\u9632\u5b88\u6027\u51fb\u7403\u610f\u56fe\u65f6\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u548cAUC-ROC\u5206\u522b\u8d85\u8fc775%\u548c80%\u3002", "conclusion": "\u59ff\u52bf\u5206\u6790\u4e3a\u610f\u56fe\u63a8\u65ad\u63d0\u4f9b\u4e86\u5f3a\u4fe1\u53f7\uff0c\u5f31\u76d1\u7763\u65b9\u6cd5\u53ef\u89e3\u51b3\u6570\u636e\u6807\u6ce8\u9650\u5236\uff0c\u4e3a\u4f53\u80b2\u5206\u6790\u548c\u884c\u4e3a\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11910", "abs": "https://arxiv.org/abs/2507.11910", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring", "comment": "Accepted at the 28th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "Event-based sensors have emerged as a promising solution for addressing\nchallenging conditions in pedestrian and traffic monitoring systems. Their\nlow-latency and high dynamic range allow for improved response time in\nsafety-critical situations caused by distracted walking or other unusual\nmovements. However, the availability of data covering such scenarios remains\nlimited. To address this gap, we present SEPose -- a comprehensive synthetic\nevent-based human pose estimation dataset for fixed pedestrian perception\ngenerated using dynamic vision sensors in the CARLA simulator. With nearly 350K\nannotated pedestrians with body pose keypoints from the perspective of fixed\ntraffic cameras, SEPose is a comprehensive synthetic multi-person pose\nestimation dataset that spans busy and light crowds and traffic across diverse\nlighting and weather conditions in 4-way intersections in urban, suburban, and\nrural environments. We train existing state-of-the-art models such as RVT and\nYOLOv8 on our dataset and evaluate them on real event-based data to demonstrate\nthe sim-to-real generalization capabilities of the proposed dataset.", "AI": {"tldr": "SEPose\u662f\u4e00\u4e2a\u5408\u6210\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u56fa\u5b9a\u884c\u4eba\u611f\u77e5\uff0c\u586b\u8865\u4e86\u771f\u5b9e\u6570\u636e\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u4e8b\u4ef6\u4f20\u611f\u5668\u5728\u884c\u4eba\u76d1\u6d4b\u7cfb\u7edf\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6761\u4ef6\u4e0b\uff08\u5982\u5206\u5fc3\u884c\u8d70\u6216\u5f02\u5e38\u8fd0\u52a8\uff09\u3002", "method": "\u4f7f\u7528CARLA\u6a21\u62df\u5668\u548c\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u751f\u6210SEPose\u6570\u636e\u96c6\uff0c\u5305\u542b\u8fd1350K\u5e26\u6ce8\u91ca\u7684\u884c\u4eba\u59ff\u6001\u5173\u952e\u70b9\u3002", "result": "SEPose\u6570\u636e\u96c6\u8986\u76d6\u591a\u79cd\u73af\u5883\u548c\u6761\u4ef6\uff0c\u8bad\u7ec3\u73b0\u6709\u6a21\u578b\uff08\u5982RVT\u548cYOLOv8\uff09\u5e76\u9a8c\u8bc1\u5176\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SEPose\u4e3a\u4e8b\u4ef6\u4f20\u611f\u5668\u5728\u884c\u4eba\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u652f\u6301\uff0c\u5c55\u793a\u4e86\u6a21\u62df\u6570\u636e\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.11931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11931", "abs": "https://arxiv.org/abs/2507.11931", "authors": ["Jingqian Wu", "Peiqi Duan", "Zongqiang Wang", "Changwei Wang", "Boxin Shi", "Edmund Y. Lam"], "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark", "comment": null, "summary": "In low-light environments, conventional cameras often struggle to capture\nclear multi-view images of objects due to dynamic range limitations and motion\nblur caused by long exposure. Event cameras, with their high-dynamic range and\nhigh-speed properties, have the potential to mitigate these issues.\nAdditionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,\nfacilitating bright frame synthesis from multiple viewpoints in low-light\nconditions. However, naively using an event-assisted 3D GS approach still faced\nchallenges because, in low light, events are noisy, frames lack quality, and\nthe color tone may be inconsistent. To address these issues, we propose\nDark-EvGS, the first event-assisted 3D GS framework that enables the\nreconstruction of bright frames from arbitrary viewpoints along the camera\ntrajectory. Triplet-level supervision is proposed to gain holistic knowledge,\ngranular details, and sharp scene rendering. The color tone matching block is\nproposed to guarantee the color consistency of the rendered frames.\nFurthermore, we introduce the first real-captured dataset for the event-guided\nbright frame synthesis task via 3D GS-based radiance field reconstruction.\nExperiments demonstrate that our method achieves better results than existing\nmethods, conquering radiance field reconstruction under challenging low-light\nconditions. The code and sample data are included in the supplementary\nmaterial.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDark-EvGS\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u91cd\u5efa\u591a\u89c6\u89d2\u660e\u4eae\u5e27\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u3001\u5e27\u8d28\u91cf\u5dee\u548c\u8272\u8c03\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76f8\u673a\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u96be\u4ee5\u6355\u6349\u6e05\u6670\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u901f\u5ea6\u7279\u6027\u53ef\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u30023D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u6709\u52a9\u4e8e\u91cd\u5efa\u8f90\u5c04\u573a\uff0c\u4f46\u4ecd\u9762\u4e34\u566a\u58f0\u3001\u5e27\u8d28\u91cf\u5dee\u548c\u8272\u8c03\u4e0d\u4e00\u81f4\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDark-EvGS\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u91cd\u76d1\u7763\u5b66\u4e60\u83b7\u53d6\u6574\u4f53\u77e5\u8bc6\u548c\u7ec6\u8282\uff0c\u5f15\u5165\u8272\u8c03\u5339\u914d\u6a21\u5757\u4fdd\u8bc1\u5e27\u95f4\u989c\u8272\u4e00\u81f4\u6027\uff0c\u5e76\u521b\u5efa\u9996\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7528\u4e8e\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDark-EvGS\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u91cd\u5efa\u8f90\u5c04\u573a\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u660e\u4eae\u5e27\u3002", "conclusion": "Dark-EvGS\u4e3a\u4f4e\u5149\u73af\u5883\u4e0b\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11836", "abs": "https://arxiv.org/abs/2507.11836", "authors": ["Jian Gao", "Jianshe Wu", "JingYi Ding"], "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction", "comment": null, "summary": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental\ntask for modeling evolving complex systems. Existing node-centric and\nevent-centric methods focus on individual interactions or atomic states,\nfailing to capture the structural cohesion of composite hyper-events, groups of\ncausally related events. To address this, we propose HyperEvent, a framework\nreframing dynamic link prediction as hyper-event recognition. Central to\nHyperEvent is the dynamic construction of an association sequence using event\ncorrelation vectors. These vectors quantify pairwise dependencies between the\nquery event and relevant historical events, thereby characterizing the\nstructural cohesion of a potential hyper-event. The framework predicts the\noccurrence of the query event by evaluating whether it collectively forms a\nvalid hyper-event with these historical events. Notably, HyperEvent outperforms\nstate-of-the-art methods on 4 out of 5 datasets in the official leaderboard.\nFor scalability, we further introduce an efficient parallel training algorithm\nthat segments large event streams to enable concurrent training. Experiments\nvalidate HyperEvent's superior accuracy and efficiency on large-scale graphs.\nAmong which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank\nover state-of-the-art baseline on the large-scale Flight dataset while\nutilizing only 10.17% of the training time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHyperEvent\u6846\u67b6\uff0c\u5c06\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u91cd\u6784\u4e3a\u8d85\u4e8b\u4ef6\u8bc6\u522b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u5173\u6027\u5411\u91cf\u52a8\u6001\u6784\u5efa\u5173\u8054\u5e8f\u5217\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u590d\u5408\u8d85\u4e8b\u4ef6\u7684\u7ed3\u6784\u51dd\u805a\u529b\uff0c\u9650\u5236\u4e86\u52a8\u6001\u56fe\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faHyperEvent\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u5173\u6027\u5411\u91cf\u52a8\u6001\u6784\u5efa\u5173\u8054\u5e8f\u5217\uff0c\u8bc4\u4f30\u67e5\u8be2\u4e8b\u4ef6\u4e0e\u5386\u53f2\u4e8b\u4ef6\u662f\u5426\u5f62\u6210\u6709\u6548\u8d85\u4e8b\u4ef6\u3002", "result": "\u57285\u4e2a\u6570\u636e\u96c6\u4e2d4\u4e2a\u8868\u73b0\u6700\u4f73\uff0c\u5927\u89c4\u6a21Flight\u6570\u636e\u96c6\u4e0aMRR\u63d0\u53476.95%\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ec5\u970010.17%\u3002", "conclusion": "HyperEvent\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u56fe\u5efa\u6a21\u3002"}}
{"id": "2507.12105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12105", "abs": "https://arxiv.org/abs/2507.12105", "authors": ["Yiquan Gao", "Duohui Xu"], "title": "Out-of-distribution data supervision towards biomedical semantic segmentation", "comment": "This paper was published in Proceedings of SPIE Volume 13442 and is\n  reprinted with permission. The official version is available at\n  https://doi.org/10.1117/12.3052988. One personal copy is allowed.\n  Reproduction, distribution, or commercial use is prohibited", "summary": "Biomedical segmentation networks easily suffer from the unexpected\nmisclassification between foreground and background objects when learning on\nlimited and imperfect medical datasets. Inspired by the strong power of\nOut-of-Distribution (OoD) data on other visual tasks, we propose a data-centric\nframework, Med-OoD to address this issue by introducing OoD data supervision\ninto fully-supervised biomedical segmentation with none of the following needs:\n(i) external data sources, (ii) feature regularization objectives, (iii)\nadditional annotations. Our method can be seamlessly integrated into\nsegmentation networks without any modification on the architectures. Extensive\nexperiments show that Med-OoD largely prevents various segmentation networks\nfrom the pixel misclassification on medical images and achieves considerable\nperformance improvements on Lizard dataset. We also present an emerging\nlearning paradigm of training a medical segmentation network completely using\nOoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU\nas test result. We hope this learning paradigm will attract people to rethink\nthe roles of OoD data. Code is made available at\nhttps://github.com/StudioYG/Med-OoD.", "AI": {"tldr": "Med-OoD\u6846\u67b6\u901a\u8fc7\u5f15\u5165OoD\u6570\u636e\u76d1\u7763\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u524d\u666f\u4e0e\u80cc\u666f\u8bef\u5206\u7c7b\u7684\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u3001\u7279\u5f81\u6b63\u5219\u5316\u6216\u989d\u5916\u6807\u6ce8\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7f51\u7edc\u5728\u6709\u9650\u548c\u4e0d\u5b8c\u7f8e\u7684\u6570\u636e\u96c6\u4e0a\u5bb9\u6613\u53d1\u751f\u524d\u666f\u4e0e\u80cc\u666f\u7684\u8bef\u5206\u7c7b\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8d44\u6e90\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faMed-OoD\u6846\u67b6\uff0c\u5c06OoD\u6570\u636e\u76d1\u7763\u5f15\u5165\u5168\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u3001\u7279\u5f81\u6b63\u5219\u5316\u6216\u989d\u5916\u6807\u6ce8\uff0c\u4e14\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u7f51\u7edc\u67b6\u6784\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMed-OoD\u663e\u8457\u51cf\u5c11\u4e86\u50cf\u7d20\u8bef\u5206\u7c7b\uff0c\u5e76\u5728Lizard\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u4ec5\u4f7f\u7528OoD\u6570\u636e\u8bad\u7ec3\u7684\u7f51\u7edc\u8fbe\u5230\u4e8676.1%\u7684mIoU\u3002", "conclusion": "Med-OoD\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86OoD\u6570\u636e\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.12195", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12195", "abs": "https://arxiv.org/abs/2507.12195", "authors": ["Arkaprabha Basu"], "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision", "comment": null, "summary": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u521b\u65b0\u6280\u672f\uff08\u5206\u5f62\u5377\u79ef\u3001\u81ea\u654f\u611f\u74f7\u7816\u586b\u5145\u548c\u8d85\u5206\u8fa8\u7387\uff09\u7528\u4e8e\u5370\u5ea6\u53e4\u8ff9\u7684\u4fdd\u62a4\u4e0e\u4fee\u590d\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u4ee3\u6570\u5b57\u5316\u65b9\u6cd5\u6539\u53d8\u4e86\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u7684\u8303\u5f0f\uff0c\u4f46\u5370\u5ea6\u53e4\u8ff9\u7684\u7279\u6b8a\u6027\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u521b\u65b0\u7684\u6280\u672f\u6765\u5e73\u8861\u4f20\u7edf\u4e0e\u521b\u65b0\u3002", "method": "1. \u5206\u5f62\u5377\u79ef\u65b9\u6cd5\u7528\u4e8e\u56fe\u50cf\u5206\u5272\uff1b2. \u81ea\u654f\u611f\u74f7\u7816\u586b\u5145\uff08SSTF\uff09\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672fMosaicSlice\uff1b3. \u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u7f1d\u533a\u57df\u586b\u5145\u548c\u9ad8\u7ec6\u8282\u74f7\u7816\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u771f\u5b9e\u6027\u548c\u4f4e\u6210\u672c\u81ea\u52a8\u5316\u3002", "conclusion": "\u8fd9\u4e9b\u6280\u672f\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7f8e\u89c2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4f20\u7edf\u4e0e\u521b\u65b0\u7684\u5e73\u8861\u53d1\u5c55\u3002"}}
{"id": "2507.12269", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12269", "abs": "https://arxiv.org/abs/2507.12269", "authors": ["Sybelle Goedicke-Fritz", "Michelle Bous", "Annika Engel", "Matthias Flotho", "Pascal Hirsch", "Hannah Wittig", "Dino Milanovic", "Dominik Mohr", "Mathias Kaspar", "Sogand Nemat", "Dorothea Kerner", "Arno B\u00fccker", "Andreas Keller", "Sascha Meyer", "Michael Zemlin", "Philipp Flotho"], "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship", "summary": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u5229\u7528\u51fa\u751f24\u5c0f\u65f6\u5185\u7684\u80f8\u90e8X\u5149\u7247\u9884\u6d4b\u6781\u4f4e\u51fa\u751f\u4f53\u91cd\u5a74\u513f\u7684\u652f\u6c14\u7ba1\u80ba\u53d1\u80b2\u4e0d\u826f\uff08BPD\uff09\u7ed3\u5c40\u3002", "motivation": "BPD\u662f\u4e00\u79cd\u6162\u6027\u80ba\u75c5\uff0c\u5f71\u54cd35%\u7684\u6781\u4f4e\u51fa\u751f\u4f53\u91cd\u5a74\u513f\uff0c\u4e14\u9884\u9632\u63aa\u65bd\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u98ce\u9669\u3002\u65e9\u671f\u9884\u6d4bBPD\u7ed3\u5c40\u53ef\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u6bd2\u6027\u5e72\u9884\u3002", "method": "\u7814\u7a76\u4f7f\u7528163\u540d\u6781\u4f4e\u51fa\u751f\u4f53\u91cd\u5a74\u513f\u7684\u80f8\u90e8X\u5149\u7247\uff0c\u5fae\u8c03\u4e86\u9884\u8bad\u7ec3\u7684ResNet-50\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5c42\u51bb\u7ed3\u3001CutMix\u589e\u5f3a\u548c\u7ebf\u6027\u63a2\u6d4b\u6280\u672f\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u9884\u6d4b\u4e2d\u5ea6/\u91cd\u5ea6BPD\u65f6\uff0cAUROC\u4e3a0.78\uff0c\u5e73\u8861\u51c6\u786e\u7387\u4e3a0.69\uff0cF1\u5206\u6570\u4e3a0.67\u3002\u57df\u5185\u9884\u8bad\u7ec3\u663e\u8457\u4f18\u4e8eImageNet\u521d\u59cb\u5316\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57df\u7279\u5f02\u6027\u9884\u8bad\u7ec3\u7ed3\u5408\u6e10\u8fdb\u51bb\u7ed3\u548c\u7ebf\u6027\u63a2\u6d4b\uff0c\u53ef\u4ece\u5e38\u89c4X\u5149\u7247\u4e2d\u51c6\u786e\u9884\u6d4bBPD\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u5408\u4e34\u5e8a\u90e8\u7f72\u3002"}}
