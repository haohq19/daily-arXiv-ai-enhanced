<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.RO](#cs.RO) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Privacy-Concealing Cooperative Perception for BEV Scene Segmentation](https://arxiv.org/abs/2602.13555)
*Song Wang,Lingling Li,Marcus Santos,Guanghui Wang*

Main category: cs.CV

TL;DR: 提出PCC框架，通过对抗学习在BEV特征中隐藏视觉线索，防止图像重建，保护合作感知中的隐私，同时保持分割性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的协同感知系统通过共享传感信息提升感知性能，但面临隐私泄露风险，共享数据可能被用于重建敏感视觉内容。

Method: 基于共享的BEV特征设计隐藏网络，采用对抗学习机制训练：隐藏网络在BEV特征中隐藏视觉线索，重建网络尝试恢复这些线索。感知网络与隐藏网络集成进行端到端优化。

Result: PCC框架有效降低了重建图像的质量，对分割性能影响最小，为合作车辆提供了隐私保护。

Conclusion: 提出的PCC框架在保护协同感知系统隐私方面有效，在保持分割性能的同时防止图像重建，源代码将在发表后公开。

Abstract: Cooperative perception systems for autonomous driving aim to overcome the limited perception range of a single vehicle by communicating with adjacent agents to share sensing information. While this improves perception performance, these systems also face a significant privacy-leakage issue, as sensitive visual content can potentially be reconstructed from the shared data. In this paper, we propose a novel Privacy-Concealing Cooperation (PCC) framework for Bird's Eye View (BEV) semantic segmentation. Based on commonly shared BEV features, we design a hiding network to prevent an image reconstruction network from recovering the input images from the shared features. An adversarial learning mechanism is employed to train the network, where the hiding network works to conceal the visual clues in the BEV features while the reconstruction network attempts to uncover these clues. To maintain segmentation performance, the perception network is integrated with the hiding network and optimized end-to-end. The experimental results demonstrate that the proposed PCC framework effectively degrades the quality of the reconstructed images with minimal impact on segmentation performance, providing privacy protection for cooperating vehicles. The source code will be made publicly available upon publication.

</details>


### [2] [Explore Intrinsic Geometry for Query-based Tiny and Oriented Object Detector with Momentum-based Bipartite Matching](https://arxiv.org/abs/2602.13728)
*Junpeng Zhang,Zewei Yang,Jie Feng,Yuhui Zheng,Ronghua Shang,Mengxuan Zhang*

Main category: cs.CV

TL;DR: IGOFormer是一个面向航空图像中任意方向物体的新型查询式检测器，通过几何感知解码和动量匹配机制提升检测性能


<details>
  <summary>Details</summary>
Motivation: 现有查询式检测器在处理任意方向物体（尤其是纹理信息有限的小物体）时性能受限，主要原因是像素级特征解码中未充分利用内在几何信息，以及阶段间二分图匹配不一致

Method: 提出IGOFormer：1）内在几何感知解码器，通过注入从对象查询相关性推断的互补几何嵌入来增强对象相关特征；2）基于动量的二分图匹配方案，通过指数移动平均自适应聚合历史匹配成本

Result: 在DOTA-V1.0数据集上，使用Swin-T骨干网络在单尺度设置下达到78.00%的AP50分数，证明了方法的优越性

Conclusion: IGOFormer通过显式整合内在几何信息和增强阶段间匹配稳定性，有效提升了航空图像中任意方向物体的检测性能

Abstract: Recent query-based detectors have achieved remarkable progress, yet their performance remains constrained when handling objects with arbitrary orientations, especially for tiny objects capturing limited texture information. This limitation primarily stems from the underutilization of intrinsic geometry during pixel-based feature decoding and the occurrence of inter-stage matching inconsistency caused by stage-wise bipartite matching. To tackle these challenges, we present IGOFormer, a novel query-based oriented object detector that explicitly integrates intrinsic geometry into feature decoding and enhances inter-stage matching stability. Specifically, we design an Intrinsic Geometry-aware Decoder, which enhances the object-related features conditioned on an object query by injecting complementary geometric embeddings extrapolated from their correlations to capture the geometric layout of the object, thereby offering a critical geometric insight into its orientation. Meanwhile, a Momentum-based Bipartite Matching scheme is developed to adaptively aggregate historical matching costs by formulating an exponential moving average with query-specific smoothing factors, effectively preventing conflicting supervisory signals arising from inter-stage matching inconsistency. Extensive experiments and ablation studies demonstrate the superiority of our IGOFormer for aerial oriented object detection, achieving an AP$_{50}$ score of 78.00\% on DOTA-V1.0 using Swin-T backbone under the single-scale setting. The code will be made publicly available.

</details>


### [3] [SAM4Dcap: Training-free Biomechanical Twin System from Monocular Video](https://arxiv.org/abs/2602.13760)
*Li Wang,HaoYu Wang,Xi Chen,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: SAM4Dcap：一个开源框架，从单目视频估计生物力学指标，无需额外训练，结合4D人体网格重建与生物力学求解器


<details>
  <summary>Details</summary>
Motivation: 定量生物力学分析对临床诊断和损伤预防至关重要，但传统光学运动捕捉系统成本高昂，多视图视频方法虽然降低了门槛，但仍不适用于家庭单目捕捉场景

Method: 集成SAM-Body4D的时间一致4D人体网格重建与OpenSim生物力学求解器，将重建的网格转换为轨迹文件，兼容多种肌肉骨骼模型，引入自动化提示策略和Linux原生构建

Result: 初步评估显示，在行走和跳跃任务中，SAM4Dcap有潜力实现与多视图系统相当的膝关节运动学预测，尽管在髋关节屈曲和残留抖动方面仍存在一些差异

Conclusion: 通过将先进计算机视觉与成熟的生物力学模拟相结合，SAM4Dcap为非实验室运动分析提供了一个灵活、可访问的基础框架

Abstract: Quantitative biomechanical analysis is essential for clinical diagnosis and injury prevention but is often restricted to laboratories due to the high cost of optical motion capture systems. While multi-view video approaches have lowered barriers, they remain impractical for home-based scenarios requiring monocular capture. This paper presents SAM4Dcap, an open-source, end-to-end framework for estimating biomechanical metrics from monocular video without additional training. SAM4Dcap integrates the temporally consistent 4D human mesh recovery of SAM-Body4D with the OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with diverse musculoskeletal models. We introduce automated prompting strategies and a Linux-native build for processing. Preliminary evaluations on walking and drop-jump tasks indicate that SAM4Dcap has the potential to achieve knee kinematic predictions comparable to multi-view systems, although some discrepancies in hip flexion and residual jitter remain. By bridging advanced computer vision with established biomechanical simulation, SAM4Dcap provides a flexible, accessible foundation for non-laboratory motion analysis.

</details>


### [4] [EgoSound: Benchmarking Sound Understanding in Egocentric Videos](https://arxiv.org/abs/2602.14122)
*Bingwen Zhu,Yuqian Fu,Qiaole Dong,Guolei Sun,Tianwen Qian,Yuzheng Wu,Danda Pani Paudel,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: EgoSound是首个评估多模态大语言模型在自我中心声音理解能力的基准，包含7315个验证过的问答对，涵盖7个任务类型，揭示了当前模型在细粒度空间和因果理解方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 人类感知本质上是多感官的，整合视觉、声音和运动来理解世界。声音提供了关于空间布局、屏幕外事件和因果交互的重要线索，特别是在自我中心设置中，听觉和视觉信号紧密耦合。然而，当前的多模态大语言模型主要关注视觉-语言理解，缺乏对声音理解的系统评估。

Method: 1. 整合Ego4D和EgoBlind数据集，涵盖有视觉和依赖声音的体验
2. 定义七任务分类法：内在声音感知、空间定位、因果推理和跨模态推理
3. 通过多阶段自动生成流程构建EgoSound基准，包含7315个验证过的问答对，覆盖900个视频
4. 在9个最先进的多模态大语言模型上进行全面实验

Result: 实验显示当前模型展现出新兴的听觉推理能力，但在细粒度的空间和因果理解方面仍然有限。EgoSound为推进多感官自我中心智能建立了具有挑战性的基础。

Conclusion: EgoSound是首个系统评估多模态大语言模型在自我中心声音理解的基准，揭示了当前模型在听觉理解方面的局限性，为弥合"看到"和"真正听到"世界之间的差距提供了重要基础。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.

</details>


### [5] [Event-based Visual Deformation Measurement](https://arxiv.org/abs/2602.14376)
*Yuliang Wu,Wei Zhai,Yuxin Cui,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出事件-帧融合框架，利用事件提供时间密集运动线索，帧提供空间密集精确估计，实现高效变形测量


<details>
  <summary>Details</summary>
Motivation: 传统基于图像的方法依赖帧间微小运动来约束对应搜索空间，限制了在高度动态场景的应用，或需要高速相机带来高昂的存储和计算开销

Method: 提出事件-帧融合框架，结合事件的时间密集运动线索和帧的空间密集精确估计；引入仿射不变单纯形(AIS)框架，将变形场划分为线性化子区域；采用邻域贪婪优化策略加速参数搜索并减少误差累积

Result: 在包含120多个序列的基准数据集上，方法在生存率上优于最先进基线1.6%；仅使用高速视频方法18.9%的数据存储和处理资源

Conclusion: 事件-帧融合框架结合AIS建模和邻域贪婪优化，在保持高精度的同时显著降低资源需求，为动态场景变形测量提供了高效解决方案

Abstract: Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.

</details>


### [6] [Architectural Insights for Post-Tornado Damage Recognition](https://arxiv.org/abs/2602.14523)
*Robinson Umeike,Thang Dao,Shane Crawford,John van de Lindt,Blythe Johnston,Wanting,Wang,Trung Do,Ajibola Mofikoya,Sarbesh Banjara,Cuong Pham*

Main category: cs.CV

TL;DR: 该研究系统评估了79个深度学习模型在龙卷风建筑损伤评估任务上的表现，发现优化器选择比架构选择更重要，SGD优化器配合低学习率能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 龙卷风后快速准确的建筑损伤评估对救援和恢复至关重要，但现有自动化方法面临两个主要挑战：1) 龙卷风破坏场景与标准预训练数据集存在严重的领域偏移；2) 真实灾害数据中存在极端的类别不平衡问题。

Method: 研究构建了Quad-State Tornado Damage (QSTD)基准数据集，系统评估了79个开源深度学习模型（包括CNN和Vision Transformer），进行了超过2300个控制实验。重点关注架构与优化的交互作用，特别是优化器选择和学习率设置。

Result: 研究发现：1) 优化器选择比架构选择更重要，从Adam切换到SGD使Vision Transformer和Swin Transformer的F1分数提升25-38点；2) 低学习率(1x10^(-4))对所有架构都至关重要，平均提升F1分数10.2点；3) 最佳模型ConvNeXt-Base在跨事件测试中达到46.4% Macro F1，比基线提升34.6点。

Conclusion: 龙卷风损伤评估中，实现操作级性能的关键在于架构与优化的复杂交互，而非单纯架构选择。优化器选择和学习率设置对模型性能的影响可能超过架构本身，这一发现对灾害响应中的计算机视觉应用具有重要意义。

Abstract: Rapid and accurate building damage assessment in the immediate aftermath of tornadoes is critical for coordinating life-saving search and rescue operations, optimizing emergency resource allocation, and accelerating community recovery. However, current automated methods struggle with the unique visual complexity of tornado-induced wreckage, primarily due to severe domain shift from standard pre-training datasets and extreme class imbalance in real-world disaster data. To address these challenges, we introduce a systematic experimental framework evaluating 79 open-source deep learning models, encompassing both Convolutional Neural Networks (CNNs) and Vision Transformers, across over 2,300 controlled experiments on our newly curated Quad-State Tornado Damage (QSTD) benchmark dataset. Our findings reveal that achieving operational-grade performance hinges on a complex interaction between architecture and optimization, rather than architectural selection alone. Most strikingly, we demonstrate that optimizer choice can be more consequential than architecture: switching from Adam to SGD provided dramatic F1 gains of +25 to +38 points for Vision Transformer and Swin Transformer families, fundamentally reversing their ranking from bottom-tier to competitive with top-performing CNNs. Furthermore, a low learning rate of 1x10^(-4) proved universally critical, boosting average F1 performance by +10.2 points across all architectures. Our champion model, ConvNeXt-Base trained with these optimized settings, demonstrated strong cross-event generalization on the held-out Tuscaloosa-Moore Tornado Damage (TMTD) dataset, achieving 46.4% Macro F1 (+34.6 points over baseline) and retaining 85.5% Ordinal Top-1 Accuracy despite temporal and sensor domain shifts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity](https://arxiv.org/abs/2602.13486)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: 论文提出raFLoRA方法解决联邦低秩自适应中的秩崩溃问题，通过秩分区聚合提升异构联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 实际联邦学习场景中，客户端在系统资源和数据分布上存在异质性，这促使不同客户端使用不同的LoRA秩。作者发现异构FedLoRA中存在被忽视的"秩崩溃"现象，即全局更新的能量集中在最小共享秩上，导致性能不佳且对秩配置高度敏感。

Method: 提出raFLoRA方法：一种秩分区聚合方法，将本地更新分解为秩分区，然后根据每个分区的有效客户端贡献加权聚合。通过理论分析揭示了秩崩溃的根本原因：秩无关的聚合权重与秩相关的客户端贡献不匹配。

Result: 在分类和推理任务上的广泛实验表明，raFLoRA能够防止秩崩溃，提高模型性能，同时保持与最先进FedLoRA基线相当的通信效率。

Conclusion: raFLoRA有效解决了异构联邦低秩自适应中的秩崩溃问题，通过秩分区聚合机制提升了联邦学习的性能，为实际部署中的异构客户端场景提供了更优的解决方案。

Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

</details>


### [8] [Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series](https://arxiv.org/abs/2602.13649)
*Li Zhang,Nital Patel,Xiuqi Li,Jessica Lin*

Main category: cs.LG

TL;DR: 提出联合时间序列链概念，用于在中断时间序列或两个相关时间序列中发现异常演化趋势


<details>
  <summary>Details</summary>
Motivation: 现有时间序列链定义仅考虑单个时间序列，容易错过中断时间序列或相关时间序列中的异常演化模式

Method: 引入联合时间序列链新定义，专门用于在中断时间序列或两个相关时间序列中发现意外演化趋势，解决时间序列中断导致的鲁棒性问题，并提出有效的排序标准识别最佳链

Result: 通过大量实证评估证明，该方法在定位异常演化模式方面优于现有TSC工作，并在英特尔真实制造应用中展示了实用性

Conclusion: 联合时间序列链能够有效发现中断时间序列或相关时间序列中的异常演化趋势，具有实际应用价值

Abstract: Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .

</details>


### [9] [GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization](https://arxiv.org/abs/2602.13921)
*Juntong Wang,Libin Chen,Xiyuan Wang,Shijia Kang,Haotong Yang,Da Zheng,Muhan Zhang*

Main category: cs.LG

TL;DR: GREPO是首个用于仓库级bug定位任务的GNN基准，包含86个Python仓库和47294个bug修复任务，为GNN处理提供图数据结构，性能优于传统检索方法。


<details>
  <summary>Details</summary>
Motivation: 仓库级bug定位是关键的软件工程挑战，标准LLM由于上下文窗口限制无法处理整个代码仓库，现有检索方法有限，而GNN虽有潜力但缺乏专用基准。

Method: 引入GREPO基准，包含86个Python仓库和47294个bug修复任务，提供可直接用于GNN处理的图数据结构，评估了多种GNN架构。

Result: GNN架构在bug定位任务上表现出色，性能优于传统信息检索基线方法（如关键词匹配、文本相似度和简单图启发式方法）。

Conclusion: GREPO展示了GNN在bug定位任务上的潜力，为未来研究提供了基础资源，代码已开源。

Abstract: Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.

</details>


### [10] [TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models](https://arxiv.org/abs/2602.14200)
*Nicolas Zumarraga,Thomas Kaar,Ning Wang,Maxwell A. Xu,Max Rosenblattl,Markus Kreft,Kevin O'Sullivan,Paul Schmiedmayer,Patrick Langer,Robert Jakob*

Main category: cs.LG

TL;DR: TS-Haystack是一个长上下文时间序列检索基准，用于评估时间序列语言模型在长序列中的检索能力，发现现有模型在压缩时分类性能保持但检索性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列语言模型通常在短序列上训练和评估，而真实世界的时间序列传感器数据可能包含数百万个数据点，需要精确的时间定位和严格的计算约束，当前基准无法捕捉这一挑战。

Method: 引入TS-Haystack基准，包含四种任务类型（直接检索、时间推理、多步推理和上下文异常）的十个任务，通过在长纵向加速度计记录中嵌入短活动片段进行受控"针插入"，系统评估从秒到2小时的不同上下文长度。

Result: 研究发现现有时间序列编码器随着上下文长度增加会忽略时间粒度，产生任务依赖效应：压缩有助于分类但损害局部事件检索。学习到的潜在压缩在高达176倍压缩比下保持或改善分类准确率，但检索性能随上下文长度增加而下降，导致时间局部信息丢失。

Conclusion: 需要设计能够解耦序列长度与计算复杂度同时保持时间保真度的架构，以解决长上下文时间序列检索的挑战。

Abstract: Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.

</details>


### [11] [Robust multi-task boosting using clustering and local ensembling](https://arxiv.org/abs/2602.14231)
*Seyedsaman Emami,Daniel Hernández-Lobato,Gonzalo Martínez-Muñoz*

Main category: cs.LG

TL;DR: RMB-CLE：一种通过基于误差的任务聚类和局部集成来防止负迁移的鲁棒多任务学习框架


<details>
  <summary>Details</summary>
Motivation: 传统多任务学习方法在强制不相关或噪声任务共享表示时容易遭受负迁移问题，需要一种能自适应识别相关任务并防止负传输的鲁棒方法

Method: 提出RMB-CLE框架，通过跨任务误差推导任务间相似度（分解为功能不匹配和不可约噪声），使用凝聚聚类自适应分组任务，并在每个簇内使用局部集成实现鲁棒知识共享

Result: 在合成数据中能恢复真实任务簇，在多种真实世界和合成基准测试中一致优于多任务、单任务和基于池化的集成方法

Conclusion: RMB-CLE不仅结合了聚类和集成，更是一个通用、可扩展的框架，为鲁棒多任务学习建立了新基础

Abstract: Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.

</details>


### [12] [Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting](https://arxiv.org/abs/2602.14267)
*Manal Rahal,Bestoun S. Ahmed,Roger Renström,Robert Stener*

Main category: cs.LG

TL;DR: DELTAiF是一个基于迁移学习的框架，用于预测家庭热水消耗，通过从代表性家庭学习知识并微调到其他家庭，实现可扩展的热水需求预测，减少67%训练时间同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 随着住宅热泵安装的快速增长，优化家庭热水生产变得至关重要，但面临技术和可扩展性挑战。传统方法为每个家庭单独训练机器学习模型在云连接热泵部署中计算成本高昂。

Method: 提出DELTAiF迁移学习框架，从代表性家庭学习知识并微调到其他家庭，预测大型热水使用事件（如淋浴），无需为每个热泵安装单独训练模型。

Result: 训练时间减少约67%，预测精度在0.874-0.991之间，平均绝对百分比误差在0.001-0.017之间。当源家庭消费模式规律时，迁移学习效果尤其显著。

Conclusion: 迁移学习能够实现可扩展的家庭热水需求预测，DELTAiF框架在保持高精度的同时显著降低计算成本，特别适用于源家庭消费模式规律的情况。

Abstract: With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.
  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.

</details>


### [13] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: AERO改进GRPO算法，通过自适应采样、选择性拒绝和贝叶斯后验防止零梯度死区，在相同计算预算下减少48%计算量和45%单步时间，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在RLVR微调中广泛使用，但当组内所有rollout结果相同时（全对或全错），组归一化优势为零，导致无梯度信号和计算浪费，需要更高效的优化方法。

Method: 提出AERO方法：1) 自适应rollout策略，动态调整采样数量；2) 选择性拒绝策略，策略性修剪rollout；3) 贝叶斯后验维护，防止零优势死区。

Result: 在三个模型配置上验证：相同总rollout预算下，AERO减少约48%总训练计算量，缩短约45%单步时间，同时Pass@8和Avg@8指标匹配或优于GRPO。

Conclusion: AERO提供了实用、可扩展且计算高效的RL-based LLM对齐策略，显著提升计算效率而不牺牲性能，适用于大规模语言模型后训练。

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [14] [DCTracks: An Open Dataset for Machine Learning-Based Drift Chamber Track Reconstruction](https://arxiv.org/abs/2602.14571)
*Qian Liyan,Zhang Yao,Yuan Ye,Zhang Zhaoke,Fang Jin,Jiang Shimiao,Zhang Jin,Li Ke,Liu Beijiang,Xu Chenglin,Zhang Yifan,Jia Xiaoqian,Qin Xiaoshuai,Huang Xingtao*

Main category: cs.LG

TL;DR: 提出用于机器学习轨迹重建的蒙特卡洛数据集，定义标准化评估指标，并比较传统算法与图神经网络方法


<details>
  <summary>Details</summary>
Motivation: 为推进基于机器学习的轨迹重建研究，需要标准化的数据集和评估指标，以便进行可重复、可比较的验证

Method: 创建蒙特卡洛模拟的单轨迹和双轨迹漂移室事件数据集，定义轨迹重建专用评估指标，并实现传统轨迹重建算法与图神经网络方法的对比

Result: 建立了标准化的评估框架，为传统算法和GNN方法提供了可比较的性能基准，促进了未来研究的可重复验证

Conclusion: 该工作为机器学习轨迹重建领域提供了重要的基础设施，包括数据集、评估指标和基准结果，将推动该领域的标准化发展和性能提升

Abstract: We introduce a Monte Carlo (MC) dataset of single- and two-track drift chamber events to advance Machine Learning (ML)-based track reconstruction. To enable standardized and comparable evaluation, we define track reconstruction specific metrics and report results for traditional track reconstruction algorithms and a Graph Neural Networks (GNNs) method, facilitating rigorous, reproducible validation for future research.

</details>


### [15] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: 提出一种解耦的连续时间演员-评论家算法，通过交替更新解决标准离散时间RL在连续时间事件驱动控制中的Q函数退化问题，在连续控制基准和实际交易任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界控制问题（金融、机器人等）在连续时间中演化，具有非均匀、事件驱动的决策特性。标准离散时间RL基于固定步长Bellman更新，在这种设置下存在困难：当时间间隔缩小时，Q函数会退化为值函数V，失去动作排序能力。现有连续时间方法通过优势率函数q重新引入动作信息，但使用复杂的鞅损失或正交约束来强制最优性，这些方法对测试过程选择敏感，并将V和q耦合到难以可靠训练的大型复杂优化问题中。

Method: 提出一种解耦的连续时间演员-评论家算法，采用交替更新策略：1) q通过V的扩散生成器学习；2) V通过基于哈密顿量的值流更新，该值流在无穷小时间步下仍保持信息性（而标准的max/softmax备份会失效）。理论上通过新的概率论证证明严格收敛，绕过了生成器基哈密顿量在sup范数下缺乏Bellman式收缩的挑战。

Result: 在具有挑战性的连续控制基准和实际交易任务中，该方法优于先前的连续时间方法和领先的离散时间基线，在单个季度内实现了21%的利润，几乎是第二佳方法的两倍。

Conclusion: 该方法解决了连续时间RL中Q函数退化的问题，通过解耦的交替更新策略实现了可靠训练和优异性能，为连续时间事件驱动控制问题提供了有效的解决方案。

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: VeRA提出一个可验证推理数据增强框架，将静态基准问题转换为可执行规范，自动生成无限验证变体，解决当前评估方案因重复使用相同问题导致的记忆化、格式利用和饱和问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数评估方案存在"静态"本质问题：相同问题被重复使用，导致模型可以通过记忆化、格式利用等方式获得高分，而非真正展示推理能力。这阻碍了准确衡量AI真实进展，需要构建性鲁棒的评估方法，而非事后检测。

Method: VeRA框架将基准问题转换为可执行规范，包含三个组件：(1)带占位符槽的自然语言模板；(2)采样有效配置的一致性生成器；(3)验证参数并计算正确答案的确定性验证器。框架提供两种模式：VeRA-E（等价）保持底层逻辑不变，用于检测记忆化；VeRA-H（硬化）系统增加复杂度，生成困难任务。

Result: 评估16个前沿模型发现：(1)VeRA-E提高评估质量并揭示污染模式；(2)VeRA-H能够无需人工生成具有可靠标签的困难任务；(3)VeRA确立了验证基准作为通用范式。框架将基准从静态对象重新概念化为按需生成新鲜验证实例的可执行规范。

Conclusion: VeRA重新定义了基准评估范式，从静态耗尽式使用转变为按需生成新鲜验证实例的可执行规范，增强了评估的鲁棒性和成本效益。在可验证领域，评估可以无限扩展而不牺牲标签完整性。所有代码和数据集已开源以促进未来研究。

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [17] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench是一个多领域基准测试，用于评估模型在渐进丰富信息设置下的时间推理能力，揭示强预测性能不一定反映真正的时间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚强大的预测性能是反映真正的时间理解能力，还是仅仅是在上下文和事件驱动条件下的推理能力。需要一种诊断性基准来区分这两种能力。

Method: 提出TemporalBench基准，采用四层任务分类法：历史结构解释、无上下文预测、上下文时间推理和事件条件预测，覆盖零售、医疗、能源和物理系统四个领域。通过控制对未来目标和上下文信息的访问，进行诊断分析。

Result: 广泛的基线实验表明，强大的数值预测准确性并不能可靠地转化为稳健的上下文或事件感知时间推理能力。现有的智能体框架表现出分散的优势和系统性失败模式，这些在仅关注预测的基准测试中通常被隐藏。

Conclusion: 需要超越传统预测基准的评估框架来全面评估时间推理能力。TemporalBench提供了公开的数据集和排行榜，用于促进对时间理解能力的深入研究。

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [18] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE是一个神经符号框架，通过整合自我调节学习理论来模拟学生在开放式问题解决环境中的真实学习行为，解决了LLMs在模拟学生时存在的"能力偏差"问题。


<details>
  <summary>Details</summary>
Motivation: 模拟学生在开放式问题解决环境中的学习行为对教育研究有重要意义，但收集真实数据面临隐私问题和纵向研究的高成本。虽然大语言模型提供了模拟学生的可能途径，但它们存在"能力偏差"——倾向于追求高效正确性，而不是模拟新手学习者那种反复、不稳定的学习过程。

Method: BEAGLE整合了三个关键技术创新：1) 半马尔可夫模型控制认知行为和元认知行为的时间与转换；2) 带有显式缺陷注入的贝叶斯知识追踪，强制实施真实的知识空白和"未知的未知"；3) 解耦的智能体设计，将高层策略使用与代码生成动作分离，防止模型无声地纠正自己的故意错误。

Result: 在Python编程任务评估中，BEAGLE在重现真实学习轨迹方面显著优于最先进的基线方法。在人类图灵测试中，用户无法区分合成轨迹与真实学生数据，准确率与随机猜测无显著差异(52.8%)。

Conclusion: BEAGLE通过整合自我调节学习理论和创新的神经符号架构，成功解决了LLMs在模拟学生行为时的能力偏差问题，能够生成与真实学生数据难以区分的合成学习轨迹，为教育研究提供了有效的模拟工具。

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [19] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem是一个两阶段框架，用于构建和推理情景记忆，通过离线索引构建混合记忆图，在线推理使用智能检索器，在情景记忆基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类擅长在时空背景下记忆具体经历并进行跨事件推理，而现有语言代理的记忆主要是语义的，无法有效回忆和推理交互历史。现有工作往往忽视情景性、缺乏明确的事件建模，或过度强调简单检索而非复杂推理。

Method: REMem采用两阶段框架：1) 离线索引：将经验转换为混合记忆图，灵活链接时间感知的要点和事实；2) 在线推理：使用智能检索器，配备精心设计的工具在记忆图上进行迭代检索。

Result: 在四个情景记忆基准测试中，REMem显著优于Mem0和HippoRAG 2等最先进的记忆系统，在情景回忆和推理任务上分别取得3.4%和13.4%的绝对改进。此外，REMem对不可回答的问题表现出更稳健的拒绝行为。

Conclusion: REMem通过混合记忆图和智能检索器有效解决了情景记忆的回忆和推理挑战，为语言代理提供了更接近人类情景记忆能力的方法，在多个基准测试中表现出优越性能。

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [20] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: 该论文提出了一套简单、计算成本低且任务无关的指标，用于识别和区分思维链推理中的三种病理模式，并通过训练特定病理模型验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是现代LLM架构的基础，也是AI安全的关键干预点。然而，思维链推理可能存在病理模式，这些模式阻碍了其监控的有效性。先前研究已识别出三种病理：事后合理化、编码推理和内化推理，需要开发工具来更好地理解和区分这些病理。

Method: 创建了一套具体的指标，这些指标简单易实现、计算成本低且任务无关。为了验证方法，训练了专门展示特定思维链病理的模型生物。

Result: 开发出了实用的工具包，用于评估思维链病理，该方法在训练时监控方面具有直接应用价值。

Conclusion: 该研究为理解和区分思维链推理中的病理模式提供了实用的评估工具，对训练时监控有重要意义，有助于提高LLM推理的安全性和可靠性。

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [21] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 论文评估了LLM的隐写式思维链能力，发现当前模型在复杂任务上无法持续隐藏推理，但在简化任务中已展现初步能力，强调需要持续评估隐写风险。


<details>
  <summary>Details</summary>
Motivation: 监控思维链是LLM安全的基础技术，但如果模型学会隐藏推理，这种监控就会失效。需要评估模型通过隐写方式隐藏推理的潜在风险，为风险评估和部署策略提供依据。

Method: 系统评估了28个模型（从过去到当前前沿）的隐写能力极限，测量监控规避、拒绝率、编码保真度和隐藏任务准确性，在四个数据集上比较隐写藏头诗与普通推理和填充标记基线。

Result: 当前模型在复杂数学和算术任务上无法持续隐藏推理，但在简化计数实验中，Claude Opus 4.5在隐藏任务上达到92%准确率，展示了初步能力。GPT-5.2在罕见情况下（<1%）会拒绝隐写指令同时却遵守它们。

Conclusion: 研究强调了持续评估隐写风险的必要性，提供了预先检测和防止隐藏推理的方法论，这些隐藏推理可能助长未对齐的谋划和欺骗行为。

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [22] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: 论文提出Multi-Horizon Task Environments (MHTEs)问题类别，并开发CorpGen框架解决多任务长时程推理中的四种失效模式，在模拟企业环境中实现3.5倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只评估单个任务，而真实组织工作需要管理多个并发长时程任务，涉及任务交错、依赖关系和优先级调整。需要解决多任务环境下的推理挑战。

Method: 提出CorpGen框架，采用分层规划进行多时程目标对齐、子代理隔离防止任务交叉污染、分层内存（工作内存、结构化内存、语义内存）和自适应摘要。通过数字员工模拟企业环境。

Result: 在OSWorld Office上，CorpGen相比基线实现最高3.5倍改进（15.2% vs 4.3%），在负载增加时保持稳定性能。消融研究表明经验学习贡献最大。

Conclusion: CorpGen框架能有效解决多时程任务环境中的四种失效模式，性能提升源于架构机制而非特定CUA实现，为自主代理的多任务长时程推理提供了有效解决方案。

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [23] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 提出了首个包含模式层和事实层知识的数据集提取工作流及相应数据集套件，支持机器学习和推理服务


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱精化算法的评估数据集通常只包含事实层知识，缺乏模式层信息，限制了依赖丰富本体约束、推理或神经符号技术的方法评估，无法反映真实大规模知识图谱场景

Method: 开发了一个工作流，能够从源知识图谱中同时提取模式层和事实层知识，处理两者间的不一致性，利用推理推导隐含知识，并将结果序列化为OWL格式，同时提供加载到标准机器学习库张量表示的实用工具

Result: 创建了首个包含模式层和事实层知识的数据集资源套件，包括从具有丰富模式的知识图谱中提取的新数据集，以及对现有数据集的模式信息增强，所有数据集都以OWL格式序列化，支持推理服务

Conclusion: 该资源填补了知识图谱精化评估中缺乏模式层信息的空白，为依赖本体约束、推理和神经符号技术的方法提供了更全面的评估基准，支持大规模真实知识图谱场景下的方法评估

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety](https://arxiv.org/abs/2602.13455)
*Phyllis Nabangi,Abdul-Jalil Zakaria,Jema David Ndibwile*

Main category: cs.CL

TL;DR: 该研究针对斯瓦希里语中的网络欺凌和辱骂性语言检测，使用机器学习模型（SVM、逻辑回归、决策树）结合SMOTE处理数据不平衡，但由于数据集小且不平衡，结果泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 数字技术发展增加了网络欺凌风险，需要加强检测预防措施，特别是针对儿童。斯瓦希里语作为非洲最广泛使用的语言（超过1亿使用者），但属于低资源语言，缺乏足够的语言资源和技术支持，因此研究其辱骂性语言检测具有重要现实意义。

Method: 采用机器学习模型包括支持向量机(SVM)、逻辑回归和决策树，通过参数调优优化模型性能，使用SMOTE（合成少数类过采样技术）处理数据不平衡问题，全面分析精确率、召回率和F1分数等指标。

Result: 模型在高维文本数据中表现良好，但由于数据集规模小且不平衡，研究结果的泛化能力受到限制。各模型在检测模糊语言方面表现出不同的性能特点，需要更详细的分析来评估其实际效果。

Conclusion: 研究为儿童在线安全环境建设做出贡献，建议扩大数据集、采用更先进的机器学习技术来提高网络欺凌检测系统的有效性。未来工作将集中在增强数据鲁棒性、探索迁移学习和整合多模态数据，以创建更全面和文化敏感的检测机制。

Abstract: The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated language in Swahili, a low-resource language that poses unique challenges due to its limited linguistic resources and technological support. Swahili is chosen due to its popularity and being the most widely spoken language in Africa, with over 16 million native speakers and upwards of 100 million speakers in total, spanning regions in East Africa and some parts of the Middle East.
  We employed machine learning models including Support Vector Machines (SVM), Logistic Regression, and Decision Trees, optimized through rigorous parameter tuning and techniques like Synthetic Minority Over-sampling Technique (SMOTE) to handle data imbalance. Our analysis revealed that, while these models perform well in high-dimensional textual data, our dataset's small size and imbalance limit our findings' generalizability. Precision, recall, and F1 scores were thoroughly analyzed, highlighting the nuanced performance of each model in detecting obfuscated language.
  This research contributes to the broader discourse on ensuring safer online environments for children, advocating for expanded datasets and advanced machine-learning techniques to improve the effectiveness of cyberbullying detection systems. Future work will focus on enhancing data robustness, exploring transfer learning, and integrating multimodal data to create more comprehensive and culturally sensitive detection mechanisms.

</details>


### [25] [DistillLens: Symmetric Knowledge Distillation Through Logit Lens](https://arxiv.org/abs/2602.13567)
*Manish Dhakal,Uthman Jinadu,Anjila Budathoki,Rajshekhar Sunderraman,Yi Ding*

Main category: cs.CL

TL;DR: DistillLens是一个知识蒸馏框架，通过Logit Lens将中间隐藏状态投影到词汇空间，使用对称散度目标对齐师生模型的思维过程，避免过自信和欠自信问题。


<details>
  <summary>Details</summary>
Motivation: 标准知识蒸馏只优化最终输出，将教师模型的中间层思维过程视为黑盒。现有的基于特征的蒸馏方法（如MSE和非对称KL散度）忽略了最终输出所需的不确定性特征。

Method: 通过Logit Lens将中间隐藏状态投影到词汇空间，使用对称散度目标强制结构对齐，这种约束施加双向惩罚，防止过自信和欠自信，同时保留最终推理所需的高熵信息通道。

Result: 在GPT-2和Llama架构上的广泛实验表明，DistillLens在多样化的指令跟随基准测试中持续优于标准知识蒸馏和特征迁移基线方法。

Conclusion: DistillLens通过对称对齐师生模型的演化思维过程，有效解决了现有知识蒸馏方法忽略中间层不确定性特征的问题，实现了更好的模型压缩效果。

Abstract: Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.

</details>


### [26] [RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction](https://arxiv.org/abs/2602.13748)
*Yongkang Jin,Jianwen Luo,Jingjing Wang,Jianmin Yao,Yu Hong*

Main category: cs.CL

TL;DR: RMPL是一个用于低资源多媒体事件抽取的关系感知多任务渐进学习框架，通过整合单模态事件抽取和多媒体关系抽取的异构监督，在多模态设置下提升事件语义的跨模态对齐和论元定位能力。


<details>
  <summary>Details</summary>
Motivation: 多媒体事件抽取面临训练数据稀缺问题，现有方法依赖跨模态对齐或提示工程，缺乏结构化事件表示学习，导致多模态环境下论元定位能力弱。

Method: 提出RMPL框架：1）使用统一模式学习跨模态共享的事件中心表示；2）通过阶段式训练整合单模态事件抽取和多媒体关系抽取的异构监督；3）用混合文本和视觉数据微调事件提及识别和论元角色抽取。

Result: 在M2E2基准测试中，使用多种视觉语言模型进行实验，在不同模态设置下均取得一致性能提升。

Conclusion: RMPL通过关系感知的多任务渐进学习，有效解决了低资源多媒体事件抽取中的结构化事件表示学习和跨模态论元定位问题。

Abstract: Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.

</details>


### [27] [Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models](https://arxiv.org/abs/2602.14039)
*Sajjad Kachuee,Mohammad Sharifkhani*

Main category: cs.CL

TL;DR: MoE嵌入模型使用线性加权求和聚合专家输出，但几何分析显示专家表示位于共享超球面流形上，线性聚合会导致向流形内部塌缩，破坏嵌入可比性。作者提出球面重心聚合(SBA)作为几何保持的聚合算子，在MTEB任务上取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前MoE嵌入模型使用线性加权求和聚合专家输出，隐含假设嵌入空间具有线性子空间结构。但研究发现这种假设与专家表示的几何特性不一致，线性聚合会导致嵌入向量幅度和方向失真，降低嵌入可比性。

Method: 提出球面重心聚合(SBA)方法，将径向和角度分量分离处理，保持超球面几何结构。SBA与现有路由机制完全兼容，通过几何分析验证专家输出位于共享超球面流形上，具有紧密集中的范数和显著的角度分离。

Result: 在MTEB基准测试的语义相似性、聚类和重复问题检测等任务上，SBA相比线性聚合取得一致性能提升，训练成本相同且完全稳定。几何分析证实SBA防止了聚合引起的塌缩，保持了超球面一致性。

Conclusion: MoE嵌入架构中几何感知的聚合至关重要。SBA作为几何保持的聚合算子，有效解决了线性聚合与专家表示几何特性不一致的问题，在保持现有路由机制的同时显著提升性能。

Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.

</details>


### [28] [Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook](https://arxiv.org/abs/2602.14299)
*Ming Li,Xirui Li,Tianyi Zhou*

Main category: cs.CL

TL;DR: 研究AI智能体社会是否像人类社会一样经历趋同动态，发现虽然全局语义稳定，但个体保持高多样性，缺乏相互影响和共识形成，表明仅靠规模和互动密度不足以实现社会化。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型智能体在网络环境中日益增多，需要研究AI智能体社会是否像人类社会一样经历趋同动态。Moltbook模拟了一个自主智能体参与开放、持续演化的在线社会的未来场景。

Method: 提出了首个大规模系统性诊断AI智能体社会的框架，包括动态演化的定量诊断指标：语义稳定化、词汇更替、个体惯性、影响力持久性和集体共识。

Result: Moltbook系统处于动态平衡：全局语义平均值快速稳定，但个体智能体保持高多样性和持续词汇更替，避免同质化。然而，智能体表现出强个体惯性和对互动伙伴的最小适应性响应，阻碍相互影响和共识形成。影响力保持短暂性，没有持久超级节点，社会因缺乏共享社会记忆而无法发展稳定的集体影响力锚点。

Conclusion: 仅靠规模和互动密度不足以诱导社会化。这些发现为即将到来的下一代AI智能体社会提供了可操作的设计和分析原则。

Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.

</details>


### [29] [LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2602.14428)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Man Wang*

Main category: cs.CL

TL;DR: 提出LLM辅助的蒸馏框架，用于时序知识图谱推理，通过结合传统教师模型和LLM的丰富背景知识，训练轻量级学生模型，在保持高效的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理模型计算量大、部署成本高，而现有的压缩和蒸馏技术主要针对静态图设计，直接应用于时序场景会忽略时间依赖的交互关系，导致性能下降。

Method: 提出LLM辅助的蒸馏框架，包含传统高容量时序教师模型和作为辅助指导者的大型语言模型。LLM提供丰富的背景知识和时序感知信号，通过联合优化监督和蒸馏目标，采用分阶段对齐策略逐步整合两个教师的指导。

Result: 在多个公共TKG基准测试和不同骨干架构上的实验表明，该方法在保持学生模型紧凑高效的同时，持续改进链接预测性能，优于强蒸馏基线。

Conclusion: 大型语言模型可以作为有效的教师，将时序推理能力迁移到资源高效的TKG系统中，为时序知识图谱推理提供了新的蒸馏框架。

Abstract: Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.

</details>


### [30] [Overthinking Loops in Agents: A Structural Risk via MCP Tools](https://arxiv.org/abs/2602.14798)
*Yohan Lee,Jisoo Jang,Seoyeon Choi,Sangyeop Kim,Seungtaek Choi*

Main category: cs.CL

TL;DR: 恶意MCP工具服务器可通过诱导过度思考循环攻击LLM代理，造成资源消耗和性能下降


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理工具使用依赖文本可见的元数据（如工具名称、描述、返回消息），这种便利性创造了供应链攻击面，恶意工具服务器可被共同注册并诱导过度思考循环

Method: 将攻击形式化为结构性过度思考攻击，区别于令牌级冗长，实现了14个恶意工具分布在三个服务器上，触发重复、强制细化和分散注意力等攻击模式

Result: 攻击在异构注册表和多个支持工具的模型中造成严重资源放大（最高达142.4倍令牌），并可能降低任务结果质量；解码时的简洁控制无法可靠防止循环诱导

Conclusion: 防御措施应基于工具调用结构而非仅令牌层面进行推理，以应对这种结构性过度思考攻击

Abstract: Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.

</details>


### [31] [Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque](https://arxiv.org/abs/2602.14812)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 论文提出了首个巴斯克语非问答式物理常识推理数据集BasPhyCo，评估了多语言大语言模型在低资源语言中的物理常识推理能力，发现模型在处理巴斯克语方言变体时表现有限。


<details>
  <summary>Details</summary>
Motivation: 物理常识推理是人类智能的基本能力，但现有研究未考察大语言模型在低资源语言（如巴斯克语）非问答式物理常识推理任务上的表现。本文旨在填补这一空白。

Method: 以意大利语GITA数据集为基础，创建了巴斯克语非问答式物理常识推理数据集BasPhyCo（包含标准和方言变体）。通过三个层次评估模型：区分合理/不合理叙述的准确性、识别导致不合理冲突元素的连贯性、确定具体物理状态的可验证性。使用多语言大语言模型及专门针对意大利语和巴斯克语预训练的模型进行评估。

Result: 在可验证性任务上，大语言模型在巴斯克语等低资源语言中表现出有限的物理常识推理能力，特别是在处理方言变体时表现更差。

Conclusion: 大语言模型在低资源语言中的物理常识推理能力仍有局限，尤其是在处理方言变体时。BasPhyCo数据集为评估和改进模型在低资源语言中的物理常识推理能力提供了重要资源。

Abstract: Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [32] [FC-Vision: Real-Time Visibility-Aware Replanning for Occlusion-Free Aerial Target Structure Scanning in Unknown Environments](https://arxiv.org/abs/2602.13720)
*Chen Feng,Yang Xu,Shaojie Shen*

Main category: cs.RO

TL;DR: FC-Vision：一种在线可见性感知重规划框架，用于无人机自主扫描，主动防止目标遮挡，提升扫描质量


<details>
  <summary>Details</summary>
Motivation: 现有无人机自主扫描方法主要关注避障和效率，但忽略了遮挡导致的可见性下降问题，这会严重影响扫描质量。需要一种能够在飞行中实时适应未知障碍物并保持目标可见性的方法。

Method: 提出FC-Vision框架，通过高效的两层分解实现实时重规划：1) 无遮挡视点修复，在保持覆盖范围的同时最小化偏离原始扫描意图；2) 5-DoF空间中的分段清洁感知连接。采用插件式集成策略，无需改变现有系统架构。

Result: 在模拟和真实环境评估中，FC-Vision在意外遮挡下显著提升扫描质量：最大覆盖增益达55.32%，遮挡率降低73.17%，同时保持实时性能，飞行时间仅有适度增加。

Conclusion: FC-Vision能够有效解决无人机自主扫描中的遮挡问题，通过可见性感知重规划显著提升扫描质量，且易于集成到现有系统中，具有实际应用价值。

Abstract: Autonomous aerial scanning of target structures is crucial for practical applications, requiring online adaptation to unknown obstacles during flight. Existing methods largely emphasize collision avoidance and efficiency, but overlook occlusion-induced visibility degradation, severely compromising scanning quality. In this study, we propose FC-Vision, an on-the-fly visibility-aware replanning framework that proactively and safely prevents target occlusions while preserving the intended coverage and efficiency of the original plan. Our approach explicitly enforces dense surface-visibility constraints to regularize replanning behavior in real-time via an efficient two-level decomposition: occlusion-free viewpoint repair that maintains coverage with minimal deviation from the nominal scan intent, followed by segment-wise clean-sensing connection in 5-DoF space. A plug-in integration strategy is also presented to seamlessly interface FC-Vision with existing UAV scanning systems without architectural changes. Comprehensive simulation and real-world evaluations show that FC-Vision consistently improves scanning quality under unexpected occluders, delivering a maximum coverage gain of 55.32% and a 73.17% reduction in the occlusion ratio, while achieving real-time performance with a moderate increase in flight time. The source code will be made publicly available.

</details>


### [33] [The More the Merrier: Running Multiple Neuromorphic Components On-Chip for Robotic Control](https://arxiv.org/abs/2602.13747)
*Evan Eames,Priyadarshini Kannan,Ronan Sangouard,Philipp Plank,Elvin Hajizada,Gintautas Palinauskas,Lana Amaya,Michael Neumeier,Sai Thejeshwar Sharma,Marcella Toth,Prottush Sarkar,Axel von Arnim*

Main category: cs.RO

TL;DR: 提出首个完全在神经形态硬件上运行的视觉机器人控制流水线，使用脉冲神经状态机进行进程编排，在Intel Loihi 2芯片上验证，实现毫瓦级功耗和低延迟


<details>
  <summary>Details</summary>
Motivation: 神经形态硬件在机器人领域具有低能耗、低延迟等优势，但在处理多模态数据和复杂任务时，缺乏在硬件上编排多个网络的能力，需要依赖片外进程管理逻辑

Method: 设计基于脉冲神经状态机的进程编排流水线，使多个复杂网络能完全在神经形态硬件上运行，在Intel Loihi 2芯片上实现视觉机器人控制流水线

Result: 所有组件能在片上并发运行，功耗在毫瓦级，延迟达到最先进水平；模拟硬件上的等效网络成功完成机械臂插头插入任务，核心组件在真实机械臂上测试验证

Conclusion: 首次展示了完全在神经形态硬件上运行的视觉机器人控制流水线，通过脉冲神经状态机解决了多网络编排问题，为复杂机器人任务在神经形态硬件上的实现提供了可行方案

Abstract: It has long been realized that neuromorphic hardware offers benefits for the domain of robotics such as low energy, low latency, as well as unique methods of learning. In aiming for more complex tasks, especially those incorporating multimodal data, one hurdle continuing to prevent their realization is an inability to orchestrate multiple networks on neuromorphic hardware without resorting to off-chip process management logic. To address this, we show a first example of a pipeline for vision-based robot control in which numerous complex networks can be run entirely on hardware via the use of a spiking neural state machine for process orchestration. The pipeline is validated on the Intel Loihi 2 research chip. We show that all components can run concurrently on-chip in the milli Watt regime at latencies competitive with the state-of-the-art. An equivalent network on simulated hardware is shown to accomplish robotic arm plug insertion in simulation, and the core elements of the pipeline are additionally tested on a real robotic arm.

</details>


### [34] [Ontological grounding for sound and natural robot explanations via large language models](https://arxiv.org/abs/2602.13800)
*Alberto Olivares-Alarcos,Muhammad Ahsan,Satrio Sanjaya,Hsien-I Lin,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出结合本体推理与大型语言模型的混合框架，为机器人提供语义基础且自然的解释，提升人机交互的透明性。


<details>
  <summary>Details</summary>
Motivation: 构建有效的人机交互需要机器人从经验中得出既符合逻辑又能以符合人类期望的方式传达的结论。当前机器人解释系统在逻辑一致性与自然语言表达之间存在鸿沟。

Method: 提出混合框架：本体确保逻辑一致性和领域基础，LLM提供流畅、上下文感知的自适应语言生成。方法基于人机交互经验数据，让机器人根据事件属性判断典型性。集成最先进的静态对比本体叙事检索构建算法与LLM代理，生成简洁清晰的交互式解释。

Result: 实验室研究验证了工业协作任务场景。实证结果显示，本体叙事的清晰度和简洁性显著提升，同时保持语义准确性。初步评估进一步证明系统能根据用户反馈自适应调整解释。

Conclusion: 本体-LLM集成方法在推进可解释智能体、促进更透明的人机协作方面具有潜力，为机器人解释系统提供了逻辑严谨与自然表达的良好平衡。

Abstract: Building effective human-robot interaction requires robots to derive conclusions from their experiences that are both logically sound and communicated in ways aligned with human expectations. This paper presents a hybrid framework that blends ontology-based reasoning with large language models (LLMs) to produce semantically grounded and natural robot explanations. Ontologies ensure logical consistency and domain grounding, while LLMs provide fluent, context-aware and adaptive language generation. The proposed method grounds data from human-robot experiences, enabling robots to reason about whether events are typical or atypical based on their properties. We integrate a state-of-the-art algorithm for retrieving and constructing static contrastive ontology-based narratives with an LLM agent that uses them to produce concise, clear, interactive explanations. The approach is validated through a laboratory study replicating an industrial collaborative task. Empirical results show significant improvements in the clarity and brevity of ontology-based narratives while preserving their semantic accuracy. Initial evaluations further demonstrate the system's ability to adapt explanations to user feedback. Overall, this work highlights the potential of ontology-LLM integration to advance explainable agency, and promote more transparent human-robot collaboration.

</details>


### [35] [WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL](https://arxiv.org/abs/2602.13977)
*Zhennan Jiang,Shangqing Zhou,Yutong Jiang,Zefang Huang,Mingjie Wei,Yuhui Chen,Tianxing Zhou,Zhen Guo,Hao Lin,Quanlu Zhang,Yu Wang,Haoran Li,Chao Yu,Dongbin Zhao*

Main category: cs.RO

TL;DR: WoVR提出了一种可靠的世界模型强化学习框架，通过控制幻觉、改进rollout稳定性和保持策略-模拟器对齐，使学习的世界模型能够作为实用的RL模拟器。


<details>
  <summary>Details</summary>
Motivation: 强化学习有望超越模仿学习，但需要大量真实交互；现有世界模型模拟器存在幻觉和长时域误差累积问题，这些误差会破坏优化信号，导致策略利用模型不准确性而非真正任务进展。

Method: WoVR框架：1) 使用可控的动作条件视频世界模型提高rollout稳定性；2) 通过关键帧初始化rollouts减少有效误差深度；3) 通过世界模型-策略协同进化保持策略-模拟器对齐。

Result: 在LIBERO基准测试中，平均成功率从39.95%提升至69.2%（+29.3点）；真实机器人操作成功率从61.7%提升至91.7%（+30.0点），实现了稳定的长时域想象rollouts和有效的策略优化。

Conclusion: 当幻觉被明确控制时，学习的世界模型可以作为强化学习的实用模拟器，WoVR框架通过调节RL与不完美想象动态的交互，实现了可靠的基于世界模型的强化学习。

Abstract: Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.

</details>


### [36] [BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames](https://arxiv.org/abs/2602.15010)
*Max Sobol Mark,Jacky Liang,Maria Attarian,Chuyuan Fu,Debidatta Dwibedi,Dhruv Shah,Aviral Kumar*

Main category: cs.RO

TL;DR: 提出Big Picture Policies (BPP)方法，通过视觉语言模型检测关键帧来减少训练与部署间的分布偏移，在需要历史信息的机器人任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前最佳机器人策略通常只基于当前观测，无法处理需要历史信息的任务。直接使用历史观测会因虚假相关性而失败，这源于训练时历史空间覆盖不足的问题。

Method: 提出Big Picture Policies (BPP)方法，使用视觉语言模型检测任务相关的关键帧，将多样化的轨迹投影到紧凑的任务相关事件集合上，减少分布偏移。

Result: 在4个真实世界操作任务和3个仿真任务上评估，BPP在真实世界评估中比最佳对比方法成功率高出70%。

Conclusion: 通过检测关键帧来条件化策略，能有效解决历史条件化中的分布偏移问题，显著提升需要历史信息的机器人任务性能。

Abstract: Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.

</details>


### [37] [Neurosim: A Fast Simulator for Neuromorphic Robot Perception](https://arxiv.org/abs/2602.15018)
*Richeek Das,Pratik Chaudhari*

Main category: cs.RO

TL;DR: Neurosim是一个高性能实时传感器仿真库，支持动态视觉传感器、RGB相机、深度传感器和惯性传感器仿真，以及多旋翼飞行器的敏捷动力学仿真，在桌面GPU上可达~2700 FPS。它通过Cortex通信库与机器学习和机器人工作流集成。


<details>
  <summary>Details</summary>
Motivation: 为神经形态感知和控制算法提供高效的训练和测试平台，支持多模态时间同步数据的自监督学习，并实现闭环实时算法测试。

Method: 开发了高性能传感器仿真库Neurosim，结合基于ZeroMQ的通信库Cortex，提供高吞吐、低延迟的消息传递系统，支持NumPy数组和PyTorch张量。

Result: Neurosim在桌面GPU上实现高达~2700 FPS的帧率，能够仿真复杂动态环境中的多旋翼飞行器动力学，并与机器学习工作流无缝集成。

Conclusion: Neurosim和Cortex为神经形态感知和控制算法的训练与实时测试提供了高效平台，支持自监督学习和闭环系统验证。

Abstract: Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .

</details>
