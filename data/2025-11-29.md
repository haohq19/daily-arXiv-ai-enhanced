<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2511.20795)
*Souradeep Dutta,Keshav Bulia,Neena S Nair*

Main category: cs.CV

TL;DR: 本文对KRISP模型进行了轻量级复现，参数大幅减少但性能保持原模型的75%，揭示了原模型的设计缺陷和实际问题，并在资源受限条件下评估了知识增强VQA架构的可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 原KRISP模型虽然有效但计算需求大、参数多且与大型骨干网络紧密耦合，不适合资源受限环境。本文旨在重新审视该模型，开发轻量级版本以适应边缘设备部署。

Method: 通过系统消融研究，对KRISP进行轻量级复现，使用低参数配置并约束在外部知识图谱领域内，防止AI幻觉，在合成VQA数据和DAQUAR数据集上进行评估。

Result: 复现模型参数大幅减少，性能达到原模型的75%，能够在智能手机和AR-VR等边缘设备上运行，改善离线视觉推理能力。

Conclusion: 轻量级KRISP复现揭示了原模型的设计问题，证明了在资源受限条件下知识增强VQA架构的可行性，为边缘设备上的视觉推理应用提供了实用方案。

Abstract: Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.

</details>


### [2] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: Δ-NeRF是一个用于增量式NeRF精化的模块化残差框架，通过残差控制器、不确定性感知门控机制和视图选择策略，实现在不访问历史数据的情况下高效更新NeRF模型，显著减少训练时间和数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF框架在引入新视图时需要完全重新训练，这在卫星地形分析等数据顺序到达的领域存在局限性。增量式NeRF精化研究不足，简单方法在历史数据不可用时会出现灾难性遗忘问题。

Method: 提出Δ-NeRF框架，包含：1）残差控制器向冻结的基础NeRF注入逐层修正；2）不确定性感知门控机制自适应结合基础和精化预测；3）视图选择策略减少47%训练数据；4）知识蒸馏将增强模型压缩至原大小20%。

Result: 在卫星图像上，Δ-NeRF性能与联合训练相当，训练时间减少30-42%。相比简单微调，PSNR提升高达43.5%，在某些指标上甚至超过联合训练。

Conclusion: Δ-NeRF为增量式NeRF精化提供了有效解决方案，在保持性能的同时显著提升效率，适用于卫星地形分析等需要顺序数据处理的领域。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [3] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 提出Null-Text Test-Time Alignment (Null-TTA)方法，通过优化无分类器引导中的无条件嵌入来实现扩散模型的测试时对齐，防止奖励黑客问题，在保持跨奖励泛化能力的同时实现最优的目标测试时对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时对齐方法容易导致目标奖励函数欠优化或过优化（奖励黑客），需要一种更有效的方法来平衡目标奖励优化和泛化能力。

Method: Null-TTA通过优化无分类器引导中的无条件嵌入而非操纵潜在变量或噪声变量，在语义一致的流形上进行对齐，防止利用非语义噪声模式来提升奖励。

Result: Null-TTA实现了最先进的目标测试时对齐性能，同时保持了强大的跨奖励泛化能力。

Conclusion: 语义空间优化是测试时对齐的一种有效且原则性的新范式。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


### [4] [Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI](https://arxiv.org/abs/2511.20983)
*Al Amin,Kamrul Hasan,Liang Hong,Sharif Ullah*

Main category: cs.CV

TL;DR: 提出了一种结合Vision Transformers和同态加密的隐私保护联邦学习框架，用于安全的跨机构病理学分类，通过加密CLS令牌实现通信效率提升和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 医疗数据共享受隐私法规限制，传统联邦学习的梯度仍易受重建攻击泄露敏感信息，需要更安全的隐私保护方法。

Method: 使用Vision Transformers的CLS令牌作为紧凑特征表示，采用CKKS同态加密对CLS令牌进行加密后传输，实现安全聚合。

Result: 梯度易受模型反转攻击（PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741），而CLS保护的HE方法能防止此类攻击，加密数据传输仅需326 KB/轮，分类准确率在未加密域达96.12%，加密域达90.02%。

Conclusion: 该方法在保持强隐私保护的同时显著降低通信开销，为跨机构医疗协作学习提供了可行的隐私保护解决方案。

Abstract: Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.

</details>


### [5] [Scaling Foundation Models for Radar Scene Understanding](https://arxiv.org/abs/2511.21105)
*Pushkal Mishra,Kshitiz Bansal,Dinesh Bharadia*

Main category: cs.CV

TL;DR: 提出了RadarFM雷达基础模型，通过结构化空间语言监督学习统一的场景级表示，解决了现有雷达方法碎片化、任务特定化的问题。


<details>
  <summary>Details</summary>
Motivation: 雷达传感器在恶劣天气、光照和远距离条件下提供可靠的感知，但现有雷达方法碎片化且任务特定，每个下游任务使用不同的架构和训练目标，阻碍了跨任务迁移。

Method: 1) 结构化标题框架，在原生雷达坐标中编码车辆分布；2) 哈希感知对比学习目标，量化连续场景相似性而非二元匹配，实现细粒度空间推理；利用CARLA模拟器生成大规模标注雷达数据集。

Result: 提出了定位感知指标，评估超越传统检测措施的空间准确性。

Conclusion: RadarFM通过结构化空间语言监督学习统一场景表示，解决了雷达感知中的碎片化问题，为雷达基础模型的发展提供了新方向。

Abstract: Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.

</details>


### [6] [Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting](https://arxiv.org/abs/2511.21265)
*Juncheng Chen,Chao Xu,Yanjun Cao*

Main category: cs.CV

TL;DR: MatchGS是一个基于3D高斯泼溅的零样本图像匹配框架，通过几何校正和2D-3D表示对齐，生成高质量对应标签并提升匹配器性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅在几何精度和深度渲染偏差方面的问题，为学习型图像匹配提供大规模、多样且几何准确的训练数据。

Method: 1) 几何保真数据生成管道，精炼3DGS几何以产生精确对应标签；2) 2D-3D表示对齐策略，将3DGS的显式3D知识注入2D匹配器。

Result: 生成的对应标签将极线误差降低达40倍，在公共基准测试中零样本性能提升高达17.7%。

Conclusion: 通过适当的几何精炼，3DGS可作为可扩展、高保真且结构丰富的数据源，为新一代鲁棒零样本图像匹配器铺平道路。

Abstract: Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.

</details>


### [7] [Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure](https://arxiv.org/abs/2511.21337)
*Munish Rathee,Boris Bačić,Maryam Doborjeh*

Main category: cs.CV

TL;DR: SIFT-SNN框架：一种低延迟的神经形态信号处理管道，用于实时检测交通基础设施中的结构异常，结合SIFT特征提取和SNN分类，在奥克兰海港大桥数据集上达到92.3%准确率和9.5ms推理时间。


<details>
  <summary>Details</summary>
Motivation: 开发实时、低功耗的边缘部署系统，用于交通基础设施的结构安全监测，相比传统CNN方法提供更好的可解释性和空间特征保持能力。

Method: 集成尺度不变特征变换(SIFT)进行空间特征编码，使用延迟驱动的脉冲转换层和Leaky Integrate-and-Fire(LIF)脉冲神经网络(SNN)进行分类。

Result: 在包含6000个标记帧的奥克兰海港大桥数据集上，分类准确率达到92.3%(±0.8%)，每帧推理时间为9.5ms，脉冲活动稀疏度为8.1%。

Conclusion: SIFT-SNN框架实现了亚10毫秒延迟和低功耗边缘部署，增强了可解释性和透明决策，但未见现场条件的泛化能力仍需验证。

Abstract: This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.

</details>


### [8] [SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning](https://arxiv.org/abs/2511.21420)
*Futian Wang,Mengqi Wang,Xiao Wang,Haowen Wang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于SAM基础模型的遥感变化描述方法，通过提取区域级表示和注入感兴趣区域知识来提升变化描述性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化描述方法通常使用CNN/Transformer提取视觉表示或引入辅助任务，但存在区域感知能力弱和时间对齐有限的问题。

Method: 使用CNN/Transformer提取全局视觉特征，利用SAM基础模型分割语义和运动变化区域，构建知识图谱提供感兴趣对象信息，通过交叉注意力融合异质信息，最后用Transformer解码器生成自然语言描述。

Result: 在多个广泛使用的基准数据集上实现了最先进的性能。

Conclusion: 基于SAM基础模型的方法能有效提升遥感变化描述任务的性能，源代码将在GitHub上发布。

Abstract: Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning

</details>


### [9] [E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework](https://arxiv.org/abs/2511.21422)
*Adeela Islam,Stefano Fiorini,Manuel Lecha,Theodore Tsesmelis,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: E-M3RF是一个等变多模态3D重组框架，通过结合几何和颜色特征来解决传统仅依赖几何特征方法在几何信息不足或模糊时的局限性，使用SE(3)流匹配预测碎片重组所需的变换。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的3D重组方法主要依赖几何特征，在几何信息不足（如碎片小、腐蚀或对称）时表现不佳，且缺乏防止重叠组装的物理约束。

Method: 使用旋转等变编码器提取几何特征，Transformer提取颜色特征，形成多模态表示，通过SE(3)流匹配预测碎片变换。

Result: 在四个数据集（包括合成和真实文化遗产数据集）上测试，在RePAIR数据集上相比竞争方法旋转误差降低23.1%，平移误差降低13.2%，Chamfer距离减少18.4%。

Conclusion: E-M3RF通过结合几何和颜色特征的多模态方法，显著提升了3D碎片重组的准确性，特别是在几何信息模糊的情况下。

Abstract: 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.

</details>


### [10] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang,Fan Zhang,Xiao Wang,Mengqi Wang,Dexing Huang,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种基于超图的时空事件流补全机制，通过超图连接不同时间和空间位置的事件标记，利用上下文信息传递来补全稀疏事件，并能灵活整合RGB标记实现多模态信息补全。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生的异步事件流在空间上稀疏但时间上密集，现有方法使用事件帧、体素或张量作为输入，但难以解决空间稀疏性导致的欠采样问题。

Method: 使用超图连接不同时间和空间位置的事件标记，通过上下文信息传递补全稀疏事件；可整合RGB标记实现多模态超图信息补全；通过自注意力聚合不同时间步的超图节点信息。

Result: 在单标签和多标签事件分类任务上的大量实验充分验证了所提框架的有效性。

Conclusion: 提出的超图引导时空事件流补全机制能够有效解决事件流空间稀疏性问题，实现多模态特征的有效学习和融合。

Abstract: Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding](https://arxiv.org/abs/2511.20696)
*Dan Li,Hye-Bin Shin,Yeon-Woo Choi*

Main category: cs.LG

TL;DR: 提出了ProNECL框架，通过原型引导的非示例持续学习来解决跨个体EEG解码中的灾难性遗忘问题，无需存储历史EEG样本。


<details>
  <summary>Details</summary>
Motivation: 由于EEG信号在个体间存在显著变异性，在持续EEG解码任务中，随着新受试者的引入，先前获得的知识往往会被覆盖。现有方法主要依赖存储历史数据作为回放缓冲区，但这存在隐私问题和内存限制。

Method: ProNECL构建类级原型来总结每个受试者的判别性表示，并通过跨受试者特征对齐和知识蒸馏，增量地将新特征空间与全局原型记忆对齐。

Result: 在BCI Competition IV 2a和2b数据集上验证，该框架有效平衡了知识保留和适应性，在跨受试者持续EEG解码任务中实现了优越性能。

Conclusion: ProNECL框架能够在无需访问历史EEG样本的情况下有效保留先验知识，为持续EEG解码提供了一种实用的解决方案。

Abstract: Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.

</details>


### [12] [ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training](https://arxiv.org/abs/2511.20718)
*Chenliang Li,Adel Elmahdy,Alex Boyd,Zhongruo Wang,Alfredo Garcia,Parminder Bhatia,Taha Kass-Hout,Cao Xiao,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了两种稳定多轮对话中PPO训练的技术：轮级重要性采样和裁剪偏差校正，解决了token级PPO在LLM训练中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: PPO在多轮对话和推理任务中训练LLM时性能不稳定且容易崩溃，主要原因是token级重要性采样与多轮环境结构不匹配，以及离策略样本产生的高方差优势估计。

Method: 提出了两种互补的稳定技术：1) 轮级重要性采样，使优化与多轮推理的自然结构对齐；2) 裁剪偏差校正，通过降低不可靠离策略样本的权重来标准化梯度。组合得到三种变体：Turn-PPO、S-PPO和ST-PPO。

Result: 在通用QA、多跳QA和医疗选择题基准上的实验表明，ST-PPO和S-PPO能防止大模型训练中的性能崩溃，保持较低的裁剪比率，并获得比标准token级PPO更高的任务性能。

Conclusion: 结合轮级重要性采样和裁剪偏差校正为稳定多轮LLM智能体训练提供了实用且可扩展的解决方案。

Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.

</details>


### [13] [Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge](https://arxiv.org/abs/2511.20726)
*Yuhang Wang,Heye Huang,Zhenhua Xu,Kailai Sun,Baoshen Guo,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出了一种结合条件变分自编码器(CVAE)和大语言模型(LLM)的高保真场景生成框架，用于生成罕见长尾事件和复杂多智能体交互场景，以增强自动驾驶系统的安全验证能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在罕见长尾事件和复杂多智能体交互方面面临关键挑战，这些场景在真实世界数据中稀缺但对鲁棒安全验证至关重要。

Method: CVAE编码历史轨迹和地图信息学习潜在交通结构，生成物理一致的基础场景；LLM作为对抗推理引擎，将非结构化场景描述解析为领域特定损失函数，动态指导不同风险级别的场景生成。

Result: 在CARLA和SMARTS中的实验表明，该框架显著增加了高风险和长尾事件的覆盖率，改善了模拟与真实交通分布的一致性，暴露了比现有方法更具挑战性的交互场景。

Conclusion: 为安全验证建立了新途径，能够在罕见但重要的事件下对自动驾驶系统进行原则性压力测试。

Abstract: Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.

</details>


### [14] [A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems](https://arxiv.org/abs/2511.21032)
*Yuxuan Zhu,Cong Fu,Yabo Ni,Anxiang Zeng,Yuan Fang*

Main category: cs.LG

TL;DR: 提出了ELBO$_{TDS}$概率框架，通过数据增强和因果图建模解决推荐系统中的时间分布偏移问题，已在Shopee产品搜索中成功部署。


<details>
  <summary>Details</summary>
Motivation: 时间分布偏移会侵蚀推荐系统的长期准确性，而现有的周期性增量训练方法难以同时捕捉稳定和瞬态模式，现有方法如不变学习和自监督学习存在时间泛化不稳定、表示崩溃或数据利用效率低等问题。

Method: 1. 通过真实生产数据的统计分析识别关键偏移因素，设计有效的数据增强策略重新采样这些时变因素以扩展训练支持；2. 使用因果图建模时序推荐场景，基于因果结构推导出自监督变分目标ELBO$_{TDS}$。

Result: 大量实验表明该方法实现了优越的时间泛化能力，每个用户的GMV提升了2.33%，并已在Shopee产品搜索中成功部署。

Conclusion: ELBO$_{TDS}$框架能够有效解决推荐系统中的时间分布偏移问题，在保持表示质量的同时提升模型的时间泛化性能。

Abstract: Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.

</details>


### [15] [Efficient Diffusion Planning with Temporal Diffusion](https://arxiv.org/abs/2511.21054)
*Jiaming Guo,Rui Zhang,Zerun Li,Yunkai Gao,Shaohui Peng,Siming Lan,Xing Hu,Zidong Du,Xishan Zhang,Ling Li*

Main category: cs.LG

TL;DR: TDP通过将去噪步骤分布在时间维度上，提高决策效率。它生成一个随时间逐渐模糊的初始计划，并在后续时间步用少量去噪步骤更新计划，而不是生成全新计划。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划方法在每个时间步生成新计划，导致计算开销大、决策频率低。受人类制定短期详细计划和长期模糊计划的启发，希望提高决策效率。

Method: 提出时间扩散规划器(TDP)，生成随时间逐渐模糊的初始计划，后续时间步用少量去噪步骤更新计划而非重新生成，并引入自动重规划机制防止计划与现实偏差过大。

Result: 在D4RL基准测试中，相比每时间步生成新计划的方法，TDP将决策频率提高了11-24.8倍，同时达到更高或相当的性能。

Conclusion: TDP通过时间维度分布去噪步骤，显著提高了扩散规划的决策效率，同时保持或提升了性能表现。

Abstract: Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.

</details>


### [16] [Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning](https://arxiv.org/abs/2511.21075)
*Zhenchao Tang,Fang Wang,Haohuai He,Jiale Zhou,Tianxu Lv,Jun Zhu,Shouzhi Chen,Minghao Yang,Yu Wang,Jiayang Wu,Yidong Song,Jianhua Yao*

Main category: cs.LG

TL;DR: 提出平衡微调(BFT)方法，通过双层加权机制解决生物医学领域稀疏数据下的复杂推理问题，无需外部奖励信号即可有效学习专业知识


<details>
  <summary>Details</summary>
Motivation: 现有方法在生物医学领域存在局限性：标准监督微调容易过拟合表面指令模式而无法内化碎片化科学知识；强化学习因需要实验验证奖励而不可行

Method: BFT采用双层加权机制：1) 令牌级别通过预测概率缩放损失来稳定梯度防止过拟合；2) 样本级别使用"最小组置信度"自适应增强困难样本学习

Result: BFT显著优于标准SFT，在医学任务中学习到SFT遗漏的知识，在生物学任务中超越GeneAgent，其文本嵌入可直接用于下游任务如基因相互作用和单细胞扰动响应预测

Conclusion: BFT促进了LLMs在生物医学研究中的广泛应用，能够从稀疏数据中有效学习复杂推理

Abstract: Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses "minimum group confidence" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.

</details>


### [17] [Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination](https://arxiv.org/abs/2511.21118)
*Pius Onobhayedo,Paul Osemudiame Oamen*

Main category: cs.LG

TL;DR: 本文提出了一种解决联邦学习系统中关键问题的框架，包括聚合问责、激励机制、可扩展性和治理问题，通过密码学收据、几何新颖性测量、并行对象所有权和时间锁定策略来实现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然承诺让数十亿边缘设备能够在不泄露原始数据的情况下共同改进模型，但当前存在聚合器缺乏问责、经济机制易被操纵、协调机制限制可扩展性以及治理允许追溯性操纵等问题。

Method: 使用密码学收据证明聚合正确性，几何新颖性测量防止激励操纵，并行对象所有权实现线性可扩展性，时间锁定策略检查追溯性操纵。

Result: 提出的方法解决了联邦学习系统中的关键组成差距，实现了更安全、可扩展和公平的分布式AI训练环境。

Conclusion: 通过密码学和机制设计的结合，为分布式AI训练提供了可行的解决方案，使民主化的联邦学习愿景更接近现实。

Abstract: Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.

</details>


### [18] [Privacy in Federated Learning with Spiking Neural Networks](https://arxiv.org/abs/2511.21181)
*Dogukan Aksu,Jesus Martinez del Rincon,Ihsen Alouani*

Main category: cs.LG

TL;DR: 本文首次系统研究了脉冲神经网络(SNNs)中的梯度泄露问题，发现与传统人工神经网络(ANNs)相比，SNNs的梯度重构效果显著更差，显示出更强的隐私保护潜力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)中的梯度反转攻击是严重的隐私威胁，但这一漏洞在SNNs中的影响尚未被充分探索。SNNs的非可微分性和替代梯度训练可能使其梯度信息量减少。

Method: 将不同的梯度泄露攻击方法适配到脉冲域，在多种数据领域进行实证研究，比较SNNs和ANNs的梯度重构效果。

Result: 实验结果显示：ANN梯度能可靠暴露显著输入内容，而SNN梯度只能产生噪声大、时间不一致的重构，无法恢复有意义的空间或时间结构。

Conclusion: 事件驱动动态和替代梯度训练的结合显著降低了梯度的信息量，突触计算具有固有的隐私保护潜力。

Abstract: Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.

</details>


### [19] [An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids](https://arxiv.org/abs/2511.21590)
*Muhammad Siddique,Sohaib Zafar*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的智能电网数字取证框架，该框架部署在云端，结合数据采集、认证通信、云存储和自动化取证分析，用于实时异常检测、事件重建和入侵分析。


<details>
  <summary>Details</summary>
Motivation: 智能电网融合了传统电力基础设施和先进通信网络，这种集成带来了可能破坏电网稳定性和可靠性的漏洞，需要数字取证技术来识别、检测和缓解安全事件。

Method: 开发了一个综合的机器学习数字取证框架，使用监督和无监督学习算法（如随机森林、支持向量机、梯度提升树和深度神经网络），结合传感器级数据采集、认证通信和可扩展云存储。

Result: 通过对实时智能电表数据流的模拟和实验研究，该框架在应对数据篡改、虚假数据注入和协调控制回路操纵等网络攻击时表现出高准确性、可扩展性和弹性。

Conclusion: 云服务是大数据驱动取证工作流程的最佳骨干，使能源公用事业能够实现快速态势感知和智能事件响应。

Abstract: Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.

</details>


### [20] [DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving](https://arxiv.org/abs/2511.21669)
*Fengze Yu,Leshu Li,Brad McDanel,Saiqian Zhang*

Main category: cs.LG

TL;DR: DSD是一个分布式推测解码框架，通过协调草稿-目标执行将推测解码扩展到多设备部署，解决了LLM推理中的高延迟和异构边缘-云环境可扩展性限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理存在高解码延迟和在异构边缘-云环境中可扩展性有限的问题。现有的推测解码技术虽然能加速令牌生成，但仅限于单节点执行。

Method: 提出DSD分布式推测解码框架，通过协调草稿-目标执行实现多设备部署。首先开发DSD-Sim离散事件模拟器来捕捉网络、批处理和调度动态，然后基于模拟器洞察设计自适应窗口控制策略来动态调整推测窗口大小。

Result: 在多样化工作负载上的实验表明，DSD相比现有SD基线实现了最高1.1倍的加速和9.7%的吞吐量提升。

Conclusion: DSD能够在边缘和云环境中实现敏捷且可扩展的LLM服务。

Abstract: Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术从NASA航天器模拟发展而来，在医疗领域通过整合影像、生物传感器和计算模型创建患者特异性虚拟副本，用于诊断、治疗规划和药物开发，面临互操作性、数据隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 将数字孪生技术从工业领域引入医疗，旨在实现从被动治疗向预测性、预防性和个性化医疗的转变。

Method: 整合医学影像、生物传感器和计算模型，创建动态数据驱动的患者特异性虚拟副本，支持双向交互和实时更新。

Result: 代表性应用包括心脏数字孪生预测心律失常治疗结果、肿瘤数字孪生跟踪肿瘤进展优化放疗、药理学数字孪生加速药物发现。

Conclusion: 未来需在多器官数字孪生、基因组学整合和伦理治理方面取得进展，以充分发挥数字孪生在医疗转型中的潜力。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [22] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 提出了ICPO方法，通过利用LLM生成不同响应的概率来反映其推理过程的自评估，结合偏好优势分数和可验证奖励来提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在奖励粒度粗、奖励噪声和探索效率低等问题，导致训练不稳定和熵崩溃。

Method: ICPO通过计算同一输入提示下多个响应的相对生成概率来获得偏好优势分数，并将其与可验证奖励结合指导探索过程。

Result: 在四个通用领域基准和三个数学基准上的实验表明，ICPO相比GRPO能稳定提升推理能力。

Conclusion: ICPO能有效缓解奖励粒度粗和噪声问题，抑制过度自信错误，增强被低估高质量响应的相对优势，防止模型过拟合特定策略，促进更彻底的探索。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [23] [EWE: An Agentic Framework for Extreme Weather Analysis](https://arxiv.org/abs/2511.21444)
*Zhe Jiang,Jiong Wang,Xiaoyu Yue,Zijie Guo,Wenlong Zhang,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.AI

TL;DR: EWE是首个用于极端天气自动诊断推理的智能代理框架，通过模拟专家工作流程，从原始气象数据自主生成和解释多模态可视化，实现全面诊断分析。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件风险加剧，但传统专家驱动的人工诊断范式存在分析瓶颈，阻碍科学进展。AI在地球科学预测方面取得进展，但自动诊断推理这一同等重要的挑战尚未探索。

Method: EWE通过知识引导规划、闭环推理和领域定制气象工具包模拟专家工作流程，从原始气象数据自主生成和解释多模态可视化。

Result: 开发了该领域的首个基准测试，包含103个高影响事件的精选数据集和新的逐步评估指标。

Conclusion: EWE标志着向自动科学发现迈出的一步，有望民主化专业知识和智力资源，特别是对易受极端天气影响的发展中国家。

Abstract: Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.

</details>
