<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: 提出LVLMs-Saliency框架，通过融合注意力权重和输入梯度来量化视觉基础强度，发现幻觉常出现在前序token对下一token预测的显著性较低时，并提出两种推理时机制来缓解幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖前向注意力模式，忽略了梯度信号，无法可靠区分幻觉和事实基础输出。需要结合梯度信息来更好理解token在网络中的影响传播。

Method: 提出LVLMs-Saliency梯度感知诊断框架，融合注意力权重和输入梯度量化视觉基础强度。基于分析发现，提出两种推理时机制：显著性引导拒绝采样（SGRS）动态过滤候选token；局部一致性增强（LocoRE）模块加强当前token对最近前序token的注意力。

Result: 在多个LVLM上的实验表明，该方法显著降低幻觉率，同时保持流畅性和任务性能，提供了增强模型可靠性的鲁棒且可解释的解决方案。

Conclusion: LVLMs-Saliency框架通过梯度感知分析揭示了幻觉产生的关键模式，提出的推理时机制能有效缓解幻觉问题，为提升大视觉语言模型的可靠性提供了新思路。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [2] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: Li-ViP3D++：基于查询的多模态感知与预测框架，通过查询门控可变形融合（QGDF）在查询空间集成多视角RGB和LiDAR数据，实现端到端的检测、跟踪和轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有模块化感知预测流程存在信息流受限和误差放大问题，而现有的基于查询的端到端模型在相机和LiDAR融合方面探索不足，通常使用启发式对齐和离散选择步骤，导致信息利用不充分和偏差引入。

Method: 提出查询门控可变形融合（QGDF）：1）通过掩码注意力跨相机和特征层聚合图像证据；2）通过带学习偏移量的完全可微分BEV采样提取LiDAR上下文；3）应用查询条件门控自适应加权每个智能体的视觉和几何线索。架构联合优化检测、跟踪和多假设轨迹预测。

Result: 在nuScenes数据集上，Li-ViP3D++提升了端到端行为质量和检测质量：EPA达到0.335，mAP达到0.502，显著降低误报率（FP ratio 0.147），且比前代Li-ViP3D更快（139.82 ms vs. 145.91 ms）。

Conclusion: 查询空间中的完全可微分相机-LiDAR融合能够在不牺牲部署性的情况下提高端到端感知预测的鲁棒性，证明了多模态融合在查询空间的有效性。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: 提出Oculomix分层采样策略，用于视网膜图像混合样本增强，通过约束混合空间到患者和检查层级，更好地保留患者特异性特征，在心血管事件预测中优于传统图像级增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像级混合样本增强方法（如CutMix、MixUp）会扰动患者特异性属性（如医疗共病和临床因素），因为它们只考虑图像和标签，不适合需要保留患者特定特征的医学影像分析。

Method: 提出Oculomix分层采样策略，基于两个临床先验：1）同一患者同一时间点采集的图像共享相同属性；2）同一患者不同时间点采集的图像具有软时间趋势（发病率随时间增加）。该方法将混合空间约束到患者和检查层级，利用分层关系保留患者特异性特征。

Result: 在大型多种族人群（Alzeye）中验证，用于预测5年主要不良心血管事件（MACE）。Oculomix在AUROC上比图像级CutMix和MixUp提升高达3%，证明该方法在眼组学中的必要性和价值。

Conclusion: Oculomix通过分层采样策略解决了传统混合样本增强在医学影像中的局限性，更好地保留患者特异性特征，在心血管事件预测任务中表现优异，为眼组学应用提供了更有效的增强方法。

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [4] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: 提出PIL方法，使用线性代理模型生成扰动，实现高效数据保护，同时揭示不可学习示例通过诱导模型线性化发挥作用


<details>
  <summary>Details</summary>
Motivation: 现有不可学习示例方法依赖深度神经网络作为代理模型生成扰动，计算成本高，需要更高效的方法

Method: 提出Perturbation-Induced Linearization (PIL)方法，仅使用线性代理模型生成扰动，大幅降低计算成本

Result: PIL达到或超越现有基于代理模型方法的性能，同时显著减少计算时间，并揭示不可学习示例通过诱导深度模型线性化发挥作用

Conclusion: PIL提供了实用的数据保护方法，同时揭示了不可学习示例有效性的机制，为理解数据保护方法提供了新视角

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [5] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出Domain Expansion框架，通过正交池化机制构建多任务正交子空间，解决多目标训练中的潜在表示崩溃问题


<details>
  <summary>Details</summary>
Motivation: 多目标训练中，不同任务的梯度冲突会导致共享表示退化，陷入对所有任务都次优的妥协状态，即潜在表示崩溃问题

Method: 提出Domain Expansion框架，使用新颖的正交池化机制构建潜在空间，为每个目标分配相互正交的子空间

Result: 在ShapeNet、MPIIGaze和Rotated MNIST等多个基准测试中验证，该框架不仅防止表示崩溃，还产生可解释、可组合的潜在空间

Conclusion: 通过正交子空间结构有效解决多目标训练中的表示冲突问题，实现可解释、可操作的潜在表示

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [6] [Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models](https://arxiv.org/abs/2601.20367)
*Qing Lyu,Zhe Fu,Alexandre Bayen*

Main category: cs.LG

TL;DR: 提出基於多智能體Transformer的無監督異常檢測框架，用於識別自動駕駛中的安全關鍵場景，通過預測殘差建模正常駕駛並檢測偏差，在NGSIM數據集上驗證有效性。


<details>
  <summary>Details</summary>
Motivation: 自動駕駛中安全關鍵場景稀少，監督標註不現實；傳統基於規則的指標過於簡單，無法捕捉複雜交互風險；現有方法缺乏系統性驗證統計異常是否真正反映物理危險。

Method: 使用多智能體Transformer建模正常駕駛行為，通過預測殘差測量偏差；提出雙重評估方案：穩定性（Kendall秩相關係數和Jaccard指數）和物理對齊性（與替代安全指標的相關性）。

Result: 在NGSIM數據集上，最大殘差聚合器實現最高物理對齊性同時保持穩定性；識別出388個被傳統方法遺漏的獨特異常，捕捉到如橫向漂移下的反應制動等細微多智能體風險；異常聚類為四種可解釋風險類型。

Conclusion: 提出的無監督異常檢測框架能有效識別自動駕駛中的安全關鍵場景，彌補了傳統方法的不足，為仿真和測試提供可操作的見解。

Abstract: Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.

</details>


### [7] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: 提出RARL框架，通过强化学习显式学习序数依赖关系，结合回归和排序任务，使用排序感知奖励和响应变异操作提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以建模序数回归和排序中的序数依赖关系，需要一种能够显式学习这些关系的新框架。

Method: 提出RARL框架：1) 统一目标函数协同整合回归和排序学习；2) 排序感知可验证奖励联合评估回归精度和排序准确性；3) 响应变异操作注入受控噪声改善探索；4) 通过策略优化直接更新模型。

Result: 在三个不同基准测试上进行了广泛实验验证了RARL的有效性。

Conclusion: RARL成功解决了序数依赖建模问题，通过强化学习框架实现了回归和排序任务的协同改进。

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [8] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: MORPHIN是一个自适应的Q学习框架，能够在非平稳环境中动态调整学习参数，适应奖励函数变化和动作空间扩展，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在现实应用中面临非平稳环境的挑战，特别是当奖励函数变化或动作空间扩展时，传统方法需要完全重新训练，效率低下。

Method: MORPHIN框架集成了概念漂移检测机制，能够动态调整学习和探索超参数，适应奖励函数变化和动作空间扩展，同时保留先前的策略知识。

Result: 在Gridworld基准和交通信号控制模拟中，MORPHIN相比标准Q学习基线实现了1.7倍的学习效率提升，具有更快的收敛速度和持续的适应能力。

Conclusion: MORPHIN框架为强化学习在非平稳环境中的应用提供了有效的解决方案，能够在动态变化中保持学习效率和策略稳定性。

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [9] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: 提出Cox-MT模型，结合半监督学习的Mean Teacher框架，利用标记和未标记数据训练ANN-based Cox模型，显著提升癌症预后预测性能


<details>
  <summary>Details</summary>
Motivation: 传统基于人工神经网络的Cox比例风险模型需要大量标记样本，但实际应用中标记数据有限，限制了模型性能。需要解决标记数据不足的问题。

Method: 采用深度半监督学习方法，基于Mean Teacher框架开发单模态和多模态ANN-based Cox模型（Cox-MT），利用标记和未标记数据进行训练。

Result: 在TCGA数据集上，单模态Cox-MT模型（使用RNA-seq或全切片图像）在四种癌症类型上显著优于现有Cox-nnet模型；随着未标记样本增加，性能显著提升；多模态Cox-MT模型性能优于单模态模型。

Conclusion: Cox-MT模型能有效利用标记和未标记数据，相比仅使用标记数据的现有ANN-based Cox模型，显著提高了预测准确性。

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: 该论文基于2025年8月研讨会，探讨神经科学与人工智能的交叉领域，提出NeuroAI概念，旨在通过神经科学启发改进AI算法，同时深化对生物神经计算的理解。


<details>
  <summary>Details</summary>
Motivation: 神经科学与人工智能近年来各自取得显著进展，但两者之间的连接仍然松散。论文旨在识别这两个领域当前和未来的协同机会，促进更紧密的交叉融合。

Method: 基于2025年8月举行的研讨会，聚焦于具身性、语言与通信、机器人学、人类与机器学习、神经形态工程等子领域，收集多位领先研究人员的个人观点，并附上研究人员和学员的SWOT分析。

Result: 识别了神经科学与AI之间的协同领域，提出了NeuroAI概念——一种神经科学启发的人工智能，认为这种交叉融合既能显著提升AI算法的范围和效率，又能改变对生物神经计算的理解方式。

Conclusion: 倡导发展NeuroAI，这种神经科学启发的人工智能具有巨大潜力，需要继续探索神经科学与AI之间的协同作用，同时通过SWOT分析评估了NeuroAI的益处和风险。

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [CascadeMind at SemEval-2026 Task 4: A Hybrid Neuro-Symbolic Cascade for Narrative Similarity](https://arxiv.org/abs/2601.19931)
*Sebastien Kawada,Dylan Holyoak*

Main category: cs.CL

TL;DR: 提出混合神经符号系统用于叙事故事相似性任务，结合神经自一致性投票与多尺度叙事分析集成作为平局决胜器


<details>
  <summary>Details</summary>
Motivation: 解决叙事故事相似性评估中的模糊案例，通过选择性延迟到符号方法来增强神经预测能力

Method: 级联架构：神经组件使用大语言模型进行多轮自一致性投票，达到超多数阈值时做出决策；完美平局时，符号集成结合五种叙事相似性信号（词汇重叠、语义嵌入、故事语法结构、事件链对齐、叙事张力曲线）作为最终决策

Result: 在开发集上达到81%准确率，证明选择性延迟到符号方法可以增强神经预测在真正模糊的叙事比较中的表现

Conclusion: 混合神经符号系统通过结合神经投票和符号分析集成，有效处理叙事相似性评估中的模糊案例，展示了选择性延迟策略的价值

Abstract: We present a hybrid neuro-symbolic system for the SemEval-2026 Task 4 on Narrative Story Similarity. Our approach combines neural self-consistency voting with a novel Multi-Scale Narrative Analysis Ensemble that operates as a symbolic tiebreaker. The neural network component uses a large language model with multiple parallel votes, applying a supermajority threshold for confident decisions and escalating uncertain cases to additional voting rounds. When votes result in a perfect tie, a symbolic ensemble combining five narrative similarity signals (lexical overlap, semantic embeddings, story grammar structure, event chain alignment, and narrative tension curves) provides the final decision. Our cascade architecture achieves 81% accuracy on the development set, demonstrating that selective deferral to symbolic methods can enhance neural predictions on genuinely ambiguous narrative comparisons.

</details>


### [12] [SERA: Soft-Verified Efficient Repository Agents](https://arxiv.org/abs/2601.20789)
*Ethan Shen,Danny Tormoen,Saurabh Shah,Ali Farhadi,Tim Dettmers*

Main category: cs.CL

TL;DR: SERA是一种高效训练代码代理的方法，通过监督微调实现开源模型最佳性能，成本比强化学习低26倍，比合成数据方法低57倍，可针对私有代码库进行专业化训练。


<details>
  <summary>Details</summary>
Motivation: 开源权重代码代理相对于闭源系统应具有根本优势：能够针对私有代码库进行专业化训练，将仓库特定信息直接编码到权重中。但由于训练成本和复杂性，这一优势一直停留在理论层面。

Method: 提出Soft-Verified Efficient Repository Agents (SERA)方法，使用监督微调(SFT)和Soft Verified Generation (SVG)技术。SVG能从单个代码仓库生成数千条轨迹，结合成本效益实现私有代码库专业化。

Result: SERA在完全开源模型中达到最先进性能，匹配Devstral-Small-2等前沿开源权重模型。训练成本比强化学习低26倍，比先前合成数据方法低57倍。生成超过20万条合成轨迹用于详细分析。

Conclusion: SERA使针对私有代码库的专业化训练变得实用，将加速开源代码代理研究，展示开源模型在私有代码库专业化方面的优势。发布SERA作为Ai2开源代码代理系列的首个模型。

Abstract: Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [13] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: STORM是一个轻量级的对象中心适配模块，通过语义感知的槽位增强冻结的视觉基础模型，用于机器人操作任务，采用多阶段训练策略提升泛化能力和控制性能。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型为机器人提供强大的感知特征，但其密集表示缺乏明确的对象级结构，限制了操作任务中的鲁棒性和可收缩性。需要将通用基础模型特征转化为任务感知的对象中心表示。

Method: 提出STORM模块，采用多阶段训练策略：1) 使用语言嵌入进行视觉-语义预训练以稳定对象中心槽位；2) 与下游操作策略联合适配。避免重新训练大型骨干网络，防止槽位退化并保持语义一致性。

Result: 在对象发现基准和模拟操作任务上的实验表明，STORM相比直接使用冻结基础模型特征或端到端训练对象中心表示，能更好地泛化到视觉干扰物，并提升控制性能。

Conclusion: 多阶段适配是高效将通用基础模型特征转化为任务感知对象中心表示的有效机制，为机器人控制提供了更好的感知-任务对齐。

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [14] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO提出了一种单步生成的机器人控制策略框架，通过MeanFlow、分散正则化和RL微调实现实时控制，相比多步采样方法有5-20倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散和流匹配的生成策略需要多步采样，无法满足实时机器人控制的时间关键场景需求。

Method: 提出DMPO框架：1) MeanFlow实现数学推导的单步推理，无需知识蒸馏；2) 分散正则化防止表示崩溃；3) RL微调超越专家演示。

Result: 在RoboMimic操作和OpenAI Gym运动基准测试中表现优于多步基线，推理速度提升5-20倍，达到>120Hz实时控制要求，在Franka机器人上验证了实际应用性。

Conclusion: DMPO通过单步生成实现了实时机器人控制，解决了现有生成策略的时间限制问题，具有实际部署价值。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>
