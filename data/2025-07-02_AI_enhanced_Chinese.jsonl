{"id": "2507.00042", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00042", "abs": "https://arxiv.org/abs/2507.00042", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "comment": "ICANN 2025", "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faER-EMU\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ecf\u9a8c\u56de\u653e\u89e3\u51b3\u8fb9\u7f18\u6a21\u578b\u5728\u52a8\u6001\u4ea4\u901a\u73af\u5883\u4e2d\u56e0\u707e\u96be\u6027\u9057\u5fd8\u5bfc\u81f4\u7684\u77e5\u8bc6\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u52a8\u6001\u4ea4\u901a\u73af\u5883\uff08\u5982\u663c\u591c\u53d8\u5316\uff09\u4e2d\uff0c\u8fb9\u7f18\u6a21\u578b\u5728\u9002\u5e94\u65b0\u6570\u636e\u5206\u5e03\u65f6\u4f1a\u9057\u5fd8\u65e7\u77e5\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u9ad8\u6548\u5229\u7528\u5386\u53f2\u6570\u636e\u3002", "method": "ER-EMU\u7ed3\u5408FIFO\u7ecf\u9a8c\u7f13\u51b2\u533a\u548c\u57fa\u4e8e\u57df\u8ddd\u79bb\u5ea6\u91cf\u7684\u7ecf\u9a8c\u9009\u62e9\u7b97\u6cd5\uff08DDM-ES\uff09\uff0c\u4f18\u5148\u9009\u62e9\u4e0e\u5f53\u524d\u57df\u5dee\u5f02\u5927\u7684\u5386\u53f2\u6570\u636e\u3002", "result": "\u5728Bellevue\u4ea4\u901a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0cER-EMU\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u4e91\u8fb9\u534f\u4f5c\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u7684\u6027\u80fd\u3002", "conclusion": "ER-EMU\u901a\u8fc7\u4f18\u5316\u5386\u53f2\u6570\u636e\u9009\u62e9\u548c\u7f13\u51b2\u7ba1\u7406\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u3002"}}
{"id": "2507.00070", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00070", "abs": "https://arxiv.org/abs/2507.00070", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zu\u00f1iga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "title": "An efficient plant disease detection using transfer learning approach", "comment": "15 pages , 4 figures. Scientific Reports 2025", "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv7\u548cYOLOv8\u7684\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u5bf9\u7ec6\u83cc\u3001\u771f\u83cc\u548c\u75c5\u6bd2\u75c5\u5bb3\u7684\u51c6\u786e\u8bc6\u522b\uff0cYOLOv8\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u690d\u7269\u75c5\u5bb3\u5bf9\u519c\u4e1a\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u6280\u672f\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u4f7f\u7528YOLOv7\u548cYOLOv8\u6a21\u578b\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5bf9\u690d\u7269\u53f6\u7247\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u68c0\u6d4b\u591a\u79cd\u75c5\u5bb3\u3002", "result": "\u6a21\u578b\u6027\u80fd\u6307\u6807\uff08mAP\u3001F1-score\u3001Precision\u3001Recall\uff09\u5206\u522b\u4e3a91.05\u300189.40\u300191.22\u548c87.66\uff0cYOLOv8\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u690d\u7269\u75c5\u5bb3\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u4f5c\u7269\u4ea7\u91cf\u548c\u53ef\u6301\u7eed\u519c\u4e1a\u3002"}}
{"id": "2507.00979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00979", "abs": "https://arxiv.org/abs/2507.00979", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCIP\u7684\u65b0\u6280\u672f\uff0c\u5229\u7528\u56e0\u679c\u5f71\u54cd\u56fe\uff08CID\uff09\u6765\u8bc6\u522b\u548c\u51cf\u8f7b\u81ea\u4e3b\u4ee3\u7406\u51b3\u7b56\u4e2d\u7684\u98ce\u9669\uff0c\u4ece\u800c\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u4e3b\u4ee3\u7406\u5728\u8f85\u52a9\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u5b89\u5168\u53ef\u9760\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u907f\u514d\u610f\u5916\u540e\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1\uff09\u57fa\u4e8e\u4efb\u52a1\u89c4\u8303\u521d\u59cb\u5316CID\u4ee5\u63cf\u8ff0\u51b3\u7b56\u8fc7\u7a0b\uff1b2\uff09\u5229\u7528CID\u6307\u5bfc\u4ee3\u7406\u4e0e\u73af\u5883\u4ea4\u4e92\uff1b3\uff09\u6839\u636e\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u548c\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316CID\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ee3\u7801\u6267\u884c\u548c\u79fb\u52a8\u8bbe\u5907\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "CIP\u901a\u8fc7\u7ed3\u6784\u5316\u56e0\u679c\u5173\u7cfb\u8868\u793a\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2507.00080", "categories": ["cs.LG", "nlin.AO", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.00080", "abs": "https://arxiv.org/abs/2507.00080", "authors": ["Ali Tavasoli", "Heman Shakeri"], "title": "Online Meal Detection Based on CGM Data Dynamics", "comment": null, "summary": "We utilize dynamical modes as features derived from Continuous Glucose\nMonitoring (CGM) data to detect meal events. By leveraging the inherent\nproperties of underlying dynamics, these modes capture key aspects of glucose\nvariability, enabling the identification of patterns and anomalies associated\nwith meal consumption. This approach not only improves the accuracy of meal\ndetection but also enhances the interpretability of the underlying glucose\ndynamics. By focusing on dynamical features, our method provides a robust\nframework for feature extraction, facilitating generalization across diverse\ndatasets and ensuring reliable performance in real-world applications. The\nproposed technique offers significant advantages over traditional approaches,\nimproving detection accuracy,", "AI": {"tldr": "\u5229\u7528\u52a8\u6001\u6a21\u5f0f\u4ece\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\u4ee5\u68c0\u6d4b\u8fdb\u9910\u4e8b\u4ef6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u8fdb\u9910\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u6a21\u5f0f\u6355\u6349\u8840\u7cd6\u53d8\u5f02\u6027\uff0c\u8bc6\u522b\u4e0e\u8fdb\u9910\u76f8\u5173\u7684\u6a21\u5f0f\u548c\u5f02\u5e38\u3002", "result": "\u63d0\u9ad8\u4e86\u8fdb\u9910\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u589e\u5f3a\u4e86\u8840\u7cd6\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.00090", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00090", "abs": "https://arxiv.org/abs/2507.00090", "authors": ["Corbeau Michael", "Claeys Emmanuelle", "Serrurier Mathieu", "Zarat\u00e9 Pascale"], "title": "Generating Heterogeneous Multi-dimensional Data : A Comparative Study", "comment": "accepted at IEEE SMC 2025 Vienna", "summary": "Allocation of personnel and material resources is highly sensible in the case\nof firefighter interventions. This allocation relies on simulations to\nexperiment with various scenarios. The main objective of this allocation is the\nglobal optimization of the firefighters response. Data generation is then\nmandatory to study various scenarios In this study, we propose to compare\ndifferent data generation methods. Methods such as Random Sampling, Tabular\nVariational Autoencoders, standard Generative Adversarial Networks, Conditional\nTabular Generative Adversarial Networks and Diffusion Probabilistic Models are\nexamined to ascertain their efficacy in capturing the intricacies of\nfirefighter interventions. Traditional evaluation metrics often fall short in\ncapturing the nuanced requirements of synthetic datasets for real-world\nscenarios. To address this gap, an evaluation of synthetic data quality is\nconducted using a combination of domain-specific metrics tailored to the\nfirefighting domain and standard measures such as the Wasserstein distance.\nDomain-specific metrics include response time distribution, spatial-temporal\ndistribution of interventions, and accidents representation. These metrics are\ndesigned to assess data variability, the preservation of fine and complex\ncorrelations and anomalies such as event with a very low occurrence, the\nconformity with the initial statistical distribution and the operational\nrelevance of the synthetic data. The distribution has the particularity of\nbeing highly unbalanced, none of the variables following a Gaussian\ndistribution, adding complexity to the data generation process.", "AI": {"tldr": "\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u91c7\u6837\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7b49\uff09\u5728\u6d88\u9632\u5458\u5e72\u9884\u573a\u666f\u4e2d\u7684\u6548\u679c\uff0c\u63d0\u51fa\u9886\u57df\u7279\u5b9a\u6307\u6807\u8bc4\u4f30\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u4f18\u5316\u6d88\u9632\u5458\u8d44\u6e90\u5206\u914d\u9700\u4f9d\u8d56\u6a21\u62df\u573a\u666f\uff0c\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u6ee1\u8db3\u5408\u6210\u6570\u636e\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "\u6bd4\u8f83\u591a\u79cd\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u6307\u6807\uff08\u5982\u54cd\u5e94\u65f6\u95f4\u5206\u5e03\u3001\u65f6\u7a7a\u5206\u5e03\uff09\u548c\u6807\u51c6\u6307\u6807\uff08\u5982Wasserstein\u8ddd\u79bb\uff09\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\u3002", "result": "\u53d1\u73b0\u6570\u636e\u5206\u5e03\u9ad8\u5ea6\u4e0d\u5e73\u8861\u4e14\u975e\u9ad8\u65af\u5206\u5e03\uff0c\u589e\u52a0\u4e86\u6570\u636e\u751f\u6210\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u6307\u6807\u80fd\u66f4\u6709\u6548\u8bc4\u4f30\u5408\u6210\u6570\u636e\u7684\u5b9e\u7528\u6027\u548c\u76f8\u5173\u6027\u3002"}}
{"id": "2507.00304", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.00304", "abs": "https://arxiv.org/abs/2507.00304", "authors": ["Yujun Zhang", "Runlong Li", "Xiaoxiang Liang", "Xinhao Yang", "Tian Su", "Bo Liu", "Yan Zhou"], "title": "MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic", "comment": "16 pages", "summary": "The abnormal fluctuations in network traffic may indicate potential security\nthreats or system failures. Therefore, efficient network traffic prediction and\nanomaly detection methods are crucial for network security and traffic\nmanagement. This paper proposes a novel network traffic prediction and anomaly\ndetection model, MamNet, which integrates time-domain modeling and\nfrequency-domain feature extraction. The model first captures the long-term\ndependencies of network traffic through the Mamba module (time-domain\nmodeling), and then identifies periodic fluctuations in the traffic using\nFourier Transform (frequency-domain feature extraction). In the feature fusion\nlayer, multi-scale information is integrated to enhance the model's ability to\ndetect network traffic anomalies. Experiments conducted on the UNSW-NB15 and\nCAIDA datasets demonstrate that MamNet outperforms several recent mainstream\nmodels in terms of accuracy, recall, and F1-Score. Specifically, it achieves an\nimprovement of approximately 2% to 4% in detection performance for complex\ntraffic patterns and long-term trend detection. The results indicate that\nMamNet effectively captures anomalies in network traffic across different time\nscales and is suitable for anomaly detection tasks in network security and\ntraffic management. Future work could further optimize the model structure by\nincorporating external network event information, thereby improving the model's\nadaptability and stability in complex network environments.", "AI": {"tldr": "MamNet\u6a21\u578b\u7ed3\u5408\u65f6\u57df\u5efa\u6a21\u548c\u9891\u57df\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u6ce2\u52a8\u53ef\u80fd\u9884\u793a\u5b89\u5168\u5a01\u80c1\u6216\u7cfb\u7edf\u6545\u969c\uff0c\u9700\u9ad8\u6548\u9884\u6d4b\u4e0e\u68c0\u6d4b\u65b9\u6cd5\u4fdd\u969c\u7f51\u7edc\u5b89\u5168\u3002", "method": "MamNet\u901a\u8fc7Mamba\u6a21\u5757\uff08\u65f6\u57df\u5efa\u6a21\uff09\u548c\u5085\u91cc\u53f6\u53d8\u6362\uff08\u9891\u57df\u7279\u5f81\u63d0\u53d6\uff09\u6355\u6349\u6d41\u91cf\u957f\u671f\u4f9d\u8d56\u4e0e\u5468\u671f\u6027\u6ce2\u52a8\uff0c\u878d\u5408\u591a\u5c3a\u5ea6\u4fe1\u606f\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5728UNSW-NB15\u548cCAIDA\u6570\u636e\u96c6\u4e0a\uff0cMamNet\u5728\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1-Score\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u6a21\u578b\uff0c\u590d\u6742\u6d41\u91cf\u548c\u957f\u671f\u8d8b\u52bf\u68c0\u6d4b\u6027\u80fd\u63d0\u53472%-4%\u3002", "conclusion": "MamNet\u80fd\u6709\u6548\u6355\u83b7\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u6d41\u91cf\u5f02\u5e38\uff0c\u9002\u7528\u4e8e\u7f51\u7edc\u5b89\u5168\u4e0e\u6d41\u91cf\u7ba1\u7406\u3002\u672a\u6765\u53ef\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u4e8b\u4ef6\u4fe1\u606f\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u3002"}}
{"id": "2507.00525", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00525", "abs": "https://arxiv.org/abs/2507.00525", "authors": ["Djamahl Etchegaray", "Yuxia Fu", "Zi Huang", "Yadan Luo"], "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving", "comment": null, "summary": "Interpretable communication is essential for safe and trustworthy autonomous\ndriving, yet current vision-language models (VLMs) often operate under\nidealized assumptions and struggle to capture user intent in real-world\nscenarios. Existing driving-oriented VQA datasets are limited to full-scene\ndescriptions or waypoint prediction, preventing the assessment of whether VLMs\ncan respond to localized user-driven queries. We introduce Box-QAymo, a\nbox-referring dataset and benchmark designed to both evaluate and finetune VLMs\non spatial and temporal reasoning over user-specified objects. Users express\nintent by drawing bounding boxes, offering a fast and intuitive interface for\nfocused queries in complex scenes. Specifically, we propose a hierarchical\nevaluation protocol that begins with binary sanity-check questions to assess\nbasic model capacities, and progresses to (1) attribute prediction for\nbox-referred objects, (2) motion understanding of target instances, and (3)\nspatiotemporal motion reasoning over inter-object dynamics across frames. To\nsupport this, we crowd-sourced fine-grained object classes and visual\nattributes that reflect the complexity drivers encounter, and extract object\ntrajectories to construct temporally grounded QA pairs. Rigorous quality\ncontrol through negative sampling, temporal consistency checks, and\ndifficulty-aware balancing guarantee dataset robustness and diversity. Our\ncomprehensive evaluation reveals significant limitations in current VLMs when\nqueried about perception questions, highlighting the gap in achieving\nreal-world performance. This work provides a foundation for developing more\nrobust and interpretable autonomous driving systems that can communicate\neffectively with users under real-world conditions. Project page and dataset\nare available at https://djamahl99.github.io/qaymo-pages/.", "AI": {"tldr": "Box-QAymo\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u7528\u6237\u6307\u5b9a\u5bf9\u8c61\u7684\u65f6\u7a7a\u63a8\u7406\uff0c\u901a\u8fc7\u8fb9\u754c\u6846\u548c\u5c42\u6b21\u5316\u8bc4\u4f30\u534f\u8bae\u63d0\u5347\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u9a7e\u9a76\u5bfc\u5411\u7684VQA\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5168\u573a\u666f\u63cf\u8ff0\u6216\u8def\u5f84\u70b9\u9884\u6d4b\uff0c\u65e0\u6cd5\u8bc4\u4f30VLMs\u5bf9\u5c40\u90e8\u7528\u6237\u9a71\u52a8\u67e5\u8be2\u7684\u54cd\u5e94\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165Box-QAymo\u6570\u636e\u96c6\uff0c\u7528\u6237\u901a\u8fc7\u7ed8\u5236\u8fb9\u754c\u6846\u8868\u8fbe\u610f\u56fe\uff0c\u91c7\u7528\u5c42\u6b21\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u62ec\u57fa\u7840\u80fd\u529b\u6d4b\u8bd5\u3001\u5c5e\u6027\u9884\u6d4b\u3001\u8fd0\u52a8\u7406\u89e3\u548c\u65f6\u7a7a\u63a8\u7406\u3002", "result": "\u5f53\u524dVLMs\u5728\u611f\u77e5\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u63ed\u793a\u4e86\u4e0e\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\u7684\u5dee\u8ddd\u3002", "conclusion": "Box-QAymo\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u652f\u6301\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u7528\u6237\u4ea4\u4e92\u3002"}}
{"id": "2507.00593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00593", "abs": "https://arxiv.org/abs/2507.00593", "authors": ["Fernando Alonso-Fernandez", "Talha Hanif Butt", "Prayag Tiwari"], "title": "Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods", "comment": "Under review at ESWA", "summary": "Safe overtaking manoeuvres in trucks are vital for preventing accidents and\nensuring efficient traffic flow. Accurate prediction of such manoeuvres is\nessential for Advanced Driver Assistance Systems (ADAS) to make timely and\ninformed decisions. In this study, we focus on overtake detection using\nController Area Network (CAN) bus data collected from five in-service trucks\nprovided by the Volvo Group. We evaluate three common classifiers for vehicle\nmanoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and\nSupport Vector Machines (SVM), and analyse how different preprocessing\nconfigurations affect performance. We find that variability in traffic\nconditions strongly influences the signal patterns, particularly in the\nno-overtake class, affecting classification performance if training data lacks\nadequate diversity. Since the data were collected under unconstrained,\nreal-world conditions, class diversity cannot be guaranteed a priori. However,\ntraining with data from multiple vehicles improves generalisation and reduces\ncondition-specific bias. Our pertruck analysis also reveals that classification\naccuracy, especially for overtakes, depends on the amount of training data per\nvehicle. To address this, we apply a score-level fusion strategy, which yields\nthe best per-truck performance across most cases. Overall, we achieve an\naccuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True\nPositive Rate). This research has been part of the BIG FUN project, which\nexplores how Artificial Intelligence can be applied to logged vehicle data to\nunderstand and predict driver behaviour, particularly in relation to Camera\nMonitor Systems (CMS), being introduced as digital replacements for traditional\nexterior mirrors.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5361\u8f66CAN\u603b\u7ebf\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5206\u7c7b\u5668\uff08ANN\u3001RF\u3001SVM\uff09\u7528\u4e8e\u8d85\u8f66\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u591a\u8f66\u6570\u636e\u8bad\u7ec3\u548c\u5206\u6570\u7ea7\u878d\u5408\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u5361\u8f66\u5b89\u5168\u8d85\u8f66\u5bf9\u9632\u6b62\u4e8b\u6545\u548c\u4ea4\u901a\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0cADAS\u9700\u8981\u51c6\u786e\u9884\u6d4b\u8d85\u8f66\u884c\u4e3a\u4ee5\u505a\u51fa\u53ca\u65f6\u51b3\u7b56\u3002", "method": "\u4f7f\u7528Volvo\u63d0\u4f9b\u7684\u4e94\u8f86\u5361\u8f66CAN\u603b\u7ebf\u6570\u636e\uff0c\u8bc4\u4f30ANN\u3001RF\u548cSVM\u4e09\u79cd\u5206\u7c7b\u5668\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u9884\u5904\u7406\u914d\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u591a\u8f66\u6570\u636e\u8bad\u7ec3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5206\u6570\u7ea7\u878d\u5408\u7b56\u7565\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u6700\u7ec8\u5b9e\u73b0TNR=93%\u548cTPR=86.5%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u591a\u6837\u6027\u548c\u591a\u8f66\u8bad\u7ec3\u7684\u91cd\u8981\u6027\uff0c\u4e3aADAS\u8d85\u8f66\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.00608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00608", "abs": "https://arxiv.org/abs/2507.00608", "authors": ["Zehua Fu", "Chenguang Liu", "Yuyu Chen", "Jiaqi Zhou", "Qingjie Liu", "Yunhong Wang"], "title": "De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection", "comment": "Accepted by IEEE Transactions on Intelligent Transportation Systems.\n  15 pages, 10 figures", "summary": "Despite its significant success, object detection in traffic and\ntransportation scenarios requires time-consuming and laborious efforts in\nacquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation\n(UDA) for object detection has recently gained increasing research attention.\nUDA for object detection has been dominated by domain alignment methods, which\nachieve top performance. Recently, self-labeling methods have gained popularity\ndue to their simplicity and efficiency. In this paper, we investigate the\nlimitations that prevent self-labeling detectors from achieving commensurate\nperformance with domain alignment methods. Specifically, we identify the high\nproportion of simple samples during training, i.e., the simple-label bias, as\nthe central cause. We propose a novel approach called De-Simplifying Pseudo\nLabels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level\nmemory bank to implement an innovative pseudo label updating strategy. Then,\nadversarial samples are introduced during training to enhance the proportion.\nFurthermore, we propose an adaptive weighted loss to avoid the model suffering\nfrom an abundance of false positive pseudo labels in the late training period.\nExperimental results demonstrate that DeSimPL effectively reduces the\nproportion of simple samples during training, leading to a significant\nperformance improvement for self-labeling detectors. Extensive experiments\nconducted on four benchmarks validate our analysis and conclusions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDeSimPL\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u4e2d\u7b80\u5355\u6837\u672c\u7684\u6bd4\u4f8b\uff0c\u63d0\u5347\u81ea\u6807\u8bb0\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u4ea4\u901a\u573a\u666f\u4e2d\u76ee\u6807\u68c0\u6d4b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002\u81ea\u6807\u8bb0\u65b9\u6cd5\u867d\u7b80\u5355\u9ad8\u6548\uff0c\u4f46\u6027\u80fd\u4e0d\u53ca\u57df\u5bf9\u9f50\u65b9\u6cd5\uff0c\u539f\u56e0\u662f\u8bad\u7ec3\u4e2d\u7b80\u5355\u6837\u672c\u6bd4\u4f8b\u8fc7\u9ad8\uff08\u7b80\u5355\u6807\u7b7e\u504f\u5dee\uff09\u3002", "method": "\u63d0\u51faDeSimPL\u65b9\u6cd5\uff0c\u5229\u7528\u5b9e\u4f8b\u7ea7\u8bb0\u5fc6\u5e93\u66f4\u65b0\u4f2a\u6807\u7b7e\uff0c\u5e76\u5f15\u5165\u5bf9\u6297\u6837\u672c\u589e\u52a0\u6837\u672c\u591a\u6837\u6027\uff0c\u540c\u65f6\u91c7\u7528\u81ea\u9002\u5e94\u52a0\u6743\u635f\u5931\u907f\u514d\u540e\u671f\u8bad\u7ec3\u4e2d\u4f2a\u6807\u7b7e\u7684\u8bef\u62a5\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeSimPL\u663e\u8457\u51cf\u5c11\u7b80\u5355\u6837\u672c\u6bd4\u4f8b\uff0c\u63d0\u5347\u81ea\u6807\u8bb0\u68c0\u6d4b\u5668\u6027\u80fd\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DeSimPL\u901a\u8fc7\u89e3\u51b3\u7b80\u5355\u6807\u7b7e\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u6807\u8bb0\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u4e3aUDA\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.00698", "abs": "https://arxiv.org/abs/2507.00698", "authors": ["Qihang Fan", "Huaibo Huang", "Yuang Ai", "ran He"], "title": "Rectifying Magnitude Neglect in Linear Attention", "comment": "Accepted by ICCV2025", "summary": "As the core operator of Transformers, Softmax Attention exhibits excellent\nglobal modeling capabilities. However, its quadratic complexity limits its\napplicability to vision tasks. In contrast, Linear Attention shares a similar\nformulation with Softmax Attention while achieving linear complexity, enabling\nefficient global information modeling. Nevertheless, Linear Attention suffers\nfrom a significant performance degradation compared to standard Softmax\nAttention. In this paper, we analyze the underlying causes of this issue based\non the formulation of Linear Attention. We find that, unlike Softmax Attention,\nLinear Attention entirely disregards the magnitude information of the Query.\nThis prevents the attention score distribution from dynamically adapting as the\nQuery scales. As a result, despite its structural similarity to Softmax\nAttention, Linear Attention exhibits a significantly different attention score\ndistribution. Based on this observation, we propose Magnitude-Aware Linear\nAttention (MALA), which modifies the computation of Linear Attention to fully\nincorporate the Query's magnitude. This adjustment allows MALA to generate an\nattention score distribution that closely resembles Softmax Attention while\nexhibiting a more well-balanced structure. We evaluate the effectiveness of\nMALA on multiple tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, natural language processing,\nspeech recognition, and image generation. Our MALA achieves strong results on\nall of these tasks. Code will be available at https://github.com/qhfan/MALA", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\uff08Linear Attention\uff09\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5MALA\uff0c\u901a\u8fc7\u878d\u5165\u67e5\u8be2\uff08Query\uff09\u7684\u5e45\u5ea6\u4fe1\u606f\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1Softmax Attention\u7684\u6027\u80fd\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u7136\u590d\u6742\u5ea6\u4f4e\uff0c\u4f46\u6027\u80fd\u663e\u8457\u4f4e\u4e8eSoftmax Attention\u3002\u7814\u7a76\u53d1\u73b0\u5176\u5ffd\u7565\u4e86\u67e5\u8be2\u7684\u5e45\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u5e03\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u3002", "method": "\u63d0\u51faMagnitude-Aware Linear Attention (MALA)\uff0c\u5728\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u878d\u5165\u67e5\u8be2\u7684\u5e45\u5ea6\u4fe1\u606f\uff0c\u4f7f\u5176\u5206\u5e03\u66f4\u63a5\u8fd1Softmax Attention\u3002", "result": "MALA\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MALA\u901a\u8fc7\u6539\u8fdb\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.00574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.00574", "abs": "https://arxiv.org/abs/2507.00574", "authors": ["Haresh Rengaraj Rajamohan", "Xiang Gao", "Weicheng Zhu", "Shih-Lun Huang", "Long Chen", "Kyunghyun Cho", "Cem M. Deniz", "Narges Razavian"], "title": "Foundation Models for Clinical Records at Health System Scale", "comment": "Accepted to ICML 2025 Workshop on Foundation Models for Structured\n  Data", "summary": "Large-scale pretraining has transformed modeling of language and other data\ntypes, but its potential remains underexplored in healthcare with structured\nelectronic health records (EHRs). We present a novel generative pretraining\nstrategy for sequential EHR data using next-visit event prediction. Our model\nlearns to autoregressively generate various tokenized clinical events for the\nnext visit based on patient history and inherently handles the joint prediction\nof heterogeneous data types. Additionally, we introduce regularization on\npredicting repeated events and highlight a key pitfall in EHR-based foundation\nmodel evaluations: repeated event tokens can inflate performance metrics when\nnew onsets are not distinguished from subsequent occurrences. Our model is\nevaluated via zero-shot prediction for forecasting dementia and knee\nosteoarthritis incidence within 2 and 5 years, and the model performance rivals\na fully fine-tuned masked pretrained Transformer baseline, demonstrating that\nour approach captures complex clinical dependencies without requiring costly\ntask-specific fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u6b21\u5c31\u8bca\u4e8b\u4ef6\u6765\u5b66\u4e60\u60a3\u8005\u5386\u53f2\u6570\u636e\uff0c\u5e76\u5728\u4e0d\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5728\u8bed\u8a00\u548c\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u7ed3\u6784\u5316EHR\u6570\u636e\u4e2d\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e0b\u4e00\u6b21\u5c31\u8bca\u4e8b\u4ef6\u9884\u6d4b\u7684\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\uff0c\u5904\u7406\u5f02\u6784\u6570\u636e\u7c7b\u578b\u7684\u8054\u5408\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u5bf9\u91cd\u590d\u4e8b\u4ef6\u7684\u9884\u6d4b\u6b63\u5219\u5316\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\uff0c\u5bf9\u75f4\u5446\u548c\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u7684\u53d1\u75c5\u7387\u9884\u6d4b\u8868\u73b0\u4e0e\u5b8c\u5168\u5fae\u8c03\u7684Transformer\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7b56\u7565\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u4e34\u5e8a\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u4e3aEHR\u6570\u636e\u7684\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.00969", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00969", "abs": "https://arxiv.org/abs/2507.00969", "authors": ["Alberto Neri", "Maximilan Fehrentz", "Veronica Penza", "Leonardo S. Mattos", "Nazim Haouchine"], "title": "Surgical Neural Radiance Fields from One Image", "comment": null, "summary": "Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D\nreconstruction and view synthesis, yet their reliance on extensive multi-view\ndata limits their application in surgical intraoperative settings where only\nlimited data is available. In particular, collecting such extensive data\nintraoperatively is impractical due to time constraints. This work addresses\nthis challenge by leveraging a single intraoperative image and preoperative\ndata to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera\nviewpoints and images needed for robust and unobstructed training.\nIntraoperatively, the appearance of the surgical image is transferred to the\npre-constructed training set through neural style transfer, specifically\ncombining WTC2 and STROTSS to prevent over-stylization. This process enables\nthe creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases.\nQuantitative comparisons to NeRF models trained on real surgical microscope\nimages demonstrate strong synthesis agreement, with similarity metrics\nindicating high reconstruction fidelity and stylistic alignment. When compared\nwith ground truth, our method demonstrates high structural similarity,\nconfirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF\ntraining in surgical settings, overcoming the limitations of traditional\nmulti-view methods.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u672f\u524dMRI\u6570\u636e\u548c\u672f\u4e2d\u5355\u5f20\u56fe\u50cf\uff0c\u5229\u7528\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u5b9e\u73b0\u5feb\u901f\u5355\u56fe\u50cfNeRF\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u672f\u4e2d\u591a\u89c6\u56fe\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "NeRF\u5728\u672f\u4e2d\u573a\u666f\u4e2d\u56e0\u591a\u89c6\u56fe\u6570\u636e\u9700\u6c42\u53d7\u9650\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u5355\u5f20\u672f\u4e2d\u56fe\u50cf\u548c\u672f\u524d\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "\u5229\u7528\u672f\u524dMRI\u6570\u636e\u5b9a\u4e49\u76f8\u673a\u89c6\u89d2\uff0c\u901a\u8fc7\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\uff08\u7ed3\u5408WTC2\u548cSTROTSS\uff09\u5c06\u672f\u4e2d\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u81f3\u8bad\u7ec3\u96c6\uff0c\u5b9e\u73b0\u5feb\u901f\u5355\u56fe\u50cfNeRF\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u795e\u7ecf\u5916\u79d1\u6848\u4f8b\u4e2d\u9a8c\u8bc1\uff0c\u4e0e\u771f\u5b9e\u624b\u672f\u663e\u5fae\u955c\u56fe\u50cf\u8bad\u7ec3\u7684NeRF\u6a21\u578b\u76f8\u6bd4\uff0c\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u5bf9\u9f50\u5ea6\u9ad8\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5355\u56fe\u50cfNeRF\u8bad\u7ec3\u5728\u624b\u672f\u573a\u666f\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u591a\u89c6\u56fe\u65b9\u6cd5\u7684\u9650\u5236\u3002"}}
{"id": "2507.00971", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00971", "abs": "https://arxiv.org/abs/2507.00971", "authors": ["Taeyoun Kim", "Fahim Tajwar", "Aditi Raghunathan", "Aviral Kumar"], "title": "Reasoning as an Adaptive Defense for Safety", "comment": "42 pages, 11 Figures, 7 Tables", "summary": "Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful,\nharmless, and ambiguous prompts to prevent shortcut behaviors such as too many\nrefusals, and (3) a reward function to prevent degeneration of reasoning\ncapabilities during training. Models trained with TARS exhibit adaptive\nbehaviors by spending more compute on ambiguous queries, leading to better\nsafety-refusal trade-offs. They also internally learn to better distinguish\nbetween safe and unsafe prompts and attain greater robustness to both white-box\n(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an\neffective, open recipe for training LLMs against jailbreaks and harmful\nrequests by reasoning per prompt.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTARS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u6765\u8bad\u7ec3LLM\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u5b89\u5168\u6f0f\u6d1e\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\u63d0\u5347LLM\u5728\u5b89\u5168\u9886\u57df\u7684\u8868\u73b0\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u8ffd\u8e2a\u548c\u5e73\u8861\u5b89\u5168\u4e0e\u4efb\u52a1\u5b8c\u6210\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u8bbe\u8ba1TARS\u8bad\u7ec3\u6846\u67b6\u3002", "result": "TARS\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6a21\u7cca\u67e5\u8be2\u4e0a\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5b89\u5168\u6027\u4e0e\u62d2\u7edd\u5e73\u8861\uff0c\u5e76\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "TARS\u4e3a\u8bad\u7ec3LLM\u62b5\u5fa1\u8d8a\u72f1\u548c\u6709\u5bb3\u8bf7\u6c42\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5f00\u653e\u65b9\u6cd5\u3002"}}
