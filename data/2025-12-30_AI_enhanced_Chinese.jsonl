{"id": "2512.22199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22199", "abs": "https://arxiv.org/abs/2512.22199", "authors": ["Teja Chinthala"], "title": "Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation", "comment": "10 pages, 2 figures, 2 tables. 36 experiments across 4 datasets with 3 random seeds. Code available upon request", "summary": "Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.", "AI": {"tldr": "Bidirectional RAG \u662f\u4e00\u79cd\u65b0\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u67b6\u6784\uff0c\u901a\u8fc7\u9a8c\u8bc1\u9ad8\u8d28\u91cf\u751f\u6210\u54cd\u5e94\u7684\u5199\u56de\u673a\u5236\u5b9e\u73b0\u5b89\u5168\u8bed\u6599\u5e93\u6269\u5c55\uff0c\u76f8\u6bd4\u6807\u51c6RAG\u5c06\u5e73\u5747\u8986\u76d6\u7387\u4ece20.33%\u63d0\u5347\u81f340.58%\uff0c\u540c\u65f6\u6bd4\u6734\u7d20\u5199\u56de\u65b9\u6cd5\u51cf\u5c1172%\u7684\u6587\u6863\u6dfb\u52a0\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u4f7f\u7528\u9759\u6001\u77e5\u8bc6\u5e93\uff0c\u65e0\u6cd5\u4ece\u7528\u6237\u4ea4\u4e92\u4e2d\u5b66\u4e60\u548c\u8fdb\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b89\u5168\u6269\u5c55\u8bed\u6599\u5e93\u7684RAG\u67b6\u6784\uff0c\u65e2\u80fd\u79ef\u7d2f\u77e5\u8bc6\u53c8\u907f\u514d\u5e7b\u89c9\u6c61\u67d3\u3002", "method": "\u63d0\u51faBidirectional RAG\u67b6\u6784\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u63a5\u53d7\u5c42\uff0c\u7ed3\u5408\u57fa\u4e8eNLI\u7684\u8574\u542b\u9a8c\u8bc1\u3001\u5f52\u56e0\u68c0\u67e5\u548c\u65b0\u9896\u6027\u68c0\u6d4b\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u751f\u6210\u54cd\u5e94\u7684\u5b89\u5168\u5199\u56de\uff0c\u5b9e\u73b0\u8bed\u6599\u5e93\u7684\u9a8c\u8bc1\u6269\u5c55\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08Natural Questions\u3001TriviaQA\u3001HotpotQA\u3001Stack Overflow\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cBidirectional RAG\u5e73\u5747\u8986\u76d6\u7387\u8fbe\u523040.58%\uff0c\u51e0\u4e4e\u662f\u6807\u51c6RAG\uff0820.33%\uff09\u7684\u4e24\u500d\uff0c\u540c\u65f6\u6bd4\u6734\u7d20\u5199\u56de\u65b9\u6cd5\u5c11\u6dfb\u52a072%\u7684\u6587\u6863\uff08140 vs 500\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e25\u683c\u9a8c\u8bc1\u673a\u5236\u7ba1\u7406\u4e0b\uff0c\u81ea\u6211\u6539\u8fdb\u7684RAG\u7cfb\u7edf\u662f\u53ef\u884c\u4e14\u5b89\u5168\u7684\uff0c\u4e3a\u4ece\u90e8\u7f72\u4e2d\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2512.22575", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22575", "abs": "https://arxiv.org/abs/2512.22575", "authors": ["Xuewei Zhang", "Bailing Tian", "Kai Zheng", "Yulin Hui", "Junjie Lu", "Zhiyu Li"], "title": "ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation", "comment": null, "summary": "Real-time and collision-free motion planning remains challenging for robotic manipulation in unknown environments due to continuous perception updates and the need for frequent online replanning. To address these challenges, we propose a parallel mapping and motion planning framework that tightly integrates Euclidean Distance Transform (EDT)-based environment representation with a sampling-based model predictive control (SMPC) planner. On the mapping side, a dense distance-field-based representation is constructed using a GPU-based EDT and augmented with a robot-masked update mechanism to prevent false self-collision detections during online perception. On the planning side, motion generation is formulated as a stochastic optimization problem with a unified objective function and efficiently solved by evaluating large batches of candidate rollouts in parallel within a SMPC framework, in which a geometrically consistent pose tracking metric defined on SE(3) is incorporated to ensure fast and accurate convergence to the target pose. The entire mapping and planning pipeline is implemented on the GPU to support high-frequency replanning. The effectiveness of the proposed framework is validated through extensive simulations and real-world experiments on a 7-DoF robotic manipulator. More details are available at: https://zxw610.github.io/ParaMaP.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u884c\u5efa\u56fe\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7GPU\u52a0\u901f\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u53d8\u6362\u548c\u91c7\u6837\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5b9e\u73b0\u672a\u77e5\u73af\u5883\u4e0b\u673a\u68b0\u81c2\u5b9e\u65f6\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u6301\u7eed\u611f\u77e5\u66f4\u65b0\u548c\u9891\u7e41\u5728\u7ebf\u91cd\u89c4\u5212\u7684\u9700\u6c42\uff0c\u5b9e\u65f6\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "1) \u5efa\u56fe\u7aef\uff1a\u4f7f\u7528GPU\u52a0\u901f\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u53d8\u6362\u6784\u5efa\u5bc6\u96c6\u8ddd\u79bb\u573a\u8868\u793a\uff0c\u5e76\u52a0\u5165\u673a\u5668\u4eba\u63a9\u7801\u66f4\u65b0\u673a\u5236\u9632\u6b62\u9519\u8bef\u81ea\u78b0\u649e\u68c0\u6d4b\n2) \u89c4\u5212\u7aef\uff1a\u5c06\u8fd0\u52a8\u751f\u6210\u5efa\u6a21\u4e3a\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u5728SE(3)\u4e0a\u5b9a\u4e49\u51e0\u4f55\u4e00\u81f4\u7684\u59ff\u6001\u8ddf\u8e2a\u5ea6\u91cf\uff0c\u901a\u8fc7\u91c7\u6837\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u5e76\u884c\u8bc4\u4f30\u5927\u91cf\u5019\u9009\u8f68\u8ff9", "result": "\u6574\u4e2a\u5efa\u56fe\u4e0e\u89c4\u5212\u6d41\u7a0b\u5728GPU\u4e0a\u5b9e\u73b0\u4ee5\u652f\u6301\u9ad8\u9891\u91cd\u89c4\u5212\uff0c\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u5e76\u884c\u5efa\u56fe\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u672a\u77e5\u73af\u5883\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b9e\u65f6\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u6311\u6218"}}
{"id": "2512.22200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22200", "abs": "https://arxiv.org/abs/2512.22200", "authors": ["Dhruv Tiwari"], "title": "Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents", "comment": "7 pages, 3 figures. arXiv preprint", "summary": "The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this \"extrinsic maximization\" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.", "AI": {"tldr": "\u63d0\u51fa\u60c5\u611f\u542f\u53d1\u5b66\u4e60\u4fe1\u53f7\uff08EILS\uff09\u6846\u67b6\uff0c\u7528\u751f\u7269\u542f\u53d1\u7684\u5185\u90e8\u7a33\u6001\u63a7\u5236\u673a\u5236\u66ff\u4ee3\u4f20\u7edf\u5916\u90e8\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u63d0\u5347AI\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524dAI\u4f9d\u8d56\u5916\u90e8\u5b9a\u4e49\u7684\u9759\u6001\u5956\u52b1\u51fd\u6570\uff0c\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u5728\u5f00\u653e\u3001\u975e\u5e73\u7a33\u7684\u771f\u5b9e\u4e16\u754c\u4e2d\u8106\u5f31\u3002\u6807\u51c6\u667a\u80fd\u4f53\u7f3a\u4e4f\u5185\u90e8\u81ea\u4e3b\u6027\uff0c\u96be\u4ee5\u5728\u6ca1\u6709\u5bc6\u96c6\u53cd\u9988\u65f6\u63a2\u7d22\uff0c\u65e0\u6cd5\u9002\u5e94\u5206\u5e03\u53d8\u5316\uff0c\u9700\u8981\u5927\u91cf\u624b\u52a8\u8c03\u53c2\u3002", "method": "\u5f15\u5165\u60c5\u611f\u542f\u53d1\u5b66\u4e60\u4fe1\u53f7\uff08EILS\uff09\u6846\u67b6\uff0c\u5c06\u60c5\u611f\u5efa\u6a21\u4e3a\u8fde\u7eed\u7684\u7a33\u6001\u8bc4\u4f30\u4fe1\u53f7\uff08\u5982\u597d\u5947\u5fc3\u3001\u538b\u529b\u3001\u4fe1\u5fc3\uff09\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u4ece\u4ea4\u4e92\u5386\u53f2\u4e2d\u63a8\u5bfc\u4e3a\u5411\u91cf\u503c\u5185\u90e8\u72b6\u6001\uff0c\u5b9e\u65f6\u52a8\u6001\u8c03\u8282\u667a\u80fd\u4f53\u7684\u4f18\u5316\u666f\u89c2\u3002", "result": "\u5047\u8bbe\u8fd9\u79cd\u95ed\u73af\u7a33\u6001\u8c03\u8282\u80fd\u4f7fEILS\u667a\u80fd\u4f53\u5728\u6837\u672c\u6548\u7387\u548c\u975e\u5e73\u7a33\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u751f\u7269\u60c5\u611f\u4f5c\u4e3a\u9ad8\u7ea7\u7a33\u6001\u63a7\u5236\u673a\u5236\uff0cEILS\u6846\u67b6\u4e3a\u6784\u5efa\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u66f4\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.22214", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22214", "abs": "https://arxiv.org/abs/2512.22214", "authors": ["Naichuan Zheng", "Xiahai Lun", "Weiyi Li", "Yuchen Du"], "title": "Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition", "comment": null, "summary": "Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.", "AI": {"tldr": "\u63d0\u51faSignal-SGN++\u6846\u67b6\uff0c\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u7684\u8109\u51b2\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc71D\u8109\u51b2\u56fe\u5377\u79ef\u548c\u9891\u7387\u8109\u51b2\u5377\u79ef\u63d0\u53d6\u65f6\u7a7a-\u9891\u8c31\u7279\u5f81\uff0c\u5d4c\u5165\u62d3\u6251\u8f6c\u79fb\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u878d\u5408\u5206\u652f\uff0c\u5728\u4fdd\u6301\u9ad8\u80fd\u6548\u7684\u540c\u65f6\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fe\u5377\u79ef\u7f51\u7edc(GCNs)\u5728\u9aa8\u9abc\u52a8\u4f5c\u8bc6\u522b\u4e2d\u8868\u73b0\u826f\u597d\u4f46\u8ba1\u7b97\u80fd\u8017\u9ad8\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNNs)\u867d\u7136\u80fd\u6548\u9ad8\u4f46\u96be\u4ee5\u6355\u6349\u4eba\u4f53\u8fd0\u52a8\u7684\u65f6\u7a7a-\u9891\u7387\u8026\u5408\u4f9d\u8d56\u548c\u62d3\u6251\u7ed3\u6784\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u5f00\u53d1\u65e2\u80fd\u9ad8\u6548\u5904\u7406\u62d3\u6251\u7ed3\u6784\u53c8\u5177\u6709\u4f4e\u80fd\u8017\u7684\u6846\u67b6\u3002", "method": "1. \u4f7f\u75281D\u8109\u51b2\u56fe\u5377\u79ef(1D-SGC)\u548c\u9891\u7387\u8109\u51b2\u5377\u79ef(FSC)\u4f5c\u4e3a\u4e3b\u5e72\u7f51\u7edc\uff0c\u8054\u5408\u63d0\u53d6\u65f6\u7a7a\u548c\u9891\u8c31\u7279\u5f81\uff1b2. \u5d4c\u5165\u62d3\u6251\u8f6c\u79fb\u81ea\u6ce8\u610f\u529b(TSSA)\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u5728\u5b66\u4e60\u5230\u7684\u9aa8\u9abc\u62d3\u6251\u4e0a\u8def\u7531\u6ce8\u610f\u529b\uff1b3. \u5f15\u5165\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u878d\u5408(MWTF)\u5206\u652f\uff0c\u5c06\u8109\u51b2\u7279\u5f81\u5206\u89e3\u4e3a\u591a\u5206\u8fa8\u7387\u65f6\u9891\u8868\u793a\uff1b4. \u4f7f\u7528\u62d3\u6251\u611f\u77e5\u65f6\u9891\u878d\u5408(TATF)\u5355\u5143\uff0c\u7ed3\u5408\u7ed3\u6784\u5148\u9a8c\u4fdd\u6301\u62d3\u6251\u4e00\u81f4\u7684\u9891\u8c31\u878d\u5408\u3002", "result": "\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSignal-SGN++\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u57fa\u4e8eSNN\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u663e\u8457\u964d\u4f4e\u80fd\u8017\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdbGCNs\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "Signal-SGN++\u6210\u529f\u5730\u5c06\u62d3\u6251\u611f\u77e5\u4e0e\u8109\u51b2\u52a8\u6001\u76f8\u7ed3\u5408\uff0c\u4e3a\u9aa8\u9abc\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u590d\u6742GCNs\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002"}}
{"id": "2512.22734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22734", "abs": "https://arxiv.org/abs/2512.22734", "authors": ["Michelle Valenzuela", "Francisco Leiva", "Javier Ruiz-del-Solar"], "title": "Sistema de navegaci\u00f3n de cobertura para veh\u00edculos no holon\u00f3micos en ambientes de exterior", "comment": "13 pages, in Spanish language, 12 figures, accepted at Tercer Congreso Iberoamericano de Miner\u00eda Subterranea y a Cielo Abierto, UMining 2024", "summary": "In mobile robotics, coverage navigation refers to the deliberate movement of a robot with the purpose of covering a certain area or volume. Performing this task properly is fundamental for the execution of several activities, for instance, cleaning a facility with a robotic vacuum cleaner. In the mining industry, it is required to perform coverage in several unit processes related with material movement using industrial machinery, for example, in cleaning tasks, in dumps, and in the construction of tailings dam walls. The automation of these processes is fundamental to enhance the security associated with their execution. In this work, a coverage navigation system for a non-holonomic robot is presented. This work is intended to be a proof of concept for the potential automation of various unit processes that require coverage navigation like the ones mentioned before. The developed system includes the calculation of routes that allow a mobile platform to cover a specific area, and incorporates recovery behaviors in case that an unforeseen event occurs, such as the arising of dynamic or previously unmapped obstacles in the terrain to be covered, e.g., other machines or pedestrians passing through the area, being able to perform evasive maneuvers and post-recovery to ensure a complete coverage of the terrain. The system was tested in different simulated and real outdoor environments, obtaining results near 90% of coverage in the majority of experiments. The next step of development is to scale up the utilized robot to a mining machine/vehicle whose operation will be validated in a real environment. The result of one of the tests performed in the real world can be seen in the video available in https://youtu.be/gK7_3bK1P5g.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u975e\u5b8c\u6574\u673a\u5668\u4eba\u7684\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e8\u5728\u5b9e\u73b0\u77ff\u533a\u6e05\u6d01\u3001\u7b51\u575d\u7b49\u8986\u76d6\u4efb\u52a1\u7684\u81ea\u52a8\u5316\uff0c\u7cfb\u7edf\u5305\u542b\u8def\u5f84\u89c4\u5212\u4e0e\u5f02\u5e38\u6062\u590d\u529f\u80fd\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8fbe\u5230\u7ea690%\u7684\u8986\u76d6\u7387\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u8986\u76d6\u5bfc\u822a\u5728\u77ff\u4e1a\u7b49\u5de5\u4e1a\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u5982\u6e05\u6d01\u3001\u7b51\u575d\u7b49\u4efb\u52a1\u3002\u81ea\u52a8\u5316\u8fd9\u4e9b\u8fc7\u7a0b\u5bf9\u63d0\u5347\u4f5c\u4e1a\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u975e\u5b8c\u6574\u673a\u5668\u4eba\u7684\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u8def\u5f84\u8ba1\u7b97\u548c\u6062\u590d\u884c\u4e3a\u7684\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf\u3002\u7cfb\u7edf\u80fd\u4e3a\u79fb\u52a8\u5e73\u53f0\u8ba1\u7b97\u8986\u76d6\u7279\u5b9a\u533a\u57df\u7684\u8def\u5f84\uff0c\u5e76\u80fd\u5904\u7406\u52a8\u6001\u969c\u788d\u3001\u672a\u6620\u5c04\u969c\u788d\u7b49\u7a81\u53d1\u60c5\u51b5\uff0c\u6267\u884c\u89c4\u907f\u548c\u6062\u590d\u64cd\u4f5c\u4ee5\u786e\u4fdd\u5b8c\u6574\u8986\u76d6\u3002", "result": "\u5728\u591a\u79cd\u6a21\u62df\u548c\u771f\u5b9e\u5ba4\u5916\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u5927\u591a\u6570\u5b9e\u9a8c\u83b7\u5f97\u63a5\u8fd190%\u7684\u8986\u76d6\u7387\u3002\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5904\u7406\u52a8\u6001\u969c\u788d\u548c\u6062\u590d\u8986\u76d6\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u8986\u76d6\u5bfc\u822a\u5728\u77ff\u4e1a\u7b49\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u5316\u7684\u6f5c\u529b\u3002\u4e0b\u4e00\u6b65\u5c06\u6269\u5c55\u5230\u771f\u5b9e\u91c7\u77ff\u673a\u68b0\u4e0a\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u64cd\u4f5c\u6027\u80fd\u3002"}}
{"id": "2512.22226", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22226", "abs": "https://arxiv.org/abs/2512.22226", "authors": ["Naishan Zheng", "Jie Huang", "Qingpei Guo", "Feng Zhao"], "title": "VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs", "comment": "11 pages, 4 figures", "summary": "Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.", "AI": {"tldr": "VideoScaffold\u662f\u4e00\u4e2a\u7528\u4e8e\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u7684\u52a8\u6001\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u4e8b\u4ef6\u7c92\u5ea6\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u957f\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u7b56\u7565\uff08\u5982\u7a00\u758f\u91c7\u6837\u3001\u5e27\u538b\u7f29\u548c\u805a\u7c7b\uff09\u5728\u5904\u7406\u8fde\u7eed\u89c6\u9891\u6d41\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f1a\u4ea7\u751f\u788e\u7247\u5316\u6216\u8fc7\u5ea6\u538b\u7f29\u7684\u8f93\u51fa\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faVideoScaffold\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5f39\u6027\u5c3a\u5ea6\u4e8b\u4ef6\u5206\u5272\uff08EES\uff09\u8fdb\u884c\u9884\u6d4b\u5f15\u5bfc\u7684\u52a8\u6001\u8fb9\u754c\u7ec6\u5316\uff0c\u4ee5\u53ca\u5206\u5c42\u4e8b\u4ef6\u6574\u5408\uff08HEC\uff09\u5c06\u8bed\u4e49\u76f8\u5173\u7247\u6bb5\u9010\u6b65\u805a\u5408\u4e3a\u591a\u5c42\u6b21\u62bd\u8c61\u8868\u793a\u3002", "result": "\u5728\u79bb\u7ebf\u548c\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6846\u67b6\u6a21\u5757\u5316\u4e14\u5373\u63d2\u5373\u7528\uff0c\u80fd\u591f\u65e0\u7f1d\u6269\u5c55\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u50cf\u7684MLLMs\u5230\u8fde\u7eed\u89c6\u9891\u7406\u89e3\u3002", "conclusion": "VideoScaffold\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4e8b\u4ef6\u7c92\u5ea6\uff0c\u5728\u4fdd\u6301\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u4e49\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4ece\u5e27\u7ea7\u7406\u89e3\u5230\u4e8b\u4ef6\u7ea7\u63a8\u7406\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5197\u4f59\u548c\u65f6\u5e8f\u8fde\u8d2f\u6027\u95ee\u9898\u3002"}}
{"id": "2512.22823", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22823", "abs": "https://arxiv.org/abs/2512.22823", "authors": ["Sameer Sitoula", "Tej Bahadur Shahi", "Laxmi Prasad Bhatt", "Anisha Pokhrel", "Arjun Neupane"], "title": "NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with Linguistic Insights and Temporal Trends", "comment": "This paper is under consideration in Neural Computing & Applications (Springer) journal. This version may be deleted or updated at any time, depending on the journal's policy upon acceptance", "summary": "Social media (SM) platforms (e.g. Facebook, Twitter, and Reddit) are increasingly leveraged to share opinions and emotions, specifically during challenging events, such as natural disasters, pandemics, and political elections, and joyful occasions like festivals and celebrations. Among the SM platforms, Reddit provides a unique space for its users to anonymously express their experiences and thoughts on sensitive issues such as health and daily life. In this work, we present a novel dataset, called NepEMO, for multi-label emotion (MLE) and sentiment classification (SC) on the Nepali subreddit post. We curate and build a manually annotated dataset of 4,462 posts (January 2019- June 2025) written in English, Romanised Nepali and Devanagari script for five emotions (fear, anger, sadness, joy, and depression) and three sentiment classes (positive, negative, and neutral). We perform a detailed analysis of posts to capture linguistic insights, including emotion trends, co-occurrence of emotions, sentiment-specific n-grams, and topic modelling using Latent Dirichlet Allocation and TF-IDF keyword extraction. Finally, we compare various traditional machine learning (ML), deep learning (DL), and transformer models for MLE and SC tasks. The result shows that transformer models consistently outperform the ML and DL models for both tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NepEMO\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5c3c\u6cca\u5c14\u8bedReddit\u5e16\u5b50\u7684\u591a\u6807\u7b7e\u60c5\u611f\u548c\u60c5\u611f\u5206\u7c7b\uff0c\u5305\u542b4,462\u4e2a\u5e16\u5b50\uff0c\u6db5\u76d65\u79cd\u60c5\u7eea\u548c3\u79cd\u60c5\u611f\u7c7b\u522b\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5982Reddit\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u533f\u540d\u8868\u8fbe\u654f\u611f\u8bdd\u9898\uff08\u5982\u5065\u5eb7\u548c\u751f\u6d3b\uff09\u7684\u7a7a\u95f4\u3002\u5c3c\u6cca\u5c14\u8bedReddit\u5e16\u5b50\u4e2d\u7684\u60c5\u611f\u5206\u6790\u7f3a\u4e4f\u4e13\u95e8\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u5c3c\u6cca\u5c14\u8bed\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86NepEMO\u6570\u636e\u96c6\uff0c\u5305\u542b4,462\u4e2a\u5e16\u5b50\uff082019\u5e741\u6708-2025\u5e746\u6708\uff09\uff0c\u4f7f\u7528\u82f1\u8bed\u3001\u7f57\u9a6c\u5316\u5c3c\u6cca\u5c14\u8bed\u548c\u5929\u57ce\u4f53\u6587\u5b57\uff0c\u6807\u6ce8\u4e865\u79cd\u60c5\u7eea\uff08\u6050\u60e7\u3001\u6124\u6012\u3001\u60b2\u4f24\u3001\u559c\u60a6\u3001\u6291\u90c1\uff09\u548c3\u79cd\u60c5\u611f\u7c7b\u522b\uff08\u79ef\u6781\u3001\u6d88\u6781\u3001\u4e2d\u6027\uff09\u3002\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u8bed\u8a00\u5206\u6790\uff0c\u5305\u62ec\u60c5\u611f\u8d8b\u52bf\u3001\u60c5\u611f\u5171\u73b0\u3001\u60c5\u611f\u7279\u5b9an-gram\uff0c\u4ee5\u53ca\u4f7f\u7528LDA\u548cTF-IDF\u7684\u4e3b\u9898\u5efa\u6a21\u3002\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u548cTransformer\u6a21\u578b\u3002", "result": "Transformer\u6a21\u578b\u5728\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u548c\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "NepEMO\u6570\u636e\u96c6\u586b\u8865\u4e86\u5c3c\u6cca\u5c14\u8bed\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002Transformer\u6a21\u578b\u5728\u5c3c\u6cca\u5c14\u8bed\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2512.23162", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23162", "abs": "https://arxiv.org/abs/2512.23162", "authors": ["Yufan He", "Pengfei Guo", "Mengya Xu", "Zhaoshuo Li", "Andriy Myronenko", "Dillan Imans", "Bingjie Liu", "Dongren Yang", "Mingxue Gu", "Yongnan Ji", "Yueming Jin", "Ren Zhao", "Baiyong Shen", "Daguang Xu"], "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling", "comment": null, "summary": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.", "AI": {"tldr": "\u5229\u7528SurgWorld\u4e16\u754c\u6a21\u578b\u751f\u6210\u5408\u6210\u624b\u672f\u89c6\u9891\uff0c\u901a\u8fc7\u9006\u52a8\u529b\u5b66\u6a21\u578b\u63a8\u65ad\u4f2a\u8fd0\u52a8\u5b66\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u624b\u672f\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u624b\u672f\u673a\u5668\u4eba\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u7684\u6839\u672c\u969c\u788d\uff0c\u867d\u7136\u6709\u5927\u91cf\u624b\u672f\u89c6\u9891\u4f46\u7f3a\u4e4f\u5bf9\u5e94\u7684\u52a8\u4f5c\u6807\u7b7e\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u6a21\u4eff\u5b66\u4e60\u6216VLA\u8bad\u7ec3\u3002", "method": "1. \u6784\u5efaSATA\u6570\u636e\u96c6\uff08\u624b\u672f\u52a8\u4f5c\u6587\u672c\u5bf9\u9f50\uff09\uff1b2. \u57fa\u4e8e\u6700\u5148\u8fdb\u7684\u7269\u7406AI\u4e16\u754c\u6a21\u578b\u548cSATA\u6784\u5efaSurgWorld\uff0c\u751f\u6210\u591a\u6837\u5316\u3001\u53ef\u6cdb\u5316\u7684\u771f\u5b9e\u624b\u672f\u89c6\u9891\uff1b3. \u9996\u6b21\u4f7f\u7528\u9006\u52a8\u529b\u5b66\u6a21\u578b\u4ece\u5408\u6210\u624b\u672f\u89c6\u9891\u63a8\u65ad\u4f2a\u8fd0\u52a8\u5b66\u6570\u636e\uff0c\u751f\u6210\u5408\u6210\u7684\u914d\u5bf9\u89c6\u9891-\u52a8\u4f5c\u6570\u636e\u3002", "result": "\u4f7f\u7528\u8fd9\u4e9b\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684\u624b\u672fVLA\u7b56\u7565\u5728\u771f\u5b9e\u624b\u672f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6f14\u793a\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u624b\u672f\u89c6\u9891\u7684\u4e30\u5bcc\u6027\u548c\u751f\u6210\u5f0f\u4e16\u754c\u5efa\u6a21\uff0c\u4e3a\u81ea\u4e3b\u624b\u672f\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u6253\u5f00\u4e86\u901a\u5411\u53ef\u6cdb\u5316\u4e14\u6570\u636e\u9ad8\u6548\u7684\u624b\u672f\u673a\u5668\u4eba\u7b56\u7565\u7684\u5927\u95e8\u3002"}}
{"id": "2512.22280", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22280", "abs": "https://arxiv.org/abs/2512.22280", "authors": ["Varshith Gudur"], "title": "Valori: A Deterministic Memory Substrate for AI Systems", "comment": "7 pages, 1 figure. systems paper with empirical evaluation and determinism validation experiments. Code available at https://github.com/varshith-Git/Valori-Kernel", "summary": "Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).", "AI": {"tldr": "Valori\u63d0\u51fa\u786e\u5b9a\u6027AI\u5185\u5b58\u57fa\u677f\uff0c\u7528\u5b9a\u70b9\u7b97\u672f(Q16.16)\u66ff\u4ee3\u6d6e\u70b9\u8fd0\u7b97\uff0c\u4fdd\u8bc1\u8de8\u5e73\u53f0\u6bd4\u7279\u4e00\u81f4\u6027\uff0c\u89e3\u51b3AI\u7cfb\u7edf\u4e2d\u56e0\u786c\u4ef6\u5dee\u5f02\u5bfc\u81f4\u7684\u975e\u786e\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u4f9d\u8d56\u6d6e\u70b9\u8fd0\u7b97\u7684\u5411\u91cf\u5d4c\u5165\u5b58\u50a8\u548c\u641c\u7d22\uff0c\u4f46\u76f8\u540c\u6a21\u578b\u3001\u8f93\u5165\u548c\u4ee3\u7801\u5728\u4e0d\u540c\u786c\u4ef6\u67b6\u6784\uff08\u5982x86 vs ARM\uff09\u4e0a\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u5185\u5b58\u72b6\u6001\u548c\u68c0\u7d22\u7ed3\u679c\u3002\u8fd9\u79cd\u975e\u786e\u5b9a\u6027\u7834\u574f\u4e86\u53ef\u91cd\u73b0\u6027\u548c\u5b89\u5168\u90e8\u7f72\uff0c\u5bfc\u81f4\u6570\u636e\u9759\u9ed8\u5206\u6b67\uff0c\u5f71\u54cd\u53d7\u76d1\u7ba1\u884c\u4e1a\u7684\u5ba1\u8ba1\u8ffd\u8e2a\u3002", "method": "Valori\u91c7\u7528\u5b9a\u70b9\u7b97\u672f(Q16.16)\u66ff\u4ee3\u6d6e\u70b9\u5185\u5b58\u64cd\u4f5c\uff0c\u5c06\u5185\u5b58\u5efa\u6a21\u4e3a\u53ef\u91cd\u653e\u72b6\u6001\u673a\uff0c\u5728\u5185\u5b58\u8fb9\u754c\u5f3a\u5236\u6267\u884c\u786e\u5b9a\u6027\uff0c\u4fdd\u8bc1\u6bd4\u7279\u4e00\u81f4\u7684\u5185\u5b58\u72b6\u6001\u3001\u5feb\u7167\u548c\u641c\u7d22\u7ed3\u679c\u3002", "result": "Valori\u80fd\u591f\u4fdd\u8bc1\u8de8\u5e73\u53f0\u7684\u6bd4\u7279\u4e00\u81f4\u6027\u5185\u5b58\u72b6\u6001\u548c\u68c0\u7d22\u7ed3\u679c\uff0c\u8bc1\u660e\u975e\u786e\u5b9a\u6027\u5728\u7d22\u5f15\u6216\u68c0\u7d22\u4e4b\u524d\u5c31\u5df2\u4ea7\u751f\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5728\u5185\u5b58\u8fb9\u754c\u5f3a\u5236\u6267\u884c\u786e\u5b9a\u6027\u3002", "conclusion": "\u786e\u5b9a\u6027\u5185\u5b58\u662f\u53ef\u4fe1AI\u7cfb\u7edf\u7684\u5fc5\u8981\u57fa\u7840\u7ec4\u4ef6\uff0cValori\u4e3a\u89e3\u51b3AI\u7cfb\u7edf\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5176\u53c2\u8003\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.22291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22291", "abs": "https://arxiv.org/abs/2512.22291", "authors": ["Qingyue Cao", "Bo Jin", "Changwei Gong", "Xin Tong", "Wenzheng Li", "Xiaodong Zhou"], "title": "Multi-Head Spectral-Adaptive Graph Anomaly Detection", "comment": null, "summary": "Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This 'one-size-fits-all' approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a 'spectral fingerprint' containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.", "AI": {"tldr": "\u63d0\u51faMHSA-GNN\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210Chebyshev\u6ee4\u6ce2\u5668\u53c2\u6570\uff0c\u7ed3\u5408\u53cc\u6b63\u5219\u5316\u7b56\u7565\u89e3\u51b3\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5f02\u5e38\u8282\u70b9\u4f2a\u88c5\u548c\u5f02\u8d28\u6027\u5171\u5b58\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5f02\u5e38\u6a21\u5f0f\u65f6\u9762\u4e34\u6311\u6218\uff1a\u5f02\u5e38\u8282\u70b9\u5e38\u4f2a\u88c5\u6210\u6b63\u5e38\u8282\u70b9\uff0c\u5bfc\u81f4\u56fe\u4e2d\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5171\u5b58\u3002\u73b0\u6709\u8c31\u56fe\u795e\u7ecf\u7f51\u7edc\u4f7f\u7528\u56fa\u5b9a\u5168\u5c40\u6ee4\u6ce2\u5668\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u5ea6\u5e73\u6ed1\uff0c\u4e22\u5931\u5f02\u5e38\u68c0\u6d4b\u6240\u9700\u7684\u9ad8\u9891\u4fe1\u53f7\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u56fe\u5b9e\u4f8b\u7684\u81ea\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u591a\u5934\u8c31\u81ea\u9002\u5e94\u56fe\u795e\u7ecf\u7f51\u7edc(MHSA-GNN)\uff1a1) \u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\uff0c\u57fa\u4e8e\u5305\u542b\u7ed3\u6784\u7edf\u8ba1\u548cRayleigh\u5546\u7279\u5f81\u7684\"\u8c31\u6307\u7eb9\"\uff0c\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u52a8\u6001\u751f\u6210Chebyshev\u6ee4\u6ce2\u5668\u53c2\u6570\uff1b2) \u5f15\u5165\u53cc\u6b63\u5219\u5316\u7b56\u7565\u9632\u6b62\u591a\u5934\u673a\u5236\u6a21\u5f0f\u5d29\u6e83\uff1a\u7ed3\u5408\u5e08\u751f\u5bf9\u6bd4\u5b66\u4e60(TSC)\u786e\u4fdd\u8868\u793a\u51c6\u786e\u6027\uff0c\u4f7f\u7528Barlow Twins\u591a\u6837\u6027\u635f\u5931(BTD)\u5f3a\u5236\u5934\u95f4\u6b63\u4ea4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u9ad8\u9891\u5f02\u5e38\u4fe1\u53f7\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u9ad8\u5ea6\u5f02\u8d28\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MHSA-GNN\u901a\u8fc7\u5b9e\u4f8b\u7279\u5b9a\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\u7b56\u7565\u89e3\u51b3\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u7559\u9ad8\u9891\u5f02\u5e38\u4fe1\u53f7\u548c\u9002\u5e94\u56fe\u5f02\u8d28\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u91d1\u878d\u6b3a\u8bc8\u548c\u98ce\u9669\u63a7\u5236\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22441", "abs": "https://arxiv.org/abs/2512.22441", "authors": ["Zibin Liu", "Banglei Guana", "Yang Shanga", "Zhenbao Yu", "Yifei Bian", "Qifeng Yu"], "title": "LECalib: Line-Based Event Camera Calibration", "comment": "9 Pages, 6 figures", "summary": "Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u76f4\u7ebf\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u6846\u67b6\uff0c\u5229\u7528\u4eba\u9020\u73af\u5883\u4e2d\u5e38\u89c1\u7269\u4f53\u7684\u51e0\u4f55\u76f4\u7ebf\u8fdb\u884c\u6807\u5b9a\uff0c\u65e0\u9700\u4e13\u7528\u6807\u5b9a\u677f", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u95ea\u70c1\u56fe\u6848\u3001\u91cd\u5efa\u5f3a\u5ea6\u56fe\u50cf\u6216\u624b\u52a8\u653e\u7f6e\u6807\u5b9a\u7269\uff0c\u8017\u65f6\u4e14\u65e0\u6cd5\u9002\u5e94\u5feb\u901f\u53d8\u5316\u573a\u666f", "method": "\u76f4\u63a5\u4ece\u4e8b\u4ef6\u6d41\u4e2d\u68c0\u6d4b\u76f4\u7ebf\uff0c\u5229\u7528\u4e8b\u4ef6-\u76f4\u7ebf\u6807\u5b9a\u6a21\u578b\u751f\u6210\u76f8\u673a\u53c2\u6570\u521d\u59cb\u4f30\u8ba1\uff0c\u7136\u540e\u8fdb\u884c\u975e\u7ebf\u6027\u4f18\u5316\u7cbe\u4fee\uff0c\u9002\u7528\u4e8e\u5e73\u9762\u548c\u975e\u5e73\u9762\u76f4\u7ebf", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\uff0c\u5728\u5355\u76ee\u548c\u53cc\u76ee\u4e8b\u4ef6\u76f8\u673a\u4e0a\u5747\u8fdb\u884c\u4e86\u9a8c\u8bc1", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u76f4\u7ebf\u7684\u6807\u5b9a\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u95ee\u9898\uff0c\u65e0\u9700\u4e13\u7528\u6807\u5b9a\u677f\uff0c\u9002\u5e94\u5feb\u901f\u53d8\u5316\u573a\u666f"}}
{"id": "2512.22317", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22317", "abs": "https://arxiv.org/abs/2512.22317", "authors": ["Xudong Ling", "Tianxi Huang", "Qian Dong", "Tao He", "Chaorong Li", "Guiduo Duan"], "title": "LangPrecip: Language-Aware Multimodal Precipitation Nowcasting", "comment": null, "summary": "Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \\% and 19\\% gains in heavy-rainfall CSI at an 80-minute lead time.", "AI": {"tldr": "LangPrecip\uff1a\u4e00\u4e2a\u8bed\u8a00\u611f\u77e5\u7684\u591a\u6a21\u6001\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6c14\u8c61\u6587\u672c\u4f5c\u4e3a\u964d\u6c34\u6f14\u53d8\u7684\u8bed\u4e49\u8fd0\u52a8\u7ea6\u675f\uff0c\u5728Rectified Flow\u8303\u5f0f\u4e0b\u5c06\u6587\u672c\u548c\u96f7\u8fbe\u4fe1\u606f\u9ad8\u6548\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u6781\u7aef\u964d\u6c34\u9884\u62a5\u6027\u80fd\u3002", "motivation": "\u77ed\u671f\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u5177\u6709\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\u548c\u7ea6\u675f\u4e0d\u8db3\u7684\u7279\u70b9\uff0c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u6761\u4ef6\uff0c\u5bfc\u81f4\u672a\u6765\u8fd0\u52a8\u7ea6\u675f\u5f31\u4e14\u6a21\u7cca\u3002\u9700\u8981\u66f4\u5f3a\u7684\u8bed\u4e49\u7ea6\u675f\u6765\u6539\u5584\u9884\u62a5\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u611f\u77e5\u591a\u6a21\u6001\u9884\u62a5\u6846\u67b6(LangPrecip)\uff0c\u5c06\u6c14\u8c61\u6587\u672c\u4f5c\u4e3a\u964d\u6c34\u6f14\u53d8\u7684\u8bed\u4e49\u8fd0\u52a8\u7ea6\u675f\uff0c\u5728Rectified Flow\u8303\u5f0f\u4e0b\u5c06\u4e34\u8fd1\u9884\u62a5\u6784\u5efa\u4e3a\u8bed\u4e49\u7ea6\u675f\u7684\u8f68\u8ff9\u751f\u6210\u95ee\u9898\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9ad8\u6548\u96c6\u6210\u6587\u672c\u548c\u96f7\u8fbe\u4fe1\u606f\u3002", "result": "\u5728\u745e\u5178\u548cMRMS\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u572880\u5206\u949f\u9884\u62a5\u65f6\u6548\u4e0a\uff0c\u5f3a\u964d\u6c34CSI\u5206\u522b\u63d0\u5347\u8d85\u8fc760%\u548c19%\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b16\u4e07\u5bf9\u96f7\u8fbe\u5e8f\u5217\u548c\u8fd0\u52a8\u63cf\u8ff0\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6LangPrecip-160k\u3002", "conclusion": "\u5c06\u8bed\u8a00\u4f5c\u4e3a\u8bed\u4e49\u8fd0\u52a8\u7ea6\u675f\u80fd\u663e\u8457\u6539\u5584\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u4e2d\u3002\u591a\u6a21\u6001\u65b9\u6cd5\u4e3a\u6c14\u8c61\u9884\u62a5\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2512.23508", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.23508", "abs": "https://arxiv.org/abs/2512.23508", "authors": ["Alessio Benavoli", "Alessandro Facchini", "Marco Zaffalon"], "title": "Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities", "comment": null, "summary": "How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8AI\u5bf9\u9f50\u4e0e\u5b89\u5168\uff0c\u63d0\u51fa\u901a\u8fc7AI\u534f\u52a9\u6e38\u620f\u548c\u5173\u673a\u6e38\u620f\u6846\u67b6\u7814\u7a76\u5982\u4f55\u786e\u4fddAI\u7cfb\u7edf\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u5e76\u4fdd\u6301\u5b89\u5168\uff0c\u5f3a\u8c03\u9700\u8981\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u963f\u57fa\u7c73\u5fb7\u504f\u597d", "motivation": "\u786e\u4fddAI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u5e76\u4fdd\u6301\u5b89\u5168\u662f\u91cd\u8981\u6311\u6218\u3002\u73b0\u6709\u6846\u67b6\u5982AI\u534f\u52a9\u95ee\u9898\u548c\u5173\u673a\u95ee\u9898\u63ed\u793a\u4e86AI\u8bbe\u8ba1\u4e2d\u7684\u6838\u5fc3\u56f0\u96be\uff1aAI\u9700\u8981\u5b66\u4e60\u672a\u77e5\u7684\u4eba\u7c7b\u6548\u7528\u51fd\u6570\uff0c\u540c\u65f6\u8981\u6b63\u786e\u5904\u7406\u5173\u673a\u6307\u4ee4\u800c\u4e0d\u8bd5\u56fe\u5e72\u9884\u5173\u673a\u8fc7\u7a0b", "method": "\u91c7\u7528AI\u534f\u52a9\u6e38\u620f\u548cAI\u5173\u673a\u6e38\u620f\u4f5c\u4e3a\u5206\u6790\u6846\u67b6\u3002AI\u534f\u52a9\u95ee\u9898\u5173\u6ce8\u8bbe\u8ba1\u80fd\u5e2e\u52a9\u4eba\u7c7b\u6700\u5927\u5316\u5176\u6548\u7528\u51fd\u6570\u7684AI\u4ee3\u7406\uff0c\u4f46AI\u4e0d\u77e5\u9053\u8fd9\u4e9b\u51fd\u6570\u5fc5\u987b\u5b66\u4e60\u3002\u5173\u673a\u95ee\u9898\u8981\u6c42AI\u5728\u6309\u4e0b\u5173\u673a\u6309\u94ae\u65f6\u5173\u673a\uff0c\u4e0d\u5e72\u9884\u6309\u94ae\u6309\u4e0b\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u80fd\u529b", "result": "\u5206\u6790\u8868\u660e\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u9700\u8981AI\u4ee3\u7406\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u540c\u65f6\u5904\u7406\u4e0d\u5b8c\u5168\u504f\u597d\u548c\u975e\u963f\u57fa\u7c73\u5fb7\u504f\u597d\u3002\u8fd9\u610f\u5473\u7740AI\u9700\u8981\u80fd\u591f\u63a8\u7406\u672a\u77e5\u7684\u4eba\u7c7b\u6548\u7528\u51fd\u6570\uff0c\u5e76\u5728\u9762\u5bf9\u5173\u673a\u6307\u4ee4\u65f6\u505a\u51fa\u9002\u5f53\u54cd\u5e94", "conclusion": "\u786e\u4fddAI\u5b89\u5168\u5bf9\u9f50\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u504f\u597d\u7684AI\u7cfb\u7edf\u3002AI\u534f\u52a9\u548c\u5173\u673a\u6e38\u620f\u6846\u67b6\u4e3a\u5206\u6790\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u5bf9\u9c81\u68d2AI\u63a8\u7406\u80fd\u529b\u7684\u9700\u6c42"}}
{"id": "2512.22454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22454", "abs": "https://arxiv.org/abs/2512.22454", "authors": ["Haley Mody", "Namish Bansal", "Dennies Kiprono Bor", "Edward J. Oughton"], "title": "Comparing Object Detection Models for Electrical Substation Component Mapping", "comment": "26 pages, 13 figures", "summary": "Electrical substations are a significant component of an electrical grid. Indeed, the assets at these substations (e.g., transformers) are prone to disruption from many hazards, including hurricanes, flooding, earthquakes, and geomagnetically induced currents (GICs). As electrical grids are considered critical national infrastructure, any failure can have significant economic and public safety implications. To help prevent and mitigate these failures, it is thus essential that we identify key substation components to quantify vulnerability. Unfortunately, traditional manual mapping of substation infrastructure is time-consuming and labor-intensive. Therefore, an autonomous solution utilizing computer vision models is preferable, as it allows for greater convenience and efficiency. In this research paper, we train and compare the outputs of 3 models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US substation images. Each model is evaluated for detection accuracy, precision, and efficiency. We present the key strengths and limitations of each model, identifying which provides reliable and large-scale substation component mapping. Additionally, we utilize these models to effectively map the various substation components in the United States, showcasing a use case for machine learning in substation mapping.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff08YOLOv8\u3001YOLOv11\u3001RF-DETR\uff09\u5728\u7535\u529b\u53d8\u7535\u7ad9\u7ec4\u4ef6\u81ea\u52a8\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u65e8\u5728\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u53d8\u7535\u7ad9\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u6548\u6620\u5c04\u3002", "motivation": "\u7535\u529b\u53d8\u7535\u7ad9\u662f\u7535\u7f51\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u8d44\u4ea7\uff08\u5982\u53d8\u538b\u5668\uff09\u5bb9\u6613\u53d7\u5230\u98d3\u98ce\u3001\u6d2a\u6c34\u3001\u5730\u9707\u548c\u5730\u78c1\u611f\u5e94\u7535\u6d41\u7b49\u591a\u79cd\u707e\u5bb3\u7684\u7834\u574f\u3002\u7535\u7f51\u4f5c\u4e3a\u5173\u952e\u56fd\u5bb6\u57fa\u7840\u8bbe\u65bd\uff0c\u4efb\u4f55\u6545\u969c\u90fd\u53ef\u80fd\u5e26\u6765\u91cd\u5927\u7684\u7ecf\u6d4e\u548c\u516c\u5171\u5b89\u5168\u5f71\u54cd\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u53d8\u7535\u7ad9\u57fa\u7840\u8bbe\u65bd\u6620\u5c04\u65b9\u6cd5\u8017\u65f6\u8017\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u4e3b\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u6548\u7387\u548c\u4fbf\u5229\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u624b\u52a8\u6807\u6ce8\u7684\u7f8e\u56fd\u53d8\u7535\u7ad9\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff1aYOLOv8\u3001YOLOv11\u548cRF-DETR\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u548c\u6548\u7387\u3002\u7814\u7a76\u8fd8\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u5bf9\u7f8e\u56fd\u5404\u5730\u7684\u53d8\u7535\u7ad9\u7ec4\u4ef6\u8fdb\u884c\u4e86\u6709\u6548\u6620\u5c04\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u4e09\u79cd\u6a21\u578b\u5728\u53d8\u7535\u7ad9\u7ec4\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5206\u6790\u4e86\u5404\u81ea\u7684\u5173\u952e\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u54ea\u79cd\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u5927\u89c4\u6a21\u53d8\u7535\u7ad9\u7ec4\u4ef6\u6620\u5c04\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u673a\u5668\u5b66\u4e60\u5728\u53d8\u7535\u7ad9\u6620\u5c04\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u548c\u7ef4\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u9884\u9632\u548c\u51cf\u8f7b\u53d8\u7535\u7ad9\u6545\u969c\u5e26\u6765\u7684\u98ce\u9669\u3002"}}
{"id": "2512.22474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22474", "abs": "https://arxiv.org/abs/2512.22474", "authors": ["Taihang Lei", "Banglei Guan", "Minzu Liang", "Pengju Sun", "Jing Tao", "Yang Shang", "Qifeng Yu"], "title": "Event-based high temporal resolution measurement of shock wave motion field", "comment": null, "summary": "Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u51b2\u51fb\u6ce2\u8fd0\u52a8\u53c2\u6570\u6d4b\u91cf\u6846\u67b6\uff0c\u901a\u8fc7\u6781\u5750\u6807\u7f16\u7801\u3001\u81ea\u9002\u5e94ROI\u63d0\u53d6\u548c\u8fed\u4ee3\u659c\u7387\u5206\u6790\u5b9e\u73b0\u51b2\u51fb\u6ce2\u524d\u63d0\u53d6\uff0c\u6700\u7ec8\u5b9e\u73b03D\u91cd\u5efa\u548c\u7206\u70b8\u5f53\u91cf\u53cd\u6f14\uff0c\u6d4b\u91cf\u8bef\u5dee\u6700\u5c0f0.06%\uff0c\u6700\u59275.20%\u3002", "motivation": "\u51b2\u51fb\u6ce2\u5728\u7535\u529b\u573a\u6d4b\u8bd5\u548c\u635f\u4f24\u8bc4\u4f30\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u7cbe\u786e\u6d4b\u91cf\uff0c\u4f46\u51b2\u51fb\u6ce2\u5feb\u901f\u4e0d\u5747\u5300\u4f20\u64ad\u548c\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u6761\u4ef6\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002", "method": "1) \u5efa\u7acb\u6781\u5750\u6807\u7cfb\u7f16\u7801\u4e8b\u4ef6\u4ee5\u63ed\u793a\u51b2\u51fb\u6ce2\u4f20\u64ad\u6a21\u5f0f\uff0c\u901a\u8fc7\u4e8b\u4ef6\u504f\u79fb\u8ba1\u7b97\u8fdb\u884c\u81ea\u9002\u5e94ROI\u63d0\u53d6\uff1b2) \u5229\u7528\u901f\u5ea6\u53d8\u5316\u8fde\u7eed\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u659c\u7387\u5206\u6790\u63d0\u53d6\u51b2\u51fb\u6ce2\u524d\u4e8b\u4ef6\uff1b3) \u57fa\u4e8e\u4e8b\u4ef6\u5149\u5b66\u6210\u50cf\u6a21\u578b\u63a8\u5bfc\u4e8b\u4ef6\u51e0\u4f55\u6a21\u578b\u548c\u51b2\u51fb\u6ce2\u8fd0\u52a8\u53c2\u6570\uff0c\u7ed3\u54083D\u91cd\u5efa\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u89d2\u5ea6\u51b2\u51fb\u6ce2\u6d4b\u91cf\u3001\u8fd0\u52a8\u573a\u91cd\u5efa\u548c\u7206\u70b8\u5f53\u91cf\u53cd\u6f14\u3002\u901f\u5ea6\u6d4b\u91cf\u7ed3\u679c\u4e0e\u538b\u529b\u4f20\u611f\u5668\u548c\u7ecf\u9a8c\u516c\u5f0f\u5bf9\u6bd4\uff0c\u6700\u5927\u8bef\u5dee5.20%\uff0c\u6700\u5c0f\u8bef\u5dee0.06%\uff0c\u5b9e\u73b0\u4e86\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u901f\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u51b2\u51fb\u6ce2\u5feb\u901f\u4e0d\u5747\u5300\u4f20\u64ad\u7684\u6d4b\u91cf\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u51b2\u51fb\u6ce2\u8fd0\u52a8\u573a\u6d4b\u91cf\uff0c\u4ee3\u8868\u4e86\u8be5\u9886\u57df\u7684\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2512.22428", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.22428", "abs": "https://arxiv.org/abs/2512.22428", "authors": ["Jianxiang Xie", "Yuncheng Hua"], "title": "Causality-Inspired Safe Residual Correction for Multivariate Time Series", "comment": null, "summary": "While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also \"help in the wrong way\" by overcorrecting reliable predictions and causing local failures in unseen scenarios.\n  To address this critical \"safety gap,\" we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.\n  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.", "AI": {"tldr": "CRC\u662f\u4e00\u4e2a\u56e0\u679c\u542f\u53d1\u7684\u5b89\u5168\u6b8b\u5dee\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u81ea\u53d8\u91cf\u548c\u4ea4\u53c9\u53d8\u91cf\u52a8\u6001\u6765\u786e\u4fdd\u9884\u6d4b\u6027\u80fd\u4e0d\u9000\u5316\uff0c\u5177\u6709\u4e25\u683c\u7684\u5b89\u5168\u673a\u5236\u9632\u6b62\u6709\u5bb3\u66f4\u65b0\u3002", "motivation": "\u73b0\u4ee3\u591a\u5143\u9884\u6d4b\u5668\uff08\u5982Transformers\u548cGNNs\uff09\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5b58\u5728\u7279\u5b9a\u53d8\u91cf\u6216\u65f6\u95f4\u8303\u56f4\u7684\u7cfb\u7edf\u6027\u8bef\u5dee\uff0c\u4e14\u7f3a\u4e4f\u90e8\u7f72\u65f6\u6027\u80fd\u4e0d\u9000\u5316\u7684\u4fdd\u8bc1\u3002\u73b0\u6709\u6b8b\u5dee\u6821\u6b63\u65b9\u6cd5\u867d\u7136\u53ef\u80fd\u63d0\u9ad8\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f46\u53ef\u80fd\"\u4ee5\u9519\u8bef\u7684\u65b9\u5f0f\u5e2e\u52a9\"\uff0c\u5728\u672a\u89c1\u573a\u666f\u4e2d\u5bfc\u81f4\u5c40\u90e8\u5931\u8d25\u3002", "method": "CRC\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\uff1a1\uff09\u4f7f\u7528\u56e0\u679c\u542f\u53d1\u7684\u7f16\u7801\u5668\u901a\u8fc7\u89e3\u8026\u81ea\u53d8\u91cf\u548c\u4ea4\u53c9\u53d8\u91cf\u52a8\u6001\u6765\u66b4\u9732\u65b9\u5411\u611f\u77e5\u7ed3\u6784\uff1b2\uff09\u4f7f\u7528\u6df7\u5408\u6821\u6b63\u5668\u5efa\u6a21\u6b8b\u5dee\u8bef\u5dee\uff1b3\uff09\u901a\u8fc7\u4e25\u683c\u7684\u56db\u91cd\u5b89\u5168\u673a\u5236\u63a7\u5236\u6821\u6b63\u8fc7\u7a0b\uff0c\u9632\u6b62\u6709\u5bb3\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9884\u6d4b\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRC\u80fd\u6301\u7eed\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u5176\u6838\u5fc3\u5b89\u5168\u673a\u5236\u80fd\u786e\u4fdd\u6781\u9ad8\u7684\u975e\u9000\u5316\u7387\uff08NDR\uff09\uff0c\u4f7f\u5176\u6210\u4e3a\u9002\u5408\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u7684\u6821\u6b63\u6846\u67b6\u3002", "conclusion": "CRC\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u5b89\u5168\u6b8b\u5dee\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u542f\u53d1\u7684\u7ed3\u6784\u548c\u4e25\u683c\u7684\u5b89\u5168\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u90e8\u7f72\u65f6\u53ef\u80fd\u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u9884\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2512.22699", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22699", "abs": "https://arxiv.org/abs/2512.22699", "authors": ["Antar Kumar Biswas", "Masoud H. Nazari"], "title": "Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors", "comment": "This is a preprint of a manuscript currently under review at Electric Power Systems Research. The content may be subject to change following peer review", "summary": "This paper presents a novel learning-based framework for predicting power outages caused by extreme events. The proposed approach specifically targets low-probability, high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records (2014-2024) with weather, socio-economic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals underlying patterns of community vulnerability and provides a clearer understanding of outage risk during extreme conditions. Four machine learning models (Random Forest (RF), Support Vector Machine (SVM), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM)) are evaluated. Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves the lowest prediction error. Additionally, the results demonstrate that stronger economic conditions and more developed infrastructure are associated with lower outage occurrence.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6781\u7aef\u4e8b\u4ef6\u5bfc\u81f4\u505c\u7535\u7684\u6846\u67b6\uff0c\u6574\u5408\u591a\u6e90\u6570\u636e\uff0c\u5728\u5bc6\u6b47\u6839\u5dde\u9a8c\u8bc1\u4e2dLSTM\u8868\u73b0\u6700\u4f73\uff0c\u53d1\u73b0\u7ecf\u6d4e\u6761\u4ef6\u548c\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u4e0e\u505c\u7535\u53d1\u751f\u7387\u8d1f\u76f8\u5173\u3002", "motivation": "\u9488\u5bf9\u4f4e\u6982\u7387\u9ad8\u540e\u679c\u7684\u505c\u7535\u573a\u666f\uff0c\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u5168\u9762\u3002\u9700\u8981\u6574\u5408\u5929\u6c14\u3001\u793e\u4f1a\u7ecf\u6d4e\u3001\u57fa\u7840\u8bbe\u65bd\u7b49\u591a\u7ef4\u5ea6\u6570\u636e\uff0c\u5e76\u8003\u8651\u793e\u533a\u8106\u5f31\u6027\u56e0\u7d20\uff0c\u4ee5\u66f4\u51c6\u786e\u9884\u6d4b\u6781\u7aef\u4e8b\u4ef6\u4e0b\u7684\u505c\u7535\u98ce\u9669\u3002", "method": "\u6574\u5408EAGLE-I\u505c\u7535\u8bb0\u5f55\uff082014-2024\uff09\u4e0e\u5929\u6c14\u3001\u793e\u4f1a\u7ecf\u6d4e\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u5b63\u8282\u6027\u4e8b\u4ef6\u6570\u636e\uff0c\u52a0\u5165\u793e\u4f1a\u4eba\u53e3\u6307\u6807\u5206\u6790\u793e\u533a\u8106\u5f31\u6027\u3002\u8bc4\u4f30\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1a\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u81ea\u9002\u5e94\u63d0\u5347\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u3002\u5728\u5bc6\u6b47\u6839\u5dde\u4e0b\u534a\u5c9b\u53bf\u533a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\uff0cLSTM\u7f51\u7edc\u53d6\u5f97\u4e86\u6700\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\u3002\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u66f4\u5f3a\u7684\u7ecf\u6d4e\u6761\u4ef6\u548c\u66f4\u53d1\u8fbe\u7684\u57fa\u7840\u8bbe\u65bd\u4e0e\u66f4\u4f4e\u7684\u505c\u7535\u53d1\u751f\u7387\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u9884\u6d4b\u6781\u7aef\u4e8b\u4ef6\u5bfc\u81f4\u7684\u505c\u7535\uff0cLSTM\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002\u793e\u4f1a\u7ecf\u6d4e\u548c\u57fa\u7840\u8bbe\u65bd\u56e0\u7d20\u5bf9\u505c\u7535\u98ce\u9669\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u89c4\u5212\u548c\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2512.22903", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22903", "abs": "https://arxiv.org/abs/2512.22903", "authors": ["Chumeng Liang", "Zhanyang Jin", "Zahaib Akhtar", "Mona Pereira", "Haofei Yu", "Jiaxuan You"], "title": "Debugging Tabular Log as Dynamic Graphs", "comment": null, "summary": "Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.", "AI": {"tldr": "\u63d0\u51faGraphLogDebugger\u6846\u67b6\uff0c\u57fa\u4e8e\u52a8\u6001\u56fe\u6765\u8c03\u8bd5\u8868\u683c\u65e5\u5fd7\uff0c\u901a\u8fc7\u6784\u5efa\u5bf9\u8c61\u548c\u4e8b\u4ef6\u7684\u5f02\u8d28\u8282\u70b9\u8fde\u63a5\uff0c\u5c06\u7cfb\u7edf\u6062\u590d\u4e3a\u6f14\u5316\u52a8\u6001\u56fe\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u52a8\u6001GNN\u5373\u53ef\u8d85\u8d8aLLMs\u7684\u8c03\u8bd5\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5904\u7406\u6587\u672c\u4e30\u5bcc\u7684\u8868\u683c\u65e5\u5fd7\u6570\u636e\u8fc7\u5ea6\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5176\u4ed6\u91cd\u578b\u6a21\u578b\uff0c\u5bfc\u81f4\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8c03\u8bd5\u8868\u683c\u65e5\u5fd7\u3002", "method": "\u63d0\u51faGraphLogDebugger\u6846\u67b6\uff1a1) \u4e3a\u5bf9\u8c61\u548c\u4e8b\u4ef6\u6784\u5efa\u5f02\u8d28\u8282\u70b9\uff1b2) \u8fde\u63a5\u8282\u70b9\u95f4\u8fb9\uff1b3) \u5c06\u7cfb\u7edf\u6062\u590d\u4e3a\u6f14\u5316\u52a8\u6001\u56fe\uff1b4) \u4f7f\u7528\u7b80\u5355\u7684\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8c03\u8bd5\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u7cfb\u7edf\u548c\u5b66\u672f\u8bba\u6587\u7684\u771f\u5b9e\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u7b80\u5355\u7684\u52a8\u6001GNN\u5728\u8c03\u8bd5\u8868\u683c\u65e5\u5fd7\u65b9\u9762\u80fd\u591f\u8d85\u8d8a\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u56fe\u5efa\u6a21\uff0cGraphLogDebugger\u6846\u67b6\u80fd\u591f\u6709\u6548\u8c03\u8bd5\u8868\u683c\u65e5\u5fd7\uff0c\u76f8\u6bd4\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.22772", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22772", "abs": "https://arxiv.org/abs/2512.22772", "authors": ["Xuyan Li", "Jie Wang", "Zheng Yan"], "title": "GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks", "comment": null, "summary": "Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.", "AI": {"tldr": "GRExplainer\uff1a\u9996\u4e2a\u901a\u7528\u3001\u9ad8\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684TGNN\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u8282\u70b9\u5e8f\u5217\u7edf\u4e00\u7279\u5f81\u8868\u793a\uff0c\u9002\u7528\u4e8e\u5feb\u7167\u5f0f\u548c\u4e8b\u4ef6\u5f0fTGNN\uff0c\u5229\u7528BFS\u548c\u65f6\u5e8f\u4fe1\u606f\u63d0\u9ad8\u6548\u7387\uff0c\u57fa\u4e8eRNN\u81ea\u52a8\u751f\u6210\u89e3\u91ca\u3002", "motivation": "TGNN\u5728\u52a8\u6001\u56fe\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709TGNN\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u9488\u5bf9\u7279\u5b9aTGNN\u7c7b\u578b\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\uff1b2\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\uff1b3\uff09\u5ffd\u89c6\u89e3\u91ca\u7684\u7ed3\u6784\u8fde\u901a\u6027\u4e14\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\uff0c\u7528\u6237\u53cb\u597d\u6027\u5dee\u3002", "method": "\u63d0\u51faGRExplainer\u65b9\u6cd5\uff1a1\uff09\u63d0\u53d6\u8282\u70b9\u5e8f\u5217\u4f5c\u4e3a\u7edf\u4e00\u7279\u5f81\u8868\u793a\uff0c\u4f7f\u5176\u72ec\u7acb\u4e8e\u7279\u5b9a\u8f93\u5165\u683c\u5f0f\uff0c\u9002\u7528\u4e8e\u5feb\u7167\u5f0f\u548c\u4e8b\u4ef6\u5f0fTGNN\uff1b2\uff09\u5229\u7528\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\uff08BFS\uff09\u548c\u65f6\u5e8f\u4fe1\u606f\u6784\u5efa\u8f93\u5165\u8282\u70b9\u5e8f\u5217\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff1b3\uff09\u8bbe\u8ba1\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u7684\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u8fde\u7eed\u89e3\u91ca\u751f\u6210\u3002", "result": "\u57286\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c3\u4e2a\u76ee\u6807TGNN\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRExplainer\u5728\u901a\u7528\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u53cb\u597d\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GRExplainer\u662f\u9996\u4e2a\u901a\u7528\u3001\u9ad8\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684TGNN\u89e3\u91ca\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8282\u70b9\u5e8f\u5217\u8868\u793a\u3001\u4f18\u5316\u7684\u8ba1\u7b97\u7b56\u7565\u548c\u81ea\u52a8\u5316\u751f\u6210\u673a\u5236\uff0c\u4e3aTGNN\u7684\u900f\u660e\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22979", "abs": "https://arxiv.org/abs/2512.22979", "authors": ["Huiming Yang", "Linglin Liao", "Fei Ding", "Sibo Wang", "Zijian Zeng"], "title": "PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects", "comment": null, "summary": "Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.", "AI": {"tldr": "PoseStreamer\uff1a\u7528\u4e8e\u9ad8\u901f\u8fd0\u52a8\u573a\u666f\u7684\u9c81\u68d2\u591a\u6a21\u60016DoF\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u59ff\u6001\u8bb0\u5fc6\u961f\u5217\u3001\u7269\u4f53\u4e2d\u5fc32D\u8ddf\u8e2a\u5668\u548c\u5c04\u7ebf\u59ff\u6001\u6ee4\u6ce2\u5668\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6MoCapCube6D\u4e0a\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u4f20\u7edfRGB\u76f8\u673a\u5728\u9ad8\u901f\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u5b58\u5728\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u800c\u73b0\u67096DoF\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u9ad8\u901f\u7269\u4f53\u8fd0\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u9ad8\u901f\u8fd0\u52a8\u573a\u666f\u8bbe\u8ba1\u7684\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u3002", "method": "\u63d0\u51faPoseStreamer\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u81ea\u9002\u5e94\u59ff\u6001\u8bb0\u5fc6\u961f\u5217\uff0c\u5229\u7528\u5386\u53f2\u65b9\u5411\u7ebf\u7d22\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff1b2) \u7269\u4f53\u4e2d\u5fc32D\u8ddf\u8e2a\u5668\uff0c\u63d0\u4f9b\u5f3a2D\u5148\u9a8c\u4ee5\u63d0\u53473D\u4e2d\u5fc3\u53ec\u56de\uff1b3) \u5c04\u7ebf\u59ff\u6001\u6ee4\u6ce2\u5668\uff0c\u6cbf\u76f8\u673a\u5c04\u7ebf\u8fdb\u884c\u51e0\u4f55\u7ec6\u5316\u3002\u540c\u65f6\u6784\u5efa\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6MoCapCube6D\u7528\u4e8e\u5feb\u901f\u8fd0\u52a8\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPoseStreamer\u5728\u9ad8\u901f\u8fd0\u52a8\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4f5c\u4e3a\u65e0\u6a21\u677f\u6846\u67b6\u5bf9\u672a\u89c1\u8fc7\u7684\u8fd0\u52a8\u7269\u4f53\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PoseStreamer\u4e3a\u89e3\u51b3\u9ad8\u901f\u8fd0\u52a8\u573a\u666f\u4e0b\u76846DoF\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u65f6\u95f4\u4e00\u81f4\u6027\u30012D\u5148\u9a8c\u548c\u51e0\u4f55\u7ec6\u5316\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2512.23262", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23262", "abs": "https://arxiv.org/abs/2512.23262", "authors": ["Tao Li", "Peilin Li", "Kui Lu", "Yilei Wang", "Junliang Shang", "Guangshun Li", "Huiyu Zhou"], "title": "PFed-Signal: An ADR Prediction Model based on Federated Learning", "comment": "IEEE International Conference on Bioinformatics and Biomedicine", "summary": "The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.", "AI": {"tldr": "PFed-signal\uff1a\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u4fe1\u53f7\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u6b27\u6c0f\u8ddd\u79bb\u6d88\u9664FAERS\u6570\u636e\u4e2d\u7684\u504f\u5dee\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "FAERS\u7cfb\u7edf\u4e2d\u5b58\u5728\u504f\u5dee\u8bb0\u5f55\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u9884\u6d4b\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u53ef\u80fd\u8bef\u5bfc\u5728\u7ebf\u8bca\u65ad\u3002\u4f20\u7edf\u65b9\u6cd5\u5982ROR\u548cPRR\u4f9d\u8d56\u7edf\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u6d88\u9664\u504f\u5dee\u6570\u636e\uff0c\u5bfc\u81f4\u4fe1\u53f7\u9884\u6d4b\u4e0d\u51c6\u786e", "method": "\u63d0\u51faPFed-signal\u8054\u90a6\u5b66\u4e60\u6a21\u578b\uff1a1) Pfed-Split\u65b9\u6cd5\u6309ADR\u5206\u5272\u539f\u59cb\u6570\u636e\u96c6\uff1b2) ADR-signal\u6a21\u578b\u5305\u62ec\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u504f\u5dee\u6570\u636e\u8bc6\u522b\uff08\u4f7f\u7528\u6b27\u6c0f\u8ddd\u79bb\uff09\u548c\u57fa\u4e8eTransformer\u7684ADR\u9884\u6d4b\u6a21\u578b", "result": "\u5728\u6e05\u6d17\u540e\u7684\u6570\u636e\u96c6\u4e0a\uff0cROR\u548cPRR\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002PFed-Signal\u7684\u51c6\u786e\u73870.887\u3001F1\u5206\u65700.890\u3001\u53ec\u56de\u73870.913\u3001AUC 0.957\uff0c\u5747\u9ad8\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "PFed-signal\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u548c\u6b27\u6c0f\u8ddd\u79bb\u6709\u6548\u6d88\u9664FAERS\u6570\u636e\u504f\u5dee\uff0c\u663e\u8457\u63d0\u9ad8\u4e86ADR\u4fe1\u53f7\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5"}}
{"id": "2512.23239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23239", "abs": "https://arxiv.org/abs/2512.23239", "authors": ["Fan Wei", "Runmin Dong", "Yushan Lai", "Yixiang Yang", "Zhaoyang Luo", "Jinxiao Zhang", "Miao Yang", "Shuai Yuan", "Jiyao Zhao", "Bin Luo", "Haohuan Fu"], "title": "RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models", "comment": null, "summary": "Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u6570\u636e\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u503c\u7b5b\u9009\u548c\u573a\u666f\u611f\u77e5\u805a\u7c7b\uff0c\u5728\u9ad8\u526a\u679d\u7387\u4e0b\u9009\u62e9\u9ad8\u8d28\u91cf\u9065\u611f\u6570\u636e\u5b50\u96c6\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u57fa\u7840\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u9065\u611f\u6269\u6563\u57fa\u7840\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u5168\u5c40\u4ee3\u8868\u6027\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5b58\u5728\u5197\u4f59\u3001\u566a\u58f0\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u964d\u4f4e\u8bad\u7ec3\u6548\u7387\u5e76\u963b\u788d\u6536\u655b\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u7b80\u5355\u805a\u5408\u591a\u4e2a\u5206\u7c7b\u6570\u636e\u96c6\u6216\u5e94\u7528\u7b80\u5355\u53bb\u91cd\uff0c\u5ffd\u89c6\u4e86\u751f\u6210\u6a21\u578b\u7684\u5206\u5e03\u9700\u6c42\u548c\u9065\u611f\u56fe\u50cf\u7684\u5f02\u8d28\u6027\u3002", "method": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u6570\u636e\u526a\u679d\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u71b5\u7684\u51c6\u5219\u9ad8\u6548\u53bb\u9664\u4f4e\u4fe1\u606f\u6837\u672c\uff1b2) \u5229\u7528\u9065\u611f\u573a\u666f\u5206\u7c7b\u6570\u636e\u96c6\u4f5c\u4e3a\u53c2\u8003\u57fa\u51c6\uff0c\u8fdb\u884c\u573a\u666f\u611f\u77e5\u805a\u7c7b\u548c\u5206\u5c42\u91c7\u6837\uff0c\u5e73\u8861\u805a\u7c7b\u7ea7\u5747\u5300\u6027\u548c\u6837\u672c\u4ee3\u8868\u6027\uff0c\u5728\u9ad8\u526a\u679d\u7387\u4e0b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u9009\u62e9\u3002", "result": "\u5373\u4f7f\u526a\u679d85%\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u4ecd\u80fd\u663e\u8457\u6539\u5584\u6536\u655b\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bad\u7ec3\u7684\u6269\u6563\u57fa\u7840\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8d85\u5206\u8fa8\u7387\u548c\u8bed\u4e49\u56fe\u50cf\u5408\u6210\uff09\u4e2d\u6301\u7eed\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u6570\u636e\u526a\u679d\u8303\u5f0f\u4e3a\u5f00\u53d1\u9065\u611f\u751f\u6210\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u80fd\u591f\u5728\u9ad8\u526a\u679d\u7387\u4e0b\u9009\u62e9\u9ad8\u8d28\u91cf\u6570\u636e\u5b50\u96c6\uff0c\u4f7f\u521d\u6b65\u57fa\u7840\u6a21\u578b\u5feb\u901f\u6536\u655b\u5e76\u4f5c\u4e3a\u591a\u529f\u80fd\u9aa8\u5e72\u7f51\u7edc\u3002"}}
{"id": "2512.23255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23255", "abs": "https://arxiv.org/abs/2512.23255", "authors": ["Masaya Takabe", "Hiroshi Watanabe", "Sujun Hong", "Tomohiro Ikai", "Zheming Fan", "Ryo Ishimoto", "Kakeru Sugimoto", "Ruri Imichi"], "title": "Contour Information Aware 2D Gaussian Splatting for Image Representation", "comment": null, "summary": "Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f6e\u5ed3\u4fe1\u606f\u76842D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7269\u4f53\u5206\u5272\u5148\u9a8c\u6765\u6539\u5584\u56fe\u50cf\u538b\u7f29\u65f6\u7684\u8fb9\u7f18\u8d28\u91cf", "motivation": "\u73b0\u6709\u76842D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728Gaussian\u6570\u91cf\u8f83\u5c11\u65f6\u4f1a\u4ea7\u751f\u6a21\u7cca\u6216\u4e0d\u6e05\u6670\u7684\u8fb9\u754c\uff0c\u7f3a\u4e4f\u8f6e\u5ed3\u611f\u77e5\u80fd\u529b", "method": "\u63d0\u51fa\u8f6e\u5ed3\u4fe1\u606f\u611f\u77e5\u76842D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u5c06\u7269\u4f53\u5206\u5272\u5148\u9a8c\u878d\u5165\u9ad8\u65af\u8868\u793a\uff0c\u901a\u8fc7\u7ea6\u675f\u6bcf\u4e2aGaussian\u5728\u7279\u5b9a\u5206\u5272\u533a\u57df\u5185\u8fdb\u884c\u5149\u6805\u5316\uff0c\u9632\u6b62\u8de8\u8fb9\u754c\u6df7\u5408\uff0c\u5e76\u5f15\u5165\u9884\u70ed\u65b9\u6848\u7a33\u5b9a\u8bad\u7ec3", "result": "\u5728\u5408\u6210\u8272\u5361\u548cDAVIS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u8fb9\u7f18\u533a\u57df\u5b9e\u73b0\u4e86\u6bd4\u73b0\u67092DGS\u65b9\u6cd5\u66f4\u9ad8\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728Gaussian\u6570\u91cf\u5f88\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4fdd\u6301\u5feb\u901f\u6e32\u67d3\u548c\u4f4e\u5185\u5b58\u4f7f\u7528", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5206\u5272\u5148\u9a8c\u548c\u8f6e\u5ed3\u611f\u77e5\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e862D\u9ad8\u65af\u6cfc\u6e85\u5728\u538b\u7f29\u65f6\u7684\u8fb9\u7f18\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8fb9\u7f18\u4fdd\u6301\u6027\u80fd"}}
{"id": "2512.23379", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23379", "abs": "https://arxiv.org/abs/2512.23379", "authors": ["Le Shen", "Qiao Qian", "Tan Yu", "Ke Zhou", "Tianhang Yu", "Yu Zhan", "Zhenjie Wang", "Ming Tao", "Shunshun Yin", "Siyuan Liu"], "title": "SoulX-LiveTalk Technical Report", "comment": "12 pages, 6 figures", "summary": "Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \\textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \\textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \\textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \\textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \\textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.", "AI": {"tldr": "SoulX-LiveTalk\u662f\u4e00\u4e2a14B\u53c2\u6570\u7684\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u865a\u62df\u4eba\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u84b8\u998f\u548c\u81ea\u6821\u6b63\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u542f\u52a8\u5ef6\u8fdf\u548c32FPS\u5b9e\u65f6\u541e\u5410\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5728\u5b9e\u65f6\u3001\u65e0\u9650\u65f6\u957f\u3001\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u4eba\u751f\u6210\u4e2d\u5b58\u5728\u8ba1\u7b97\u8d1f\u8f7d\u4e0e\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u7684\u51b2\u7a81\uff0c\u901a\u5e38\u9700\u8981\u727a\u7272\u89c6\u89c9\u8d28\u91cf\uff08\u5f3a\u5236\u5355\u5411\u6ce8\u610f\u529b\u6216\u51cf\u5c11\u6a21\u578b\u5bb9\u91cf\uff09\u3002", "method": "\u91c7\u752814B\u53c2\u6570\u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u6211\u6821\u6b63\u53cc\u5411\u84b8\u998f\u7b56\u7565\u4fdd\u7559\u89c6\u9891\u5757\u5185\u7684\u53cc\u5411\u6ce8\u610f\u529b\uff0c\u4fdd\u6301\u65f6\u7a7a\u76f8\u5173\u6027\uff1b\u5f15\u5165\u591a\u6b65\u56de\u987e\u81ea\u6821\u6b63\u673a\u5236\u9632\u6b62\u7d2f\u79ef\u9519\u8bef\uff1b\u5f00\u53d1\u5168\u6808\u63a8\u7406\u52a0\u901f\u5957\u4ef6\uff0c\u5305\u62ec\u6df7\u5408\u5e8f\u5217\u5e76\u884c\u3001\u5e76\u884cVAE\u548c\u5185\u6838\u7ea7\u4f18\u5316\u3002", "result": "\u9996\u6b21\u572814B\u89c4\u6a21\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u542f\u52a8\u5ef6\u8fdf\uff080.87\u79d2\uff09\u548c32FPS\u5b9e\u65f6\u541e\u5410\uff0c\u4e3a\u9ad8\u4fdd\u771f\u4ea4\u4e92\u5f0f\u6570\u5b57\u4eba\u5408\u6210\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "SoulX-LiveTalk\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5411\u6ce8\u610f\u529b\u84b8\u998f\u548c\u81ea\u6821\u6b63\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5728\u5b9e\u65f6\u865a\u62df\u4eba\u751f\u6210\u4e2d\u7684\u8ba1\u7b97-\u5ef6\u8fdf\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u751f\u6210\u3002"}}
{"id": "2512.23537", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23537", "abs": "https://arxiv.org/abs/2512.23537", "authors": ["Binhe Yu", "Zhen Wang", "Kexin Li", "Yuqian Yuan", "Wenqiao Zhang", "Long Chen", "Juncheng Li", "Jun Xiao", "Yueting Zhuang"], "title": "AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization", "comment": null, "summary": "Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.", "AI": {"tldr": "AnyMS\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u4e3b\u4f53\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7ea7\u6ce8\u610f\u529b\u89e3\u8026\u673a\u5236\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u534f\u8c03\u6587\u672c\u63d0\u793a\u3001\u4e3b\u4f53\u56fe\u50cf\u548c\u5e03\u5c40\u7ea6\u675f\uff0c\u5b9e\u73b0\u6587\u672c\u5bf9\u9f50\u3001\u4e3b\u4f53\u8eab\u4efd\u4fdd\u6301\u548c\u5e03\u5c40\u63a7\u5236\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u591a\u4e3b\u4f53\u5b9a\u5236\u65b9\u6cd5\u5728\u5e73\u8861\u6587\u672c\u5bf9\u9f50\u3001\u4e3b\u4f53\u8eab\u4efd\u4fdd\u6301\u548c\u5e03\u5c40\u63a7\u5236\u4e09\u4e2a\u5173\u952e\u76ee\u6807\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u4f9d\u8d56\u989d\u5916\u8bad\u7ec3\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faAnyMS\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u7684\u53cc\u7ea7\u6ce8\u610f\u529b\u89e3\u8026\u673a\u5236\uff1a\u5168\u5c40\u89e3\u8026\u5206\u79bb\u6587\u672c\u548c\u89c6\u89c9\u6761\u4ef6\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u786e\u4fdd\u6587\u672c\u5bf9\u9f50\uff1b\u5c40\u90e8\u89e3\u8026\u5c06\u6bcf\u4e2a\u4e3b\u4f53\u7684\u6ce8\u610f\u529b\u9650\u5236\u5728\u5176\u6307\u5b9a\u533a\u57df\u9632\u6b62\u51b2\u7a81\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u9002\u914d\u5668\u63d0\u53d6\u4e3b\u4f53\u7279\u5b9a\u7279\u5f81\uff0c\u65e0\u9700\u4e3b\u4f53\u5b66\u4e60\u6216\u9002\u914d\u5668\u8c03\u4f18\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eAnyMS\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u590d\u6742\u7ec4\u5408\u5e76\u6269\u5c55\u5230\u66f4\u591a\u4e3b\u4f53\u6570\u91cf\u3002", "conclusion": "AnyMS\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u89e3\u8026\u673a\u5236\u548c\u9884\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e3b\u4f53\u5b9a\u5236\u4e2d\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23576", "abs": "https://arxiv.org/abs/2512.23576", "authors": ["Ethan Chern", "Zhulin Hu", "Bohao Tang", "Jiadi Su", "Steffi Chern", "Zhijie Deng", "Pengfei Liu"], "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "comment": null, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b020\u500d\u63a8\u7406\u52a0\u901f\uff0c\u5e76\u6784\u5efa\u4e86LiveTalk\u7cfb\u7edf\u5b9e\u73b0\u5b9e\u65f6\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u5b58\u5728\u89c6\u89c9\u4f2a\u5f71\u548c\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u81ea\u7136\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u84b8\u998f\u914d\u65b9\uff0c\u91cd\u70b9\u5173\u6ce8\u6761\u4ef6\u8f93\u5165\u8d28\u91cf\u4ee5\u53ca\u5728\u7ebf\u7b56\u7565\u4f18\u5316\u7684\u521d\u59cb\u5316\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u5c06\u6a21\u578b\u4ece\u53cc\u5411\u53bb\u566a\u84b8\u998f\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002", "result": "\u5728HDTF\u3001AVSpeech\u548cCelebV-HQ\u7b49\u591a\u6a21\u6001\u6761\u4ef6\u5316\u5934\u50cf\u89c6\u9891\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u84b8\u998f\u6a21\u578b\u4ee520\u500d\u66f4\u4f4e\u7684\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u5339\u914d\u4e86\u5b8c\u6574\u6b65\u9aa4\u53cc\u5411\u57fa\u7ebf\u7684\u89c6\u89c9\u8d28\u91cf\u3002\u6784\u5efa\u7684LiveTalk\u7cfb\u7edf\u5728\u54cd\u5e94\u5ef6\u8fdf\u4e0a\u4ece1-2\u5206\u949f\u964d\u4f4e\u5230\u5b9e\u65f6\u751f\u6210\uff0c\u5728\u591a\u8f6e\u89c6\u9891\u8fde\u8d2f\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u4e0a\u4f18\u4e8eSora2\u3001Veo3\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u89c6\u9891\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f7f\u65e0\u7f1d\u7684\u4eba\u673a\u591a\u6a21\u6001\u4ea4\u4e92\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u591a\u6a21\u6001\u4ea4\u4e92AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.23628", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23628", "abs": "https://arxiv.org/abs/2512.23628", "authors": ["Shu Pu", "Boya Zeng", "Kaichen Zhou", "Mengyu Wang", "Zhuang Liu"], "title": "Memorization in 3D Shape Generation: An Empirical Study", "comment": null, "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u6765\u91cf\u53163D\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u6570\u636e\u6a21\u6001\u3001\u591a\u6837\u6027\u548c\u5efa\u6a21\u8bbe\u8ba1\u5bf9\u8bb0\u5fc6\u5316\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u51cf\u5c11\u8bb0\u5fc6\u5316\u7684\u6709\u6548\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u57283D\u89c6\u89c9\u4e2d\u7528\u4e8e\u5408\u6210\u65b0\u5f62\u72b6\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5176\u751f\u6210\u662f\u5426\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u8bad\u7ec3\u5f62\u72b6\u3002\u7406\u89e3\u8bb0\u5fc6\u5316\u6709\u52a9\u4e8e\u9632\u6b62\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\u5e76\u63d0\u9ad8\u751f\u6210\u7ed3\u679c\u7684\u591a\u6837\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u6765\u91cf\u53163D\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\uff0c\u9996\u5148\u5e94\u7528\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7136\u540e\u901a\u8fc7\u6f5c\u5728\u5411\u91cf\u96c6\u6269\u6563\u6a21\u578b\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u7814\u7a76\u6570\u636e\u548c\u5efa\u6a21\u8bbe\u8ba1\u5bf9\u8bb0\u5fc6\u5316\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u8bb0\u5fc6\u5316\u53d6\u51b3\u4e8e\u6570\u636e\u6a21\u6001\uff0c\u968f\u6570\u636e\u591a\u6837\u6027\u548c\u66f4\u7ec6\u7c92\u5ea6\u6761\u4ef6\u800c\u589e\u52a0\uff1b\u5728\u5efa\u6a21\u65b9\u9762\uff0c\u8bb0\u5fc6\u5316\u5728\u4e2d\u7b49\u5f15\u5bfc\u5c3a\u5ea6\u8fbe\u5230\u5cf0\u503c\uff0c\u53ef\u901a\u8fc7\u66f4\u957f\u7684\u5411\u91cf\u96c6\u548c\u7b80\u5355\u7684\u65cb\u8f6c\u589e\u5f3a\u6765\u7f13\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u5bf93D\u751f\u6210\u6a21\u578b\u4e2d\u8bb0\u5fc6\u5316\u7684\u5b9e\u8bc1\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u7b80\u5355\u6709\u6548\u7684\u7b56\u7565\u6765\u51cf\u5c11\u8bb0\u5fc6\u5316\u800c\u4e0d\u964d\u4f4e\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2512.23646", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23646", "abs": "https://arxiv.org/abs/2512.23646", "authors": ["Keda Tao", "Wenjie Du", "Bohan Yu", "Weiqiang Wang", "Jian Liu", "Huan Wang"], "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding", "comment": "Website:https://kd-tao.github.io/OmniAgent/", "summary": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.", "AI": {"tldr": "OmniAgent\uff1a\u4e00\u79cd\u5b8c\u5168\u97f3\u9891\u5f15\u5bfc\u7684\u4e3b\u52a8\u611f\u77e5\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u7f16\u6392\u4e13\u7528\u5de5\u5177\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u97f3\u9891-\u89c6\u89c9\u63a8\u7406\uff0c\u5728\u4e09\u4e2a\u97f3\u9891-\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7edf\u4e00\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u96be\u4ee5\u5b9e\u73b0\u591a\u6a21\u6001\u5bf9\u9f50\u3002\u9700\u8981\u4ece\u88ab\u52a8\u54cd\u5e94\u751f\u6210\u8f6c\u5411\u4e3b\u52a8\u591a\u6a21\u6001\u67e5\u8be2\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51faOmniAgent\uff0c\u91c7\u7528\u52a8\u6001\u89c4\u5212\u81ea\u4e3b\u6309\u9700\u7f16\u6392\u5de5\u5177\u8c03\u7528\uff0c\u6218\u7565\u6027\u5730\u5c06\u611f\u77e5\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\u4e0a\u3002\u6838\u5fc3\u662f\u65b0\u9896\u7684\u7c97\u5230\u7ec6\u97f3\u9891\u5f15\u5bfc\u611f\u77e5\u8303\u5f0f\uff0c\u5229\u7528\u97f3\u9891\u7ebf\u7d22\u5b9a\u4f4d\u65f6\u95f4\u4e8b\u4ef6\u5e76\u6307\u5bfc\u540e\u7eed\u63a8\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u97f3\u9891-\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\uff0cOmniAgent\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u9886\u5148\u7684\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b10%-20%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "OmniAgent\u5c55\u793a\u4e86\u4ece\u88ab\u52a8\u54cd\u5e94\u751f\u6210\u5230\u4e3b\u52a8\u591a\u6a21\u6001\u67e5\u8be2\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u7f16\u6392\u548c\u97f3\u9891\u5f15\u5bfc\u611f\u77e5\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u7684\u97f3\u9891-\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
