{"id": "2507.09068", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09068", "abs": "https://arxiv.org/abs/2507.09068", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "title": "Infinite Video Understanding", "comment": null, "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f53\u524d\u89c6\u9891\u7406\u89e3\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u2018\u65e0\u9650\u89c6\u9891\u7406\u89e3\u2019\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u5904\u7406\u4efb\u610f\u65f6\u957f\u7684\u89c6\u9891\u5185\u5bb9\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u9762\u4e34\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u2018\u65e0\u9650\u89c6\u9891\u7406\u89e3\u2019\u6982\u5ff5\uff0c\u5efa\u8bae\u7814\u7a76\u6d41\u5f0f\u67b6\u6784\u3001\u6301\u4e45\u5185\u5b58\u673a\u5236\u3001\u5206\u5c42\u8868\u793a\u7b49\u65b9\u6cd5\u3002", "result": "\u5c1a\u672a\u5b9e\u73b0\uff0c\u4f46\u660e\u786e\u4e86\u6838\u5fc3\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u2018\u65e0\u9650\u89c6\u9891\u7406\u89e3\u2019\u662f\u591a\u5a92\u4f53\u548cAI\u7814\u7a76\u7684\u91cd\u8981\u76ee\u6807\uff0c\u5c06\u63a8\u52a8\u6280\u672f\u521b\u65b0\u3002"}}
{"id": "2507.09369", "categories": ["cs.AI", "68T01", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8eAI\u53ef\u80fd\u5bfc\u81f4\u5168\u4eba\u7c7b\u706d\u7edd\u4e8b\u4ef6\u7684\u5206\u7c7b\u548c\u793a\u4f8b\uff0c\u65e8\u5728\u901a\u8fc7\u516c\u5f00\u8ba8\u8bba\u652f\u6301\u9884\u9632\u63aa\u65bd\u3002", "motivation": "\u63a2\u8ba8AI\u53ef\u80fd\u5e26\u6765\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u4ee5\u4fc3\u8fdb\u516c\u4f17\u652f\u6301\u9884\u9632\u884c\u52a8\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\u5e76\u5217\u4e3e\u6f5c\u5728\u7684\u5168\u4eba\u7c7b\u706d\u7edd\u573a\u666f\u3002", "result": "\u5c55\u793a\u4e86\u591a\u79cd\u53ef\u80fd\u7684AI\u707e\u96be\u6027\u98ce\u9669\uff0c\u5f3a\u8c03\u5176\u53ef\u907f\u514d\u6027\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00\u8ba8\u8bba\u8fd9\u4e9b\u53ef\u80fd\u6027\uff0c\u53ef\u4ee5\u63a8\u52a8\u9884\u9632\u63aa\u65bd\u7684\u5b9e\u65bd\u3002"}}
{"id": "2507.09105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u624b\u8bed\u751f\u6210\uff08SLP\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u65b9\u6cd5\u5728\u63a8\u7406\u9636\u6bb5\u56e0\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\uff0c\u800c\u6269\u6563\u6a21\u578b\u56e0\u8fed\u4ee3\u7279\u6027\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u6a21\u5757\u548c\u7f6e\u4fe1\u611f\u77e5\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6548\u7387\u7684\u63d0\u5347\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bef\u5dee\u7d2f\u79ef\u548c\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u624b\u8bed\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.09540", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09540", "abs": "https://arxiv.org/abs/2507.09540", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMetropolis-Hastings\u91c7\u6837\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u907f\u514d\u4e86\u68af\u5ea6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u5b9e\u65f6\u63a7\u5236\u7cfb\u7edf\u4e2d\u5177\u6709\u751f\u7269\u542f\u53d1\u6027\u548c\u9ad8\u80fd\u6548\u6027\uff0c\u4f46\u5176\u8bad\u7ec3\u56e0\u8109\u51b2\u901a\u4fe1\u7684\u4e0d\u53ef\u5fae\u5206\u6027\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528Metropolis-Hastings\u91c7\u6837\uff0c\u4e00\u79cd\u8d1d\u53f6\u65af\u63a8\u65ad\u6280\u672f\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u51fa\u5e76\u6982\u7387\u63a5\u53d7\u7f51\u7edc\u53c2\u6570\u66f4\u65b0\uff0c\u57fa\u4e8e\u7d2f\u79ef\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728AcroBot\u548cCartPole\u4e24\u4e2a\u6807\u51c6\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u3001\u51cf\u5c11\u7f51\u7edc\u8d44\u6e90\u548c\u8bad\u7ec3\u5468\u671f\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6Q\u5b66\u4e60\u548c\u73b0\u6709SNN\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aSNN\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u68af\u5ea6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u52a8\u6001\u4ee3\u7406\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09469", "abs": "https://arxiv.org/abs/2507.09469", "authors": ["Haoyang Wang", "Jingao Xu", "Xinyu Luo", "Ting Zhang", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Yunhao Liu", "Jianfeng Zheng", "Weijie Hong", "Xinlei Chen"], "title": "mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization", "comment": "17 pages, 34 figures. arXiv admin note: substantial text overlap with\n  arXiv:2502.14992", "summary": "For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we upgrade\ntraditional frame camera with event camera, a novel sensor that harmonizes in\nsampling frequency with mmWave radar within ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for precise drone landings. To fully exploit the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these two modalities,\nwe propose two innovative modules: \\textit{(i)} the Consistency-instructed\nCollaborative Tracking module, which further leverages the drone's physical\nknowledge of periodic micro-motions and structure for accurate measurements\nextraction, and \\textit{(ii)} the Graph-informed Adaptive Joint Optimization\nmodule, which integrates drone motion information for efficient sensor fusion\nand drone localization. Real-world experiments conducted in landing scenarios\nwith a drone delivery company demonstrate that mmE-Loc significantly\noutperforms state-of-the-art methods in both accuracy and latency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ammE-Loc\u7684\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u5730\u9762\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u7cbe\u786e\u964d\u843d\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u91c7\u6837\u9891\u7387\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u76f8\u673a\u66ff\u4ee3\u4f20\u7edf\u5e27\u76f8\u673a\uff0c\u63d0\u51fa\u4e00\u81f4\u6027\u534f\u4f5c\u8ddf\u8e2a\u6a21\u5757\u548c\u56fe\u81ea\u9002\u5e94\u8054\u5408\u4f18\u5316\u6a21\u5757\uff0c\u5145\u5206\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cmmE-Loc\u5728\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "mmE-Loc\u4e3a\u65e0\u4eba\u673a\u7cbe\u786e\u964d\u843d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09538", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09538", "abs": "https://arxiv.org/abs/2507.09538", "authors": ["Zainab Ali", "Lujayn Al-Amir", "Ali Safa"], "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks", "comment": null, "summary": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u907f\u969c\u4e2d\u4f7f\u7528SNN\u5904\u7406LIDAR\u6570\u636e\uff0c\u53d1\u73b0\u901a\u8fc7\u8c03\u6574LIF\u795e\u7ecf\u5143\u7684\u819c\u7535\u4f4d\u6cc4\u6f0f\u5e38\u6570\uff0c\u53ef\u4ee5\u8fbe\u5230\u4e0eCNN\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u3002", "motivation": "\u7531\u4e8eSNN\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e2d\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u4f18\u52bf\uff0c\u9002\u5408\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5e94\u7528\u3002", "method": "\u642d\u5efa\u4e86\u4e00\u4e2a\u914d\u5907LIDAR\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6536\u96c6\u5e26\u6807\u7b7e\u7684LIDAR\u6570\u636e\u53ca\u4eba\u5de5\u64cd\u4f5c\u7684\u63a7\u5236\u547d\u4ee4\uff0c\u7814\u7a76\u4e86SNN\u4e2d\u795e\u7ecf\u5143\u819c\u6cc4\u6f0f\u5bf9LIDAR\u6570\u636e\u5904\u7406\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u8c03\u6574LIF\u795e\u7ecf\u5143\u7684\u819c\u7535\u4f4d\u6cc4\u6f0f\u5e38\u6570\uff0cSNN\u5728\u673a\u5668\u4eba\u63a7\u5236\u7cbe\u5ea6\u4e0a\u4e0e\u975e\u8109\u51b2CNN\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86SNN\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.09269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09269", "abs": "https://arxiv.org/abs/2507.09269", "authors": ["Shuhan Ye", "Yuanbin Qian", "Chong Wang", "Sunqi Lin", "Jiazhen Xu", "Jiangbo Qian", "Yuqi Li"], "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks", "comment": "This paper has been accepted by ICME2025", "summary": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in\ncomputer vision domain due to their high biological plausibility, event-driven\ncharacteristic and energy-saving efficiency. Still, limited annotated\nevent-based datasets and immature SNN architectures result in their performance\ninferior to that of Artificial Neural Networks (ANNs). To enhance the\nperformance of SNNs on their optimal data format, DVS data, we explore using\nRGB data and well-performing ANNs to implement knowledge distillation. In this\ncase, solving cross-modality and cross-architecture challenges is necessary. In\nthis paper, we propose cross knowledge distillation (CKD), which not only\nleverages semantic similarity and sliding replacement to mitigate the\ncross-modality challenge, but also uses an indirect phased knowledge\ndistillation to mitigate the cross-architecture challenge. We validated our\nmethod on main-stream neuromorphic datasets, including N-Caltech101 and\nCEP-DVS. The experimental results show that our method outperforms current\nState-of-the-Art methods. The code will be available at\nhttps://github.com/ShawnYE618/CKD", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8de8\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u6ed1\u52a8\u66ff\u6362\u89e3\u51b3\u8de8\u6a21\u6001\u95ee\u9898\uff0c\u95f4\u63a5\u5206\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u8de8\u67b6\u6784\u95ee\u9898\uff0c\u63d0\u5347SNN\u5728DVS\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u548c\u67b6\u6784\u4e0d\u6210\u719f\uff0c\u6027\u80fd\u4e0d\u5982\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u3002\u4e3a\u63d0\u5347SNNs\u5728DVS\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5229\u7528RGB\u6570\u636e\u548cANNs\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002", "method": "\u63d0\u51fa\u8de8\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\uff0c\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u6ed1\u52a8\u66ff\u6362\u89e3\u51b3\u8de8\u6a21\u6001\u95ee\u9898\uff0c\u95f4\u63a5\u5206\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u8de8\u67b6\u6784\u95ee\u9898\u3002", "result": "\u5728\u4e3b\u6d41\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\uff08\u5982N-Caltech101\u548cCEP-DVS\uff09\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "CKD\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u548c\u8de8\u67b6\u6784\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86SNNs\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10204", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10204", "abs": "https://arxiv.org/abs/2507.10204", "authors": ["Abdelhakim Amer", "Mohit Mehindratta", "Yury Brodskiy", "Bilal Wehbe", "Erdal Kayacan"], "title": "REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles", "comment": null, "summary": "Inspection of complex underwater structures with tethered underwater vehicles\nis often hindered by the risk of tether entanglement. We propose REACT\n(real-time entanglement-aware coverage path planning for tethered underwater\nvehicles), a framework designed to overcome this limitation. REACT comprises a\nfast geometry-based tether model using the signed distance field (SDF) map for\naccurate, real-time simulation of taut tether configurations around arbitrary\nstructures in 3D. This model enables an efficient online replanning strategy by\nenforcing a maximum tether length constraint, thereby actively preventing\nentanglement. By integrating REACT into a coverage path planning framework, we\nachieve safe and optimal inspection paths, previously challenging due to tether\nconstraints. The complete REACT framework's efficacy is validated in a pipe\ninspection scenario, demonstrating safe, entanglement-free navigation and\nfull-coverage inspection. Simulation results show that REACT achieves complete\ncoverage while maintaining tether constraints and completing the total mission\n20% faster than conventional planners, despite a longer inspection time due to\nproactive avoidance of entanglement that eliminates extensive post-mission\ndisentanglement. Real-world experiments confirm these benefits, where REACT\ncompletes the full mission, while the baseline planner fails due to physical\ntether entanglement.", "AI": {"tldr": "REACT\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u51e0\u4f55\u6a21\u578b\u548c\u8def\u5f84\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u56e0\u7f06\u7ef3\u7f20\u7ed5\u5bfc\u81f4\u7684\u4efb\u52a1\u53d7\u9650\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u5168\u8986\u76d6\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u6c34\u4e0b\u673a\u5668\u4eba\u68c0\u6d4b\u590d\u6742\u7ed3\u6784\u65f6\uff0c\u7f06\u7ef3\u7f20\u7ed5\u98ce\u9669\u9650\u5236\u4e86\u4efb\u52a1\u6548\u7387\u548c\u5b89\u5168\u3002", "method": "REACT\u4f7f\u7528\u57fa\u4e8e\u51e0\u4f55\u7684\u5b9e\u65f6\u7f06\u7ef3\u6a21\u578b\uff08SDF\uff09\u548c\u8def\u5f84\u89c4\u5212\u7b56\u7565\uff0c\u4e3b\u52a8\u907f\u514d\u7f20\u7ed5\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u663e\u793a\uff0cREACT\u5728\u4fdd\u8bc1\u5168\u8986\u76d6\u7684\u540c\u65f6\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed20%\uff0c\u4e14\u65e0\u7f20\u7ed5\u95ee\u9898\u3002", "conclusion": "REACT\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u68c0\u6d4b\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.09323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09323", "abs": "https://arxiv.org/abs/2507.09323", "authors": ["Kaixuan Cong", "Yifan Wang", "Rongkun Xue", "Yuyang Jiang", "Yiming Feng", "Jing Yang"], "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition", "comment": null, "summary": "Humans do not understand individual events in isolation; rather, they\ngeneralize concepts within classes and compare them to others. Existing\naudio-video pre-training paradigms only focus on the alignment of the overall\naudio-video modalities, without considering the reinforcement of distinguishing\neasily confused classes through cognitive induction and contrast during\ntraining. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder\n(DICCAE), an encoder that aligns audio-video representations at a fine-grained,\ncategory-level. DICCAE addresses category confusion by dynamically adjusting\nthe confusion loss based on inter-class confusion degrees, thereby enhancing\nthe model's ability to distinguish between similar activities. To further\nextend the application of DICCAE, we also introduce a novel training framework\nthat incorporates both audio and video modalities, as well as their fusion. To\nmitigate the scarcity of audio-video data in the human activity recognition\ntask, we propose a cluster-guided audio-video self-supervised pre-training\nstrategy for DICCAE. DICCAE achieves near state-of-the-art performance on the\nVGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its\nfeature representation quality through extensive ablation studies, validating\nthe necessity of each module.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u7c7b\u95f4\u6df7\u6dc6\u635f\u5931\u7684\u97f3\u9891-\u89c6\u9891\u9884\u8bad\u7ec3\u7f16\u7801\u5668DICCAE\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u8ba4\u77e5\u5bf9\u6bd4\u589e\u5f3a\u6a21\u578b\u5bf9\u76f8\u4f3c\u6d3b\u52a8\u7684\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u9891\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6574\u4f53\u6a21\u6001\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u901a\u8fc7\u8ba4\u77e5\u5f52\u7eb3\u548c\u5bf9\u6bd4\u5f3a\u5316\u6613\u6df7\u6dc6\u7c7b\u522b\u7684\u533a\u5206\u80fd\u529b\u3002", "method": "\u63d0\u51faDICCAE\u7f16\u7801\u5668\uff0c\u52a8\u6001\u8c03\u6574\u7c7b\u95f4\u6df7\u6dc6\u635f\u5931\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u878d\u5408\u8bad\u7ec3\u6846\u67b6\u548c\u805a\u7c7b\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728VGGSound\u6570\u636e\u96c6\u4e0a\u8fbe\u523065.5%\u7684top-1\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "DICCAE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u52a8\u6001\u6df7\u6dc6\u635f\u5931\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u76f8\u4f3c\u6d3b\u52a8\u7684\u533a\u5206\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.09252", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09252", "abs": "https://arxiv.org/abs/2507.09252", "authors": ["Shukai Gong", "Yiyang Fu", "Fengyuan Ran", "Feng Zhou"], "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding", "comment": null, "summary": "We propose TPP-SD, a novel approach that accelerates Transformer temporal\npoint process (TPP) sampling by adapting speculative decoding (SD) techniques\nfrom language models. By identifying the structural similarities between\nthinning algorithms for TPPs and speculative decoding for language models, we\ndevelop an efficient sampling framework that leverages a smaller draft model to\ngenerate multiple candidate events, which are then verified by the larger\ntarget model in parallel. TPP-SD maintains the same output distribution as\nautoregressive sampling while achieving significant acceleration. Experiments\non both synthetic and real datasets demonstrate that our approach produces\nsamples from identical distributions as standard methods, but with 2-6$\\times$\nspeedup. Our ablation studies analyze the impact of hyperparameters such as\ndraft length and draft model size on sampling efficiency. TPP-SD bridges the\ngap between powerful Transformer TPP models and the practical need for rapid\nsequence sampling.", "AI": {"tldr": "TPP-SD\u662f\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408Transformer\u65f6\u95f4\u70b9\u8fc7\u7a0b\u548c\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u6765\u52a0\u901f\u91c7\u6837\u7684\u65b0\u65b9\u6cd5\uff0c\u4fdd\u6301\u8f93\u51fa\u5206\u5e03\u4e0d\u53d8\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3Transformer\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u91c7\u6837\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\u751f\u6210\u5019\u9009\u4e8b\u4ef6\uff0c\u7531\u5927\u578b\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u70b9\u8fc7\u7a0b\u7684\u7ec6\u5316\u7b97\u6cd5\u548c\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cTPP-SD\u5b9e\u73b0\u4e86\u4e0e\u6807\u51c6\u65b9\u6cd5\u76f8\u540c\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u4f46\u901f\u5ea6\u63d0\u5347\u4e862-6\u500d\u3002", "conclusion": "TPP-SD\u5728\u4fdd\u6301\u91c7\u6837\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\uff0c\u586b\u8865\u4e86\u5f3a\u5927\u6a21\u578b\u4e0e\u5b9e\u9645\u5feb\u901f\u91c7\u6837\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.09394", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09394", "abs": "https://arxiv.org/abs/2507.09394", "authors": ["Nandan Kumar Jha", "Brandon Reagen"], "title": "A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention", "comment": "ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD)", "summary": "In this work, we study how multi-head latent attention (MLA), a popular\nstrategy for compressing key/value memory, affects a transformer's internal\ncapacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)\ndiagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\\top$ gram matrix\nthroughout training, comparing three variants: the standard multi-head\nattention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,\nand MLA-Decoupled, which shares a single rotary sub-vector across all heads.\nOur random matrix analysis reveals \\textbf{three key findings:} \\textbf{ i)}\ncapacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,\nearly spikes in specific layers that persist and propagate, disrupting the\nbalance between bulk and outlier directions; \\textbf{ ii)} these spikes\ncoincide with rank collapse, concentrating the model's expressivity into narrow\nsubspaces; \\textbf{ iii)} only the decoupled variant prevents this cascade,\nmaintaining broad spectral support and suppressing outlier formation across\nlayers. These results underscore that \\emph{how} rotary embeddings are applied\nis just as critical as \\emph{where} compression occurs. Sharing rotary\ncomponents across heads mitigates spectral fragmentation and preserves\nrepresentational capacity.", "AI": {"tldr": "\u7814\u7a76\u4e86\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u5bf9\u9884\u8bad\u7ec3\u4e2dTransformer\u5185\u90e8\u5bb9\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u65cb\u8f6c\u5d4c\u5165\u7684\u5e94\u7528\u65b9\u5f0f\u5bf9\u9632\u6b62\u5bb9\u91cf\u74f6\u9888\u548c\u8c31\u5206\u88c2\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u63a2\u8ba8MLA\u538b\u7f29\u952e/\u503c\u8bb0\u5fc6\u65f6\u5982\u4f55\u5f71\u54cdTransformer\u7684\u9884\u8bad\u7ec3\u5bb9\u91cf\uff0c\u7279\u522b\u662f\u65cb\u8f6c\u5d4c\u5165\u7684\u5e94\u7528\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528Marchenko-Pastur\u8bca\u65ad\u5206\u6790$W_{Q}W_{K}^\\top$\u77e9\u9635\u8c31\uff0c\u6bd4\u8f83\u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u3001MLA-PreRoPE\u548cMLA-Decoupled\u4e09\u79cd\u53d8\u4f53\u3002", "result": "\u53d1\u73b0MHA\u548cMLA-PreRoPE\u5728\u7279\u5b9a\u5c42\u51fa\u73b0\u5bb9\u91cf\u74f6\u9888\u548c\u79e9\u5d29\u6e83\uff0c\u800cMLA-Decoupled\u80fd\u7ef4\u6301\u8c31\u652f\u6301\u5e76\u6291\u5236\u5f02\u5e38\u503c\u3002", "conclusion": "\u65cb\u8f6c\u5d4c\u5165\u7684\u5e94\u7528\u65b9\u5f0f\u4e0e\u538b\u7f29\u4f4d\u7f6e\u540c\u6837\u91cd\u8981\uff0c\u8de8\u5934\u5171\u4eab\u65cb\u8f6c\u7ec4\u4ef6\u53ef\u907f\u514d\u8c31\u5206\u88c2\u5e76\u4fdd\u6301\u5bb9\u91cf\u3002"}}
{"id": "2507.09443", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.09443", "abs": "https://arxiv.org/abs/2507.09443", "authors": ["Luiz Aldeia Machado", "Victor Coppo Leite", "Elia Merzari", "Arthur Motta", "Roberto Ponciroli", "Lander Ibarra", "Lise Charlot"], "title": "Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components", "comment": "Preprint - Nureth 21 paper", "summary": "Proactive maintenance strategies, such as Predictive Maintenance (PdM), play\nan important role in the operation of Nuclear Power Plants (NPPs), particularly\ndue to their capacity to reduce offline time by preventing unexpected shutdowns\ncaused by component failures.\n  In this work, we explore the use of a Convolutional Neural Network (CNN)\narchitecture combined with a computational thermomechanical model to calculate\nthe temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel\nrod during operation. This estimation relies on a limited number of temperature\nmeasurements from the cladding's outer surface. This methodology can\npotentially aid in developing PdM tools for nuclear reactors by enabling\nreal-time monitoring of such systems.\n  The training, validation, and testing datasets were generated through coupled\nsimulations involving BISON, a finite element-based nuclear fuel performance\ncode, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven\nsimulations, varying the peak linear heat generation rates. Of these, eight\nwere used for training, two for validation, and one for testing.\n  The CNN was trained for over 1,000 epochs without signs of overfitting,\nachieving highly accurate temperature distribution predictions. These were then\nused in a thermomechanical model to determine the stress and strain\ndistribution within the fuel rod.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548c\u70ed\u529b\u5b66\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u6838\u7535\u7ad9\u71c3\u6599\u68d2\u7684\u6e29\u5ea6\u3001\u5e94\u529b\u548c\u5e94\u53d8\uff0c\u4ee5\u652f\u6301\u9884\u6d4b\u6027\u7ef4\u62a4\u3002", "motivation": "\u6838\u7535\u7ad9\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff08PdM\uff09\u9700\u8981\u5b9e\u65f6\u76d1\u6d4b\u71c3\u6599\u68d2\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u6e29\u5ea6\u6d4b\u91cf\u6570\u636e\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528CNN\u67b6\u6784\u7ed3\u5408\u70ed\u529b\u5b66\u6a21\u578b\uff0c\u57fa\u4e8e\u71c3\u6599\u68d2\u5916\u8868\u9762\u6e29\u5ea6\u6570\u636e\u9884\u6d4b\u5185\u90e8\u72b6\u6001\u3002\u6570\u636e\u96c6\u901a\u8fc7BISON\u548cMOOSE-THM\u4eff\u771f\u751f\u6210\u3002", "result": "CNN\u8bad\u7ec31000\u6b21\u672a\u8fc7\u62df\u5408\uff0c\u80fd\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u6e29\u5ea6\u5206\u5e03\uff0c\u8fdb\u800c\u8ba1\u7b97\u71c3\u6599\u68d2\u7684\u5e94\u529b\u548c\u5e94\u53d8\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6838\u7535\u7ad9\u9884\u6d4b\u6027\u7ef4\u62a4\u63d0\u4f9b\u4e86\u5b9e\u65f6\u76d1\u6d4b\u5de5\u5177\uff0c\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09545", "abs": "https://arxiv.org/abs/2507.09545", "authors": ["Ilaria Vascotto", "Valentina Blasone", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events", "comment": "Late Breaking Work presented at the 3rd World Conference on\n  eXplainable Artificial Intelligence (XAI2025)", "summary": "The usage of eXplainable Artificial Intelligence (XAI) methods has become\nessential in practical applications, given the increasing deployment of\nArtificial Intelligence (AI) models and the legislative requirements put\nforward in the latest years. A fundamental but often underestimated aspect of\nthe explanations is their robustness, a key property that should be satisfied\nin order to trust the explanations. In this study, we provide some preliminary\ninsights on evaluating the reliability of explanations in the specific case of\nunbalanced datasets, which are very frequent in high-risk use-cases, but at the\nsame time considerably challenging for both AI models and XAI methods. We\npropose a simple evaluation focused on the minority class (i.e. the less\nfrequent one) that leverages on-manifold generation of neighbours, explanation\naggregation and a metric to test explanation consistency. We present a use-case\nbased on a tabular dataset with numerical features focusing on the occurrence\nof frost events.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\uff0c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5c11\u6570\u7c7b\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8eAI\u6a21\u578b\u7684\u5e7f\u6cdb\u90e8\u7f72\u548c\u7acb\u6cd5\u8981\u6c42\uff0cXAI\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u6210\u4e3a\u5173\u952e\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u7684\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5f62\u90bb\u57df\u751f\u6210\u3001\u89e3\u91ca\u805a\u5408\u548c\u4e00\u81f4\u6027\u5ea6\u91cf\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u5c11\u6570\u7c7b\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u6570\u503c\u7279\u5f81\u7684\u8868\u683c\u6570\u636e\u96c6\uff08\u971c\u51bb\u4e8b\u4ef6\uff09\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bc4\u4f30XAI\u89e3\u91ca\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.10403", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.10403", "abs": "https://arxiv.org/abs/2507.10403", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "comment": null, "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCrisisLandMark\u6570\u636e\u96c6\u548cCLOSP\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u5149\u5b66\u4e0eSAR\u56fe\u50cf\uff0c\u63d0\u5347\u68c0\u7d22\u6027\u80fd54%\uff0c\u5e76\u5f15\u5165GeoCLOSP\u7ed3\u5408\u5730\u7406\u5750\u6807\u4f18\u5316\u7279\u5b9a\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u591a\u9650\u4e8eRGB\u6570\u636e\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u4f20\u611f\u5668\uff08\u5982SAR\u548c\u591a\u5149\u8c31\uff09\u7684\u72ec\u7279\u7269\u7406\u4fe1\u606f\u3002", "method": "\u6784\u5efaCrisisLandMark\u6570\u636e\u96c6\uff0c\u63d0\u51faCLOSP\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u5149\u5b66\u4e0eSAR\u56fe\u50cf\uff0c\u5e76\u6269\u5c55\u4e3aGeoCLOSP\u6574\u5408\u5730\u7406\u5750\u6807\u3002", "result": "CLOSP\u63d0\u5347\u68c0\u7d22\u6027\u80fd54%\uff0cGeoCLOSP\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u591a\u4f20\u611f\u5668\u6570\u636e\u4e0e\u5730\u7406\u80cc\u666f\u7684\u6574\u5408\u5bf9\u9065\u611f\u6863\u6848\u7684\u6f5c\u529b\u5f00\u53d1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.09815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09815", "abs": "https://arxiv.org/abs/2507.09815", "authors": ["Younggun Kim", "Ahmed S. Abdelrahman", "Mohamed Abdel-Aty"], "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding", "comment": "22 pages, 11 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86VRU-Accident\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6d89\u53ca\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u9ad8\u98ce\u9669\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u6d89\u53caVRUs\u7684\u4e8b\u6545\u540e\u679c\u4e25\u91cd\uff0c\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u5b9a\u91cf\u8bc4\u4f30MLLMs\u5728\u590d\u6742\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1K\u771f\u5b9e\u4e8b\u6545\u89c6\u9891\u30016K\u591a\u9009\u95ee\u7b54\u5bf9\u548c1K\u5bc6\u96c6\u573a\u666f\u63cf\u8ff0\u7684VRU-Accident\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e8617\u79cd\u5148\u8fdbMLLMs\u3002", "result": "MLLMs\u5728\u89c6\u89c9\u5c5e\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e8b\u6545\u539f\u56e0\u3001\u7c7b\u578b\u548c\u53ef\u9884\u9632\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "conclusion": "VRU-Accident\u586b\u8865\u4e86\u8bc4\u4f30MLLMs\u5728VRU\u5b89\u5168\u573a\u666f\u4e2d\u80fd\u529b\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.09910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09910", "abs": "https://arxiv.org/abs/2507.09910", "authors": ["Yadong Qu", "Shancheng Fang", "Yuxin Wang", "Xiaorui Wang", "Zhineng Chen", "Hongtao Xie", "Yongdong Zhang"], "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation", "comment": "ICCV 2025", "summary": "Graphic design visually conveys information and data by creating and\ncombining text, images and graphics. Two-stage methods that rely primarily on\nlayout generation lack creativity and intelligence, making graphic design still\nlabor-intensive. Existing diffusion-based methods generate non-editable graphic\ndesign files at image level with poor legibility in visual text rendering,\nwhich prevents them from achieving satisfactory and practical automated graphic\ndesign. In this paper, we propose Instructional Graphic Designer (IGD) to\nswiftly generate multimodal layers with editable flexibility with only natural\nlanguage instructions. IGD adopts a new paradigm that leverages parametric\nrendering and image asset generation. First, we develop a design platform and\nestablish a standardized format for multi-scenario design files, thus laying\nthe foundation for scaling up data. Second, IGD utilizes the multimodal\nunderstanding and reasoning capabilities of MLLM to accomplish attribute\nprediction, sequencing and layout of layers. It also employs a diffusion model\nto generate image content for assets. By enabling end-to-end training, IGD\narchitecturally supports scalability and extensibility in complex graphic\ndesign tasks. The superior experimental results demonstrate that IGD offers a\nnew solution for graphic design.", "AI": {"tldr": "IGD\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5feb\u901f\u751f\u6210\u53ef\u7f16\u8f91\u591a\u6a21\u6001\u5c42\u7684\u56fe\u5f62\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u53c2\u6570\u5316\u6e32\u67d3\u548c\u56fe\u50cf\u8d44\u4ea7\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u521b\u9020\u529b\u548c\u53ef\u7f16\u8f91\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u5f62\u8bbe\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u521b\u9020\u529b\u548c\u53ef\u7f16\u8f91\u6027\uff0c\u5bfc\u81f4\u81ea\u52a8\u5316\u8bbe\u8ba1\u6548\u679c\u4e0d\u4f73\u3002", "method": "IGD\u5229\u7528\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u9884\u6d4b\u5c5e\u6027\u3001\u6392\u5e8f\u548c\u5e03\u5c40\uff0c\u5e76\u91c7\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u8d44\u4ea7\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eIGD\u5728\u56fe\u5f62\u8bbe\u8ba1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "IGD\u4e3a\u56fe\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u590d\u6742\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.09890", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.09890", "abs": "https://arxiv.org/abs/2507.09890", "authors": ["Ping Xu", "Pengfei Wang", "Zhiyuan Ning", "Meng Xiao", "Min Wu", "Yuanchun Zhou"], "title": "Soft Graph Clustering for single-cell RNA Sequencing Data", "comment": null, "summary": "Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)\ndata analysis for elucidating cellular heterogeneity and diversity. Recent\ngraph-based scRNA-seq clustering methods, particularly graph neural networks\n(GNNs), have significantly improved in tackling the challenges of\nhigh-dimension, high-sparsity, and frequent dropout events that lead to\nambiguous cell population boundaries. However, their reliance on hard graph\nconstructions derived from thresholded similarity matrices presents\nchallenges:(i) The simplification of intercellular relationships into binary\nedges (0 or 1) by applying thresholds, which restricts the capture of\ncontinuous similarity features among cells and leads to significant information\nloss.(ii) The presence of significant inter-cluster connections within hard\ngraphs, which can confuse GNN methods that rely heavily on graph structures,\npotentially causing erroneous message propagation and biased clustering\noutcomes. To tackle these challenges, we introduce scSGC, a Soft Graph\nClustering for single-cell RNA sequencing data, which aims to more accurately\ncharacterize continuous similarities among cells through non-binary edge\nweights, thereby mitigating the limitations of rigid data structures. The scSGC\nframework comprises three core components: (i) a zero-inflated negative\nbinomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed\nsoft graph embedding module; and (iii) an optimal transport-based clustering\noptimization module. Extensive experiments across ten datasets demonstrate that\nscSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,\ncell type annotation, and computational efficiency. These results highlight its\nsubstantial potential to advance scRNA-seq data analysis and deepen our\nunderstanding of cellular heterogeneity.", "AI": {"tldr": "scSGC\u662f\u4e00\u79cd\u57fa\u4e8e\u8f6f\u56fe\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u4e8c\u503c\u8fb9\u6743\u91cd\u66f4\u51c6\u786e\u5730\u8868\u5f81\u7ec6\u80de\u95f4\u8fde\u7eed\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u4e86\u786c\u56fe\u6784\u5efa\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u805a\u7c7b\u6df7\u6dc6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u65b9\u6cd5\uff08\u5982GNN\uff09\u4f9d\u8d56\u786c\u56fe\u6784\u5efa\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u805a\u7c7b\u504f\u5dee\u3002scSGC\u65e8\u5728\u901a\u8fc7\u8f6f\u56fe\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "scSGC\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u57fa\u4e8eZINB\u7684\u7279\u5f81\u81ea\u7f16\u7801\u5668\u3001\u53cc\u901a\u9053\u5207\u5272\u611f\u77e5\u8f6f\u56fe\u5d4c\u5165\u6a21\u5757\u548c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u805a\u7c7b\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5728\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cscSGC\u5728\u805a\u7c7b\u51c6\u786e\u6027\u3001\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e13\u79cd\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u6a21\u578b\u3002", "conclusion": "scSGC\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u5206\u6790\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u66f4\u6df1\u5165\u5730\u7406\u89e3\u7ec6\u80de\u5f02\u8d28\u6027\u3002"}}
{"id": "2507.10014", "categories": ["cs.LG", "92D30, 62M10, 68T07", "G.3; I.6.3; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.10014", "abs": "https://arxiv.org/abs/2507.10014", "authors": ["Ali Sarabi", "Arash Sarabi", "Hao Yan", "Beckett Sterner", "Petar Jevti\u0107"], "title": "Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach", "comment": null, "summary": "Coccidioidomycosis, commonly known as Valley Fever, remains a significant\npublic health concern in endemic regions of the southwestern United States.\nThis study develops the first graph neural network (GNN) model for forecasting\nValley Fever incidence in Arizona. The model integrates surveillance case data\nwith environmental predictors using graph structures, including soil\nconditions, atmospheric variables, agricultural indicators, and air quality\nmetrics. Our approach explores correlation-based relationships among variables\ninfluencing disease transmission. The model captures critical delays in disease\nprogression through lagged effects, enhancing its capacity to reflect complex\ntemporal dependencies in disease ecology. Results demonstrate that the GNN\narchitecture effectively models Valley Fever trends and provides insights into\nkey environmental drivers of disease incidence. These findings can inform early\nwarning systems and guide resource allocation for disease prevention efforts in\nhigh-risk areas.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u9996\u4e2a\u7528\u4e8e\u9884\u6d4b\u4e9a\u5229\u6851\u90a3\u5dde\u5c71\u8c37\u70ed\u53d1\u75c5\u7387\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6a21\u578b\uff0c\u6574\u5408\u4e86\u73af\u5883\u56e0\u7d20\u548c\u75c5\u4f8b\u6570\u636e\uff0c\u63ed\u793a\u4e86\u75be\u75c5\u4f20\u64ad\u7684\u5173\u952e\u73af\u5883\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u5c71\u8c37\u70ed\u5728\u897f\u5357\u7f8e\u56fd\u6d41\u884c\u5730\u533a\u4ecd\u662f\u91cd\u5927\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u6765\u6307\u5bfc\u9632\u63a7\u3002", "method": "\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6574\u5408\u75c5\u4f8b\u6570\u636e\u548c\u73af\u5883\u9884\u6d4b\u56e0\u5b50\uff08\u5982\u571f\u58e4\u6761\u4ef6\u3001\u5927\u6c14\u53d8\u91cf\u7b49\uff09\uff0c\u63a2\u7d22\u53d8\u91cf\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5f15\u5165\u6ede\u540e\u6548\u5e94\u6355\u6349\u75be\u75c5\u8fdb\u5c55\u7684\u5ef6\u8fdf\u3002", "result": "GNN\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u5c71\u8c37\u70ed\u8d8b\u52bf\uff0c\u5e76\u8bc6\u522b\u5173\u952e\u73af\u5883\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4e3a\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u548c\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u79d1\u5b66\u4f9d\u636e\uff0c\u52a9\u529b\u9ad8\u98ce\u9669\u5730\u533a\u7684\u75be\u75c5\u9884\u9632\u3002"}}
{"id": "2507.10209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10209", "abs": "https://arxiv.org/abs/2507.10209", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "title": "Is Micro-expression Ethnic Leaning?", "comment": null, "summary": "How much does ethnicity play its part in emotional expression? Emotional\nexpression and micro-expression research probe into understanding human\npsychological responses to emotional stimuli, thereby revealing substantial\nhidden yet authentic emotions that can be useful in the event of diagnosis and\ninterviews. While increased attention had been provided to micro-expression\nanalysis, the studies were done under Ekman's assumption of emotion\nuniversality, where emotional expressions are identical across cultures and\nsocial contexts. Our computational study uncovers some of the influences of\nethnic background in expression analysis, leading to an argument that the\nemotional universality hypothesis is an overgeneralization from the perspective\nof manual psychological analysis. In this research, we propose to investigate\nthe level of influence of ethnicity in a simulated micro-expression scenario.\nWe construct a cross-cultural micro-expression database and algorithmically\nannotate the ethnic labels to facilitate the investigation. With the ethnically\nannotated dataset, we perform a prima facie study to compare mono-ethnicity and\nstereo-ethnicity in a controlled environment, which uncovers a certain\ninfluence of ethnic bias via an experimental way. Building on this finding, we\npropose a framework that integrates ethnic context into the emotional feature\nlearning process, yielding an ethnically aware framework that recognises\nethnicity differences in micro-expression recognition. For improved\nunderstanding, qualitative analyses have been done to solidify the preliminary\ninvestigation into this new realm of research. Code is publicly available at\nhttps://github.com/IcedDoggie/ICMEW2025_EthnicMER", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u79cd\u65cf\u80cc\u666f\u5bf9\u60c5\u7eea\u8868\u8fbe\u7684\u5f71\u54cd\uff0c\u6311\u6218\u4e86\u60c5\u7eea\u666e\u904d\u6027\u5047\u8bbe\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u79cd\u65cf\u5dee\u5f02\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\u3002", "motivation": "\u60c5\u7eea\u666e\u904d\u6027\u5047\u8bbe\u8ba4\u4e3a\u60c5\u7eea\u8868\u8fbe\u5728\u4e0d\u540c\u6587\u5316\u548c\u793e\u4f1a\u80cc\u666f\u4e0b\u76f8\u540c\uff0c\u4f46\u7814\u7a76\u6307\u51fa\u8fd9\u79cd\u5047\u8bbe\u53ef\u80fd\u8fc7\u5ea6\u6982\u62ec\uff0c\u79cd\u65cf\u80cc\u666f\u53ef\u80fd\u5bf9\u60c5\u7eea\u8868\u8fbe\u6709\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u8de8\u6587\u5316\u5fae\u8868\u60c5\u6570\u636e\u5e93\uff0c\u7b97\u6cd5\u6807\u6ce8\u79cd\u65cf\u6807\u7b7e\uff0c\u8fdb\u884c\u5355\u4e00\u79cd\u65cf\u4e0e\u6df7\u5408\u79cd\u65cf\u7684\u5bf9\u6bd4\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u6574\u5408\u79cd\u65cf\u80cc\u666f\u7684\u60c5\u7eea\u7279\u5f81\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u79cd\u65cf\u504f\u89c1\u5bf9\u5fae\u8868\u60c5\u8bc6\u522b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7684\u79cd\u65cf\u611f\u77e5\u6846\u67b6\u5728\u8bc6\u522b\u4e2d\u8003\u8651\u4e86\u79cd\u65cf\u5dee\u5f02\u3002", "conclusion": "\u79cd\u65cf\u80cc\u666f\u5bf9\u60c5\u7eea\u8868\u8fbe\u6709\u663e\u8457\u5f71\u54cd\uff0c\u672a\u6765\u7684\u60c5\u7eea\u8bc6\u522b\u7814\u7a76\u5e94\u7eb3\u5165\u79cd\u65cf\u56e0\u7d20\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2507.10349", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10349", "abs": "https://arxiv.org/abs/2507.10349", "authors": ["Zhiyuan Zhao", "Sitan Yang", "Kin G. Olivares", "Boris N. Oreshkin", "Stan Vitebsky", "Michael W. Mahoney", "B. Aditya Prakash", "Dmitry Efimov"], "title": "TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting", "comment": "9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI\n  for Supply Chain: Today and Future", "summary": "Multi-horizon time series forecasting has many practical applications such as\ndemand forecasting. Accurate demand prediction is critical to help make buying\nand inventory decisions for supply chain management of e-commerce and physical\nretailers, and such predictions are typically required for future horizons\nextending tens of weeks. This is especially challenging during high-stake sales\nevents when demand peaks are particularly difficult to predict accurately.\nHowever, these events are important not only for managing supply chain\noperations but also for ensuring a seamless shopping experience for customers.\nTo address this challenge, we propose Temporal-Aligned Transformer (TAT), a\nmulti-horizon forecaster leveraging apriori-known context variables such as\nholiday and promotion events information for improving predictive performance.\nOur model consists of an encoder and decoder, both embedded with a novel\nTemporal Alignment Attention (TAA), designed to learn context-dependent\nalignment for peak demand forecasting. We conduct extensive empirical analysis\non two large-scale proprietary datasets from a large e-commerce retailer. We\ndemonstrate that TAT brings up to 30% accuracy improvement on peak demand\nforecasting while maintaining competitive overall performance compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTemporal-Aligned Transformer (TAT)\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u65f6\u95f4\u8303\u56f4\u9700\u6c42\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u9500\u552e\u9ad8\u5cf0\u671f\uff0c\u901a\u8fc7\u7ed3\u5408\u5df2\u77e5\u4e0a\u4e0b\u6587\u53d8\u91cf\uff08\u5982\u8282\u5047\u65e5\u548c\u4fc3\u9500\u4fe1\u606f\uff09\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u9700\u6c42\u9884\u6d4b\u5bf9\u4f9b\u5e94\u94fe\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9500\u552e\u9ad8\u5cf0\u671f\uff0c\u51c6\u786e\u9884\u6d4b\u9700\u6c42\u5cf0\u503c\u5c24\u4e3a\u56f0\u96be\u3002", "method": "TAT\u6a21\u578b\u5305\u542b\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u5747\u5d4c\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684Temporal Alignment Attention (TAA)\u673a\u5236\uff0c\u7528\u4e8e\u5b66\u4e60\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u5bf9\u9f50\u65b9\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u578b\u7535\u5546\u6570\u636e\u96c6\u4e0a\uff0cTAT\u5728\u9700\u6c42\u5cf0\u503c\u9884\u6d4b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe30%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u6574\u4f53\u6027\u80fd\u4e0e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "TAT\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5cf0\u503c\u9700\u6c42\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4f9b\u5e94\u94fe\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
