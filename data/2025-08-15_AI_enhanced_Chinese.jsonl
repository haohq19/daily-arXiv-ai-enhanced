{"id": "2508.09999", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09999", "abs": "https://arxiv.org/abs/2508.09999", "authors": ["Yuzhuo Xiao", "Zeyu Han", "Yuhan Wang", "Huaizu Jiang"], "title": "XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs", "comment": "For associated code and dataset, see https://github.com/neu-vi/XFacta", "summary": "The rapid spread of multimodal misinformation on social media calls for more\neffective and robust detection methods. Recent advances leveraging multimodal\nlarge language models (MLLMs) have shown the potential in addressing this\nchallenge. However, it remains unclear exactly where the bottleneck of existing\napproaches lies (evidence retrieval v.s. reasoning), hindering the further\nadvances in this field. On the dataset side, existing benchmarks either contain\noutdated events, leading to evaluation bias due to discrepancies with\ncontemporary social media scenarios as MLLMs can simply memorize these events,\nor artificially synthetic, failing to reflect real-world misinformation\npatterns. Additionally, it lacks comprehensive analyses of MLLM-based model\ndesign strategies. To address these issues, we introduce XFacta, a\ncontemporary, real-world dataset that is better suited for evaluating\nMLLM-based detectors. We systematically evaluate various MLLM-based\nmisinformation detection strategies, assessing models across different\narchitectures and scales, as well as benchmarking against existing detection\nmethods. Building on these analyses, we further enable a semi-automatic\ndetection-in-the-loop framework that continuously updates XFacta with new\ncontent to maintain its contemporary relevance. Our analysis provides valuable\ninsights and practices for advancing the field of multimodal misinformation\ndetection. The code and data have been released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faXFacta\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u74f6\u9888\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4ee5\u53ca\u6570\u636e\u96c6\u8fc7\u65f6\u6216\u4eba\u5de5\u5408\u6210\u5bfc\u81f4\u7684\u8bc4\u4f30\u504f\u5dee\u3002", "method": "\u5f15\u5165XFacta\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540cMLLM\u67b6\u6784\u548c\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u534a\u81ea\u52a8\u68c0\u6d4b\u6846\u67b6\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9MLLM\u6a21\u578b\u8bbe\u8ba1\u7b56\u7565\u7684\u5168\u9762\u5206\u6790\uff0c\u5e76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6570\u636e\u3002", "conclusion": "XFacta\u6570\u636e\u96c6\u548c\u5206\u6790\u4e3a\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u5b9e\u8df5\u3002"}}
{"id": "2508.10241", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10241", "abs": "https://arxiv.org/abs/2508.10241", "authors": ["Mark Zilberman"], "title": "Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence", "comment": "10 pages", "summary": "This work demonstrates how the concept of the entropic potential of events --\na parameter quantifying the influence of discrete events on the expected future\nentropy of a system -- can enhance uncertainty quantification, decision-making,\nand interpretability in artificial intelligence (AI). Building on its original\nformulation in physics, the framework is adapted for AI by introducing an\nevent-centric measure that captures how actions, observations, or other\ndiscrete occurrences impact uncertainty at future time horizons. Both the\noriginal and AI-adjusted definitions of entropic potential are formalized, with\nthe latter emphasizing conditional expectations to account for counterfactual\nscenarios. Applications are explored in policy evaluation, intrinsic reward\ndesign, explainable AI, and anomaly detection, highlighting the metric's\npotential to unify and strengthen uncertainty modeling in intelligent systems.\nConceptual examples illustrate its use in reinforcement learning, Bayesian\ninference, and anomaly detection, while practical considerations for\ncomputation in complex AI models are discussed. The entropic potential\nframework offers a theoretically grounded, interpretable, and versatile\napproach to managing uncertainty in AI, bridging principles from\nthermodynamics, information theory, and machine learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u71b5\u52bf\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u589e\u5f3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u901a\u8fc7\u91cf\u5316\u79bb\u6563\u4e8b\u4ef6\u5bf9\u7cfb\u7edf\u672a\u6765\u71b5\u7684\u5f71\u54cd\uff0c\u6539\u8fdbAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "method": "\u5c06\u7269\u7406\u5b66\u4e2d\u7684\u71b5\u52bf\u6982\u5ff5\u8c03\u6574\u5230AI\u9886\u57df\uff0c\u5f15\u5165\u4e8b\u4ef6\u4e2d\u5fc3\u5ea6\u91cf\uff0c\u8003\u8651\u53cd\u4e8b\u5b9e\u60c5\u666f\u3002", "result": "\u5728\u7b56\u7565\u8bc4\u4f30\u3001\u5185\u5728\u5956\u52b1\u8bbe\u8ba1\u3001\u53ef\u89e3\u91caAI\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5c55\u793a\u4e86\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u71b5\u52bf\u6846\u67b6\u4e3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.10171", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10171", "abs": "https://arxiv.org/abs/2508.10171", "authors": ["Aaditya Baranwal", "Abdul Mueez", "Jason Voelker", "Guneet Bhatia", "Shruti Vyas"], "title": "SynSpill: Improved Industrial Spill Detection With Synthetic Data", "comment": "Accepted at ICCV (VISION'25 Workshop) 2025", "summary": "Large-scale Vision-Language Models (VLMs) have transformed general-purpose\nvisual recognition through strong zero-shot capabilities. However, their\nperformance degrades significantly in niche, safety-critical domains such as\nindustrial spill detection, where hazardous events are rare, sensitive, and\ndifficult to annotate. This scarcity -- driven by privacy concerns, data\nsensitivity, and the infrequency of real incidents -- renders conventional\nfine-tuning of detectors infeasible for most industrial settings.\n  We address this challenge by introducing a scalable framework centered on a\nhigh-quality synthetic data generation pipeline. We demonstrate that this\nsynthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of\nVLMs and substantially boosts the performance of state-of-the-art object\ndetectors such as YOLO and DETR. Notably, in the absence of synthetic data\n(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than\nthese detectors. When SynSpill is used, both VLMs and detectors achieve marked\nimprovements, with their performance becoming comparable.\n  Our results underscore that high-fidelity synthetic data is a powerful means\nto bridge the domain gap in safety-critical applications. The combination of\nsynthetic generation and lightweight adaptation offers a cost-effective,\nscalable pathway for deploying vision systems in industrial environments where\nreal data is scarce/impractical to obtain.\n  Project Page: https://synspill.vercel.app", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5de5\u4e1a\u6cc4\u6f0f\u68c0\u6d4b\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u3001\u6570\u636e\u654f\u611f\u6027\u548c\u4e8b\u4ef6\u7f55\u89c1\u6027\uff0c\u5de5\u4e1a\u6cc4\u6f0f\u68c0\u6d4b\u7b49\u9886\u57df\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\uff0c\u63d0\u5347VLM\u548c\u68c0\u6d4b\u5668\uff08\u5982YOLO\u548cDETR\uff09\u7684\u6027\u80fd\u3002", "result": "\u5408\u6210\u6570\u636e\uff08SynSpill\uff09\u663e\u8457\u63d0\u5347\u4e86VLM\u548c\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u672a\u89c1\u6cc4\u6f0f\u573a\u666f\u4e2d\u8868\u73b0\u63a5\u8fd1\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u662f\u89e3\u51b3\u5b89\u5168\u5173\u952e\u9886\u57df\u6570\u636e\u7a00\u7f3a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u6280\u672f\uff0c\u4e3a\u5de5\u4e1a\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10233", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10233", "abs": "https://arxiv.org/abs/2508.10233", "authors": ["Li Sun", "Shuheng Chen", "Junyi Fan", "Yong Si", "Minoo Ahmadi", "Elham Pishgar", "Kamiar Alaei", "Maryam Pishgar"], "title": "Interpretable Machine Learning Model for Early Prediction of Acute Kidney Injury in Critically Ill Patients with Cirrhosis: A Retrospective Study", "comment": null, "summary": "Background: Cirrhosis is a progressive liver disease with high mortality and\nfrequent complications, notably acute kidney injury (AKI), which occurs in up\nto 50% of hospitalized patients and worsens outcomes. AKI stems from complex\nhemodynamic, inflammatory, and metabolic changes, making early detection\nessential. Many predictive tools lack accuracy, interpretability, and alignment\nwith intensive care unit (ICU) workflows. This study developed an interpretable\nmachine learning model for early AKI prediction in critically ill patients with\ncirrhosis.\n  Methods: We conducted a retrospective analysis of the MIMIC-IV v2.2 database,\nidentifying 1240 adult ICU patients with cirrhosis and excluding those with ICU\nstays under 48 hours or missing key data. Laboratory and physiological\nvariables from the first 48 hours were extracted. The pipeline included\npreprocessing, missingness filtering, LASSO feature selection, and SMOTE class\nbalancing. Six algorithms-LightGBM, CatBoost, XGBoost, logistic regression,\nnaive Bayes, and neural networks-were trained and evaluated using AUROC,\naccuracy, F1-score, sensitivity, specificity, and predictive values.\n  Results: LightGBM achieved the best performance (AUROC 0.808, 95% CI\n0.741-0.856; accuracy 0.704; NPV 0.911). Key predictors included prolonged\npartial thromboplastin time, absence of outside-facility 20G placement, low pH,\nand altered pO2, consistent with known cirrhosis-AKI mechanisms and suggesting\nactionable targets.\n  Conclusion: The LightGBM-based model enables accurate early AKI risk\nstratification in ICU patients with cirrhosis using routine clinical variables.\nIts high negative predictive value supports safe de-escalation for low-risk\npatients, and interpretability fosters clinician trust and targeted prevention.\nExternal validation and integration into electronic health record systems are\nwarranted.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u65e9\u671f\u9884\u6d4b\u809d\u786c\u5316\u91cd\u75c7\u60a3\u8005\u7684\u6025\u6027\u80be\u635f\u4f24\uff08AKI\uff09\uff0cLightGBM\u8868\u73b0\u6700\u4f73\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u809d\u786c\u5316\u60a3\u8005\u4e2dAKI\u53d1\u751f\u7387\u9ad8\u4e14\u9884\u540e\u5dee\uff0c\u73b0\u6709\u9884\u6d4b\u5de5\u5177\u7f3a\u4e4f\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65e9\u671f\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u7814\u7a76\u56de\u987e\u6027\u5206\u6790\u4e86MIMIC-IV\u6570\u636e\u5e93\uff0c\u7b5b\u9009\u4e861240\u540d\u809d\u786c\u5316ICU\u60a3\u8005\uff0c\u63d0\u53d6\u4e8648\u5c0f\u65f6\u5185\u7684\u5b9e\u9a8c\u5ba4\u548c\u751f\u7406\u6570\u636e\uff0c\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "LightGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08AUROC 0.808\uff09\uff0c\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u5305\u62ec\u90e8\u5206\u51dd\u8840\u6d3b\u9176\u65f6\u95f4\u5ef6\u957f\u3001\u4f4epH\u503c\u7b49\uff0c\u4e0eAKI\u673a\u5236\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u809d\u786c\u5316ICU\u60a3\u8005\u7684AKI\u98ce\u9669\uff0c\u9ad8\u9634\u6027\u9884\u6d4b\u503c\u652f\u6301\u4f4e\u98ce\u9669\u60a3\u8005\u7684\u5b89\u5168\u7ba1\u7406\uff0c\u672a\u6765\u9700\u5916\u90e8\u9a8c\u8bc1\u548c\u7535\u5b50\u75c5\u5386\u7cfb\u7edf\u6574\u5408\u3002"}}
{"id": "2508.10010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10010", "abs": "https://arxiv.org/abs/2508.10010", "authors": ["Ayana Hussain", "Patrick Zhao", "Nicholas Vincent"], "title": "An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs", "comment": null, "summary": "Large Language Models (LLMs) are a double-edged sword capable of generating\nharmful misinformation -- inadvertently, or when prompted by \"jailbreak\"\nattacks that attempt to produce malicious outputs. LLMs could, with additional\nresearch, be used to detect and prevent the spread of misinformation. In this\npaper, we investigate the efficacy and characteristics of LLM-produced\njailbreak attacks that cause other models to produce harmful medical\nmisinformation. We also study how misinformation generated by jailbroken LLMs\ncompares to typical misinformation found on social media, and how effectively\nit can be detected using standard machine learning approaches. Specifically, we\nclosely examine 109 distinct attacks against three target LLMs and compare the\nattack prompts to in-the-wild health-related LLM queries. We also examine the\nresulting jailbreak responses, comparing the generated misinformation to\nhealth-related misinformation on Reddit. Our findings add more evidence that\nLLMs can be effectively used to detect misinformation from both other LLMs and\nfrom people, and support a body of work suggesting that with careful design,\nLLMs can contribute to a healthier overall information ecosystem.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u6709\u5bb3\u533b\u7597\u9519\u8bef\u4fe1\u606f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528LLMs\u68c0\u6d4b\u548c\u9632\u6b62\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\u3002", "motivation": "LLMs\u53ef\u80fd\u88ab\u6ee5\u7528\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff0c\u4f46\u4e5f\u53ef\u7528\u4e8e\u68c0\u6d4b\u548c\u9632\u6b62\u5176\u4f20\u64ad\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "method": "\u5206\u6790\u4e86109\u79cd\u9488\u5bf9\u4e09\u79cd\u76ee\u6807LLMs\u7684\u201c\u8d8a\u72f1\u201d\u653b\u51fb\uff0c\u6bd4\u8f83\u4e86\u653b\u51fb\u63d0\u793a\u4e0e\u771f\u5b9e\u5065\u5eb7\u76f8\u5173\u67e5\u8be2\uff0c\u5e76\u8bc4\u4f30\u4e86\u751f\u6210\u7684\u9519\u8bef\u4fe1\u606f\u4e0eReddit\u4e0a\u7684\u9519\u8bef\u4fe1\u606f\u7684\u5dee\u5f02\u3002", "result": "LLMs\u80fd\u6709\u6548\u68c0\u6d4b\u6765\u81ea\u5176\u4ed6LLMs\u548c\u4eba\u7c7b\u7684\u9519\u8bef\u4fe1\u606f\uff0c\u652f\u6301\u5176\u5728\u5065\u5eb7\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u79ef\u6781\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\uff0cLLMs\u53ef\u4ee5\u4e3a\u6784\u5efa\u66f4\u5065\u5eb7\u7684\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2508.10021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10021", "abs": "https://arxiv.org/abs/2508.10021", "authors": ["Egor Fadeev", "Dzhambulat Mollaev", "Aleksei Shestov", "Dima Korolev", "Omar Zoloev", "Ivan Kireev", "Andrey Savchenko", "Maksim Makarenko"], "title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients", "comment": null, "summary": "Learning clients embeddings from sequences of their historic communications\nis central to financial applications. While large language models (LLMs) offer\ngeneral world knowledge, their direct use on long event sequences is\ncomputationally expensive and impractical in real-world pipelines. In this\npaper, we propose LATTE, a contrastive learning framework that aligns raw event\nembeddings with semantic embeddings from frozen LLMs. Behavioral features are\nsummarized into short prompts, embedded by the LLM, and used as supervision via\ncontrastive loss. The proposed approach significantly reduces inference cost\nand input size compared to conventional processing of complete sequence by LLM.\nWe experimentally show that our method outperforms state-of-the-art techniques\nfor learning event sequence representations on real-world financial datasets\nwhile remaining deployable in latency-sensitive environments.", "AI": {"tldr": "LATTE\u662f\u4e00\u79cd\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u539f\u59cb\u4e8b\u4ef6\u5d4c\u5165\u4e0e\u51bb\u7ed3LLM\u7684\u8bed\u4e49\u5d4c\u5165\u5bf9\u9f50\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u548c\u8f93\u5165\u5927\u5c0f\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u91d1\u878d\u5e94\u7528\u4e2d\uff0c\u4ece\u5ba2\u6237\u5386\u53f2\u901a\u4fe1\u5e8f\u5217\u5b66\u4e60\u5d4c\u5165\u662f\u5173\u952e\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528LLM\u5904\u7406\u957f\u5e8f\u5217\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51faLATTE\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5c06\u539f\u59cb\u4e8b\u4ef6\u5d4c\u5165\u4e0eLLM\u751f\u6210\u7684\u8bed\u4e49\u5d4c\u5165\u5bf9\u9f50\uff0c\u901a\u8fc7\u77ed\u63d0\u793a\u548c\u884c\u4e3a\u7279\u5f81\u8fdb\u884c\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLATTE\u5728\u771f\u5b9e\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u5ef6\u8fdf\u654f\u611f\u73af\u5883\u3002", "conclusion": "LATTE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u91d1\u878d\u9886\u57df\u7684\u5e8f\u5217\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2508.10025", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10025", "abs": "https://arxiv.org/abs/2508.10025", "authors": ["Silvia Garc\u00eda-M\u00e9ndez", "Francisco de Arriba-P\u00e9rez"], "title": "Detecting and explaining postpartum depression in real-time with generative artificial intelligence", "comment": null, "summary": "Among the many challenges mothers undergo after childbirth, postpartum\ndepression (PPD) is a severe condition that significantly impacts their mental\nand physical well-being. Consequently, the rapid detection of ppd and their\nassociated risk factors is critical for in-time assessment and intervention\nthrough specialized prevention procedures. Accordingly, this work addresses the\nneed to help practitioners make decisions with the latest technological\nadvancements to enable real-time screening and treatment recommendations.\nMainly, our work contributes to an intelligent PPD screening system that\ncombines Natural Language Processing, Machine Learning (ML), and Large Language\nModels (LLMs) towards an affordable, real-time, and non-invasive free speech\nanalysis. Moreover, it addresses the black box problem since the predictions\nare described to the end users thanks to the combination of LLMs with\ninterpretable ml models (i.e., tree-based algorithms) using feature importance\nand natural language. The results obtained are 90 % on ppd detection for all\nevaluation metrics, outperforming the competing solutions in the literature.\nUltimately, our solution contributes to the rapid detection of PPD and their\nassociated risk factors, critical for in-time and proper assessment and\nintervention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4ea7\u540e\u6291\u90c1\u75c7\u7b5b\u67e5\u7cfb\u7edf\uff0c\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u3001\u975e\u4fb5\u5165\u6027\u7684\u514d\u8d39\u8bed\u97f3\u5206\u6790\uff0c\u5e76\u89e3\u51b3\u9ed1\u76d2\u95ee\u9898\u3002", "motivation": "\u4ea7\u540e\u6291\u90c1\u75c7\uff08PPD\uff09\u4e25\u91cd\u5f71\u54cd\u6bcd\u4eb2\u7684\u8eab\u5fc3\u5065\u5eb7\uff0c\u5feb\u901f\u68c0\u6d4bPPD\u53ca\u5176\u76f8\u5173\u98ce\u9669\u56e0\u7d20\u5bf9\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u6700\u65b0\u6280\u672f\u5e2e\u52a9\u4ece\u4e1a\u8005\u8fdb\u884c\u5b9e\u65f6\u7b5b\u67e5\u548c\u6cbb\u7597\u63a8\u8350\u3002", "method": "\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u3001\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u6811\u7684\u7b97\u6cd5\u548c\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002", "result": "\u5728PPD\u68c0\u6d4b\u4e2d\uff0c\u6240\u6709\u8bc4\u4f30\u6307\u6807\u8fbe\u523090%\uff0c\u4f18\u4e8e\u73b0\u6709\u6587\u732e\u4e2d\u7684\u7ade\u4e89\u65b9\u6848\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u6709\u52a9\u4e8e\u5feb\u901f\u68c0\u6d4bPPD\u53ca\u5176\u98ce\u9669\u56e0\u7d20\uff0c\u4e3a\u53ca\u65f6\u8bc4\u4f30\u548c\u5e72\u9884\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2508.10471", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10471", "abs": "https://arxiv.org/abs/2508.10471", "authors": ["Xinrui Li", "Qilin Fan", "Tianfu Wang", "Kaiwen Wei", "Ke Yu", "Xu Zhang"], "title": "GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation", "comment": null, "summary": "Federated graph learning (FGL) enables multiple clients to collaboratively\ntrain powerful graph neural networks without sharing their private,\ndecentralized graph data. Inherited from generic federated learning, FGL is\ncritically challenged by statistical heterogeneity, where non-IID data\ndistributions across clients can severely impair model performance. A\nparticularly destructive form of this is class imbalance, which causes the\nglobal model to become biased towards majority classes and fail at identifying\nrare but critical events. This issue is exacerbated in FGL, as nodes from a\nminority class are often surrounded by biased neighborhood information,\nhindering the learning of expressive embeddings. To grapple with this\nchallenge, we propose GraphFedMIG, a novel FGL framework that reframes the\nproblem as a federated generative data augmentation task. GraphFedMIG employs a\nhierarchical generative adversarial network where each client trains a local\ngenerator to synthesize high-fidelity feature representations. To provide\ntailored supervision, clients are grouped into clusters, each sharing a\ndedicated discriminator. Crucially, the framework designs a mutual\ninformation-guided mechanism to steer the evolution of these client generators.\nBy calculating each client's unique informational value, this mechanism\ncorrects the local generator parameters, ensuring that subsequent rounds of\nmutual information-guided generation are focused on producing high-value,\nminority-class features. We conduct extensive experiments on four real-world\ndatasets, and the results demonstrate the superiority of the proposed\nGraphFedMIG compared with other baselines.", "AI": {"tldr": "GraphFedMIG\u662f\u4e00\u4e2a\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u591a\u6570\u7c7b\uff0c\u5f71\u54cd\u5c11\u6570\u7c7b\u8bc6\u522b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u5ba2\u6237\u7aef\u8bad\u7ec3\u672c\u5730\u751f\u6210\u5668\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u5f15\u5bfc\u673a\u5236\u4f18\u5316\u751f\u6210\u5668\u53c2\u6570\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86GraphFedMIG\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "GraphFedMIG\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2508.10192", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2508.10192", "abs": "https://arxiv.org/abs/2508.10192", "authors": ["Igor Halperin"], "title": "Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models", "comment": "24 pages, 3 figures", "summary": "The proliferation of Large Language Models (LLMs) is challenged by\nhallucinations, critical failure modes where models generate non-factual,\nnonsensical or unfaithful text. This paper introduces Semantic Divergence\nMetrics (SDM), a novel lightweight framework for detecting Faithfulness\nHallucinations -- events of severe deviations of LLMs responses from input\ncontexts. We focus on a specific implementation of these LLM errors,\n{confabulations, defined as responses that are arbitrary and semantically\nmisaligned with the user's query. Existing methods like Semantic Entropy test\nfor arbitrariness by measuring the diversity of answers to a single, fixed\nprompt. Our SDM framework improves upon this by being more prompt-aware: we\ntest for a deeper form of arbitrariness by measuring response consistency not\nonly across multiple answers but also across multiple, semantically-equivalent\nparaphrases of the original prompt. Methodologically, our approach uses joint\nclustering on sentence embeddings to create a shared topic space for prompts\nand answers. A heatmap of topic co-occurances between prompts and responses can\nbe viewed as a quantified two-dimensional visualization of the user-machine\ndialogue. We then compute a suite of information-theoretic metrics to measure\nthe semantic divergence between prompts and responses. Our practical score,\n$\\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein\ndistance to quantify this divergence, with a high score indicating a\nFaithfulness hallucination. Furthermore, we identify the KL divergence\nKL(Answer $||$ Prompt) as a powerful indicator of \\textbf{Semantic\nExploration}, a key signal for distinguishing different generative behaviors.\nThese metrics are further combined into the Semantic Box, a diagnostic\nframework for classifying LLM response types, including the dangerous,\nconfident confabulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6SDM\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u751f\u6210\u7684\u6587\u672c\u662f\u5426\u4e0e\u8f93\u5165\u4e0a\u4e0b\u6587\u4e25\u91cd\u504f\u79bb\uff0c\u5373\u201c\u5fe0\u5b9e\u6027\u5e7b\u89c9\u201d\u3002\u901a\u8fc7\u591a\u63d0\u793a\u548c\u591a\u56de\u7b54\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u6d4b\u8bd5\uff0c\u7ed3\u5408\u4fe1\u606f\u8bba\u6307\u6807\u91cf\u5316\u8bed\u4e49\u5dee\u5f02\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u975e\u4e8b\u5b9e\u3001\u65e0\u610f\u4e49\u6216\u4e0d\u5fe0\u5b9e\u6587\u672c\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u201c\u5fe0\u5b9e\u6027\u5e7b\u89c9\u201d\u73b0\u8c61\u3002", "method": "\u4f7f\u7528\u8054\u5408\u805a\u7c7b\u521b\u5efa\u63d0\u793a\u548c\u56de\u7b54\u7684\u5171\u4eab\u4e3b\u9898\u7a7a\u95f4\uff0c\u901a\u8fc7\u70ed\u56fe\u548c\u4fe1\u606f\u8bba\u6307\u6807\uff08\u5982Jensen-Shannon\u6563\u5ea6\u548cWasserstein\u8ddd\u79bb\uff09\u91cf\u5316\u8bed\u4e49\u5dee\u5f02\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u8bc4\u5206$\\mathcal{S}_H$\u548cKL\u6563\u5ea6\u6307\u6807\uff0c\u7528\u4e8e\u533a\u5206\u4e0d\u540c\u751f\u6210\u884c\u4e3a\uff0c\u5e76\u6784\u5efa\u4e86\u201c\u8bed\u4e49\u76d2\u201d\u8bca\u65ad\u6846\u67b6\u3002", "conclusion": "SDM\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4bLLM\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\uff0c\u4e3aLLM\u751f\u6210\u6587\u672c\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2508.10473", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.10473", "abs": "https://arxiv.org/abs/2508.10473", "authors": ["Liangrui Pan", "xiaoyu Li", "Guang Zhu", "Guanting Li", "Ruixin Wang", "Jiadi Luo", "Yaning Yang", "Liang qingchun", "Shaoliang Peng"], "title": "STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images", "comment": "Submit to AAAI2026", "summary": "Spread through air spaces (STAS) constitutes a novel invasive pattern in lung\nadenocarcinoma (LUAD), associated with tumor recurrence and diminished survival\nrates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive\nendeavor, compounded by the propensity for oversight and misdiagnosis due to\nits distinctive pathological characteristics and morphological features.\nConsequently, there is a pressing clinical imperative to leverage deep learning\nmodels for STAS diagnosis. This study initially assembled histopathological\nimages from STAS patients at the Second Xiangya Hospital and the Third Xiangya\nHospital of Central South University, alongside the TCGA-LUAD cohort. Three\nsenior pathologists conducted cross-verification annotations to construct the\nSTAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern\nattention-aware multiple instance learning framework, named STAMP, to analyze\nand diagnose the presence of STAS across multi-center histopathology images.\nSpecifically, the dual-branch architecture guides the model to learn\nSTAS-associated pathological features from distinct semantic spaces.\nTransformer-based instance encoding and a multi-pattern attention aggregation\nmodules dynamically selects regions closely associated with STAS pathology,\nsuppressing irrelevant noise and enhancing the discriminative power of global\nrepresentations. Moreover, a similarity regularization constraint prevents\nfeature redundancy across branches, thereby improving overall diagnostic\naccuracy. Extensive experiments demonstrated that STAMP achieved competitive\ndiagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,\n0.8017, and 0.7928, respectively, surpassing the clinical level.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTAMP\u7684\u591a\u6a21\u5f0f\u6ce8\u610f\u529b\u611f\u77e5\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65ad\u80ba\u817a\u764c\u4e2d\u7684STAS\u6a21\u5f0f\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "STAS\uff08\u901a\u8fc7\u7a7a\u6c14\u4f20\u64ad\uff09\u662f\u80ba\u817a\u764c\u7684\u4e00\u79cd\u65b0\u578b\u4fb5\u88ad\u6a21\u5f0f\uff0c\u4e0e\u80bf\u7624\u590d\u53d1\u548c\u751f\u5b58\u7387\u964d\u4f4e\u76f8\u5173\uff0c\u4f46\u5927\u89c4\u6a21\u8bca\u65ad\u5b58\u5728\u56f0\u96be\uff0c\u4e9f\u9700\u6df1\u5ea6\u5b66\u4e60\u8f85\u52a9\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e86\u591a\u4e2d\u5fc3\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u6784\u5efa\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86STAMP\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u548cTransformer\u7f16\u7801\uff0c\u52a8\u6001\u9009\u62e9\u4e0eSTAS\u76f8\u5173\u7684\u75c5\u7406\u533a\u57df\u3002", "result": "STAMP\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684AUC\u5206\u522b\u4e3a0.8058\u30010.8017\u548c0.7928\uff0c\u8d85\u8d8a\u4e86\u4e34\u5e8a\u6c34\u5e73\u3002", "conclusion": "STAMP\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86STAS\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.10542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10542", "abs": "https://arxiv.org/abs/2508.10542", "authors": ["Mengyu Ren", "Yutong Li", "Hua Li", "Runmin Cong", "Sam Kwong"], "title": "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images", "comment": null, "summary": "Salient object detection (SOD) in optical remote sensing images (ORSIs) faces\nnumerous challenges, including significant variations in target scales and low\ncontrast between targets and the background. Existing methods based on vision\ntransformers (ViTs) and convolutional neural networks (CNNs) architectures aim\nto leverage both global and local features, but the difficulty in effectively\nintegrating these heterogeneous features limits their overall performance. To\novercome these limitations, we propose a graph-enhanced contextual and regional\nperception network (GCRPNet), which builds upon the Mamba architecture to\nsimultaneously capture long-range dependencies and enhance regional feature\nrepresentation. Specifically, we employ the visual state space (VSS) encoder to\nextract multi-scale features. To further achieve deep guidance and enhancement\nof these features, we first design a difference-similarity guided hierarchical\ngraph attention module (DS-HGAM). This module strengthens cross-layer\ninteraction capabilities between features of different scales while enhancing\nthe model's structural perception,allowing it to distinguish between foreground\nand background more effectively. Then, we design the LEVSS block as the decoder\nof GCRPNet. This module integrates our proposed adaptive scanning strategy and\nmulti-granularity collaborative attention enhancement module (MCAEM). It\nperforms adaptive patch scanning on feature maps processed via multi-scale\nconvolutions, thereby capturing rich local region information and enhancing\nMamba's local modeling capability. Extensive experimental results demonstrate\nthat the proposed model achieves state-of-the-art performance, validating its\neffectiveness and superiority.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u7684GCRPNet\uff0c\u901a\u8fc7\u56fe\u589e\u5f3a\u6a21\u5757\u548c\u81ea\u9002\u5e94\u626b\u63cf\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u4e2d\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u4e2d\u76ee\u6807\u5c3a\u5ea6\u53d8\u5316\u5927\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u878d\u5408\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u56fe\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u626b\u63cf\u7b56\u7565\u548c\u591a\u7c92\u5ea6\u6ce8\u610f\u529b\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGCRPNet\u5728\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GCRPNet\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u4e2d\u663e\u8457\u76ee\u6807\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2508.10582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10582", "abs": "https://arxiv.org/abs/2508.10582", "authors": ["Yixing Liu", "Minggui Teng", "Yifei Xia", "Peiqi Duan", "Boxin Shi"], "title": "EvTurb: Event Camera Guided Turbulence Removal", "comment": null, "summary": "Atmospheric turbulence degrades image quality by introducing blur and\ngeometric tilt distortions, posing significant challenges to downstream\ncomputer vision tasks. Existing single-image and multi-frame methods struggle\nwith the highly ill-posed nature of this problem due to the compositional\ncomplexity of turbulence-induced distortions. To address this, we propose\nEvTurb, an event guided turbulence removal framework that leverages high-speed\nevent streams to decouple blur and tilt effects. EvTurb decouples blur and tilt\neffects by modeling event-based turbulence formation, specifically through a\nnovel two-step event-guided network: event integrals are first employed to\nreduce blur in the coarse outputs. This is followed by employing a variance\nmap, derived from raw event streams, to eliminate the tilt distortion for the\nrefined outputs. Additionally, we present TurbEvent, the first real-captured\ndataset featuring diverse turbulence scenarios. Experimental results\ndemonstrate that EvTurb surpasses state-of-the-art methods while maintaining\ncomputational efficiency.", "AI": {"tldr": "EvTurb\u5229\u7528\u4e8b\u4ef6\u6d41\u89e3\u8026\u6e4d\u6d41\u5f15\u8d77\u7684\u6a21\u7cca\u548c\u503e\u659c\u5931\u771f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4e8b\u4ef6\u5f15\u5bfc\u7f51\u7edc\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u771f\u5b9e\u6e4d\u6d41\u6570\u636e\u96c6TurbEvent\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u56fe\u50cf\u6a21\u7cca\u548c\u51e0\u4f55\u503e\u659c\u5931\u771f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e00\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51faEvTurb\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u6d41\u5efa\u6a21\u6e4d\u6d41\u5f62\u6210\uff0c\u5206\u4e24\u6b65\uff1a\u4e8b\u4ef6\u79ef\u5206\u51cf\u5c11\u6a21\u7cca\uff0c\u65b9\u5dee\u56fe\u6d88\u9664\u503e\u659c\u5931\u771f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEvTurb\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "EvTurb\u901a\u8fc7\u4e8b\u4ef6\u6d41\u6709\u6548\u89e3\u8026\u6e4d\u6d41\u5931\u771f\uff0c\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u771f\u5b9e\u6e4d\u6d41\u6570\u636e\u96c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.10701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10701", "abs": "https://arxiv.org/abs/2508.10701", "authors": ["Tianlong Yu", "Lihong Liu", "Ziyi Zhou", "Fudu Xing", "Kailong Wang", "Yang Yang"], "title": "REFN: A Reinforcement-Learning-From-Network Framework against 1-day/n-day Exploitations", "comment": null, "summary": "The exploitation of 1 day or n day vulnerabilities poses severe threats to\nnetworked devices due to massive deployment scales and delayed patching\n(average Mean Time To Patch exceeds 60 days). Existing defenses, including host\nbased patching and network based filtering, are inadequate due to limited\nscalability across diverse devices, compatibility issues especially with\nembedded or legacy systems, and error prone deployment process (manual patch\nvalidation). To address these issues, we introduce REFN (Reinforcement Learning\nFrom Network), a novel framework that trains Large Language Models (LLMs) to\nautonomously generate network filters to prevent 1 day or n day exploitations.\nREFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven\nby online network rewards instead of traditional Human Feedback (RLHF). REFN\nguarantees compatibility via unified deployment on edge security gateways\n(Amazon Eero). REFN provides robustness via online validation using real\nnetwork traffic. Crucially, REFN addresses three core challenges in training\nLLMs for exploit prevention: 1) expanding current LLMs limited vulnerability\nfixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging\ncurrent LLMs language to network gaps through an RL From VNF Pipeline that\ntranslates language context (vulnerability description) into network\nenforcement, 3) addressing the LLM hallucination and non determinism via the\nOnline Agentic Validation that penalizes erroneous outputs. Evaluated across 22\nfamilies of 1 day or n day exploits, REFN demonstrates effectiveness (21.1\npercent higher accuracy than alternatives), efficiency (Mean Time To Patch of\n3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an\ninitial step toward training LLMs to rapidly prevent massive scale 1 day or n\nday exploitations.", "AI": {"tldr": "REFN\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u81ea\u52a8\u751f\u6210\u7f51\u7edc\u8fc7\u6ee4\u5668\uff0c\u6709\u6548\u5e94\u5bf91\u5929\u6216N\u5929\u6f0f\u6d1e\u5a01\u80c1\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u5728\u6269\u5c55\u6027\u3001\u517c\u5bb9\u6027\u548c\u90e8\u7f72\u51c6\u786e\u6027\u4e0a\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5927\u89c4\u6a21\u6f0f\u6d1e\u5a01\u80c1\u3002", "method": "REFN\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u7f51\u7edc\u5956\u52b1\uff0c\u901a\u8fc7\u8fb9\u7f18\u5b89\u5168\u7f51\u5173\u7edf\u4e00\u90e8\u7f72\uff0c\u5e76\u91c7\u7528\u5728\u7ebf\u9a8c\u8bc1\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "result": "REFN\u572822\u7c7b\u6f0f\u6d1e\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u63d0\u534721.1%\uff0c\u5e73\u5747\u4fee\u590d\u65f6\u95f4\u7f29\u77ed\u81f33.65\u5c0f\u65f6\u3002", "conclusion": "REFN\u4e3a\u5229\u7528LLM\u5feb\u901f\u9632\u5fa1\u5927\u89c4\u6a21\u6f0f\u6d1e\u63d0\u4f9b\u4e86\u521d\u6b65\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10704", "abs": "https://arxiv.org/abs/2508.10704", "authors": ["Zhanwen Liu", "Yujing Sun", "Yang Wang", "Nan Yang", "Shengbo Eben Li", "Xiangmo Zhao"], "title": "Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios", "comment": null, "summary": "The dynamic range limitation of conventional RGB cameras reduces global\ncontrast and causes loss of high-frequency details such as textures and edges\nin complex traffic environments (e.g., nighttime driving, tunnels), hindering\ndiscriminative feature extraction and degrading frame-based object detection.\nTo address this, we integrate a bio-inspired event camera with an RGB camera to\nprovide high dynamic range information and propose a motion cue fusion network\n(MCFNet), which achieves optimal spatiotemporal alignment and adaptive\ncross-modal feature fusion under challenging lighting. Specifically, an event\ncorrection module (ECM) temporally aligns asynchronous event streams with image\nframes via optical-flow-based warping, jointly optimized with the detection\nnetwork to learn task-aware event representations. The event dynamic upsampling\nmodule (EDUM) enhances spatial resolution of event frames to match image\nstructures, ensuring precise spatiotemporal alignment. The cross-modal mamba\nfusion module (CMM) uses adaptive feature fusion with a novel interlaced\nscanning mechanism, effectively integrating complementary information for\nrobust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD\ndatasets demonstrate that MCFNet significantly outperforms existing methods in\nvarious poor lighting and fast moving traffic scenarios. Notably, on the\nDSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best\nexisting methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The\ncode is available at https://github.com/Charm11492/MCFNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548cRGB\u76f8\u673a\u7684\u65b9\u6cd5\uff08MCFNet\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8303\u56f4\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRGB\u76f8\u673a\u52a8\u6001\u8303\u56f4\u6709\u9650\uff0c\u5bfc\u81f4\u590d\u6742\u4ea4\u901a\u73af\u5883\uff08\u5982\u591c\u95f4\u9a7e\u9a76\u3001\u96a7\u9053\uff09\u4e2d\u5168\u5c40\u5bf9\u6bd4\u5ea6\u964d\u4f4e\u548c\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\uff0c\u5f71\u54cd\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faMCFNet\uff0c\u5305\u542b\u4e8b\u4ef6\u6821\u6b63\u6a21\u5757\uff08ECM\uff09\u3001\u4e8b\u4ef6\u52a8\u6001\u4e0a\u91c7\u6837\u6a21\u5757\uff08EDUM\uff09\u548c\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\uff08CMM\uff09\uff0c\u5b9e\u73b0\u65f6\u7a7a\u5bf9\u9f50\u548c\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728DSEC-Det\u548cPKU-DAVIS-SOD\u6570\u636e\u96c6\u4e0a\uff0cMCFNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cDSEC-Det\u4e0amAP50\u63d0\u53477.4%\uff0cmAP\u63d0\u53471.7%\u3002", "conclusion": "MCFNet\u901a\u8fc7\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548cRGB\u76f8\u673a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5149\u7167\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
