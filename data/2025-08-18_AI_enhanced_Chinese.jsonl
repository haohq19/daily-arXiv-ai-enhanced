{"id": "2508.10918", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10918", "abs": "https://arxiv.org/abs/2508.10918", "authors": ["Samantha Aziz", "Oleg Komogortsev"], "title": "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder", "comment": "IJCB 2025; 11 pages, 7 figures", "summary": "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9690\u79c1\u589e\u5f3a\u673a\u5236\uff0c\u901a\u8fc7\u6f5c\u5728\u566a\u58f0\u81ea\u7f16\u7801\u5668\u4fdd\u62a4\u7528\u6237\u89c6\u7ebf\u6570\u636e\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u7559\u6570\u636e\u53ef\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u89c6\u7ebf\u6570\u636e\u5728\u8de8\u4f1a\u8bdd\u4e2d\u88ab\u91cd\u65b0\u8bc6\u522b\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u4ecd\u53ef\u7528\u4e8e\u826f\u6027\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u566a\u58f0\u81ea\u7f16\u7801\u5668\uff0c\u5e73\u8861\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\uff0c\u4fdd\u7559\u751f\u7406\u4e0a\u5408\u7406\u7684\u89c6\u7ebf\u6a21\u5f0f\u3002", "result": "\u663e\u8457\u964d\u4f4e\u751f\u7269\u7279\u5f81\u53ef\u8bc6\u522b\u6027\uff0c\u540c\u65f6\u5b9e\u7528\u6027\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "\u8be5\u673a\u5236\u4e3a\u89c6\u7ebf\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10975", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10975", "abs": "https://arxiv.org/abs/2508.10975", "authors": ["Pratyush Maini", "Vineeth Dorna", "Parth Doshi", "Aldo Carranza", "Fan Pan", "Jack Urbanek", "Paul Burstein", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Charvi Bannur", "Christina Baek", "Darren Teh", "David Schwab", "Haakon Mongstad", "Haoli Yin", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Ricardo Monti", "Rishabh Adiga", "Siddharth Joshi", "Spandan Das", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining", "comment": null, "summary": "Recent advances in large language model (LLM) pretraining have shown that\nsimply scaling data quantity eventually leads to diminishing returns, hitting a\ndata wall. In response, the use of synthetic data for pretraining has emerged\nas a promising paradigm for pushing the frontier of performance. Despite this,\nthe factors affecting synthetic data quality remain poorly understood. In this\nwork, we introduce BeyondWeb, a synthetic data generation framework that\nproduces high-quality synthetic data for pretraining. BeyondWeb significantly\nextends the capabilities of traditional web-scale datasets, outperforming\nstate-of-the-art synthetic pretraining datasets such as Cosmopedia and\nNemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1\npercentage points (pp) and 2.6pp, respectively, when averaged across a suite of\n14 benchmark evaluations. It delivers up to 7.7x faster training than open web\ndata and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for\n180B tokens on BeyondWeb outperforms an 8B model trained for the same token\nbudget on Cosmopedia. We also present several insights from BeyondWeb on\nsynthetic data for pretraining: what drives its benefits, which data to\nrephrase and how, and the impact of model size and family on data quality.\nOverall, our work shows that there's no silver bullet for generating\nhigh-quality synthetic pretraining data. The best outcomes require jointly\noptimizing many factors, a challenging task that requires rigorous science and\npractical expertise. Naive approaches can yield modest improvements,\npotentially at great cost, while well-executed methods can yield transformative\nimprovements, as exemplified by BeyondWeb.", "AI": {"tldr": "BeyondWeb\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5408\u6210\u6570\u636e\u8d28\u91cf\u7684\u6df1\u5165\u89c1\u89e3\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\uff0c\u6570\u636e\u91cf\u7684\u7b80\u5355\u6269\u5c55\u5df2\u9047\u5230\u74f6\u9888\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u63d0\u5347\u6027\u80fd\u7684\u65b0\u65b9\u5411\uff0c\u4f46\u5176\u8d28\u91cf\u5f71\u54cd\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f00\u53d1\u4e86BeyondWeb\u6846\u67b6\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u5e76\u901a\u8fc714\u9879\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "BeyondWeb\u5728\u6027\u80fd\u4e0a\u8d85\u8d8aCosmopedia\u548cNemotron-Synth\uff0c\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\uff0c\u5c0f\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5927\u6a21\u578b\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u9700\u591a\u56e0\u7d20\u8054\u5408\u4f18\u5316\uff0cBeyondWeb\u5c55\u793a\u4e86\u5176\u6f5c\u529b\uff0c\u4f46\u9700\u79d1\u5b66\u65b9\u6cd5\u548c\u5b9e\u8df5\u7ecf\u9a8c\u652f\u6301\u3002"}}
{"id": "2508.11009", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11009", "abs": "https://arxiv.org/abs/2508.11009", "authors": ["Wenpeng Xing", "Lanyi Wei", "Haixiao Hu", "Rongchang Li", "Mohan Li", "Changting Lin", "Meng Han"], "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth", "comment": null, "summary": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSproutBench\uff0c\u4e00\u4e2a\u9488\u5bf9\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b89\u5168\u8bc4\u4f30\u5957\u4ef6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5b89\u5168\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86LLM\u5728\u5e74\u9f84\u7279\u5b9a\u98ce\u9669\u4e0a\u7684\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709AI\u5b89\u5168\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u6210\u4eba\uff0c\u5ffd\u89c6\u4e86\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u5728\u8ba4\u77e5\u3001\u60c5\u611f\u548c\u793e\u4ea4\u65b9\u9762\u7684\u72ec\u7279\u8106\u5f31\u6027\u3002", "method": "\u5f15\u5165SproutBench\uff0c\u5305\u542b1,283\u4e2a\u57fa\u4e8e\u53d1\u5c55\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u8bc4\u4f3047\u79cdLLM\u5728\u60c5\u611f\u4f9d\u8d56\u3001\u9690\u79c1\u4fb5\u72af\u7b49\u98ce\u9669\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0LLM\u5b58\u5728\u663e\u8457\u5b89\u5168\u6f0f\u6d1e\uff0c\u5b89\u5168\u6027\u4e0e\u98ce\u9669\u9884\u9632\u5448\u6b63\u76f8\u5173\uff0c\u4ea4\u4e92\u6027\u4e0e\u5e74\u9f84\u9002\u5b9c\u6027\u5448\u8d1f\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4e3a\u513f\u7ae5\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2508.11016", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11016", "abs": "https://arxiv.org/abs/2508.11016", "authors": ["Qingbin Li", "Rongkun Xue", "Jie Wang", "Ming Zhou", "Zhi Li", "Xiaofeng Ji", "Yongqi Wang", "Miao Liu", "Zheming Yang", "Minghui Qiu", "Jing Yang"], "title": "CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention", "comment": null, "summary": "Recent advances in Reinforcement Learning with Verified Reward (RLVR) have\ndriven the emergence of more sophisticated cognitive behaviors in large\nlanguage models (LLMs), thereby enhancing their reasoning capabilities.\nHowever, in prior RLVR pipelines, the repeated use of static initial-state\nsampling drawn exactly from the dataset distribution during each sampling phase\nproduced overly deterministic, low diversity model behavior, which manifested\nas rapid entropy collapse and hindered sustained performance gains during\nprolonged training. To address this issue, we introduce CURE\n(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a\ntwo-stage framework that balances exploration and exploitation. Specifically,\nin the first stage, to deliberately steer the model toward novel yet coherent\ncontexts, we re-generate at high-entropy critical tokens and jointly optimize\nthe original and the branched trajectories. The further comparison with vanilla\nDAPO shows that the regeneration process achieves a better performance on math\nreasoning tasks while sustaining a high-level entropy degree for exploration.\nIn the second stage, we continue training with static initial-state sampling by\nDAPO, intentionally placing the model in a familiar state to gradually\nstrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,\ncompared to other RLVR methods, CURE achieves a 5% performance gain across six\nmath benchmarks, establishing state-of-the-art performance in both entropy and\naccuracy. A series of experiments further validate the effectiveness of our\napproach. Code is available at https://github.com/CURE-Project/CURE.", "AI": {"tldr": "CURE\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3RLVR\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u9759\u6001\u521d\u59cb\u72b6\u6001\u91c7\u6837\u5bfc\u81f4\u6a21\u578b\u884c\u4e3a\u8fc7\u4e8e\u786e\u5b9a\u6027\u548c\u4f4e\u591a\u6837\u6027\uff0c\u963b\u788d\u6301\u7eed\u6027\u80fd\u63d0\u5347\u3002", "method": "CURE\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u9ad8\u71b5\u5173\u952e\u4ee4\u724c\u5f15\u5bfc\u63a2\u7d22\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7ee7\u7eed\u9759\u6001\u91c7\u6837\u4ee5\u52a0\u5f3a\u5229\u7528\u3002", "result": "\u5728Qwen-2.5-Math-7B\u4e0a\uff0cCURE\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u53475%\uff0c\u71b5\u548c\u51c6\u786e\u7387\u5747\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "CURE\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.11286", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11286", "abs": "https://arxiv.org/abs/2508.11286", "authors": ["Che Rin Yu", "Daewon Chae", "Dabin Seo", "Sangwon Lee", "Hyeongwoo Im", "Jinkyu Kim"], "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "comment": null, "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u5f53\u524d\u573a\u666f\u56fe\u4e0e\u53c2\u8003\u573a\u666f\u56fe\uff0c\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u68c0\u6d4b\u5e76\u4fee\u6b63\u6f5c\u5728\u5931\u8d25\uff0c\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4eba\u7c7b\u80fd\u6839\u636e\u73af\u5883\u72b6\u6001\u8c03\u6574\u884c\u4e3a\uff0c\u800c\u673a\u5668\u4eba\u7f3a\u4e4f\u8fd9\u79cd\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u6267\u884c\u5931\u8d25\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u88ab\u52a8\u54cd\u5e94\uff0c\u6548\u7387\u4f4e\u3002", "method": "\u6784\u5efa\u5f53\u524dRGB-D\u89c2\u6d4b\u7684\u573a\u666f\u56fe\u4e0e\u6210\u529f\u6f14\u793a\u7684\u53c2\u8003\u56fe\u5bf9\u6bd4\uff0c\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u5757\u8bca\u65ad\u4e0d\u5339\u914d\u5e76\u8c03\u6574\u8ba1\u5212\u3002", "result": "\u5728AI2-THOR\u6a21\u62df\u5668\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u524d\u68c0\u6d4b\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6846\u67b6\u6709\u6548\u9884\u9632\u6267\u884c\u5931\u8d25\uff0c\u63d0\u5347\u673a\u5668\u4eba\u9002\u5e94\u6027\u3002"}}
{"id": "2508.11197", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.11197", "abs": "https://arxiv.org/abs/2508.11197", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "AI": {"tldr": "E-CaTCH\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\uff0c\u901a\u8fc7\u805a\u7c7b\u4f2a\u4e8b\u4ef6\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u65f6\u95f4\u5efa\u6a21\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9762\u4e34\u6a21\u6001\u4e0d\u4e00\u81f4\u3001\u65f6\u95f4\u6a21\u5f0f\u53d8\u5316\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u4e8b\u4ef6\u7ea7\u7ed3\u6784\u3002", "method": "E-CaTCH\u901a\u8fc7\u805a\u7c7b\u4f2a\u4e8b\u4ef6\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5bf9\u9f50\u548c\u65f6\u95f4\u611f\u77e5LSTM\u5efa\u6a21\u4e8b\u4ef6\u52a8\u6001\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7c7b\u522b\u6743\u91cd\u548c\u786c\u6837\u672c\u6316\u6398\u4f18\u5316\u5b66\u4e60\u3002", "result": "\u5728Fakeddit\u3001IND\u548cCOVID-19 MISINFOGRAPH\u6570\u636e\u96c6\u4e0a\uff0cE-CaTCH\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6570\u636e\u96c6\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "E-CaTCH\u901a\u8fc7\u4e8b\u4ef6\u7ea7\u5efa\u6a21\u548c\u591a\u6a21\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.11092", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11092", "abs": "https://arxiv.org/abs/2508.11092", "authors": ["Cindy Shih-Ting Huang", "Clarence Boon Liang Ng", "Marek Rei"], "title": "Predictive Multimodal Modeling of Diagnoses and Treatments in EHR", "comment": "10 pages, 1 figure", "summary": "While the ICD code assignment problem has been widely studied, most works\nhave focused on post-discharge document classification. Models for early\nforecasting of this information could be used for identifying health risks,\nsuggesting effective treatments, or optimizing resource allocation. To address\nthe challenge of predictive modeling using the limited information at the\nbeginning of a patient stay, we propose a multimodal system to fuse clinical\nnotes and tabular events captured in electronic health records. The model\nintegrates pre-trained encoders, feature pooling, and cross-modal attention to\nlearn optimal representations across modalities and balance their presence at\nevery temporal point. Moreover, we present a weighted temporal loss that\nadjusts its contribution at each point in time. Experiments show that these\nstrategies enhance the early prediction model, outperforming the current\nstate-of-the-art systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e34\u5e8a\u8bb0\u5f55\u548c\u8868\u683c\u6570\u636e\uff0c\u7528\u4e8e\u65e9\u671f\u9884\u6d4bICD\u7f16\u7801\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65e9\u671f\u9884\u6d4bICD\u7f16\u7801\u6709\u52a9\u4e8e\u8bc6\u522b\u5065\u5eb7\u98ce\u9669\u3001\u4f18\u5316\u6cbb\u7597\u548c\u8d44\u6e90\u5206\u914d\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u7279\u5f81\u6c60\u5316\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff0c\u7ed3\u5408\u52a0\u6743\u65f6\u95f4\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7cfb\u7edf\u3002", "conclusion": "\u591a\u6a21\u6001\u7cfb\u7edf\u548c\u52a0\u6743\u65f6\u95f4\u635f\u5931\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u65e9\u671f\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.11190", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2508.11190", "abs": "https://arxiv.org/abs/2508.11190", "authors": ["Feng-ao Wang", "Shaobo Chen", "Yao Xuan", "Junwei Liu", "Qi Gao", "Hongdong Zhu", "Junjie Hou", "Lixin Yuan", "Jinyu Cheng", "Chenxin Yi", "Hai Wei", "Yin Ma", "Tao Xu", "Kai Wen", "Yixue Li"], "title": "Quantum-Boosted High-Fidelity Deep Learning", "comment": null, "summary": "A fundamental limitation of probabilistic deep learning is its predominant\nreliance on Gaussian priors. This simplistic assumption prevents models from\naccurately capturing the complex, non-Gaussian landscapes of natural data,\nparticularly in demanding domains like complex biological data, severely\nhindering the fidelity of the model for scientific discovery. The\nphysically-grounded Boltzmann distribution offers a more expressive\nalternative, but it is computationally intractable on classical computers. To\ndate, quantum approaches have been hampered by the insufficient qubit scale and\noperational stability required for the iterative demands of deep learning.\nHere, we bridge this gap by introducing the Quantum Boltzmann\nMachine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable\nhybrid quantum-classical architecture. Our framework leverages a quantum\nprocessor for efficient sampling from the Boltzmann distribution, enabling its\nuse as a powerful prior within a deep generative model. Applied to\nmillion-scale single-cell datasets from multiple sources, the QBM-VAE generates\na latent space that better preserves complex biological structures,\nconsistently outperforming conventional Gaussian-based deep learning models\nlike VAE and SCVI in essential tasks such as omics data integration, cell-type\nclassification, and trajectory inference. It also provides a typical example of\nintroducing a physics priori into deep learning to drive the model to acquire\nscientific discovery capabilities that breaks through data limitations. This\nwork provides the demonstration of a practical quantum advantage in deep\nlearning on a large-scale scientific problem and offers a transferable\nblueprint for developing hybrid quantum AI models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784QBM-VAE\uff0c\u7528\u4e8e\u89e3\u51b3\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5bf9\u9ad8\u65af\u5148\u9a8c\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5728\u751f\u7269\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u4f9d\u8d56\u9ad8\u65af\u5148\u9a8c\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u590d\u6742\u975e\u9ad8\u65af\u6570\u636e\uff08\u5982\u751f\u7269\u6570\u636e\uff09\u7684\u5206\u5e03\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u79d1\u5b66\u53d1\u73b0\u80fd\u529b\u3002", "method": "\u5f15\u5165QBM-VAE\u67b6\u6784\uff0c\u5229\u7528\u91cf\u5b50\u5904\u7406\u5668\u9ad8\u6548\u91c7\u6837\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u4f5c\u4e3a\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u5148\u9a8c\u3002", "result": "\u5728\u767e\u4e07\u7ea7\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\uff0cQBM-VAE\u751f\u6210\u7684\u6f5c\u5728\u7a7a\u95f4\u80fd\u66f4\u597d\u4fdd\u7559\u590d\u6742\u751f\u7269\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u9ad8\u65af\u6a21\u578b\uff08\u5982VAE\u548cSCVI\uff09\u3002", "conclusion": "QBM-VAE\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u4f18\u52bf\uff0c\u5e76\u4e3a\u5f00\u53d1\u6df7\u5408\u91cf\u5b50AI\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u84dd\u56fe\u3002"}}
{"id": "2508.11115", "categories": ["cs.CV", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11115", "abs": "https://arxiv.org/abs/2508.11115", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.", "AI": {"tldr": "UWB-PostureGuard\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u5bbd\u5e26\uff08UWB\uff09\u7684\u65e0\u63a5\u89e6\u5750\u59ff\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u548cPoseGBDT\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76d1\u6d4b\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u548c\u8212\u9002\u6027\u95ee\u9898\u3002", "motivation": "\u957f\u65f6\u95f4\u4f7f\u7528\u7535\u8111\u65f6\u7684\u4e0d\u826f\u5750\u59ff\u5df2\u6210\u4e3a\u516c\u5171\u5065\u5eb7\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u548c\u8212\u9002\u6027\u969c\u788d\u3002", "method": "\u5229\u7528\u5546\u7528UWB\u8bbe\u5907\u63d0\u53d6\u5750\u59ff\u7279\u5f81\uff0c\u5f00\u53d1PoseGBDT\u6a21\u578b\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u572810\u540d\u53c2\u4e0e\u8005\u548c19\u79cd\u59ff\u52bf\u7684\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u8fbe\u523099.11%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5bf9\u73af\u5883\u53d8\u91cf\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "UWB-PostureGuard\u4e3a\u4f4e\u6210\u672c\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u79fb\u52a8\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4e3b\u52a8\u6539\u5584\u5750\u59ff\u7ba1\u7406\u3002"}}
{"id": "2508.11258", "categories": ["cs.LG", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11258", "abs": "https://arxiv.org/abs/2508.11258", "authors": ["Ruicheng Xian", "Yuxuan Wan", "Han Zhao"], "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing", "comment": null, "summary": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63d0\u793a\u8bbe\u8ba1\u4ece\u5c01\u95ed\u6743\u91cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u63d0\u53d6\u516c\u5e73\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u7684\u516c\u5e73\u6027\u8981\u6c42\u573a\u666f\u3002", "motivation": "\u968f\u7740\u6307\u4ee4\u5fae\u8c03LLM\u7684\u666e\u53ca\uff0c\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u9700\u8981\u786e\u4fdd\u7fa4\u4f53\u516c\u5e73\u6027\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u5c01\u95ed\u6743\u91cdLLM\u3002", "method": "\u5c06LLM\u89c6\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u8bbe\u8ba1\u7684\u63d0\u793a\u83b7\u53d6\u6982\u7387\u9884\u6d4b\u7279\u5f81\uff0c\u518d\u5e94\u7528\u516c\u5e73\u7b97\u6cd5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e14\u6570\u636e\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5c01\u95ed\u6743\u91cdLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u516c\u5e73\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2508.11434", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11434", "abs": "https://arxiv.org/abs/2508.11434", "authors": ["Aditi Dutta", "Susan Banducci"], "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "comment": null, "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533a\u5206\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u4e0e\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u65f6\u7684\u56f0\u96be\uff0c\u53d1\u73b0\u6a21\u578b\u5e38\u5c06\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u8bef\u5224\u4e3a\u6709\u5bb3\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u654f\u611f\u4e8b\u4ef6\u4e2d\u3002", "motivation": "\u81ea\u52a8\u5316\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u53ef\u80fd\u8bef\u5224\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\uff0c\u5bfc\u81f4\u6311\u6218\u6027\u522b\u6b67\u89c6\u7684\u58f0\u97f3\u88ab\u538b\u5236\uff0c\u5c24\u5176\u662f\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u58f0\u97f3\u3002", "method": "\u5206\u6790\u4e86\u4e94\u79cdLLMs\u5bf9\u82f1\u56fd2022\u5e74\u6d89\u53ca\u5973\u6027\u8bae\u5458\u7684\u63a8\u6587\uff08\u6027\u522b\u6b67\u89c6\u3001\u53cd\u6027\u522b\u6b67\u89c6\u548c\u4e2d\u7acb\uff09\u7684\u5206\u7c7b\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5e38\u5c06\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u8bef\u5224\u4e3a\u6709\u5bb3\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u654f\u611f\u4e8b\u4ef6\u4e2d\uff0c\u4fee\u8f9e\u98ce\u683c\u76f8\u4f3c\u65f6\u3002", "conclusion": "\u5efa\u8bae\u5ba1\u6838\u8bbe\u8ba1\u8d85\u8d8a\u4e8c\u5143\u5206\u7c7b\uff0c\u6574\u5408\u4eba\u5de5\u5ba1\u6838\uff0c\u5e76\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u660e\u786e\u5305\u542b\u53cd\u8a00\u8bba\uff0c\u4ee5\u4fdd\u62a4\u6570\u5b57\u653f\u6cbb\u7a7a\u95f4\u4e2d\u7684\u62b5\u6297\u8a00\u8bba\u3002"}}
{"id": "2508.11442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11442", "abs": "https://arxiv.org/abs/2508.11442", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "AI": {"tldr": "CoDiEmb\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u4e13\u7528\u76ee\u6807\u3001\u52a8\u6001\u91c7\u6837\u5668\u548c\u6a21\u578b\u878d\u5408\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\uff08STS\uff09\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "motivation": "\u8054\u5408\u8bad\u7ec3IR\u548cSTS\u4efb\u52a1\u65f6\uff0c\u8d1f\u8fc1\u79fb\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u89e3\u8026\u4efb\u52a1\u7279\u5b9a\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "CoDiEmb\u91c7\u7528\u4efb\u52a1\u4e13\u7528\u76ee\u6807\u3001\u52a8\u6001\u91c7\u6837\u5668\u548cdelta\u5f15\u5bfc\u7684\u6a21\u578b\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u8054\u5408\u4f18\u5316\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0cCoDiEmb\u4e0d\u4ec5\u7f13\u89e3\u4e86\u8de8\u4efb\u52a1\u6743\u8861\uff0c\u8fd8\u63d0\u5347\u4e86\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\u3002", "conclusion": "CoDiEmb\u4e3a\u7edf\u4e00\u6587\u672c\u5d4c\u5165\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2508.11173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11173", "abs": "https://arxiv.org/abs/2508.11173", "authors": ["Ruobing Jiang", "Yang Liu", "Haobing Liu", "Yanwei Yu", "Chunyang Wang"], "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery", "comment": "Accepted by CIKM 2025. 10 pages, 5 figures,", "summary": "Continuous category discovery (CCD) aims to automatically discover novel\ncategories in continuously arriving unlabeled data. This is a challenging\nproblem considering that there is no number of categories and labels in the\nnewly arrived data, while also needing to mitigate catastrophic forgetting.\nMost CCD methods cannot handle the contradiction between novel class discovery\nand classification well. They are also prone to accumulate errors in the\nprocess of gradually discovering novel classes. Moreover, most of them use\nknowledge distillation and data replay to prevent forgetting, occupying more\nstorage space. To address these limitations, we propose Independence-based\nDiversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes\nindependent enrichment of diversity module, joint discovery of novelty module,\nand continuous increment by orthogonality module. In independent enrichment,\nthe backbone is trained separately using contrastive loss to avoid it focusing\nonly on features for classification. Joint discovery transforms multi-stage\nnovel class discovery into single-stage, reducing error accumulation impact.\nContinuous increment by orthogonality module generates mutually orthogonal\nprototypes for classification and prevents forgetting with lower space overhead\nvia representative representation replay. Experimental results show that on\nchallenging fine-grained datasets, our method outperforms the state-of-the-art\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIDOD\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fde\u7eed\u7c7b\u522b\u53d1\u73b0\uff08CCD\uff09\u4e2d\u7684\u65b0\u7c7b\u522b\u53d1\u73b0\u4e0e\u5206\u7c7b\u77db\u76fe\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u9519\u8bef\u79ef\u7d2f\u548c\u5b58\u50a8\u5360\u7528\u3002", "motivation": "CCD\u9762\u4e34\u65b0\u6570\u636e\u65e0\u6807\u7b7e\u3001\u7c7b\u522b\u6570\u91cf\u672a\u77e5\u4ee5\u53ca\u707e\u96be\u6027\u9057\u5fd8\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u65b0\u7c7b\u522b\u53d1\u73b0\u4e0e\u5206\u7c7b\uff0c\u4e14\u6613\u79ef\u7d2f\u9519\u8bef\u3002", "method": "IDOD\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u72ec\u7acb\u591a\u6837\u6027\u589e\u5f3a\u6a21\u5757\u3001\u8054\u5408\u65b0\u7c7b\u522b\u53d1\u73b0\u6a21\u5757\u548c\u6b63\u4ea4\u6027\u589e\u91cf\u6a21\u5757\uff0c\u5206\u522b\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u3001\u5355\u9636\u6bb5\u53d1\u73b0\u548c\u6b63\u4ea4\u539f\u578b\u51cf\u5c11\u9519\u8bef\u548c\u5b58\u50a8\u9700\u6c42\u3002", "result": "\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0cIDOD\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IDOD\u6709\u6548\u89e3\u51b3\u4e86CCD\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.11534", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11534", "abs": "https://arxiv.org/abs/2508.11534", "authors": ["Monika Jotautait\u0117", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u8868\u73b0\u51fa\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\u53ca\u5176\u5bf9\u975e\u4eba\u7c7b\u52a8\u7269\u7684\u8bc4\u4ef7\uff0c\u53d1\u73b0LLMs\u80fd\u8bc6\u522b\u7269\u79cd\u4e3b\u4e49\u8a00\u8bba\u4f46\u5f88\u5c11\u8c34\u8d23\uff0c\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u5148\u4eba\u7c7b\u800c\u975e\u52a8\u7269\u3002", "motivation": "\u968f\u7740LLMs\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8bc4\u4f30\u5176\u4f26\u7406\u503e\u5411\uff0c\u7279\u522b\u662f\u5bf9\u975e\u4eba\u7c7b\u52a8\u7269\u7684\u6b67\u89c6\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u8303\u5f0f\u7814\u7a76\uff1aSpeciesismBench\u57fa\u51c6\u6d4b\u8bd5\u3001\u5fc3\u7406\u5b66\u6d4b\u91cf\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u6a21\u578b\u53cd\u5e94\u3001\u6587\u672c\u751f\u6210\u4efb\u52a1\u3002", "result": "LLMs\u80fd\u8bc6\u522b\u7269\u79cd\u4e3b\u4e49\u8a00\u8bba\u4f46\u5f88\u5c11\u8c34\u8d23\uff0c\u5728\u6743\u8861\u4e2d\u66f4\u503e\u5411\u4e8e\u4eba\u7c7b\uff0c\u4f46\u82e5\u52a8\u7269\u80fd\u529b\u66f4\u5f3a\u5219\u4f18\u5148\u52a8\u7269\u3002", "conclusion": "\u9700\u6269\u5c55AI\u516c\u5e73\u6027\u6846\u67b6\u4ee5\u5305\u542b\u975e\u4eba\u7c7b\u9053\u5fb7\u4e3b\u4f53\uff0c\u51cf\u5c11\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\u3002"}}
{"id": "2508.11576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11576", "abs": "https://arxiv.org/abs/2508.11576", "authors": ["Yumeng Shi", "Quanyu Long", "Yin Wu", "Wenya Wang"], "title": "Causality Matters: How Temporal Information Emerges in Video Language Models", "comment": null, "summary": "Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u53bb\u9664\u6216\u4fee\u6539\u4f4d\u7f6e\u7f16\u7801\u5bf9\u65f6\u95f4\u7406\u89e3\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u53cd\u8f6c\u5e27\u5e8f\u5217\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\u3002\u65f6\u95f4\u4fe1\u606f\u901a\u8fc7\u5e27\u95f4\u6ce8\u610f\u529b\u9010\u6b65\u5408\u6210\uff0c\u6700\u7ec8\u6574\u5408\u5230\u67e5\u8be2\u4ee4\u724c\u4e2d\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6548\u7387\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u65f6\u95f4\u7406\u89e3\u7684\u673a\u5236\uff0c\u5c24\u5176\u662f\u4f4d\u7f6e\u7f16\u7801\u7684\u4f5c\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b9e\u9a8c\u63ed\u793a\u65f6\u95f4\u4fe1\u606f\u7684\u6574\u5408\u8def\u5f84\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u6548\u7387\u4f18\u5316\u7b56\u7565\uff1a\u5206\u9636\u6bb5\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u9000\u51fa\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e24\u79cd\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u63a8\u7406\u6e90\u4e8e\u5e27\u95f4\u6ce8\u610f\u529b\u4ea4\u4e92\u3002", "conclusion": "\u65f6\u95f4\u7406\u89e3\u4f9d\u8d56\u4e8e\u5e27\u95f4\u6ce8\u610f\u529b\u4ea4\u4e92\uff0c\u800c\u975e\u663e\u5f0f\u4f4d\u7f6e\u7f16\u7801\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
