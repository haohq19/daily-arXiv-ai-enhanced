{"id": "2509.06974", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06974", "abs": "https://arxiv.org/abs/2509.06974", "authors": ["Xueyi Wang", "Elisabeth Wilhelm"], "title": "Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model", "comment": null, "summary": "Sleep quality significantly impacts well-being. Therefore, healthcare\nproviders and individuals need accessible and reliable forecasting tools for\npreventive interventions. This paper introduces an interpretable,\nindividualized two-stage adaptive spatial-temporal model for predicting sleep\nquality scores. Our proposed framework combines multi-scale convolutional\nlayers to model spatial interactions across multiple input variables, recurrent\nlayers and attention mechanisms to capture long-term temporal dependencies, and\na two-stage domain adaptation strategy to enhance generalization. The first\nadaptation stage is applied during training to mitigate overfitting on the\ntraining set. In the second stage, a source-free test-time adaptation mechanism\nis employed to adapt the model to new users without requiring labels. We\nconducted various experiments with five input window sizes (3, 5, 7, 9, and 11\ndays) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model\nconsistently outperformed time series forecasting baseline approaches,\nincluding Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The\nbest performance was achieved with a three-day input window and a one-day\nprediction window, yielding a root mean square error (RMSE) of 0.216.\nFurthermore, the model demonstrated good predictive performance even for longer\nforecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction\nwindow), highlighting its practical utility for real-world applications. We\nalso conducted an explainability analysis to examine how different features\ninfluence sleep quality. These findings proved that the proposed framework\noffers a robust, adaptive, and explainable solution for personalized sleep\nforecasting using sparse data from commercial wearable devices.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u4e24\u9636\u6bb5\u9002\u914d\u6027\u7a7a\u95f4-\u65f6\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7761\u7720\u8d28\u91cf\u5206\u6570\uff0c\u5728\u591a\u79cd\u9884\u6d4b\u7a97\u53e3\u4e0b\u90fd\u8d85\u8fc7\u4e86\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u4f73RMSE\u4e3a0.216\u3002", "motivation": "\u7761\u7720\u8d28\u91cf\u5f71\u54cd\u5065\u5eb7\u72b6\u51b5\uff0c\u9700\u8981\u53ef\u8bbf\u95ee\u548c\u53ef\u9760\u7684\u9884\u6d4b\u5de5\u5177\u6765\u8fdb\u884c\u9884\u9632\u6027\u5e72\u9884\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u4e2a\u4f53\u5316\u4e24\u9636\u6bb5\u9002\u914d\u6027\u7a7a\u95f4-\u65f6\u95f4\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u5377\u79ef\u5c42\u6a21\u62df\u591a\u91cd\u8f93\u5165\u53d8\u91cf\u7684\u7a7a\u95f4\u4ea4\u4e92\u4f5c\u7528\uff0c\u9012\u5f52\u5c42\u548c\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u4e24\u9636\u6bb5\u57df\u9002\u914d\u7b56\u7565\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u666e\u904d\u6027\u3002", "result": "\u6a21\u578b\u57285\u79cd\u8f93\u5165\u7a97\u53e3\u5927\u5c0f\uff083\u30015\u30017\u30019\u300111\u5929\uff09\u548c5\u79cd\u9884\u6d4b\u7a97\u53e3\u5927\u5c0f\uff081\u30013\u30015\u30017\u30019\u5929\uff09\u7684\u5b9e\u9a8c\u4e2d\u90fd\u8d85\u8fc7\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u7ebf\u65b9\u6cd5\uff08LSTM\u3001Informer\u3001PatchTST\u3001TimesNet\uff09\u3002\u6700\u4f73\u6027\u80fd\u662f\u4f7f\u75283\u5929\u8f93\u5165\u7a97\u53e3\u548c1\u5929\u9884\u6d4b\u7a97\u53e3\uff0cRMSE\u4e3a0.216\u3002\u751a\u81f3\u5728\u66f4\u957f\u7684\u9884\u6d4b\u8ddd\u79bb\u4e0b\u4e5f\u8868\u73b0\u826f\u597d\uff08\u4f8b\u59823\u5929\u9884\u6d4b\u7a97\u53e3\u7684RMSE\u4e3a0.257\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f7f\u7528\u5546\u4e1a\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u7a00\u758f\u6570\u636e\u8fdb\u884c\u4e2a\u6027\u5316\u7761\u7720\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u3001\u9002\u914d\u6027\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07412", "abs": "https://arxiv.org/abs/2509.07412", "authors": ["Zhen Tian", "Fujiang Yuan", "Yangfan He", "Qinghao Li", "Changlin Chen", "Huilin Chen", "Tianxiang Xu", "Jianyu Duan", "Yanhong Peng", "Zhihao Lin"], "title": "Attention and Risk-Aware Decision Framework for Safe Autonomous Driving", "comment": null, "summary": "Autonomous driving has attracted great interest due to its potential\ncapability in full-unsupervised driving. Model-based and learning-based methods\nare widely used in autonomous driving. Model-based methods rely on pre-defined\nmodels of the environment and may struggle with unforeseen events. Proximal\npolicy optimization (PPO), an advanced learning-based method, can adapt to the\nabove limits by learning from interactions with the environment. However,\nexisting PPO faces challenges with poor training results, and low training\nefficiency in long sequences. Moreover, the poor training results are\nequivalent to collisions in driving tasks. To solve these issues, this paper\ndevelops an improved PPO by introducing the risk-aware mechanism, a\nrisk-attention decision network, a balanced reward function, and a\nsafety-assisted mechanism. The risk-aware mechanism focuses on highlighting\nareas with potential collisions, facilitating safe-driving learning of the PPO.\nThe balanced reward function adjusts rewards based on the number of surrounding\nvehicles, promoting efficient exploration of the control strategy during\ntraining. Additionally, the risk-attention network enhances the PPO to hold\nchannel and spatial attention for the high-risk areas of input images.\nMoreover, the safety-assisted mechanism supervises and prevents the actions\nwith risks of collisions during the lane keeping and lane changing. Simulation\nresults on a physical engine demonstrate that the proposed algorithm\noutperforms benchmark algorithms in collision avoidance, achieving higher peak\nreward with less training time, and shorter driving time remaining on the risky\nareas among multiple testing traffic flow scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684PPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u98ce\u9669\u611f\u77e5\u673a\u5236\u3001\u98ce\u9669\u6ce8\u610f\u529b\u51b3\u7b56\u7f51\u7edc\u3001\u5e73\u8861\u5956\u52b1\u51fd\u6570\u548c\u5b89\u5168\u8f85\u52a9\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2dPPO\u7b97\u6cd5\u8bad\u7ec3\u6548\u679c\u5dee\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u610f\u5916\u4e8b\u4ef6\uff0c\u800c\u73b0\u6709\u7684PPO\u7b97\u6cd5\u5728\u957f\u5e8f\u5217\u8bad\u7ec3\u4e2d\u5b58\u5728\u8bad\u7ec3\u6548\u679c\u5dee\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u8bad\u7ec3\u6548\u679c\u5dee\u7b49\u540c\u4e8e\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u78b0\u649e\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86\u6539\u8fdb\u7684PPO\u7b97\u6cd5\uff0c\u5305\u542b\uff1a1)\u98ce\u9669\u611f\u77e5\u673a\u5236\u7a81\u51fa\u6f5c\u5728\u78b0\u649e\u533a\u57df\uff1b2)\u5e73\u8861\u5956\u52b1\u51fd\u6570\u6839\u636e\u5468\u56f4\u8f66\u8f86\u6570\u91cf\u8c03\u6574\u5956\u52b1\uff1b3)\u98ce\u9669\u6ce8\u610f\u529b\u7f51\u7edc\u589e\u5f3a\u5bf9\u9ad8\u98ce\u9669\u533a\u57df\u7684\u5173\u6ce8\uff1b4)\u5b89\u5168\u8f85\u52a9\u673a\u5236\u76d1\u7763\u548c\u9632\u6b62\u5371\u9669\u52a8\u4f5c\u3002", "result": "\u5728\u7269\u7406\u5f15\u64ce\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u907f\u78b0\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5cf0\u503c\u5956\u52b1\u3001\u66f4\u5c11\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u4ee5\u53ca\u5728\u591a\u79cd\u6d4b\u8bd5\u4ea4\u901a\u6d41\u573a\u666f\u4e2d\u5728\u98ce\u9669\u533a\u57df\u505c\u7559\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdbPPO\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u89e3\u51b3PPO\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07324", "abs": "https://arxiv.org/abs/2509.07324", "authors": ["Nakyung Lee", "Yeongoon Kim", "Minhae Oh", "Suhwan Kim", "Jin Woo Koo", "Hyewon Jo", "Jungwoo Lee"], "title": "Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation", "comment": "Accepted at EMNLP 2025", "summary": "Transformer-based self-attention mechanism serves as the core of modern\nlanguage models, yet it often suffers from localization, where attentions\ncollapse onto a limited subset of tokens and fail to capture long-range\ndependencies. To address this issue, we propose Self-Attention One-step Belief\nPropagation (SAOBP), a refinement framework that injects multi-hop\nrelationships through a belief propagation process. To interpret and quantify\nthese interactions, we introduce Global Token Dependency (GTD) that captures\nthe relative contribution of multihop connections within the attention graph.\nEmpirical results indicate that SAOBP helps prevent entropy collapse in deeper\nlayers and adaptively maintains GTD at task-appropriate levels, thereby\nsupporting improvements in model performance. Importantly, we observe\ncompetitive gains in small-scale models, highlighting its potential for\nimproving inference quality in resource-constrained scenarios.", "AI": {"tldr": "\u901a\u8fc7\u4fe1\u5ff1\u4f20\u64ad\u8fc7\u7a0b\u6ce8\u5165\u591a\u8df3\u5173\u7cfb\uff0cSAOBP\u6846\u67b6\u89e3\u51b3\u4e86Transformer\u81ea\u6ce8\u610f\u529b\u7684\u5c40\u90e8\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf", "motivation": "\u89e3\u51b3Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u7684\u5c40\u90e8\u5316\u95ee\u9898\uff0c\u5373\u6ce8\u610f\u529b\u5938\u7f29\u5230\u5c11\u6570token\u4e0a\u800c\u65e0\u6cd5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb", "method": "\u63d0\u51faSelf-Attention One-step Belief Propagation (SAOBP)\u7cbe\u70bc\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u5ff1\u4f20\u64ad\u8fc7\u7a0b\u6ce8\u5165\u591a\u8df3\u5173\u7cfb\uff0c\u5e76\u5f15\u5165Global Token Dependency (GTD)\u6765\u91cf\u5316\u591a\u8df3\u8fde\u63a5\u7684\u76f8\u5bf9\u8d21\u732e", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSAOBP\u80fd\u591f\u963b\u6b62\u6df1\u5c42\u7684\u71b5\u589e\u5938\u7f29\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u9700\u6c42\u81ea\u9002\u5e94\u5730\u7ef4\u6301GTD\u6c34\u5e73\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u4e5f\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7684\u6536\u76ca", "conclusion": "SAOBP\u6846\u67b6\u901a\u8fc7\u4fe1\u5ff1\u4f20\u64ad\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u6ce8\u610f\u529b\u7684\u5c40\u90e8\u5316\u95ee\u9898\uff0c\u7279\u522b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u663e\u793a\u51fa\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u6536\u96c6\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2509.07019", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07019", "abs": "https://arxiv.org/abs/2509.07019", "authors": ["Xinquan Wu", "Xuefeng Yan", "Mingqiang Wei", "Donghai Guan"], "title": "An efficient deep reinforcement learning environment for flexible job-shop scheduling", "comment": null, "summary": "The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial\noptimization problem that has a wide-range of applications in the real world.\nIn order to generate fast and accurate scheduling solutions for FJSP, various\ndeep reinforcement learning (DRL) scheduling methods have been developed.\nHowever, these methods are mainly focused on the design of DRL scheduling\nAgent, overlooking the modeling of DRL environment. This paper presents a\nsimple chronological DRL environment for FJSP based on discrete event\nsimulation and an end-to-end DRL scheduling model is proposed based on the\nproximal policy optimization (PPO). Furthermore, a short novel state\nrepresentation of FJSP is proposed based on two state variables in the\nscheduling environment and a novel comprehensible reward function is designed\nbased on the scheduling area of machines. Experimental results on public\nbenchmark instances show that the performance of simple priority dispatching\nrules (PDR) is improved in our scheduling environment and our DRL scheduling\nmodel obtains competing performance compared with OR-Tools, meta-heuristic, DRL\nand PDR scheduling methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u7684\u7b80\u5355\u65f6\u5e8fDRL\u73af\u5883\u7528\u4e8e\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff0c\u5e76\u57fa\u4e8ePPO\u7b97\u6cd5\u6784\u5efa\u7aef\u5230\u7aefDRL\u8c03\u5ea6\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u7684\u72b6\u6001\u8868\u793a\u548c\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8c03\u5ea6\u667a\u80fd\u4f53\u7684\u8bbe\u8ba1\uff0c\u800c\u5ffd\u89c6\u4e86DRL\u73af\u5883\u7684\u5efa\u6a21\uff0c\u9700\u8981\u4e3aFJSP\u95ee\u9898\u5f00\u53d1\u66f4\u6709\u6548\u7684\u73af\u5883\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u6784\u5efa\u65f6\u5e8fDRL\u73af\u5883\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u5efa\u7acb\u7aef\u5230\u7aef\u8c03\u5ea6\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e24\u4e2a\u72b6\u6001\u53d8\u91cf\u7684\u77ed\u72b6\u6001\u8868\u793a\u548c\u57fa\u4e8e\u673a\u5668\u8c03\u5ea6\u533a\u57df\u7684\u53ef\u7406\u89e3\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u5b9e\u4f8b\u4e2d\uff0c\u7b80\u5355\u4f18\u5148\u7ea7\u8c03\u5ea6\u89c4\u5219\u5728\u8be5\u73af\u5883\u4e2d\u7684\u6027\u80fd\u5f97\u5230\u63d0\u5347\uff0cDRL\u8c03\u5ea6\u6a21\u578b\u76f8\u6bd4OR-Tools\u3001\u5143\u542f\u53d1\u5f0f\u3001DRL\u548cPDR\u65b9\u6cd5\u83b7\u5f97\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b80\u5355\u65f6\u5e8fDRL\u73af\u5883\u548c\u7aef\u5230\u7aef\u8c03\u5ea6\u6a21\u578b\u4e3aFJSP\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7b80\u5355\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u8c03\u5ea6\u6027\u80fd\u3002"}}
{"id": "2509.07464", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.07464", "abs": "https://arxiv.org/abs/2509.07464", "authors": ["Rui Yang", "Lei Zheng", "Shuzhi Sam Ge", "Jun Ma"], "title": "Safe and Non-Conservative Contingency Planning for Autonomous Vehicles via Online Learning-Based Reachable Set Barriers", "comment": "16 pages, 13 figures", "summary": "Autonomous vehicles must navigate dynamically uncertain environments while\nbalancing the safety and driving efficiency. This challenge is exacerbated by\nthe unpredictable nature of surrounding human-driven vehicles (HVs) and\nperception inaccuracies, which require planners to adapt to evolving\nuncertainties while maintaining safe trajectories. Overly conservative planners\ndegrade driving efficiency, while deterministic approaches may encounter\nserious issues and risks of failure when faced with sudden and unexpected\nmaneuvers. To address these issues, we propose a real-time contingency\ntrajectory optimization framework in this paper. By employing event-triggered\nonline learning of HV control-intent sets, our method dynamically quantifies\nmulti-modal HV uncertainties and refines the forward reachable set (FRS)\nincrementally. Crucially, we enforce invariant safety through FRS-based barrier\nconstraints that ensure safety without reliance on accurate trajectory\nprediction of HVs. These constraints are embedded in contingency trajectory\noptimization and solved efficiently through consensus alternative direction\nmethod of multipliers (ADMM). The system continuously adapts to the\nuncertainties in HV behaviors, preserving feasibility and safety without\nresorting to excessive conservatism. High-fidelity simulations on highway and\nurban scenarios, as well as a series of real-world experiments demonstrate\nsignificant improvements in driving efficiency and passenger comfort while\nmaintaining safety under uncertainty. The project page is available at\nhttps://pathetiue.github.io/frscp.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u5b9e\u65f6\u5e94\u6025\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u89e6\u53d1\u5728\u7ebf\u5b66\u4e60\u91cf\u5316\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528FRS\u5c4f\u969c\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u7684\u540c\u65f6\u63d0\u5347\u9a7e\u9a76\u6548\u7387\u548c\u8212\u9002\u6027", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5e73\u8861\u5b89\u5168\u6027\u548c\u9a7e\u9a76\u6548\u7387\uff0c\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\u5f71\u54cd\u6548\u7387\uff0c\u8981\u4e48\u786e\u5b9a\u6027\u65b9\u6cd5\u5728\u9047\u5230\u7a81\u53d1\u60c5\u51b5\u65f6\u5b58\u5728\u5b89\u5168\u98ce\u9669", "method": "\u91c7\u7528\u4e8b\u4ef6\u89e6\u53d1\u5728\u7ebf\u5b66\u4e60HV\u63a7\u5236\u610f\u56fe\u96c6\uff0c\u52a8\u6001\u91cf\u5316\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7FRS\u5c4f\u969c\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\u4e0d\u53d8\u6027\uff0c\u4f7f\u7528\u5171\u8bc6ADMM\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3\u5e94\u6025\u8f68\u8ff9\u4f18\u5316\u95ee\u9898", "result": "\u5728\u9ad8\u901f\u516c\u8def\u548c\u57ce\u5e02\u573a\u666f\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u6548\u7387\u548c\u4e58\u5ba2\u8212\u9002\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b89\u5168\u6027", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6301\u7eed\u9002\u5e94HV\u884c\u4e3a\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4e0d\u727a\u7272\u5b89\u5168\u6027\u7684\u524d\u63d0\u4e0b\u907f\u514d\u8fc7\u5ea6\u4fdd\u5b88\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07027", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07027", "abs": "https://arxiv.org/abs/2509.07027", "authors": ["Jisung Hwang", "Jaihoon Kim", "Minhyuk Sung"], "title": "Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models", "comment": "Submitted to NeurIPS 2025", "summary": "We propose a novel regularization loss that enforces standard Gaussianity,\nencouraging samples to align with a standard Gaussian distribution. This\nfacilitates a range of downstream tasks involving optimization in the latent\nspace of text-to-image models. We treat elements of a high-dimensional sample\nas one-dimensional standard Gaussian variables and define a composite loss that\ncombines moment-based regularization in the spatial domain with power\nspectrum-based regularization in the spectral domain. Since the expected values\nof moments and power spectrum distributions are analytically known, the loss\npromotes conformity to these properties. To ensure permutation invariance, the\nlosses are applied to randomly permuted inputs. Notably, existing\nGaussianity-based regularizations fall within our unified framework: some\ncorrespond to moment losses of specific orders, while the previous\ncovariance-matching loss is equivalent to our spectral loss but incurs higher\ntime complexity due to its spatial-domain computation. We showcase the\napplication of our regularization in generative modeling for test-time reward\nalignment with a text-to-image model, specifically to enhance aesthetics and\ntext alignment. Our regularization outperforms previous Gaussianity\nregularization, effectively prevents reward hacking and accelerates\nconvergence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u7a7a\u95f4\u57df\u77e9\u6b63\u5219\u5316\u548c\u9891\u8c31\u57df\u529f\u7387\u8c31\u6b63\u5219\u5316\u7684\u7ec4\u5408\uff0c\u5f3a\u5236\u6f5c\u5728\u7a7a\u95f4\u6837\u672c\u7b26\u5408\u6807\u51c6\u9ad8\u65af\u5206\u5e03\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u6027\u6b63\u5219\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u6709\u6548\u7ea6\u675f\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\uff0c\u4ee5\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u4f18\u5316\u4efb\u52a1\uff0c\u5982\u7f8e\u5b66\u63d0\u5347\u548c\u6587\u672c\u5bf9\u9f50\u3002", "method": "\u5c06\u9ad8\u7ef4\u6837\u672c\u5143\u7d20\u89c6\u4e3a\u4e00\u7ef4\u6807\u51c6\u9ad8\u65af\u53d8\u91cf\uff0c\u7ed3\u5408\u7a7a\u95f4\u57df\u77e9\u6b63\u5219\u5316\u548c\u9891\u8c31\u57df\u529f\u7387\u8c31\u6b63\u5219\u5316\u6784\u5efa\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u968f\u673a\u6392\u5217\u786e\u4fdd\u6392\u5217\u4e0d\u53d8\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u9ad8\u65af\u6027\u6b63\u5219\u5316\uff0c\u6709\u6548\u9632\u6b62\u5956\u52b1\u7834\u89e3\uff0c\u52a0\u901f\u6536\u655b\uff0c\u5728\u6d4b\u8bd5\u65f6\u5956\u52b1\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u4e3a\u9ad8\u65af\u6027\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.07475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07475", "abs": "https://arxiv.org/abs/2509.07475", "authors": ["Saumya Goswami", "Siddharth Kurra"], "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention", "comment": null, "summary": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements.", "AI": {"tldr": "HALT-RAG\u662f\u4e00\u4e2a\u540e\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u4f7f\u7528NLI\u6a21\u578b\u548c\u8bcd\u6c47\u7279\u5f81\u6765\u68c0\u6d4bRAG\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u68c0\u6d4b\u751f\u6210\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u4e0e\u6e90\u6587\u672c\u77db\u76fe\u6216\u7f3a\u4e4f\u652f\u6301\u7684\u5185\u5bb9\uff0c\u5bf9\u4e8e\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7ba1\u9053\u4e2d\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u51bb\u7ed3\u7684\u73b0\u6210NLI\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u8bcd\u6c47\u4fe1\u53f7\u6784\u5efa\u901a\u7528\u7279\u5f81\u96c6\uff0c\u8bad\u7ec3\u7b80\u5355\u3001\u6821\u51c6\u4e14\u4efb\u52a1\u9002\u5e94\u7684\u5143\u5206\u7c7b\u5668\uff0c\u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u9632\u6b62\u6570\u636e\u6cc4\u9732\u3002", "result": "\u5728HaluEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHALT-RAG\u5728\u6458\u8981\u3001\u95ee\u7b54\u548c\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u5206\u522b\u83b7\u5f97\u4e860.7756\u30010.9786\u548c0.7391\u7684F1\u5206\u6570\uff0c\u5177\u6709\u826f\u597d\u7684\u6821\u51c6\u6982\u7387\u548c\u5b9e\u7528\u7684\u5f03\u6743\u673a\u5236\u3002", "conclusion": "HALT-RAG\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u4efb\u52a1\u9002\u5e94\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u5b89\u5168\u9700\u6c42\uff0c\u4e3aRAG\u7ba1\u9053\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2509.07050", "categories": ["cs.CV", "cs.AI", "cs.CY", "I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2509.07050", "abs": "https://arxiv.org/abs/2509.07050", "authors": ["Juan Manuel Contreras"], "title": "Automated Evaluation of Gender Bias Across 13 Large Multimodal Models", "comment": null, "summary": "Large multimodal models (LMMs) have revolutionized text-to-image generation,\nbut they risk perpetuating the harmful social biases in their training data.\nPrior work has identified gender bias in these models, but methodological\nlimitations prevented large-scale, comparable, cross-model analysis. To address\nthis gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for\nassessing social bias in AI-generated images. We test 13 commercially available\nLMMs using 75 procedurally-generated, gender-neutral prompts to generate people\nin stereotypically-male, stereotypically-female, and non-stereotypical\nprofessions. We then use a validated LLM-as-a-judge system to score the 965\nresulting images for gender representation. Our results reveal (p < .001 for\nall): 1) LMMs systematically not only reproduce but actually amplify\noccupational gender stereotypes relative to real-world labor data, generating\nmen in 93.0% of images for male-stereotyped professions but only 22.5% for\nfemale-stereotyped professions; 2) Models exhibit a strong default-male bias,\ngenerating men in 68.3% of the time for non-stereotyped professions; and 3) The\nextent of bias varies dramatically across models, with overall male\nrepresentation ranging from 46.7% to 73.3%. Notably, the top-performing model\nde-amplified gender stereotypes and approached gender parity, achieving the\nhighest fairness scores. This variation suggests high bias is not an inevitable\noutcome but a consequence of design choices. Our work provides the most\ncomprehensive cross-model benchmark of gender bias to date and underscores the\nnecessity of standardized, automated evaluation tools for promoting\naccountability and fairness in AI development.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86Aymara\u56fe\u50cf\u516c\u5e73\u6027\u8bc4\u4f30\u57fa\u51c6\uff0c\u6d4b\u8bd513\u4e2a\u5546\u4e1a\u5927\u6a21\u578b\u5728\u804c\u4e1a\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u7cfb\u7edf\u6027\u653e\u5927\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u5b58\u5728\u9ed8\u8ba4\u7537\u6027\u504f\u89c1\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u504f\u89c1\u7a0b\u5ea6\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5b58\u5728\u65b9\u6cd5\u5b66\u9650\u5236\uff0c\u65e0\u6cd5\u8fdb\u884c\u5927\u89c4\u6a21\u3001\u53ef\u6bd4\u8f83\u7684\u8de8\u6a21\u578b\u5206\u6790\uff0c\u9700\u8981\u5f00\u53d1\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u6765\u8bc6\u522b\u548c\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u95ee\u9898\u3002", "method": "\u4f7f\u752875\u4e2a\u7a0b\u5e8f\u751f\u6210\u7684\u6027\u522b\u4e2d\u6027\u63d0\u793a\u8bcd\uff0c\u6d4b\u8bd513\u4e2a\u5546\u4e1aLMM\u6a21\u578b\u5728\u523b\u677f\u5370\u8c61\u7537\u6027\u3001\u5973\u6027\u548c\u975e\u523b\u677f\u5370\u8c61\u804c\u4e1a\u4e2d\u751f\u6210\u4eba\u7269\u56fe\u50cf\uff0c\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684LLM-as-judge\u7cfb\u7edf\u5bf9965\u5f20\u56fe\u50cf\u8fdb\u884c\u6027\u522b\u8868\u5f81\u8bc4\u5206\u3002", "result": "\u6a21\u578b\u7cfb\u7edf\u6027\u653e\u5927\u804c\u4e1a\u6027\u522b\u523b\u677f\u5370\u8c61\uff08\u7537\u6027\u804c\u4e1a93.0%\u751f\u6210\u7537\u6027\uff0c\u5973\u6027\u804c\u4e1a\u4ec522.5%\u751f\u6210\u7537\u6027\uff09\uff0c\u5b58\u5728\u5f3a\u70c8\u9ed8\u8ba4\u7537\u6027\u504f\u89c1\uff08\u975e\u523b\u677f\u804c\u4e1a68.3%\u751f\u6210\u7537\u6027\uff09\uff0c\u4e0d\u540c\u6a21\u578b\u504f\u89c1\u7a0b\u5ea6\u5dee\u5f02\u663e\u8457\uff0846.7%-73.3%\uff09\u3002", "conclusion": "\u9ad8\u5ea6\u504f\u89c1\u4e0d\u662f\u5fc5\u7136\u7ed3\u679c\u800c\u662f\u8bbe\u8ba1\u9009\u62e9\u540e\u679c\uff0c\u6700\u4f73\u6a21\u578b\u53ef\u5b9e\u73b0\u6027\u522b\u5e73\u7b49\uff0c\u5f3a\u8c03\u9700\u8981\u6807\u51c6\u5316\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u6765\u4fc3\u8fdbAI\u5f00\u53d1\u7684\u95ee\u8d23\u5236\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2509.07525", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07525", "abs": "https://arxiv.org/abs/2509.07525", "authors": ["Gianluca Amprimo", "Alberto Ancilotto", "Alessandro Savino", "Fabio Quazzolo", "Claudia Ferraris", "Gabriella Olmo", "Elisabetta Farella", "Stefano Di Carlo"], "title": "EHWGesture -- A dataset for multimodal understanding of clinical gestures", "comment": "Accepted at ICCV 2025 Workshop on AI-driven Skilled Activity\n  Understanding, Assessment & Feedback Generation", "summary": "Hand gesture understanding is essential for several applications in\nhuman-computer interaction, including automatic clinical assessment of hand\ndexterity. While deep learning has advanced static gesture recognition, dynamic\ngesture understanding remains challenging due to complex spatiotemporal\nvariations. Moreover, existing datasets often lack multimodal and multi-view\ndiversity, precise ground-truth tracking, and an action quality component\nembedded within gestures. This paper introduces EHWGesture, a multimodal video\ndataset for gesture understanding featuring five clinically relevant gestures.\nIt includes over 1,100 recordings (6 hours), captured from 25 healthy subjects\nusing two high-resolution RGB-Depth cameras and an event camera. A motion\ncapture system provides precise ground-truth hand landmark tracking, and all\ndevices are spatially calibrated and synchronized to ensure cross-modal\nalignment. Moreover, to embed an action quality task within gesture\nunderstanding, collected recordings are organized in classes of execution speed\nthat mirror clinical evaluations of hand dexterity. Baseline experiments\nhighlight the dataset's potential for gesture classification, gesture trigger\ndetection, and action quality assessment. Thus, EHWGesture can serve as a\ncomprehensive benchmark for advancing multimodal clinical gesture\nunderstanding.", "AI": {"tldr": "EHWGesture\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u624b\u52bf\u7406\u89e3\u6570\u636e\u96c6\uff0c\u5305\u542b5\u79cd\u4e34\u5e8a\u76f8\u5173\u624b\u52bf\uff0c\u8d85\u8fc71100\u6761\u8bb0\u5f55\uff086\u5c0f\u65f6\uff09\uff0c\u6765\u81ea25\u540d\u5065\u5eb7\u53d7\u8bd5\u8005\uff0c\u4f7f\u7528RGB-Depth\u76f8\u673a\u548c\u4e8b\u4ef6\u76f8\u673a\u91c7\u96c6\uff0c\u5e76\u63d0\u4f9b\u7cbe\u786e\u7684\u624b\u90e8\u5173\u952e\u70b9\u8ddf\u8e2a\u548c\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u52a8\u6001\u624b\u52bf\u7406\u89e3\u5728\u4e34\u5e8a\u624b\u90e8\u7075\u6d3b\u6027\u8bc4\u4f30\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6a21\u6001\u591a\u89c6\u89d2\u591a\u6837\u6027\u3001\u7cbe\u786e\u7684\u771f\u503c\u8ddf\u8e2a\u4ee5\u53ca\u5d4c\u5165\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u9ad8\u5206\u8fa8\u7387RGB-Depth\u76f8\u673a\u548c\u4e00\u4e2a\u4e8b\u4ef6\u76f8\u673a\u91c7\u96c6\u6570\u636e\uff0c\u901a\u8fc7\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u63d0\u4f9b\u7cbe\u786e\u7684\u624b\u90e8\u5730\u6807\u8ddf\u8e2a\uff0c\u6240\u6709\u8bbe\u5907\u7ecf\u8fc7\u7a7a\u95f4\u6821\u51c6\u548c\u65f6\u95f4\u540c\u6b65\u4ee5\u786e\u4fdd\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc71100\u6761\u8bb0\u5f55\uff086\u5c0f\u65f6\uff09\uff0c\u6765\u81ea25\u540d\u5065\u5eb7\u53d7\u8bd5\u8005\uff0c\u6309\u7167\u6267\u884c\u901f\u5ea6\u7b49\u7ea7\u7ec4\u7ec7\u4ee5\u53cd\u6620\u4e34\u5e8a\u624b\u90e8\u7075\u6d3b\u6027\u8bc4\u4f30\u3002\u57fa\u7ebf\u5b9e\u9a8c\u5c55\u793a\u4e86\u6570\u636e\u96c6\u5728\u624b\u52bf\u5206\u7c7b\u3001\u624b\u52bf\u89e6\u53d1\u68c0\u6d4b\u548c\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "EHWGesture\u53ef\u4f5c\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u4e34\u5e8a\u624b\u52bf\u7406\u89e3\u7684\u7efc\u5408\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2509.07523", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07523", "abs": "https://arxiv.org/abs/2509.07523", "authors": ["Jad Yehya", "Mansour Benbakoura", "C\u00e9dric Allain", "Beno\u00eet Malezieux", "Matthieu Kowalski", "Thomas Moreau"], "title": "RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection", "comment": null, "summary": "Identifying recurring patterns and rare events in large-scale signals is a\nfundamental challenge in fields such as astronomy, physical simulations, and\nbiomedical science. Convolutional Dictionary Learning (CDL) offers a powerful\nframework for modeling local structures in signals, but its use for detecting\nrare or anomalous events remains largely unexplored. In particular, CDL faces\ntwo key challenges in this setting: high computational cost and sensitivity to\nartifacts and outliers. In this paper, we introduce RoseCDL, a scalable and\nrobust CDL algorithm designed for unsupervised rare event detection in long\nsignals. RoseCDL combines stochastic windowing for efficient training on large\ndatasets with inline outlier detection to enhance robustness and isolate\nanomalous patterns. This reframes CDL as a practical tool for event discovery\nand characterization in real-world signals, extending its role beyond\ntraditional tasks like compression or denoising.", "AI": {"tldr": "RoseCDL\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u5377\u79ef\u5b57\u5178\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u957f\u4fe1\u53f7\u4e2d\u7684\u65e0\u76d1\u7763\u7f55\u89c1\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u901a\u8fc7\u968f\u673a\u7a97\u53e3\u5316\u548c\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u4e86\u4f20\u7edfCDL\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5f02\u5e38\u654f\u611f\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u4fe1\u53f7\u4e2d\u8bc6\u522b\u91cd\u590d\u6a21\u5f0f\u548c\u7f55\u89c1\u4e8b\u4ef6\u662f\u5929\u6587\u5b66\u3001\u7269\u7406\u6a21\u62df\u548c\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u7684\u57fa\u672c\u6311\u6218\u3002\u5377\u79ef\u5b57\u5178\u5b66\u4e60(CDL)\u867d\u7136\u80fd\u6709\u6548\u5efa\u6a21\u4fe1\u53f7\u5c40\u90e8\u7ed3\u6784\uff0c\u4f46\u5728\u7f55\u89c1\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5f02\u5e38\u654f\u611f\u6027\u4e24\u5927\u6311\u6218\u3002", "method": "RoseCDL\u7ed3\u5408\u4e86\u968f\u673a\u7a97\u53e3\u5316\u6280\u672f\u4ee5\u5b9e\u73b0\u5927\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u5e76\u9694\u79bb\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u8be5\u7b97\u6cd5\u5c06CDL\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u5b9e\u9645\u4e8b\u4ef6\u53d1\u73b0\u548c\u7279\u5f81\u63d0\u53d6\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u5176\u5728\u538b\u7f29\u6216\u53bb\u566a\u7b49\u4f20\u7edf\u4efb\u52a1\u4e4b\u5916\u7684\u5e94\u7528\u8303\u56f4\u3002", "conclusion": "RoseCDL\u4e3a\u73b0\u5b9e\u4e16\u754c\u4fe1\u53f7\u4e2d\u7684\u4e8b\u4ef6\u53d1\u73b0\u548c\u7279\u5f81\u63d0\u53d6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684CDL\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7f55\u89c1\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.07969", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07969", "abs": "https://arxiv.org/abs/2509.07969", "authors": ["Xin Lai", "Junyi Li", "Wei Li", "Tao Liu", "Tianjian Li", "Hengshuang Zhao"], "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search", "comment": "Code, datasets, models are available at\n  https://github.com/Mini-o3/Mini-o3. Project Page: https://mini-o3.github.io/", "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.", "AI": {"tldr": "\u5c0f\u578bo3\u7cfb\u7edf\u901a\u8fc7\u6269\u5927\u5de5\u5177\u4ea4\u4e92\u8f6e\u6570\u548c\u591a\u6837\u5316\u63a8\u7406\u6a21\u5f0f\uff0c\u5728\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6df1\u5ea6\u591a\u8f6e\u63a8\u7406\u548c\u72ec\u521b\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u9898\u4e2d\u63a8\u7406\u6a21\u5f0f\u5355\u8c03\u3001\u4ea4\u4e92\u8f6e\u6570\u6709\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u9700\u8981\u8bd5\u9519\u63a2\u7d22\u7684\u590d\u6742\u4efb\u52a1\u65e0\u6cd5\u5904\u7406", "method": "\u6784\u5efaVisual Probe\u6570\u636e\u96c6\u3001\u8fed\u4ee3\u6570\u636e\u6536\u96c6\u6d41\u6c34\u7ebf\u83b7\u53d6\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\u3001\u91c7\u7528\u8d85\u8f6e\u63a9\u7801\u7b56\u7565\u907f\u514d\u8fc5\u901f\u8f6e\u6570\u5904\u7f5a", "result": "\u6a21\u578b\u5728\u4ec5\u8bad\u7ec36\u8f6e\u4ea4\u4e92\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u751f\u6210\u5341\u51e0\u8f6e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u51c6\u786e\u7387\u968f\u8f6e\u6570\u589e\u52a0\u800c\u63d0\u5347\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230\u72ec\u521b\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6269\u5927\u5de5\u5177\u4ea4\u4e92\u89c4\u6a21\u548c\u63d0\u4f9b\u591a\u6837\u5316\u63a8\u7406\u6a21\u5f0f\uff0cMini-o3\u7cfb\u7edf\u80fd\u591f\u5728\u89c6\u89c9\u95ee\u9898\u4e2d\u5b9e\u73b0\u6df1\u5ea6\u601d\u8003\u548c\u6709\u6548\u63a2\u7d22"}}
{"id": "2509.07782", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07782", "abs": "https://arxiv.org/abs/2509.07782", "authors": ["Hugo Blanc", "Jean-Emmanuel Deschaud", "Alexis Paljic"], "title": "RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis", "comment": "Project page with videos and code: https://raygaussx.github.io/", "summary": "RayGauss has achieved state-of-the-art rendering quality for novel-view\nsynthesis on synthetic and indoor scenes by representing radiance and density\nfields with irregularly distributed elliptical basis functions, rendered via\nvolume ray casting using a Bounding Volume Hierarchy (BVH). However, its\ncomputational cost prevents real-time rendering on real-world scenes. Our\napproach, RayGaussX, builds on RayGauss by introducing key contributions that\naccelerate both training and inference. Specifically, we incorporate volumetric\nrendering acceleration strategies such as empty-space skipping and adaptive\nsampling, enhance ray coherence, and introduce scale regularization to reduce\nfalse-positive intersections. Additionally, we propose a new densification\ncriterion that improves density distribution in distant regions, leading to\nenhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x\nto 12x faster training and 50x to 80x higher rendering speeds (FPS) on\nreal-world datasets while improving visual quality by up to +0.56 dB in PSNR.\nProject page with videos and code: https://raygaussx.github.io/.", "AI": {"tldr": "RayGaussX\u5728RayGauss\u57fa\u7840\u4e0a\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u5f15\u5165\u4f53\u79ef\u6e32\u67d3\u52a0\u901f\u7b56\u7565\u3001\u589e\u5f3a\u5149\u7ebf\u8fde\u8d2f\u6027\u548c\u5c3a\u5ea6\u6b63\u5219\u5316\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u4e865-12\u500d\u8bad\u7ec3\u52a0\u901f\u548c50-80\u500d\u6e32\u67d3\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "RayGauss\u867d\u7136\u5728\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u3002", "method": "\u5f15\u5165\u4f53\u79ef\u6e32\u67d3\u52a0\u901f\u7b56\u7565\uff08\u7a7a\u7a7a\u95f4\u8df3\u8fc7\u548c\u81ea\u9002\u5e94\u91c7\u6837\uff09\u3001\u589e\u5f3a\u5149\u7ebf\u8fde\u8d2f\u6027\u3001\u5c3a\u5ea6\u6b63\u5219\u5316\u51cf\u5c11\u8bef\u62a5\u4ea4\u96c6\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u81f4\u5bc6\u5316\u6807\u51c6\u6539\u5584\u8fdc\u8ddd\u79bb\u533a\u57df\u5bc6\u5ea6\u5206\u5e03\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e865-12\u500d\u8bad\u7ec3\u52a0\u901f\u548c50-80\u500d\u6e32\u67d3\u901f\u5ea6\u63d0\u5347\uff08FPS\uff09\uff0c\u89c6\u89c9\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe+0.56 dB PSNR\u3002", "conclusion": "RayGaussX\u6210\u529f\u89e3\u51b3\u4e86RayGauss\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u3002"}}
