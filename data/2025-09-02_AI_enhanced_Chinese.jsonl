{"id": "2508.21201", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21201", "abs": "https://arxiv.org/abs/2508.21201", "authors": ["Arash Ahmadi", "Sarah Sharif", "Yaser Banad"], "title": "Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization", "comment": null, "summary": "Analyzing the human factors behind aviation accidents is crucial for\npreventing future incidents, yet traditional methods using the Human Factors\nAnalysis and Classification System (HFACS) are limited by scalability and\nconsistency. To address this, we introduce an automated HFACS classification\nframework for aviation safety analysis that utilizes Reinforcement Learning\nwith Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B\nlanguage model. Our approach incorporates a multi-component reward system\ntailored for aviation safety analysis and integrates synthetic data generation\nto overcome class imbalance in accident datasets. The resulting GRPO-optimized\nmodel achieved noticeable performance gains, including a 350% increase in exact\nmatch accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy\nof 0.8800. Significantly, our specialized model outperforms state-of-the-art\nLLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key\nmetrics. This research also proposes exact match accuracy in multi-label HFACS\nclassification problem as a new benchmarking methodology to evaluate the\nadvanced reasoning capabilities of language models. Ultimately, our work\nvalidates that smaller, domain-optimized models can provide a computationally\nefficient and better solution for critical safety analysis. This approach makes\npowerful, low-latency deployment on resource-constrained edge devices feasible.", "AI": {"tldr": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548cGRPO\u4f18\u5316\u7684\u81ea\u52a8\u5316HFACS\u5206\u6790\u6846\u67b6\uff0c\u5c06Llama-3.1 8B\u6a21\u578b\u5728\u822a\u7a7a\u5b89\u5168\u9886\u57df\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u5339\u914d\u51c6\u786e\u5ea6\u7684350%\u63d0\u5347\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u4f20\u7edfHFACS\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u822a\u7a7a\u4e8b\u6545\u4eba\u56e0\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u7cbe\u8c03Llama-3.1 8B\u6a21\u578b\uff0c\u6784\u5efa\u591a\u7ec4\u4ef6\u5956\u52b1\u7cfb\u7edf\uff0c\u5e76\u96c6\u6210\u5408\u6210\u6570\u636e\u751f\u6210\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6a21\u578b\u7cbe\u786e\u5339\u914d\u51c6\u786e\u5ea6\u4ece0.0400\u63d0\u5347\u52300.1800(\u589e\u957f350%)\uff0c\u90e8\u5206\u5339\u914d\u51c6\u786e\u5ea6\u8fbe\u52300.8800\uff0c\u8d85\u8d8aGPT-5-mini\u548cGemini-2.5-flash\u7b49\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "\u9886\u57df\u4f18\u5316\u7684\u8f7b\u91cf\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u66f4\u4f18\u79c0\u7684\u5b89\u5168\u5206\u6790\u65b9\u6848\uff0c\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5904\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2508.21475", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21475", "abs": "https://arxiv.org/abs/2508.21475", "authors": ["Xijia Tao", "Yihua Teng", "Xinxing Su", "Xinyu Fu", "Jihao Wu", "Chaofan Tao", "Ziru Liu", "Haoli Bai", "Rui Liu", "Lingpeng Kong"], "title": "MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents", "comment": "Project Page: https://mmsearch-plus.github.io", "summary": "Large multimodal language models (MLLMs) are increasingly deployed as web\nagents, yet many multimodal browsing benchmarks can be solved by shallow, fixed\nworkflows that lean on high-recall image search and nearby text-masking the\ngenuinely multimodal challenges of fine-grained visual reasoning, provenance\nverification, and long-horizon tool use. We introduce MMSearch-Plus, a\nbenchmark of 311 tasks that highly demand multimodal understanding while\npreserving the difficulty profile of strong text-only browsing suites. Each\nitem is constructed to contain multiple weak, localized visual signals that\nmust be extracted, propagated through iterative text-image search, and\ncross-validated under retrieval noise before answering. Our curation procedure,\nSpatial-Temporal Extrapolation, seeds questions whose answers require\nextrapolating from spatial cues (micro-text, part-level appearance, layouts,\nsignage) and temporal traces (broadcast overlays, seasonal context) to\nout-of-image facts such as events, dates, and venues. We provide a\nmodel-agnostic agent framework with browsing tools and evaluate a range of\nclosed and open MLLMs. The strongest agent (o3) attains 15.1% without search\nand 36.0% accuracy with rollout under our framework, while a strong open-source\nmodel (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20\nrounds of search. Beyond answer accuracy, we assess bounding-box production and\ncropped-image search, and conduct an error analysis that surfaces failures in\nsource verification, part-based reasoning, and long-horizon planning.", "AI": {"tldr": "MMSearch-Plus\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b311\u4e2a\u9700\u8981\u6df1\u5ea6\u89c6\u89c9\u7406\u89e3\u548c\u8fed\u4ee3\u641c\u7d22\u7684\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6d45\u5c42\u5de5\u4f5c\u6d41\u7a0b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u6d4f\u89c8\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u53ef\u4ee5\u901a\u8fc7\u6d45\u5c42\u7684\u56fa\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u89e3\u51b3\uff0c\u65e0\u6cd5\u771f\u6b63\u6d4b\u8bd5\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a8\u7406\u3001\u6765\u6e90\u9a8c\u8bc1\u548c\u957f\u65f6\u7a0b\u5de5\u5177\u4f7f\u7528\u7b49\u771f\u6b63\u591a\u6a21\u6001\u6311\u6218\u3002", "method": "\u91c7\u7528\u7a7a\u95f4-\u65f6\u95f4\u5916\u63a8\u6cd5\u6784\u5efa\u4efb\u52a1\uff0c\u8981\u6c42\u4ece\u7a7a\u95f4\u7ebf\u7d22\uff08\u5fae\u6587\u672c\u3001\u5c40\u90e8\u5916\u89c2\u3001\u5e03\u5c40\u3001\u6807\u5fd7\uff09\u548c\u65f6\u95f4\u75d5\u8ff9\uff08\u5e7f\u64ad\u53e0\u52a0\u3001\u5b63\u8282\u4e0a\u4e0b\u6587\uff09\u5916\u63a8\u5230\u56fe\u50cf\u5916\u7684\u4e8b\u5b9e\u3002\u63d0\u4f9b\u6a21\u578b\u65e0\u5173\u7684\u4ee3\u7406\u6846\u67b6\u548c\u6d4f\u89c8\u5de5\u5177\u3002", "result": "\u6700\u5f3a\u4ee3\u7406\uff08o3\uff09\u65e0\u641c\u7d22\u51c6\u786e\u7387\u4e3a15.1%\uff0c\u6709\u641c\u7d22\u51c6\u786e\u7387\u4e3a36.0%\uff1b\u6700\u5f3a\u5f00\u6e90\u6a21\u578b\uff08Qwen-2.5-VL-72B-Instruct\uff09\u65e0\u641c\u7d22\u51c6\u786e\u7387\u4e3a0.0%\uff0c20\u8f6e\u641c\u7d22\u540e\u4e3a6.9%\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6765\u6e90\u9a8c\u8bc1\u3001\u57fa\u4e8e\u90e8\u4ef6\u7684\u63a8\u7406\u548c\u957f\u65f6\u7a0b\u89c4\u5212\u65b9\u9762\u7684\u5931\u8d25\uff0c\u4e3a\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u771f\u5b9e\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u6807\u51c6\u3002"}}
{"id": "2508.21190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21190", "abs": "https://arxiv.org/abs/2508.21190", "authors": ["M\u00e5rten Wadenb\u00e4ck", "Marcus Valtonen \u00d6rnhag", "Johan Edstedt"], "title": "Radially Distorted Homographies, Revisited", "comment": null, "summary": "Homographies are among the most prevalent transformations occurring in\ngeometric computer vision and projective geometry, and homography estimation is\nconsequently a crucial step in a wide assortment of computer vision tasks. When\nworking with real images, which are often afflicted with geometric distortions\ncaused by the camera lens, it may be necessary to determine both the homography\nand the lens distortion-particularly the radial component, called radial\ndistortion-simultaneously to obtain anything resembling useful estimates. When\nconsidering a homography with radial distortion between two images, there are\nthree conceptually distinct configurations for the radial distortion; (i)\ndistortion in only one image, (ii) identical distortion in the two images, and\n(iii) independent distortion in the two images. While these cases have been\naddressed separately in the past, the present paper provides a novel and\nunified approach to solve all three cases. We demonstrate how the proposed\napproach can be used to construct new fast, stable, and accurate minimal\nsolvers for radially distorted homographies. In all three cases, our proposed\nsolvers are faster than the existing state-of-the-art solvers while maintaining\nsimilar accuracy. The solvers are tested on well-established benchmarks\nincluding images taken with fisheye cameras. The source code for our solvers\nwill be made available in the event our paper is accepted for publication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u540c\u65f6\u4f30\u8ba1\u5355\u5e94\u6027\u53d8\u6362\u548c\u5f84\u5411\u7578\u53d8\uff0c\u89e3\u51b3\u4e86\u4e09\u79cd\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u5f84\u5411\u7578\u53d8\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u5feb\u901f\u3001\u7a33\u5b9a\u7684\u6700\u5c0f\u6c42\u89e3\u5668\u3002", "motivation": "\u5728\u771f\u5b9e\u56fe\u50cf\u5904\u7406\u4e2d\uff0c\u76f8\u673a\u955c\u5934\u5f15\u8d77\u7684\u51e0\u4f55\u7578\u53d8\uff08\u7279\u522b\u662f\u5f84\u5411\u7578\u53d8\uff09\u4f1a\u5f71\u54cd\u5355\u5e94\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u5904\u7406\u4e09\u79cd\u4e0d\u540c\u7684\u5f84\u5411\u7578\u53d8\u914d\u7f6e\uff0c\u7f3a\u4e4f\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e09\u79cd\u5f84\u5411\u7578\u53d8\u914d\u7f6e\uff1a\u5355\u56fe\u50cf\u7578\u53d8\u3001\u4e24\u56fe\u50cf\u76f8\u540c\u7578\u53d8\u548c\u4e24\u56fe\u50cf\u72ec\u7acb\u7578\u53d8\u3002\u57fa\u4e8e\u6b64\u65b9\u6cd5\u6784\u5efa\u4e86\u65b0\u7684\u6700\u5c0f\u6c42\u89e3\u5668\u3002", "result": "\u5728\u6240\u6709\u4e09\u79cd\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u7684\u6c42\u89e3\u5668\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u5feb\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u7cbe\u5ea6\u3002\u5728\u5305\u62ec\u9c7c\u773c\u76f8\u673a\u56fe\u50cf\u5728\u5185\u7684\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f84\u5411\u7578\u53d8\u5355\u5e94\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u53d1\u7684\u6c42\u89e3\u5668\u5177\u6709\u5feb\u901f\u3001\u7a33\u5b9a\u548c\u51c6\u786e\u7684\u7279\u70b9\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2508.21249", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2508.21249", "abs": "https://arxiv.org/abs/2508.21249", "authors": ["Mohammad Amin Nabian", "Sanjay Choudhry"], "title": "A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics", "comment": null, "summary": "The computational cost associated with high-fidelity CFD simulations remains\na significant bottleneck in the automotive design and optimization cycle. While\nML-based surrogate models have emerged as a promising alternative to accelerate\naerodynamic predictions, the field is characterized by a diverse and rapidly\nevolving landscape of specialized neural network architectures, with no single\nmodel demonstrating universal superiority. This paper introduces a novel\nmeta-learning framework that leverages this architectural diversity as a\nstrength. We propose a Mixture of Experts (MoE) model that employs a dedicated\ngating network to dynamically and optimally combine the predictions from three\nheterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable\nmulti-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph\nneural network; and FigConvNet, a factorized implicit global convolution\nnetwork. The gating network learns a spatially-variant weighting strategy,\nassigning credibility to each expert based on its localized performance in\npredicting surface pressure and wall shear stress fields. To prevent model\ncollapse and encourage balanced expert contributions, we integrate an entropy\nregularization term into the training loss function. The entire system is\ntrained and validated on the DrivAerML dataset, a large-scale, public benchmark\nof high-fidelity CFD simulations for automotive aerodynamics. Quantitative\nresults demonstrate that the MoE model achieves a significant reduction in L-2\nprediction error, outperforming not only the ensemble average but also the most\naccurate individual expert model across all evaluated physical quantities. This\nwork establishes the MoE framework as a powerful and effective strategy for\ncreating more robust and accurate composite surrogate models by synergistically\ncombining the complementary strengths of specialized architectures.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6(MoE)\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u6574\u5408\u4e09\u79cd\u5148\u8fdbCFD\u4ee3\u7406\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u5728\u6c7d\u8f66\u7a7a\u6c14\u52a8\u529b\u5b66\u9884\u6d4b\u4e2d\u663e\u8457\u964d\u4f4e\u8bef\u5dee", "motivation": "\u9ad8\u4fdd\u771fCFD\u4eff\u771f\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709ML\u4ee3\u7406\u6a21\u578b\u67b6\u6784\u591a\u6837\u4f46\u65e0\u5355\u4e00\u6700\u4f18\u65b9\u6848\uff0c\u9700\u8981\u5229\u7528\u67b6\u6784\u591a\u6837\u6027\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6", "method": "\u4f7f\u7528\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u7ec4\u5408DoMINO\u3001X-MeshGraphNet\u548cFigConvNet\u4e09\u79cd\u5f02\u6784\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u9632\u6b62\u6a21\u578b\u574d\u584c\uff0c\u5728DrivAerML\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u9a8c\u8bc1", "result": "MoE\u6a21\u578b\u5728\u6240\u6709\u8bc4\u4f30\u7269\u7406\u91cf\u4e0a\u5747\u663e\u8457\u964d\u4f4eL-2\u9884\u6d4b\u8bef\u5dee\uff0c\u4f18\u4e8e\u96c6\u6210\u5e73\u5747\u548c\u6700\u51c6\u786e\u7684\u5355\u4e2a\u4e13\u5bb6\u6a21\u578b", "conclusion": "MoE\u6846\u67b6\u901a\u8fc7\u534f\u540c\u6574\u5408\u4e13\u7528\u67b6\u6784\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u51c6\u786e\u7684\u590d\u5408\u4ee3\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565"}}
{"id": "2508.21690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21690", "abs": "https://arxiv.org/abs/2508.21690", "authors": ["Olger Siebinga", "David Abbink"], "title": "Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?", "comment": null, "summary": "Pedestrians approaching each other on a sidewalk sometimes end up in an\nawkward interaction known as the \"sidewalk salsa\": they both (repeatedly)\ndeviate to the same side to avoid a collision. This provides an interesting use\ncase to study interactions between pedestrians and mobile robots because, in\nthe vast majority of cases, this phenomenon is avoided through a negotiation\nbased on implicit communication. Understanding how it goes wrong and how\npedestrians end up in the sidewalk salsa will therefore provide insight into\nthe implicit communication. This understanding can be used to design safe and\nacceptable robotic behaviour. In a previous attempt to gain this understanding,\na model of pedestrian behaviour based on the Communication-Enabled Interaction\n(CEI) framework was developed that can replicate the sidewalk salsa. However,\nit is unclear how to leverage this model in robotic planning and\ndecision-making since it violates the assumptions of game theory, a much-used\nframework in planning and decision-making. Here, we present a proof-of-concept\nfor an approach where a Reinforcement Learning (RL) agent leverages the model\nto learn how to interact with pedestrians. The results show that a basic RL\nagent successfully learned to interact with the CEI model. Furthermore, a\nrisk-averse RL agent that had access to the perceived risk of the CEI model\nlearned how to effectively communicate its intention through its motion and\nthereby substantially lowered the perceived risk, and displayed effort by the\nmodelled pedestrian. These results show this is a promising approach and\nencourage further exploration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4e0e\u884c\u4eba\u884c\u4e3a\u6a21\u578b\u4ea4\u4e92\uff0c\u6210\u529f\u89e3\u51b3\u4e86\"\u4eba\u884c\u9053\u5c2c\u821e\"\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u964d\u4f4e\u4e86\u884c\u4eba\u7684\u611f\u77e5\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u4eba\u884c\u9053\u4e0a\u884c\u4eba\u76f8\u9047\u65f6\u7684\"\u4eba\u884c\u9053\u5c2c\u821e\"\u73b0\u8c61\uff0c\u7406\u89e3\u9690\u5f0f\u901a\u4fe1\u673a\u5236\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u8bbe\u8ba1\u5b89\u5168\u53ef\u63a5\u53d7\u7684\u884c\u4e3a\u63d0\u4f9b insights\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60(RL)\u4ee3\u7406\u4e0e\u57fa\u4e8e\u901a\u4fe1\u4f7f\u80fd\u4ea4\u4e92(CEI)\u6846\u67b6\u7684\u884c\u4eba\u884c\u4e3a\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\u5b66\u4e60\u3002", "result": "\u57fa\u7840RL\u4ee3\u7406\u6210\u529f\u5b66\u4f1a\u4e86\u4e0eCEI\u6a21\u578b\u4ea4\u4e92\uff1b\u98ce\u9669\u89c4\u907f\u578bRL\u4ee3\u7406\u901a\u8fc7\u52a8\u4f5c\u6709\u6548\u4f20\u8fbe\u610f\u56fe\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u884c\u4eba\u7684\u611f\u77e5\u98ce\u9669\u3002", "conclusion": "\u8fd9\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86RL\u53ef\u4ee5\u5b66\u4e60\u4e0e\u8fdd\u53cd\u535a\u5f08\u8bba\u5047\u8bbe\u7684\u6a21\u578b\u8fdb\u884c\u6709\u6548\u4ea4\u4e92\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2508.21622", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21622", "abs": "https://arxiv.org/abs/2508.21622", "authors": ["Saravanan Venkatachalam"], "title": "Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study", "comment": null, "summary": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u4f20\u7edf\u7f51\u7edc\u4f18\u5316\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u4e3a\u4f9b\u5e94\u94fe\u89c4\u5212\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u3001\u53ef\u89e3\u91ca\u6027\u548c\u89d2\u8272\u611f\u77e5\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "motivation": "\u5e73\u6cfd\u64cd\u4f5c\u7814\u7a76\u8f93\u51fa\u7684\u590d\u6742\u6027\u4e0e\u4e1a\u52a1\u5229\u76ca\u76f8\u5173\u8005\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6458\u8981\u3001\u4e0a\u4e0b\u6587\u53ef\u89c6\u5316\u548c\u5b9a\u5236KPI\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\u6a21\u578b\u5904\u7406\u591a\u5468\u671f\u591a\u7269\u54c1\u7684\u6218\u672f\u6027\u5e93\u5b58\u91cd\u65b0\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7AI\u4ee3\u7406\u3001RESTful API\u548c\u52a8\u6001\u7528\u6237\u754c\u9762\u6784\u5efa\u6280\u672f\u67b6\u6784\uff0c\u652f\u6301\u5b9e\u65f6\u4ea4\u4e92\u3001\u914d\u7f6e\u66f4\u65b0\u548c\u6a21\u62df\u89c1\u89e3\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u8be5\u7cfb\u7edf\u80fd\u591f\u6539\u5584\u89c4\u5212\u6548\u679c\uff0c\u5305\u62ec\u9884\u9632\u7f3a\u8d27\u3001\u964d\u4f4e\u6210\u672c\u548c\u7ef4\u6301\u670d\u52a1\u6c34\u5e73\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4f18\u5316\u6a21\u578b\u548cLLM\u6280\u672f\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u96c6\u6210\u79c1\u6709LLM\u3001\u8fc1\u79fb\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u6765\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3001\u9002\u5e94\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2508.21741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21741", "abs": "https://arxiv.org/abs/2508.21741", "authors": ["Yao Wang", "Di Liang", "Minlong Peng"], "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.", "AI": {"tldr": "\u63d0\u51faCPI-FT\u6846\u67b6\uff0c\u901a\u8fc7\u6838\u5fc3\u53c2\u6570\u9694\u79bb\u548c\u878d\u5408\u6280\u672f\u89e3\u51b3\u591a\u4efb\u52a1\u5fae\u8c03\u4e2d\u7684\u8df7\u8df7\u677f\u73b0\u8c61\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898", "motivation": "\u76d1\u7763\u5fae\u8c03\u4e2d\u53c2\u6570\u66f4\u65b0\u4f1a\u5bfc\u81f4\u67d0\u4e9b\u4efb\u52a1\u8fdb\u6b65\u800c\u5176\u4ed6\u4efb\u52a1\u9000\u5316\u7684\u8df7\u8df7\u677f\u73b0\u8c61\uff0c\u9700\u8981\u89e3\u51b3\u591a\u4efb\u52a1\u95f4\u7684\u5e72\u6270\u548c\u9057\u5fd8\u95ee\u9898", "method": "1) \u72ec\u7acb\u5fae\u8c03\u8bc6\u522b\u6838\u5fc3\u53c2\u6570\u533a\u57df 2) \u57fa\u4e8e\u533a\u57df\u91cd\u53e0\u7684\u4efb\u52a1\u805a\u7c7b 3) \u6838\u5fc3\u53c2\u6570\u79fb\u690d+\u975e\u6838\u5fc3\u53c2\u6570SLERP\u878d\u5408 4) \u8f7b\u91cf\u7ea7\u6d41\u6c34\u7ebfSFT\u8bad\u7ec3\uff0c\u51bb\u7ed3\u6838\u5fc3\u533a\u57df", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u7f13\u89e3\u4efb\u52a1\u5e72\u6270\u548c\u9057\u5fd8\uff0c\u6301\u7eed\u4f18\u4e8e\u666e\u901a\u591a\u4efb\u52a1\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u57fa\u7ebf", "conclusion": "CPI-FT\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u9694\u79bb\u548c\u667a\u80fd\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5fae\u8c03\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3aLLM\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.21496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21496", "abs": "https://arxiv.org/abs/2508.21496", "authors": ["Hao Lu", "Jiahao Wang", "Yaolun Zhang", "Ruohui Wang", "Xuanyu Zheng", "Yepeng Tang", "Dahua Lin", "Lewei Lu"], "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding", "comment": null, "summary": "Video multimodal large language models (Video-MLLMs) have achieved remarkable\nprogress in video understanding. However, they remain vulnerable to\nhallucination-producing content inconsistent with or unrelated to video inputs.\nPrevious video hallucination benchmarks primarily focus on short-videos. They\nattribute hallucinations to factors such as strong language priors, missing\nframes, or vision-language biases introduced by the visual encoder. While these\ncauses indeed account for most hallucinations in short videos, they still\noversimplify the cause of hallucinations. Sometimes, models generate incorrect\noutputs but with correct frame-level semantics. We refer to this type of\nhallucination as Semantic Aggregation Hallucination (SAH), which arises during\nthe process of aggregating frame-level semantics into event-level semantic\ngroups. Given that SAH becomes particularly critical in long videos due to\nincreased semantic complexity across multiple events, it is essential to\nseparate and thoroughly investigate the causes of this type of hallucination.\nTo address the above issues, we introduce ELV-Halluc, the first benchmark\ndedicated to long-video hallucination, enabling a systematic investigation of\nSAH. Our experiments confirm the existence of SAH and show that it increases\nwith semantic complexity. Additionally, we find that models are more prone to\nSAH on rapidly changing semantics. Moreover, we discuss potential approaches to\nmitigate SAH. We demonstrate that positional encoding strategy contributes to\nalleviating SAH, and further adopt DPO strategy to enhance the model's ability\nto distinguish semantics within and across events. To support this, we curate a\ndataset of 8K adversarial data pairs and achieve improvements on both\nELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ELV-Halluc\u57fa\u51c6\uff0c\u4e13\u95e8\u9488\u5bf9\u957f\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u805a\u5408\u5e7b\u89c9\u95ee\u9898\u8fdb\u884c\u7814\u7a76\uff0c\u53d1\u73b0\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u548cDPO\u8bad\u7ec3\u80fd\u6709\u6548\u51cf\u5c11\u8fd9\u7c7b\u5e7b\u89c9", "motivation": "\u73b0\u6709\u89c6\u9891MLLMs\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u4e4b\u524d\u7684\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u77ed\u89c6\u9891\uff0c\u5ffd\u7565\u4e86\u957f\u89c6\u9891\u4e2d\u7531\u4e8e\u8bed\u4e49\u590d\u6742\u6027\u5bfc\u81f4\u7684\u8bed\u4e49\u805a\u5408\u5e7b\u89c9(SAH)\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u8fd9\u7c7b\u5e7b\u89c9", "method": "\u6784\u5efaELV-Halluc\u957f\u89c6\u9891\u5e7b\u89c9\u57fa\u51c6\uff0c\u5206\u6790SAH\u73b0\u8c61\uff0c\u91c7\u7528\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u548cDPO\u8bad\u7ec3\u65b9\u6cd5\u6765\u7f13\u89e3SAH\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e868K\u5bf9\u6297\u6570\u636e\u5bf9", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86SAH\u7684\u5b58\u5728\uff0c\u53d1\u73b0\u5176\u968f\u8bed\u4e49\u590d\u6742\u6027\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5728\u5feb\u901f\u53d8\u5316\u7684\u8bed\u4e49\u4e0a\u66f4\u5bb9\u6613\u51fa\u73b0\u3002\u901a\u8fc7\u4f4d\u7f6e\u7f16\u7801\u548cDPO\u8bad\u7ec3\uff0c\u5728ELV-Halluc\u548cVideo-MME\u57fa\u51c6\u4e0a\u53d6\u5f97\u6539\u8fdb\uff0cSAH\u6bd4\u7387\u5927\u5e45\u964d\u4f4e27.7%", "conclusion": "\u957f\u89c6\u9891\u4e2d\u7684\u8bed\u4e49\u805a\u5408\u5e7b\u89c9\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u6765\u89e3\u51b3\uff0c\u4f4d\u7f6e\u7f16\u7801\u548cDPO\u8bad\u7ec3\u662f\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565"}}
{"id": "2508.21505", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21505", "abs": "https://arxiv.org/abs/2508.21505", "authors": ["Vishal Pandey", "Debasmita Biswas"], "title": "Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control", "comment": "Preprint (31 pages, 19 images, 7 tables)", "summary": "Reinforcement learning agents based on Transformer architectures have\nachieved impressive performance on sequential decision-making tasks, but their\nreliance on dense matrix operations makes them ill-suited for\nenergy-constrained, edge-oriented platforms. Spiking neural networks promise\nultra-low-power, event-driven inference, yet no prior work has seamlessly\nmerged spiking dynamics with return-conditioned sequence modeling. We present\nthe Spiking Decision Transformer (SNN-DT), which embeds Leaky\nIntegrate-and-Fire neurons into each self-attention block, trains end-to-end\nvia surrogate gradients, and incorporates biologically inspired three-factor\nplasticity, phase-shifted spike-based positional encodings, and a lightweight\ndendritic routing module. Our implementation matches or exceeds standard\nDecision Transformer performance on classic control benchmarks (CartPole-v1,\nMountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes\nper decision, an energy proxy suggesting over four orders-of-magnitude\nreduction in per inference energy. By marrying sequence modeling with\nneuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power\ncontrol on embedded and wearable devices.", "AI": {"tldr": "SNN-DT\u5c06\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e0e\u51b3\u7b56\u53d8\u6362\u5668\u7ed3\u5408\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907", "motivation": "\u4f20\u7edfTransformer\u51b3\u7b56\u667a\u80fd\u4f53\u4f9d\u8d56\u5bc6\u96c6\u77e9\u9635\u8fd0\u7b97\uff0c\u80fd\u8017\u9ad8\uff0c\u4e0d\u9002\u5408\u80fd\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u5e73\u53f0\u3002\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u8d85\u4f4e\u529f\u8017\u7279\u6027\uff0c\u4f46\u5c1a\u672a\u4e0e\u56de\u62a5\u6761\u4ef6\u5e8f\u5217\u5efa\u6a21\u7ed3\u5408", "method": "\u5728\u81ea\u6ce8\u610f\u529b\u5757\u4e2d\u5d4c\u5165Leaky Integrate-and-Fire\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u66ff\u4ee3\u68af\u5ea6\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5305\u542b\u751f\u7269\u542f\u53d1\u7684\u4e09\u56e0\u5b50\u53ef\u5851\u6027\u3001\u76f8\u79fb\u8109\u51b2\u4f4d\u7f6e\u7f16\u7801\u548c\u8f7b\u91cf\u7ea7\u6811\u7a81\u8def\u7531\u6a21\u5757", "result": "\u5728\u7ecf\u5178\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u6807\u51c6\u51b3\u7b56\u53d8\u6362\u5668\u6027\u80fd\uff0c\u6bcf\u4e2a\u51b3\u7b56\u53d1\u5c04\u5c11\u4e8e10\u4e2a\u8109\u51b2\uff0c\u80fd\u8017\u964d\u4f4e\u8d85\u8fc74\u4e2a\u6570\u91cf\u7ea7", "conclusion": "SNN-DT\u901a\u8fc7\u5c06\u5e8f\u5217\u5efa\u6a21\u4e0e\u795e\u7ecf\u5f62\u6001\u6548\u7387\u7ed3\u5408\uff0c\u4e3a\u5d4c\u5165\u5f0f\u53ef\u7a7f\u6234\u8bbe\u5907\u5f00\u8f9f\u4e86\u5b9e\u65f6\u4f4e\u529f\u8017\u63a7\u5236\u7684\u65b0\u9014\u5f84"}}
{"id": "2508.21722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21722", "abs": "https://arxiv.org/abs/2508.21722", "authors": ["Siddharth Mangalik", "Ojas Deshpande", "Adithya V. Ganesan", "Sean A. P. Clouston", "H. Andrew Schwartz"], "title": "Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety", "comment": null, "summary": "Estimating community-specific mental health effects of local events is vital\nfor public health policy. While forecasting mental health scores alone offers\nlimited insights into the impact of events on community well-being,\nquasi-experimental designs like the Longitudinal Regression Discontinuity\nDesign (LRDD) from econometrics help researchers derive more effects that are\nmore likely to be causal from observational data. LRDDs aim to extrapolate the\nsize of changes in an outcome (e.g. a discontinuity in running scores for\nanxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond\ntraditional forecasting into a statistical learning framework whereby future\ndiscontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear\ntrajectories) are estimated given a location's history of the score, dynamic\ncovariates (other running assessments), and exogenous variables (static\nrepresentations). Applying our framework to predict discontinuities in the\nanxiety of US counties from COVID-19 events, we found the task was difficult\nbut more achievable as the sophistication of models was increased, with the\nbest results coming from integrating exogenous and dynamic covariates. Our\napproach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$\nfor slope) over traditional static community representations. Discontinuity\nforecasting raises new possibilities for estimating the idiosyncratic effects\nof potential future or hypothetical events on specific communities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u7eb5\u5411\u56de\u5f52\u65ad\u70b9\u8bbe\u8ba1(LRDD)\u4ece\u4f20\u7edf\u9884\u6d4b\u6269\u5c55\u5230\u7edf\u8ba1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bCOVID-19\u4e8b\u4ef6\u5bf9\u7f8e\u56fd\u53bf\u7126\u8651\u8bc4\u5206\u7684\u65ad\u70b9\u548c\u659c\u7387\u53d8\u5316\uff0c\u7ed3\u679c\u663e\u793a\u6574\u5408\u5916\u751f\u548c\u52a8\u6001\u534f\u53d8\u91cf\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u4f30\u8ba1\u5730\u65b9\u4e8b\u4ef6\u5bf9\u7279\u5b9a\u793e\u533a\u5fc3\u7406\u5065\u5eb7\u7684\u5f71\u54cd\u5bf9\u516c\u5171\u536b\u751f\u653f\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5355\u7eaf\u9884\u6d4b\u5fc3\u7406\u5065\u5eb7\u8bc4\u5206\u65e0\u6cd5\u6df1\u5165\u4e86\u89e3\u4e8b\u4ef6\u5bf9\u793e\u533a\u798f\u7949\u7684\u56e0\u679c\u5f71\u54cd\u3002", "method": "\u5c06LRDD\u6269\u5c55\u5230\u7edf\u8ba1\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4f4d\u7f6e\u8bc4\u5206\u5386\u53f2\u3001\u52a8\u6001\u534f\u53d8\u91cf\u548c\u5916\u751f\u53d8\u91cf\u6765\u9884\u6d4b\u672a\u6765\u7684\u65ad\u70b9\uff08\u65f6\u95f4\u7279\u5b9a\u53d8\u5316\uff09\u548c\u659c\u7387\uff08\u7ebf\u6027\u8f68\u8ff9\uff09\u53d8\u5316\u3002", "result": "\u9884\u6d4bCOVID-19\u4e8b\u4ef6\u5bfc\u81f4\u7684\u7126\u8651\u8bc4\u5206\u65ad\u70b9\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u968f\u7740\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\u800c\u53d8\u5f97\u53ef\u884c\uff0c\u6700\u4f73\u7ed3\u679c\u6765\u81ea\u6574\u5408\u5916\u751f\u548c\u52a8\u6001\u534f\u53d8\u91cf\uff08\u65ad\u70b9r=+0.46\uff0c\u659c\u7387r=+0.65\uff09\u3002", "conclusion": "\u65ad\u70b9\u9884\u6d4b\u4e3a\u4f30\u8ba1\u672a\u6765\u6216\u5047\u8bbe\u4e8b\u4ef6\u5bf9\u7279\u5b9a\u793e\u533a\u7684\u5f02\u8d28\u6027\u5f71\u54cd\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u9759\u6001\u793e\u533a\u8868\u5f81\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2508.21816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21816", "abs": "https://arxiv.org/abs/2508.21816", "authors": ["Yiming Lin", "Yuchen Niu", "Shang Wang", "Kaizhu Huang", "Qiufeng Wang", "Xiao-Bo Jin"], "title": "The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning", "comment": "Accepted by ICDM 2025", "summary": "Context recognition (SR) is a fundamental task in computer vision that aims\nto extract structured semantic summaries from images by identifying key events\nand their associated entities. Specifically, given an input image, the model\nmust first classify the main visual events (verb classification), then identify\nthe participating entities and their semantic roles (semantic role labeling),\nand finally localize these entities in the image (semantic role localization).\nExisting methods treat verb classification as a single-label problem, but we\nshow through a comprehensive analysis that this formulation fails to address\nthe inherent ambiguity in visual event recognition, as multiple verb categories\nmay reasonably describe the same image. This paper makes three key\ncontributions: First, we reveal through empirical analysis that verb\nclassification is inherently a multi-label problem due to the ubiquitous\nsemantic overlap between verb categories. Second, given the impracticality of\nfully annotating large-scale datasets with multiple labels, we propose to\nreformulate verb classification as a single positive multi-label learning\n(SPMLL) problem - a novel perspective in SR research. Third, we design a\ncomprehensive multi-label evaluation benchmark for SR that is carefully\ndesigned to fairly evaluate model performance in a multi-label setting. To\naddress the challenges of SPMLL, we futher develop the Graph Enhanced Verb\nMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks to\ncapture label correlations and adversarial training to optimize decision\nboundaries. Extensive experiments on real-world datasets show that our approach\nachieves more than 3\\% MAP improvement while remaining competitive on\ntraditional top-1 and top-5 accuracy metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u573a\u666f\u8bc6\u522b\u4e2d\u7684\u52a8\u8bcd\u5206\u7c7b\u672c\u8d28\u4e0a\u662f\u591a\u6807\u7b7e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5355\u6b63\u4f8b\u591a\u6807\u7b7e\u5b66\u4e60\u6846\u67b6\u548cGE-VerbMLP\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u4f20\u7edf\u6307\u6807\u7ade\u4e89\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e863%\u4ee5\u4e0a\u7684MAP\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u52a8\u8bcd\u5206\u7c7b\u89c6\u4e3a\u5355\u6807\u7b7e\u95ee\u9898\uff0c\u4f46\u5b9e\u9645\u56fe\u50cf\u4e2d\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u591a\u4e2a\u52a8\u8bcd\u7c7b\u522b\u53ef\u80fd\u5408\u7406\u63cf\u8ff0\u540c\u4e00\u56fe\u50cf\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u8bbe\u5b9a\u3002", "method": "\u5c06\u52a8\u8bcd\u5206\u7c7b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5355\u6b63\u4f8b\u591a\u6807\u7b7e\u5b66\u4e60(SPMLL)\u95ee\u9898\uff0c\u63d0\u51faGraph Enhanced Verb MLP\u6a21\u578b\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u6807\u7b7e\u76f8\u5173\u6027\u548c\u5bf9\u6297\u8bad\u7ec3\u4f18\u5316\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301top-1\u548ctop-5\u51c6\u786e\u7387\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc73%\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c(MAP)\u63d0\u5347\u3002", "conclusion": "\u52a8\u8bcd\u5206\u7c7b\u672c\u8d28\u4e0a\u662f\u591a\u6807\u7b7e\u95ee\u9898\uff0c\u63d0\u51fa\u7684SPMLL\u6846\u67b6\u548cGE-VerbMLP\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u4e3a\u573a\u666f\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
