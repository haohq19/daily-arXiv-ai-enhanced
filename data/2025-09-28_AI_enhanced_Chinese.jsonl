{"id": "2509.20367", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.20367", "abs": "https://arxiv.org/abs/2509.20367", "authors": ["Leyi Ouyang"], "title": "Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models", "comment": "2 Figures, 7 Tables, 1 Algorithm", "summary": "Diplomatic events consistently prompt widespread public discussion and\ndebate. Public sentiment plays a critical role in diplomacy, as a good\nsentiment provides vital support for policy implementation, helps resolve\ninternational issues, and shapes a nation's international image. Traditional\nmethods for gauging public sentiment, such as large-scale surveys or manual\ncontent analysis of media, are typically time-consuming, labor-intensive, and\nlack the capacity for forward-looking analysis. We propose a novel framework\nthat identifies specific modifications for diplomatic event narratives to shift\npublic sentiment from negative to neutral or positive. First, we train a\nlanguage model to predict public reaction towards diplomatic events. To this\nend, we construct a dataset comprising descriptions of diplomatic events and\ntheir associated public discussions. Second, guided by communication theories\nand in collaboration with domain experts, we predetermined several textual\nfeatures for modification, ensuring that any alterations changed the event's\nnarrative framing while preserving its core facts.We develop a counterfactual\ngeneration algorithm that employs a large language model to systematically\nproduce modified versions of an original text. The results show that this\nframework successfully shifted public sentiment to a more favorable state with\na 70\\% success rate. This framework can therefore serve as a practical tool for\ndiplomats, policymakers, and communication specialists, offering data-driven\ninsights on how to frame diplomatic initiatives or report on events to foster a\nmore desirable public sentiment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4fee\u6539\u5916\u4ea4\u4e8b\u4ef6\u53d9\u4e8b\u6846\u67b6\u6765\u6539\u53d8\u516c\u4f17\u60c5\u7eea\u7684AI\u6846\u67b6\uff0c\u6210\u529f\u5c06\u8d1f\u9762\u60c5\u7eea\u8f6c\u4e3a\u4e2d\u6027\u6216\u6b63\u9762\u7684\u6210\u529f\u7387\u8fbe70%", "motivation": "\u4f20\u7edf\u516c\u4f17\u60c5\u7eea\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u8017\u529b\u4e14\u7f3a\u4e4f\u524d\u77bb\u6027\uff0c\u800c\u516c\u4f17\u60c5\u7eea\u5728\u5916\u4ea4\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u548c\u5f15\u5bfc\u8206\u8bba", "method": "\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u516c\u4f17\u5bf9\u5916\u4ea4\u4e8b\u4ef6\u7684\u53cd\u5e94\uff0c\u6784\u5efa\u5916\u4ea4\u4e8b\u4ef6\u63cf\u8ff0\u4e0e\u516c\u4f17\u8ba8\u8bba\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u4f20\u64ad\u7406\u8bba\u548c\u4e13\u5bb6\u610f\u89c1\u786e\u5b9a\u53ef\u4fee\u6539\u7684\u6587\u672c\u7279\u5f81\uff0c\u5f00\u53d1\u53cd\u4e8b\u5b9e\u751f\u6210\u7b97\u6cd5\u7cfb\u7edf\u4fee\u6539\u539f\u59cb\u6587\u672c", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u516c\u4f17\u60c5\u7eea\u8f6c\u5411\u66f4\u6709\u5229\u72b6\u6001\uff0c\u6210\u529f\u7387\u8fbe\u523070%", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u5916\u4ea4\u5b98\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u4f20\u64ad\u4e13\u5bb6\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\uff0c\u5e2e\u52a9\u6784\u5efa\u66f4\u7406\u60f3\u7684\u516c\u4f17\u60c5\u7eea"}}
{"id": "2509.20513", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20513", "abs": "https://arxiv.org/abs/2509.20513", "authors": ["Samer Alshaer", "Ala Khalifeh", "Roman Obermaisser"], "title": "Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems", "comment": "14 pages, 10 figures", "summary": "Adaptive scheduling is crucial for ensuring the reliability and safety of\ntime-triggered systems (TTS) in dynamic operational environments. Scheduling\nframeworks face significant challenges, including message collisions, locked\nloops from incorrect precedence handling, and the generation of incomplete or\ninvalid schedules, which can compromise system safety and performance. To\naddress these challenges, this paper presents a novel reconstruction framework\ndesigned to dynamically validate and assemble schedules. The proposed\nreconstruction models operate by systematically transforming AI-generated or\nheuristically derived scheduling priorities into fully executable schedules,\nensuring adherence to critical system constraints such as precedence rules and\ncollision-free communication. It incorporates robust safety checks, efficient\nallocation algorithms, and recovery mechanisms to handle unexpected context\nevents, including hardware failures and mode transitions. Comprehensive\nexperiments were conducted across multiple performance profiles, including\nmakespan minimisation, workload balancing, and energy efficiency, to validate\nthe operational effectiveness of the reconstruction models. Results demonstrate\nthat the proposed framework significantly enhances system adaptability,\noperational integrity, and runtime performance while maintaining computational\nefficiency. Overall, this work contributes a practical and scalable solution to\nthe problem of safe schedule generation in safety-critical TTS, enabling\nreliable and flexible real-time scheduling even under highly dynamic and\nuncertain operational conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cd\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u9a8c\u8bc1\u548c\u7ec4\u88c5\u65f6\u95f4\u89e6\u53d1\u7cfb\u7edf\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u89e3\u51b3\u6d88\u606f\u78b0\u649e\u3001\u4f18\u5148\u7ea7\u5904\u7406\u4e0d\u5f53\u7b49\u95ee\u9898\uff0c\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u89e6\u53d1\u7cfb\u7edf\u5728\u52a8\u6001\u64cd\u4f5c\u73af\u5883\u4e2d\u9762\u4e34\u6d88\u606f\u78b0\u649e\u3001\u4f18\u5148\u7ea7\u5904\u7406\u4e0d\u5f53\u5bfc\u81f4\u7684\u6b7b\u9501\u3001\u4ee5\u53ca\u751f\u6210\u4e0d\u5b8c\u6574\u6216\u65e0\u6548\u8c03\u5ea6\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u90fd\u4f1a\u5371\u53ca\u7cfb\u7edf\u5b89\u5168\u548c\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u91cd\u6784\u6a21\u578b\u5c06AI\u751f\u6210\u6216\u542f\u53d1\u5f0f\u5f97\u51fa\u7684\u8c03\u5ea6\u4f18\u5148\u7ea7\u7cfb\u7edf\u6027\u5730\u8f6c\u6362\u4e3a\u5b8c\u5168\u53ef\u6267\u884c\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u5305\u542b\u5b89\u5168\u68c0\u67e5\u3001\u9ad8\u6548\u5206\u914d\u7b97\u6cd5\u548c\u6062\u590d\u673a\u5236\u6765\u5904\u7406\u610f\u5916\u4e8b\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u9002\u5e94\u6027\u3001\u64cd\u4f5c\u5b8c\u6574\u6027\u548c\u8fd0\u884c\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b89\u5168\u5173\u952e\u65f6\u95f4\u89e6\u53d1\u7cfb\u7edf\u7684\u5b89\u5168\u8c03\u5ea6\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u64cd\u4f5c\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u53ef\u9760\u7075\u6d3b\u7684\u5b9e\u65f6\u8c03\u5ea6\u3002"}}
{"id": "2509.20520", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20520", "abs": "https://arxiv.org/abs/2509.20520", "authors": ["Samer Alshaer", "Ala Khalifeh", "Roman Obermaisser"], "title": "Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications", "comment": "18 pages, 21 figures", "summary": "Metascheduling in time-triggered architectures has been crucial in adapting\nto dynamic and unpredictable environments, ensuring the reliability and\nefficiency of task execution. However, traditional approaches face significant\nchallenges when training Artificial Intelligence (AI) scheduling inferences\noffline, particularly due to the complexities involved in constructing a\ncomprehensive Multi-Schedule Graph (MSG) that accounts for all possible\nscenarios. The process of generating an MSG that captures the vast probability\nspace, especially when considering context events like hardware failures, slack\nvariations, or mode changes, is resource-intensive and often infeasible. To\naddress these challenges, we propose an adaptive online learning unit\nintegrated within the metascheduler to enhance performance in real-time. The\nprimary motivation for developing this unit stems from the limitations of\noffline training, where the MSG created is inherently a subset of the complete\nspace, focusing only on the most probable and critical context events. In the\nonline mode, Reinforcement Learning (RL) plays a pivotal role by continuously\nexploring and discovering new scheduling solutions, thus expanding the MSG and\nenhancing system performance over time. This dynamic adaptation allows the\nsystem to handle unexpected events and complex scheduling scenarios more\neffectively. Several RL models were implemented within the online learning\nunit, each designed to address specific challenges in scheduling. These models\nnot only facilitate the discovery of new solutions but also optimize existing\nschedulers, particularly when stricter deadlines or new performance criteria\nare introduced. By continuously refining the AI inferences through real-time\ntraining, the system remains flexible and capable of meeting evolving demands,\nthus ensuring robustness and efficiency in large-scale, safety-critical\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5728\u5143\u8c03\u5ea6\u5668\u4e2d\u7684\u81ea\u9002\u5e94\u5728\u7ebf\u5b66\u4e60\u5355\u5143\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u52a8\u6001\u6269\u5c55\u591a\u8c03\u5ea6\u56fe\uff0c\u4ee5\u89e3\u51b3\u79bb\u7ebf\u8bad\u7ec3\u4e2d\u65e0\u6cd5\u8986\u76d6\u6240\u6709\u53ef\u80fd\u573a\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u79bb\u7ebf\u8bad\u7ec3\u65b9\u6cd5\u5728\u6784\u5efa\u5168\u9762\u7684\u591a\u8c03\u5ea6\u56fe\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u751f\u6210\u5305\u542b\u6240\u6709\u53ef\u80fd\u573a\u666f\u7684\u6982\u7387\u7a7a\u95f4\u8ba1\u7b97\u91cf\u5927\u4e14\u4e0d\u53ef\u884c\u3002\u5728\u7ebf\u5b66\u4e60\u53ef\u4ee5\u6301\u7eed\u63a2\u7d22\u65b0\u8c03\u5ea6\u65b9\u6848\uff0c\u5904\u7406\u610f\u5916\u4e8b\u4ef6\u3002", "method": "\u5728\u5143\u8c03\u5ea6\u5668\u4e2d\u96c6\u6210\u81ea\u9002\u5e94\u5728\u7ebf\u5b66\u4e60\u5355\u5143\uff0c\u4f7f\u7528\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u6765\u53d1\u73b0\u65b0\u8c03\u5ea6\u65b9\u6848\u5e76\u4f18\u5316\u73b0\u6709\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u5b9e\u65f6\u8bad\u7ec3\u6301\u7eed\u6539\u8fdbAI\u63a8\u7406\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u52a8\u6001\u9002\u5e94\u610f\u5916\u4e8b\u4ef6\u548c\u590d\u6742\u8c03\u5ea6\u573a\u666f\uff0c\u6269\u5c55\u591a\u8c03\u5ea6\u56fe\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\uff0c\u5728\u4e25\u683c\u622a\u6b62\u671f\u9650\u6216\u65b0\u6027\u80fd\u6807\u51c6\u4e0b\u4f18\u5316\u8c03\u5ea6\u5668\u3002", "conclusion": "\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u7cfb\u7edf\u4fdd\u6301\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u6ee1\u8db3\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\uff0c\u786e\u4fdd\u5927\u89c4\u6a21\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.20705", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20705", "abs": "https://arxiv.org/abs/2509.20705", "authors": ["Reza Akhavian", "Mani Amani", "Johannes Mootz", "Robert Ashe", "Behrad Beheshti"], "title": "Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework", "comment": null, "summary": "The adoption of cyber-physical systems and jobsite intelligence that connects\ndesign models, real-time site sensing, and autonomous field operations can\ndramatically enhance digital management in the construction industry. This\npaper introduces BIM2RDT (Building Information Models to Robot-Ready Site\nDigital Twins), an agentic artificial intelligence (AI) framework designed to\ntransform static Building Information Modeling (BIM) into dynamic, robot-ready\ndigital twins (DTs) that prioritize safety during execution. The framework\nbridges the gap between pre-existing BIM data and real-time site conditions by\nintegrating three key data streams: geometric and semantic information from BIM\nmodels, activity data from IoT sensor networks, and visual-spatial data\ncollected by robots during site traversal. The methodology introduces\nSemantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that\nleverages large language model (LLM) reasoning. Unlike traditional methods,\nSG-ICP utilizes an LLM to infer object-specific, plausible orientation priors\nbased on BIM semantics, improving alignment accuracy by avoiding convergence on\nlocal minima. This creates a feedback loop where robot-collected data updates\nthe DT, which in turn optimizes paths for missions. The framework employs YOLOE\nobject detection and Shi-Tomasi corner detection to identify and track\nconstruction elements while using BIM geometry as a priori maps. The framework\nalso integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping\nsensor-detected safety events to the digital twin using IFC standards for\nintervention. Experiments demonstrate SG-ICP's superiority over standard ICP,\nachieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with\noccluded features, ensuring plausible orientations. HAV integration triggers\nwarnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.", "AI": {"tldr": "BIM2RDT\u6846\u67b6\u901a\u8fc7AI\u5c06\u9759\u6001BIM\u8f6c\u6362\u4e3a\u52a8\u6001\u673a\u5668\u4eba\u5c31\u7eea\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u96c6\u6210BIM\u6570\u636e\u3001IoT\u4f20\u611f\u5668\u548c\u673a\u5668\u4eba\u89c6\u89c9\u6570\u636e\uff0c\u4f7f\u7528SG-ICP\u7b97\u6cd5\u63d0\u5347\u70b9\u4e91\u914d\u51c6\u7cbe\u5ea6\uff0c\u5e76\u6574\u5408\u5b9e\u65f6\u5b89\u5168\u76d1\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5efa\u7b51\u884c\u4e1a\u73b0\u6709BIM\u6570\u636e\u4e0e\u5b9e\u65f6\u73b0\u573a\u6761\u4ef6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u6570\u5b57\u7ba1\u7406\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u901a\u8fc7\u52a8\u6001\u6570\u5b57\u5b6a\u751f\u4f18\u5316\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u3002", "method": "\u63d0\u51faBIM2RDT\u6846\u67b6\uff0c\u96c6\u6210BIM\u51e0\u4f55\u8bed\u4e49\u4fe1\u606f\u3001IoT\u6d3b\u52a8\u6570\u636e\u548c\u673a\u5668\u4eba\u89c6\u89c9\u7a7a\u95f4\u6570\u636e\uff1b\u5f00\u53d1SG-ICP\u70b9\u4e91\u914d\u51c6\u7b97\u6cd5\uff0c\u5229\u7528LLM\u63a8\u7406\u5bf9\u8c61\u65b9\u5411\u5148\u9a8c\uff1b\u91c7\u7528YOLOE\u76ee\u6807\u68c0\u6d4b\u548cShi-Tomasi\u89d2\u70b9\u68c0\u6d4b\u8ddf\u8e2a\u5efa\u7b51\u5143\u7d20\uff1b\u6574\u5408\u5b9e\u65f6\u624b\u90e8\u632f\u52a8\u76d1\u6d4b\u3002", "result": "SG-ICP\u76f8\u6bd4\u6807\u51c6ICP\u5728\u7279\u5f81\u906e\u6321\u573a\u666f\u4e2d\u914d\u51c6RMSE\u964d\u4f4e64.3%-88.3%\uff1b\u5b9e\u65f6HAV\u76d1\u6d4b\u5728\u8d85\u8fc7\u66b4\u9732\u9650\u503c\u65f6\u89e6\u53d1\u8b66\u544a\uff0c\u7b26\u5408ISO 5349-1\u6807\u51c6\u3002", "conclusion": "BIM2RDT\u6846\u67b6\u6210\u529f\u5c06\u9759\u6001BIM\u8f6c\u5316\u4e3a\u52a8\u6001\u673a\u5668\u4eba\u5c31\u7eea\u6570\u5b57\u5b6a\u751f\uff0c\u663e\u8457\u63d0\u5347\u73b0\u573a\u6570\u636e\u914d\u51c6\u7cbe\u5ea6\u548c\u5b89\u5168\u7ba1\u7406\u80fd\u529b\uff0c\u4e3a\u5efa\u7b51\u884c\u4e1a\u6570\u5b57\u5316\u7ba1\u7406\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20785", "abs": "https://arxiv.org/abs/2509.20785", "authors": ["Jincai Song", "Haipeng Chen", "Jun Qin", "Na Zhao"], "title": "Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization", "comment": "13 pages, 14 figures", "summary": "Semi-supervised domain generalization (SSDG) in medical image segmentation\noffers a promising solution for generalizing to unseen domains during testing,\naddressing domain shift challenges and minimizing annotation costs. However,\nconventional SSDG methods assume labeled and unlabeled data are available for\neach source domain in the training set, a condition that is not always met in\npractice. The coexistence of limited annotation and domain shift in the\ntraining set is a prevalent issue. Thus, this paper explores a more practical\nand challenging scenario, cross-domain semi-supervised domain generalization\n(CD-SSDG), where domain shifts occur between labeled and unlabeled training\ndata, in addition to shifts between training and testing sets. Existing SSDG\nmethods exhibit sub-optimal performance under such domain shifts because of\ninaccurate pseudolabels. To address this issue, we propose a novel\ndual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG.\nBuilding upon the co-training paradigm with two sub-models offering cross\npseudo supervision, our DAC framework integrates extra feature-level\nsupervision and asymmetric auxiliary tasks for each sub-model. This\nfeature-level supervision serves to address inaccurate pseudo supervision\ncaused by domain shifts between labeled and unlabeled data, utilizing\ncomplementary supervision from the rich feature space. Additionally, two\ndistinct auxiliary self-supervised tasks are integrated into each sub-model to\nenhance domain-invariant discriminative feature learning and prevent model\ncollapse. Extensive experiments on real-world medical image segmentation\ndatasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust\ngeneralizability of the proposed DAC framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8de8\u57df\u534a\u76d1\u7763\u57df\u6cdb\u5316(CD-SSDG)\u7684\u53cc\u76d1\u7763\u975e\u5bf9\u79f0\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u6ce8\u6570\u636e\u6709\u9650\u548c\u57df\u504f\u79fb\u540c\u65f6\u5b58\u5728\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\u5047\u8bbe\u6bcf\u4e2a\u6e90\u57df\u90fd\u6709\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u4e2d\u6807\u6ce8\u6570\u636e\u6709\u9650\u4e14\u5b58\u5728\u57df\u504f\u79fb\u3002\u672c\u6587\u63a2\u7d22\u66f4\u5b9e\u7528\u7684\u573a\u666f\uff0c\u5373\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u57df\u504f\u79fb\u3002", "method": "\u63d0\u51fa\u53cc\u76d1\u7763\u975e\u5bf9\u79f0\u534f\u540c\u8bad\u7ec3(DAC)\u6846\u67b6\uff0c\u57fa\u4e8e\u53cc\u5b50\u6a21\u578b\u534f\u540c\u8bad\u7ec3\uff0c\u96c6\u6210\u7279\u5f81\u7ea7\u76d1\u7763\u548c\u975e\u5bf9\u79f0\u8f85\u52a9\u4efb\u52a1\uff0c\u89e3\u51b3\u4f2a\u6807\u7b7e\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "result": "\u5728Fundus\u3001Polyp\u548cSCGM\u7b49\u771f\u5b9e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0DAC\u6846\u67b6\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DAC\u6846\u67b6\u901a\u8fc7\u7279\u5f81\u7ea7\u76d1\u7763\u548c\u8f85\u52a9\u81ea\u76d1\u7763\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86CD-SSDG\u4e2d\u7684\u4f2a\u6807\u7b7e\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u57df\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.20641", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20641", "abs": "https://arxiv.org/abs/2509.20641", "authors": ["Giovana Morais", "Magdalena Fuentes"], "title": "Investigating Modality Contribution in Audio LLMs for Music", "comment": null, "summary": "Audio Large Language Models (Audio LLMs) enable human-like conversation about\nmusic, yet it is unclear if they are truly listening to the audio or just using\ntextual reasoning, as recent benchmarks suggest. This paper investigates this\nissue by quantifying the contribution of each modality to a model's output. We\nadapt the MM-SHAP framework, a performance-agnostic score based on Shapley\nvalues that quantifies the relative contribution of each modality to a model's\nprediction. We evaluate two models on the MuChoMusic benchmark and find that\nthe model with higher accuracy relies more on text to answer questions, but\nfurther inspection shows that even if the overall audio contribution is low,\nmodels can successfully localize key sound events, suggesting that audio is not\nentirely ignored. Our study is the first application of MM-SHAP to Audio LLMs\nand we hope it will serve as a foundational step for future research in\nexplainable AI and audio.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.21266", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21266", "abs": "https://arxiv.org/abs/2509.21266", "authors": ["Zijian Shao", "Haiyang Shen", "Mugeng Liu", "Gecheng Fu", "Yaoqi Guo", "Yanfeng Wang", "Yun Ma"], "title": "Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support", "comment": "under review", "summary": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53cd\u5c04\u8ba4\u77e5\u67b6\u6784\uff08RCA\uff09\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2aLLM\u4ece\u76f4\u63a5\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u548c\u9ad8\u8d28\u91cf\u89e3\u91ca\u7684\u5e73\u8861\uff0c\u5728\u533b\u7597\u75be\u75c5\u9884\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u548c\u900f\u660e\u3001\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u89e3\u91ca\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6df1\u5ea6\u7406\u89e3\u6570\u636e\u5e76\u4ea7\u751f\u53ef\u4fe1\u89e3\u91ca\u7684\u6846\u67b6\u3002", "method": "RCA\u6846\u67b6\u5305\u542b\u8fed\u4ee3\u89c4\u5219\u7cbe\u70bc\u673a\u5236\uff08\u4ece\u9884\u6d4b\u9519\u8bef\u4e2d\u6539\u8fdb\u903b\u8f91\uff09\u548c\u5206\u5e03\u611f\u77e5\u89c4\u5219\u68c0\u67e5\u673a\u5236\uff08\u57fa\u4e8e\u6570\u636e\u96c6\u5168\u5c40\u7edf\u8ba1\u8fdb\u884c\u63a8\u7406\uff09\uff0c\u4f7f\u7528\u9884\u6d4b\u51c6\u786e\u6027\u4f5c\u4e3a\u9a71\u52a8\u6df1\u5ea6\u7406\u89e3\u7684\u4fe1\u53f7\u3002", "result": "\u57281\u4e2a\u79c1\u6709\u548c2\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd422\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0cRCA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u76f8\u5bf9\u6539\u8fdb\u9ad8\u8fbe40%\uff0c\u5e76\u80fd\u751f\u6210\u6e05\u6670\u3001\u903b\u8f91\u3001\u57fa\u4e8e\u8bc1\u636e\u4e14\u5e73\u8861\u7684\u89e3\u91ca\u3002", "conclusion": "RCA\u901a\u8fc7\u6784\u5efa\u5f3a\u5927\u7684\u6570\u636e\u5185\u90e8\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u8d28\u91cf\u89e3\u91ca\u662f\u76f8\u4e92\u4fc3\u8fdb\u7684\u76ee\u6807\uff0c\u5177\u6709\u521b\u5efa\u771f\u6b63\u53ef\u4fe1\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.20899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20899", "abs": "https://arxiv.org/abs/2509.20899", "authors": ["Patrick Knab", "Sascha Marton", "Philipp J. Schubert", "Drago Guggiana", "Christian Bartelt"], "title": "Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification", "comment": null, "summary": "Conceptual models such as Concept Bottleneck Models (CBMs) have driven\nsubstantial progress in improving interpretability for image classification by\nleveraging human-interpretable concepts. However, extending these models from\nstatic images to sequences of images, such as video data, introduces a\nsignificant challenge due to the temporal dependencies inherent in videos,\nwhich are essential for capturing actions and events. In this work, we\nintroduce MoTIF (Moving Temporal Interpretable Framework), an architectural\ndesign inspired by a transformer that adapts the concept bottleneck framework\nfor video classification and handles sequences of arbitrary length. Within the\nvideo domain, concepts refer to semantic entities such as objects, attributes,\nor higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across\ntime - forming motifs collectively describing and explaining actions. Our\ndesign explicitly enables three complementary perspectives: global concept\nimportance across the entire video, local concept relevance within specific\nwindows, and temporal dependencies of a concept over time. Our results\ndemonstrate that the concept-based modeling paradigm can be effectively\ntransferred to video data, enabling a better understanding of concept\ncontributions in temporal contexts while maintaining competitive performance.\nCode available at github.com/patrick-knab/MoTIF.", "AI": {"tldr": "MoTIF\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u67b6\u6784\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u4e13\u95e8\u4e3a\u89c6\u9891\u5206\u7c7b\u8bbe\u8ba1\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u901a\u8fc7\u5168\u5c40\u6982\u5ff5\u91cd\u8981\u6027\u3001\u5c40\u90e8\u6982\u5ff5\u76f8\u5173\u6027\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u4e09\u4e2a\u89c6\u89d2\u6765\u589e\u5f3a\u89c6\u9891\u52a8\u4f5c\u7406\u89e3\u7684\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u56fe\u50cf\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u89c6\u9891\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u800c\u65f6\u95f4\u4f9d\u8d56\u5bf9\u4e8e\u6355\u6349\u52a8\u4f5c\u548c\u4e8b\u4ef6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528transformer\u67b6\u6784\uff0c\u5c06\u6982\u5ff5\u74f6\u9888\u6846\u67b6\u6269\u5c55\u5230\u89c6\u9891\u5206\u7c7b\uff0c\u901a\u8fc7\u8bc6\u522b\u8de8\u65f6\u95f4\u91cd\u590d\u51fa\u73b0\u7684\u8bed\u4e49\u5b9e\u4f53\uff08\u5982\u5bf9\u8c61\u3001\u5c5e\u6027\u3001\u52a8\u4f5c\u7ec4\u4ef6\uff09\u6765\u5f62\u6210\u63cf\u8ff0\u52a8\u4f5c\u7684motif\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6982\u5ff5\u5efa\u6a21\u7684\u8303\u5f0f\u53ef\u4ee5\u6709\u6548\u5730\u8fc1\u79fb\u5230\u89c6\u9891\u6570\u636e\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u540c\u65f6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u6982\u5ff5\u5728\u65f6\u95f4\u4e0a\u4e0b\u6587\u4e2d\u7684\u8d21\u732e\u3002", "conclusion": "MoTIF\u6210\u529f\u5730\u5c06\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u6269\u5c55\u5230\u89c6\u9891\u9886\u57df\uff0c\u4e3a\u89c6\u9891\u52a8\u4f5c\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u89e3\u91ca\u6027\u89c6\u89d2\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2509.20793", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20793", "abs": "https://arxiv.org/abs/2509.20793", "authors": ["Zhengxiao Li", "Liming Lu", "Xu Zheng", "Siyuan Liang", "Zhenghan Chen", "Yongbin Zhou", "Shuchao Pang"], "title": "FERD: Fairness-Enhanced Data-Free Robustness Distillation", "comment": null, "summary": "Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from\nthe teacher to the student without accessing the training data. While existing\nmethods focus on overall robustness, they overlook the robust fairness issues,\nleading to severe disparity of robustness across different categories. In this\npaper, we find two key problems: (1) student model distilled with equal class\nproportion data behaves significantly different across distinct categories; and\n(2) the robustness of student model is not stable across different attacks\ntarget. To bridge these gaps, we present the first Fairness-Enhanced data-free\nRobustness Distillation (FERD) framework to adjust the proportion and\ndistribution of adversarial examples. For the proportion, FERD adopts a\nrobustness-guided class reweighting strategy to synthesize more samples for the\nless robust categories, thereby improving robustness of them. For the\ndistribution, FERD generates complementary data samples for advanced robustness\ndistillation. It generates Fairness-Aware Examples (FAEs) by enforcing a\nuniformity constraint on feature-level predictions, which suppress the\ndominance of class-specific non-robust features, providing a more balanced\nrepresentation across all categories. Then, FERD constructs Uniform-Target\nAdversarial Examples (UTAEs) from FAEs by applying a uniform target class\nconstraint to avoid biased attack directions, which distribute the attack\ntargets across all categories and prevents overfitting to specific vulnerable\ncategories. Extensive experiments on three public datasets show that FERD\nachieves state-of-the-art worst-class robustness under all adversarial attack\n(e.g., the worst-class robustness under FGSM and AutoAttack are improved by\n15.1\\% and 6.4\\% using MobileNet-V2 on CIFAR-10), demonstrating superior\nperformance in both robustness and fairness aspects.", "AI": {"tldr": "\u63d0\u51fa\u4e86FERD\u6846\u67b6\uff0c\u9996\u4e2a\u5173\u6ce8\u516c\u5e73\u6027\u7684\u6570\u636e\u81ea\u7531\u9c81\u68d2\u6027\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u5bf9\u6297\u6837\u672c\u7684\u6bd4\u4f8b\u548c\u5206\u5e03\u6765\u89e3\u51b3\u4e0d\u540c\u7c7b\u522b\u95f4\u9c81\u68d2\u6027\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u81ea\u7531\u9c81\u68d2\u6027\u84b8\u998f\u65b9\u6cd5\u53ea\u5173\u6ce8\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u7c7b\u522b\u95f4\u7684\u9c81\u68d2\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u540c\u7c7b\u522b\u95f4\u7684\u9c81\u68d2\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "FERD\u91c7\u7528\u9c81\u68d2\u6027\u5f15\u5bfc\u7684\u7c7b\u522b\u91cd\u52a0\u6743\u7b56\u7565\u8c03\u6574\u6837\u672c\u6bd4\u4f8b\uff0c\u751f\u6210\u516c\u5e73\u611f\u77e5\u6837\u672c(FAEs)\u548c\u5747\u5300\u76ee\u6807\u5bf9\u6297\u6837\u672c(UTAEs)\u6765\u5e73\u8861\u5206\u5e03\uff0c\u901a\u8fc7\u7279\u5f81\u7ea7\u9884\u6d4b\u7684\u5747\u5300\u6027\u7ea6\u675f\u907f\u514d\u7c7b\u522b\u7279\u5b9a\u975e\u9c81\u68d2\u7279\u5f81\u7684\u652f\u914d\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFERD\u5728\u6240\u6709\u5bf9\u6297\u653b\u51fb\u4e0b\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6700\u5dee\u7c7b\u522b\u9c81\u68d2\u6027\uff0c\u5982\u5728CIFAR-10\u4e0a\u4f7f\u7528MobileNet-V2\u65f6\uff0cFGSM\u548cAutoAttack\u4e0b\u7684\u6700\u5dee\u7c7b\u522b\u9c81\u68d2\u6027\u5206\u522b\u63d0\u9ad8\u4e8615.1%\u548c6.4%\u3002", "conclusion": "FERD\u6846\u67b6\u5728\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u81ea\u7531\u9c81\u68d2\u6027\u84b8\u998f\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2509.20927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20927", "abs": "https://arxiv.org/abs/2509.20927", "authors": ["Akihisa Watanabe", "Jiawei Ren", "Li Siyao", "Yichen Peng", "Erwin Wu", "Edgar Simo-Serra"], "title": "SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation", "comment": null, "summary": "Generating physically plausible human motion is crucial for applications such\nas character animation and virtual reality. Existing approaches often\nincorporate a simulator-based motion projection layer to the diffusion process\nto enforce physical plausibility. However, such methods are computationally\nexpensive due to the sequential nature of the simulator, which prevents\nparallelization. We show that simulator-based motion projection can be\ninterpreted as a form of guidance, either classifier-based or classifier-free,\nwithin the diffusion process. Building on this insight, we propose SimDiff, a\nSimulator-constrained Diffusion Model that integrates environment parameters\n(e.g., gravity, wind) directly into the denoising process. By conditioning on\nthese parameters, SimDiff generates physically plausible motions efficiently,\nwithout repeated simulator calls at inference, and also provides fine-grained\ncontrol over different physical coefficients. Moreover, SimDiff successfully\ngeneralizes to unseen combinations of environmental parameters, demonstrating\ncompositional generalization.", "AI": {"tldr": "SimDiff\u662f\u4e00\u4e2a\u6a21\u62df\u5668\u7ea6\u675f\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u53c2\u6570\u76f4\u63a5\u96c6\u6210\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9700\u5728\u63a8\u7406\u65f6\u91cd\u590d\u8c03\u7528\u6a21\u62df\u5668\u5373\u53ef\u9ad8\u6548\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u5e76\u63d0\u4f9b\u5bf9\u7269\u7406\u7cfb\u6570\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u8fd0\u52a8\u6295\u5f71\u5c42\u6574\u5408\u5230\u6269\u6563\u8fc7\u7a0b\u4e2d\u4ee5\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\uff0c\u4f46\u7531\u4e8e\u6a21\u62df\u5668\u7684\u987a\u5e8f\u6027\u8d28\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u65e0\u6cd5\u5e76\u884c\u5316\u3002", "method": "\u5c06\u6a21\u62df\u5668\u8fd0\u52a8\u6295\u5f71\u89e3\u91ca\u4e3a\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u4e00\u79cd\u5f15\u5bfc\u5f62\u5f0f\uff0c\u63d0\u51faSimDiff\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u53c2\u6570\uff08\u5982\u91cd\u529b\u3001\u98ce\u529b\uff09\u76f4\u63a5\u6761\u4ef6\u5316\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u907f\u514d\u63a8\u7406\u65f6\u91cd\u590d\u7684\u6a21\u62df\u5668\u8c03\u7528\u3002", "result": "SimDiff\u80fd\u9ad8\u6548\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u8fd0\u52a8\uff0c\u63d0\u4f9b\u5bf9\u7269\u7406\u7cfb\u6570\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u80fd\u6210\u529f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u73af\u5883\u53c2\u6570\u7ec4\u5408\uff0c\u5c55\u793a\u4e86\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SimDiff\u901a\u8fc7\u5c06\u6a21\u62df\u5668\u7ea6\u675f\u6574\u5408\u4e3a\u6269\u6563\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u7269\u7406\u5408\u7406\u8fd0\u52a8\u751f\u6210\uff0c\u540c\u65f6\u5177\u5907\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.20840", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20840", "abs": "https://arxiv.org/abs/2509.20840", "authors": ["Jiaqi Tang", "Yinsong Xu", "Yang Liu", "Qingchao Chen"], "title": "Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition", "comment": null, "summary": "Multi-modal fusion often suffers from modality competition during joint\ntraining, where one modality dominates the learning process, leaving others\nunder-optimized. Overlooking the critical impact of the model's initial state,\nmost existing methods address this issue during the joint learning stage. In\nthis study, we introduce a two-stage training framework to shape the initial\nstates through unimodal training before the joint training. First, we propose\nthe concept of Effective Competitive Strength (ECS) to quantify a modality's\ncompetitive strength. Our theoretical analysis further reveals that properly\nshaping the initial ECS by unimodal training achieves a provably tighter error\nbound. However, ECS is computationally intractable in deep neural networks. To\nbridge this gap, we develop a framework comprising two core components: a\nfine-grained computable diagnostic metric and an asynchronous training\ncontroller. For the metric, we first prove that mutual information(MI) is a\nprincipled proxy for ECS. Considering MI is induced by per-modality marginals\nand thus treats each modality in isolation, we further propose FastPID, a\ncomputationally efficient and differentiable solver for partial information\ndecomposition, which decomposes the joint distribution's information into\nfine-grained measurements: modality-specific uniqueness, redundancy, and\nsynergy. Guided by these measurements, our asynchronous controller dynamically\nbalances modalities by monitoring uniqueness and locates the ideal initial\nstate to start joint training by tracking peak synergy. Experiments on diverse\nbenchmarks demonstrate that our method achieves state-of-the-art performance.\nOur work establishes that shaping the pre-fusion models' initial state is a\npowerful strategy that eases competition before it starts, reliably unlocking\nsynergistic multi-modal fusion.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6a21\u6001\u7ade\u4e89\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u6a21\u6001\u8bad\u7ec3\u5851\u9020\u521d\u59cb\u72b6\u6001\uff0c\u5f15\u5165\u6709\u6548\u7ade\u4e89\u5f3a\u5ea6\u6982\u5ff5\u548c\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u53ef\u8ba1\u7b97\u8bca\u65ad\u6307\u6807\uff0c\u5b9e\u73b0\u52a8\u6001\u5e73\u8861\u6a21\u6001\u7ade\u4e89\u3002", "motivation": "\u591a\u6a21\u6001\u878d\u5408\u4e2d\u5e38\u51fa\u73b0\u6a21\u6001\u7ade\u4e89\u95ee\u9898\uff0c\u4e00\u4e2a\u6a21\u6001\u4e3b\u5bfc\u5b66\u4e60\u8fc7\u7a0b\u800c\u5176\u4ed6\u6a21\u6001\u4f18\u5316\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5728\u8054\u5408\u5b66\u4e60\u9636\u6bb5\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u6a21\u578b\u521d\u59cb\u72b6\u6001\u7684\u5173\u952e\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u5355\u6a21\u6001\u8bad\u7ec3\u9636\u6bb5\u5851\u9020\u521d\u59cb\u72b6\u6001\uff1b2\uff09\u5f15\u5165\u6709\u6548\u7ade\u4e89\u5f3a\u5ea6\u6982\u5ff5\u91cf\u5316\u6a21\u6001\u7ade\u4e89\u529b\uff1b3\uff09\u5f00\u53d1\u53ef\u8ba1\u7b97\u8bca\u65ad\u6307\u6807FastPID\u5206\u89e3\u4fe1\u606f\u4e3a\u72ec\u7279\u6027\u3001\u5197\u4f59\u6027\u548c\u534f\u540c\u6027\uff1b4\uff09\u5f02\u6b65\u8bad\u7ec3\u63a7\u5236\u5668\u52a8\u6001\u5e73\u8861\u6a21\u6001\u5e76\u5b9a\u4f4d\u7406\u60f3\u521d\u59cb\u72b6\u6001\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u901a\u8fc7\u5851\u9020\u9884\u878d\u5408\u6a21\u578b\u7684\u521d\u59cb\u72b6\u6001\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u6a21\u6001\u7ade\u4e89\uff0c\u53ef\u9760\u5730\u5b9e\u73b0\u534f\u540c\u591a\u6a21\u6001\u878d\u5408\u3002", "conclusion": "\u5851\u9020\u9884\u878d\u5408\u6a21\u578b\u7684\u521d\u59cb\u72b6\u6001\u662f\u4e00\u79cd\u5f3a\u5927\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u7ade\u4e89\u5f00\u59cb\u524d\u7f13\u89e3\u6a21\u6001\u7ade\u4e89\uff0c\u53ef\u9760\u5730\u5b9e\u73b0\u534f\u540c\u591a\u6a21\u6001\u878d\u5408\u3002"}}
{"id": "2509.21155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21155", "abs": "https://arxiv.org/abs/2509.21155", "authors": ["Chantal Shaib", "Vinith M. Suriyakumar", "Levent Sagun", "Byron C. Wallace", "Marzyeh Ghassemi"], "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models", "comment": "NeurIPS 2025 Spotlight", "summary": "For an LLM to correctly respond to an instruction it must understand both the\nsemantics and the domain (i.e., subject area) of a given task-instruction pair.\nHowever, syntax can also convey implicit information Recent work shows that\nsyntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are\nprevalent in training data and often appear in model outputs. In this work we\ncharacterize syntactic templates, domain, and semantics in task-instruction\npairs. We identify cases of spurious correlations between syntax and domain,\nwhere models learn to associate a domain with syntax during training; this can\nsometimes override prompt semantics. Using a synthetic training dataset, we\nfind that the syntactic-domain correlation can lower performance (mean 0.51 +/-\n0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an\nevaluation framework to detect this phenomenon in trained models, and show that\nit occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;\nLlama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study\non the implications for safety finetuning, showing that unintended\nsyntactic-domain correlations can be used to bypass refusals in OLMo-2-7B\nInstruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test\nfor syntactic-domain correlations, and (2) to ensure syntactic diversity in\ntraining data, specifically within domains, to prevent such spurious\ncorrelations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0LLM\u8bad\u7ec3\u6570\u636e\u4e2d\u5b58\u5728\u53e5\u6cd5\u6a21\u677f\u4e0e\u9886\u57df\u4e4b\u95f4\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u8fd9\u79cd\u76f8\u5173\u6027\u4f1a\u5e72\u6270\u6a21\u578b\u5bf9\u6307\u4ee4\u8bed\u4e49\u7684\u7406\u89e3\uff0c\u751a\u81f3\u53ef\u80fd\u88ab\u7528\u4e8e\u7ed5\u8fc7\u5b89\u5168\u5fae\u8c03\u673a\u5236\u3002", "motivation": "LLM\u9700\u8981\u7406\u89e3\u6307\u4ee4\u7684\u8bed\u4e49\u548c\u9886\u57df\u624d\u80fd\u6b63\u786e\u54cd\u5e94\uff0c\u4f46\u53e5\u6cd5\u4e5f\u80fd\u4f20\u9012\u9690\u542b\u4fe1\u606f\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\u53e5\u6cd5\u6a21\u677f\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5e76\u7ecf\u5e38\u51fa\u73b0\u5728\u6a21\u578b\u8f93\u51fa\u4e2d\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u53e5\u6cd5\u6a21\u677f\u3001\u9886\u57df\u548c\u8bed\u4e49\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5408\u6210\u8bad\u7ec3\u6570\u636e\u96c6\u7814\u7a76\u53e5\u6cd5-\u9886\u57df\u76f8\u5173\u6027\u5bf9OLMo-2\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u5f00\u53d1\u8bc4\u4f30\u6846\u67b6\u68c0\u6d4b\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u8fd9\u79cd\u73b0\u8c61\uff1b\u5728FlanV2\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff1b\u8fdb\u884c\u5b89\u5168\u5fae\u8c03\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u53e5\u6cd5-\u9886\u57df\u76f8\u5173\u6027\u663e\u8457\u964d\u4f4e\u4e86OLMo-2\u6a21\u578b\u5728\u5b9e\u4f53\u77e5\u8bc6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff08\u5747\u503c0.51\u00b10.06\uff09\uff1b\u5728OLMo-2-7B\u3001Llama-4-Maverick\u548cGPT-4o\u4e2d\u5747\u68c0\u6d4b\u5230\u8fd9\u79cd\u73b0\u8c61\uff1b\u610f\u5916\u53e5\u6cd5-\u9886\u57df\u76f8\u5173\u6027\u53ef\u88ab\u7528\u4e8e\u7ed5\u8fc7OLMo-2-7B Instruct\u548cGPT-4o\u7684\u62d2\u7edd\u673a\u5236\u3002", "conclusion": "\u9700\u8981\u660e\u786e\u6d4b\u8bd5\u53e5\u6cd5-\u9886\u57df\u76f8\u5173\u6027\uff0c\u5e76\u786e\u4fdd\u8bad\u7ec3\u6570\u636e\u4e2d\u7279\u522b\u662f\u9886\u57df\u5185\u7684\u53e5\u6cd5\u591a\u6837\u6027\uff0c\u4ee5\u9632\u6b62\u6b64\u7c7b\u865a\u5047\u76f8\u5173\u6027\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u5b89\u5168\u3002"}}
{"id": "2509.21008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21008", "abs": "https://arxiv.org/abs/2509.21008", "authors": ["Qinqin He", "Jiaqi Weng", "Jialing Tao", "Hui Xue"], "title": "A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image models exhibit remarkable capabilities in image generation.\nHowever, they also pose safety risks of generating harmful content. A key\nchallenge of existing concept erasure methods is the precise removal of target\nconcepts while minimizing degradation of image quality. In this paper, we\npropose Single Neuron-based Concept Erasure (SNCE), a novel approach that can\nprecisely prevent harmful content generation by manipulating only a single\nneuron. Specifically, we train a Sparse Autoencoder (SAE) to map text\nembeddings into a sparse, disentangled latent space, where individual neurons\nalign tightly with atomic semantic concepts. To accurately locate neurons\nresponsible for harmful concepts, we design a novel neuron identification\nmethod based on the modulated frequency scoring of activation patterns. By\nsuppressing activations of the harmful concept-specific neuron, SNCE achieves\nsurgical precision in concept erasure with minimal disruption to image quality.\nExperiments on various benchmarks demonstrate that SNCE achieves\nstate-of-the-art results in target concept erasure, while preserving the\nmodel's generation capabilities for non-target concepts. Additionally, our\nmethod exhibits strong robustness against adversarial attacks, significantly\noutperforming existing methods.", "AI": {"tldr": "\u63d0\u51faSNCE\u65b9\u6cd5\uff0c\u901a\u8fc7\u64cd\u7eb5\u5355\u4e2a\u795e\u7ecf\u5143\u7cbe\u786e\u9632\u6b62\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u6982\u5ff5\u64e6\u9664", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b58\u5728\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u96be\u4ee5\u5728\u7cbe\u786e\u79fb\u9664\u76ee\u6807\u6982\u5ff5\u7684\u540c\u65f6\u6700\u5c0f\u5316\u56fe\u50cf\u8d28\u91cf\u9000\u5316", "method": "\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5c06\u6587\u672c\u5d4c\u5165\u6620\u5c04\u5230\u7a00\u758f\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u8c03\u5236\u9891\u7387\u8bc4\u5206\u8bc6\u522b\u6709\u5bb3\u6982\u5ff5\u5bf9\u5e94\u7684\u795e\u7ecf\u5143\uff0c\u6291\u5236\u5176\u6fc0\u6d3b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u76ee\u6807\u6982\u5ff5\u64e6\u9664\u6548\u679c\uff0c\u4fdd\u6301\u975e\u76ee\u6807\u6982\u5ff5\u7684\u751f\u6210\u80fd\u529b\uff0c\u5bf9\u5bf9\u6297\u653b\u51fb\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027", "conclusion": "SNCE\u65b9\u6cd5\u80fd\u591f\u4ee5\u624b\u672f\u7cbe\u5ea6\u5b9e\u73b0\u6982\u5ff5\u64e6\u9664\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u9632\u6b62\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2509.21265", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21265", "abs": "https://arxiv.org/abs/2509.21265", "authors": ["Xinyu Liu", "Guolei Sun", "Cheng Wang", "Yixuan Yuan", "Ender Konukoglu"], "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation", "comment": "ICCV 2025", "summary": "High-resolution (HR) medical videos are vital for accurate diagnosis, yet are\nhard to acquire due to hardware limitations and physiological constraints.\nClinically, the collected low-resolution (LR) medical videos present unique\nchallenges for video super-resolution (VSR) models, including camera shake,\nnoise, and abrupt frame transitions, which result in significant optical flow\nerrors and alignment difficulties. Additionally, tissues and organs exhibit\ncontinuous and nuanced structures, but current VSR models are prone to\nintroducing artifacts and distorted features that can mislead doctors. To this\nend, we propose MedVSR, a tailored framework for medical VSR. It first employs\nCross State-Space Propagation (CSSP) to address the imprecise alignment by\nprojecting distant frames as control matrices within state-space models,\nenabling the selective propagation of consistent and informative features to\nneighboring frames for effective alignment. Moreover, we design an Inner\nState-Space Reconstruction (ISSR) module that enhances tissue structures and\nreduces artifacts with joint long-range spatial feature learning and\nlarge-kernel short-range information aggregation. Experiments across four\ndatasets in diverse medical scenarios, including endoscopy and cataract\nsurgeries, show that MedVSR significantly outperforms existing VSR models in\nreconstruction performance and efficiency. Code released at\nhttps://github.com/CUHK-AIM-Group/MedVSR.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedVSR\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u533b\u5b66\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u901a\u8fc7\u4ea4\u53c9\u72b6\u6001\u7a7a\u95f4\u4f20\u64ad\u548c\u5185\u90e8\u72b6\u6001\u7a7a\u95f4\u91cd\u5efa\u6a21\u5757\u89e3\u51b3\u533b\u5b66\u89c6\u9891\u7279\u6709\u7684\u5bf9\u9f50\u56f0\u96be\u548c\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u533b\u5b66\u89c6\u9891\u5bf9\u51c6\u786e\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u786c\u4ef6\u9650\u5236\u548c\u751f\u7406\u7ea6\u675f\u4f7f\u5176\u96be\u4ee5\u83b7\u53d6\u3002\u73b0\u6709VSR\u6a21\u578b\u5728\u5904\u7406\u533b\u5b66\u89c6\u9891\u65f6\u9762\u4e34\u76f8\u673a\u6296\u52a8\u3001\u566a\u58f0\u3001\u5e27\u95f4\u7a81\u53d8\u7b49\u72ec\u7279\u6311\u6218\uff0c\u5bfc\u81f4\u5149\u6d41\u8bef\u5dee\u5927\u3001\u5bf9\u9f50\u56f0\u96be\uff0c\u4e14\u5bb9\u6613\u4ea7\u751f\u4f2a\u5f71\u548c\u7279\u5f81\u626d\u66f2\uff0c\u53ef\u80fd\u8bef\u5bfc\u533b\u751f\u8bca\u65ad\u3002", "method": "1. \u4ea4\u53c9\u72b6\u6001\u7a7a\u95f4\u4f20\u64ad(CSSP)\uff1a\u5c06\u8fdc\u8ddd\u79bb\u5e27\u6295\u5f71\u4e3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u63a7\u5236\u77e9\u9635\uff0c\u9009\u62e9\u6027\u4f20\u64ad\u4e00\u81f4\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\u5230\u76f8\u90bb\u5e27\uff0c\u5b9e\u73b0\u6709\u6548\u5bf9\u9f50\u30022. \u5185\u90e8\u72b6\u6001\u7a7a\u95f4\u91cd\u5efa(ISSR)\uff1a\u901a\u8fc7\u8054\u5408\u957f\u8ddd\u79bb\u7a7a\u95f4\u7279\u5f81\u5b66\u4e60\u548c\u5927\u6838\u77ed\u8ddd\u79bb\u4fe1\u606f\u805a\u5408\uff0c\u589e\u5f3a\u7ec4\u7ec7\u7ed3\u6784\u5e76\u51cf\u5c11\u4f2a\u5f71\u3002", "result": "\u5728\u5305\u62ec\u5185\u7aa5\u955c\u548c\u767d\u5185\u969c\u624b\u672f\u5728\u5185\u7684\u56db\u4e2a\u4e0d\u540c\u533b\u5b66\u573a\u666f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedVSR\u5728\u91cd\u5efa\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709VSR\u6a21\u578b\u3002", "conclusion": "MedVSR\u662f\u9488\u5bf9\u533b\u5b66\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u5b9a\u5236\u5316\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u533b\u5b66\u89c6\u9891\u7279\u6709\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2509.21161", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21161", "abs": "https://arxiv.org/abs/2509.21161", "authors": ["Giuseppe Serra", "Florian Buettner"], "title": "DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning", "comment": null, "summary": "Continual Learning (CL) is recently gaining increasing attention for its\nability to enable a single model to learn incrementally from a sequence of new\nclasses. In this scenario, it is important to keep consistent predictive\nperformance across all the classes and prevent the so-called Catastrophic\nForgetting (CF). However, in safety-critical applications, predictive\nperformance alone is insufficient. Predictive models should also be able to\nreliably communicate their uncertainty in a calibrated manner - that is, with\nconfidence scores aligned to the true frequencies of target events. Existing\napproaches in CL address calibration primarily from a data-centric perspective,\nrelying on a single temperature shared across all tasks. Such solutions\noverlook task-specific differences, leading to large fluctuations in\ncalibration error across tasks. For this reason, we argue that a more\nprincipled approach should adapt the temperature according to the distance to\nthe current task. However, the unavailability of the task information at test\ntime/during deployment poses a major challenge to achieve the intended\nobjective. For this, we propose Distance-Aware Temperature Scaling (DATS),\nwhich combines prototype-based distance estimation with distance-aware\ncalibration to infer task proximity and assign adaptive temperatures without\nprior task information. Through extensive empirical evaluation on both standard\nbenchmarks and real-world, imbalanced datasets taken from the biomedical\ndomain, our approach demonstrates to be stable, reliable and consistent in\nreducing calibration error across tasks compared to state-of-the-art\napproaches.", "AI": {"tldr": "\u63d0\u51faDATS\u65b9\u6cd5\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6821\u51c6\u95ee\u9898\uff0c\u901a\u8fc7\u8ddd\u79bb\u611f\u77e5\u7684\u6e29\u5ea6\u7f29\u653e\u5b9e\u73b0\u4efb\u52a1\u81ea\u9002\u5e94\u6821\u51c6\uff0c\u65e0\u9700\u4efb\u52a1\u4fe1\u606f\u5373\u53ef\u964d\u4f4e\u6821\u51c6\u8bef\u5dee", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4ece\u6570\u636e\u89d2\u5ea6\u89e3\u51b3\u6821\u51c6\u95ee\u9898\uff0c\u4f7f\u7528\u5355\u4e00\u6e29\u5ea6\u53c2\u6570\u5ffd\u7565\u4e86\u4efb\u52a1\u95f4\u5dee\u5f02\uff0c\u5bfc\u81f4\u4e0d\u540c\u4efb\u52a1\u7684\u6821\u51c6\u8bef\u5dee\u6ce2\u52a8\u8f83\u5927", "method": "DATS\u7ed3\u5408\u539f\u578b\u8ddd\u79bb\u4f30\u8ba1\u548c\u8ddd\u79bb\u611f\u77e5\u6821\u51c6\uff0c\u63a8\u65ad\u4efb\u52a1\u63a5\u8fd1\u5ea6\u5e76\u5206\u914d\u81ea\u9002\u5e94\u6e29\u5ea6\uff0c\u65e0\u9700\u5148\u9a8c\u4efb\u52a1\u4fe1\u606f", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u4e0d\u5e73\u8861\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u4efb\u52a1\u95f4\u6821\u51c6\u8bef\u5dee\u65b9\u9762\u66f4\u7a33\u5b9a\u3001\u53ef\u9760\u548c\u4e00\u81f4", "conclusion": "DATS\u65b9\u6cd5\u901a\u8fc7\u8ddd\u79bb\u611f\u77e5\u7684\u6e29\u5ea6\u7f29\u653e\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u80fd\u529b"}}
{"id": "2509.21250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21250", "abs": "https://arxiv.org/abs/2509.21250", "authors": ["Zifan Wang", "Anqi Dong", "Mahmoud Selim", "Michael M. Zavlanos", "Karl H. Johansson"], "title": "Federated Flow Matching", "comment": null, "summary": "Data today is decentralized, generated and stored across devices and\ninstitutions where privacy, ownership, and regulation prevent centralization.\nThis motivates the need to train generative models directly from distributed\ndata locally without central aggregation. In this paper, we introduce Federated\nFlow Matching (FFM), a framework for training flow matching models under\nprivacy constraints. Specifically, we first examine FFM-vanilla, where each\nclient trains locally with independent source and target couplings, preserving\nprivacy but yielding curved flows that slow inference. We then develop FFM-LOT,\nwhich employs local optimal transport couplings to improve straightness within\neach client but lacks global consistency under heterogeneous data. Finally, we\npropose FFM-GOT, a federated strategy based on the semi-dual formulation of\noptimal transport, where a shared global potential function coordinates\ncouplings across clients. Experiments on synthetic and image datasets show that\nFFM enables privacy-preserving training while enhancing both the flow\nstraightness and sample quality in federated settings, with performance\ncomparable to the centralized baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8054\u90a6\u6d41\u5339\u914d\uff08FFM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u8bad\u7ec3\u6d41\u5339\u914d\u6a21\u578b\uff0c\u5305\u62ecFFM-vanilla\u3001FFM-LOT\u548cFFM-GOT\u4e09\u79cd\u65b9\u6cd5\uff0c\u5176\u4e2dFFM-GOT\u901a\u8fc7\u5171\u4eab\u5168\u5c40\u52bf\u51fd\u6570\u534f\u8c03\u5ba2\u6237\u7aef\u95f4\u7684\u8026\u5408\uff0c\u5728\u8054\u90a6\u8bbe\u7f6e\u4e2d\u63d0\u9ad8\u4e86\u6d41\u76f4\u5ea6\u548c\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u5206\u6563\u5728\u8bbe\u5907\u548c\u673a\u6784\u4e2d\uff0c\u9690\u79c1\u3001\u6240\u6709\u6743\u548c\u6cd5\u89c4\u9650\u5236\u963b\u6b62\u4e86\u6570\u636e\u96c6\u4e2d\u5316\uff0c\u56e0\u6b64\u9700\u8981\u5728\u5206\u5e03\u5f0f\u6570\u636e\u4e0a\u76f4\u63a5\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u96c6\u4e2d\u805a\u5408\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u8054\u90a6\u6d41\u5339\u914d\u65b9\u6cd5\uff1aFFM-vanilla\uff08\u672c\u5730\u72ec\u7acb\u8bad\u7ec3\uff09\u3001FFM-LOT\uff08\u4f7f\u7528\u5c40\u90e8\u6700\u4f18\u4f20\u8f93\u8026\u5408\uff09\u548cFFM-GOT\uff08\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u534a\u5bf9\u5076\u516c\u5f0f\uff0c\u4f7f\u7528\u5171\u4eab\u5168\u5c40\u52bf\u51fd\u6570\u534f\u8c03\u5ba2\u6237\u7aef\u8026\u5408\uff09\u3002", "result": "\u5728\u5408\u6210\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFFM\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u8054\u90a6\u8bbe\u7f6e\u4e2d\u7684\u6d41\u76f4\u5ea6\u548c\u6837\u672c\u8d28\u91cf\uff0c\u6027\u80fd\u4e0e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "FFM\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u8bad\u7ec3\u6d41\u5339\u914d\u6a21\u578b\uff0cFFM-GOT\u65b9\u6cd5\u901a\u8fc7\u5168\u5c40\u534f\u8c03\u673a\u5236\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2509.21282", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21282", "abs": "https://arxiv.org/abs/2509.21282", "authors": ["Madeleine Dwyer", "Adam Sobey", "Adriane Chapman"], "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL", "comment": null, "summary": "Training large language models (LLMs) with reinforcement learning (RL)\nmethods such as PPO and GRPO commonly relies on ratio clipping to stabilise\nupdates. While effective at preventing instability, clipping discards\ninformation and introduces gradient discontinuities. We propose Probability\nSmoothing Policy Optimisation (PSPO), which smooths the current policy's\nprobabilities toward the old (behaviour) policy before computing the importance\nratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient\nsignal, while interpolation toward the old policy creates a soft trust region\nthat discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and\nQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset\ngeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO\n(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar\nperformance but improves the reasoning leading to clearer and more concise\nresponses which are more logical. Compared to clipped GRPO, GR-PSPO\nsubstantially improves performance both the 0.5B and 1.5B models, with a boost\nof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).", "AI": {"tldr": "\u63d0\u51faPSPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u6ed1\u5f53\u524d\u7b56\u7565\u6982\u7387\u5411\u65e7\u7b56\u7565\u63d2\u503c\u6765\u66ff\u4ee3\u4f20\u7edf\u7684\u6bd4\u7387\u88c1\u526a\uff0c\u5728\u4fdd\u6301\u68af\u5ea6\u4fe1\u53f7\u7684\u540c\u65f6\u521b\u5efa\u8f6f\u4fe1\u4efb\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5982PPO\u548cGRPO\u4f9d\u8d56\u6bd4\u7387\u88c1\u526a\u6765\u7a33\u5b9a\u66f4\u65b0\uff0c\u4f46\u88c1\u526a\u4f1a\u4e22\u5f03\u4fe1\u606f\u5e76\u5f15\u5165\u68af\u5ea6\u4e0d\u8fde\u7eed\u6027\uff0c\u9700\u8981\u66f4\u5e73\u6ed1\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6982\u7387\u5e73\u6ed1\u7b56\u7565\u4f18\u5316(PSPO)\uff0c\u5728\u8ba1\u7b97\u91cd\u8981\u6027\u6bd4\u7387\u524d\u5c06\u5f53\u524d\u7b56\u7565\u6982\u7387\u5411\u65e7\u7b56\u7565\u5e73\u6ed1\u63d2\u503c\uff0c\u7c7b\u4f3c\u4e8e\u6807\u7b7e\u5e73\u6ed1\uff0c\u5f62\u6210\u8f6f\u4fe1\u4efb\u533a\u57df\u3002", "result": "\u5728GSM8K\u6d4b\u8bd5\u96c6\u4e0a\uff0cGR-PSPO\u76f8\u6bd4\u88c1\u526aGRPO\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1a0.5B\u6a21\u578b\u4ece17.6%\u63d0\u5347\u523039.7%\uff0c1.5B\u6a21\u578b\u4ece37.8%\u63d0\u5347\u523059.4%\uff0c\u540c\u65f6\u751f\u6210\u66f4\u6e05\u6670\u3001\u7b80\u6d01\u548c\u903b\u8f91\u6027\u5f3a\u7684\u56de\u7b54\u3002", "conclusion": "PSPO\u901a\u8fc7\u6982\u7387\u5e73\u6ed1\u6709\u6548\u66ff\u4ee3\u6bd4\u7387\u88c1\u526a\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
