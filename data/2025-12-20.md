<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4是一个无需训练的四维时空检索增强推理框架，为视觉语言模型提供结构化终身记忆，通过构建4D知识数据库实现时空感知推理。


<details>
  <summary>Details</summary>
Motivation: 受人类能够通过构建持久结构化内部表征来感知和理解四维时空环境的能力启发，希望为视觉语言模型赋予类似的时空记忆和推理能力，使其能够在动态环境中进行更复杂的推理。

Method: R4框架持续构建4D知识数据库，将物体级语义描述锚定在度量空间和时间中，形成持久的世界模型。推理时将自然语言查询分解为语义、空间和时间键来检索相关观察，并将其整合到视觉语言模型的推理过程中。

Result: 在具身问答和导航基准测试中，R4在时空信息检索和推理方面显著优于基线方法，展示了无需训练即可实现时空推理的有效性。

Conclusion: R4为动态环境中的具身4D推理提供了新范式，通过结构化终身记忆系统增强了视觉语言模型的时空感知和推理能力，实现了无需训练的检索增强推理。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [2] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种基于准直器闪烁星型图案的事件相机标定方法，用于解决长距离高精度测量场景下的几何标定问题


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高动态范围和高时间分辨率等优势，但在长距离测量场景下的几何标定（包括内参和外参）仍然是一个重大挑战。需要同时满足长距离和高精度的双重测量要求。

Method: 使用带有闪烁星型图案的准直器进行标定。首先基于准直器的球体运动模型线性求解相机参数，然后通过非线性优化对这些参数进行高精度细化。

Result: 通过在不同条件下的综合真实世界实验验证，该方法在准确性和可靠性方面始终优于现有的事件相机标定方法。

Conclusion: 提出的基于准直器闪烁星型图案的事件相机标定方法能够有效解决长距离高精度测量场景下的几何标定问题，为事件相机的实际应用提供了可靠的技术支持。

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [3] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 提出CattleAct方法，通过将牛群行为交互分解为个体动作组合，利用对比学习在预训练动作潜空间上微调，实现从单张图像自动检测放牧牛群的行为交互。


<details>
  <summary>Details</summary>
Motivation: 智能畜牧业需要自动检测牛群行为交互（如发情检测），但现有研究主要关注人类交互检测，牛群交互检测面临缺乏全面行为数据集的挑战，因为放牧牛群的交互是罕见事件。

Method: 首先从大规模牛群动作数据集学习动作潜空间，然后通过对比学习在预训练潜空间上微调，将罕见交互嵌入到统一的动作-交互潜空间中，最后开发集成视频和GPS输入的实用系统。

Result: 在商业规模牧场上的实验表明，该方法相比基线实现了准确的行为交互检测，并开发了可用的实用系统。

Conclusion: CattleAct通过数据高效的方法解决了牛群交互检测的数据稀缺问题，为智能畜牧业提供了有效的自动交互检测解决方案。

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [4] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: C-DGPA提出了一种基于类中心的双对齐生成提示适应方法，通过双分支架构同时优化边缘分布对齐和条件分布对齐，解决了VLM在UDA任务中的领域差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有的提示调优策略主要关注边缘分布对齐，但忽略了条件分布差异，导致类原型错位和语义判别性下降等问题。需要一种能同时处理两种分布差异的方法来提升VLM在UDA任务中的性能。

Method: C-DGPA采用双分支架构：1）边缘分布对齐分支使用动态对抗训练框架来桥接边缘分布差异；2）条件分布对齐分支引入类映射机制（CMM），通过标准化语义提示理解和防止源域过依赖来对齐条件分布差异。

Result: 在OfficeHome、Office31和VisDA-2017等基准测试上的广泛实验验证了C-DGPA的优越性，在所有基准上都取得了新的最先进结果。

Conclusion: C-DGPA通过协同优化的双对齐策略，将领域知识有效整合到提示学习中，确保了领域不变且语义判别性的表示，显著提升了VLM在UDA任务中的性能。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [5] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 事件相机在自动驾驶中相比传统相机能更好地应对昼夜光照变化的域差距问题，无需额外调整即可保持性能一致性


<details>
  <summary>Details</summary>
Motivation: 传统相机在端到端驾驶中面临域差距问题，特别是当训练数据条件与部署环境不匹配时性能会大幅下降。本文针对昼夜光照差异这一具体域差距问题，探索事件相机作为替代方案的潜力。

Method: 提出使用事件相机替代传统相机，评估其在昼夜光照变化条件下的性能表现，并与灰度帧进行对比分析。

Result: 事件相机在不同光照条件下保持更一致的性能，其域偏移惩罚通常与灰度帧相当或更小，在跨域场景中提供更优越的基线性能。

Conclusion: 事件相机是应对自动驾驶中昼夜光照域差距问题的有效替代方案，能够在不需额外调整的情况下保持跨域性能一致性。

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [6] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出基于Grad-CAM的弱监督深度学习框架，用于肺炎分类和定位，仅需图像级标签即可生成肺炎区域热力图，无需像素级标注。


<details>
  <summary>Details</summary>
Motivation: 胸部X光常用于肺炎诊断，但精确定位肺炎区域需要昂贵的像素级标注。为降低标注成本，需要开发仅使用图像级标签的弱监督方法。

Method: 使用Grad-CAM生成肺炎区域热力图，评估7种预训练模型（包括Vision Transformer），采用focal loss和患者级数据分割防止数据泄露。

Result: 所有模型分类准确率达96-98%，ResNet-18和EfficientNet-B0表现最佳，MobileNet-V3提供轻量级替代方案。Grad-CAM热力图显示模型聚焦临床相关肺区域。

Conclusion: 弱监督可解释AI模型在肺炎筛查中具有潜力，能增强临床透明度与信任，无需昂贵像素级标注即可实现肺炎定位。

Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.

</details>


### [7] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: TTP：针对CLIP等视觉语言模型的轻量级测试时防御框架，通过空间填充前后的余弦相似度偏移检测对抗样本，并使用可训练填充修复注意力模式，实现对抗鲁棒性提升且不损害干净准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在零样本识别方面表现优异，但对对抗扰动高度敏感，存在安全隐患。训练时防御需要标注数据和昂贵重训练，而现有测试时策略无法可靠区分干净和对抗输入，无法同时优化对抗鲁棒性和干净准确率。

Method: 提出测试时填充（TTP）框架：1）通过空间填充前后CLIP特征嵌入的余弦相似度偏移检测对抗输入，使用通用阈值；2）对检测到的对抗样本，使用可训练填充修复被破坏的注意力模式，结合相似度感知集成策略；3）对干净输入默认保持不变或可选集成现有测试时适应技术。

Result: 在多种CLIP主干网络和细粒度基准测试上的综合实验表明，TTP持续超越最先进的测试时防御方法，显著提升对抗鲁棒性而不损害干净准确率。

Conclusion: TTP提供了一种轻量级、有效的测试时防御框架，解决了视觉语言模型对抗脆弱性问题，无需重训练或标注数据，在保持干净准确率的同时显著提升鲁棒性。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [8] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个多模态可提示世界事件框架，结合文本、轨迹和参考图像实现用户导向的丰富模拟，支持多智能体交互、对象进出、参考引导外观等复杂事件生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：纯文本方法缺乏视觉控制，现有轨迹控制图像到视频方法无法充分表达语义意图。需要一种能够结合运动、语义和视觉信息的多模态框架，实现更丰富、可控的世界事件模拟。

Method: 采用多模态方法，结合三种输入：1)轨迹编码运动、时序和可见性；2)自然语言表达语义意图；3)参考图像提供对象身份的视觉基础。通过这种组合实现连贯可控的事件生成，支持多智能体交互、对象进出、参考引导外观和反直觉事件。

Result: 生成的视频不仅具有时间连贯性，还展现出涌现一致性，能够保持对象身份和场景的连续性，即使对象暂时消失也能保持一致。框架将世界模型从被动预测器提升为交互式、用户可塑的模拟器。

Conclusion: WorldCanvas通过多模态方法实现了丰富、用户导向的世界事件模拟，结合轨迹、文本和参考图像的优势，为世界模型提供了从被动预测到交互式模拟的重要进展。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: 提出CSA-TTA框架，通过跨样本增强测试时适应来改善术中低血压预测，利用其他患者的低血压事件增强训练数据，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 术中低血压(IOH)预测具有挑战性，因为患者特异性差异大且低血压事件罕见，导致测试时训练不可靠。需要一种能够利用其他患者数据来增强个性化预测的方法。

Method: 提出CSA-TTA框架：1)构建跨样本库，将历史数据分为低血压和非低血压样本；2)采用粗到细检索策略：先用K-Shape聚类识别代表性聚类中心，再基于当前患者信号检索top-K语义相似样本；3)训练时结合自监督掩码重建和回顾性序列预测信号。

Result: 在VitalDB数据集和真实医院数据集上评估，与TimesFM和UniTS等先进时间序列预测模型集成。在VitalDB上，微调场景下Recall和F1分别提升+1.33%和+1.13%，零样本场景下提升+7.46%和+5.07%，表现出强鲁棒性和泛化能力。

Conclusion: CSA-TTA框架通过跨样本增强有效解决了术中低血压预测中测试时训练不可靠的问题，显著提升了预测性能，为个性化医疗预测提供了有前景的解决方案。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [10] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 该研究将LDA主题模型中的主题数量T选择问题形式化为离散黑盒优化问题，比较了四种优化方法在固定评估预算下的性能。


<details>
  <summary>Details</summary>
Motivation: LDA主题模型中主题数量T的选择对统计拟合和可解释性有重要影响，但传统方法通常需要大量计算资源。研究旨在找到更高效的优化方法来选择最优主题数量。

Method: 将主题数量选择视为离散黑盒优化问题，每个函数评估对应训练LDA模型并测量验证困惑度。在固定评估预算下比较四种优化器：遗传算法(GA)、进化策略(ES)、偏好摊销黑盒优化(PABBO)和锐度感知黑盒优化(SABBO)。

Result: 虽然四种方法最终都能达到相似的困惑度水平，但摊销优化器(PABBO和SABBO)在样本和时间效率上显著更高。SABBO通常只需一次评估就能找到接近最优的主题数量，PABBO在几次评估内找到竞争性配置，而GA和ES需要几乎全部预算才能达到相同区域。

Conclusion: 摊销黑盒优化方法在LDA主题数量选择问题上比传统进化方法更高效，特别是SABBO能够以极少的评估次数找到接近最优解，为实际应用提供了更实用的解决方案。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [11] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: AIMM框架通过融合Reddit活动、机器人指标和市场数据，为每只股票生成每日操纵风险评分，用于检测社交媒体驱动的市场操纵


<details>
  <summary>Details</summary>
Motivation: 市场操纵日益来自协调的社交媒体活动而非孤立交易，零售投资者、监管机构和经纪商需要工具来连接在线叙事、协调模式与市场行为

Method: 开发AIMM框架，融合Reddit活动、机器人和协调指标以及OHLCV市场特征，使用parquet-native流水线和Streamlit仪表板，构建AIMM-GT数据集（33个标记的股票-交易日），采用前向评估和预测日志记录

Result: AIMM在GME事件中提前22天发出预警，虽然当前标记集较小（33个股票-交易日，3个正面事件），但显示出初步的判别能力和早期预警能力

Conclusion: AIMM框架为社交媒体驱动的市场监控研究提供了代码、数据集架构和仪表板设计，支持检测协调的社交媒体市场操纵活动

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [12] [Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models](https://arxiv.org/abs/2512.16244)
*Xueqi Ma,Xingjun Ma,Sarah Monazam Erfani,Danilo Mandic,James Bailey*

Main category: cs.LG

TL;DR: 提出CFC框架，利用大语言模型进行图数据集的开放集分类，实现从粗到细的OOD检测与分类，无需真实标签信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法将OOD样本视为单一类别，但实际应用需要更深入了解OOD样本的可能标签。需要在不依赖真实标签的情况下扩展OOD检测到OOD分类。

Method: CFC框架包含三个组件：1) 粗分类器使用LLM提示进行OOD检测和异常标签生成；2) GNN细分类器利用粗分类器识别的OOD样本训练，增强OOD检测和ID分类；3) 通过LLM提示和后处理的OOD标签实现精细化OOD分类。

Result: 在图和文本领域，CFC将OOD检测性能提升10%优于SOTA方法；在图数据集上OOD分类准确率达到70%。

Conclusion: CFC框架成功实现了无需真实标签的OOD分类，利用语义OOD实例提高了可解释性和实用性，为开放世界图神经网络应用提供了有效解决方案。

Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.

</details>


### [13] [Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning](https://arxiv.org/abs/2512.16476)
*Pengfei Sun,Wenyu Jiang,Piew Yoong Chee,Paul Devos,Dick Botteldooren*

Main category: cs.LG

TL;DR: 提出了一种无需批量归一化(BN)的完全整数量化神经网络训练方法，通过渐进式逐层蒸馏实现整数推理，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有量化神经网络(QNNs)大多仍依赖批量归一化(BN)层，这阻碍了真正的整数推理部署。虽然已有尝试通过参数折叠或定制初始化来移除BN，但这些方法难以恢复BN的稳定性和准确性，且通常需要特殊约束。

Method: 采用渐进式逐层蒸馏方案：从预训练的BN-enabled教师模型开始，使用逐层目标和渐进补偿来训练学生模型。该方法完全消除BN操作，实现仅使用整数算术的推理，并能直接集成到标准量化工作流程中。

Result: 在ImageNet数据集上使用AlexNet架构，BN-free模型在激进量化下获得了具有竞争力的Top-1准确率。该方法实现了端到端的整数推理，适用于资源受限的边缘和嵌入式设备。

Conclusion: 提出了一种有效的BN-free完全整数量化神经网络训练方法，通过渐进式逐层蒸馏解决了传统QNN依赖BN的问题，实现了真正的整数推理部署，为边缘计算提供了实用解决方案。

Abstract: Quantised neural networks (QNNs) shrink models and reduce inference energy through low-bit arithmetic, yet most still depend on a running statistics batch normalisation (BN) layer, preventing true integer-only deployment. Prior attempts remove BN by parameter folding or tailored initialisation; while helpful, they rarely recover BN's stability and accuracy and often impose bespoke constraints. We present a BN-free, fully integer QNN trained via a progressive, layer-wise distillation scheme that slots into existing low-bit pipelines. Starting from a pretrained BN-enabled teacher, we use layer-wise targets and progressive compensation to train a student that performs inference exclusively with integer arithmetic and contains no BN operations. On ImageNet with AlexNet, the BN-free model attains competitive Top-1 accuracy under aggressive quantisation. The procedure integrates directly with standard quantisation workflows, enabling end-to-end integer-only inference for resource-constrained settings such as edge and embedded devices.

</details>


### [14] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: Abacus：通过预测用户事件经验频率分布的自监督预训练方法，结合序列学习目标，提升展示广告中的用户购买行为建模效果


<details>
  <summary>Details</summary>
Motivation: 展示广告系统中用户购买行为建模面临正样本稀疏、用户行为随机性导致的类别不平衡和事件时间不规则问题。现有方法依赖手工"计数器"特征，忽略了用户意图的细粒度时间演化，而当前序列模型又缺少有用的事件计数统计信息。

Method: 提出Abacus方法，通过预测用户事件的经验频率分布进行自监督预训练。进一步提出混合目标函数，将Abacus与序列学习目标统一，结合聚合统计的稳定性和序列建模的敏感性。

Result: 在两个真实世界数据集上的实验表明，Abacus预训练优于现有方法，加速了下游任务的收敛，混合方法相比基线AUC提升高达+6.1%。

Conclusion: Abacus通过自监督预训练预测用户事件频率分布，结合序列建模，有效解决了展示广告中用户购买行为建模的挑战，显著提升了预测性能。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [15] [MEPIC: Memory Efficient Position Independent Caching for LLM Serving](https://arxiv.org/abs/2512.16822)
*Qian Wang,Zahra Yousefijamarani,Morgan Lindsay Heisler,Rongzhi Gu,Bai Xiaolong,Shan Yizhou,Wei Zhang,Wang Lan,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: MEPIC是一个内存高效的KV缓存系统，通过分块对齐、块级重计算和RoPE融合技术，实现跨位置、请求和批次的KV缓存重用，显著减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用（如深度研究助手、编码代理和RAG系统）需要反复处理包含共享文档或代码块的长提示历史，给KV缓存带来巨大压力。现有前缀缓存和位置无关缓存存在限制，导致即使许多请求重用相同内容，内存节省效果也有限。

Method: 1) 将块KV对齐到分页存储；2) 将重计算从令牌级转移到块级，使只有第一个块是请求特定的；3) 通过注意力内核中的RoPE融合移除位置编码；4) 使剩余块完全可共享。

Result: 相比最先进的PIC，在保持相似延迟和准确性的情况下，HBM使用减少高达2倍；对于长提示，减少高达5倍，无需任何模型更改。

Conclusion: MEPIC通过消除HBM中的大部分重复块KV，显著提高了内存效率，解决了LLM应用中KV缓存的内存压力问题。

Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

</details>


### [16] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 提出Knowledge Transformation (KT)方法，结合知识蒸馏、主动学习和因果推理，为在线边缘机器学习中未见数据生成伪标签，解决标签获取难题。


<details>
  <summary>Details</summary>
Motivation: 现有边缘机器学习方法通常假设静态模型在中心训练后部署，无法有效处理未见数据。在线边缘机器学习允许模型直接在边缘设备上训练并持续更新，但面临关键挑战：如何为真正未来的未见数据点确定标签。

Method: 提出Knowledge Transformation (KT)混合方法，结合知识蒸馏、主动学习和因果推理。KT在主动学习中扮演预言机角色，通过将教师模型的知识转化为学生模型的伪标签。方法验证采用两种仿真实验设置：(1)使用稳定性较低的教师模型，(2)使用相对更稳定的教师模型。

Result: 实验结果表明，当提供稳定的教师模型时，学生模型最终能达到其预期最大性能。KT在以下场景中具有潜在优势：(1)教师任务具有通用性，现有预训练模型可能足够完成任务，无需从头训练教师模型；(2)学生任务的标签难以获取或成本高昂。

Conclusion: KT方法为解决在线边缘机器学习中未见数据的标签问题提供了有效解决方案，特别适用于教师任务通用或学生标签获取困难的场景，能够通过知识转化实现模型性能的最大化。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 提出比例责任原则（PPD），将道德责任建模为随认知状态变化的函数，揭示不确定性不会消除责任而是将其转化为修复责任，并通过数学公式和模拟验证框架有效性。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架难以处理不确定性下的决策，通常将其简单视为行动约束。需要一种能动态建模道德责任如何随认知状态变化的新框架。

Method: 引入比例责任原则（PPD），用数学公式 D_total = K[(1-HI) + HI * g(C_signal)] 建模总责任，其中K为知识，HI为不确定性/谦逊度，C_signal为情境信号强度。使用蒙特卡洛模拟验证框架稳定性。

Result: 模拟显示保持基准谦逊系数（λ>0）的系统能产生更稳定的责任分配，降低过度自信决策风险。该框架在临床伦理、权利法、经济治理和人工智能四个领域验证了跨学科有效性。

Conclusion: 比例责任原则通过将谦逊度作为系统参数形式化，提供了可数学处理的责任建模方法，可作为复杂系统中的稳定原则，防止过度行动和疏忽，平衡认知信心与情境风险。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [18] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: AI流行病学是一个通过应用群体层面监测方法来治理和解释AI系统的框架，它通过统计关联而非理解内部机制来预测AI输出失败，类似于流行病学在理解分子机制前通过统计证据实现公共卫生干预。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性方法（如SHAP和机制可解释性）在面对大规模部署模型时存在模型复杂性问题。需要一种能够绕过模型复杂性、提供持续治理、且不增加专家负担的AI监管框架。

Method: 通过标准化捕获AI-专家交互为结构化评估字段（风险等级、对齐分数、准确度分数），将这些作为暴露变量来统计预测输出失败。通过被动跟踪专家与AI建议的趋同与分歧，提供自动审计追踪，并验证输出失败关联与专家覆盖和实际结果的一致性。

Result: 该框架为零负担专家提供自动审计追踪，在机构更新模型和切换供应商时保持治理连续性，通过可靠性分数和语义评估（如"此建议类似于500个因违反指南而被专家覆盖的案例"）使专家和机构能够在AI输出造成危害前检测不可靠输出。

Conclusion: AI流行病学通过使领域专家无需机器学习专业知识即可治理AI系统，实现了AI监管的民主化，为复杂AI系统提供了可扩展、实用的治理框架。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [19] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 论文提出KalshiBench基准，评估大语言模型对未来事件的认知校准能力，发现所有前沿模型都存在系统性过度自信问题，即使推理增强模型也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多样任务上表现出色，但其认知校准能力（即模型置信度与实际准确度的匹配程度）仍未被充分理解。现有基准主要评估静态知识准确性，缺乏对未来未知事件不确定性量化的评估。

Method: 引入KalshiBench基准，包含300个来自受监管交易所Kalshi的预测市场问题，这些问题的真实结果发生在模型训练截止之后。评估了Claude Opus 4.5、GPT-5.2、DeepSeek-V3.2、Qwen3-235B和Kimi-K2等五个前沿模型，使用预期校准误差（ECE）和Brier技能分数等指标衡量校准性能。

Result: 所有模型都表现出系统性过度自信。最佳校准模型Claude Opus 4.5的ECE为0.120，仍有显著校准误差。推理增强模型如GPT-5.2-XHigh校准更差（ECE=0.395）。仅一个模型获得正Brier技能分数，表明大多数模型表现不如简单预测基准率。

Conclusion: 模型规模和推理增强不会自动带来校准改进，认知校准是一种需要针对性开发的独立能力。研究强调了开发专门校准技术的重要性，以提升模型在真实世界不确定性下的可靠性。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [20] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: 提出了AMUSE基准测试评估多模态大语言模型在多说话人对话场景中的代理推理能力，并开发了RAFT框架通过奖励优化和选择性参数适应来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（如GPT-4o和Qwen3-Omni）在感知方面表现强劲，但在需要代理推理的多说话人对话场景中存在困难，包括追踪说话者、维持角色和跨时间接地事件。这些场景在多模态音视频理解中至关重要，如对话式视频助手和会议分析。

Method: 1. 引入AMUSE基准测试，围绕需要代理推理的任务设计，要求模型将复杂的音视频交互分解为规划、接地和反思步骤；2. 在三种模式（零样本、引导、代理）和六个任务族中评估MLLMs；3. 提出RAFT框架，整合奖励优化与内在多模态自我评估作为奖励，并进行选择性参数适应以实现数据和参数高效更新。

Result: 当前模型在所有模式下都表现出弱的多说话人推理能力和不一致的行为。使用RAFT框架，在基准测试上实现了高达39.52%的相对准确率提升。

Conclusion: AMUSE和RAFT共同为检验多模态模型中的代理推理能力并提升其性能提供了一个实用平台，解决了多说话人对话场景中的关键挑战。

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [21] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 提出了一种新的元启发式优化算法PCIA，灵感来源于人类构建和使用路径的方式，通过随机种群寻找最佳路径，在数学和约束优化问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 受人类路径构建行为的启发：人们通常偏好热门路线，当路径关闭时会智能地混合现有路径构建新路线，并随机选择不同路径探索未知目的地。这些行为模式可以转化为有效的优化算法。

Method: PCIA模仿人类路径构建过程：1) 生成随机种群（类似群体算法），每个粒子代表一条通往目标的路径；2) 结合人类路径选择的三个关键特征：偏好热门路线、智能混合现有路径、随机探索未知路径。

Result: 在53个数学优化问题和13个约束优化问题上测试，PCIA与流行及最新的元启发式算法相比表现出高度竞争力。

Conclusion: PCIA是一种有效的元启发式优化算法，基于人类路径构建行为的灵感，在多种优化问题上表现优异，具有实际应用价值。

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [22] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协作实现，而非单一系统。为此需要超越个体对齐，建立分布式AGI安全框架，通过虚拟代理沙盒经济来管理智能体间交互风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究主要关注单一AGI系统的对齐，但忽视了另一种可能性：通用智能可能首先通过多个子AGI智能体的协作和互补能力实现。随着具备工具使用和协调能力的AI智能体快速部署，这种"拼凑式AGI"带来的安全风险需要紧急关注。

Method: 提出分布式AGI安全框架，核心是设计和实施虚拟代理沙盒经济（不可渗透或半渗透），通过稳健的市场机制管理智能体间交易，结合适当的可审计性、声誉管理和监督来缓解集体风险。

Result: 建立了从个体对齐到群体协调安全的范式转变框架，为应对拼凑式AGI风险提供了系统性的安全方案，强调通过经济机制和监管结构来管理智能体集体行为。

Conclusion: 拼凑式AGI假说需要认真对待，AI安全研究必须超越个体智能体对齐，开发相应的分布式安全框架。虚拟代理沙盒经济结合市场机制和监督措施，是应对智能体群体协调风险的关键方向。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: SocialStoryFrames：一种基于对话上下文和叙事理论的读者响应建模形式化方法，用于分析社交媒体故事中的作者意图、推理、情感反应和价值判断。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以捕捉读者对故事的丰富解释性、情感性和评价性反应（如推断叙事意图、判断角色等），限制了细粒度分析。需要一种能够建模读者响应的计算框架。

Method: 提出SocialStoryFrames形式化方法，基于叙事理论、语言语用学和心理学构建分类法。开发SSF-Generator和SSF-Classifier两个模型，通过人类调查（382名参与者）和专家标注验证。应用于SSF-Corpus数据集（6,140个社交媒体故事）。

Result: 模型验证有效，能够分析故事讲述意图的频率和相互依赖关系，比较不同社区的叙事实践多样性。展示了该形式化方法在大规模研究故事讲述中的实用性。

Conclusion: SocialStoryFrames通过将细粒度、上下文敏感的建模与通用的读者响应分类法相结合，为在线社区故事讲述研究提供了新工具。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [24] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 开发了一个基于Qwen2.5-7B模型和LoRA微调的信息提取管道，用于从警方通告中提取结构化信息，在死亡检测、死亡人数和省份位置提取等任务上达到95%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 从警方事件通告中提取结构化信息对及时准确的数据处理至关重要，但由于社交媒体文本的变异性大和非正式性，这一任务面临显著挑战。

Method: 采用领域适应的提取管道，结合针对性提示工程和基于LoRA的参数高效微调Qwen2.5-7B模型，处理噪声和异构文本，从4,933个手动标注的微博警方通告实例中提取15个关键字段。

Result: LoRA微调显著优于基模型和指令微调模型，死亡检测准确率超过98.36%，死亡人数精确匹配率达95.31%，省份位置提取精确匹配率达95.54%。

Conclusion: 该管道为专业领域多任务结构化信息提取提供了验证有效的高效解决方案，为社会科学研究中将非结构化文本转化为可靠结构化数据提供了实用框架。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation](https://arxiv.org/abs/2512.16302)
*Zixuan Chen,Chongkai Gao,Lin Shao,Jieqi Shi,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: ManiLong-Shot：一种通过物理交互事件分解长时程任务、使用VLM或规则启发式驱动原语序列化、实现单次模仿学习长时程抓取操作的新框架


<details>
  <summary>Details</summary>
Motivation: 当前单次模仿学习方法主要局限于短时程任务，限制了其在复杂长时程操作任务中的应用，需要解决长时程任务的单次模仿学习问题

Method: 围绕物理交互事件结构化长时程任务，将其重构为交互感知原语的序列化问题；使用视觉语言模型或基于机器人状态变化的规则启发式驱动原语分解；为每个原语预测关键交互不变区域，建立演示与当前观察的对应关系，计算目标末端执行器位姿

Result: 在仅10个短时程任务上训练后，能泛化到20个未见的长时程任务（三个难度级别），相比SOTA获得22.8%的相对提升；真实机器人实验验证了在三个长时程操作任务上通过单次模仿学习的鲁棒执行能力

Conclusion: ManiLong-Shot框架有效解决了长时程任务的单次模仿学习问题，通过交互感知原语分解方法实现了从短时程任务到复杂长时程任务的泛化，具有实际应用价值

Abstract: One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.

</details>
