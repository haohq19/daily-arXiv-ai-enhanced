{"id": "2512.06227", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06227", "abs": "https://arxiv.org/abs/2512.06227", "authors": ["Junyu Mao", "Anthony Hills", "Talia Tseriotou", "Maria Liakata", "Aya Shamir", "Dan Sayda", "Dana Atzil-Slonim", "Natalie Djohari", "Arpan Mandal", "Silke Roth", "Pamela Ugwudike", "Mahesan Niranjan", "Stuart E. Middleton"], "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety", "comment": null, "summary": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7f6e\u4fe1\u611f\u77e5\u7ec6\u7c92\u5ea6\u8fa9\u8bba\uff08CFD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2aLLM\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u6807\u6ce8\u5458\u4ea4\u6362\u7ec6\u7c92\u5ea6\u8bc1\u636e\u8fbe\u6210\u5171\u8bc6\uff0c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff0c\u5728\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6307\u6807\uff08\u5982\u5fc3\u7406\u5065\u5eb7\u4e8b\u4ef6\u3001\u5728\u7ebf\u5b89\u5168\u98ce\u9669\uff09\u5bf9NLP\u4efb\u52a1\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8fd9\u7c7b\u4e8b\u4ef6\u7684\u52a8\u6001\u6027\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u6807\u6ce8\u8fd9\u4e9b\u4fe1\u606f\u6210\u672c\u9ad8\u4e14\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u7f6e\u4fe1\u611f\u77e5\u7ec6\u7c92\u5ea6\u8fa9\u8bba\uff08CFD\uff09\u6846\u67b6\uff1a\u591a\u4e2aLLM\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u6807\u6ce8\u5458\uff0c\u4ea4\u6362\u7ec6\u7c92\u5ea6\u8bc1\u636e\u8fbe\u6210\u5171\u8bc6\uff1b\u521b\u5efa\u4e86\u4e24\u4e2a\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff1a\u5fc3\u7406\u5065\u5eb7Reddit\u5e78\u798f\u6570\u636e\u96c6\u548c\u5728\u7ebf\u5b89\u5168Facebook\u5206\u4eab\u98ce\u9669\u6570\u636e\u96c6\u3002", "result": "CFD\u6846\u67b6\u76f8\u6bd4\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u6700\u7a33\u5065\u7684\u6570\u636e\u589e\u5f3a\u6027\u80fd\uff1b\u6570\u636e\u589e\u5f3a\u80fd\u6301\u7eed\u6539\u5584\u4e0b\u6e38\u4efb\u52a1\uff1b\u901a\u8fc7\u8fa9\u8bba\u8bb0\u5f55\u6574\u5408\u7684\u589e\u5f3a\u7279\u5f81\u5e26\u6765\u6700\u5927\u63d0\u5347\uff0c\u5728\u7ebf\u5b89\u5168\u4efb\u52a1\u4e0a\u6bd4\u975e\u589e\u5f3a\u57fa\u7ebf\u63d0\u9ad810.1%\u3002", "conclusion": "CFD\u6846\u67b6\u4e3a\u73b0\u5b9e\u4e16\u754c\u6307\u6807\u7684\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u8fa9\u8bba\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2512.07094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07094", "abs": "https://arxiv.org/abs/2512.07094", "authors": ["Christopher Cruz"], "title": "VIGIL: A Reflective Runtime for Self-Healing Agents", "comment": null, "summary": "Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.", "AI": {"tldr": "VIGIL\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u68c0\u67e5\u548c\u9632\u62a4\u8fed\u4ee3\u5b66\u4e60\u6846\u67b6\uff0c\u4f5c\u4e3a\u53cd\u5c04\u8fd0\u884c\u65f6\u76d1\u7763\u5144\u5f1f\u4ee3\u7406\uff0c\u901a\u8fc7\u60c5\u611f\u5206\u6790\u3001\u884c\u4e3a\u8bca\u65ad\u548c\u81ea\u52a8\u4fee\u590d\u5b9e\u73b0\u81ea\u4e3b\u7ef4\u62a4\uff0c\u800c\u975e\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u6846\u67b6\u8106\u5f31\uff0c\u7f3a\u4e4f\u8fd0\u884c\u65f6\u81ea\u7701\u80fd\u529b\uff0c\u65e0\u6cd5\u8bca\u65ad\u81ea\u8eab\u6545\u969c\u6a21\u5f0f\uff0c\u9700\u8981\u4eba\u5de5\u5e72\u9884\u624d\u80fd\u6539\u8fdb\u3002\u5927\u591a\u6570\u4ee3\u7406\u7cfb\u7edf\u9000\u5316\u4e3a\u88c5\u9970\u6027\u7684LLM\u8c03\u7528\u94fe\uff0c\u6ca1\u6709\u53ef\u9760\u6027\u7684\u7ed3\u6784\u673a\u5236\u3002", "method": "VIGIL\u4f5c\u4e3a\u53cd\u5c04\u8fd0\u884c\u65f6\uff0c\u6444\u5165\u884c\u4e3a\u65e5\u5fd7\uff0c\u5c06\u4e8b\u4ef6\u8bc4\u4f30\u4e3a\u7ed3\u6784\u5316\u60c5\u611f\u8868\u793a\uff0c\u7ef4\u62a4\u5177\u6709\u8870\u51cf\u548c\u4e0a\u4e0b\u6587\u7b56\u7565\u7684\u6301\u4e45\u60c5\u611f\u94f6\u884c\uff0c\u751f\u6210RBT\u8bca\u65ad\uff08\u4f18\u52bf\u3001\u673a\u4f1a\u3001\u5931\u8d25\uff09\uff0c\u5e76\u4ea7\u751f\u9632\u62a4\u6027\u63d0\u793a\u66f4\u65b0\u548c\u53ea\u8bfb\u4ee3\u7801\u63d0\u6848\u3002\u91c7\u7528\u72b6\u6001\u95e8\u63a7\u7ba1\u9053\uff0c\u975e\u6cd5\u8f6c\u6362\u4f1a\u4ea7\u751f\u663e\u5f0f\u9519\u8bef\u3002", "result": "\u5728\u63d0\u9192\u5ef6\u8fdf\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cVIGIL\u8bc6\u522b\u51fa\u5ef6\u8fdf\u5347\u9ad8\uff0c\u63d0\u51fa\u63d0\u793a\u548c\u4ee3\u7801\u4fee\u590d\u65b9\u6848\u3002\u5f53\u81ea\u8eab\u8bca\u65ad\u5de5\u5177\u56e0\u6a21\u5f0f\u51b2\u7a81\u5931\u8d25\u65f6\uff0c\u80fd\u66b4\u9732\u5185\u90e8\u9519\u8bef\uff0c\u751f\u6210\u5907\u7528\u8bca\u65ad\u5e76\u53d1\u51fa\u4fee\u590d\u8ba1\u5212\uff0c\u5c55\u793a\u4e86\u90e8\u7f72\u4ee3\u7406\u8fd0\u884c\u65f6\u7684\u5143\u7ea7\u81ea\u6211\u4fee\u590d\u80fd\u529b\u3002", "conclusion": "VIGIL\u5b9e\u73b0\u4e86\u4ee3\u7406\u8fd0\u884c\u65f6\u7684\u81ea\u4e3b\u7ef4\u62a4\u548c\u5143\u7ea7\u81ea\u6211\u4fee\u590d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u4ee3\u7406\u6846\u67b6\u7f3a\u4e4f\u81ea\u7701\u548c\u6539\u8fdb\u80fd\u529b\u7684\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.06848", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06848", "abs": "https://arxiv.org/abs/2512.06848", "authors": ["Sepyan Purnama Kristanto", "Lutfi Hakim", "Hermansyah"], "title": "AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices", "comment": "9Pages, 3 figure, Politeknik Negeri Banyuwangi", "summary": "Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.", "AI": {"tldr": "AquaFusionNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u5c06\u663e\u5fae\u6210\u50cf\u4e0e\u6c34\u8d28\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\uff0c\u7528\u4e8e\u5c0f\u578b\u996e\u7528\u6c34\u7cfb\u7edf\u7684\u5b9e\u65f6\u5fae\u751f\u7269\u6c61\u67d3\u76d1\u6d4b\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u76d1\u6d4b\u5de5\u5177\u53ea\u80fd\u6355\u83b7\u5fae\u751f\u7269\u6c61\u67d3\u7684\u7247\u6bb5\u4fe1\u606f\uff0c\u663e\u5fae\u6210\u50cf\u548c\u7269\u7406\u5316\u5b66\u4f20\u611f\u5668\u6570\u636e\u9700\u8981\u5206\u5f00\u89e3\u8bfb\uff0c\u5bfc\u81f4\u5b9e\u65f6\u51b3\u7b56\u4e0d\u53ef\u9760\u3002\u5728\u996e\u7528\u6c34\u9886\u57df\uff0c\u516c\u5f00\u53ef\u7528\u7684\u663e\u5fae\u6570\u636e\u96c6\u7a00\u7f3a\u3002", "method": "\u63d0\u51faAquaFusionNet\u6846\u67b6\uff0c\u901a\u8fc7\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u5fae\u751f\u7269\u5916\u89c2\u4e0e\u4f20\u611f\u5668\u52a8\u6001\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u5173\u7cfb\u3002\u521b\u5efaAquaMicro12K\u6570\u636e\u96c6\uff0812,846\u5f20\u6807\u6ce8\u663e\u5fae\u56fe\u50cf\uff09\uff0c\u4e13\u95e8\u9488\u5bf9\u996e\u7528\u6c34\u73af\u5883\u3002", "result": "\u5728\u5370\u5ea6\u5c3c\u897f\u4e9a\u4e1c\u722a\u54c77\u4e2a\u8bbe\u65bd\u90e8\u7f726\u4e2a\u6708\uff0c\u5904\u7406184\u4e07\u5e27\u56fe\u50cf\uff0c\u6c61\u67d3\u4e8b\u4ef6\u68c0\u6d4bmAP@0.5\u8fbe94.8%\uff0c\u5f02\u5e38\u9884\u6d4b\u51c6\u786e\u738796.3%\uff0c\u529f\u8017\u4ec54.8W\u3002\u8de8\u6a21\u6001\u8026\u5408\u51cf\u5c11\u4e86\u5355\u6a21\u6001\u68c0\u6d4b\u5668\u7684\u5e38\u89c1\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "AquaFusionNet\u5728\u53ef\u6bd4\u6216\u66f4\u4f4e\u529f\u8017\u4e0b\u63d0\u4f9b\u66f4\u9ad8\u7cbe\u5ea6\uff0c\u516c\u5f00\u6240\u6709\u6a21\u578b\u3001\u6570\u636e\u548c\u786c\u4ef6\u8bbe\u8ba1\uff0c\u4fc3\u8fdb\u5206\u6563\u5f0f\u6c34\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u5236\u548c\u9002\u5e94\u3002"}}
{"id": "2512.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06258", "abs": "https://arxiv.org/abs/2512.06258", "authors": ["Chaoyang Wang", "Yangfan He", "Yiyang Zhou", "Yixuan Wang", "Jiaqi Liu", "Peng Xia", "Zhengzhong Tu", "Mohit Bansal", "Huaxiu Yao"], "title": "Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs", "comment": null, "summary": "We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8def\u5f84\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff1a\u5373\u4f7f\u77e5\u9053\u6b63\u786e\u7b54\u6848\uff0c\u4e5f\u5e38\u901a\u8fc7\u9519\u8bef\u63a8\u7406\u8def\u5f84\u5f97\u51fa\u7ed3\u679c\u3002\u63d0\u51faPSO\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u9009\u62e9\u4f18\u5316\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7f3a\u9677\uff1a\u5373\u4f7f\u6a21\u578b\u77e5\u9053\u6b63\u786e\u7b54\u6848\uff0c\u4e5f\u7ecf\u5e38\u901a\u8fc7\u9519\u8bef\u7684\u63a8\u7406\u8def\u5f84\u5f97\u51fa\u7ed3\u679c\u3002\u6838\u5fc3\u95ee\u9898\u4e0d\u662f\u77e5\u8bc6\u7f3a\u4e4f\uff0c\u800c\u662f\u5728\u5e9e\u5927\u7684\u63a8\u7406\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u9009\u62e9\u504f\u5dee\u3002\u6a21\u578b\u867d\u7136\u80fd\u591f\u91c7\u6837\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u8f68\u8ff9\uff0c\u4f46\u4e0d\u6210\u6bd4\u4f8b\u5730\u504f\u5411\u4e0d\u7a33\u5b9a\u6216\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u8def\u5f84\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7a33\u5b9a\u548c\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faPSO\uff08\u8def\u5f84\u9009\u62e9\u4f18\u5316\uff09\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5e26\u6709\u6a21\u677f\u548c\u7b54\u6848\u5956\u52b1\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u6765\u57f9\u517b\u7ed3\u6784\u5316\u3001\u9010\u6b65\u63a8\u7406\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u5728\u7ebf\u504f\u597d\u4f18\u5316\uff0c\u6a21\u578b\u4eceGRPO\u751f\u6210\u7684\u6570\u636e\u4e2d\u91c7\u6837\u63a8\u7406\u8def\u5f84\uff0c\u81ea\u6211\u8bc4\u4f30\uff0c\u5e76\u5bf9\u9f50\u5230\u4f18\u9009\u8f68\u8ff9\u3002\u9519\u8bef\u6216\u6b21\u4f18\u8def\u5f84\u540c\u65f6\u5b58\u50a8\u5728\u8d1f\u5411\u56de\u653e\u8bb0\u5fc6\u4e2d\u4f5c\u4e3a\u786c\u8d1f\u6837\u672c\uff0c\u5b9a\u671f\u56de\u987e\u4ee5\u9632\u6b62\u6a21\u578b\u91cd\u590d\u5148\u524d\u9519\u8bef\u5e76\u4fc3\u8fdb\u6301\u7eed\u63a8\u7406\u6539\u8fdb\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPSO\u6709\u6548\u4fee\u526a\u65e0\u6548\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\uff08\u5e73\u5747\u63d0\u53477.4%\uff09\uff0c\u5e76\u4ea7\u751f\u66f4\u7a33\u5b9a\u4e00\u81f4\u7684\u601d\u7ef4\u94fe\u3002", "conclusion": "PSO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8def\u5f84\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u8def\u5f84\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u6539\u5584LVLM\u7684\u63a8\u7406\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2512.06269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06269", "abs": "https://arxiv.org/abs/2512.06269", "authors": ["Quan Tran", "Tuan Dang"], "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting", "comment": "10 pages", "summary": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u89c6\u56fe\u4e09\u89d2\u6d4b\u91cf\u7ea6\u675f\u6765\u589e\u5f3a3D\u9ad8\u65af\u6cfc\u6e85\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u4ec5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u5bfc\u81f4\u7684\u6d6e\u6e38\u4f2a\u5f71\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u8fdb\u884c\u91cd\u5efa\uff0c\u5bfc\u81f4\u51e0\u4f55\u4e0d\u4e00\u81f4\u3001\u6d6e\u6e38\u4f2a\u5f71\u548c\u7ed3\u6784\u6df7\u4e71\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9ad8\u8d28\u91cf\u8868\u9762\u7684\u63d0\u53d6\u3002", "method": "\u901a\u8fc7\u7ea6\u675f\u591a\u89c6\u56fe\u4e09\u89d2\u6d4b\u91cf\u6765\u589e\u5f3a\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5229\u7528\u591a\u4e2a\u4f30\u8ba1\u89c6\u56fe\u8fbe\u62103D\u8868\u793a\u7684\u5171\u8bc6\uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u4ece\u76f8\u90bb\u89c6\u56fe\u675f\u91cd\u65b0\u4e09\u89d2\u5316\u5f97\u5230\u9c81\u68d2\u5171\u8bc6\u70b9\uff0c\u5e76\u60e9\u7f5a\u6e32\u67d33D\u70b9\u4e0e\u5171\u8bc6\u70b9\u7684\u504f\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728DTU\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e860.50mm\u7684\u5e73\u5747Chamfer\u8ddd\u79bb\uff0c\u4f18\u4e8e\u540c\u7c7b\u663e\u5f0f\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ea6\u675f\u591a\u89c6\u56fe\u4e09\u89d2\u6d4b\u91cf\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4f2a\u5f71\uff0c\u6539\u5584\u91cd\u5efa\u8d28\u91cf\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u9a8c\u8bc1\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2512.06919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06919", "abs": "https://arxiv.org/abs/2512.06919", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla"], "title": "Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles", "comment": "13 pages, 2 figures", "summary": "The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.", "AI": {"tldr": "\u5f00\u53d1\u81ea\u52a8\u5316\u65b9\u6cd5\u4ece\u5386\u53f2\u5b89\u5168\u6570\u636e\u4e2d\u9009\u62e9\u6700\u5c0f\u4f46\u5168\u9762\u7684PRO-CTCAE\u75c7\u72b6\u5b50\u96c6\uff0c\u5e73\u8861\u4fe1\u53f7\u8986\u76d6\u4e0e\u60a3\u8005\u8d1f\u62c5", "motivation": "PRO-CTCAE\u7cfb\u7edf\u5305\u542b\u5927\u91cf\u75c7\u72b6\u9879\u76ee\uff0c\u9009\u62e9\u8fc7\u591a\u4f1a\u589e\u52a0\u60a3\u8005\u8d1f\u62c5\u964d\u4f4e\u4f9d\u4ece\u6027\uff0c\u9009\u62e9\u8fc7\u5c11\u53ef\u80fd\u9057\u6f0f\u91cd\u8981\u5b89\u5168\u4fe1\u53f7\uff0c\u9700\u8981\u5ba2\u89c2\u65b9\u6cd5\u4f18\u5316\u9879\u76ee\u9009\u62e9", "method": "\u5c06PRO-CTCAE\u75c7\u72b6\u6620\u5c04\u5230MedDRA\u672f\u8bed\uff0c\u7f16\u7801\u5230Safeterm\u8bed\u4e49\u7a7a\u95f4\uff0c\u7ed3\u5408\u76f8\u5173\u6027\u548c\u53d1\u751f\u7387\u8ba1\u7b97\u6548\u7528\u51fd\u6570\uff0c\u901a\u8fc7\u8c31\u5206\u6790\u8bc6\u522b\u6b63\u4ea4\u533b\u5b66\u6982\u5ff5\u96c6\uff0c\u6309\u91cd\u8981\u6027\u6392\u5e8f\u5e76\u57fa\u4e8e\u4fe1\u606f\u89e3\u91ca\u786e\u5b9a\u622a\u65ad\u70b9", "result": "\u5f00\u53d1\u4e86\u96c6\u6210\u5230Safeterm\u8bd5\u9a8c\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u80bf\u7624\u5b66\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u6027\u80fd\uff0c\u63d0\u4f9b\u5ba2\u89c2\u53ef\u91cd\u590d\u7684\u65b9\u6cd5", "conclusion": "\u81ea\u52a8\u5316\u65b9\u6cd5\u5229\u7528MedDRA\u8bed\u4e49\u548c\u5386\u53f2\u6570\u636e\u7b80\u5316PRO-CTCAE\u8bbe\u8ba1\uff0c\u5e73\u8861\u4fe1\u53f7\u8986\u76d6\u4e0e\u60a3\u8005\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4e34\u5e8a\u8bd5\u9a8c\u6548\u7387"}}
{"id": "2512.06282", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06282", "abs": "https://arxiv.org/abs/2512.06282", "authors": ["Lyn Chao-ling Chen", "Kuan-Wen Chen", "Yi-Ping Hung"], "title": "A Sleep Monitoring System Based on Audio, Video and Depth Information", "comment": "Accepted in the Computer Vision, Graphics and Image Processing (CVGIP 2013)", "summary": "For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u4e8b\u4ef6\u65b9\u6cd5\u7684\u975e\u4fb5\u5165\u5f0f\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ea2\u5916\u6df1\u5ea6\u4f20\u611f\u5668\u3001RGB\u6444\u50cf\u5934\u548c\u9ea6\u514b\u98ce\u9635\u5217\u68c0\u6d4b\u8fd0\u52a8\u3001\u706f\u5149\u5f00\u5173\u548c\u566a\u97f3\u4e09\u7c7b\u4e8b\u4ef6\uff0c\u7528\u4e8e\u5bb6\u5ead\u73af\u5883\u7761\u7720\u969c\u788d\u5b9a\u91cf\u8bc4\u4f30\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u7684\u5b9a\u91cf\u8bc4\u4f30\u7761\u7720\u969c\u788d\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u76d1\u6d4b\u7761\u7720\u5e72\u6270\u4e8b\u4ef6\uff0c\u907f\u514d\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u5bf9\u7761\u7720\u7684\u5e72\u6270\u3002", "method": "\u4f7f\u7528\u7ea2\u5916\u6df1\u5ea6\u4f20\u611f\u5668\u3001RGB\u6444\u50cf\u5934\u548c\u56db\u9ea6\u514b\u98ce\u9635\u5217\u8bbe\u5907\uff0c\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u76d1\u6d4b\u7761\u7720\u3002\u5efa\u7acb\u6df1\u5ea6\u4fe1\u53f7\u80cc\u666f\u6a21\u578b\u68c0\u6d4b\u8fd0\u52a8\u5e45\u5ea6\uff0c\u5efa\u7acb\u5f69\u8272\u56fe\u50cf\u80cc\u666f\u6a21\u578b\u68c0\u6d4b\u5149\u7167\u53d8\u5316\uff0c\u7ed3\u5408\u4e8b\u4ef6\u68c0\u6d4b\u7b97\u6cd5\u4ece\u4e09\u7c7b\u4f20\u611f\u5668\u6570\u636e\u4e2d\u8bc6\u522b\u4e8b\u4ef6\u53d1\u751f\u3002", "result": "\u7cfb\u7edf\u5728\u7761\u7720\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u8fd0\u52a8\u3001\u706f\u5149\u5f00\u5173\u548c\u566a\u97f3\u4e09\u7c7b\u7761\u7720\u5e72\u6270\u4e8b\u4ef6\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4e8b\u4ef6\u65b9\u6cd5\u7684\u975e\u4fb5\u5165\u5f0f\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\u80fd\u591f\u53ef\u9760\u5730\u5b9a\u91cf\u8bc4\u4f30\u7761\u7720\u969c\u788d\uff0c\u4e3a\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u7761\u7720\u8d28\u91cf\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2512.06427", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06427", "abs": "https://arxiv.org/abs/2512.06427", "authors": ["Andrea Combette", "Antoine Venaille", "Nelly Pustelnik"], "title": "A new initialisation to Control Gradients in Sinusoidal Neural network", "comment": null, "summary": "Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \\texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \\texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \\texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u6b63\u5f26\u6fc0\u6d3b\u51fd\u6570\u7f51\u7edc\uff08\u5982SIREN\uff09\u7684\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u68af\u5ea6\u548c\u9884\u6fc0\u6d3b\u5206\u5e03\u6765\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u521d\u59cb\u5316\u7b56\u7565\u5bf9\u7f13\u89e3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u7206\u70b8\u6216\u6d88\u5931\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u4e8e\u67d0\u4e9b\u6210\u719f\u67b6\u6784\uff08\u5982SIREN\uff09\u7684\u521d\u59cb\u5316\u53c2\u6570\u5f71\u54cd\u4ecd\u7f3a\u4e4f\u7cbe\u786e\u7684\u7406\u8bba\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u9884\u6fc0\u6d3b\u5206\u5e03\u6536\u655b\u548c\u96c5\u53ef\u6bd4\u77e9\u9635\u5e8f\u5217\u65b9\u5dee\u7684\u56fa\u5b9a\u70b9\uff0c\u63a8\u5bfc\u51fa\u53c2\u6570\u521d\u59cb\u5316\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u63a7\u5236\u68af\u5ea6\u5e76\u9488\u5bf9\u9884\u6fc0\u6d3b\u6d88\u5931\uff0c\u9632\u6b62\u4e0d\u9002\u5f53\u9891\u7387\u7684\u51fa\u73b0\u3002", "result": "\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\u5728\u51fd\u6570\u62df\u5408\u548c\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\u4e00\u81f4\u4f18\u4e8e\u539f\u59cbSIREN\u65b9\u6848\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u6d89\u53ca\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u5e7f\u6cdb\u91cd\u5efa\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u521d\u59cb\u5316\u7b56\u7565\u901a\u8fc7\u63a7\u5236\u68af\u5ea6\u548c\u9884\u6fc0\u6d3b\u5206\u5e03\uff0c\u6539\u5584\u4e86SIREN\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5728\u591a\u79cd\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.06306", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06306", "abs": "https://arxiv.org/abs/2512.06306", "authors": ["Haoxian Zhou", "Chuanzhi Xu", "Langyi Chen", "Haodong Chen", "Yuk Ying Chung", "Qiang Qu", "Xaoming Chen", "Weidong Cai"], "title": "Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation", "comment": null, "summary": "Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u70b9\u4e91\u6846\u67b6\u7684\u4e8b\u4ef6\u6d41\u65f6\u7a7a\u7279\u6027\u5229\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8b\u4ef6\u65f6\u95f4\u5207\u7247\u5377\u79ef\u548c\u4e8b\u4ef6\u5207\u7247\u5e8f\u5217\u6a21\u5757\u589e\u5f3a\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u4e8b\u4ef6\u6d41\u8f6c\u6362\u4e3a\u5bc6\u96c6\u4e8b\u4ef6\u5e27\u4f1a\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u5e76\u727a\u7272\u4e8b\u4ef6\u4fe1\u53f7\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u9700\u8981\u66f4\u597d\u5730\u5229\u7528\u4e8b\u4ef6\u6d41\u7684\u65f6\u7a7a\u7279\u6027", "method": "\u57fa\u4e8e\u70b9\u4e91\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e8b\u4ef6\u65f6\u95f4\u5207\u7247\u5377\u79ef\u6a21\u5757\u6355\u83b7\u4e8b\u4ef6\u5207\u7247\u95f4\u7684\u77ed\u671f\u4f9d\u8d56\uff0c\u7ed3\u5408\u4e8b\u4ef6\u5207\u7247\u5e8f\u5217\u6a21\u5757\u8fdb\u884c\u7ed3\u6784\u5316\u65f6\u95f4\u5efa\u6a21\uff0c\u5e76\u5728\u70b9\u4e91\u8868\u793a\u4e2d\u5e94\u7528\u8fb9\u7f18\u589e\u5f3a", "result": "\u5728DHP19\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728PointNet\u3001DGCNN\u548cPoint Transformer\u4e09\u79cd\u4ee3\u8868\u6027\u70b9\u4e91\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u4e8b\u4ef6\u6d41\u7684\u65f6\u7a7a\u7279\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u540c\u65f6\u63d0\u5347\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6027\u80fd"}}
{"id": "2512.07032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07032", "abs": "https://arxiv.org/abs/2512.07032", "authors": ["Runcong Wang", "Fengyi Wang", "Gordon Cheng"], "title": "A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator", "comment": null, "summary": "This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u5f02\u8054\u60f3\u5e8f\u5217\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u7ed1\u5b9a\u5b66\u4e60\u5173\u8282\u72b6\u6001\u4e0e\u89e6\u89c9\u89c2\u6d4b\u7684\u7d27\u51d1\u8868\u793a\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u5185\u5b58\u6210\u672c\u7684\u9010\u6b65\u52a8\u4f5c\u51b3\u7b56\u3002", "motivation": "\u79fb\u52a8\u673a\u68b0\u81c2\u9700\u8981\u80fd\u591f\u5feb\u901f\u5b66\u4e60\u3001\u7ecf\u6d4e\u9ad8\u6548\u5730\u54cd\u5e94\u89e6\u89c9\u8f93\u5165\u5e76\u6267\u884c\u590d\u6742\u52a8\u4f5c\u5e8f\u5217\u7684\u7cfb\u7edf\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u7f3a\u4e4f\u89e6\u89c9\u51e0\u4f55\u4fe1\u606f\u7684\u6709\u6548\u6574\u5408\u3002", "method": "\u4f7f\u7528\u7fa4\u4f53\u4f4d\u7f6e\u7f16\u7801\u8868\u793a\u5173\u8282\u89d2\u5ea6\uff0c\u901a\u8fc7Izhikevich\u795e\u7ecf\u5143\u6a21\u578b\u5c06\u76ae\u80a4\u6d4b\u5f97\u7684\u529b\u8f6c\u6362\u4e3a\u8109\u51b2\u7387\u7279\u5f81\uff0c\u8f6c\u5316\u4e3a\u53cc\u6781\u4e8c\u8fdb\u5236\u5411\u91cf\u540e\u9010\u5143\u7d20\u7ed1\u5b9a\u521b\u5efa\u5173\u8054\uff0c\u5b58\u50a8\u5728\u5927\u578b\u5bb9\u91cf\u5e8f\u5217\u8bb0\u5fc6\u4e2d\u3002\u5f15\u51653D\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u6839\u636e\u611f\u77e5\u529b\u65b9\u5411\u65cb\u8f6c\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7softmax\u52a0\u6743\u53ec\u56de\u5b9e\u73b0\u6a21\u7cca\u68c0\u7d22\u3002", "result": "\u5728\u4e30\u7530HSR\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u4f2a\u67d4\u987a\u63a7\u5236\u5668\uff0c\u80fd\u591f\u6839\u636e\u65bd\u52a0\u529b\u7684\u5927\u5c0f\u548c\u65b9\u5411\u79fb\u52a8\u8fde\u6746\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u89e6\u89c9\u8f93\u5165\u68c0\u7d22\u591a\u5173\u8282\u6293\u53d6\u5e8f\u5217\u3002\u7cfb\u7edf\u8bbe\u7f6e\u5feb\u901f\uff0c\u8bad\u7ec3\u540c\u6b65\u72b6\u6001\u548c\u89c2\u6d4b\u6d41\uff0c\u5177\u6709\u4e00\u5b9a\u6cdb\u5316\u80fd\u529b\u4e14\u4fdd\u6301\u7ecf\u6d4e\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u8054\u60f3\u53ec\u56de\u5b9e\u73b0\u4e86\u5355\u5173\u8282\u548c\u5168\u81c2\u884c\u4e3a\uff0c\u5c55\u793a\u4e86\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u591a\u6a21\u6001\u96c6\u6210\u65b9\u9762\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u89e6\u89c9\u5f15\u5bfc\u884c\u4e3a\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2512.07710", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07710", "abs": "https://arxiv.org/abs/2512.07710", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "comment": null, "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "AI": {"tldr": "CompassMax-V3-Thinking\u662f\u4e00\u4e2a\u5343\u4ebf\u89c4\u6a21\u7684MoE\u63a8\u7406\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u7684RL\u6846\u67b6\u8bad\u7ec3\uff0c\u6838\u5fc3\u539f\u5219\u662f\"\u6bcf\u4e2a\u63d0\u793a\u90fd\u5fc5\u987b\u91cd\u8981\"\u3002\u901a\u8fc7\u591a\u9879\u6280\u672f\u521b\u65b0\u89e3\u51b3\u4e86\u5927\u89c4\u6a21RL\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u5927\u89c4\u6a21MoE\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230\u5343\u4ebf\u89c4\u6a21\u65f6\u66b4\u9732\u4e86\u5173\u952e\u6548\u7387\u95ee\u9898\uff1a\u96f6\u65b9\u5dee\u63d0\u793a\u6d6a\u8d39rollout\u8d44\u6e90\u3001\u957f\u65f6\u57df\u91cd\u8981\u6027\u91c7\u6837\u4e0d\u7a33\u5b9a\u3001\u6807\u51c6\u5956\u52b1\u6a21\u578b\u5bfc\u81f4\u7684\u4f18\u52bf\u53cd\u8f6c\uff0c\u4ee5\u53carollout\u5904\u7406\u7684\u7cfb\u7edf\u6027\u74f6\u9888\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u4f7f\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684RL\u8bad\u7ec3\u7a33\u5b9a\u9ad8\u6548\u3002", "method": "1. \u591a\u9636\u6bb5\u96f6\u65b9\u5dee\u6d88\u9664\uff1a\u8fc7\u6ee4\u975e\u4fe1\u606f\u6027\u63d0\u793a\uff0c\u7a33\u5b9a\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\n2. ESPO\uff1a\u71b5\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\uff0c\u5e73\u8861token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u91c7\u6837\n3. Router Replay\u7b56\u7565\uff1a\u5bf9\u9f50\u8bad\u7ec3\u65f6MoE\u8def\u7531\u5668\u548c\u63a8\u7406\u65f6\u884c\u4e3a\uff0c\u9632\u6b62\u4f18\u52bf\u53cd\u8f6c\n4. \u9ad8\u541e\u5410RL\u7cfb\u7edf\uff1aFP8\u7cbe\u5ea6rollout\u3001\u91cd\u53e0\u5956\u52b1\u8ba1\u7b97\u3001\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6", "result": "\u6a21\u578b\u5728\u5185\u90e8\u548c\u516c\u5f00\u8bc4\u4f30\u4e2d\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5f62\u6210\u4e86\u4f7f\u5343\u4ebf\u89c4\u6a21MoE\u6a21\u578bRL\u8bad\u7ec3\u7a33\u5b9a\u9ad8\u6548\u7684\u6574\u4f53\u7ba1\u9053\u3002", "conclusion": "\u8fd9\u4e9b\u521b\u65b0\u6784\u6210\u4e86\u4e00\u4e2a\u8fde\u8d2f\u7684\u7ba1\u9053\uff0c\u4f7f\u5343\u4ebf\u89c4\u6a21MoE\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u53d8\u5f97\u7a33\u5b9a\u9ad8\u6548\u3002CompassMax-V3-Thinking\u6a21\u578b\u5c55\u793a\u4e86\u5728\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u4e2d\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.07761", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07761", "abs": "https://arxiv.org/abs/2512.07761", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "comment": "19 pages, 15 figures", "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u653b\u51fb\u8005LLM\u6765\u4ece\u9ed1\u76d2\u6a21\u578b\u4e2d\u5f15\u51fa\u6709\u5bb3\u5185\u5bb9\uff0c\u76f8\u6bd4\u5355\u8f6e\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u5f71\u54cd\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u8f6e\u4f18\u5316\uff0c\u4e0d\u8db3\u4ee5\u5b66\u4e60\u957f\u671f\u653b\u51fb\u7b56\u7565\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u8f6e\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u76f4\u63a5\u4f18\u5316\u6700\u7ec8\u8f6e\u8f93\u51fa\u7684\u6709\u5bb3\u6027\u4f5c\u4e3a\u7ed3\u679c\u5956\u52b1\u3002\u63d0\u51fa\u4e24\u79cd\u542f\u53d1\u5f0f\u8fc7\u7a0b\u5956\u52b1\uff1a1)\u63a7\u5236\u4e2d\u95f4\u8f93\u51fa\u7684\u6709\u5bb3\u6027\u4ee5\u907f\u514d\u89e6\u53d1\u9ed1\u76d2\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\uff1b2)\u4fdd\u6301\u4e2d\u95f4\u8f93\u51fa\u7684\u8bed\u4e49\u76f8\u5173\u6027\u4ee5\u907f\u514d\u504f\u79bb\u4e3b\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u6301\u7eed\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u591a\u8f6e\u653b\u51fb\u7b56\u7565\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5b9e\u65bd\u9ed1\u76d2\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u9632\u5fa1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2512.06353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06353", "abs": "https://arxiv.org/abs/2512.06353", "authors": ["Kaicheng Yang", "Kaisen Yang", "Baiting Wu", "Xun Zhang", "Qianrui Yang", "Haotong Qin", "He Zhang", "Yulun Zhang"], "title": "TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search", "comment": "Code and Supplementary Material could be found at https://github.com/racoonykc/TreeQ", "summary": "Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ", "AI": {"tldr": "TreeQ\u662f\u4e00\u4e2a\u7edf\u4e00\u7684DiT\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u7ed3\u6784\u641c\u7d22\u3001\u73af\u5883\u566a\u58f0\u5f15\u5bfc\u548c\u901a\u7528Monarch\u5206\u652f\u89e3\u51b3DiT\u91cf\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u9996\u6b21\u5728DiT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u635f\u76844\u4f4dPTQ\u6027\u80fd\u3002", "motivation": "DiT\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002\u73b0\u6709\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u5728U-Net\u4e0a\u6210\u529f\uff0c\u4f46\u5728DiT\u67b6\u6784\u4e0a\u5e94\u7528\u6709\u9650\u4e14\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "1. \u6811\u7ed3\u6784\u641c\u7d22(TSS)\uff1a\u5229\u7528DiT\u7684\u7ebf\u6027\u7279\u6027\u5728O(n)\u65f6\u95f4\u5185\u904d\u5386\u89e3\u7a7a\u95f4\uff1b2. \u73af\u5883\u566a\u58f0\u5f15\u5bfc(ENG)\uff1a\u4f7f\u7528\u5355\u4e00\u8d85\u53c2\u6570\u5bf9\u9f50PTQ\u548cQAT\u914d\u7f6e\uff1b3. \u901a\u7528Monarch\u5206\u652f(GMB)\uff1a\u7ed3\u6784\u5316\u7a00\u758f\u5206\u652f\u9632\u6b62\u8d85\u4f4e\u4f4d\u91cf\u5316\u4e2d\u7684\u4fe1\u606f\u74f6\u9888\u3002", "result": "\u5728DiT-XL/2\u6a21\u578b\u4e0a\uff0c\u5728W3A3\u548cW4A4 PTQ/PEFT\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9996\u6b21\u5728DiT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u635f\u76844\u4f4dPTQ\u6027\u80fd\u3002", "conclusion": "TreeQ\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86DiT\u91cf\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3aDiT\u6a21\u578b\u7684\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2512.07266", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07266", "abs": "https://arxiv.org/abs/2512.07266", "authors": ["Florian Tretter", "Daniel Fl\u00f6gel", "Alexandru Vasilache", "Max Grobbel", "J\u00fcrgen Becker", "S\u00f6ren Hohmann"], "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks", "comment": "8 pages, 6 figures", "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u793e\u4f1a\u96c6\u6210DRL\u65b9\u6cd5\uff0c\u7ed3\u5408SNN\u548cANN\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u793e\u4f1a\u5bfc\u822a\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017", "motivation": "\u5c06\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u96c6\u6210\u5230\u4eba\u7c7b\u73af\u5883\u4e2d\u9700\u8981\u7c7b\u4eba\u51b3\u7b56\u548c\u8282\u80fd\u7684\u4e8b\u4ef6\u9a71\u52a8\u8ba1\u7b97\u3002\u5c3d\u7ba1\u6709\u8fdb\u5c55\uff0c\u4f46\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5f88\u5c11\u5e94\u7528\u4e8eDRL\u5bfc\u822a\u65b9\u6cd5\uff0c\u56e0\u4e3a\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u91c7\u7528\u6df7\u5408\u793e\u4f1a\u96c6\u6210DRL\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff1a\u6f14\u5458\u4f7f\u7528SNN\uff0c\u8bc4\u8bba\u5bb6\u4f7f\u7528ANN\uff0c\u5e76\u914d\u5907\u795e\u7ecf\u5f62\u6001\u7279\u5f81\u63d0\u53d6\u5668\u6765\u6355\u6349\u65f6\u95f4\u4eba\u7fa4\u52a8\u6001\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "result": "\u63d0\u9ad8\u4e86\u793e\u4f1a\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u5c06\u4f30\u8ba1\u80fd\u8017\u964d\u4f4e\u4e86\u7ea61.69\u4e2a\u6570\u91cf\u7ea7\uff08\u7ea698%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728DRL\u5bfc\u822a\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8282\u80fd\u9ad8\u6548\u7684\u793e\u4f1a\u5bfc\u822a\u3002"}}
{"id": "2512.07472", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07472", "abs": "https://arxiv.org/abs/2512.07472", "authors": ["Siyu Xu", "Zijian Wang", "Yunke Wang", "Chenghao Xia", "Tao Huang", "Chang Xu"], "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($\u03c0_{0}$ and $\u03c0_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.", "AI": {"tldr": "\u63d0\u51faAffordance Field Intervention (AFI)\u6846\u67b6\uff0c\u901a\u8fc73D\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u573a\u5f15\u5bfcVLA\u6a21\u578b\uff0c\u89e3\u51b3\u5176\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8bb0\u5fc6\u9677\u9631\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u9c81\u68d2\u6027\u3002", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u5bb9\u6613\u9677\u5165\"\u8bb0\u5fc6\u9677\u9631\"\u2014\u2014\u91cd\u590d\u8bb0\u5fc6\u8f68\u8ff9\u800c\u975e\u9002\u5e94\u65b0\u73af\u5883\u3002\u8fd9\u6e90\u4e8e\u7aef\u5230\u7aef\u8bbe\u8ba1\u7f3a\u4e4f\u663e\u5f0f3D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u964c\u751f\u73af\u5883\u4e2d\u7684\u53ef\u64cd\u4f5c\u533a\u57df\u3002", "method": "\u63d0\u51faAFI\u6df7\u5408\u6846\u67b6\uff1a1) \u4f7f\u75283D\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u573a(SAFs)\u4f5c\u4e3a\u6309\u9700\u63d2\u4ef6\u63d0\u4f9b\u51e0\u4f55\u8868\u793a\uff1b2) \u901a\u8fc7\u672c\u4f53\u611f\u77e5\u68c0\u6d4b\u8bb0\u5fc6\u9677\u9631\uff1b3) \u5c06\u673a\u5668\u4eba\u91cd\u65b0\u5b9a\u4f4d\u5230\u9ad8\u53ef\u64cd\u4f5c\u6027\u533a\u57df\uff1b4) \u63d0\u51fa\u53ef\u64cd\u4f5c\u6027\u9a71\u52a8\u7684\u8def\u5f84\u70b9\u6765\u951a\u5b9aVLA\u751f\u6210\u7684\u52a8\u4f5c\uff1b5) \u57fa\u4e8eSAF\u7684\u8bc4\u5206\u5668\u9009\u62e9\u7d2f\u79ef\u53ef\u64cd\u4f5c\u6027\u6700\u9ad8\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff0c\u4e0d\u540cVLA\u9aa8\u5e72\u7f51\u7edc(\u03c0\u2080\u548c\u03c0\u2080.\u2085)\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u5e73\u5747\u63d0\u534723.5%\uff1b\u5728LIBERO-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u63d0\u534720.2%\uff0c\u663e\u8457\u589e\u5f3a\u4e86VLA\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "AFI\u6846\u67b6\u901a\u8fc7\u6574\u54083D\u7a7a\u95f4\u53ef\u64cd\u4f5c\u6027\u573a\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u8bb0\u5fc6\u9677\u9631\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df7\u5408\u65b9\u6cd5\uff0c\u5728\u4e0d\u6539\u53d8VLA\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07478", "abs": "https://arxiv.org/abs/2512.07478", "authors": ["Zhuoran Zhuang", "Ye Chen", "Jianghao Su", "Chao Luo", "Luhui Liu", "Xia Zeng"], "title": "Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization", "comment": null, "summary": "Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPRS\u548cVSPO\u4e24\u79cd\u6280\u672f\u89e3\u51b3\u5de5\u5177\u96c6\u6210\u63a8\u7406LLM\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff1aPRS\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5956\u52b1\u5851\u9020\u63d0\u4f9b\u5bc6\u96c6\u7684\u9636\u6bb5\u53cd\u9988\uff0cVSPO\u901a\u8fc7\u57fa\u4e8e\u4ef7\u503c\u7684\u91c7\u6837\u7b56\u7565\u4f18\u5316\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5de5\u5177\u96c6\u6210\u63a8\u7406LLM\u5728\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u4e2d\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u7a00\u758f\u7684\u4e8c\u5143\u5956\u52b1\u4fe1\u53f7\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u6307\u5bfc\u6709\u9650\u4e14\u6536\u655b\u6162\uff1b2) GRPO\u4e2d\u76f8\u540c\u5956\u52b1\u7ec4\u5bfc\u81f4\u96f6\u4f18\u52bf\u4f30\u8ba1\uff0c\u964d\u4f4e\u6837\u672c\u6548\u7387\u5e76\u7834\u574f\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u6280\u672f\uff1aPRS\uff08\u6e10\u8fdb\u5f0f\u5956\u52b1\u5851\u9020\uff09\u91c7\u7528\u8bfe\u7a0b\u5f0f\u5956\u52b1\u8bbe\u8ba1\uff0c\u5206\u9636\u6bb5\u63d0\u4f9b\u5bc6\u96c6\u53cd\u9988\uff1bVSPO\uff08\u57fa\u4e8e\u4ef7\u503c\u7684\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff09\u662fGRPO\u7684\u589e\u5f3a\u53d8\u4f53\uff0c\u901a\u8fc7\u4ef7\u503c\u5ea6\u91cf\u5e73\u8861\u96be\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u6765\u7b5b\u9009\u63d0\u793a\uff0c\u5e76\u5e94\u7528\u4ef7\u503c\u5e73\u6ed1\u88c1\u526a\u7a33\u5b9a\u68af\u5ea6\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u77ed\u5f62\u5f0f\u548c\u957f\u5f62\u5f0fQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRS\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u4e8c\u5143\u5956\u52b1\uff0cVSPO\u76f8\u6bd4PPO\u3001GRPO\u3001CISPO\u548cSFT\u57fa\u7ebf\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "PRS\u548cVSPO\u5171\u540c\u4ea7\u751f\u4e86\u80fd\u591f\u66f4\u597d\u8de8\u9886\u57df\u6cdb\u5316\u7684\u57fa\u4e8eLLM\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u548c\u68af\u5ea6\u9000\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2512.06434", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06434", "abs": "https://arxiv.org/abs/2512.06434", "authors": ["Lucas R. Mareque", "Ricardo L. Armentano", "Leandro J. Cymberknop"], "title": "Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening", "comment": "8 pages, 2 figures, 3 tables", "summary": "Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u81ea\u52a8\u65b9\u6cd5\uff0c\u4ece2D\u5408\u6210\u4eba\u4f53\u56fe\u50cf\u4f30\u8ba15\u9879\u5173\u952e\u4eba\u4f53\u6d4b\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u8fd0\u52a8\u5458\u5fc3\u8840\u7ba1\u98ce\u9669\u8bc4\u4f30\uff0c\u6a21\u578b\u8fbe\u5230\u4e9a\u5398\u7c73\u7ea7\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u5458\u5fc3\u8840\u7ba1\u68c0\u67e5\u4e2d\u7684\u4eba\u5de5\u6d4b\u91cf\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u64cd\u4f5c\u4f9d\u8d56\u6027\u5f3a\u3001\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u5927\u89c4\u6a21\u8fd0\u52a8\u5458\u7b5b\u67e5\u3002", "method": "\u4f7f\u7528\u4ece3D\u8eab\u4f53\u7f51\u683c\u751f\u6210\u7684100,000\u5f20\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5e76\u8bc4\u4f30VGG19\u3001ResNet50\u548cDenseNet121\u6a21\u578b\uff0c\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u8fdb\u884c\u56de\u5f52\u5206\u6790\uff0c\u9884\u6d4b5\u9879\u5173\u952e\u4eba\u4f53\u6d4b\u91cf\u6307\u6807\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u8fbe\u5230\u4e9a\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0cResNet50\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6240\u6709\u6d4b\u91cf\u6307\u6807\u4e0a\u7684\u5e73\u5747MAE\u4e3a0.668\u5398\u7c73\uff0c\u8bc1\u660e\u6df1\u5ea6\u5b66\u4e60\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u4eba\u4f53\u6d4b\u91cf\u6570\u636e\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u89c4\u6a21\u5316\u63d0\u4f9b\u51c6\u786e\u7684\u4eba\u4f53\u6d4b\u91cf\u6570\u636e\uff0c\u53ef\u4f5c\u4e3a\u8fd0\u52a8\u5458\u7b5b\u67e5\u534f\u8bae\u7684\u5b9e\u9645\u5de5\u5177\u8865\u5145\uff0c\u672a\u6765\u5c06\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u6a21\u578b\u4ee5\u6269\u5c55\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2512.07552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07552", "abs": "https://arxiv.org/abs/2512.07552", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries", "comment": "8 pages, 3 figures", "summary": "In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.", "AI": {"tldr": "SafeTerm AMQ\u7cfb\u7edf\u901a\u8fc7AI\u81ea\u52a8\u68c0\u7d22\u4e0e\u533b\u5b66\u67e5\u8be2\u76f8\u5173\u7684MedDRA\u672f\u8bed\uff0c\u5728\u836f\u7269\u5b89\u5168\u5ba1\u67e5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e73\u8861\u4e86\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u3002", "motivation": "\u5728\u836f\u7269\u4e0a\u5e02\u524d\u5b89\u5168\u5ba1\u67e5\u4e2d\uff0c\u5c06\u76f8\u5173\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\u5206\u7ec4\u5230SMQs\u6216OCMQs\u5bf9\u4fe1\u53f7\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u8bc4\u4f30\u81ea\u52a8\u5316\u7cfb\u7edf\u5728MedDRA SMQs\u4e0a\u7684\u6027\u80fd\uff0c\u4ee5\u8f85\u52a9\u4eba\u5de5\u5ba1\u67e5\u3002", "method": "SafeTerm AMQ\u662f\u4e00\u4e2a\u5b9a\u91cfAI\u7cfb\u7edf\uff0c\u5c06\u533b\u5b66\u67e5\u8be2\u672f\u8bed\u548cMedDRA PT\u5d4c\u5165\u591a\u7ef4\u5411\u91cf\u7a7a\u95f4\uff0c\u5e94\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6781\u503c\u805a\u7c7b\u751f\u6210\u6392\u540dPT\u5217\u8868\u3002\u5728110\u4e2aSMQs\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8ba1\u7b97\u4e0d\u540c\u76f8\u4f3c\u5ea6\u9608\u503c\u4e0b\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "result": "\u5728\u4e2d\u7b49\u76f8\u4f3c\u5ea6\u9608\u503c\u4e0b\u83b7\u5f97\u9ad8\u53ec\u56de\u7387(94%)\uff0c\u66f4\u9ad8\u9608\u503c\u53ef\u63d0\u9ad8\u7cbe\u786e\u7387(\u8fbe89%)\u3002\u6700\u4f18\u9608\u503c(0.70)\u4e0b\u603b\u4f53\u53ec\u56de\u738748%\u3001\u7cbe\u786e\u738745%\u3002\u7a84\u672f\u8bed\u5728\u66f4\u9ad8\u9608\u503c\u4e0b\u8868\u73b0\u66f4\u597d\u3002\u81ea\u52a8\u9608\u503c\u9009\u62e9(0.66)\u4f18\u5148\u53ec\u56de\u7387(0.58)\u800c\u975e\u7cbe\u786e\u7387(0.29)\u3002", "conclusion": "SafeTerm AMQ\u5728SMQs\u548cOCMQs\u4e0a\u8868\u73b0\u76f8\u5f53\u4e14\u4ee4\u4eba\u6ee1\u610f\uff0c\u662f\u81ea\u52a8\u751f\u6210MedDRA\u67e5\u8be2\u7684\u53ef\u884c\u8865\u5145\u65b9\u6cd5\uff0c\u80fd\u5e73\u8861\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u3002\u5efa\u8bae\u4f7f\u7528\u5408\u9002\u7684MedDRA PT\u672f\u8bed\u5e76\u5e94\u7528\u81ea\u52a8\u9608\u503c\u65b9\u6cd5\u4f18\u5316\u53ec\u56de\u7387\u3002"}}
{"id": "2512.06752", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06752", "abs": "https://arxiv.org/abs/2512.06752", "authors": ["Chang Liu", "Vivian Li", "Linus Leong", "Vladimir Radenkovic", "Pietro Li\u00f2", "Chaitanya K. Joshi"], "title": "Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets", "comment": "Presented at Machine Learning in Structural Biology, 2025. Open-source code: https://github.com/VirtualProteins/GNN_UNet", "summary": "Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.", "AI": {"tldr": "\u63d0\u51faGeometric Graph U-Nets\uff0c\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u9012\u5f52\u7c97\u5316\u548c\u7ec6\u5316\u86cb\u767d\u8d28\u56fe\u6765\u5b66\u4e60\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u5728\u86cb\u767d\u8d28\u6298\u53e0\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u51e0\u4f55\u56fe\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u4f9d\u8d56\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u86cb\u767d\u8d28\u529f\u80fd\u4e2d\u7684\u5c42\u6b21\u4ea4\u4e92\uff08\u5982\u5168\u5c40\u57df\u548c\u957f\u7a0b\u53d8\u6784\u8c03\u8282\uff09\u3002\u4f5c\u8005\u8ba4\u4e3a\u7f51\u7edc\u67b6\u6784\u672c\u8eab\u5e94\u8be5\u53cd\u6620\u8fd9\u79cd\u751f\u7269\u5c42\u6b21\u7ed3\u6784\u3002", "method": "\u5f15\u5165Geometric Graph U-Nets\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7c7b\u522b\uff0c\u901a\u8fc7\u9012\u5f52\u7c97\u5316\u548c\u7ec6\u5316\u86cb\u767d\u8d28\u56fe\u6765\u5b66\u4e60\u591a\u5c3a\u5ea6\u8868\u793a\u3002\u8be5\u5c42\u6b21\u5316\u8bbe\u8ba1\u7406\u8bba\u4e0a\u6bd4\u6807\u51c6\u51e0\u4f55GNN\u66f4\u5177\u8868\u8fbe\u529b\u3002", "result": "\u5728\u86cb\u767d\u8d28\u6298\u53e0\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cGeometric U-Nets\u663e\u8457\u4f18\u4e8e\u4e0d\u53d8\u548c\u7b49\u53d8\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u5b66\u4e60\u5b9a\u4e49\u86cb\u767d\u8d28\u6298\u53e0\u7684\u5168\u5c40\u7ed3\u6784\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bbe\u8ba1\u80fd\u591f\u5b66\u4e60\u751f\u7269\u5206\u5b50\u591a\u5c3a\u5ea6\u7ed3\u6784\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2512.06598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06598", "abs": "https://arxiv.org/abs/2512.06598", "authors": ["Muhammad Adil", "Patrick J. Clemins", "Andrew W. Schroth", "Panagiotis D. Oikonomou", "Donna M. Rizzo", "Peter D. F. Isles", "Xiaohan Zhang", "Kareem I. Hannoun", "Scott Turnbull", "Noah B. Beckage", "Asim Zia", "Safwan Wshah"], "title": "From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain", "comment": "23 pages, 15 figures", "summary": "Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7ed3\u5408Transformer\u548cBiLSTM\u7684\u9065\u611f\u9884\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u536b\u661f\u6570\u636e\u9884\u6d4b\u84dd\u85fb\u6c34\u534e\u5f3a\u5ea6\uff0c\u53ef\u5728\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u5b9e\u73b014\u5929\u63d0\u524d\u9884\u8b66\u3002", "motivation": "\u84dd\u85fb\u6709\u5bb3\u85fb\u534e\u5bf9\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u548c\u516c\u5171\u5065\u5eb7\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5c1a\u666e\u5170\u6e56\u7b49\u6c34\u57df\u56e0\u8425\u517b\u5bcc\u96c6\u548c\u6c14\u5019\u53d8\u5f02\u6027\u6613\u53d7\u5176\u5f71\u54cd\u3002\u9065\u611f\u6280\u672f\u53ef\u4e3a\u76d1\u6d4b\u548c\u9884\u62a5\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u8865\u73b0\u573a\u89c2\u6d4b\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528Transformer-BiLSTM\u6df7\u5408\u6a21\u578b\uff0c\u5229\u7528\u84dd\u85fb\u8bc4\u4f30\u7f51\u7edc\u7684\u84dd\u85fb\u6307\u6570\u548c\u4e2d\u5206\u8fa8\u7387\u6210\u50cf\u5149\u8c31\u4eea\u7684\u6e29\u5ea6\u6570\u636e\u3002\u9488\u5bf9\u6570\u636e\u7a00\u758f\u95ee\u9898\uff08\u84dd\u85fb\u6307\u6570\u7f3a\u593130%\uff0c\u6e29\u5ea6\u6570\u636e\u7f3a\u593190%\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u9884\u5904\u7406\u6d41\u7a0b\uff1a\u50cf\u7d20\u7ea7\u524d\u5411\u586b\u5145\u548c\u52a0\u6743\u65f6\u95f4\u63d2\u8865\uff0c\u7136\u540e\u8fdb\u884c\u5e73\u6ed1\u5904\u7406\u3002\u901a\u8fc7\u7b49\u9891\u5206\u7bb1\u63d0\u53d6\u84dd\u85fb\u6307\u6570\u7279\u5f81\uff0c\u5e76\u63d0\u53d6\u6e29\u5ea6\u7edf\u8ba1\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u8868\u73b0\u4f18\u5f02\uff1a1\u5929\u30012\u5929\u30013\u5929\u9884\u6d4b\u7684F1\u5206\u6570\u5206\u522b\u4e3a89.5%\u300186.4%\u300185.5%\uff1b14\u5929\u9884\u6d4b\u7684F1\u5206\u6570\u4e3a78.9%\uff0cAUC\u4e3a82.6%\u3002\u8bc1\u660e\u6a21\u578b\u80fd\u4ece\u7a00\u758f\u536b\u661f\u6570\u636e\u4e2d\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u52a8\u6001\u3002", "conclusion": "\u8be5\u9065\u611f\u9884\u6d4b\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u7a00\u758f\u536b\u661f\u6570\u636e\uff0c\u4e3a\u84dd\u85fb\u6c34\u534e\u7ba1\u7406\u63d0\u4f9b\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u73af\u5883\u76d1\u6d4b\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.07694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07694", "abs": "https://arxiv.org/abs/2512.07694", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map", "comment": "12 pages, 4 figures", "summary": "In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.", "AI": {"tldr": "SafeTerm\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5411\u91cf\u5d4c\u5165\u548c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u81ea\u52a8\u4eceMedDRA\u672f\u8bed\u4e2d\u68c0\u7d22\u76f8\u5173\u4e0d\u826f\u53cd\u5e94\u672f\u8bed\uff0c\u4e3a\u836f\u7269\u5b89\u5168\u5ba1\u67e5\u63d0\u4f9b\u81ea\u52a8\u5316\u67e5\u8be2\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5728\u836f\u7269\u4e0a\u5e02\u524d\u5b89\u5168\u5ba1\u67e5\u4e2d\uff0c\u5c06\u76f8\u5173\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\u5206\u7ec4\u5230\u6807\u51c6\u5316MedDRA\u67e5\u8be2\u6216FDA OCMQs\u4e2d\u5bf9\u4e8e\u4fe1\u53f7\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\uff0c\u6548\u7387\u8f83\u4f4e\u4e14\u53ef\u80fd\u5b58\u5728\u4e3b\u89c2\u504f\u5dee\u3002", "method": "\u7cfb\u7edf\u5c06\u533b\u5b66\u67e5\u8be2\u672f\u8bed\u548cMedDRA\u9996\u9009\u672f\u8bed\u5d4c\u5165\u5230\u591a\u7ef4\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u5e94\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6781\u503c\u805a\u7c7b\u65b9\u6cd5\uff0c\u751f\u6210\u6309\u76f8\u5173\u6027\u5206\u6570\u6392\u540d\u7684\u672f\u8bed\u5217\u8868\u3002", "result": "\u5728FDA OCMQ v3.0\uff08104\u4e2a\u67e5\u8be2\uff09\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u9ad8\u53ec\u56de\u7387\uff08>95%\uff09\u5728\u4e2d\u7b49\u9608\u503c\u4e0b\u5b9e\u73b0\uff0c\u66f4\u9ad8\u9608\u503c\u53ef\u5c06\u7cbe\u786e\u5ea6\u63d0\u9ad8\u523086%\u3002\u6700\u4f73\u9608\u503c\uff08~0.70-0.75\uff09\u4ea7\u751f\u7ea650%\u7684\u53ec\u56de\u7387\u548c33%\u7684\u7cbe\u786e\u5ea6\u3002", "conclusion": "SafeTerm AI\u9a71\u52a8\u7cfb\u7edf\u4e3a\u81ea\u52a8\u5316MedDRA\u67e5\u8be2\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8865\u5145\u65b9\u6cd5\u3002\u5efa\u8bae\u521d\u59cb\u4f7f\u7528\u7ea60.60\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\uff0c\u5bf9\u4e8e\u7cbe\u70bc\u672f\u8bed\u9009\u62e9\u53ef\u589e\u52a0\u9608\u503c\u3002"}}
{"id": "2512.07777", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07777", "abs": "https://arxiv.org/abs/2512.07777", "authors": ["Karin de Langis", "P\u00fcren \u00d6ncel", "Ryan Peters", "Andrew Elfenbein", "Laura Kristen Allen", "Andreas Schramm", "Dongyeop Kang"], "title": "Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?", "comment": null, "summary": "Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.", "AI": {"tldr": "LLMs\u80fd\u8bc6\u522b\u4e0d\u8fde\u8d2f\u6545\u4e8b\u4f46\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u5bf9\u8fdd\u53cd\u4e16\u754c\u77e5\u8bc6\u7684\u9519\u8bef\u66f4\u654f\u611f\uff0c\u8868\u660e\u5bf9\u53d9\u4e8b\u8fde\u8d2f\u6027\u7406\u89e3\u4e0d\u5b8c\u6574", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u53ef\u9760\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u7684\u6545\u4e8b\uff0c\u63a2\u7d22\u5176\u53d9\u4e8b\u7406\u89e3\u80fd\u529b", "method": "\u4f7f\u7528\u914d\u5bf9\u53d9\u4e8b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u63a2\u6d4b\u7814\u7a76\u5206\u6790LLMs\u5185\u90e8\u8868\u5f81\uff0c\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u53d8\u4f53\u4e0b\u7684\u56de\u7b54\u8868\u73b0", "result": "LLMs\u5185\u90e8\u8868\u5f81\u80fd\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u4f46\u751f\u6210\u56de\u7b54\u65e0\u6cd5\u53ef\u9760\u533a\u5206\uff1b\u5bf9\u8fdd\u53cd\u8bbe\u5b9a\u7684\u9519\u8bef\u6bd4\u5bf9\u8fdd\u53cd\u89d2\u8272\u7279\u8d28\u7684\u9519\u8bef\u66f4\u654f\u611f\uff1b\u601d\u7ef4\u94fe\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u5185\u90e8\u72b6\u6001\u4e0e\u884c\u4e3a\u5dee\u5f02", "conclusion": "LLMs\u5bf9\u53d9\u4e8b\u8fde\u8d2f\u6027\u7406\u89e3\u4e0d\u5b8c\u6574\uff0c\u66f4\u4f9d\u8d56\u539f\u578b\u4e16\u754c\u77e5\u8bc6\u800c\u975e\u57fa\u4e8e\u610f\u4e49\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u6784\u5efa"}}
{"id": "2512.06673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06673", "abs": "https://arxiv.org/abs/2512.06673", "authors": ["Shida Gao", "Feng Xue", "Xiangfeng Wang", "Anlong Ming", "Teng Long", "Yihua Shao", "Haozhe Wang", "Zhaowen Lin", "Wei Wang", "Nicu Sebe"], "title": "1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning", "comment": null, "summary": "Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.", "AI": {"tldr": "DEViL\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u53c2\u8003\u8bed\u4e49\u4ee4\u724c\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u89e3\u51b3\u65f6\u7a7a\u5b9a\u4f4d\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u5728STVG\u548cGroundedVQA\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u7a7a\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u91c7\u7528\u81ea\u56de\u5f52\u7a7a\u95f4\u89e3\u7801\uff0c\u5bfc\u81f4\u8f93\u51fa\u5e8f\u5217\u8fc7\u957f\uff0c\u7a7a\u95f4\u8bef\u5dee\u968f\u65f6\u95f4\u7d2f\u79ef\uff0c\u5b9a\u4f4d\u7ed3\u679c\u5728\u89c6\u9891\u4e2d\u9010\u6e10\u6f02\u79fb\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u65f6\u7a7a\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faDEViL\u7cfb\u7edf\uff0c\u5c06\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u901a\u8fc7\u53c2\u8003\u8bed\u4e49\u4ee4\u724c\u8fde\u63a5\u3002RST\u65e2\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\uff0c\u53c8\u66ff\u4ee3\u68c0\u6d4b\u5668\u7684\u6587\u672c\u5d4c\u5165\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\u3002\u540c\u65f6\u63d0\u51fa\u7ba1\u72b6\u6316\u6398\u65f6\u95f4\u6b63\u5219\u5316\uff0c\u786e\u4fdd\u68c0\u6d4b\u5668\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u76ee\u6807\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDEViL\u5728\u5404\u79cd\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u7279\u522b\u662f\u5728\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u548c\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "DEViL\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u7a7a\u95f4\u89e3\u7801\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u4e00\u81f4\u7684\u65f6\u7a7a\u5b9a\u4f4d\u4e0e\u63a8\u7406\u3002"}}
{"id": "2512.07141", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07141", "abs": "https://arxiv.org/abs/2512.07141", "authors": ["Fenghua Weng", "Chaochao Lu", "Xia Hu", "Wenqi Shao", "Wenjie Wang"], "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models", "comment": null, "summary": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.", "AI": {"tldr": "\u63d0\u51faThink-Reflect-Revise (TRR)\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u5f15\u5bfc\u7684\u81ea\u6211\u53cd\u601d\u589e\u5f3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u5c06\u5b89\u5168\u54cd\u5e94\u7387\u4ece42.8%\u63d0\u5347\u81f387.7%\u3002", "motivation": "\u73b0\u6709\u5355\u6b21\u63a8\u7406\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u4e0a\u4e0b\u6587\u6216\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u81ea\u8eab\u8f93\u51fa\u4e2d\u7684\u6709\u5bb3\u5185\u5bb9\u3002\u5173\u952e\u6d1e\u5bdf\u662f\u5229\u7528\u88ab\u6d6a\u8d39\u7684\u4fe1\u53f7\u2014\u2014\u901a\u8fc7\u53cd\u601d\u5229\u7528\u7b2c\u4e00\u8f6e\u63a8\u7406\u4e2d\u63ed\u793a\u7684\u6076\u610f\u5185\u5bb9\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u6211\u4fee\u6b63\u3002", "method": "1) \u6784\u5efa\u5305\u542b5000\u4e2a\u793a\u4f8b\u7684Reflective Safety Reasoning (ReSafe)\u6570\u636e\u96c6\uff0c\u9075\u5faathink-reflect-revise\u6d41\u7a0b\uff1b2) \u4f7f\u7528ReSafe\u6570\u636e\u96c6\u5fae\u8c03\u76ee\u6807\u6a21\u578b\u4ee5\u521d\u59cb\u5316\u53cd\u601d\u884c\u4e3a\uff1b3) \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f3a\u5316\u7b56\u7565\u5f15\u5bfc\u7684\u53cd\u601d\u3002", "result": "TRR\u663e\u8457\u63d0\u5347\u4e86LVLMs\u5728\u5b89\u5168\u611f\u77e5\u57fa\u51c6\u548c\u8d8a\u72f1\u653b\u51fb\u8bc4\u4f30\u4e2d\u7684\u5b89\u5168\u6027\u80fd\uff0cQwen2.5-VL-7B\u7684\u6574\u4f53\u5b89\u5168\u54cd\u5e94\u7387\u4ece42.8%\u63d0\u9ad8\u523087.7%\uff0c\u540c\u65f6\u5728MMMU\u548cMMStar\u7b49\u901a\u7528\u57fa\u51c6\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684TRR\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u673a\u5236\u6709\u6548\u589e\u5f3a\u4e86LVLMs\u7684\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5355\u6b21\u63a8\u7406\u65b9\u6cd5\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5728\u4fdd\u6301\u901a\u7528\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u5b89\u5168\u54cd\u5e94\u7387\u3002"}}
{"id": "2512.07198", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07198", "abs": "https://arxiv.org/abs/2512.07198", "authors": ["Xiujie Song", "Qi Jia", "Shota Watanabe", "Xiaoyi Pang", "Ruijie Chen", "Mengyue Wu", "Kenny Q. Zhu"], "title": "Generating Storytelling Images with Rich Chains-of-Reasoning", "comment": null, "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.", "AI": {"tldr": "\u63d0\u51faStorytelling Image Generation\u4efb\u52a1\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7ba1\u9053StorytellingPainter\u7ed3\u5408LLM\u548cT2I\u6a21\u578b\u751f\u6210\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u548c\u63a8\u7406\u94fe\u7684\u53d9\u4e8b\u56fe\u50cf\uff0c\u5e76\u5f00\u53d1\u4e13\u7528\u8bc4\u4f30\u6846\u67b6\u548c\u8f7b\u91cf\u7ea7Mini-Storytellers\u6a21\u578b\u3002", "motivation": "\u53d9\u4e8b\u56fe\u50cf\u80fd\u901a\u8fc7\u4e30\u5bcc\u7684\u89c6\u89c9\u7ebf\u7d22\u4f20\u8fbe\u590d\u6742\u6545\u4e8b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u7684\u8bed\u4e49\u7279\u6027\uff0c\u8fd9\u7c7b\u56fe\u50cf\u96be\u4ee5\u521b\u5efa\u4e14\u6570\u91cf\u7a00\u5c11\u3002\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5229\u7528\u751f\u6210\u5f0fAI\u6a21\u578b\u6765\u521b\u5efa\u8fd9\u7c7b\u56fe\u50cf\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7ba1\u9053StorytellingPainter\uff1a1\uff09\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u521b\u9020\u6027\u63a8\u7406\u751f\u6210\u6545\u4e8b\uff1b2\uff09\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u5408\u6210\u3002\u540c\u65f6\u5f00\u53d1\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u8bc4\u4f30\u5668\u7684\u4e13\u7528\u8bc4\u4f30\u6846\u67b6\uff1a\u8bed\u4e49\u590d\u6742\u6027\u8bc4\u4f30\u5668\u3001\u57fa\u4e8eKNN\u7684\u591a\u6837\u6027\u8bc4\u4f30\u5668\u548c\u6545\u4e8b-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u5668\u3002\u9488\u5bf9\u5f00\u6e90\u4e0e\u4e13\u6709LLM\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63a2\u7d22\u5b9a\u5236\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5f00\u53d1\u8f7b\u91cf\u7ea7Mini-Storytellers\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002StorytellingPainter\u80fd\u591f\u751f\u6210\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u8fde\u63a5\u7684\u53d9\u4e8b\u56fe\u50cf\uff0c\u8bc4\u4f30\u6846\u67b6\u80fd\u5168\u9762\u8bc4\u4f30\u751f\u6210\u8d28\u91cf\uff0cMini-Storytellers\u6a21\u578b\u6709\u6548\u7f29\u5c0f\u4e86\u5f00\u6e90\u4e0e\u4e13\u6709LLM\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u6210\u529f\u5b9a\u4e49\u4e86Storytelling Image Generation\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u751f\u6210\u7ba1\u9053\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u89e3\u51b3\u4e86\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u53d9\u4e8b\u56fe\u50cf\u7684\u81ea\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.06840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06840", "abs": "https://arxiv.org/abs/2512.06840", "authors": ["Satoshi Hashimoto", "Tatsuya Konishi", "Tomoya Kaichi", "Kazunori Matsumoto", "Mori Kurokawa"], "title": "CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles", "comment": "Accepted to WACV 2026", "summary": "Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the \"incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.", "AI": {"tldr": "CADE\u662f\u9996\u4e2a\u5c06\u6301\u7eed\u5b66\u4e60\u4e0e\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u751f\u6210\u5668\u548c\u591a\u5224\u522b\u5668\u96c6\u6210\u6765\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u3001\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u548c\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u9759\u6001\u6570\u636e\u96c6\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u57df\u53ef\u80fd\u53d8\u5316\u7684\u95ee\u9898\u3002\u5f53\u6570\u636e\u57df\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u4ec5\u7528\u65b0\u6570\u636e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u4e0b\u964d\uff08\u9057\u5fd8\uff09\uff0c\u56e0\u6b64\u9700\u8981\u6301\u7eed\u5b66\u4e60\u89c6\u89d2\u3002", "method": "\u63d0\u51faCADE\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u53cc\u751f\u6210\u5668\uff08DG\uff09\u5904\u7406WVAD\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff1b2\uff09\u63d0\u51fa\u591a\u5224\u522b\u5668\uff08MD\uff09\u96c6\u6210\uff0c\u6355\u6349\u56e0\u9057\u5fd8\u800c\u9519\u8fc7\u7684\u8fc7\u53bb\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u5728ShanghaiTech\u548cCharlotte Anomaly\u7b49\u591a\u573a\u666fVAD\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCADE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VAD\u65b9\u6cd5\u3002", "conclusion": "CADE\u662f\u9996\u4e2a\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u548c\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u57df\u53d8\u5316\u3001\u9057\u5fd8\u548c\u5f02\u5e38\u68c0\u6d4b\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u5728\u516c\u5171\u5b89\u5168\u548c\u72af\u7f6a\u9884\u9632\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.07400", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07400", "abs": "https://arxiv.org/abs/2512.07400", "authors": ["Giulia Lanzillotta", "Damiano Meier", "Thomas Hofmann"], "title": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse", "comment": null, "summary": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7279\u5f81\u7a7a\u95f4\u4e0e\u5206\u7c7b\u5668\u5c42\u9762\u9057\u5fd8\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u53d1\u73b0\u5c0f\u7f13\u51b2\u533a\u8db3\u4ee5\u9632\u6b62\u6df1\u5ea6\u7279\u5f81\u9057\u5fd8\uff0c\u4f46\u9700\u8981\u5927\u7f13\u51b2\u533a\u624d\u80fd\u7f13\u89e3\u6d45\u5c42\u5206\u7c7b\u5668\u9057\u5fd8\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u6301\u4e45\u6096\u8bba\uff1a\u795e\u7ecf\u7f51\u7edc\u5373\u4f7f\u8f93\u51fa\u9884\u6d4b\u5931\u8d25\uff0c\u4ecd\u80fd\u4fdd\u7559\u8fc7\u53bb\u4efb\u52a1\u7684\u7ebf\u6027\u53ef\u5206\u8868\u793a\u3002\u4f5c\u8005\u5e0c\u671b\u5f62\u5f0f\u5316\u7279\u5f81\u7a7a\u95f4\u4e0e\u5206\u7c7b\u5668\u5c42\u9762\u9057\u5fd8\u7684\u533a\u522b\uff0c\u5e76\u89e3\u91ca\u7ecf\u9a8c\u56de\u653e\u4e2d\u7f13\u51b2\u533a\u5bb9\u91cf\u7684\u4e0d\u5bf9\u79f0\u9700\u6c42\u3002", "method": "\u5c06\u795e\u7ecf\u5d29\u6e83\u6846\u67b6\u6269\u5c55\u5230\u5e8f\u5217\u8bbe\u7f6e\uff0c\u5206\u6790\u6df1\u5ea6\u9057\u5fd8\u4f5c\u4e3a\u5411\u5206\u5e03\u5916\u5b50\u7a7a\u95f4\u7684\u51e0\u4f55\u6f02\u79fb\uff0c\u8bc1\u660e\u4efb\u4f55\u975e\u96f6\u56de\u653e\u5206\u6570\u90fd\u80fd\u4fdd\u8bc1\u7ebf\u6027\u53ef\u5206\u6027\u7684\u4fdd\u7559\u3002\u540c\u65f6\u8bc6\u522b\u5c0f\u7f13\u51b2\u533a\u5f15\u8d77\u7684\"\u5f3a\u5d29\u6e83\"\u5bfc\u81f4\u534f\u65b9\u5dee\u79e9\u4e0d\u8db3\u548c\u7c7b\u522b\u5747\u503c\u81a8\u80c0\uff0c\u4f7f\u5206\u7c7b\u5668\u65e0\u6cd5\u8bc6\u522b\u771f\u5b9e\u7fa4\u4f53\u8fb9\u754c\u3002", "result": "\u63ed\u793a\u4e86\u7ecf\u9a8c\u56de\u653e\u4e2d\u7684\u5173\u952e\u4e0d\u5bf9\u79f0\u6027\uff1a\u6700\u5c0f\u7f13\u51b2\u533a\u6210\u529f\u951a\u5b9a\u7279\u5f81\u51e0\u4f55\u5e76\u9632\u6b62\u6df1\u5ea6\u9057\u5fd8\uff0c\u4f46\u7f13\u89e3\u6d45\u5c42\u9057\u5fd8\u901a\u5e38\u9700\u8981\u66f4\u5927\u7684\u7f13\u51b2\u533a\u5bb9\u91cf\u3002\u8bc1\u660e\u4e86\u975e\u96f6\u56de\u653e\u5206\u6570\u80fd\u4fdd\u8bc1\u7ebf\u6027\u53ef\u5206\u6027\u7684\u4fdd\u7559\uff0c\u800c\u5c0f\u7f13\u51b2\u533a\u4f1a\u5bfc\u81f4\u7edf\u8ba1\u4f2a\u5f71\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6301\u7eed\u5b66\u4e60\u4e0e\u5206\u5e03\u5916\u68c0\u6d4b\u7edf\u4e00\uff0c\u6311\u6218\u4e86\u5bf9\u5927\u7f13\u51b2\u533a\u7684\u666e\u904d\u4f9d\u8d56\uff0c\u8868\u660e\u663e\u5f0f\u7ea0\u6b63\u8fd9\u4e9b\u7edf\u8ba1\u4f2a\u5f71\u53ef\u80fd\u4ee5\u6700\u5c0f\u56de\u653e\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2512.07417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07417", "abs": "https://arxiv.org/abs/2512.07417", "authors": ["Giray \u00d6n\u00fcr", "Azita Dabiri", "Bart De Schutter"], "title": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning", "comment": null, "summary": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u72b6\u6001\u53cd\u9988\u4ea4\u901a\u63a7\u5236\u5668\u7684\u53c2\u6570\uff0c\u7ed3\u5408\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u53cd\u5e94\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u63d0\u9ad8\u4ea4\u901a\u63a7\u5236\u6548\u679c\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u7ba1\u7406\u7b56\u7565\uff08\u5982\u8def\u7ebf\u5f15\u5bfc\u3001\u531d\u9053\u63a7\u5236\u3001\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff09\u901a\u5e38\u4f9d\u8d56\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u867d\u7136\u7b80\u5355\u53cd\u5e94\u5feb\uff0c\u4f46\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u591a\u53d8\u7684\u4ea4\u901a\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u8c03\u6574\u72b6\u6001\u53cd\u9988\u4ea4\u901a\u63a7\u5236\u5668\u7684\u53c2\u6570\uff08\u800c\u975e\u76f4\u63a5\u9ad8\u9891\u63a7\u5236\uff09\uff0c\u4ee5\u8f83\u4f4e\u9891\u7387\u8c03\u6574\u53c2\u6570\uff0c\u7ed3\u5408\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u53cd\u5e94\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\u3002\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u5c40\u90e8\u63a7\u5236\u5668\u5728\u90e8\u5206\u6545\u969c\u65f6\u53ef\u72ec\u7acb\u8fd0\u884c\u3002", "result": "\u5728\u6a21\u62df\u7684\u591a\u7c7b\u522b\u4ea4\u901a\u7f51\u7edc\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff1a\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4f18\u4e8e\u65e0\u63a7\u5236\u548c\u56fa\u5b9a\u53c2\u6570\u72b6\u6001\u53cd\u9988\u63a7\u5236\uff0c\u4e0e\u5355\u667a\u80fd\u4f53RL\u81ea\u9002\u5e94\u72b6\u6001\u53cd\u9988\u63a7\u5236\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u5bf9\u90e8\u5206\u6545\u969c\u7684\u6062\u590d\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u53cd\u5e94\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u53c2\u6570\u8c03\u6574\u800c\u975e\u76f4\u63a5\u63a7\u5236\u5b9e\u73b0\u4e86\u8bad\u7ec3\u6548\u7387\u63d0\u5347\uff0c\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2512.07430", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07430", "abs": "https://arxiv.org/abs/2512.07430", "authors": ["Yangle Li", "Danli Luo", "Haifeng Hu"], "title": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis", "comment": null, "summary": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.", "AI": {"tldr": "\u63d0\u51faMIDG\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e0d\u53d8\u4e13\u5bb6\u6a21\u578b\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u589e\u5f3a\u6a21\u6001\u95f4\u534f\u540c\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u8de8\u6a21\u6001\u9002\u914d\u5668\u901a\u8fc7\u77e5\u8bc6\u6ce8\u5165\u4e30\u5bcc\u591a\u6a21\u6001\u8868\u793a\uff0c\u5728\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u5728\u63d0\u53d6\u4e0d\u53d8\u7279\u5f81\u65f6\u5ffd\u7565\u4e86\u6a21\u6001\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u591a\u6a21\u6001\u6570\u636e\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u3002\u540c\u65f6\uff0c\u77e5\u8bc6\u6ce8\u5165\u6280\u672f\u5b58\u5728\u8de8\u6a21\u6001\u77e5\u8bc6\u788e\u7247\u5316\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u8d85\u8d8a\u5355\u6a21\u6001\u754c\u9650\u7684\u7279\u5b9a\u8868\u793a\u3002", "method": "1. \u6df7\u5408\u4e0d\u53d8\u4e13\u5bb6\u6a21\u578b\uff1a\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u6a21\u6001\u95f4\u534f\u540c\u5173\u7cfb\u7684\u80fd\u529b\u30022. \u8de8\u6a21\u6001\u9002\u914d\u5668\uff1a\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u6ce8\u5165\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u7684\u8bed\u4e49\u4e30\u5bcc\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u9886\u57df\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MIDG\u6846\u67b6\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684MIDG\u6846\u67b6\u901a\u8fc7\u6709\u6548\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u6ce8\u5165\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9886\u57df\u6cdb\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.07078", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07078", "abs": "https://arxiv.org/abs/2512.07078", "authors": ["Bo Gao", "Jingcheng Tong", "Xingsheng Chen", "Han Yu", "Zichen Li"], "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection", "comment": "16 pages", "summary": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.\n  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.\n  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.", "AI": {"tldr": "DFIR-DETR\uff1a\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u9065\u611f\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u548c\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u7684\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u805a\u5408\u548c\u9891\u57df\u5904\u7406\u89e3\u51b3\u7279\u5f81\u9000\u5316\u3001\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u7279\u5f81\u81a8\u80c0\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u9065\u611f\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u548c\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u9762\u4e34\u5171\u540c\u6311\u6218\uff1a\u7279\u5f81\u7a00\u758f\u4e14\u5f31\u3001\u80cc\u666f\u6742\u4e71\u3001\u76ee\u6807\u5c3a\u5ea6\u53d8\u5316\u5927\u3002\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u68c0\u6d4b\u5668\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7279\u5f81\u968f\u7f51\u7edc\u4e0b\u91c7\u6837\u4e25\u91cd\u9000\u5316\u3001\u7a7a\u95f4\u5377\u79ef\u65e0\u6cd5\u6709\u6548\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u6807\u51c6\u4e0a\u91c7\u6837\u65b9\u6cd5\u5bfc\u81f4\u7279\u5f81\u56fe\u4e0d\u5fc5\u8981\u81a8\u80c0\u3002", "method": "\u63d0\u51faDFIR-DETR\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) DCFA\u6a21\u5757\u4f7f\u7528\u52a8\u6001K\u7a00\u758f\u6ce8\u610f\u529b\u5c06\u590d\u6742\u5ea6\u4eceO(N\u00b2)\u964d\u81f3O(NK)\uff0c\u5e76\u91c7\u7528\u7a7a\u95f4\u95e8\u63a7\u7ebf\u6027\u5355\u5143\u589e\u5f3a\u975e\u7ebf\u6027\u5efa\u6a21\uff1b2) DFPN\u6a21\u5757\u5e94\u7528\u5e45\u5ea6\u5f52\u4e00\u5316\u4e0a\u91c7\u6837\u9632\u6b62\u7279\u5f81\u81a8\u80c0\uff0c\u4f7f\u7528\u53cc\u8def\u5f84\u6d17\u724c\u5377\u79ef\u4fdd\u7559\u8de8\u5c3a\u5ea6\u7a7a\u95f4\u7ec6\u8282\uff1b3) FIRC3\u6a21\u5757\u5728\u9891\u57df\u64cd\u4f5c\uff0c\u5b9e\u73b0\u5168\u5c40\u611f\u53d7\u91ce\u800c\u4e0d\u727a\u7272\u6548\u7387\u3002", "result": "\u5728NEU-DET\u548cVisDrone\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523092.9%\u548c51.6%\u7684mAP50\u5206\u6570\uff0c\u5747\u4e3a\u5f53\u524d\u6700\u4f18\u3002\u6a21\u578b\u8f7b\u91cf\u7ea7\uff0c\u4ec511.7M\u53c2\u6570\u548c41.2 GFLOPs\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DFIR-DETR\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u805a\u5408\u548c\u9891\u57df\u5904\u7406\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4e24\u4e2a\u4e0d\u540c\u9886\u57df\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8de8\u573a\u666f\u5c0f\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2512.07624", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07624", "abs": "https://arxiv.org/abs/2512.07624", "authors": ["Yongbo Yu", "Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Time Series Foundation Models for Process Model Forecasting", "comment": null, "summary": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.", "AI": {"tldr": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u5728\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\uff08PMF\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u96f6\u6837\u672c\u4f7f\u7528\u5373\u53ef\u83b7\u5f97\u826f\u597d\u6548\u679c\uff0c\u5fae\u8c03\u63d0\u5347\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u76f4\u63a5\u8ddf\u968f\u5173\u7cfb\u65f6\u95f4\u5e8f\u5217\u7684\u7a00\u758f\u6027\u548c\u5f02\u8d28\u6027\u3002\u7814\u7a76\u63a2\u7d22\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u8bc4\u4f30\u5176\u5728PMF\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u751f\u6210\u7684\u76f4\u63a5\u8ddf\u968f\u5173\u7cfb\u65f6\u95f4\u5e8f\u5217\uff0c\u6bd4\u8f83\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u4f7f\u7528\uff08\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff09\u4e0e\u5728PMF\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u53d8\u4f53\uff0c\u5e76\u4e0e\u4ece\u5934\u8bad\u7ec3\u7684\u4f20\u7edf\u548c\u4e13\u95e8\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u5e38\u6bd4\u4f20\u7edf\u548c\u4e13\u95e8\u6a21\u578b\u83b7\u5f97\u66f4\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\uff08MAE\u548cRMSE\uff09\uff0c\u8868\u660e\u5176\u80fd\u6709\u6548\u4ece\u975e\u8fc7\u7a0b\u9886\u57df\u8fc1\u79fb\u65f6\u95f4\u7ed3\u6784\u77e5\u8bc6\u3002\u5fae\u8c03\u867d\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u589e\u76ca\u901a\u5e38\u8f83\u5c0f\uff0c\u5728\u8f83\u5c0f\u6216\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u6d88\u5931\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u8fc7\u7a0b\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\uff0c\u96f6\u6837\u672c\u4f7f\u7528\u662f\u6709\u6548\u7684\u9ed8\u8ba4\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u65f6\u95f4\u57fa\u7840\u6a21\u578b\u5728\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2512.07723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07723", "abs": "https://arxiv.org/abs/2512.07723", "authors": ["Yonggeon Lee", "Jibin Hwang", "Alfred Malengo Kondoro", "Juhyun Song", "Youngtae Noh"], "title": "Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity", "comment": "16 pages, 9 figures, AAAI'26 (accepted)", "summary": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \\ours algorithm and its contribution to sustainable transportation systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u65f6\u95f4\u5230\u4e8b\u4ef6\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u7535\u52a8\u6c7d\u8f66\u7528\u6237\u7684\u51fa\u53d1\u65f6\u95f4\uff0c\u4ee5\u4f18\u5316\u5145\u7535\u7b56\u7565\u5e76\u5ef6\u957f\u7535\u6c60\u5bff\u547d\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u9502\u7535\u6c60\u5728\u9ad8\u7535\u91cf\u72b6\u6001\u4e0b\u4f1a\u52a0\u901f\u9000\u5316\uff0c\u53ef\u4ee5\u901a\u8fc7\u5ef6\u8fdf\u5145\u6ee1\u7535\u81f3\u51fa\u53d1\u524d\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u9700\u8981\u51c6\u786e\u9884\u6d4b\u7528\u6237\u51fa\u53d1\u65f6\u95f4\u3002", "method": "\u91c7\u7528Transformer-based\u5b9e\u65f6\u5230\u4e8b\u4ef6\u6a21\u578b\uff0c\u5c06\u6bcf\u5929\u7684\u65f6\u95f4\u79bb\u6563\u5316\u4e3a\u7f51\u683c\u5316token\u5e8f\u5217\uff0c\u5229\u7528\u6d41\u5f0f\u4e0a\u4e0b\u6587\u4fe1\u606f\u800c\u975e\u4ec5\u4f9d\u8d56\u5386\u53f2\u6a21\u5f0f\u6765\u9884\u6d4b\u51fa\u53d1\u65f6\u95f4\u3002", "result": "\u572893\u540d\u7528\u6237\u7684\u771f\u5b9e\u4e16\u754c\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528\u88ab\u52a8\u667a\u80fd\u624b\u673a\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u4e2a\u4eba\u65e5\u5e38\u4e2d\u7684\u4e0d\u89c4\u5219\u51fa\u53d1\u6a21\u5f0f\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u53ef\u6301\u7eed\u4ea4\u901a\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u5145\u7535\u7b56\u7565\u5ef6\u957f\u7535\u6c60\u5bff\u547d\u3002"}}
{"id": "2512.07328", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07328", "abs": "https://arxiv.org/abs/2512.07328", "authors": ["Ziyang Mai", "Yu-Wing Tai"], "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation", "comment": null, "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.", "AI": {"tldr": "ContextAnyone\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u5b9e\u73b0\u89d2\u8272\u4e00\u81f4\u6027\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u53d1\u578b\u3001\u670d\u88c5\u3001\u4f53\u578b\u7b49\u4e0a\u4e0b\u6587\u7279\u5f81\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u901a\u5e38\u53ea\u5173\u6ce8\u9762\u90e8\u8eab\u4efd\uff0c\u800c\u5ffd\u7565\u4e86\u53d1\u578b\u3001\u670d\u88c5\u3001\u4f53\u578b\u7b49\u5173\u952e\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u4e8e\u89c6\u89c9\u8fde\u8d2f\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faContextAnyone\u6846\u67b6\uff0c\u91c7\u7528DiT-based\u6269\u6563\u4e3b\u5e72\uff0c\u901a\u8fc7Emphasize-Attention\u6a21\u5757\u9009\u62e9\u6027\u5f3a\u5316\u53c2\u8003\u611f\u77e5\u7279\u5f81\uff0c\u9632\u6b62\u8eab\u4efd\u6f02\u79fb\uff1b\u4f7f\u7528\u53cc\u5f15\u5bfc\u635f\u5931\u7ed3\u5408\u6269\u6563\u548c\u53c2\u8003\u91cd\u5efa\u76ee\u6807\uff1b\u63d0\u51faGap-RoPE\u4f4d\u7f6e\u5d4c\u5165\u5206\u79bb\u53c2\u8003\u548c\u89c6\u9891token\u4ee5\u7a33\u5b9a\u65f6\u5e8f\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660eContextAnyone\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u53c2\u8003\u5230\u89c6\u9891\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u8de8\u4e0d\u540c\u52a8\u4f5c\u548c\u573a\u666f\u7684\u8fde\u8d2f\u4e14\u4fdd\u6301\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u89d2\u8272\u89c6\u9891\u3002", "conclusion": "ContextAnyone\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89d2\u8272\u4e00\u81f4\u6027\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2512.07247", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07247", "abs": "https://arxiv.org/abs/2512.07247", "authors": ["Ziming Hong", "Tianyu Huang", "Runnan Chen", "Shanshan Ye", "Mingming Gong", "Bo Han", "Tongliang Liu"], "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing", "comment": "40 pages, 34 figures, 18 tables", "summary": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.", "AI": {"tldr": "AdLift\u662f\u9996\u4e2a\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u7f16\u8f91\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e25\u683c\u6709\u754c\u76842D\u5bf9\u6297\u6270\u52a8\u63d0\u5347\u52303D\u9ad8\u65af\u8868\u793a\u4e2d\uff0c\u9632\u6b62\u4efb\u610f\u89c6\u89d2\u548c\u7ef4\u5ea6\u7684\u6307\u4ee4\u9a71\u52a8\u7f16\u8f91\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u6269\u6563\u7684\u6307\u4ee4\u9a71\u52a82D\u56fe\u50cf\u7f16\u8f91\u6269\u5c55\u52303DGS\uff0c\u867d\u7136\u4fc3\u8fdb\u4e863D\u5185\u5bb9\u521b\u4f5c\uff0c\u4f46\u4e5f\u4f7f3DGS\u8d44\u4ea7\u9762\u4e34\u672a\u7ecf\u6388\u6743\u7f16\u8f91\u548c\u6076\u610f\u7be1\u6539\u7684\u98ce\u9669\u3002\u73b0\u6709\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u611f\u77e5\u5bf9\u6297\u6270\u52a8\u65b9\u6cd5\u5728\u4fdd\u62a42D\u56fe\u50cf\u4e0a\u6709\u6548\uff0c\u4f46\u5e94\u7528\u4e8e3DGS\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u89c6\u89d2\u901a\u7528\u6027\u4fdd\u62a4\uff0c\u4ee5\u53ca\u5e73\u8861\u4e0d\u53ef\u89c1\u6027\u4e0e\u4fdd\u62a4\u80fd\u529b\u3002", "method": "\u63d0\u51faAdLift\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u5347\u4e25\u683c\u6709\u754c\u76842D\u5bf9\u6297\u6270\u52a8\u52303D\u9ad8\u65af\u8868\u793a\u7684\u4fdd\u62a4\u5c42\u3002\u4f7f\u7528\u5b9a\u5236\u7684Lifted PGD\u8fdb\u884c\u6e10\u8fdb\u4f18\u5316\uff1a\u5728\u8bad\u7ec3\u89c6\u89d2\u4e0a\u8fdb\u884c\u68af\u5ea6\u622a\u65ad\uff0c\u4ece\u7f16\u8f91\u6a21\u578b\u53cd\u5411\u4f20\u64ad\u5230\u6e32\u67d3\u56fe\u50cf\uff0c\u5e94\u7528\u6295\u5f71\u68af\u5ea6\u4e25\u683c\u7ea6\u675f\u56fe\u50cf\u7ea7\u6270\u52a8\uff1b\u7136\u540e\u901a\u8fc7\u56fe\u50cf\u5230\u9ad8\u65af\u62df\u5408\u64cd\u4f5c\u5c06\u6270\u52a8\u53cd\u5411\u4f20\u64ad\u5230\u4fdd\u62a4\u9ad8\u65af\u53c2\u6570\u3002\u4ea4\u66ff\u8fdb\u884c\u68af\u5ea6\u622a\u65ad\u548c\u56fe\u50cf\u5230\u9ad8\u65af\u62df\u5408\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u89c6\u89d2\u7684\u4e00\u81f4\u5bf9\u6297\u4fdd\u62a4\u6027\u80fd\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAdLift\u80fd\u6709\u6548\u9632\u6b62\u6700\u5148\u8fdb\u7684\u6307\u4ee4\u9a71\u52a82D\u56fe\u50cf\u548c3DGS\u7f16\u8f91\uff0c\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u63d0\u4f9b\u4e00\u81f4\u7684\u5bf9\u6297\u4fdd\u62a4\u6027\u80fd\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u89c6\u89d2\u3002", "conclusion": "AdLift\u662f\u9996\u4e2a\u9488\u5bf93DGS\u7684\u7f16\u8f91\u4fdd\u62a4\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89d2\u901a\u7528\u6027\u4fdd\u62a4\u548c\u5e73\u8861\u4e0d\u53ef\u89c1\u6027\u4e0e\u4fdd\u62a4\u80fd\u529b\u7684\u6311\u6218\uff0c\u4e3a3DGS\u8d44\u4ea7\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2512.07415", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07415", "abs": "https://arxiv.org/abs/2512.07415", "authors": ["Gabriele Galatolo", "Mirco Nanni"], "title": "Data-driven Exploration of Mobility Interaction Patterns", "comment": null, "summary": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u6316\u6398\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u4e2a\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u79fb\u52a8\u4e8b\u4ef6\u6a21\u5f0f\uff0c\u7528\u4e8e\u6539\u8fdb\u4eba\u7fa4\u6a21\u62df\u6a21\u578b", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u57fa\u4e8e\u9884\u8bbe\u7684\u884c\u4e3a\u6a21\u578b\uff0c\u4f46\u9700\u8981\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u7406\u89e3\u4e2a\u4f53\u79fb\u52a8\u884c\u4e3a\u53ca\u5176\u76f8\u4e92\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u4eba\u7fa4\u6a21\u62df\u548c\u5e94\u6025\u7ba1\u7406\u7b49\u5e94\u7528\u4e2d", "method": "\u91c7\u7528\u6570\u636e\u6316\u6398\u89c6\u89d2\uff0c\u4ece\u6570\u636e\u4e2d\u641c\u7d22\u53ef\u80fd\u53cd\u6620\u4e2a\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u79fb\u52a8\u4e8b\u4ef6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5bfb\u627e\u590d\u6742\u3001\u6301\u4e45\u7684\u4e8b\u4ef6\u6a21\u5f0f\u548c\u968f\u65f6\u95f4\u6f14\u5316\u7684\u914d\u7f6e", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\uff08\u6c7d\u8f66\u548c\u884c\u4eba\uff09\u4e0a\u5b9e\u4f8b\u5316\u4e86\u8be5\u65b9\u6cd5\uff0c\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5305\u62ec\u6027\u80fd\u3001\u53c2\u6570\u654f\u611f\u6027\u548c\u7ed3\u679c\u89e3\u91ca", "conclusion": "\u5bf9\u8fd9\u4e9b\u6a21\u5f0f\u7684\u7814\u7a76\u53ef\u4ee5\u63d0\u4f9b\u5173\u4e8e\u4e2a\u4f53\u95f4\u79fb\u52a8\u76f8\u4e92\u4f5c\u7528\u673a\u5236\u7684\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u73b0\u6709\u7684\u6a21\u62df\u6a21\u578b"}}
{"id": "2512.07568", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.07568", "abs": "https://arxiv.org/abs/2512.07568", "authors": ["Xuecheng Li", "Weikuan Jia", "Alisher Kurbonaliev", "Qurbonaliev Alisher", "Khudzhamkulov Rustam", "Ismoilov Shuhratjon", "Eshmatov Javhariddin", "Yuanjie Zheng"], "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation", "comment": null, "summary": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\u00efve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "AI": {"tldr": "\u63d0\u51faDSRSD-Net\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u5206\u89e3\u548c\u8bed\u4e49\u89e3\u76f8\u5173\u7ea6\u675f\u6765\u89e3\u8026\u6a21\u6001\u7279\u5b9a\u548c\u6a21\u6001\u5171\u4eab\u4fe1\u606f\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u4e3b\u5bfc\u3001\u5197\u4f59\u8026\u5408\u548c\u865a\u5047\u76f8\u5173\u7b49\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u8868\u793a\u5b58\u5728\u6a21\u6001\u4e3b\u5bfc\u3001\u5197\u4f59\u4fe1\u606f\u8026\u5408\u548c\u865a\u5047\u8de8\u6a21\u6001\u76f8\u5173\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u6b21\u4f18\u6cdb\u5316\u548c\u6709\u9650\u53ef\u89e3\u91ca\u6027\u3002\u9ad8\u65b9\u5dee\u6a21\u6001\u4f1a\u63a9\u76d6\u8f83\u5f31\u4f46\u8bed\u4e49\u91cd\u8981\u7684\u4fe1\u53f7\uff0c\u800c\u7b80\u5355\u7684\u878d\u5408\u7b56\u7565\u4f1a\u4ee5\u4e0d\u53d7\u63a7\u5236\u7684\u65b9\u5f0f\u7ea0\u7f20\u6a21\u6001\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u53cc\u6d41\u6b8b\u5dee\u8bed\u4e49\u89e3\u76f8\u5173\u7f51\u7edc(DSRSD-Net)\uff1a1) \u53cc\u6d41\u8868\u793a\u5b66\u4e60\u6a21\u5757\u901a\u8fc7\u6b8b\u5dee\u6295\u5f71\u5206\u79bb\u6a21\u6001\u5185(\u79c1\u6709)\u548c\u6a21\u6001\u95f4(\u5171\u4eab)\u6f5c\u5728\u56e0\u5b50\uff1b2) \u6b8b\u5dee\u8bed\u4e49\u5bf9\u9f50\u5934\u4f7f\u7528\u5bf9\u6bd4\u548c\u56de\u5f52\u5f0f\u76ee\u6807\u5c06\u4e0d\u540c\u6a21\u6001\u7684\u5171\u4eab\u56e0\u5b50\u6620\u5c04\u5230\u5171\u540c\u7a7a\u95f4\uff1b3) \u89e3\u76f8\u5173\u548c\u6b63\u4ea4\u6027\u635f\u5931\u6b63\u5219\u5316\u5171\u4eab\u7a7a\u95f4\u7684\u534f\u65b9\u5dee\u7ed3\u6784\uff0c\u540c\u65f6\u5f3a\u5236\u5171\u4eab\u6d41\u548c\u79c1\u6709\u6d41\u4e4b\u95f4\u7684\u6b63\u4ea4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u6559\u80b2\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDSRSD-Net\u5728\u4e0b\u4e00\u6b65\u9884\u6d4b\u548c\u6700\u7ec8\u7ed3\u679c\u9884\u6d4b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u5355\u6a21\u6001\u3001\u65e9\u671f\u878d\u5408\u3001\u665a\u671f\u878d\u5408\u548c\u534f\u540c\u6ce8\u610f\u529b\u57fa\u7ebf\u3002", "conclusion": "DSRSD-Net\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u7279\u5b9a\u548c\u6a21\u6001\u5171\u4eab\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5bf9\u566a\u58f0\u6216\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07580", "abs": "https://arxiv.org/abs/2512.07580", "authors": ["Yahong Wang", "Juncheng Wu", "Zhangkai Ni", "Longzhen Yang", "Yihang Liu", "Chengmei Yang", "Ying Wen", "Xianfeng Tang", "Hui Liu", "Yuyin Zhou", "Lianghua He"], "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "comment": null, "summary": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6df1\u5c42\u89c6\u89c9token\u4fe1\u606f\u9010\u6e10\u6d88\u5931\uff0c\u63d0\u51fa\"\u4fe1\u606f\u5730\u5e73\u7ebf\"\u6982\u5ff5\uff0c\u53d1\u73b0\u5728\u6df1\u5c42\u968f\u673a\u526a\u679d\u6548\u679c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u7ed3\u5408\u968f\u673a\u526a\u679d\u80fd\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u89c6\u89c9token\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u8bad\u7ec3\u65e0\u5173\u7684token\u526a\u679d\u65b9\u6cd5\u5728\u6df1\u5c42\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u968f\u673a\u526a\u679d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u89c6\u89c9token\u4fe1\u606f\u968f\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u800c\u9010\u6e10\u6d88\u5931\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u79fb\u9664token\u540e\u6a21\u578b\u8f93\u51fa\u6982\u7387\u53d8\u5316\u6765\u91cf\u5316token\u4fe1\u606f\u542b\u91cf\u3002\u5206\u6790\u53d1\u73b0\"\u4fe1\u606f\u5730\u5e73\u7ebf\"\u73b0\u8c61\uff0c\u5373\u89c6\u89c9token\u4fe1\u606f\u5728\u4e2d\u95f4\u5c42\u540e\u53d8\u5f97\u5747\u5300\u5e76\u6700\u7ec8\u6d88\u5931\u3002\u57fa\u4e8e\u6b64\uff0c\u5728\u6df1\u5c42\u91c7\u7528\u968f\u673a\u526a\u679d\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "DivPrune\u7ed3\u5408\u968f\u673a\u526a\u679d\u8fbe\u5230SOTA\u6548\u679c\uff0c\u5728\u526a\u679d50%\u89c6\u89c9token\u65f6\u4ecd\u80fd\u4fdd\u6301Qwen-2.5-VL-7B\u6a21\u578b96.9%\u7684\u6027\u80fd\u3002\u968f\u673a\u526a\u679d\u5728\u6df1\u5c42\u80fd\u6709\u6548\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\u3002", "conclusion": "\u89c6\u89c9token\u4fe1\u606f\u968f\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u800c\u9010\u6e10\u6d88\u5931\uff0c\u5f62\u6210\"\u4fe1\u606f\u5730\u5e73\u7ebf\"\u3002\u5728\u6df1\u5c42\u91c7\u7528\u968f\u673a\u526a\u679d\u662f\u6709\u6548\u7b56\u7565\uff0c\u7ed3\u5408\u73b0\u6709\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u526a\u679d\u6548\u679c\uff0c\u4e3aVLLM\u52a0\u901f\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2512.07661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07661", "abs": "https://arxiv.org/abs/2512.07661", "authors": ["Shiaho Li", "Naisheng Ye", "Tianyu Li", "Kashyap Chitta", "Tuo An", "Peng Su", "Boyang Wang", "Haiou Liu", "Chen Lv", "Hongyang Li"], "title": "Optimization-Guided Diffusion for Interactive Scene Generation", "comment": null, "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.", "AI": {"tldr": "OMEGA\u662f\u4e00\u4e2a\u4f18\u5316\u5f15\u5bfc\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u5728\u6269\u6563\u91c7\u6837\u4e2d\u589e\u5f3a\u573a\u666f\u751f\u6210\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u4ea4\u4e92\u611f\u77e5\uff0c\u80fd\u751f\u6210\u66f4\u771f\u5b9e\u3001\u53ef\u63a7\u7684\u5b89\u5168\u5173\u952e\u9a7e\u9a76\u573a\u666f\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8bc4\u4f30\u9700\u8981\u771f\u5b9e\u591a\u6837\u7684\u591a\u667a\u80fd\u4f53\u9a7e\u9a76\u573a\u666f\uff0c\u4f46\u73b0\u6709\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7a00\u5c11\u4e14\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u573a\u666f\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u53ef\u63a7\u6027\u6216\u4ea7\u751f\u8fdd\u53cd\u7269\u7406/\u793e\u4f1a\u7ea6\u675f\u7684\u6837\u672c\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faOMEGA\u6846\u67b6\uff1a\u5728\u6269\u6563\u6a21\u578b\u7684\u53cd\u5411\u6269\u6563\u6b65\u9aa4\u4e2d\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u91cd\u65b0\u951a\u5b9a\uff0c\u5f15\u5bfc\u751f\u6210\u7269\u7406\u5408\u7406\u548c\u884c\u4e3a\u4e00\u81f4\u7684\u8f68\u8ff9\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5c06\u81ea\u6211\u8f66\u8f86-\u653b\u51fb\u8005\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5206\u5e03\u7a7a\u95f4\u7684\u535a\u5f08\u8bba\u4f18\u5316\uff0c\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u4ee5\u751f\u6210\u771f\u5b9e\u7684\u5b89\u5168\u5173\u952e\u5bf9\u6297\u573a\u666f\u3002", "result": "\u5728nuPlan\u548cWaymo\u6570\u636e\u96c6\u4e0a\uff0cOMEGA\u5c06\u7269\u7406\u548c\u884c\u4e3a\u6709\u6548\u573a\u666f\u6bd4\u4f8b\u4ece32.35%\u63d0\u5347\u523072.27%\uff08\u81ea\u7531\u63a2\u7d22\uff09\uff0c\u4ece11%\u63d0\u5347\u523080%\uff08\u53ef\u63a7\u751f\u6210\uff09\u3002\u80fd\u751f\u62105\u500d\u591a\u7684\u78b0\u649e\u524d\u5e27\uff08\u78b0\u649e\u65f6\u95f4\u5c0f\u4e8e3\u79d2\uff09\u540c\u65f6\u4fdd\u6301\u573a\u666f\u771f\u5b9e\u6027\u3002", "conclusion": "OMEGA\u901a\u8fc7\u4f18\u5316\u5f15\u5bfc\u7684\u6269\u6563\u91c7\u6837\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u771f\u5b9e\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2512.07720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07720", "abs": "https://arxiv.org/abs/2512.07720", "authors": ["Fan Yang", "Heyuan Li", "Peihao Li", "Weihao Yuan", "Lingteng Qiu", "Chaoyue Song", "Cheng Chen", "Yisheng He", "Shifeng Zhang", "Xiaoguang Han", "Steven Hoi", "Guosheng Lin"], "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation", "comment": null, "summary": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u54083D\u91cd\u5efa\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u4e0a\u8eab3D\u865a\u62df\u5f62\u8c61\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7eb9\u7406\u6a21\u7cca\u3001\u8fd0\u52a8\u50f5\u786c\u548c\u7ed3\u6784\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u865a\u62df\u5f62\u8c61\u751f\u6210\u5b58\u5728\u4e24\u96be\uff1a\u57fa\u4e8e\u91cd\u5efa\u7684\u65b9\u6cd5\u80fd\u4ea7\u751f\u7a33\u5b9a\u7ed3\u6784\u4f46\u7eb9\u7406\u6a21\u7cca\u3001\u8fd0\u52a8\u50f5\u786c\uff1b\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u7684\u65b9\u6cd5\u80fd\u4ea7\u751f\u903c\u771f\u52a8\u6001\u4f46\u7ed3\u6784\u4e0d\u7a33\u5b9a\u3001\u8eab\u4efd\u6f02\u79fb\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u4f7f\u75283D\u91cd\u5efa\u6a21\u578b\u63d0\u4f9b\u7ed3\u6784\u548c\u5916\u89c2\u5148\u9a8c\uff0c\u5f15\u5bfc\u5b9e\u65f6\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6e32\u67d3\uff0c\u878d\u5408\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u751f\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4f2a\u5f71\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u5177\u6709\u903c\u771f\u5916\u89c2\u548c\u8fde\u8d2f\u52a8\u6001\u7684\u9ad8\u4fdd\u771f\u6570\u5b57\u865a\u62df\u5f62\u8c61\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u54083D\u91cd\u5efa\u7684\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u89c6\u9891\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u65f6\u7684\u9ad8\u8d28\u91cf\u4e0a\u8eab3D\u865a\u62df\u5f62\u8c61\u751f\u6210\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6e38\u620f\u548c\u865a\u62df\u73b0\u5b9e\u7b49\u5e94\u7528\u3002"}}
{"id": "2512.07745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07745", "abs": "https://arxiv.org/abs/2512.07745", "authors": ["Jialv Zou", "Shaoyu Chen", "Bencheng Liao", "Zhiyu Zheng", "Yuehao Song", "Lefei Zhang", "Qian Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving", "comment": null, "summary": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2", "AI": {"tldr": "DiffusionDriveV2\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6269\u6563\u6a21\u578b\u7684\u6a21\u6001\u574d\u7f29\u95ee\u9898\uff0c\u901a\u8fc7\u5c3a\u5ea6\u81ea\u9002\u5e94\u566a\u58f0\u548c\u5206\u5c42GRPO\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u591a\u6837\u6027\u7684\u540c\u65f6\u63d0\u5347\u8f68\u8ff9\u8d28\u91cf\uff0c\u5728NAVSIM\u6570\u636e\u96c6\u4e0a\u521b\u4e0b\u65b0\u8bb0\u5f55\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5bb9\u6613\u53d1\u751f\u6a21\u6001\u574d\u7f29\uff0c\u503e\u5411\u4e8e\u751f\u6210\u4fdd\u5b88\u548c\u540c\u8d28\u5316\u7684\u884c\u4e3a\u3002\u867d\u7136DiffusionDrive\u4f7f\u7528\u9884\u5b9a\u4e49\u951a\u70b9\u6765\u5212\u5206\u52a8\u4f5c\u7a7a\u95f4\u5e76\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\uff0c\u4f46\u5176\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u65b9\u6cd5\u7f3a\u4e4f\u8db3\u591f\u7ea6\u675f\uff0c\u5bfc\u81f4\u591a\u6837\u6027\u4e0e\u4e00\u81f4\u9ad8\u8d28\u91cf\u4e4b\u95f4\u7684\u56f0\u5883\u3002", "method": "\u63d0\u51faDiffusionDriveV2\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u7ea6\u675f\u4f4e\u8d28\u91cf\u6a21\u6001\u5e76\u63a2\u7d22\u66f4\u4f18\u8f68\u8ff9\u3002\u91c7\u7528\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e58\u6cd5\u566a\u58f0\u4fc3\u8fdb\u5e7f\u6cdb\u63a2\u7d22\uff1b\u4f7f\u7528\u951a\u5185GRPO\u7ba1\u7406\u5355\u4e2a\u951a\u70b9\u5185\u6837\u672c\u7684\u4f18\u52bf\u4f30\u8ba1\uff0c\u4ee5\u53ca\u951a\u95f4\u622a\u65adGRPO\u6574\u5408\u4e0d\u540c\u951a\u70b9\u7684\u5168\u5c40\u89c6\u89d2\uff0c\u9632\u6b62\u4e0d\u540c\u610f\u56fe\u95f4\u7684\u4e0d\u5f53\u4f18\u52bf\u6bd4\u8f83\u3002", "result": "\u5728NAVSIM v1\u6570\u636e\u96c6\u4e0a\u83b7\u5f9791.2 PDMS\uff0c\u5728NAVSIM v2\u6570\u636e\u96c6\u4e0a\u83b7\u5f9785.5 EPDMS\uff08\u4f7f\u7528\u5bf9\u9f50\u7684ResNet-34\u9aa8\u5e72\u7f51\u7edc\uff09\uff0c\u521b\u4e0b\u65b0\u8bb0\u5f55\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u622a\u65ad\u6269\u6563\u6a21\u578b\u4e2d\u591a\u6837\u6027\u4e0e\u4e00\u81f4\u9ad8\u8d28\u91cf\u4e4b\u95f4\u7684\u56f0\u5883\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u6743\u8861\u3002", "conclusion": "DiffusionDriveV2\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7ea6\u675f\u548c\u63a2\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6269\u6563\u6a21\u578b\u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u56fa\u6709\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u89e3\u51b3\u4e86\u591a\u6837\u6027\u4e0e\u9ad8\u8d28\u91cf\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002"}}
