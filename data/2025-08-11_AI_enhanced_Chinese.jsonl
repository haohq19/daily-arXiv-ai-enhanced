{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u4f18\u5316\uff08AEPO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\uff08AER\uff09\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4e2d\u8bed\u4e49\u5bf9\u9f50\u7684\u63a2\u7d22\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728GUI\u64cd\u4f5c\u4e2d\u9762\u4e34\u8bed\u4e49\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982RLVR\uff09\u5728\u7a7a\u95f4\u5bf9\u9f50\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u8bed\u4e49\u5bf9\u9f50\u4e0a\u56e0\u63a2\u7d22\u6548\u7387\u4f4e\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51faAEPO\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u548c\u57fa\u4e8e\u6548\u7387\u7406\u8bba\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\uff08AER\uff09\u51fd\u6570\uff0c\u4ee5\u63d0\u5347\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\u3002", "result": "AEPO\u8bad\u7ec3\u7684\u6a21\u578b\uff08InfiGUI-G1-3B\u548cInfiGUI-G1-7B\uff09\u5728\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u65b0SOTA\uff0c\u76f8\u5bf9RLVR\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe9.0%\u3002", "conclusion": "AEPO\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u5bf9\u9f50\u7684\u63a2\u7d22\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728GUI\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.05937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05937", "abs": "https://arxiv.org/abs/2508.05937", "authors": ["Gen Sako", "Takuya Kiyokawa", "Kensuke Harada", "Tomoki Ishikura", "Naoya Miyaji", "Genichiro Matsuda"], "title": "Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts", "comment": "6 pages, 9 figures", "summary": "Robotic non-destructive disassembly of mating parts remains challenging due\nto the need for flexible manipulation and the limited visibility of internal\nstructures. This study presents an affordance-guided teleoperation system that\nenables intuitive human demonstrations for dual-arm fix-and-disassemble tasks\nfor mating parts. The system visualizes feasible grasp poses and disassembly\ndirections in a virtual environment, both derived from the object's geometry,\nto address occlusions and structural complexity. To prevent excessive position\ntracking under load when following the affordance, we integrate a hybrid\ncontroller that combines position and impedance control into the teleoperated\ndisassembly arm. Real-world experiments validate the effectiveness of the\nproposed system, showing improved task success rates and reduced object pose\ndeviation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u89c9\u4eba\u673a\u534f\u4f5c\u7684\u53cc\u81c2\u62c6\u5378\u7cfb\u7edf\uff0c\u901a\u8fc7\u865a\u62df\u73af\u5883\u548c\u6df7\u5408\u63a7\u5236\u5668\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u975e\u7834\u574f\u6027\u62c6\u5378\u4e2d\u56e0\u5185\u90e8\u7ed3\u6784\u4e0d\u53ef\u89c1\u548c\u64cd\u4f5c\u7075\u6d3b\u6027\u4e0d\u8db3\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u51e0\u4f55\u7684\u865a\u62df\u73af\u5883\u53ef\u89c6\u5316\u6293\u53d6\u4f4d\u59ff\u548c\u62c6\u5378\u65b9\u5411\uff0c\u5e76\u7ed3\u5408\u4f4d\u7f6e\u4e0e\u963b\u6297\u63a7\u5236\u7684\u6df7\u5408\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e14\u7269\u4f53\u4f4d\u59ff\u504f\u5dee\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u590d\u6742\u62c6\u5378\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05876", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05876", "abs": "https://arxiv.org/abs/2508.05876", "authors": ["Francesca Ferrara", "Lander W. Schillinger Arana", "Florian D\u00f6rfler", "Sarah H. Q. Li"], "title": "A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance", "comment": "16 pages, 13 figures, submitted to the 2025 Astrodynamics Specialist\n  Conference", "summary": "This work presents a Markov decision process (MDP) framework to model\ndecision-making for collision avoidance maneuver (CAM) and a reinforcement\nlearning policy gradient (RL-PG) algorithm to train an autonomous guidance\npolicy using historic CAM data. In addition to maintaining acceptable collision\nrisks, this approach seeks to minimize the average fuel consumption of CAMs by\nmaking early maneuver decisions. We model CAM as a continuous state, discrete\naction and finite horizon MDP, where the critical decision is determining when\nto initiate the maneuver. The MDP model also incorporates analytical models for\nconjunction risk, propellant consumption, and transit orbit geometry. The\nMarkov policy effectively trades-off maneuver delay-which improves the\nreliability of conjunction risk indicators-with propellant consumption-which\nincreases with decreasing maneuver time. Using historical data of tracked\nconjunction events, we verify this framework and conduct an extensive ablation\nstudy on the hyper-parameters used within the MDP. On synthetic conjunction\nevents, the trained policy significantly minimizes both the overall and average\npropellant consumption per CAM when compared to a conventional cut-off policy\nthat initiates maneuvers 24 hours before the time of closest approach (TCA). On\nhistorical conjunction events, the trained policy consumes more propellant\noverall but reduces the average propellant consumption per CAM. For both\nhistorical and synthetic conjunction events, the trained policy achieves equal\nif not higher overall collision risk guarantees.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u68af\u5ea6\uff08RL-PG\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u78b0\u649e\u907f\u514d\u673a\u52a8\uff08CAM\uff09\u51b3\u7b56\uff0c\u4ee5\u6700\u5c0f\u5316\u71c3\u6599\u6d88\u8017\u5e76\u4fdd\u6301\u78b0\u649e\u98ce\u9669\u3002", "motivation": "\u4f20\u7edfCAM\u7b56\u7565\u901a\u5e38\u5728\u63a5\u8fd1\u78b0\u649e\u65f6\u95f4\uff08TCA\uff09\u524d24\u5c0f\u65f6\u542f\u52a8\u673a\u52a8\uff0c\u53ef\u80fd\u5bfc\u81f4\u71c3\u6599\u6d6a\u8d39\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65e9\u671f\u51b3\u7b56\u4f18\u5316\u71c3\u6599\u6d88\u8017\u548c\u78b0\u649e\u98ce\u9669\u3002", "method": "\u5c06CAM\u5efa\u6a21\u4e3a\u8fde\u7eed\u72b6\u6001\u3001\u79bb\u6563\u52a8\u4f5c\u548c\u6709\u9650\u65f6\u95f4\u8303\u56f4\u7684MDP\uff0c\u7ed3\u5408\u98ce\u9669\u3001\u71c3\u6599\u6d88\u8017\u548c\u8f68\u9053\u51e0\u4f55\u7684\u89e3\u6790\u6a21\u578b\uff0c\u4f7f\u7528RL-PG\u7b97\u6cd5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u5386\u53f2\u78b0\u649e\u4e8b\u4ef6\u4e2d\uff0c\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u71c3\u6599\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5e38\u89c4\u7b56\u7565\u76f8\u540c\u6216\u66f4\u9ad8\u7684\u78b0\u649e\u98ce\u9669\u4fdd\u969c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u673a\u52a8\u5ef6\u8fdf\u548c\u71c3\u6599\u6d88\u8017\uff0c\u4e3aCAM\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06096", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06096", "abs": "https://arxiv.org/abs/2508.06096", "authors": ["Eric Jing", "Abdeslam Boularias"], "title": "Bounding Distributional Shifts in World Modeling through Novelty Detection", "comment": "7 pages, 6 figures", "summary": "Recent work on visual world models shows significant promise in latent state\ndynamics obtained from pre-trained image backbones. However, most of the\ncurrent approaches are sensitive to training quality, requiring near-complete\ncoverage of the action and state space during training to prevent divergence\nduring inference. To make a model-based planning algorithm more robust to the\nquality of the learned world model, we propose in this work to use a\nvariational autoencoder as a novelty detector to ensure that proposed action\ntrajectories during planning do not cause the learned model to deviate from the\ntraining data distribution. To evaluate the effectiveness of this approach, a\nseries of experiments in challenging simulated robot environments was carried\nout, with the proposed method incorporated into a model-predictive control\npolicy loop extending the DINO-WM architecture. The results clearly show that\nthe proposed method improves over state-of-the-art solutions in terms of data\nefficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u65b0\u9896\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u89c4\u5212\u7b97\u6cd5\u5bf9\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u8d28\u91cf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u5bf9\u8bad\u7ec3\u8d28\u91cf\u654f\u611f\uff0c\u9700\u8981\u8fd1\u4e4e\u5b8c\u6574\u7684\u52a8\u4f5c\u548c\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u4ee5\u9632\u6b62\u63a8\u7406\u65f6\u53d1\u6563\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u65b0\u9896\u6027\u68c0\u6d4b\u5668\uff0c\u786e\u4fdd\u89c4\u5212\u4e2d\u7684\u52a8\u4f5c\u8f68\u8ff9\u4e0d\u504f\u79bb\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u6a21\u62df\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u89c4\u5212\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2508.06016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models.", "AI": {"tldr": "\u5728Transformer\u6a21\u578b\u4e2d\uff0c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u662f\u4e3b\u8981\u6311\u6218\u3002\u672c\u6587\u53d1\u73b0\uff0c\u901a\u8fc7\u5728\u540e\u5904\u7406\u4e2d\u5f15\u5165\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u6a21\u578b\u7cbe\u5ea6\u53cd\u800c\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u6ce8\u610f\u529b\u7a00\u758f\u6027\u662f\u5426\u80fd\u5728\u4e0d\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63a2\u7d22\u5176\u6f5c\u5728\u7684\u6b63\u5219\u5316\u6548\u679c\u3002", "method": "\u5728DistilBERT\u6a21\u578b\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u5f15\u5165\u7ed3\u6784\u5316\u540e\u5904\u7406\u7a00\u758f\u6027\uff0c\u5e94\u7528\u4e8eSST-2\u60c5\u611f\u5206\u6790\u4efb\u52a1\u3002", "result": "80%\u7a00\u758f\u6027\u7684\u6a21\u578b\u9a8c\u8bc1\u7cbe\u5ea6\u8fbe\u523091.59%\uff0c\u6bd4\u5bc6\u96c6\u57fa\u7ebf\u63d0\u53470.97%\u3002", "conclusion": "\u6ce8\u610f\u529b\u7a00\u758f\u6027\u4e0d\u4ec5\u662f\u8ba1\u7b97\u6548\u7387\u5de5\u5177\uff0c\u8fd8\u80fd\u901a\u8fc7\u6b63\u5219\u5316\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2508.05960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05960", "abs": "https://arxiv.org/abs/2508.05960", "authors": ["Haohui Chen", "Zhiyong Chen"], "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without further environment interaction. A key challenge is the\ndistribution shift between the learned and behavior policies, leading to\nout-of-distribution (OOD) actions and overestimation. To prevent gross\noverestimation, the value function must remain conservative; however, excessive\nconservatism may hinder performance improvement. To address this, we propose\nthe mildly conservative regularized evaluation (MCRE) framework, which balances\nconservatism and performance by combining temporal difference (TD) error with a\nbehavior cloning term in the Bellman backup. Building on this, we develop the\nmildly conservative regularized Q-learning (MCRQ) algorithm, which integrates\nMCRE into an off-policy actor-critic framework. Experiments show that MCRQ\noutperforms strong baselines and state-of-the-art offline RL algorithms on\nbenchmark datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MCRE\u6846\u67b6\u548cMCRQ\u7b97\u6cd5\uff0c\u7528\u4e8e\u5e73\u8861\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fdd\u5b88\u6027\u548c\u6027\u80fd\u63d0\u5347\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5b66\u4e60\u7b56\u7565\u4e0e\u884c\u4e3a\u7b56\u7565\u7684\u5206\u5e03\u504f\u79fb\u5bfc\u81f4OOD\u52a8\u4f5c\u548c\u8fc7\u9ad8\u4f30\u8ba1\uff0c\u9700\u8981\u5e73\u8861\u4fdd\u5b88\u6027\u4e0e\u6027\u80fd\u3002", "method": "\u63d0\u51faMCRE\u6846\u67b6\uff0c\u7ed3\u5408TD\u8bef\u5dee\u548c\u884c\u4e3a\u514b\u9686\u9879\uff0c\u5e76\u5f00\u53d1MCRQ\u7b97\u6cd5\uff0c\u5c06\u5176\u878d\u5165\u79bb\u7ebfactor-critic\u6846\u67b6\u3002", "result": "MCRQ\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "conclusion": "MCRE\u548cMCRQ\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u7684\u4fdd\u5b88\u6027\u4e0e\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.05950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05950", "abs": "https://arxiv.org/abs/2508.05950", "authors": ["Yanxing Liang", "Yinghui Wang", "Jinlong Yang", "Wei Li"], "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image", "comment": null, "summary": "The lack of spatial dimensional information remains a challenge in normal\nestimation from a single image. Recent diffusion-based methods have\ndemonstrated significant potential in 2D-to-3D implicit mapping, they rely on\ndata-driven statistical priors and miss the explicit modeling of light-surface\ninteraction, leading to multi-view normal direction conflicts. Moreover, the\ndiscrete sampling mechanism of diffusion models causes gradient discontinuity\nin differentiable rendering reconstruction modules, preventing 3D geometric\nerrors from being backpropagated to the normal generation network, thereby\nforcing existing methods to depend on dense normal annotations. This paper\nproposes SINGAD, a novel Self-supervised framework from a single Image for\nNormal estimation via 3D GAussian splatting guided Diffusion. By integrating\nphysics-driven light-interaction modeling and a differentiable rendering-based\nreprojection strategy, our framework directly converts 3D geometric errors into\nnormal optimization signals, solving the challenges of multi-view geometric\ninconsistency and data dependency. Specifically, the framework constructs a\nlight-interaction-driven 3DGS reparameterization model to generate multi-scale\ngeometric features consistent with light transport principles, ensuring\nmulti-view normal consistency. A cross-domain feature fusion module is designed\nwithin a conditional diffusion model, embedding geometric priors to constrain\nnormal generation while maintaining accurate geometric error propagation.\nFurthermore, a differentiable 3D reprojection loss strategy is introduced for\nself-supervised optimization that minimizes geometric error between the\nreconstructed and input image, eliminating dependence on annotated normal\ndatasets. Quantitative evaluations on the Google Scanned Objects dataset\ndemonstrate that our method outperforms state-of-the-art approaches across\nmultiple metrics.", "AI": {"tldr": "SINGAD\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u5f15\u5bfc\u7684\u6269\u6563\u65b9\u6cd5\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u6cd5\u7ebf\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u548c\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u5148\u9a8c\uff0c\u7f3a\u4e4f\u5bf9\u5149-\u8868\u9762\u4ea4\u4e92\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u591a\u89c6\u89d2\u6cd5\u7ebf\u65b9\u5411\u51b2\u7a81\uff0c\u4e14\u6269\u6563\u6a21\u578b\u7684\u79bb\u6563\u91c7\u6837\u673a\u5236\u963b\u788d\u4e863D\u51e0\u4f55\u8bef\u5dee\u7684\u53cd\u5411\u4f20\u64ad\u3002", "method": "\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u7684\u5149\u4ea4\u4e92\u5efa\u6a21\u548c\u53ef\u5fae\u5206\u6e32\u67d3\u91cd\u6295\u5f71\u7b56\u7565\uff0c\u6784\u5efa\u5149\u4ea4\u4e92\u9a71\u52a8\u76843DGS\u91cd\u53c2\u6570\u5316\u6a21\u578b\uff0c\u8bbe\u8ba1\u8de8\u57df\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u53ef\u5fae\u52063D\u91cd\u6295\u5f71\u635f\u5931\u7b56\u7565\u3002", "result": "\u5728Google Scanned Objects\u6570\u636e\u96c6\u4e0a\uff0cSINGAD\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SINGAD\u901a\u8fc7\u81ea\u76d1\u7763\u4f18\u5316\u548c\u51e0\u4f55\u8bef\u5dee\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u6cd5\u7ebf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.05982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05982", "abs": "https://arxiv.org/abs/2508.05982", "authors": ["Qingyang Liu", "Bingjie Gao", "Weiheng Huang", "Jun Zhang", "Zhongqian Sun", "Yang Wei", "Zelin Peng", "Qianli Ma", "Shuai Yang", "Zhaohe Liao", "Haonan Zhao", "Li Niu"], "title": "AnimateScene: Camera-controllable Animation in Any Scene", "comment": null, "summary": "3D scene reconstruction and 4D human animation have seen rapid progress and\nbroad adoption in recent years. However, seamlessly integrating reconstructed\nscenes with 4D human animation to produce visually engaging results remains\nchallenging. One key difficulty lies in placing the human at the correct\nlocation and scale within the scene while avoiding unrealistic\ninterpenetration. Another challenge is that the human and the background may\nexhibit different lighting and style, leading to unrealistic composites. In\naddition, appealing character motion videos are often accompanied by camera\nmovements, which means that the viewpoints need to be reconstructed along a\nspecified trajectory. We present AnimateScene, which addresses the above issues\nin a unified framework. First, we design an accurate placement module that\nautomatically determines a plausible 3D position for the human and prevents any\ninterpenetration within the scene during motion. Second, we propose a\ntraining-free style alignment method that adapts the 4D human representation to\nmatch the background's lighting and style, achieving coherent visual\nintegration. Finally, we design a joint post-reconstruction method for both the\n4D human and the 3D scene that allows camera trajectories to be inserted,\nenabling the final rendered video to feature visually appealing camera\nmovements. Extensive experiments show that AnimateScene generates dynamic scene\nvideos with high geometric detail and spatiotemporal coherence across various\ncamera and action combinations.", "AI": {"tldr": "AnimateScene\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e863D\u573a\u666f\u91cd\u5efa\u4e0e4D\u4eba\u4f53\u52a8\u753b\u65e0\u7f1d\u96c6\u6210\u7684\u6311\u6218\uff0c\u5305\u62ec\u4f4d\u7f6e\u653e\u7f6e\u3001\u98ce\u683c\u5bf9\u9f50\u548c\u76f8\u673a\u8f68\u8ff9\u63d2\u5165\u3002", "motivation": "\u5c06\u91cd\u5efa\u76843D\u573a\u666f\u4e0e4D\u4eba\u4f53\u52a8\u753b\u65e0\u7f1d\u96c6\u6210\u5b58\u5728\u4f4d\u7f6e\u3001\u5149\u7167\u548c\u76f8\u673a\u8f68\u8ff9\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4f4d\u7f6e\u653e\u7f6e\u6a21\u5757\u3001\u65e0\u8bad\u7ec3\u98ce\u683c\u5bf9\u9f50\u65b9\u6cd5\u548c\u8054\u5408\u540e\u91cd\u5efa\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u51e0\u4f55\u7ec6\u8282\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAnimateScene\u80fd\u751f\u6210\u5177\u6709\u9ad8\u51e0\u4f55\u7ec6\u8282\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u7684\u52a8\u6001\u573a\u666f\u89c6\u9891\u3002", "conclusion": "AnimateScene\u6709\u6548\u89e3\u51b3\u4e863D\u573a\u666f\u4e0e4D\u52a8\u753b\u7684\u96c6\u6210\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u4e0a\u5438\u5f15\u4eba\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.06283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06283", "abs": "https://arxiv.org/abs/2508.06283", "authors": ["Saad Ejaz", "Marco Giberna", "Muhammad Shaheer", "Jose Andres Millan-Romera", "Ali Tourani", "Paul Kremer", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Situationally-aware Path Planning Exploiting 3D Scene Graphs", "comment": null, "summary": "3D Scene Graphs integrate both metric and semantic information, yet their\nstructure remains underutilized for improving path planning efficiency and\ninterpretability. In this work, we present S-Path, a situationally-aware path\nplanner that leverages the metric-semantic structure of indoor 3D Scene Graphs\nto significantly enhance planning efficiency. S-Path follows a two-stage\nprocess: it first performs a search over a semantic graph derived from the\nscene graph to yield a human-understandable high-level path. This also\nidentifies relevant regions for planning, which later allows the decomposition\nof the problem into smaller, independent subproblems that can be solved in\nparallel. We also introduce a replanning mechanism that, in the event of an\ninfeasible path, reuses information from previously solved subproblems to\nupdate semantic heuristics and prioritize reuse to further improve the\nefficiency of future planning attempts. Extensive experiments on both\nreal-world and simulated environments show that S-Path achieves average\nreductions of 5.7x in planning time while maintaining comparable path\noptimality to classical sampling-based planners and surpassing them in complex\nscenarios, making it an efficient and interpretable path planner for\nenvironments represented by indoor 3D Scene Graphs.", "AI": {"tldr": "S-Path\u5229\u75283D\u573a\u666f\u56fe\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89c4\u5212\u548c\u5b50\u95ee\u9898\u5206\u89e3\uff0c\u663e\u8457\u63d0\u5347\u8def\u5f84\u89c4\u5212\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8def\u5f84\u6700\u4f18\u6027\u3002", "motivation": "3D\u573a\u666f\u56fe\u7684\u8bed\u4e49\u7ed3\u6784\u672a\u88ab\u5145\u5206\u5229\u7528\uff0cS-Path\u65e8\u5728\u901a\u8fc7\u5229\u7528\u8fd9\u4e00\u7ed3\u6784\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u89c4\u5212\uff1a\u5148\u5728\u8bed\u4e49\u56fe\u4e0a\u641c\u7d22\u9ad8\u5c42\u8def\u5f84\uff0c\u518d\u5206\u89e3\u4e3a\u5e76\u884c\u5b50\u95ee\u9898\uff1b\u5f15\u5165\u91cd\u89c4\u5212\u673a\u5236\u4ee5\u590d\u7528\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cS-Path\u5e73\u5747\u51cf\u5c115.7\u500d\u89c4\u5212\u65f6\u95f4\uff0c\u8def\u5f84\u6700\u4f18\u6027\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u590d\u6742\u573a\u666f\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "S-Path\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e3D\u573a\u666f\u56fe\u8868\u793a\u7684\u73af\u5883\u3002"}}
{"id": "2508.06113", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06113", "abs": "https://arxiv.org/abs/2508.06113", "authors": ["Jian Wang", "Chaokang Jiang", "Haitao Xu"], "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving", "comment": "7 pages, 4 figures", "summary": "Diffusion-based models are redefining the state-of-the-art in end-to-end\nautonomous driving, yet their performance is increasingly hampered by a\nreliance on transformer-based fusion. These architectures face fundamental\nlimitations: quadratic computational complexity restricts the use of\nhigh-resolution features, and a lack of spatial priors prevents them from\neffectively modeling the inherent structure of Bird's Eye View (BEV)\nrepresentations. This paper introduces GMF-Drive (Gated Mamba Fusion for\nDriving), an end-to-end framework that overcomes these challenges through two\nprincipled innovations. First, we supersede the information-limited\nhistogram-based LiDAR representation with a geometrically-augmented pillar\nformat encoding shape descriptors and statistical features, preserving critical\n3D geometric details. Second, we propose a novel hierarchical gated mamba\nfusion (GM-Fusion) architecture that substitutes an expensive transformer with\na highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM\nleverages directional sequencing and adaptive fusion mechanisms to capture\nlong-range dependencies with linear complexity, while explicitly respecting the\nunique spatial properties of the driving scene. Extensive experiments on the\nchallenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new\nstate-of-the-art performance, significantly outperforming DiffusionDrive.\nComprehensive ablation studies validate the efficacy of each component,\ndemonstrating that task-specific SSMs can surpass a general-purpose transformer\nin both performance and efficiency for autonomous driving.", "AI": {"tldr": "GMF-Drive\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7Mamba\u878d\u5408\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u589e\u5f3a\u7684LiDAR\u8868\u793a\u548c\u9ad8\u6548\u7684\u7a7a\u95f4\u611f\u77e5\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u514b\u670d\u4e86\u4f20\u7edfTransformer\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56Transformer\u878d\u5408\uff0c\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u7f3a\u4e4f\u7a7a\u95f4\u5148\u9a8c\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u7684\u4f7f\u7528\u548c\u5bf9BEV\u8868\u793a\u7684\u6709\u6548\u5efa\u6a21\u3002", "method": "1. \u4f7f\u7528\u51e0\u4f55\u589e\u5f3a\u7684LiDAR\u8868\u793a\u66ff\u4ee3\u4f20\u7edf\u76f4\u65b9\u56fe\u8868\u793a\uff1b2. \u63d0\u51fa\u95e8\u63a7Mamba\u878d\u5408\u67b6\u6784\uff08GM-Fusion\uff09\uff0c\u7528\u9ad8\u6548\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u66ff\u4ee3Transformer\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGMF-Drive\u8868\u73b0\u4f18\u4e8eDiffusionDrive\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u901a\u7528Transformer\u3002"}}
{"id": "2508.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06058", "abs": "https://arxiv.org/abs/2508.06058", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention", "comment": null, "summary": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera\ncapture brightness changes as asynchronous \"events\" instead of frames, offering\nadvanced application on mobile photography. However, challenges arise from\ncombining a Quad Bayer Color Filter Array (CFA) sensor with event pixels\nlacking color information, resulting in aliasing and artifacts on the\ndemosaicing process before downstream application. Current methods struggle to\naddress these issues, especially on resource-limited mobile devices. In\nresponse, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage\nnetwork via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can\nhandle event pixels inpainting and demosaicing separately, leveraging the\nbenefits of dividing complex tasks into manageable subtasks. Furthermore, we\nintroduce a lightweight Cross-Swin State Block that uniquely utilizes\npositional prior for demosaicing and enhances global dependencies through the\nstate space model with linear complexity. In summary, TSANet demonstrates\nexcellent demosaicing performance on both simulated and real data of HybridEVS\nwhile maintaining a lightweight model, averaging better results than the\nprevious state-of-the-art method DemosaicFormer across seven diverse datasets\nin both PSNR and SSIM, while respectively reducing parameter and computation\ncosts by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities\nfor efficient image demosaicing on mobile devices. Code is available in the\nsupplementary materials.", "AI": {"tldr": "TSANet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u7f51\u7edc\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u589e\u5f3a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5904\u7406\u4e8b\u4ef6\u50cf\u7d20\u4fee\u590d\u548c\u53bb\u9a6c\u8d5b\u514b\uff0c\u663e\u8457\u63d0\u5347\u4e86HybridEVS\u76f8\u673a\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "HybridEVS\u76f8\u673a\u7ed3\u5408Quad Bayer CFA\u4f20\u611f\u5668\u548c\u4e8b\u4ef6\u50cf\u7d20\u65f6\uff0c\u7f3a\u4e4f\u989c\u8272\u4fe1\u606f\u5bfc\u81f4\u53bb\u9a6c\u8d5b\u514b\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4f2a\u5f71\u548c\u6df7\u53e0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TSANet\u91c7\u7528\u4e24\u9636\u6bb5\u7f51\u7edc\uff0c\u5206\u522b\u5904\u7406\u4e8b\u4ef6\u50cf\u7d20\u4fee\u590d\u548c\u53bb\u9a6c\u8d5b\u514b\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684Cross-Swin State Block\uff0c\u5229\u7528\u4f4d\u7f6e\u5148\u9a8c\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u589e\u5f3a\u5168\u5c40\u4f9d\u8d56\u3002", "result": "TSANet\u5728\u6a21\u62df\u548c\u771f\u5b9eHybridEVS\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u548cSSIM\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5DemosaicFormer\uff0c\u540c\u65f6\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u5206\u522b\u964d\u4f4e1.86\u500d\u548c3.29\u500d\u3002", "conclusion": "TSANet\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u56fe\u50cf\u53bb\u9a6c\u8d5b\u514b\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.06292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06292", "abs": "https://arxiv.org/abs/2508.06292", "authors": ["Sanja Karilanova", "Subhrakanti Dey", "Ay\u00e7a \u00d6z\u00e7elikkale"], "title": "Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback", "comment": "15 pages, 7 Tables, 6 Figures", "summary": "Neuromorphic computing is an emerging technology enabling low-latency and\nenergy-efficient signal processing. A key algorithmic tool in neuromorphic\ncomputing is spiking neural networks (SNNs). SNNs are biologically inspired\nneural networks which utilize stateful neurons, and provide low-bit data\nprocessing by encoding and decoding information using spikes. Similar to SNNs,\ndeep state-space models (SSMs) utilize stateful building blocks. However, deep\nSSMs, which recently achieved competitive performance in various temporal\nmodeling tasks, are typically designed with high-precision activation functions\nand no reset mechanisms. To bridge the gains offered by SNNs and the recent\ndeep SSM models, we propose a novel multiple-output spiking neuron model that\ncombines a linear, general SSM state transition with a non-linear feedback\nmechanism through reset. Compared to the existing neuron models for SNNs, our\nproposed model clearly conceptualizes the differences between the spiking\nfunction, the reset condition and the reset action. The experimental results on\nvarious tasks, i.e., a keyword spotting task, an event-based vision task and a\nsequential pattern recognition task, show that our proposed model achieves\nperformance comparable to existing benchmarks in the SNN literature. Our\nresults illustrate how the proposed reset mechanism can overcome instability\nand enable learning even when the linear part of neuron dynamics is unstable,\nallowing us to go beyond the strictly enforced stability of linear dynamics in\nrecent deep SSM models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u591a\u8f93\u51fa\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u7ed3\u5408\u7ebf\u6027\u72b6\u6001\u8f6c\u79fb\u548c\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7ed3\u5408SNN\u7684\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u80fd\u4e0e\u6df1\u5ea6SSM\u7684\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3\u5176\u7f3a\u4e4f\u91cd\u7f6e\u673a\u5236\u548c\u9ad8\u7cbe\u5ea6\u6fc0\u6d3b\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u591a\u8f93\u51fa\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u660e\u786e\u533a\u5206\u8109\u51b2\u529f\u80fd\u3001\u91cd\u7f6e\u6761\u4ef6\u548c\u91cd\u7f6e\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u73b0\u6709SNN\u57fa\u51c6\u76f8\u5f53\uff0c\u91cd\u7f6e\u673a\u5236\u514b\u670d\u4e86\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u91cd\u7f6e\u673a\u5236\u6269\u5c55\u4e86\u6df1\u5ea6SSM\u7684\u5e94\u7528\u8303\u56f4\uff0c\u7a81\u7834\u4e86\u7ebf\u6027\u52a8\u6001\u7684\u7a33\u5b9a\u6027\u9650\u5236\u3002"}}
{"id": "2508.06122", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2508.06122", "abs": "https://arxiv.org/abs/2508.06122", "authors": ["Ting-Shuo Yo", "Shih-Hao Su", "Chien-Ming Wu", "Wei-Ting Chen", "Jung-Lien Chu", "Chiao-Wei Chang", "Hung-Chi Kuo"], "title": "Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events", "comment": "37 pages, 6 figures, 3 tables", "summary": "This study applied representation learning algorithms to satellite images and\nevaluated the learned latent spaces with classifications of various weather\nevents. The algorithms investigated include the classical linear\ntransformation, i.e., principal component analysis (PCA), state-of-the-art deep\nlearning method, i.e., convolutional autoencoder (CAE), and a residual network\npre-trained with large image datasets (PT). The experiment results indicated\nthat the latent space learned by CAE consistently showed higher threat scores\nfor all classification tasks. The classifications with PCA yielded high hit\nrates but also high false-alarm rates. In addition, the PT performed\nexceptionally well at recognizing tropical cyclones but was inferior in other\ntasks. Further experiments suggested that representations learned from\nhigher-resolution datasets are superior in all classification tasks for\ndeep-learning algorithms, i.e., CAE and PT. We also found that smaller latent\nspace sizes had minor impact on the classification task's hit rate. Still, a\nlatent space dimension smaller than 128 caused a significantly higher false\nalarm rate. Though the CAE can learn latent spaces effectively and efficiently,\nthe interpretation of the learned representation lacks direct connections to\nphysical attributions. Therefore, developing a physics-informed version of CAE\ncan be a promising outlook for the current work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86PCA\u3001CAE\u548c\u9884\u8bad\u7ec3\u6b8b\u5dee\u7f51\u7edc\u5728\u536b\u661f\u56fe\u50cf\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0CAE\u6548\u679c\u6700\u4f73\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\u5728\u536b\u661f\u56fe\u50cf\u5929\u6c14\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u8bc4\u4f30\u5176\u6f5c\u5728\u7a7a\u95f4\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528PCA\u3001CAE\u548c\u9884\u8bad\u7ec3\u6b8b\u5dee\u7f51\u7edc\uff08PT\uff09\u5bf9\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u8bc4\u4f30\u5206\u7c7b\u4efb\u52a1\u7684\u8868\u73b0\u3002", "result": "CAE\u5728\u6240\u6709\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0cPCA\u547d\u4e2d\u7387\u9ad8\u4f46\u8bef\u62a5\u7387\u4e5f\u9ad8\uff0cPT\u5728\u70ed\u5e26\u6c14\u65cb\u8bc6\u522b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u5bf9\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u66f4\u6709\u5229\u3002", "conclusion": "CAE\u9ad8\u6548\u4f46\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u672a\u6765\u53ef\u5f00\u53d1\u7269\u7406\u4fe1\u606f\u589e\u5f3a\u7684CAE\u7248\u672c\u3002"}}
{"id": "2508.06170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06170", "abs": "https://arxiv.org/abs/2508.06170", "authors": ["Ojonugwa Oluwafemi Ejiga Peter", "Akingbola Oluwapemiisin", "Amalahu Chetachi", "Adeniran Opeyemi", "Fahmi Khalifa", "Md Mahmudur Rahman"], "title": "Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation", "comment": null, "summary": "Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,\nwhich is one of the main causes of cancer-related mortality globally; hence, it\nis deemed an essential technique for the prevention and early detection of\ncolorectal cancer. The research introduces a unique multidirectional\narchitectural framework to automate polyp detection within colonoscopy images\nwhile helping resolve limited healthcare dataset sizes and annotation\ncomplexities. The research implements a comprehensive system that delivers\nsynthetic data generation through Stable Diffusion enhancements together with\ndetection and segmentation algorithms. This detection approach combines Faster\nR-CNN for initial object localization while the Segment Anything Model (SAM)\nrefines the segmentation masks. The faster R-CNN detection algorithm achieved a\nrecall of 93.08% combined with a precision of 88.97% and an F1 score of\n90.98%.SAM is then used to generate the image mask. The research evaluated five\nstate-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,\nand MANet using ResNet34 as a base model. The results demonstrate the superior\nperformance of FPN with the highest scores of PSNR (7.205893) and SSIM\n(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced\nperformance in IoU (64.20%) and Dice score (77.53%).", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65b9\u5411\u67b6\u6784\u6846\u67b6\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u68c0\u6d4b\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7ed3\u80a0\u955c\u56fe\u50cf\u4e2d\u7684\u606f\u8089\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u7ed3\u80a0\u955c\u662f\u7ed3\u76f4\u80a0\u764c\u65e9\u671f\u8bca\u65ad\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u533b\u7597\u6570\u636e\u96c6\u6709\u9650\u4e14\u6807\u6ce8\u590d\u6742\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u5347\u81ea\u52a8\u5316\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u91c7\u7528Faster R-CNN\u8fdb\u884c\u521d\u59cb\u76ee\u6807\u5b9a\u4f4d\uff0c\u7ed3\u5408Segment Anything Model\uff08SAM\uff09\u4f18\u5316\u5206\u5272\u63a9\u7801\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5206\u5272\u6a21\u578b\uff08U-Net\u3001PSPNet\u3001FPN\u3001LinkNet\u3001MANet\uff09\u3002", "result": "Faster R-CNN\u7684\u53ec\u56de\u7387\u4e3a93.08%\uff0c\u7cbe\u786e\u7387\u4e3a88.97%\uff0cF1\u5206\u6570\u4e3a90.98%\u3002FPN\u5728PSNR\u548cSSIM\u4e0a\u8868\u73b0\u6700\u4f73\uff0cU-Net\u5728\u53ec\u56de\u7387\u4e0a\u9886\u5148\uff0cLinkNet\u5728IoU\u548cDice\u5206\u6570\u4e0a\u8868\u73b0\u5747\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u606f\u8089\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7ed3\u76f4\u80a0\u764c\u7684\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2508.06189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06189", "abs": "https://arxiv.org/abs/2508.06189", "authors": ["Cheng Liu", "Daou Zhang", "Tingxu Liu", "Yuhan Wang", "Jinyang Chen", "Yuexuan Li", "Xinying Xiao", "Chenbo Xin", "Ziru Wang", "Weichao Wu"], "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration", "comment": null, "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f02\u6b65\u534f\u4f5c\u7684\u72af\u7f6a\u884c\u4e3a\u9884\u6d4b\u6846\u67b6\uff08MA-CBP\uff09\uff0c\u901a\u8fc7\u5b9e\u65f6\u89c6\u9891\u6d41\u5206\u6790\u548c\u9ad8\u4f4e\u8bed\u4e49\u878d\u5408\uff0c\u5b9e\u73b0\u6f5c\u5728\u72af\u7f6a\u884c\u4e3a\u7684\u65e9\u671f\u9884\u8b66\u3002", "motivation": "\u57ce\u5e02\u5316\u52a0\u901f\u5bfc\u81f4\u516c\u5171\u573a\u666f\u72af\u7f6a\u884c\u4e3a\u589e\u591a\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u7ea7\u8bed\u4e49\u6216\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u5c06\u89c6\u9891\u6d41\u8f6c\u4e3a\u8bed\u4e49\u63cf\u8ff0\uff0c\u6784\u5efa\u56e0\u679c\u4e00\u81f4\u7684\u5386\u53f2\u6458\u8981\uff0c\u878d\u5408\u76f8\u90bb\u5e27\u8fdb\u884c\u957f\u77ed\u65f6\u4e0a\u4e0b\u6587\u8054\u5408\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u57ce\u5e02\u516c\u5171\u5b89\u5168\u63d0\u4f9b\u6709\u6548\u98ce\u9669\u9884\u8b66\u3002", "conclusion": "MA-CBP\u6846\u67b6\u5728\u72af\u7f6a\u884c\u4e3a\u9884\u6d4b\u548c\u516c\u5171\u5b89\u5168\u9884\u8b66\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.06202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06202", "abs": "https://arxiv.org/abs/2508.06202", "authors": ["Chang Che", "Ziqi Wang", "Pengwan Yang", "Qi Wang", "Hui Ma", "Zenglin Shi"], "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning", "comment": null, "summary": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.", "AI": {"tldr": "LiLoRA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u67b6\u6784\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eabLoRA\u77e9\u9635\u548c\u4f4e\u79e9\u5206\u89e3\u51cf\u5c11\u53c2\u6570\u5197\u4f59\uff0c\u540c\u65f6\u5f15\u5165\u4f59\u5f26\u6b63\u5219\u5316\u635f\u5931\u4fdd\u6301\u8868\u793a\u4e00\u81f4\u6027\u3002", "motivation": "\u6301\u7eed\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\uff08CVIT\uff09\u4e2d\uff0c\u707e\u96be\u6027\u9057\u5fd8\u548c\u53c2\u6570\u6548\u7387\u4f4e\u4e0b\u662f\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6269\u5c55\u6574\u4e2a\u5c42\u5bfc\u81f4\u53c2\u6570\u5f00\u9500\u5927\u4e14\u6269\u5c55\u6027\u5dee\u3002", "method": "LiLoRA\u5171\u4eabLoRA\u77e9\u9635A\uff0c\u5bf9\u77e9\u9635B\u8fdb\u884c\u4f4e\u79e9\u5206\u89e3\u4ee5\u51cf\u5c11\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u4f59\u5f26\u6b63\u5219\u5316\u7a33\u5b9a\u6027\u635f\u5931\u3002", "result": "\u5728\u591a\u6837\u5316CVIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLiLoRA\u5728\u987a\u5e8f\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\u3002", "conclusion": "LiLoRA\u4e3aCVIT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06318", "abs": "https://arxiv.org/abs/2508.06318", "authors": ["Giacomo D'Amicantonio", "Snehashis Majhi", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Fran\u00e7ois Bremond", "Egor Bondarev"], "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) is a challenging task due to the variability of\nanomalous events and the limited availability of labeled data. Under the\nWeakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided\nduring training, while predictions are made at the frame level. Although\nstate-of-the-art models perform well on simple anomalies (e.g., explosions),\nthey struggle with complex real-world events (e.g., shoplifting). This\ndifficulty stems from two key issues: (1) the inability of current models to\naddress the diversity of anomaly types, as they process all categories with a\nshared model, overlooking category-specific features; and (2) the weak\nsupervision signal, which lacks precise temporal information, limiting the\nability to capture nuanced anomalous patterns blended with normal events. To\naddress these challenges, we propose Gaussian Splatting-guided Mixture of\nExperts (GS-MoE), a novel framework that employs a set of expert models, each\nspecialized in capturing specific anomaly types. These experts are guided by a\ntemporal Gaussian splatting loss, enabling the model to leverage temporal\nconsistency and enhance weak supervision. The Gaussian splatting approach\nencourages a more precise and comprehensive representation of anomalies by\nfocusing on temporal segments most likely to contain abnormal events. The\npredictions from these specialized experts are integrated through a\nmixture-of-experts mechanism to model complex relationships across diverse\nanomaly patterns. Our approach achieves state-of-the-art performance, with a\n91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on\nXD-Violence and MSAD datasets. By leveraging category-specific expertise and\ntemporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6563\u5c04\u5f15\u5bfc\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08GS-MoE\uff09\uff0c\u7528\u4e8e\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff08WSVAD\uff09\uff0c\u901a\u8fc7\u4e13\u5bb6\u6a21\u578b\u548c\u65f6\u5e8f\u9ad8\u65af\u6563\u5c04\u635f\u5931\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u5f02\u5e38\u4e8b\u4ef6\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u6a21\u578b\u65e0\u6cd5\u533a\u5206\u5f02\u5e38\u7c7b\u578b\u4e14\u5f31\u76d1\u7763\u4fe1\u53f7\u7f3a\u4e4f\u7cbe\u786e\u65f6\u5e8f\u4fe1\u606f\u3002", "method": "\u63d0\u51faGS-MoE\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u5206\u522b\u6355\u6349\u7279\u5b9a\u5f02\u5e38\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u65f6\u5e8f\u9ad8\u65af\u6563\u5c04\u635f\u5931\u589e\u5f3a\u5f31\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.58%\u7684AUC\uff0c\u5e76\u5728XD-Violence\u548cMSAD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GS-MoE\u901a\u8fc7\u7c7b\u522b\u7279\u5b9a\u4e13\u5bb6\u548c\u65f6\u5e8f\u5f15\u5bfc\uff0c\u4e3a\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.06350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06350", "abs": "https://arxiv.org/abs/2508.06350", "authors": ["Yingxian Chen", "Jiahui Liu", "Ruifan Di", "Yanwei Li", "Chirui Chang", "Shizhen Zhao", "Wilton W. T. Fok", "Xiaojuan Qi", "Yik-Chung Wu"], "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models", "comment": null, "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.", "AI": {"tldr": "VA-GPT\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u4e2d\u5f02\u5e38\u4e8b\u4ef6\u7684\u603b\u7ed3\u4e0e\u5b9a\u4f4d\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u6709\u6548\u4ee4\u724c\u9009\u62e9\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u89c6\u9891\u5f02\u5e38\u4e8b\u4ef6\u65f6\u56e0\u4fe1\u606f\u5197\u4f59\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7a00\u758f\u7684\u65f6\u7a7a\u5f02\u5e38\u4fe1\u606f\u3002", "method": "\u63d0\u51faVA-GPT\u6a21\u578b\uff0c\u5305\u542b\u7a7a\u95f4\u6709\u6548\u4ee4\u724c\u9009\u62e9\uff08SETS\uff09\u548c\u65f6\u95f4\u6709\u6548\u4ee4\u724c\u751f\u6210\uff08TETG\uff09\u6a21\u5757\uff0c\u4f18\u5316\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u7528\u4e8e\u5fae\u8c03\u7684\u6307\u4ee4\u8ddf\u968f\u6570\u636e\u96c6\u548c\u8de8\u57df\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "VA-GPT\u901a\u8fc7\u6539\u8fdb\u7684\u4ee4\u724c\u9009\u62e9\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u4e8b\u4ef6\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
