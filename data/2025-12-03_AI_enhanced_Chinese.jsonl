{"id": "2512.02055", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02055", "abs": "https://arxiv.org/abs/2512.02055", "authors": ["Mirela G. Tulbure", "Julio Caineta", "Mark Broich", "Mollie D. Gaines", "Philippe Rufin", "Leon-Friedrich Thomas", "Hamed Alemohammad", "Jan Hemmerling", "Patrick Hostert"], "title": "Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale", "comment": null, "summary": "Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.\n  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.\n  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5fae\u8c03TerraMind\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5149\u5b66\u548cSAR\u6570\u636e\uff0c\u63d0\u5347\u4e86\u5168\u7403\u6d2a\u6c34\u8303\u56f4\u5236\u56fe\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6d2a\u6c34\u662f\u6700\u5177\u7834\u574f\u6027\u7684\u5929\u6c14\u76f8\u5173\u707e\u5bb3\u4e4b\u4e00\uff0c2024\u5e74\u4f5c\u4e3a\u6709\u8bb0\u5f55\u4ee5\u6765\u6700\u6696\u5e74\u4efd\uff0c\u6781\u7aef\u6d2a\u6c34\u4e8b\u4ef6\u5f71\u54cd\u4e86\u4e94\u5927\u6d32\u7684\u793e\u533a\u3002\u867d\u7136\u5730\u7403\u89c2\u6d4b\u536b\u661f\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u6d2a\u6c34\u76d1\u6d4b\u8986\u76d6\uff0c\u4f46\u64cd\u4f5c\u51c6\u786e\u6027\u4e25\u91cd\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u96c6\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u6700\u8fd1\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff08\u5982ESA-IBM\u7684TerraMind\uff09\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6cdb\u5316\u6027\uff0c\u4f46\u5176\u5728\u5168\u7403\u591a\u6837\u5316\u6d2a\u6c34\u4e8b\u4ef6\u4e2d\u7684\u6027\u80fd\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u4f7f\u7528FloodsNet\u6570\u636e\u96c6\uff08\u5305\u542b85\u4e2a\u5168\u7403\u6d2a\u6c34\u4e8b\u4ef6\u7684\u534f\u8c03\u591a\u6a21\u6001\u6570\u636e\uff0c\u5305\u62ecSentinel-1 SAR\u548cSentinel-2\u5149\u5b66\u56fe\u50cf\uff09\u5bf9TerraMind\u8fdb\u884c\u5fae\u8c03\u3002\u6d4b\u8bd5\u4e86\u56db\u79cd\u914d\u7f6e\uff08\u57fa\u7840\u6a21\u578bvs\u5927\u578b\u6a21\u578b\uff1b\u51bb\u7ed3vs\u89e3\u51bb\u9aa8\u5e72\u7f51\u7edc\uff09\uff0c\u5e76\u4e0eTerraMind Sen1Floods11\u793a\u4f8b\u4ee5\u53ca\u540c\u65f6\u5728FloodsNet\u548cSen1Floods11\u4e0a\u8bad\u7ec3\u7684U-Net\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u57fa\u7840-\u89e3\u51bb\u914d\u7f6e\u5728\u51c6\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u5927\u578b\u6a21\u578b\u3002\u5927\u578b\u89e3\u51bb\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u9ad8\u53ec\u56de\u7387\u3002\u5728FloodsNet\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u53ec\u56de\u7387\u4e0a\u4f18\u4e8eSen1Floods11\u8bad\u7ec3\u7684\u793a\u4f8b\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u603b\u4f53\u51c6\u786e\u6027\u3002U-Net\u6bd4\u6240\u6709GFM\u914d\u7f6e\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ec\u56de\u7387\uff0c\u4f46\u51c6\u786e\u6027\u548c\u7cbe\u786e\u5ea6\u7565\u4f4e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6574\u5408\u591a\u6a21\u6001\u5149\u5b66\u548cSAR\u6570\u636e\u5e76\u5fae\u8c03\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u589e\u5f3a\u8fd1\u5b9e\u65f6\u6d2a\u6c34\u5236\u56fe\u80fd\u529b\u3002\u8fd9\u9879\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u6279\u5168\u7403\u89c4\u6a21\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u6d2a\u6c34\u5206\u5272\u8bc4\u4f30\u4e4b\u4e00\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u6c14\u5019\u9002\u5e94\u548c\u707e\u5bb3\u6062\u590d\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\u6027\u3002"}}
{"id": "2512.02061", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.02061", "abs": "https://arxiv.org/abs/2512.02061", "authors": ["Zhenliang Ni", "Xiaowen Ma", "Zhenkai Wu", "Shuai Xiao", "Han Shu", "Xinghao Chen"], "title": "Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting", "comment": null, "summary": "Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.", "AI": {"tldr": "\u63d0\u51faAda-MoGE\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9ad8\u65af\u6df7\u5408\u4e13\u5bb6\u89e3\u51b3\u4f20\u7edfMoE\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u9891\u7387\u8986\u76d6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u6570\u636e\u9891\u8c31\u5206\u5e03\u4f1a\u968f\u65f6\u95f4\u6f14\u53d8\uff0c\u5bfc\u81f4\u4e3b\u5bfc\u9891\u7387\u504f\u79fb\u3002\u4f20\u7edf\u56fa\u5b9a\u4e13\u5bb6\u6570\u91cf\u7684MoE\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u8fd9\u79cd\u53d8\u5316\uff0c\u51fa\u73b0\u9891\u7387\u8986\u76d6\u4e0d\u5e73\u8861\u95ee\u9898\uff1a\u4e13\u5bb6\u592a\u5c11\u4f1a\u5ffd\u7565\u5173\u952e\u4fe1\u606f\uff0c\u592a\u591a\u4f1a\u5f15\u5165\u566a\u58f0\u3002", "method": "\u63d0\u51faAda-MoGE\u81ea\u9002\u5e94\u9ad8\u65af\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff1a1) \u96c6\u6210\u9891\u8c31\u5f3a\u5ea6\u548c\u9891\u7387\u54cd\u5e94\u6765\u81ea\u9002\u5e94\u786e\u5b9a\u4e13\u5bb6\u6570\u91cf\uff0c\u786e\u4fdd\u4e0e\u8f93\u5165\u6570\u636e\u9891\u7387\u5206\u5e03\u5bf9\u9f50\uff1b2) \u4f7f\u7528\u9ad8\u65af\u5e26\u901a\u6ee4\u6ce2\u5668\u5e73\u6ed1\u5206\u89e3\u9891\u57df\u7279\u5f81\uff0c\u907f\u514d\u76f4\u63a5\u9891\u5e26\u622a\u65ad\u5f15\u5165\u566a\u58f0\u3002", "result": "\u57286\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ec5\u4f7f\u752820\u4e07\u53c2\u6570\u3002", "conclusion": "Ada-MoGE\u901a\u8fc7\u81ea\u9002\u5e94\u786e\u5b9a\u4e13\u5bb6\u6570\u91cf\u548c\u9ad8\u65af\u5e26\u901a\u6ee4\u6ce2\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u7684\u9891\u7387\u8986\u76d6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.02280", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02280", "abs": "https://arxiv.org/abs/2512.02280", "authors": ["Noorbakhsh Amiri Golilarz", "Sindhuja Penchala", "Shahram Rahimi"], "title": "Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence", "comment": null, "summary": "Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f53\u524dAI\u7cfb\u7edf\u7684\u4e03\u5927\u6838\u5fc3\u7f3a\u9677\uff0c\u63d0\u51fa\u9700\u8981\u5411\u57fa\u4e8e\u8ba4\u77e5\u539f\u7406\u7684AI\uff08\u8ba4\u77e5\u81ea\u4e3b\u6027\uff09\u8fdb\u884c\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u4e3b\u9002\u5e94\u548c\u52a8\u6001\u884c\u4e3a\u7ba1\u7406\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u611f\u77e5\u3001\u8bed\u8a00\u3001\u63a8\u7406\u548c\u591a\u6a21\u6001\u9886\u57df\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u73b0\u4ee3AI\u7cfb\u7edf\u5728\u81ea\u6211\u76d1\u63a7\u3001\u81ea\u6211\u7ea0\u6b63\u548c\u81ea\u4e3b\u884c\u4e3a\u8c03\u8282\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86AI\u5b9e\u73b0\u7a33\u5065\u6cdb\u5316\u3001\u7ec8\u8eab\u9002\u5e94\u6027\u548c\u771f\u5b9e\u4e16\u754c\u81ea\u4e3b\u6027\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u548c\u5206\u6790\u5f53\u4ee3AI\u6a21\u578b\u7684\u4e03\u5927\u6838\u5fc3\u7f3a\u9677\uff1a\u7f3a\u4e4f\u5185\u5728\u81ea\u6211\u76d1\u63a7\u3001\u5143\u8ba4\u77e5\u610f\u8bc6\u7f3a\u5931\u3001\u56fa\u5b9a\u975e\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\u3001\u65e0\u6cd5\u91cd\u6784\u76ee\u6807\u3001\u7f3a\u4e4f\u8868\u5f81\u7ef4\u62a4\u3001\u4e0d\u8db3\u7684\u5177\u8eab\u53cd\u9988\u3001\u5185\u5728\u80fd\u52a8\u6027\u7f3a\u5931\u3002\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u89c1\u89e3\uff0c\u8fdb\u884c\u4eba\u5de5\u7cfb\u7edf\u4e0e\u751f\u7269\u8ba4\u77e5\u7684\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u67b6\u6784\uff08\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8eTransformer\u7684\u7cfb\u7edf\uff09\u5b58\u5728\u7ed3\u6784\u6027\u9650\u5236\uff0c\u4ec5\u9760\u89c4\u6a21\u6269\u5c55\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u8fd9\u4e9b\u7f3a\u9677\u5bfc\u81f4AI\u7cfb\u7edf\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u4e3b\u9002\u5e94\u3001\u52a8\u6001\u8868\u5f81\u7ba1\u7406\u548c\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u3002", "conclusion": "\u4e3b\u5f20\u5411\u57fa\u4e8e\u8ba4\u77e5\u539f\u7406\u7684AI\uff08\u8ba4\u77e5\u81ea\u4e3b\u6027\uff09\u8fdb\u884c\u8303\u5f0f\u8f6c\u53d8\uff0c\u8fd9\u79cdAI\u80fd\u591f\u5b9e\u73b0\u81ea\u6211\u5bfc\u5411\u7684\u9002\u5e94\u3001\u52a8\u6001\u8868\u5f81\u7ba1\u7406\u548c\u6709\u610f\u56fe\u7684\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\uff0c\u540c\u65f6\u914d\u5907\u6539\u9769\u6027\u76d1\u7763\u673a\u5236\uff0c\u786e\u4fdd\u81ea\u4e3b\u7cfb\u7edf\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6cbb\u7406\u6027\u5e76\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2512.02834", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02834", "abs": "https://arxiv.org/abs/2512.02834", "authors": ["Siyuan Yang", "Yang Zhang", "Haoran He", "Ling Pan", "Xiu Li", "Chenjia Bai", "Xuelong Li"], "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach", "comment": "The first two authors contributed equally. Yang Zhang leads the whole project", "summary": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \\textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.", "AI": {"tldr": "\u63d0\u51faTACO\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4f2a\u8ba1\u6570\u4f30\u8ba1\u5668\u5728\u63a8\u7406\u65f6\u9a8c\u8bc1\u52a8\u4f5c\u5757\uff0c\u89e3\u51b3VLA\u6a21\u578b\u5fae\u8c03\u540e\u7684\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898", "motivation": "VLA\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u878d\u5408\u4e86\u591a\u6837\u5316\u7684\u6570\u636e\u6a21\u5f0f\uff0c\u800c\u5fae\u8c03\u6570\u636e\u96c6\u5f80\u5f80\u5305\u542b\u8fd0\u52a8\u5b66\u4e0a\u4e0d\u7406\u60f3\u6216\u6b21\u4f18\u7684\u6f14\u793a\u6570\u636e\uff0c\u5bfc\u81f4\u5b58\u5728\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6210\u529f\u52a8\u4f5c\u6a21\u5f0f\u65e0\u5173\u7684\u5197\u4f59\u52a8\u4f5c\u6a21\u5f0f\u3002\u8fd9\u9020\u6210\u4e86\u9884\u8bad\u7ec3VLA\u6a21\u578b\u5728\u76d1\u7763\u5fae\u8c03\u540e\uff0c\u5728\u4e0d\u540c\u91c7\u6837\u566a\u58f0\u4e0b\u8868\u73b0\u51fa\u63a8\u7406\u65f6\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51faTACO\uff08\u6d4b\u8bd5\u65f6\u7f29\u653e\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4f2a\u8ba1\u6570\u4f30\u8ba1\u5668\u4f5c\u4e3a\u52a8\u4f5c\u5757\u7684\u9ad8\u4fdd\u771f\u9a8c\u8bc1\u5668\u3002VLA\u6a21\u578b\u96c6\u6210TACO\u540e\uff0c\u53ef\u4ee5\u4ece\u6240\u6709\u91c7\u6837\u7684\u52a8\u4f5c\u5757\u4e2d\u6267\u884c\u5177\u6709\u6700\u5927\u4f2a\u8ba1\u6570\u7684\u52a8\u4f5c\uff0c\u4ece\u800c\u9632\u6b62\u5206\u5e03\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u6301VLA\u7684\u6cdb\u5316\u80fd\u529b\uff08\u7ea6\u675f\u4ec5\u5728\u63a8\u7406\u65f6\u5e94\u7528\uff09\u3002\u8be5\u65b9\u6cd5\u7c7b\u4f3c\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7ecf\u5178\u53cd\u63a2\u7d22\u539f\u5219\uff0c\u4e14\u65e0\u9700\u68af\u5ea6\u8ba1\u7b97\uff0c\u76f8\u6bd4RL\u66f4\u65b0\u5177\u6709\u663e\u8457\u8ba1\u7b97\u4f18\u52bf\u3002", "result": "\u5728\u56db\u4e2a\u4eff\u771f\u57fa\u51c6\uff08RoboTwin2.0\u3001Robotwin\u3001LIBERO\u3001SimplerEnv\uff09\u548c\u53cc\u81c2\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u6210\u529f\u7387\u3002", "conclusion": "TACO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5fae\u8c03\u540e\u7684\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u52a8\u4f5c\u9a8c\u8bc1\u9632\u6b62\u5206\u5e03\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u57fa\u4e8e\u6d41\u5339\u914d\u6216\u6269\u6563\u76ee\u6807\u7684VLA\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u65b9\u6848\u3002"}}
{"id": "2512.02358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02358", "abs": "https://arxiv.org/abs/2512.02358", "authors": ["Ran Zhang", "Kun Ouyang", "Tiancheng Ma", "Yida Yang", "Dong Fang"], "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games", "comment": null, "summary": "Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.", "AI": {"tldr": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u6a21\u62dfMMO\u6e38\u620f\u73a9\u5bb6\u884c\u4e3a\uff0c\u4e3a\u6570\u503c\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u79bb\u7ebf\u4eff\u771f\u6846\u67b6", "motivation": "\u4f20\u7edfMMO\u6e38\u620f\u6570\u503c\u7cfb\u7edf\u4f18\u5316\u4f9d\u8d56\u5927\u89c4\u6a21\u5728\u7ebf\u5b9e\u9a8c\u6216\u57fa\u4e8e\u7edf\u8ba1\u6a21\u578b\u7684\u53c2\u6570\u8c03\u4f18\uff0c\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u53ef\u80fd\u7834\u574f\u73a9\u5bb6\u4f53\u9a8c\u3002\u73b0\u6709\u79bb\u7ebf\u4eff\u771f\u7cfb\u7edf\u4fdd\u771f\u5ea6\u6709\u9650\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u73a9\u5bb6\u7684\u63a8\u7406\u548c\u5e72\u9884\u53cd\u5e94", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53MMO\u4eff\u771f\u7cfb\u7edf\uff1a1) \u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u73a9\u5bb6\u884c\u4e3a\u6570\u636e\u4e0a\u8bad\u7ec3LLM\uff0c\u4f7f\u5176\u5177\u5907\u6e38\u620f\u7279\u5b9a\u9886\u57df\u7684\u51b3\u7b56\u80fd\u529b\uff1b2) \u57fa\u4e8e\u771f\u5b9e\u6e38\u620f\u65e5\u5fd7\u8bad\u7ec3\u6570\u636e\u9a71\u52a8\u7684\u73af\u5883\u6a21\u578b\uff0c\u91cd\u5efa\u52a8\u6001\u6e38\u620f\u7cfb\u7edf", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u4e0e\u771f\u5b9e\u4e16\u754c\u73a9\u5bb6\u884c\u4e3a\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5728\u5e72\u9884\u4e0b\u80fd\u4ea7\u751f\u5408\u7406\u7684\u56e0\u679c\u54cd\u5e94\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u6570\u503c\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6846\u67b6", "conclusion": "LLM\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u4eff\u771f\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u6a21\u62dfMMO\u6e38\u620f\u73a9\u5bb6\u884c\u4e3a\uff0c\u4e3a\u6e38\u620f\u6570\u503c\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4fdd\u771f\u4e14\u53ef\u89e3\u91ca\u7684\u79bb\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2512.02436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02436", "abs": "https://arxiv.org/abs/2512.02436", "authors": ["Agostino Capponi", "Alfio Gliozzo", "Brian Zhu"], "title": "Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets", "comment": null, "summary": "Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2aAI\u4ee3\u7406\u7ba1\u9053\uff0c\u81ea\u52a8\u805a\u7c7b\u9884\u6d4b\u5e02\u573a\u5e76\u8bc6\u522b\u5e02\u573a\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u4ea4\u6613\u7b56\u7565\u9a8c\u8bc1\u53d1\u73b0\u7684\u5173\u7cfb\u5177\u6709\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u9884\u6d4b\u5e02\u573a\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u5305\u62ec\u91cd\u53e0\u95ee\u9898\u3001\u9690\u542b\u7b49\u4ef7\u6027\u548c\u9690\u85cf\u77db\u76fe\uff0c\u9700\u8981\u7cfb\u7edf\u65b9\u6cd5\u6765\u53d1\u73b0\u5e02\u573a\u95f4\u7684\u8bed\u4e49\u7ed3\u6784\u5173\u7cfb\u3002", "method": "\u5f00\u53d1AI\u4ee3\u7406\u7ba1\u9053\uff1a1) \u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u5bf9\u5e02\u573a\u5408\u540c\u6587\u672c\u548c\u5143\u6570\u636e\u8fdb\u884c\u805a\u7c7b\u5206\u6790\uff1b2) \u8bc6\u522b\u805a\u7c7b\u5185\u5e02\u573a\u5bf9\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff08\u6b63\u76f8\u5173\u548c\u8d1f\u76f8\u5173\uff09\uff1b3) \u57fa\u4e8e\u5386\u53f2Polymarket\u6570\u636e\u8bc4\u4f30\u5173\u7cfb\u9884\u6d4b\u51c6\u786e\u6027\uff1b4) \u5c06\u53d1\u73b0\u7684\u5173\u7cfb\u8f6c\u5316\u4e3a\u7b80\u5355\u4ea4\u6613\u7b56\u7565\u3002", "result": "AI\u8bc6\u522b\u7684\u5e02\u573a\u5173\u7cfb\u51c6\u786e\u7387\u8fbe\u523060-70%\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u5173\u7cfb\u6784\u5efa\u7684\u4ea4\u6613\u7b56\u7565\u5728\u4e00\u5468\u65f6\u95f4\u8303\u56f4\u5185\u83b7\u5f97\u7ea620%\u7684\u5e73\u5747\u56de\u62a5\u3002", "conclusion": "\u4ee3\u7406AI\u548c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u53d1\u73b0\u9884\u6d4b\u5e02\u573a\u4e2d\u6f5c\u5728\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u8fd9\u4e9b\u53d1\u73b0\u7684\u5173\u7cfb\u5177\u6709\u5b9e\u9645\u4ea4\u6613\u4ef7\u503c\uff0c\u4e3a\u5e02\u573a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.02402", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.02402", "abs": "https://arxiv.org/abs/2512.02402", "authors": ["Yunchao Wang", "Guodao Sun", "Zihang Fu", "Zhehao Liu", "Kaixing Du", "Haidong Gao", "Ronghua Liang"], "title": "TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models", "comment": "11 pages", "summary": "With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.", "AI": {"tldr": "TaleFrame\u662f\u4e00\u4e2a\u7ed3\u5408LLM\u548cHCI\u7684\u6545\u4e8b\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u6545\u4e8b\u5206\u89e3\u4e3a\u5b9e\u4f53\u3001\u4e8b\u4ef6\u3001\u5173\u7cfb\u548c\u6545\u4e8b\u5927\u7eb2\u56db\u4e2a\u57fa\u672c\u5355\u5143\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u63a7\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u4ea4\u4e92\u754c\u9762\u3002", "motivation": "\u5f53\u524d\u6545\u4e8b\u751f\u6210\u7cfb\u7edf\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u6e05\u6670\u7684\u8f93\u5165\u89c4\u8303\uff0c\u96be\u4ee5\u51c6\u786e\u5c06\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u6ee1\u610f\u7684\u6545\u4e8b\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u6027\u3002", "method": "1. \u5c06\u6545\u4e8b\u7ed3\u6784\u5206\u89e3\u4e3a\u5b9e\u4f53\u3001\u4e8b\u4ef6\u3001\u5173\u7cfb\u548c\u6545\u4e8b\u5927\u7eb2\u56db\u4e2a\u57fa\u672c\u5355\u5143\uff1b2. \u5229\u7528Tinystories\u6570\u636e\u96c6\u6784\u5efa\u5305\u542b9,851\u4e2aJSON\u6761\u76ee\u7684\u504f\u597d\u6570\u636e\u96c6\uff1b3. \u5fae\u8c03\u672c\u5730Llama\u6a21\u578b\u5b9e\u73b0JSON2Story\u8f6c\u6362\uff1b4. \u63d0\u4f9b\u76f4\u89c2\u754c\u9762\u652f\u6301\u62d6\u653e\u3001\u9644\u52a0\u548c\u8fde\u63a5\u7b49\u4ea4\u4e92\u64cd\u4f5c\uff1b5. \u5728\u4e03\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6545\u4e8b\u5e76\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u8bc1\u660e\u4e86TaleFrame\u7684\u6709\u6548\u6027\uff0c\u6570\u636e\u96c6\u5df2\u5728Hugging Face\u4e0a\u516c\u5f00\u3002", "conclusion": "TaleFrame\u901a\u8fc7\u7ed3\u6784\u5316\u4fe1\u606f\u751f\u6210\u6545\u4e8b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u610f\u56fe\u7ffb\u8bd1\u548c\u63a7\u5236\u7c92\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.02589", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.02589", "abs": "https://arxiv.org/abs/2512.02589", "authors": ["Junyi Hou", "Andre Lin Huikai", "Nuo Chen", "Yiwei Gong", "Bingsheng He"], "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing", "comment": null, "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.", "AI": {"tldr": "PaperDebugger\uff1a\u4e00\u4e2a\u57fa\u4e8e\u591a\u4ee3\u7406\u7684Overleaf\u63d2\u4ef6\u5f0f\u5b66\u672f\u5199\u4f5c\u52a9\u624b\uff0c\u5c06LLM\u63a8\u7406\u76f4\u63a5\u96c6\u6210\u5230LaTeX\u7f16\u8f91\u5668\u4e2d\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6587\u6863\u64cd\u4f5c\u548c\u4fee\u8ba2\u7ba1\u7406\u3002", "motivation": "\u73b0\u6709AI\u5199\u4f5c\u52a9\u624b\u4e0e\u7f16\u8f91\u5668\u5206\u79bb\uff0c\u65e0\u6cd5\u6df1\u5ea6\u4ea4\u4e92\u6587\u6863\u72b6\u6001\u3001\u7ed3\u6784\u548c\u4fee\u8ba2\u5386\u53f2\uff0c\u65e0\u6cd5\u5728LaTeX\u7f16\u8f91\u5668\uff08\u5982Overleaf\uff09\u4e2d\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1Chrome\u6269\u5c55\u63d2\u4ef6\uff0c\u91c7\u7528Kubernetes\u539f\u751f\u7f16\u6392\u5c42\u548cMCP\u5de5\u5177\u94fe\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u53cc\u5411\u7f16\u8f91\u5668\u540c\u6b65\u3001\u7ec6\u7c92\u5ea6\u7248\u672c\u63a7\u5236\u3001\u5b89\u5168\u72b6\u6001\u7ba1\u7406\u3001\u591a\u4ee3\u7406\u8c03\u5ea6\u548c\u5916\u90e8\u5de5\u5177\u96c6\u6210\u3002", "result": "\u5b9e\u73b0\u4e86\u5b8c\u5168\u96c6\u6210\u7684\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u672c\u5730\u5316\u7f16\u8f91\u3001\u7ed3\u6784\u5316\u8bc4\u5ba1\u3001\u5e76\u884c\u4ee3\u7406\u6267\u884c\u548c\u57fa\u4e8e\u5dee\u5f02\u7684\u66f4\u65b0\uff0c\u7528\u6237\u754c\u9762\u5e72\u6270\u6700\u5c0f\uff0c\u65e9\u671f\u5206\u6790\u663e\u793a\u79ef\u6781\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "PaperDebugger\u9a8c\u8bc1\u4e86\u7f16\u8f91\u5668\u539f\u751f\u3001\u4ee3\u7406\u5f0f\u5199\u4f5c\u52a9\u624b\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u5b66\u672f\u5199\u4f5c\u63d0\u4f9b\u4e86\u6df1\u5ea6\u96c6\u6210\u7684AI\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2512.02633", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02633", "abs": "https://arxiv.org/abs/2512.02633", "authors": ["Mattia Giuri", "Mathias Jackermeier", "Alessandro Abate"], "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations", "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5e03\u5c14\u516c\u5f0f\u5e8f\u5217\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u9075\u5faa\u4efb\u610fLTL\u6307\u4ee4\u7684\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u9ad8\u5c42\u4e8b\u4ef6\u540c\u65f6\u53d1\u751f\u4e14\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5c06LTL\u6307\u4ee4\u89e3\u91ca\u4e3a\u6709\u9650\u81ea\u52a8\u673a\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u539f\u5b50\u547d\u9898\u540c\u65f6\u4e3a\u771f\u4e14\u53ef\u80fd\u590d\u6742\u4ea4\u4e92\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u590d\u6742\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7f16\u7801\u7b80\u5355\u5e03\u5c14\u516c\u5f0f\u5e8f\u5217\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u516c\u5f0f\u76f4\u63a5\u5bf9\u5e94\u81ea\u52a8\u673a\u4e2d\u7684\u8f6c\u79fb\uff0c\u4ece\u800c\u751f\u6210\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\uff0c\u7528\u4e8e\u6761\u4ef6\u5316\u7b56\u7565\u3002", "result": "\u5728\u590d\u6742\u7684\u57fa\u4e8e\u8c61\u68cb\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u4e2a\u9ad8\u5c42\u4e8b\u4ef6\u540c\u65f6\u53d1\u751f\u4e14\u590d\u6742\u4ea4\u4e92\u7684\u73af\u5883\uff0c\u4e3a\u9075\u5faa\u4efb\u610fLTL\u6307\u4ee4\u7684\u591a\u4efb\u52a1\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02699", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02699", "abs": "https://arxiv.org/abs/2512.02699", "authors": ["Hyeongseop Rha", "Jeong Hun Yeo", "Junil Won", "Se Jin Park", "Yong Man Ro"], "title": "Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding", "comment": "16 pages, 8 figures", "summary": "In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.", "AI": {"tldr": "MIGR\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u6a21\u6001\u91cd\u8981\u6027\u673a\u5236\uff0c\u4ece\u60c5\u611f\u4e3b\u5bfc\u6a21\u6001\u5f00\u59cb\u63a8\u7406\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u4e2d\u7684\u63a8\u7406\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63a8\u7406\u7684\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6f02\u79fb\u95ee\u9898\uff1a\u6a21\u578b\u9010\u6e10\u4f9d\u8d56\u81ea\u8eab\u751f\u6210\u7684\u6587\u672c\u800c\u975e\u591a\u6a21\u6001\u8bc1\u636e\uff0c\u4e14\u89e3\u91ca\u8fc7\u5ea6\u53d7\u89c6\u89c9\u63a8\u7406\u8def\u5f84\u5f71\u54cd\uff0c\u5bfc\u81f4\u60c5\u611f\u4e0d\u4e00\u81f4\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u91cd\u8981\u6027\u673a\u5236\u8bc6\u522b\u60c5\u611f\u4e3b\u5bfc\u6a21\u6001\uff0c\u57fa\u4e8e\u6b64\u91cd\u65b0\u7ec4\u7ec7\u63a8\u7406\u5e8f\u5217\uff0c\u4f7f\u89e3\u91ca\u4ece\u6700\u5173\u952e\u7684\u60c5\u611f\u6a21\u6001\u5f00\u59cb\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u6a21\u6001\u5bf9\u9f50\u76d1\u7763\u5fae\u8c03\u548c\u6a21\u6001\u611f\u77e5\u5956\u52b1\u4f18\u5316\u3002", "result": "\u5728DFEW\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMIGR\u663e\u8457\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027\uff0c\u5c06\u6b63\u786e\u9884\u6d4b\u4f46\u60c5\u611f\u4e0d\u4e00\u81f4\u89e3\u91ca\u7684\u6bd4\u4f8b\u4ece18.10%\u964d\u4f4e\u52307.37%\u3002", "conclusion": "\u4ece\u60c5\u611f\u4e3b\u5bfc\u6a21\u6001\u5f00\u59cb\u63a8\u7406\u80fd\u6709\u6548\u9632\u6b62\u65e9\u671f\u63a8\u7406\u88ab\u8bef\u5bfc\uff0c\u751f\u6210\u60c5\u611f\u57fa\u7840\u624e\u5b9e\u3001\u56e0\u679c\u76f8\u5173\u4e14\u4fdd\u6301\u8fde\u8d2f\u6027\u7684\u89e3\u91ca\uff0c\u8bc1\u5b9e\u4e86\u6a21\u6001\u91cd\u8981\u6027\u5f15\u5bfc\u63a8\u7406\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.02720", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02720", "abs": "https://arxiv.org/abs/2512.02720", "authors": ["He Wang", "Wenyilin Xiao", "Songqiao Han", "Hailiang Huang"], "title": "StockMem: An Event-Reflection Memory Framework for Stock Forecasting", "comment": null, "summary": "Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.", "AI": {"tldr": "StockMem\uff1a\u4e00\u4e2a\u4e8b\u4ef6-\u53cd\u601d\u53cc\u5c42\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65b0\u95fb\u4e3a\u4e8b\u4ef6\u5e76\u8fdb\u884c\u6a2a\u5411\u6574\u5408\u4e0e\u7eb5\u5411\u8ffd\u8e2a\uff0c\u6784\u5efa\u4e8b\u4ef6\u77e5\u8bc6\u5e93\u548c\u56e0\u679c\u7ecf\u9a8c\u77e5\u8bc6\u5e93\uff0c\u7528\u4e8e\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002", "motivation": "\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u9762\u4e34\u5e02\u573a\u6ce2\u52a8\u6027\u548c\u5bf9\u5b9e\u65f6\u4e8b\u4ef6\u654f\u611f\u6027\u7684\u6311\u6218\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u57fa\u4e8e\u6587\u672c\u7684\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u566a\u58f0\u65b0\u95fb\u6570\u636e\u548c\u6587\u672c\u4e2d\u7f3a\u4e4f\u660e\u786e\u7b54\u6848\u7684\u9650\u5236\u3002\u901a\u7528\u8bb0\u5fc6\u67b6\u6784\u96be\u4ee5\u8bc6\u522b\u4ef7\u683c\u53d8\u52a8\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u63d0\u51faStockMem\u4e8b\u4ef6-\u53cd\u601d\u53cc\u5c42\u8bb0\u5fc6\u6846\u67b6\uff1a1\uff09\u5c06\u65b0\u95fb\u7ed3\u6784\u5316\u5904\u7406\u4e3a\u4e8b\u4ef6\uff1b2\uff09\u6a2a\u5411\u6574\u5408\u6574\u5408\u6bcf\u65e5\u4e8b\u4ef6\uff1b3\uff09\u7eb5\u5411\u8ffd\u8e2a\u6355\u6349\u4e8b\u4ef6\u6f14\u5316\u4ee5\u63d0\u53d6\u53cd\u6620\u5e02\u573a\u9884\u671f\u5dee\u5f02\u7684\u589e\u91cf\u4fe1\u606f\uff1b4\uff09\u6784\u5efa\u65f6\u5e8f\u4e8b\u4ef6\u77e5\u8bc6\u5e93\uff1b5\uff09\u901a\u8fc7\u5206\u6790\u4e8b\u4ef6-\u4ef7\u683c\u52a8\u6001\u5f62\u6210\u56e0\u679c\u7ecf\u9a8c\u7684\u53cd\u601d\u77e5\u8bc6\u5e93\uff1b6\uff09\u9884\u6d4b\u65f6\u68c0\u7d22\u7c7b\u4f3c\u5386\u53f2\u573a\u666f\uff0c\u7ed3\u5408\u5f53\u524d\u4e8b\u4ef6\u3001\u589e\u91cf\u6570\u636e\u548c\u8fc7\u53bb\u7ecf\u9a8c\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eStockMem\u4f18\u4e8e\u73b0\u6709\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u8ffd\u8e2a\u5f71\u54cd\u4ef7\u683c\u7684\u4fe1\u606f\u94fe\u63d0\u4f9b\u66f4\u4f18\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u51b3\u7b56\u900f\u660e\u5ea6\u3002", "conclusion": "StockMem\u901a\u8fc7\u4e8b\u4ef6-\u53cd\u601d\u53cc\u5c42\u8bb0\u5fc6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u80a1\u7968\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u4e86\u91d1\u878d\u9884\u6d4b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.02425", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02425", "abs": "https://arxiv.org/abs/2512.02425", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Jaehong Yoon", "Sung Ju Hwang"], "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning", "comment": "Project page : https://worldmm.github.io", "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.", "AI": {"tldr": "WorldMM\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8bb0\u5fc6\u4ee3\u7406\uff0c\u901a\u8fc7\u6784\u5efa\u6587\u672c\u548c\u89c6\u89c9\u7684\u4e92\u8865\u8bb0\u5fc6\u6765\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u95ee\u9898\uff0c\u5728\u4e94\u4e2a\u957f\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53478.4%\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5c0f\u65f6\u6216\u5929\u7ea7\u522b\u7684\u957f\u89c6\u9891\u65f6\u9762\u4e34\u6311\u6218\uff1a\u4e0a\u4e0b\u6587\u5bb9\u91cf\u6709\u9650\uff0c\u62bd\u8c61\u8fc7\u7a0b\u4e2d\u5173\u952e\u89c6\u89c9\u7ec6\u8282\u4e22\u5931\u3002\u73b0\u6709\u7684\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u6458\u8981\uff0c\u65e0\u6cd5\u5728\u590d\u6742\u573a\u666f\u63a8\u7406\u4e2d\u5229\u7528\u89c6\u89c9\u8bc1\u636e\uff0c\u4e14\u56fa\u5b9a\u65f6\u95f4\u5c3a\u5ea6\u7684\u68c0\u7d22\u9650\u5236\u4e86\u6355\u6349\u53ef\u53d8\u65f6\u957f\u4e8b\u4ef6\u7684\u80fd\u529b\u3002", "method": "WorldMM\u6784\u5efa\u4e09\u79cd\u4e92\u8865\u8bb0\u5fc6\uff1a\u60c5\u666f\u8bb0\u5fc6\uff08\u8de8\u591a\u65f6\u95f4\u5c3a\u5ea6\u7d22\u5f15\u4e8b\u5b9e\u4e8b\u4ef6\uff09\u3001\u8bed\u4e49\u8bb0\u5fc6\uff08\u6301\u7eed\u66f4\u65b0\u9ad8\u5c42\u6982\u5ff5\u77e5\u8bc6\uff09\u3001\u89c6\u89c9\u8bb0\u5fc6\uff08\u4fdd\u7559\u573a\u666f\u8be6\u7ec6\u4fe1\u606f\uff09\u3002\u63a8\u7406\u65f6\uff0c\u81ea\u9002\u5e94\u68c0\u7d22\u4ee3\u7406\u8fed\u4ee3\u9009\u62e9\u6700\u76f8\u5173\u7684\u8bb0\u5fc6\u6e90\uff0c\u5229\u7528\u591a\u65f6\u95f4\u7c92\u5ea6\uff0c\u76f4\u5230\u6536\u96c6\u8db3\u591f\u4fe1\u606f\u3002", "result": "\u5728\u4e94\u4e2a\u957f\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e73\u5747\u6027\u80fd\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u53478.4%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u957f\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "WorldMM\u901a\u8fc7\u591a\u6a21\u6001\u8bb0\u5fc6\u67b6\u6784\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u5173\u952e\u6311\u6218\uff0c\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u8868\u793a\uff0c\u81ea\u9002\u5e94\u591a\u65f6\u95f4\u5c3a\u5ea6\u68c0\u7d22\uff0c\u4e3a\u957f\u89c6\u9891\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02914", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02914", "abs": "https://arxiv.org/abs/2512.02914", "authors": ["Zhonghao He", "Tianyi Qiu", "Hirokazu Shirado", "Maarten Sap"], "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning", "comment": "NeurIPS 2025", "summary": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9785\u6027\u8d28\u7684\u65e0\u76d1\u7763\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u63a8\u7406\u4e2d\u7684\u4fe1\u5ff5\u56fa\u5316\u73b0\u8c61\uff0c\u53d1\u73b0\u5f53\u524d\u4fe1\u5ff5\u80fd\u6b63\u5411\u9884\u6d4b\u672a\u6765\u4fe1\u5ff5\u66f4\u65b0\uff0c\u8868\u660e\u5b58\u5728\u786e\u8ba4\u504f\u8bef\u800c\u975e\u7406\u6027\u8d1d\u53f6\u65af\u66f4\u65b0\u3002", "motivation": "\u5c3d\u7ba1LLM\u63a8\u7406\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u8fed\u4ee3\u63a8\u7406\u53ef\u80fd\u52a0\u5267\u4fe1\u5ff5\u56fa\u5316\u548c\u786e\u8ba4\u504f\u8bef\uff0c\u800c\u975e\u4fc3\u8fdb\u771f\u76f8\u5bfb\u6c42\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u662f\u5426\u771f\u6b63\u9075\u5faa\u7406\u6027\u8d1d\u53f6\u65af\u66f4\u65b0\u539f\u5219\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9785\u6027\u8d28\u7684\u65e0\u76d1\u7763\u56de\u5f52\u8bc4\u5206\u65b9\u6cd5\uff08Martingale Score\uff09\uff0c\u8be5\u6027\u8d28\u8981\u6c42\u7406\u6027\u4fe1\u5ff5\u66f4\u65b0\u4e2d\u672a\u6765\u4fe1\u5ff5\u7684\u671f\u671b\u503c\u7b49\u4e8e\u5f53\u524d\u4fe1\u5ff5\u3002\u901a\u8fc7\u6d4b\u91cf\u8fdd\u53cd\u6b64\u6027\u8d28\u7684\u7a0b\u5ea6\u6765\u8bc4\u4f30\u4fe1\u5ff5\u56fa\u5316\u3002", "result": "\u5728\u4e8b\u4ef6\u9884\u6d4b\u3001\u4ef7\u503c\u8d1f\u8f7d\u95ee\u9898\u548c\u5b66\u672f\u8bba\u6587\u8bc4\u5ba1\u7b49\u5f00\u653e\u9886\u57df\uff0c\u53d1\u73b0\u4fe1\u5ff5\u56fa\u5316\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u5f53\u524d\u4fe1\u5ff5\u80fd\u6b63\u5411\u9884\u6d4b\u672a\u6765\u4fe1\u5ff5\u66f4\u65b0\u3002\u8be5\u8bc4\u5206\u80fd\u9884\u6d4b\u6709\u6807\u7b7e\u9886\u57df\u7684\u771f\u5b9e\u51c6\u786e\u6027\u3002", "conclusion": "Martingale Score\u4f5c\u4e3a\u65e0\u76d1\u7763\u6307\u6807\uff0c\u80fd\u6709\u6548\u8bc4\u4f30LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u771f\u76f8\u5bfb\u6c42\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u63a8\u7406\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4fe1\u5ff5\u56fa\u5316\u95ee\u9898\uff0c\u800c\u975e\u771f\u6b63\u7684\u7406\u6027\u8d1d\u53f6\u65af\u66f4\u65b0\u3002"}}
{"id": "2512.02465", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02465", "abs": "https://arxiv.org/abs/2512.02465", "authors": ["Xingwang Li", "Mengyun Chen", "Jiamou Liu", "Sijie Wang", "Shuanggen Jin", "Jafet C. M. Andersson", "Jonas Olsson", "Remco", "van de Beek", "Hai Victor Habi", "Congzheng Han"], "title": "TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links", "comment": null, "summary": "In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.", "AI": {"tldr": "\u63d0\u51fa\u540d\u4e3aTabGRU\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408Transformer\u548c\u53cc\u5411\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff0c\u7528\u4e8e\u5546\u4e1a\u5fae\u6ce2\u94fe\u8def\u964d\u96e8\u76d1\u6d4b\uff0c\u5728\u745e\u5178\u54e5\u5fb7\u5821\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7269\u7406\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "motivation": "\u9762\u5bf9\u5168\u7403\u57ce\u5e02\u5316\u52a0\u901f\u548c\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u589e\u591a\uff0c\u9ad8\u5206\u8fa8\u7387\u57ce\u5e02\u964d\u96e8\u76d1\u6d4b\u5bf9\u5efa\u8bbe\u97e7\u6027\u667a\u6167\u57ce\u5e02\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u7684CML\u964d\u96e8\u53cd\u6f14\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4fe1\u53f7\u566a\u58f0\u548c\u975e\u7ebf\u6027\u8870\u51cf\u7b49\u590d\u6742\u95ee\u9898\u3002", "method": "\u63d0\u51faTabGRU\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408Transformer\u548c\u53cc\u5411\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff0c\u6355\u6349CML\u4fe1\u53f7\u6570\u636e\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u548c\u5c40\u90e8\u5e8f\u5217\u7279\u5f81\u3002\u91c7\u7528\u53ef\u5b66\u4e60\u4f4d\u7f6e\u5d4c\u5165\u548c\u6ce8\u610f\u529b\u6c60\u5316\u673a\u5236\u589e\u5f3a\u52a8\u6001\u7279\u5f81\u63d0\u53d6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u745e\u5178\u54e5\u5fb7\u5821\u6570\u636e\u96c6\uff082015\u5e746-9\u6708\uff09\u4e0a\u9a8c\u8bc1\uff0c\u4f7f\u752812\u4e2a\u5b50\u94fe\u8def\u548c\u4e24\u4e2a\u96e8\u91cf\u8ba1\uff08Torp\u548cBarl\uff09\uff0c\u6d4b\u8bd5\u671f\u8986\u76d6\u7ea610\u6b21\u964d\u96e8\u4e8b\u4ef6\u3002TabGRU\u5728Torp\u7ad9\u70b9\u7684R\u00b2\u4e3a0.91\uff0cBarl\u7ad9\u70b9\u4e3a0.96\uff0c\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u76f8\u6bd4\u7269\u7406\u6a21\u578b\u80fd\u6709\u6548\u7f13\u89e3\u5cf0\u503c\u964d\u96e8\u65f6\u7684\u8fc7\u4f30\u8ba1\u95ee\u9898\u3002", "conclusion": "TabGRU\u6a21\u578b\u80fd\u6709\u6548\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u4e3a\u57fa\u4e8eCML\u7684\u57ce\u5e02\u964d\u96e8\u76d1\u6d4b\u63d0\u4f9b\u7a33\u5065\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02447", "abs": "https://arxiv.org/abs/2512.02447", "authors": ["Fan Luo", "Zeyu Gao", "Xinhao Luo", "Kai Zhao", "Yanfeng Lu"], "title": "Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors", "comment": null, "summary": "Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.", "AI": {"tldr": "TDE\u901a\u8fc7Spiking Encoder\u751f\u6210\u591a\u6837\u5316\u65f6\u95f4\u523a\u6fc0\u548cAttention Gating Module\u5efa\u6a21\u65f6\u57df\u4f9d\u8d56\uff0c\u7ed3\u5408Spike-Driven Attention\u964d\u4f4e\u80fd\u8017\uff0c\u663e\u8457\u63d0\u5347SNN\u68c0\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709SNN\u901a\u5e38\u76f4\u63a5\u590d\u5236\u8f93\u5165\u6216\u56fa\u5b9a\u95f4\u9694\u805a\u5408\uff0c\u5bfc\u81f4\u795e\u7ecf\u5143\u63a5\u6536\u51e0\u4e4e\u76f8\u540c\u7684\u65f6\u95f4\u523a\u6fc0\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u5982\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4e0d\u8db3", "method": "\u63d0\u51faTDE\u6846\u67b6\uff1a1) Spiking Encoder\u751f\u6210\u591a\u6837\u5316\u65f6\u95f4\u6b65\u8f93\u5165\u523a\u6fc0\uff1b2) Attention Gating Module\u57fa\u4e8e\u65f6\u57df\u4f9d\u8d56\u6307\u5bfcSE\u751f\u6210\uff1b3) Spike-Driven Attention\u6d88\u9664\u9ad8\u80fd\u8017\u4e58\u6cd5\u64cd\u4f5c", "result": "\u5728PASCAL VOC\u9759\u6001\u6570\u636e\u96c6\u8fbe\u523057.7% mAP50-95\uff0c\u5728\u795e\u7ecf\u5f62\u6001EvDET200K\u6570\u636e\u96c6\u8fbe\u523047.6%\uff1bSDA\u80fd\u8017\u4ec5\u4e3a\u4f20\u7edf\u6ce8\u610f\u529b\u6a21\u5757\u76840.240\u500d", "conclusion": "TDE\u80fd\u65e0\u7f1d\u96c6\u6210\u73b0\u6709SNN\u68c0\u6d4b\u5668\uff0c\u663e\u8457\u63d0\u5347\u65f6\u57df\u4fe1\u606f\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u901a\u8fc7SDA\u5927\u5e45\u964d\u4f4e\u80fd\u8017\uff0c\u4e3a\u9ad8\u6548SNN\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.03001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03001", "abs": "https://arxiv.org/abs/2512.03001", "authors": ["Thomas Rivasseau"], "title": "Invasive Context Engineering to Control Large Language Models", "comment": "4 pages", "summary": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.", "AI": {"tldr": "\u63d0\u51fa\u4fb5\u5165\u5f0f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u901a\u8fc7\u5411LLM\u4e0a\u4e0b\u6587\u63d2\u5165\u63a7\u5236\u8bed\u53e5\u6765\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u4e0b\u7684\u5b89\u5168\u9632\u62a4\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u5373\u53ef\u9632\u6b62\u8d8a\u72f1\u548c\u6076\u610f\u884c\u4e3a", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u65b9\u6cd5\uff08\u504f\u597d\u8bad\u7ec3\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u8f93\u5165\u8f93\u51fa\u8fc7\u6ee4\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\uff0c\u8d8a\u72f1\u6982\u7387\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u4e0a\u5347\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u957f\u4e0a\u4e0b\u6587\u5b89\u5168\u4fdd\u8bc1", "method": "\u63d0\u51fa\u4fb5\u5165\u5f0f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u5728LLM\u4e0a\u4e0b\u6587\u4e2d\u63d2\u5165\u63a7\u5236\u8bed\u53e5\u6765\u7ea6\u675f\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u53ef\u5c06\u8be5\u6280\u672f\u63a8\u5e7f\u5230\u601d\u7ef4\u94fe\u8fc7\u7a0b\u4e2d\u9632\u6b62\u6076\u610f\u63a8\u7406", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56LLM\u8bad\u7ec3\uff0c\u907f\u514d\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u4fb5\u5165\u5f0f\u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u89e3\u51b3LLM\u957f\u4e0a\u4e0b\u6587\u5b89\u5168\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u90e8\u5206\u89e3\u51b3\u73b0\u6709\u5b89\u5168\u673a\u5236\u7684\u4e0d\u8db3\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.02457", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02457", "abs": "https://arxiv.org/abs/2512.02457", "authors": ["Jianzong Wu", "Hao Lian", "Dachao Hao", "Ye Tian", "Qingyu Shi", "Biaolong Chen", "Hao Jiang"], "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation", "comment": "Project page at https://jianzongwu.github.io/projects/does-hearing-help-seeing/", "summary": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.", "AI": {"tldr": "\u97f3\u9891-\u89c6\u9891\u8054\u5408\u53bb\u566a\u8bad\u7ec3\u80fd\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u5373\u4f7f\u53ea\u5173\u6ce8\u89c6\u9891\u8d28\u91cf\u672c\u8eab", "motivation": "\u63a2\u7d22\u97f3\u9891-\u89c6\u9891\u8054\u5408\u53bb\u566a\u8bad\u7ec3\u662f\u5426\u80fd\u591f\u6539\u5584\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u5373\u4f7f\u6700\u7ec8\u53ea\u5173\u5fc3\u89c6\u9891\u8d28\u91cf\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u8026\u5408\u4e0d\u4ec5\u6709\u5229\u4e8e\u97f3\u9891-\u89c6\u9891\u540c\u6b65\uff0c\u4e5f\u53ef\u80fd\u63d0\u5347\u89c6\u9891\u672c\u8eab\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u53c2\u6570\u9ad8\u6548\u7684AVFullDiT\u67b6\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u89c6\u9891(T2V)\u548c\u6587\u672c\u5230\u97f3\u9891(T2A)\u6a21\u5757\u8fdb\u884c\u8054\u5408\u53bb\u566a\u3002\u8bad\u7ec3T2AV\u6a21\u578b\u548cT2V-only\u5bf9\u7167\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u9996\u6b21\u7cfb\u7edf\u8bc1\u660e\u97f3\u9891-\u89c6\u9891\u8054\u5408\u53bb\u566a\u80fd\u5e26\u6765\u8d85\u8d8a\u540c\u6b65\u7684\u6539\u8fdb\u3002\u5728\u5177\u6709\u5927\u8fd0\u52a8\u548c\u7269\u4f53\u63a5\u89e6\u8fd0\u52a8\u7684\u6311\u6218\u6027\u5b50\u96c6\u4e0a\u89c2\u5bdf\u5230\u4e00\u81f4\u6539\u8fdb\u3002\u9884\u6d4b\u97f3\u9891\u4f5c\u4e3a\u7279\u6743\u4fe1\u53f7\uff0c\u9f13\u52b1\u6a21\u578b\u5185\u5316\u89c6\u89c9\u4e8b\u4ef6\u4e0e\u5176\u58f0\u5b66\u540e\u679c\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "\u8de8\u6a21\u6001\u534f\u540c\u8bad\u7ec3\u662f\u5f00\u53d1\u66f4\u5f3a\u3001\u66f4\u5177\u7269\u7406\u57fa\u7840\u7684\u4e16\u754c\u6a21\u578b\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002\u97f3\u9891\u9884\u6d4b\u901a\u8fc7\u5efa\u7acb\u89c6\u89c9\u4e8b\u4ef6\u4e0e\u58f0\u5b66\u540e\u679c\u7684\u56e0\u679c\u5173\u7cfb\u6765\u6b63\u5219\u5316\u89c6\u9891\u52a8\u6001\u3002"}}
{"id": "2512.02835", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02835", "abs": "https://arxiv.org/abs/2512.02835", "authors": ["Yifan Li", "Yingda Yin", "Lingting Zhu", "Weikai Chen", "Shengju Qian", "Xin Wang", "Yanwei Fu"], "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning", "comment": null, "summary": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .", "AI": {"tldr": "ReVSeg\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u5206\u89e3\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5c06\u89c6\u9891\u76ee\u6807\u5206\u5272\u4efb\u52a1\u5206\u89e3\u4e3a\u8bed\u4e49\u89e3\u91ca\u3001\u65f6\u5e8f\u8bc1\u636e\u9009\u62e9\u548c\u7a7a\u95f4\u5b9a\u4f4d\u4e09\u4e2a\u6b65\u9aa4\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u6b65\u63a8\u7406\u94fe\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u76ee\u6807\u5206\u5272\u65b9\u6cd5\u901a\u5e38\u5c06\u590d\u6742\u7684\u52a8\u6001\u3001\u56e0\u679c\u5173\u7cfb\u548c\u65f6\u5e8f\u4ea4\u4e92\u63a8\u7406\u7b80\u5316\u4e3a\u6f5c\u5728\u5d4c\u5165\u8868\u793a\uff0c\u5bfc\u81f4\u63a8\u7406\u94fe\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u8ffd\u8e2a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u5206\u89e3\u63a8\u7406\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u663e\u5f0f\u5206\u89e3\u89c6\u89d2\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u4e09\u4e2a\u987a\u5e8f\u64cd\u4f5c\uff1a\u8bed\u4e49\u89e3\u91ca\u3001\u65f6\u5e8f\u8bc1\u636e\u9009\u62e9\u3001\u7a7a\u95f4\u5b9a\u4f4d\u3002\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u539f\u751f\u63a5\u53e3\u6267\u884c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u6b65\u63a8\u7406\u94fe\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u7ed3\u679c\u9a71\u52a8\u7684\u4fe1\u53f7\u4e2d\u81ea\u6211\u4f18\u5316\u51b3\u7b56\u8d28\u91cf\u3002", "result": "ReVSeg\u5728\u6807\u51c6\u89c6\u9891\u76ee\u6807\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5206\u89e3\u63a8\u7406\u8fc7\u7a0b\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u6b65\u51b3\u7b56\u94fe\uff0cReVSeg\u4e0d\u4ec5\u63d0\u5347\u4e86\u89c6\u9891\u76ee\u6807\u5206\u5272\u7684\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u63a8\u7406\u4e0d\u900f\u660e\u7684\u95ee\u9898\u3002"}}
{"id": "2512.02901", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02901", "abs": "https://arxiv.org/abs/2512.02901", "authors": ["Feiyu Wang", "Xinyu Tan", "Bokai Huang", "Yihao Zhang", "Guoan Wang", "Peizhuang Cong", "Tong Yang"], "title": "FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization", "comment": "15 pages, 3 figures", "summary": "Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.", "AI": {"tldr": "Fairy2i\u662f\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3\u5b9e\u503cLLM\u8f6c\u6362\u4e3a\u7b49\u6548\u590d\u503c\u8868\u793a\u7684\u901a\u7528\u6846\u67b6\uff0c\u652f\u6301\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\uff08\u59822\u6bd4\u7279\uff09\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u91cd\u7528\u73b0\u6709\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u663e\u8457\u63d0\u5347\u91cf\u5316\u6548\u7387\u3002", "motivation": "\u5f53\u524dLLM\u91cf\u5316\u5df2\u903c\u8fd1\u5355\u6bd4\u7279\u7406\u8bba\u6781\u9650\uff0c\u800c\u590d\u503c\u6a21\u578b\uff08\u5982iFairy\uff09\u867d\u5728\u4f4e\u6bd4\u7279\u8868\u793a\u4e0a\u66f4\u6709\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4ece\u5934\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5229\u7528\u73b0\u6709\u5e9e\u5927\u7684\u9884\u8bad\u7ec3\u5b9e\u503c\u57fa\u7840\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u3002", "method": "1) \u8bc1\u660e\u5b9e\u503c\u4e0e\u5e7f\u6cdb\u7ebf\u6027\u590d\u503c\u6620\u5c04\u4e4b\u95f4\u7684\u65e0\u635f\u6570\u5b66\u7b49\u4ef7\u6027\uff0c\u5c06\u6807\u51c6Transformer\u8f6c\u6362\u5230\u590d\u57df\uff1b2) \u91c7\u7528\u76f8\u4f4d\u611f\u77e5\u91cf\u5316\u65b9\u6848\uff0c\u4f7f\u7528\u56db\u6b21\u5355\u4f4d\u6839\u7684\u9ad8\u6548\u7801\u672c\uff1b3) \u5f15\u5165\u9012\u5f52\u6b8b\u5dee\u91cf\u5316\u673a\u5236\uff0c\u8fed\u4ee3\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\uff0c\u5b9e\u73b0\u65e0\u4e58\u6cd5\u7d2f\u52a0\u7684\u9ad8\u6548\u63a8\u7406\u3002", "result": "Fairy2i\u5c06LLaMA-2 7B\u6a21\u578b\u5728\u6709\u65482\u6bd4\u7279\u7cbe\u5ea6\u4e0b\u7684\u6027\u80fd\u6062\u590d\u5230\u63a5\u8fd1\u5168\u7cbe\u5ea6\u57fa\u7ebf\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5b9e\u503c\u4e8c\u503c\u5316\u548c\u4e09\u503c\u5316\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86\u590d\u503c\u7b97\u672f\u8868\u793a\u6548\u7387\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u9645\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
