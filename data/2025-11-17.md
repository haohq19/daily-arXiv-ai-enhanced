<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 12]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling](https://arxiv.org/abs/2511.10866)
*Seoik Jung,Taekyung Song,Yangro Lee,Sungjun Lee*

Main category: cs.CV

TL;DR: 提出了一种短窗口滑动学习框架，用于CCTV视频中的实时暴力检测，通过将视频分割为1-2秒片段并应用LLM自动标注来构建细粒度数据集。


<details>
  <summary>Details</summary>
Motivation: 传统长视频训练方法难以精确识别快速暴力事件，需要一种能够保持时间连续性并实现实时检测的方法。

Method: 将视频分割为1-2秒短片段，使用大语言模型进行自动标注构建数据集，充分利用所有帧保持时间连续性。

Result: 在RWF-2000数据集上达到95.25%准确率，在UCF-Crime长视频数据集上达到83.25%准确率，表现出强泛化能力。

Conclusion: 该方法在实时暴力检测中表现出色，具有强大的泛化能力和实际应用价值，适用于智能监控系统。

Abstract: This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.

</details>


### [2] [DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2511.10948)
*Ren Zhang,Huilai Li,Chao qi,Guoliang Xu,Tianyu Zhou,Wei wei,Jianqin Yin*

Main category: cs.CV

TL;DR: 提出DEFT-LLM方法，通过多专家解耦实现运动语义对齐，解决微表情识别中静态外观与动态运动纠缠以及文本标签与面部肌肉运动语义鸿沟的问题。


<details>
  <summary>Details</summary>
Motivation: 微表情识别中，静态外观与动态运动线索的纠缠阻碍模型关注细微运动，同时现有数据集的文本标签与面部肌肉运动不完全对应，存在语义鸿沟。

Method: 构建Uni-MER运动驱动指令数据集，利用光流和动作单元双重约束确保时空一致性；设计三专家架构解耦面部动态为结构、动态纹理和运动语义的独立表示。

Result: 在多个挑战性MER基准测试中达到最先进性能，在局部面部运动的可解释建模方面具有特别优势。

Conclusion: DEFT-LLM通过运动语义对齐和多专家解耦，有效注入物理先验并利用大语言模型的跨模态推理能力，能够精确捕捉细微情感线索。

Abstract: Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.

</details>


### [3] [SUPER Decoder Block for Reconstruction-Aware U-Net Variants](https://arxiv.org/abs/2511.11015)
*Siheon Joo,Hongjo Kim*

Main category: cs.CV

TL;DR: 提出SUPER解码器块，利用小波完美重构特性防止信息丢失，同时选择性抑制冗余特征，可即插即用地增强各种U-Net变体的高频细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有的跳跃连接编码器-解码器架构（U-Net变体）在逆问题中仍存在信息丢失问题，限制了细粒度高频细节的恢复。

Method: 利用小波的完美重构特性防止信息退化，同时选择性抑制冗余特征，作为即插即用的解码器块集成到各种U-Net变体中。

Result: 在多个裂缝检测基准测试中显著提升了细裂缝（宽度小于4像素）的分割性能，在智能手机图像去噪任务中也获得了PSNR的适度提升。

Conclusion: SUPER解码器块具有即插即用的通用性，能够在统一的重构感知框架内实现高频保真度和全局一致性。

Abstract: Skip-connected encoder-decoder architectures (U-Net variants) are widely adopted for inverse problems but still suffer from information loss, limiting recovery of fine high-frequency details. We present Selectively Suppressed Perfect Reconstruction (SUPER), which exploits the perfect reconstruction (PR) property of wavelets to prevent information degradation while selectively suppressing (SS) redundant features. Free from rigid framelet constraints, SUPER serves as a plug-and-play decoder block for diverse U-Net variants, eliminating their intrinsic reconstruction bottlenecks and enhancing representational richness. Experiments across diverse crack benchmarks, including state-of-the-art (SOTA) models, demonstrate the structural potential of the proposed SUPER Decoder Block. Maintaining comparable computational cost, SUPER enriches representational diversity through increased parameterization. In small-scale in-domain experiments on the CrackVision12K dataset, SUPER markedly improves thin-crack segmentation performance, particularly for cracks narrower than 4 px, underscoring its advantage in high-frequency dominant settings. In smartphone image denoising on SIDD, where low-frequency components prevail, SUPER still achieves a moderate gain in PSNR, confirming its robustness across low- and high-frequency regimes. These results validate its plug-and-play generality across U-Net variants, achieving high-frequency fidelity and global coherence within a unified, reconstruction-aware framework.

</details>


### [4] [AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning](https://arxiv.org/abs/2511.11025)
*Jirong Zha,Yuxuan Fan,Tianyu Zhang,Geng Chen,Yingfeng Chen,Chen Gao,Xinlei Chen*

Main category: cs.CV

TL;DR: AirCopBench是首个专门评估多模态大语言模型在具身空中协作感知中表现的基准测试，包含14.6k+问题，涵盖4个关键任务维度，在40个MLLMs上评估显示协作感知任务存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要针对使用高质量单智能体图像的基础感知任务，无法评估MLLMs在更复杂的以自我为中心的协作场景中的表现，特别是在真实世界退化感知条件下。

Method: 构建包含模拟器和真实世界数据的基准测试，通过模型、规则和人工方法在严格质量控制下生成大规模问题，涵盖场景理解、物体理解、感知评估和协作决策四个维度。

Result: 在40个MLLMs上的评估显示协作感知任务存在显著性能差距，最佳模型平均落后人类24.38%，且在不同任务间表现不一致。微调实验证实了空中协作感知和推理中模拟到真实迁移的可行性。

Conclusion: AirCopBench填补了多智能体协作感知评估基准的空白，揭示了MLLMs在复杂协作场景中的局限性，并为未来研究提供了重要工具。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.

</details>


### [5] [PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI](https://arxiv.org/abs/2511.11048)
*Sun Jo,Seok Young Hong,JinHyun Kim,Seungmin Kang,Ahjin Choi,Don-Gwan An,Simon Song,Je Hyeong Hong*

Main category: cs.CV

TL;DR: PINGS-X是一个基于轴对齐时空高斯表示的4D流MRI超分辨率框架，通过归一化高斯溅射、轴对齐高斯和合并过程，显著减少训练时间并提高超分辨率精度。


<details>
  <summary>Details</summary>
Motivation: 4D流MRI需要高时空分辨率来早期检测心血管疾病，但传统方法在获取速度与精度之间存在权衡。现有PINNs方法训练过程缓慢且需为每个患者单独训练，限制了实际应用。

Method: 提出PINGS-X框架，采用轴对齐时空高斯表示：1) 具有形式收敛保证的归一化高斯溅射；2) 轴对齐高斯简化高维数据训练同时保持精度和收敛性；3) 高斯合并过程防止退化解并提升计算效率。

Result: 在计算流体动力学和真实4D流MRI数据集上的实验表明，PINGS-X显著减少训练时间，同时实现优越的超分辨率精度。

Conclusion: PINGS-X通过创新的高斯表示方法，有效解决了4D流MRI超分辨率中的训练效率问题，为心血管诊断提供了实用的解决方案。

Abstract: 4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.

</details>


### [6] [NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion](https://arxiv.org/abs/2511.11051)
*Chuheng Chen,Xiaofei Zhou,Geyuan Zhang,Yong Huang*

Main category: cs.CV

TL;DR: 提出NP-LoRA方法解决LoRA融合中的干扰问题，通过零空间投影实现子空间分离，提升融合质量


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法基于权重合并，导致一个LoRA主导另一个，产生干扰和保真度下降问题

Method: 使用奇异值分解提取主风格方向，将主题LoRA投影到正交零空间，并引入软投影机制平衡保真度与一致性

Result: 实验表明NP-LoRA在DINO、CLIP指标和人类/LLM偏好得分上均优于基线方法

Conclusion: NP-LoRA能有效防止结构干扰，提升LoRA融合质量，且无需重新训练即可应用于各种骨干网络和LoRA对

Abstract: Low-Rank Adaptation (LoRA) fusion has emerged as a key technique for reusing and composing learned subject and style representations for controllable generation without costly retraining. However, existing methods rely on weight-based merging, where one LoRA often dominates the other, leading to interference and degraded fidelity. This interference is structural: separately trained LoRAs occupy low-rank high-dimensional subspaces, leading to non-orthogonal and overlapping representations. In this work, we analyze the internal structure of LoRAs and find their generative behavior is dominated by a few principal directions in the low-rank subspace, which should remain free from interference during fusion. To achieve this, we propose Null Space Projection LoRA (NP-LoRA), a projection-based framework for LoRA fusion that enforces subspace separation to prevent structural interference among principal directions. Specifically, we first extract principal style directions via singular value decomposition (SVD) and then project the subject LoRA into its orthogonal null space. Furthermore, we introduce a soft projection mechanism that enables smooth control over the trade-off between subject fidelity and style consistency. Experiments show NP-LoRA consistently improves fusion quality over strong baselines (e.g., DINO and CLIP-based metrics, with human and LLM preference scores), and applies broadly across backbones and LoRA pairs without retraining.

</details>


### [7] [From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening](https://arxiv.org/abs/2511.11065)
*Muskaan Chopra,Lorenz Sparrenberg,Armin Berger,Sarthak Khanna,Jan H. Terheyden,Rafet Sifa*

Main category: cs.CV

TL;DR: 这篇论文系统综述了2016-2025年间糖尿病视网膜病变(DR)检测的深度学习研究进展，整合了50多项研究和20多个数据集，重点关注方法创新、评估标准和临床转化挑战。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球可预防性失明的主要原因，早期检测对减少视力丧失至关重要。深度学习在过去十年中彻底改变了DR筛查，但需要系统总结技术进展和临床转化障碍。

Method: 采用系统性综述方法，分析自监督学习、半监督学习、领域泛化、联邦训练和混合神经符号模型等技术进展，同时考察评估协议、报告标准和可重复性挑战。

Result: 建立了涵盖多个数据集的基准性能表，识别了多中心验证和临床信任方面的开放性问题，将技术进步与转化障碍联系起来。

Conclusion: 为可重复、隐私保护和临床可部署的DR AI制定了实用议程，同时指出所调查的创新方法广泛适用于大规模医学成像。

Abstract: Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.

</details>


### [8] [Machine-Learning Based Detection of Coronary Artery Calcification Using Synthetic Chest X-Rays](https://arxiv.org/abs/2511.11093)
*Dylan Saeed,Ramtin Gharleghi,Susann Bier,Sonit Singh*

Main category: cs.CV

TL;DR: 本研究系统评估了数字重建放射影像（DRR）作为冠状动脉钙化检测的替代训练域，通过生成合成DRR图像并优化训练策略，实现了与现有CXR研究相当的检测性能。


<details>
  <summary>Details</summary>
Motivation: CT是冠状动脉钙化检测的金标准但成本高，胸片成本低但缺乏可靠标签，DRR通过投影CT体积生成类似胸片的图像并继承精确标签，为大规模筛查提供可行方案。

Method: 使用667个CT扫描生成合成DRR，评估模型容量、超分辨率保真度增强、预处理和训练策略，包括轻量CNN从头训练、超分辨率与对比度增强配对、课程学习等。

Result: 最佳配置达到平均AUC 0.754，与现有基于胸片的研究相当或更优，轻量CNN从头训练优于大型预训练网络，超分辨率与对比度增强结合显著提升性能。

Conclusion: DRR为冠状动脉钙化检测提供了可扩展、标签丰富的训练基础，为未来向真实胸片的迁移学习和域适应奠定了基础。

Abstract: Coronary artery calcification (CAC) is a strong predictor of cardiovascular events, with CT-based Agatston scoring widely regarded as the clinical gold standard. However, CT is costly and impractical for large-scale screening, while chest X-rays (CXRs) are inexpensive but lack reliable ground truth labels, constraining deep learning development. Digitally reconstructed radiographs (DRRs) offer a scalable alternative by projecting CT volumes into CXR-like images while inheriting precise labels. In this work, we provide the first systematic evaluation of DRRs as a surrogate training domain for CAC detection. Using 667 CT scans from the COCA dataset, we generate synthetic DRRs and assess model capacity, super-resolution fidelity enhancement, preprocessing, and training strategies. Lightweight CNNs trained from scratch outperform large pretrained networks; pairing super-resolution with contrast enhancement yields significant gains; and curriculum learning stabilises training under weak supervision. Our best configuration achieves a mean AUC of 0.754, comparable to or exceeding prior CXR-based studies. These results establish DRRs as a scalable, label-rich foundation for CAC detection, while laying the foundation for future transfer learning and domain adaptation to real CXRs.

</details>


### [9] [Reverberation: Learning the Latencies Before Forecasting Trajectories](https://arxiv.org/abs/2511.11164)
*Conghao Wong,Ziqian Zou,Beihao Xia,Xinge You*

Main category: cs.CV

TL;DR: 提出了一种基于混响变换的轨迹预测模型Rev，通过可学习的混响核显式建模和预测智能体对轨迹变化事件的延迟响应，实现可控的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测方法难以显式学习和预测智能体对轨迹变化事件的延迟响应（latencies），这会影响预测系统的因果连续性并导致不合理的轨迹。

Method: 受声学中混响曲线的启发，提出混响变换和Rev模型，使用两个显式可学习的混响核来模拟和预测每个智能体的不同延迟偏好及其随机性。

Result: 在多个数据集上的实验表明，Rev在保持竞争力的预测精度的同时，能够揭示跨智能体和场景的可解释延迟动态。

Conclusion: 混响变换具有作为通用延迟建模方法的潜力，能够实现基于预测延迟的可控轨迹预测。

Abstract: Bridging the past to the future, connecting agents both spatially and temporally, lies at the core of the trajectory prediction task. Despite great efforts, it remains challenging to explicitly learn and predict latencies, the temporal delays with which agents respond to different trajectory-changing events and adjust their future paths, whether on their own or interactively. Different agents may exhibit distinct latency preferences for noticing, processing, and reacting to any specific trajectory-changing event. The lack of consideration of such latencies may undermine the causal continuity of the forecasting system and also lead to implausible or unintended trajectories. Inspired by the reverberation curves in acoustics, we propose a new reverberation transform and the corresponding Reverberation (short for Rev) trajectory prediction model, which simulates and predicts different latency preferences of each agent as well as their stochasticity by using two explicit and learnable reverberation kernels, allowing for the controllable trajectory prediction based on these forecasted latencies. Experiments on multiple datasets, whether pedestrians or vehicles, demonstrate that Rev achieves competitive accuracy while revealing interpretable latency dynamics across agents and scenarios. Qualitative analyses further verify the properties of the proposed reverberation transform, highlighting its potential as a general latency modeling approach.

</details>


### [10] [Computationally-efficient deep learning models for nowcasting of precipitation: A solution for the Weather4cast 2025 challenge](https://arxiv.org/abs/2511.11197)
*Anushree Bhuskute,Kaushik Gopalan,Jeet Shah*

Main category: cs.CV

TL;DR: 基于ConvGRU的迁移学习框架用于短期降雨预测，使用SEVIRI红外通道数据，采用两阶段训练策略预测未来4小时降雨量，在Weather4Cast 2025竞赛中累计降雨任务获得第二名。


<details>
  <summary>Details</summary>
Motivation: 开发有效的短期降雨预测方法，为天气预报提供准确的技术支持，特别是在Weather4Cast竞赛背景下提升降雨预测性能。

Method: 使用ConvGRU模型，采用两阶段训练：第一阶段预测SEVIRI亮度温度，第二阶段通过非线性变换将预测场映射为OPERA兼容的降雨率。对于事件预测任务，使用3D事件检测和时空特征提取。

Result: 在累计降雨任务中获得第二名，事件预测任务表现与基准模型相当。

Conclusion: ConvGRU迁移学习框架在短期降雨预测中表现良好，证明了该方法的有效性，特别是在处理时空模式和降雨事件检测方面。

Abstract: This study presents a transfer-learning framework based on Convolutional Gated Recurrent Units (ConvGRU) for short-term rainfall prediction in the Weather4Cast 2025 competition. A single SEVIRI infrared channel (10.8 μm wavelength) is used as input, which consists of four observations over a one-hour period. A two-stage training strategy is applied to generate rainfall estimates up to four hours ahead. In the first stage, ConvGRU is trained to forecast the brightness temperatures from SEVIRI, enabling the model to capture relevant spatiotemporal patterns. In the second stage, an empirically derived nonlinear transformation maps the predicted fields to OPERA-compatible rainfall rates.
  For the event-prediction task, the transformed rainfall forecasts are processed using 3D event detection followed by spatiotemporal feature extraction to identify and characterize precipitation events. Our submission achieved 2nd place in the cumulative rainfall task. Further, the same model was used out-of-the-box for the event prediction task, and resulted in similar scores as the baseline model to the competition.

</details>


### [11] [YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation](https://arxiv.org/abs/2511.11344)
*Pavel Rojtberg,Julius Kühn*

Main category: cs.CV

TL;DR: YCB-Ev SD是一个用于6DoF物体姿态估计的标准分辨率事件相机合成数据集，包含50,000个事件序列，采用基于物理的渲染和线性相机运动模拟生成。


<details>
  <summary>Details</summary>
Motivation: 解决事件视觉领域缺乏与帧基计算机视觉相媲美的综合性合成数据资源的问题。

Method: 使用基于物理的渲染(PBR)场景，遵循BOP方法学，通过模拟线性相机运动生成事件序列，确保完整场景覆盖。

Result: 系统评估显示，具有线性衰减和双通道极性编码的时间表面在姿态估计性能上表现最优，显著优于指数衰减和单通道替代方案。

Conclusion: 极性信息对性能提升贡献最大，线性时间编码比指数衰减更有效地保留关键运动信息，数据集以结构化格式提供，便于立即使用和可重复基准测试。

Abstract: We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation. While synthetic data has become fundamental in frame-based computer vision, event-based vision lacks comparable comprehensive resources. Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology. Our generation framework employs simulated linear camera motion to ensure complete scene coverage, including background activity.
  Through systematic evaluation of event representations for CNN-based inference, we demonstrate that time-surfaces with linear decay and dual-channel polarity encoding achieve superior pose estimation performance, outperforming exponential decay and single-channel alternatives by significant margins. Our analysis reveals that polarity information contributes most substantially to performance gains, while linear temporal encoding preserves critical motion information more effectively than exponential decay. The dataset is provided in a structured format with both raw event streams and precomputed optimal representations to facilitate immediate research use and reproducible benchmarking.
  The dataset is publicly available at https://huggingface.co/datasets/paroj/ycbev_sd.

</details>


### [12] [BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.11421)
*Lan Li,Tao Hu,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: BOFA是一个用于类增量学习的新框架，通过仅调整CLIP的跨模态桥接层实现无额外参数的高效适应，利用正交低秩融合防止遗忘，结合跨模态混合原型提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 将CLIP应用于类增量学习面临两个挑战：下游任务适应需要额外模块增加复杂度和遗忘风险；现有方法未能充分利用多模态表示的互补优势。

Method: BOFA框架：1) 仅调整CLIP的跨模态桥接层，不增加参数；2) 正交低秩融合机制，在低秩安全子空间更新参数防止遗忘；3) 跨模态混合原型融合稳定的文本原型和视觉对应物。

Result: 在标准基准测试上的广泛实验表明，BOFA在准确性和效率方面优于现有方法。

Conclusion: BOFA通过限制性参数更新和跨模态融合，实现了高效且抗遗忘的类增量学习，无需数据回放。

Abstract: Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 提出了BREP ReFT方法，通过优化推理前缀生成、早期干预和约束干预向量幅度，解决了ReFT在数学推理任务上性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: ReFT方法在数学推理任务上表现显著下降，主要原因是难以生成有效的推理前缀，以及干扰数值编码导致错误累积。

Method: BREP ReFT通过截断训练数据优化初始推理前缀生成，在早期推理阶段进行干预防止错误累积，并约束干预向量幅度避免干扰数值编码。

Result: 在各种模型架构上的广泛实验表明，BREP在数学推理任务上优于标准ReFT和基于权重的PEFT方法，具有更好的效果、效率和泛化能力。

Conclusion: BREP ReFT有效提升了ReFT在数学推理任务上的性能，解决了其在此类任务上的局限性。

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [14] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 本文提出了一个变分量子核框架，在8个高维真实世界数据集上验证了量子核方法相对于经典RBF核的性能优势。


<details>
  <summary>Details</summary>
Motivation: 当前量子核方法研究主要局限于低维或合成数据集，无法充分评估其在真实世界高维数据上的实际潜力。

Method: 开发了使用资源高效ansatz的变分量子核框架，并引入了参数缩放技术来加速收敛。

Result: 在8个具有挑战性的真实世界高维数据集上的经典模拟结果显示，所提出的量子核比标准经典核（如RBF核）具有明显的性能优势。

Conclusion: 适当设计的量子核可以作为多功能、高性能的工具，为真实世界机器学习中的量子增强应用奠定基础，但需要进一步研究来全面评估实际量子优势。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [15] [Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters](https://arxiv.org/abs/2511.10898)
*Chenghao Duan,Chuanyi Ji*

Main category: cs.LG

TL;DR: 提出了一种基于图注意力网络(GAT)的新方法，用于预测恶劣天气导致的停电持续时间，在四个飓风影响下的501个县数据上取得了93%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 自然灾害导致的大规模停电造成巨大经济和社会影响，准确预测停电恢复时间对电网韧性至关重要。现有方法面临空间依赖性、空间异质性和事件数据有限三大挑战。

Method: 使用图注意力网络(GAT)，采用无监督预训练和半监督学习相结合的方法，利用地理空间和天气数据来估计停电持续时间。

Result: 模型在四个飓风影响下的501个县数据上表现优异，准确率超过93%，比XGBoost、随机森林、GCN和简单GAT方法高出2%-15%。

Conclusion: 提出的GAT方法在预测恶劣天气导致的停电持续时间方面表现出色，能够有效处理空间依赖性和异质性挑战，为电网韧性管理提供了有效工具。

Abstract: Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.

</details>


### [16] [SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems](https://arxiv.org/abs/2511.11111)
*Xin Wang,Pietro Lodi Rizzini,Sourav Medya,Zhiling Lan*

Main category: cs.LG

TL;DR: 提出了一种结合图神经网络和大型语言模型的替代模型，用于准确预测Dragonfly网络中的应用程序运行时间，支持高效的混合仿真。


<details>
  <summary>Details</summary>
Motivation: Dragonfly网络在高性能计算中广泛应用，但共享网络链路上的工作负载干扰是一个主要挑战。传统的并行离散事件仿真计算成本高，不适合大规模或实时场景。

Method: 开发了结合图神经网络和大型语言模型的替代模型，从端口级路由器数据中捕捉空间和时间模式。

Result: 该模型在运行时间预测方面优于现有的统计和机器学习基准方法。

Conclusion: 该模型能够准确预测运行时间，支持Dragonfly网络的高效混合仿真。

Abstract: The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.

</details>


### [17] [Adaptive Symmetrization of the KL Divergence](https://arxiv.org/abs/2511.11159)
*Omri Ben-Dov,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: 提出一种新方法来最小化Jeffreys散度，通过使用代理模型联合训练，结合归一化流和能量模型的优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用前向KL散度虽然易于处理，但其不对称性可能无法捕捉目标分布的所有特性。对称替代方案如Jeffreys散度计算困难，需要开发新方法。

Method: 使用代理模型联合训练，将任务表述为约束优化问题，通过自适应调整模型优先级来优化Jeffreys散度。

Result: 开发出实用算法，能够结合归一化流和能量模型的优势，在密度估计、图像生成和基于模拟的推理等任务中表现良好。

Conclusion: 提出的联合训练框架为最小化Jeffreys散度提供了有效解决方案，成功结合了不同概率模型的优点。

Abstract: Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.

</details>


### [18] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 使用机器学习方法改进热浪等气候极端事件预测，通过功率均值聚合集成预测显著提升分类器性能，在预测极端高温事件方面优于传统均值预测方法。


<details>
  <summary>Details</summary>
Motivation: 解决气候极端事件（特别是热浪）预测的关键挑战，提高预测准确性。

Method: 将问题构建为分类问题，预测地表气温是否会在指定时间内超过局部q分位数；通过功率均值聚合集成预测，使基于机器学习的天气预报模型具有生成性。

Result: 功率聚合方法显著提升了分类器性能，在预测极端高温事件方面优于传统均值预测，且对更高极端事件的预测效果更好。

Conclusion: 功率均值聚合方法在气候极端事件预测中显示出良好前景和适应性，其最优性能随所选分位数阈值变化，对更高极端事件的预测效果更佳。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [19] [Sparse Methods for Vector Embeddings of TPC Data](https://arxiv.org/abs/2511.11221)
*Tyler Wheeler,Michelle P. Kuchera,Raghuram Ramanujan,Ryan Krupp,Chris Wrede,Saiprasad Ravishankar,Connor L. Cross,Hoi Yan Ian Heung,Andrew J. Jones,Benjamin Votaw*

Main category: cs.LG

TL;DR: 本文探索了在时间投影室(TPC)数据上使用稀疏卷积网络进行表示学习，发现即使是随机权重的稀疏ResNet架构也能提供有用的事件向量嵌入，通过预训练可进一步提升嵌入质量。


<details>
  <summary>Details</summary>
Motivation: TPC探测器在核物理实验中广泛应用，但传统分析方法难以有效处理其复杂数据。研究旨在开发通用表示学习方法，提高TPC数据分析效率。

Method: 将原始pad级信号表示为稀疏张量，使用Minkowski Engine ResNet模型训练，在GADGET II TPC数据上进行预训练，并在AT-TPC数据上进行跨探测器测试。

Result: 未训练的稀疏ResNet模型已能提供有用的AT-TPC数据嵌入，在GADGET数据上训练后嵌入质量进一步提升，揭示了丰富的事件结构。

Conclusion: 稀疏卷积技术有潜力成为TPC实验中通用的表示学习工具，适用于不同类型的探测器数据。

Abstract: Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $β$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.

</details>


### [20] [When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering](https://arxiv.org/abs/2511.11380)
*Jiangkai Long,Yanran Zhu,Chang Tang,Kun Sun,Yuanyuan Liu,Xuesong Yan*

Main category: cs.LG

TL;DR: SemST是一个语义引导的空间转录组学数据聚类框架，利用大语言模型将基因符号转化为生物语义嵌入，并与图神经网络捕获的空间关系融合，通过细粒度语义调制模块动态注入高阶生物知识。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型将基因视为孤立数值特征，忽略了基因符号中编码的丰富生物语义，阻碍了对关键生物特征的深入理解。

Method: 使用大语言模型将每个组织位点的基因集转化为生物语义嵌入，与图神经网络捕获的空间邻域关系融合，并引入细粒度语义调制模块进行特征校准。

Result: 在公共空间转录组学数据集上的广泛实验表明，SemST实现了最先进的聚类性能，且FSM模块具有即插即用的通用性。

Conclusion: SemST通过整合生物语义和空间结构，显著提升了空间转录组学数据的聚类性能，为理解组织微环境提供了新视角。

Abstract: Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to "speak" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments](https://arxiv.org/abs/2511.10810)
*Ran Elgedawy,Sanjay Das,Ethan Seefried,Gavin Wiggins,Ryan Burchfield,Dana Hewit,Sudarshan Srinivasan,Todd Thomas,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: HARNESS是一个模块化AI框架，用于预测危险事件和分析美国能源部环境中的操作风险，通过集成大语言模型、结构化工作数据、历史事件检索和风险分析，结合专家参与形成自适应学习循环。


<details>
  <summary>Details</summary>
Motivation: 关键任务工作场所的操作安全至关重要，需要应对复杂危险的任务环境，开发能够主动预测危险事件的系统。

Method: 集成大语言模型与结构化工作数据、历史事件检索和风险分析，采用人在回路机制让领域专家优化预测，形成自适应学习循环。

Result: 初步部署显示有前景的结果，提高了预测安全系统的可靠性和效率。

Conclusion: HARNESS通过结合专家协作和迭代推理，改进了预测安全系统，未来工作将关注准确性、专家一致性和决策延迟的定量评估。

Abstract: Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.

</details>


### [22] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建事故前场景并推断车辆行为，在复杂事故案例中实现了100%准确率，超越了人类专家的92%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故重建依赖人工经验，在处理不完整多模态数据时结果不一致，需要更精确和自动化的解决方案。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言事故重建；第二阶段结合时间事件数据记录器进行深度事故推理。

Result: 在39个复杂追尾事故案例中，框架实现了完美准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，即使在处理不完整数据时也保持稳健性能。

Conclusion: 该研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建碰撞动力学和表征事故前行为方面提供了前所未有的精确度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [23] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 提出了一种基于智能AI的规划支持系统，用于生成面向需求的动态规划单元，以改进灾害规划。系统结合人类参与原则，通过扩展的自组织地图和AI代理来指导区域划分过程。


<details>
  <summary>Details</summary>
Motivation: 传统的规划单元（如人口普查区、邮政编码区等）往往无法捕捉当地社区的特定需求，缺乏灵活性来实施有效的灾害预防或应对策略。

Method: 构建了一个基于代表性初始化空间约束自组织地图（RepSC-SOM）的平台，扩展了传统SOM方法，结合自适应地理过滤和区域增长细化。AI代理能够推理、规划和行动，指导输入特征选择、空间约束设置和交互式探索。

Result: 通过佛罗里达州杰克逊维尔市的洪水相关风险案例研究，展示了平台允许用户交互式探索、生成和评估区域划分的能力，将计算严谨性与用户驱动决策相结合。

Conclusion: 该系统成功地将计算方法和人类决策相结合，为灾害规划提供了更灵活、需求导向的动态规划单元生成方案。

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [24] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个推理时框架，将大视觉语言模型的对齐重新定义为经济理性的搜索问题，通过前瞻性函数动态权衡安全性、效用和成本，在降低计算成本的同时实现强大的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法在安全性、实用性和运营成本之间存在权衡困难，且仅关注最终输出的过程盲目性会浪费大量计算预算在不安全的推理上，允许有害推理通过良性理由伪装来规避简单的安全评分。

Method: 将LVLM视为有限理性代理，逐步扩展思维图，使用前瞻性函数（类似净现值）对行动进行评分，动态权衡预期安全性、效用和成本与剩余预算，并通过最薄弱环节原则强制执行路径安全性。

Result: 在3个闭源和2个开源模型、6个数据集上的广泛实验表明，EcoAlign在较低计算成本下匹配或超越了最先进的安全性和实用性。

Conclusion: EcoAlign为强大的LVLM对齐提供了一条原则性、经济性的路径，解决了安全与效率的平衡问题。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [25] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: 提出了WordNet中副词的系统化超义分类法，填补了副词语义分类的空白，并通过实证验证了其可靠性


<details>
  <summary>Details</summary>
Motivation: WordNet为名词和动词提供了丰富的超义层次结构，但副词缺乏系统的语义分类，限制了其在自然语言处理中的应用

Method: 开发了基于语言学理论的副词超义类型学，包括方式、时间、频率、程度、领域、说话者导向和主语导向等语义域，并通过标注研究进行实证验证

Result: 试点标注研究表明，这些类别能够广泛覆盖自然文本中的副词，并且人类标注者能够可靠地分配这些类别

Conclusion: 该类型学扩展了WordNet的覆盖范围，使其更贴近语言学理论，并促进词义消歧、事件抽取、情感分析和话语建模等下游NLP应用

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>
