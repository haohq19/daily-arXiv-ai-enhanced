<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: 这篇论文提出了一个新的多分辨率多模态微表情数据集，利用事件相机收集同步RGB和事件数据，在变化照明条件下录制微表情。基于事件数据的行为单元分类和帧重建任务都取得了比传统RGB方法更好的结果。


<details>
  <summary>Details</summary>
Motivation: 微表情分析在人机交互和驾驶员监测等领域有重要应用，但传统RGB相机因为时间分辨率限制和运动模糊问题，很难准确捕捉细微而快速的面部运动。事件相机具有微秒级精度、高动态范围和低延迟的优势，但目前公开的基于事件相机的行为单元数据集仍然稀缺。

Method: 研究提出了一个新的多分辨率多模态微表情数据集，使用同步的RGB和事件相机在变化照明条件下进行录制。评估了两个基准任务：1）使用脏电神经网络进行行为单元分类；2）使用条件变分自动编码器进行帧重建。

Result: 实验结果显示：使用事件数据的行为单元分类准确率达到51.23%（RGB数据仅为23.12%）；高分辨率事件输入的帧重建任务达到SSIM=0.8513和PSNR=26.89dB的高质量。

Conclusion: 这些结果表明事件基数据可以有效用于微表情识别和帧重建，为微表情分析领域提供了一种有前景的新方法。

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [2] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD是一个基于扩散模型的通用事件边界检测方法，通过生成式视角解决事件边界检测问题，能够产生多样化的边界预测结果。


<details>
  <summary>Details</summary>
Motivation: 传统的事件边界检测方法主要关注确定性预测，忽视了事件边界的主观性和解决方案的多样性。

Method: 提出基于扩散的边界检测模型，通过时间自相似性编码相邻帧的相关变化，然后迭代地将随机噪声解码为合理的事件边界，使用分类器自由引导控制多样性。

Result: 在Kinetics-GEBD和TAPOS两个标准基准测试中表现出色，能够生成多样且合理的事件边界。

Conclusion: 扩散模型为通用事件边界检测提供了新的生成式解决方案，能够有效处理事件边界的主观多样性问题。

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [3] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef是一个三层次量化感知防御框架，通过特征不对齐惩罚、梯度感知失谐惩罚和联合量化感知训练协议，有效抵御QNN中的补丁式对抗攻击跨比特宽度迁移性


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在边缘设备中广泛部署，但现有防御要么过拟合固定量化设置，要么无法解决跨比特泛化漏洞，特别是对局部高显著性补丁攻击的迁移性防御不足

Method: TriQDef包含三个核心组件：1）特征不对齐惩罚（FDP）通过惩罚中间表示的感知相似性来强制语义不一致；2）梯度感知失谐惩罚（GPDP）通过边缘IoU和HOG余弦度量最小化结构性和方向性一致性来显式错位输入梯度；3）联合量化感知训练协议在多个量化级别上统一这些惩罚

Result: 在CIFAR-10和ImageNet上的广泛实验表明，TriQDef在未见过的补丁和量化组合上将攻击成功率降低了40%以上，同时保持高清洁准确率

Conclusion: 研究强调了破坏语义和感知梯度对齐对于减轻QNN中补丁迁移性的重要性，TriQDef框架有效解决了跨比特泛化漏洞

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [4] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 提出SNNSIR，一种全聚合刷新网络结构，通过刷新残差基础块、刷新双目卷积调制和刷新双目交叉注意力模块，在保持低功耗的同时实现了竞争性能的双目图像恢复。


<details>
  <summary>Details</summary>
Motivation: 现有的混合SNN-ANN模型仍依赖浮点运算，与SNN的二进制和事件驱动性质不兼容。需要一种全刷新驱动的简单有效结构，以实现低功耗硬件友好的双目图像恢复。

Method: 1. 轻量刷新残差基础块(SRBB)增强信息流 2. 刷新双目卷积调制(SSCM)模块通过元素乘法实现简化非线性 3. 刷新双目交叉注意力(SSCA)模块支持双向特征交互

Result: 在雨纹去除、雨滴去除、低光增强、超分辨率等多种双目图像恢复任务上，模型实现了竞争性恢复性能，同时显著降低计算开销。

Conclusion: SNNSIR演示了刷新神经网络在双目图像恢复中的潜力，为实时低功耗双目视觉应用提供了可行解决方案。

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [5] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: 提出IPGPhormer框架，通过图-Transformer结构解决病理图像生存分析中长距离空间关系和局部上下文依赖的平衡问题，并提供组织级和细胞级的可解释性


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡长距离空间关系建模和局部上下文依赖，且缺乏内在可解释性，限制了临床实用性

Method: Interpretable Pathology Graph-Transformer (IPGPhormer)框架，捕获肿瘤微环境特征并建模组织空间依赖关系，无需后处理人工标注即可提供多级可解释性

Result: 在四个公共基准数据集上全面评估，IPGPhormer在预测准确性和可解释性方面均优于最先进方法

Conclusion: IPGPhormer为癌症预后评估提供了有前景的工具，为病理学中更可靠和可解释的决策支持系统铺平了道路

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [6] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 通过多样式图像结合图像提示适配器和统计对齐技术，在去噪UNet的交叉注意力和自注意力层进行干预，实现了更准确的图像风格转换效果


<details>
  <summary>Details</summary>
Motivation: 解决现有潜圣扩散模型在图像风格转换中存在的风格匹配不准、可用风格图像数量有限、内容与风格恒结等问题

Method: 利用多张风格图像来更好表征风格特征，避免风格图像中的内容泄漏；设计了结合图像提示适配器和去噪过程中特征统计对齐的方法；在交叉注意力和自注意力层进行干预；采用聚类技术从大量风格样本关注特征中精炼出小规模代表性集合

Result: 该方法在风格化任务上达到了目前最好的结果

Conclusion: 通过多风格图像结合和多层次干预技术，有效解决了图像风格转换中的关键挑战，实现了更准确的风格控制

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [7] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: CVPR 2025事件视觉研讨会举办的时空实例分割挑战赛概述，包含任务描述、数据集、挑战细节和结果分析，以及前5名团队的方法介绍


<details>
  <summary>Details</summary>
Motivation: 推动事件相机和灰度相机数据融合的时空实例分割技术发展，为计算机视觉社区提供基准测试平台

Method: 组织挑战赛并提供数据集，收集参与者提交的像素级分割方法，对前5名团队的技术方案进行分析总结

Result: 成功举办了时空实例分割挑战赛，收集了多个团队的解决方案，展示了事件相机与灰度相机数据融合在实例分割任务中的潜力

Conclusion: 该挑战赛为事件视觉领域的时空实例分割技术发展提供了重要推动力，前5名团队的方法为后续研究提供了有价值的参考

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [8] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG是一个两阶段训练框架，通过提升编码器和解码器的泛化能力来缓解增量学习中的灾难性遗忘问题，特别在小内存场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法通常只关注编码器或解码器中的一个组件，限制了缓解灾难性遗忘的效果，特别是在小内存场景下表现更差

Method: 采用两阶段训练：第一阶段通过特征增强训练集成编码器学习泛化表示，提升解码器泛化能力；第二阶段使用知识蒸馏策略压缩集成编码器，开发新的泛化编码器

Result: 在三个基准数据集上的广泛实验显示SEDEG具有优越性能，消融研究确认了各组件有效性

Conclusion: SEDEG通过顺序提升编码器和解码器的泛化能力，有效缓解了增量学习中的灾难性遗忘问题，特别适合小内存场景

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [9] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文是一个关于多模态视觉目标跟踪(MMVOT)的全面调研报告，涵盖了数据采集、模型设计、评估标准等所有方面，包含6个MMVOT任务和338篇参考文献。


<details>
  <summary>Details</summary>
Motivation: 智慧城市产生大量多模态数据，需要综合监控城市基础设施和服务。多模态视觉目标跟踪作为关键任务，需要从多模态角度进行系统分析。

Method: 从四个关键方面对比多模态与单模态跟踪的差异：数据采集、模态对齐和标注、模型设计、评估。对现有方法按处理可见光(RGB)和其他模态(X)的不同方式进行分类。

Result: 揭示了现有MMVOT数据集中对象类别的明显长尾分布特征，以及与RGB数据集相比动物类别显著缺乏的问题。评估了多模态跟踪是否总是更优于单模态跟踪的问题。

Conclusion: 本文提供了多模态视觉目标跟踪领域的全面调研，为该领域的研究者提供了系统的知识基础和研究方向。持续的数据集偏差问题需要得到解决。

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: 本研究评估了TimesFM时间序列基础模型在美国人口统计预测中的表现，相比传统方法在86.67%的测试案例中取得了最低的MSE，特别在历史数据稀疏的少数族裔群体预测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 人口统计变化受全球化、经济条件、地缘政治事件和环境因素影响，对政策制定者和研究者构成重大挑战。准确的人口预测对于城市规划、医疗保健和经济政策等领域的决策至关重要。

Method: 使用美国人口普查局和FRED的数据集，将TimesFM时间序列基础模型与LSTM、ARIMA和线性回归等传统基线方法进行比较，在六个不同人口特征的州进行实验评估。

Result: TimesFM在86.67%的测试案例中实现了最低的均方误差，特别是在历史数据稀疏的少数族裔人口预测方面表现尤为突出。

Conclusion: 预训练的基础模型有潜力增强人口统计分析，为主动政策干预提供信息，且无需大量任务特定的微调。

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [11] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: 本文提出了一个识别多元霍克斯过程中潜在子过程和因果影响的两阶段迭代算法，能够在存在潜在子过程的情况下有效恢复因果结构。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统往往只有部分被观测，存在潜在子过程对现有方法构成重大挑战，现有方法主要关注已观测子过程间的因果结构发现。

Method: 将连续时间事件序列表示为离散时间模型，建立识别潜在子过程和因果影响的充要条件，提出两阶段迭代算法，交替推断已发现子过程间的因果关系和发现新的潜在子过程。

Result: 在合成和真实数据集上的实验表明，该方法在存在潜在子过程的情况下能有效恢复因果结构。

Conclusion: 通过离散时间模型表示和路径条件指导的迭代算法，成功解决了多元霍克斯过程中潜在子过程的识别问题，为复杂系统中的因果发现提供了有效解决方案。

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [12] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: 该研究提出了CFF方法，结合反事实和事实解释来为标记时间点过程模型提供最小且合理的解释子集，提高模型输出的可信度。


<details>
  <summary>Details</summary>
Motivation: 神经网络标记时间点过程模型在高风险应用中广泛使用，但其输出的可信度存在担忧，需要提供可解释性来识别历史事件中的最小解释子集。

Method: 提出CFF方法，结合反事实解释和事实解释，通过精心设计的技术来解决MTPP的解释问题，避免单一解释方式的不合理性。

Result: 实验证明CFF在解释质量和处理效率方面优于基线方法，能够提供正确且优越的解释结果。

Conclusion: CFF方法通过结合反事实和事实解释，有效解决了MTPP模型的可解释性问题，提供了最小且合理的解释子集，提高了模型输出的可信度。

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [13] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET是一个基于160亿次医疗事件训练的医疗基础模型，通过自回归生成医疗事件来模拟患者健康时间线，在78个医疗任务中无需微调就能达到或超越专用监督模型的表现。


<details>
  <summary>Details</summary>
Motivation: 实现规模化个性化医疗需要从纵向患者旅程中提取洞察，大规模医疗事件数据预训练的基础模型有望扩展真实世界证据生成并泛化到多样化下游任务。

Method: 使用Epic Cosmos数据集（163亿次就诊、3亿患者记录），训练了CoMET系列解码器Transformer模型，进行了最大的医疗数据缩放定律研究，预训练了计算最优模型（最多10亿参数），通过自回归生成下一个医疗事件来模拟患者时间线。

Result: 在78个真实世界任务（诊断预测、疾病预后、医疗运营）中，CoMET通常优于或匹配任务特定的监督模型，无需任务特定微调或少样本示例，预测能力随模型规模和预训练规模持续提升。

Conclusion: CoMET作为生成式医疗事件基础模型，能有效捕捉复杂临床动态，为支持临床决策、简化医疗运营和改善患者结局提供了可扩展和可泛化的框架。

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [14] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: 通过注意力集成卷积残差网络(AICRN)模型，实现心电图关键参数的高精度回归预测，提升心电图分析的可解释性和临床应用价值


<details>
  <summary>Details</summary>
Motivation: 解决传统心电图分析中的人为错误、关键特征失焦等问题，通过AI/ML技术提升心脏疾病诊断的精确性和预测能力

Method: 设计了具有空间和通道注意机制的AICRN深度学习架构，采用卷积残差网络解决核心问题，实现心电图关键参数(PR间期、QT间期等)的回归预测

Result: AICRN模型在参数回归任务中表现超过现有模型，具有更高的预测精度，能够快速识别心脏事件并减少手动分析工作量

Conclusion: 深度学习在心电图分析的可解释性和精确性方面发挥关键作用，为心脏监测和管理开启了新的临床应用前景

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [15] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一个基于度量-辛括号形式的多尺度系统粗粒化机器学习框架，能够保持热力学定律、动量守恒和涨落-耗散平衡，用于从粒子轨迹时间序列中学习粗粒化动力学。


<details>
  <summary>Details</summary>
Motivation: 多尺度系统模拟具有挑战性，粗粒化过程中信息熵损失导致出现耗散性、历史依赖性和随机性等涌现物理现象，需要开发能够保持这些特性的机器学习方法。

Method: 使用度量-辛括号形式主义框架，通过自监督学习策略识别涌现结构变量，在粒子离散化中实现，保证热力学第一和第二定律、动量守恒以及离散涨落-耗散平衡。

Result: 在基准系统上验证了方法的有效性，成功应用于星形聚合物的挑战性粗粒化水平，并能够从胶体悬浮液的高速视频中学习耦合局部重排事件与涌现随机动力学的模型。

Conclusion: 该框架为多尺度系统的粗粒化模拟提供了有效的机器学习解决方案，能够保持关键物理特性，并提供了PyTorch和LAMMPS的开源实现，适用于各种粒子系统。

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [16] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero是一种新颖的联邦学习方法，通过超网络动态生成针对非参与客户端的专用模型，解决了数据异构性和分布偏移问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理数据异构性方面取得进展，但无法泛化到具有域内分布偏移和资源约束的非参与客户端

Method: 使用基于分布感知嵌入的超网络动态生成专用模型，采用NoisyEmbed增强的提取器和平衡惩罚来防止特征崩溃，分块生成模型以适应不同数据分布

Result: 在多个数据集和模型上的广泛实验表明，HyperFedZero性能显著优于竞争方法，计算、存储和通信开销最小

Conclusion: 该方法有效解决了联邦学习中非参与客户端的泛化问题，消融研究和可视化验证了各组件的必要性，证明了HyperFedZero的有效性

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [17] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 提出TGN-SVDD方法，结合动态图建模和深度异常检测，在入侵检测任务上优于多个基线方法


<details>
  <summary>Details</summary>
Motivation: 随着全球数字化发展，网络安全重要性日益凸显。机器学习入侵检测面临检测新型未知网络事件、处理时序数据和网络通信图结构等挑战

Method: 基于现代动态图建模和深度异常检测技术，提出TGN-SVDD方法

Result: 在真实入侵检测数据上证明了该方法相对于多个基线的优越性

Conclusion: 提出了一种更具挑战性的入侵检测数据变体，TGN-SVDD方法在网络安全领域具有良好应用前景

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [18] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: 基于海底声学传感器网络和LGCP模型的海上空间异常检测框架，通过二阶概率近似和动态传感器部署提高异常分类和检测精度


<details>
  <summary>Details</summary>
Motivation: 传统方法仅使用强度函数均值进行异常检测，精度有限。需要开发能够同时考虑均值和方差的更精确异常概率估计方法，并优化传感器部署以提高实时检测能力

Method: 使用对数高斯Cox过程(LGCP)对目标到达进行建模，将正常和异常过程混合。提出二阶概率近似方法，同时考虑正常强度函数的均值和方差。结合实时近最优传感器部署策略，动态调整传感器位置

Result: 在弗吉尼亚州诺福克真实船舶交通数据上的验证表明，该方法相比仅使用均值的方法显著提高了分类准确率，并通过优化的传感器部署增强了异常检测能力

Conclusion: 提出的二阶概率近似框架为海上空间异常检测提供了更精确的解决方案，动态传感器部署策略进一步提升了实时检测性能，在真实海洋环境中表现出色

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: 利用因果模型和变分自动编码器来分析分子动力学模拟中氢键形成和解离的根本原因


<details>
  <summary>Details</summary>
Motivation: 解决分子动力学模拟中资源涉及计算重、需手动扫描输出以发现关键事件的挑战，并深入理解氢键形成和解离的根本原因

Method: 采用因果模型技术，将氢键解离视为"干预"事件，构建图形因果模型。使用变分自动编码器等机器学习模型来推断样本间的因果关系，并包含聚合分布变化的根因推断步骤

Result: 在手性分离的原子轨迹数据上验证了模型有效性，能够预测多步未来变化，并发现驱动系统变化的关键变量

Conclusion: 该框架为分子动态系统中的根因分析提供了新视角，通过建模捕捉分子相互作用在键合形成或解离过程中条件分布的变化

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: CorrSteer提出了一种基于相关性的稀疏自编码器特征选择方法，仅使用推理时激活来提取相关特征，避免了虚假相关性，在多个任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 解决稀疏自编码器在下游控制任务中需要对比数据集或大量激活存储的限制，实现更有效的特征选择和自动化控制

Method: 通过将样本正确性与推理时生成的token的SAE激活进行相关性分析来选择特征，使用平均激活获取控制系数，实现全自动化流程

Result: 在Gemma 2 2B和LLaMA 3.1 8B上，QA、偏见缓解、越狱预防和推理基准任务性能显著提升，MMLU性能提高4.1%，HarmBench提升22.9%（仅需4000样本）

Conclusion: 基于相关性的特征选择是自动化SAE控制的有效且可扩展方法，所选特征展现出与任务需求一致的语义模式，揭示了驱动性能的底层能力

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [21] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 基于LLM内部表征的统计检测方法RepreGuard，在内部和外部分布场景下都能高效识别AI生成文本，平均AUROC达94.92%


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在外部分布场景下稳健性不足，LLM内部表征包含更全面的统计特征能更有效区分AI生成文本和人类文本

Method: 使用代理模型收集LGT和HWT的表征，提取特异激活特征，通过计算文本表征在该特征方向上的投影得分并与预计算阈值比较来分类

Result: 在内部和外部分布场景下都超过所有基线方法，平均AUROC达94.92%，对不同文本长度和主流攻击都体现出稳健耐受性

Conclusion: LLM内部表征确实包含更有效的检测特征，RepreGuard方法通过统计投影实现了高效、稳健的AI生成文本检测

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [22] [Into the Wild: When Robots Are Not Welcome](https://arxiv.org/abs/2508.12075)
*Shaul Ashkenazi,Gabriel Skantze,Jane Stuart-Smith,Mary Ellen Foster*

Main category: cs.RO

TL;DR: 社交机器人在公共空间部署遇到的困难和反对声音，但最终通过建立信任关系成功完成部署


<details>
  <summary>Details</summary>
Motivation: 研究社交机器人在公共空间部署时面临的技术困难、意外用户语言以及利益相关者反对的挑战

Method: 在两个不同公共场景中部署社交机器人：1）学生服务中心；2）难民和寻求宵护者临时服务点

Result: 虽然遇到失败和困难，但最终成功获得员工信任并建立良好关系，完成了机器人部署和研究工作

Conclusion: 通过建立信任和良好关系，可以克服社交机器人在公共空间部署时遇到的各种挑战，实现成功部署

Abstract: Social robots are increasingly being deployed in public spaces, where they
face not only technological difficulties and unexpected user utterances, but
also objections from stakeholders who may not be comfortable with introducing a
robot into those spaces. We describe our difficulties with deploying a social
robot in two different public settings: 1) Student services center; 2) Refugees
and asylum seekers drop-in service. Although this is a failure report, in each
use case we eventually managed to earn the trust of the staff and form a
relationship with them, allowing us to deploy our robot and conduct our
studies.

</details>


### [23] [Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments](https://arxiv.org/abs/2508.12274)
*Jian Zhao,Yunlong Lian,Andy M Tyrrell,Michael Gienger,Jihong Zhu*

Main category: cs.RO

TL;DR: 提出一种适用于空间坐标系的双手穿衣策略，通过GMM/GMR模拟学习来适应不同人体手臂姿势，解决紧身服装穿戴难题


<details>
  <summary>Details</summary>
Motivation: 现有机器人辅助穿衣研究多关注松贴服装，对紧身服装关注少，单手机器人穿戴紧身服装容易失败

Method: 建立球坐标系编码穿衣轨迹，使用方位角作为双手操作的任务特征，采用GMM和GMR进行模仿学习

Result: 通过多种实验验证了所提方法的有效性

Conclusion: 该双手穿衣策略能够有效解决紧身服装穿戴难题，适应不同人体手臂姿势

Abstract: Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.

</details>


### [24] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: 事件相机外部标定方法，通过运动估计和时间序列分析，无需专门标定物，实现事件相机与其他传感器的时间偏移和旋转外参的高精度标定。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级延迟优势，但在多传感器融合系统中的外部标定问题研究较少。传统方法依赖事件到帧转换和专门标定物，存在限制。

Method: 使用法向流观测估角速度，采用两步流程：先通过兰联分析初始化时间偏移和旋转外参，然后通过连续时间SO(3)参数化进行非线性优化精细调整。

Result: 在公开和自收数据集上验证，该方法达到了与基于标定物方法相当的标定精度，且比纯粹CCA方法更稳定，显示了高精度、稳健性和灵活性。

Conclusion: 该方法为事件相机多传感器系统提供了一种无需专门标定物的高效外部标定方案，具有强大的实用价值和推广潜力，并将开源代码以促进相关研究。

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>
