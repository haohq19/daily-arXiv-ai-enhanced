{"id": "2510.01353", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01353", "abs": "https://arxiv.org/abs/2510.01353", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Anand Kannappan", "Rebecca Qian", "Peng Wang"], "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments", "comment": "Accepted to NeurIPS 2025 SEA Workshop", "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings"}
{"id": "2510.01409", "categories": ["cs.AI", "I.2.7; I.2.6; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.01409", "abs": "https://arxiv.org/abs/2510.01409", "authors": ["Luca Cotti", "Idilio Drago", "Anisa Rula", "Devis Bianchini", "Federico Cerutti"], "title": "OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models", "comment": "20 pages, 6 tables, 7 figures", "summary": "System logs represent a valuable source of Cyber Threat Intelligence (CTI),\ncapturing attacker behaviors, exploited vulnerabilities, and traces of\nmalicious activity. Yet their utility is often limited by lack of structure,\nsemantic inconsistency, and fragmentation across devices and sessions.\nExtracting actionable CTI from logs therefore requires approaches that can\nreconcile noisy, heterogeneous data into coherent and interoperable\nrepresentations. We introduce OntoLogX, an autonomous Artificial Intelligence\n(AI) agent that leverages Large Language Models (LLMs) to transform raw logs\ninto ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a\nlightweight log ontology with Retrieval Augmented Generation (RAG) and\niterative correction steps, ensuring that generated KGs are syntactically and\nsemantically valid. Beyond event-level analysis, the system aggregates KGs into\nsessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level\nlog evidence to higher-level adversarial objectives. We evaluate OntoLogX on\nboth logs from a public benchmark and a real-world honeypot dataset,\ndemonstrating robust KG generation across multiple KGs backends and accurate\nmapping of adversarial activity to ATT&CK tactics. Results highlight the\nbenefits of retrieval and correction for precision and recall, the\neffectiveness of code-oriented models in structured log analysis, and the value\nof ontology-grounded representations for actionable CTI extraction."}
{"id": "2510.01237", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01237", "abs": "https://arxiv.org/abs/2510.01237", "authors": ["Nandakishor M"], "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation", "comment": null, "summary": "Large Language Models suffer from hallucination, generating plausible yet\nfactually incorrect content. Current mitigation strategies focus on\npost-generation correction, which is computationally expensive and fails to\nprevent unreliable content generation. We propose a confidence-aware routing\nsystem that proactively assesses model uncertainty before generation and\nredirects queries based on estimated reliability. Our approach combines three\ncomplementary signals: semantic alignment between internal representations and\nreference embeddings, internal convergence analysis across model layers, and\nlearned confidence estimation. The unified confidence score determines routing\nto four pathways: local generation for high confidence, retrieval-augmented\ngeneration for medium confidence, larger models for low confidence, and human\nreview for very low confidence. Evaluation on knowledge-intensive QA benchmarks\ndemonstrates significant improvements in hallucination detection (0.74 vs. 0.42\nbaseline) while reducing computational costs by 40% compared to post-hoc\nmethods. The F1 score improves from 0.61 to 0.82 with low false positive rates\n(0.09). This paradigm shift from reactive correction to proactive assessment\noffers a computationally efficient approach to LLM reliability enhancement."}
{"id": "2510.01661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01661", "abs": "https://arxiv.org/abs/2510.01661", "authors": ["Yifei Simon Shao", "Yuchen Zheng", "Sunan Sun", "Pratik Chaudhari", "Vijay Kumar", "Nadia Figueroa"], "title": "Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation", "comment": "CoRL 2025 Learning Effective Abstractions for Planning (LEAP)\n  Workshop Best Paper Award (https://sites.google.com/view/symskill)", "summary": "Multi-step manipulation in dynamic environments remains challenging. Two\nmajor families of methods fail in distinct ways: (i) imitation learning (IL) is\nreactive but lacks compositional generalization, as monolithic policies do not\ndecide which skill to reuse when scenes change; (ii) classical task-and-motion\nplanning (TAMP) offers compositionality but has prohibitive planning latency,\npreventing real-time failure recovery. We introduce SymSkill, a unified\nlearning framework that combines the benefits of IL and TAMP, allowing\ncompositional generalization and failure recovery in real-time. Offline,\nSymSkill jointly learns predicates, operators, and skills directly from\nunlabeled and unsegmented demonstrations. At execution time, upon specifying a\nconjunction of one or more learned predicates, SymSkill uses a symbolic planner\nto compose and reorder learned skills to achieve the symbolic goals, while\nperforming recovery at both the motion and symbolic levels in real time.\nCoupled with a compliant controller, SymSkill enables safe and uninterrupted\nexecution under human and environmental disturbances. In RoboCasa simulation,\nSymSkill can execute 12 single-step tasks with 85% success rate. Without\nadditional data, it composes these skills into multi-step plans requiring up to\n6 skill recompositions, recovering robustly from execution failures. On a real\nFranka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented\nand unlabeled play data, is capable of performing multiple tasks simply by goal\nspecifications. The source code and additional analysis can be found on\nhttps://sites.google.com/view/symskill."}
{"id": "2510.01528", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01528", "abs": "https://arxiv.org/abs/2510.01528", "authors": ["Daniel Zhao", "Abhilash Shankarampeta", "Lanxiang Hu", "Tajana Rosing", "Hao Zhang"], "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation", "comment": null, "summary": "We propose a novel method that leverages sparse autoencoders (SAEs) and\nclustering techniques to analyze the internal token representations of large\nlanguage models (LLMs) and guide generations in mathematical reasoning tasks.\nOur approach first trains an SAE to generate sparse vector representations for\ntraining tokens, then applies k-means clustering to construct a graph where\nvertices represent token clusters and weighted edges capture sequential token\ntransitions. Using this graph, we define an edge-weight based reward function\nto quantify adherence to established reasoning traces, thereby identifying\nexploitative reasoning trajectories. Additionally, we measure generation\ndiversity from clustering to assess the extent of exploration. Our findings\nindicate that balancing both exploitation and exploration is crucial for\nachieving high accuracy in mathematical reasoning tasks. During generation, the\nSAE can serve as a scalable reward model to guide generations, ensuring a\nbalanced trade-off between exploitation and exploration. This prevents extreme\nbehaviors in either direction, ultimately fostering a higher-quality reasoning\nprocess in LLMs."}
{"id": "2510.01345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01345", "abs": "https://arxiv.org/abs/2510.01345", "authors": ["Akhlaqur Rahman Sabby", "Yi Sui", "Tongzi Wu", "Jesse C. Cresswell", "Ga Wu"], "title": "Self-Supervised Representation Learning as Mutual Information Maximization", "comment": null, "summary": "Self-supervised representation learning (SSRL) has demonstrated remarkable\nempirical success, yet its underlying principles remain insufficiently\nunderstood. While recent works attempt to unify SSRL methods by examining their\ninformation-theoretic objectives or summarizing their heuristics for preventing\nrepresentation collapse, architectural elements like the predictor network,\nstop-gradient operation, and statistical regularizer are often viewed as\nempirically motivated additions. In this paper, we adopt a first-principles\napproach and investigate whether the learning objective of an SSRL algorithm\ndictates its possible optimization strategies and model design choices. In\nparticular, by starting from a variational mutual information (MI) lower bound,\nwe derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint\nMI (JMI), each imposing distinct structural constraints and covering a set of\nexisting SSRL algorithms. SDMI inherently requires alternating optimization,\nmaking stop-gradient operations theoretically essential. In contrast, JMI\nadmits joint optimization through symmetric architectures without such\ncomponents. Under the proposed formulation, predictor networks in SDMI and\nstatistical regularizers in JMI emerge as tractable surrogates for the MI\nobjective. We show that many existing SSRL methods are specific instances or\napproximations of these two paradigms. This paper provides a theoretical\nexplanation behind the choices of different architectural components of\nexisting SSRL methods, beyond heuristic conveniences."}
{"id": "2510.01349", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01349", "abs": "https://arxiv.org/abs/2510.01349", "authors": ["Hannah Lawrence", "Elyssa Hofgard", "Vasco Portilheiro", "Yuxuan Chen", "Tess Smidt", "Robin Walters"], "title": "To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking", "comment": "A short version of this paper appeared at the ICLR AI4Mat workshop in\n  April 2025", "summary": "Symmetry-aware methods for machine learning, such as data augmentation and\nequivariant architectures, encourage correct model behavior on all\ntransformations (e.g. rotations or permutations) of the original dataset. These\nmethods can improve generalization and sample efficiency, under the assumption\nthat the transformed datapoints are highly probable, or \"important\", under the\ntest distribution. In this work, we develop a method for critically evaluating\nthis assumption. In particular, we propose a metric to quantify the amount of\nanisotropy, or symmetry-breaking, in a dataset, via a two-sample neural\nclassifier test that distinguishes between the original dataset and its\nrandomly augmented equivalent. We validate our metric on synthetic datasets,\nand then use it to uncover surprisingly high degrees of alignment in several\nbenchmark point cloud datasets. We show theoretically that distributional\nsymmetry-breaking can actually prevent invariant methods from performing\noptimally even when the underlying labels are truly invariant, as we show for\ninvariant ridge regression in the infinite feature limit. Empirically, we find\nthat the implication for symmetry-aware methods is dataset-dependent:\nequivariant methods still impart benefits on some anisotropic datasets, but not\nothers. Overall, these findings suggest that understanding equivariance -- both\nwhen it works, and why -- may require rethinking symmetry biases in the data."}
{"id": "2510.01245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01245", "abs": "https://arxiv.org/abs/2510.01245", "authors": ["Runfei Chen", "Shuyang Jiang", "Wei Huang"], "title": "SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction", "comment": "EMNLP2025", "summary": "Human mobility prediction is vital for urban services, but often fails to\naccount for abrupt changes from external events. Existing spatiotemporal models\nstruggle to leverage textual descriptions detailing these events. We propose\nSeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility\nprediction. Specifically, SeMob employs a multi-agent framework where LLM-based\nagents automatically extract and reason about spatiotemporally related text\nfrom complex online texts. Fine-grained relevant contexts are then incorporated\nwith spatiotemporal data through our proposed innovative progressive fusion\narchitecture. The rich pre-trained event prior contributes enriched insights\nabout event-driven prediction, and hence results in a more aligned forecasting\nmodel. Evaluated on a dataset constructed through our pipeline, SeMob achieves\nmaximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the\nspatiotemporal model. Notably, the framework exhibits pronounced superiority\nespecially within spatiotemporal regions close to an event's location and time\nof occurrence."}
{"id": "2510.01660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01660", "abs": "https://arxiv.org/abs/2510.01660", "authors": ["Duy Nguyen", "Dat Nguyen"], "title": "VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming", "comment": null, "summary": "Existing UDA pipelines fine-tune already well-trained backbone parameters for\nevery new source-and-target pair, resulting in the number of training\nparameters and storage memory growing linearly with each new pair, and also\npreventing the reuse of these well-trained backbone parameters.\n  Inspired by recent implications that existing backbones have textural biases,\nwe propose making use of domain-specific textural bias for domain adaptation\nvia visual reprogramming, namely VirDA.Instead of fine-tuning the full\nbackbone, VirDA prepends a domain-specific visual reprogramming layer to the\nbackbone. This layer produces visual prompts that act as an added textural bias\nto the input image, adapting its ``style'' to a target domain. To optimize\nthese visual reprogramming layers, we use multiple objective functions that\noptimize the intra- and inter-domain distribution differences when\ndomain-adapting visual prompts are applied. This process does not require\nmodifying the backbone parameters, allowing the same backbone to be reused\nacross different domains.\n  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M\ntrainable parameters. VirDA surpasses PDA, the state-of-the-art\nparameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its\nparameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans\nand FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%\nof their trainable parameters. Relative to the strongest current methods\n(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only\n2.2% and 1.1% accuracy, respectively."}
{"id": "2510.01439", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01439", "abs": "https://arxiv.org/abs/2510.01439", "authors": ["Mohamad Abou Ali", "Fadi Dornaika"], "title": "Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons", "comment": null, "summary": "Edge Artificial Intelligence (Edge AI) embeds intelligence directly into\ndevices at the network edge, enabling real-time processing with improved\nprivacy and reduced latency by processing data close to its source. This review\nsystematically examines the evolution, current landscape, and future directions\nof Edge AI through a multi-dimensional taxonomy including deployment location,\nprocessing capabilities such as TinyML and federated learning, application\ndomains, and hardware types. Following PRISMA guidelines, the analysis traces\nthe field from early content delivery networks and fog computing to modern\non-device intelligence. Core enabling technologies such as specialized hardware\naccelerators, optimized software, and communication protocols are explored.\nChallenges including resource limitations, security, model management, power\nconsumption, and connectivity are critically assessed. Emerging opportunities\nin neuromorphic hardware, continual learning algorithms, edge-cloud\ncollaboration, and trustworthiness integration are highlighted, providing a\ncomprehensive framework for researchers and practitioners."}
{"id": "2510.01857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01857", "abs": "https://arxiv.org/abs/2510.01857", "authors": ["Claudio Fanconi", "Nicolás Astorga", "Mihaela van der Schaar"], "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning", "comment": null, "summary": "We reframe and operationalise adversarial inverse reinforcement learning\n(IRL) to large language model reasoning, learning a dense, token-level reward\nmodel for process supervision directly from expert demonstrations rather than\nimitating style via supervised fine-tuning. The learned reasoning reward serves\ntwo complementary roles: (i) it provides step-level feedback to optimise a\nreasoning policy during training; and (ii) it functions at inference as a\ncritic to rerank sampled traces under fixed compute budgets. We demonstrate\nthat our approach prioritises correctness over surface form, yielding scores\nthat correlate with eventual answer validity and enabling interpretable\nlocalisation of errors within a trace. Empirically, on GSM8K with Llama3 and\nQwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a\nlearning signal to elicit reasoning, and (ii) predictive performance is\nimproved from reward-guided reranking (notably for Llama-based policies). By\nunifying training signals, inference-time selection, and token-level\ndiagnostics into a single reasoning reward, this work suggests reusable\nprocess-level rewards with broad potential to enhance multi-step reasoning in\nlanguage models."}
{"id": "2510.01270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01270", "abs": "https://arxiv.org/abs/2510.01270", "authors": ["Hoang Phan", "Victor Li", "Qi Lei"], "title": "Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) have revolutionized natural language processing\nwith their ability to generate coherent and contextually relevant text.\nHowever, their deployment raises significant concerns about the potential for\ngenerating harmful or inappropriate content. In this paper, we introduce\nProgressive Self-Reflection (PSR), a novel inference-time technique that\nempowers LLMs to self-monitor and correct their outputs dynamically.\nExperimental results demonstrate that applying our proposed method to\nLlama-3.1-8B-Instruct reduces the attack success rate from 77.5\\% to 5.9\\%, to\nLlama-3.1-8B base from 89.7\\% to 5.6\\%, and to Qwen2.5-7B-Instruct from 44.4\\%\nto 3.8\\%, without additional training, while maintaining their original\nperformance on benign tasks. Our approach acts as a test-time scaling method,\nwhere additional self-reflection rounds enhance safety at the cost of inference\noverhead. To balance safety with computational efficiency, we introduce a\nlightweight self-reflection predictor that estimates the optimal number of\nreflection rounds based on input complexity. This adaptive mechanism prevents\nunnecessary self-assessment on benign inputs while ensuring thorough evaluation\nwhen encountering potentially harmful content. Our findings suggest that\nProgressive Self-Reflection serves as a scalable test-time approach, enhancing\nLLM safety by dynamically allocating computational resources in proportion to\nthe input's risk profile."}
{"id": "2510.01784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01784", "abs": "https://arxiv.org/abs/2510.01784", "authors": ["Xiaofei Wu", "Guozhen Zhang", "Zhiyong Xu", "Yuan Zhou", "Qinglin Lu", "Xuming He"], "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation", "comment": null, "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models."}
{"id": "2510.02060", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02060", "abs": "https://arxiv.org/abs/2510.02060", "authors": ["Sanghyu Yoon", "Dongmin Kim", "Suhee Yoon", "Ye Seul Sim", "Seungdong Yoa", "Hye-Seung Cho", "Soonyoung Lee", "Hankook Lee", "Woohyung Lim"], "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection", "comment": "9 pages, 4 figures", "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD."}
{"id": "2510.01510", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01510", "abs": "https://arxiv.org/abs/2510.01510", "authors": ["Jinwoo Kim", "Xingyue Huang", "Krzysztof Olejniczak", "Kyungbin Min", "Michael Bronstein", "Seunghoon Hong", "İsmail İlkan Ceylan"], "title": "Flock: A Knowledge Graph Foundation Model via Learning on Random Walks", "comment": null, "summary": "We study the problem of zero-shot link prediction on knowledge graphs (KGs),\nwhich requires models to generalize over novel entities and novel relations.\nKnowledge graph foundation models (KGFMs) address this task by enforcing\nequivariance over both nodes and relations, learning from structural properties\nof nodes and relations, which are then transferable to novel graphs with\nsimilar structural properties. However, the conventional notion of\ndeterministic equivariance imposes inherent limits on the expressive power of\nKGFMs, preventing them from distinguishing structurally similar but\nsemantically distinct relations. To overcome this limitation, we introduce\nprobabilistic node-relation equivariance, which preserves equivariance in\ndistribution while incorporating a principled randomization to break symmetries\nduring inference. Building on this principle, we present Flock, a KGFM that\niteratively samples random walks, encodes them into sequences via a recording\nprotocol, embeds them with a sequence model, and aggregates representations of\nnodes and relations via learned pooling. Crucially, Flock respects\nprobabilistic node-relation equivariance and is a universal approximator for\nisomorphism-invariant link-level functions over KGs. Empirically, Flock\nperfectly solves our new diagnostic dataset Petals where current KGFMs fail,\nand achieves state-of-the-art performances on entity- and relation prediction\ntasks on 54 KGs from diverse domains."}
{"id": "2510.01391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01391", "abs": "https://arxiv.org/abs/2510.01391", "authors": ["Maithili Kadam", "Francis Ferraro"], "title": "TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies", "comment": "Accepted in *sem 2025", "summary": "Large language models (LLMs) excel at general language tasks but often\nstruggle with event-based questions-especially those requiring causal or\ntemporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question\nAnswering), a prompting framework that injects causal event graphs into LLM\ninputs by converting structured relations into natural-language statements.\nTAG-EQA spans nine prompting configurations, combining three strategies\n(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,\ngraph-only, text+graph), enabling a systematic analysis of when and how\nstructured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA\nimproves accuracy by 5% on average over text-only baselines, with gains up to\n12% in zero-shot settings and 18% when graph-augmented CoT prompting is\neffective. While performance varies by model and configuration, our findings\nshow that causal graphs can enhance event reasoning in LLMs without\nfine-tuning, offering a flexible way to encode structure in prompt-based QA."}
{"id": "2510.01520", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01520", "abs": "https://arxiv.org/abs/2510.01520", "authors": ["Hossein Sholehrasa", "Xuan Xu", "Doina Caragea", "Jim E. Riviere", "Majid Jaberi-Douraki"], "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties", "comment": null, "summary": "The safe use of pharmaceuticals in food-producing animals is vital to protect\nanimal welfare and human food safety. Adverse events (AEs) may signal\nunexpected pharmacokinetic or toxicokinetic effects, increasing the risk of\nviolative residues in the food chain. This study introduces a predictive\nframework for classifying outcomes (Death vs. Recovery) using ~1.28 million\nreports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary\nMedicine. A preprocessing pipeline merged relational tables and standardized\nAEs through VeDDRA ontologies. Data were normalized, missing values imputed,\nand high-cardinality features reduced; physicochemical drug properties were\nintegrated to capture chemical-residue links. We evaluated supervised models,\nincluding Random Forest, CatBoost, XGBoost, ExcelFormer, and large language\nmodels (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as\nundersampling and oversampling, with a focus on prioritizing recall for fatal\noutcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,\nachieving precision, recall, and F1-scores of 0.95. Incorporating Average\nUncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved\nminority-class detection, particularly in ExcelFormer and XGBoost.\nInterpretability via SHAP identified biologically plausible predictors,\nincluding lung, heart, and bronchial disorders, animal demographics, and drug\nphysicochemical properties. These features were strongly linked to fatal\noutcomes. Overall, the framework shows that combining rigorous data\nengineering, advanced machine learning, and explainable AI enables accurate,\ninterpretable predictions of veterinary safety outcomes. The approach supports\nFARAD's mission by enabling early detection of high-risk drug-event profiles,\nstrengthening residue risk assessment, and informing regulatory and clinical\ndecision-making."}
{"id": "2510.01954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01954", "abs": "https://arxiv.org/abs/2510.01954", "authors": ["Yongyi Su", "Haojie Zhang", "Shijie Li", "Nanqing Liu", "Jingyi Liao", "Junyi Pan", "Yuan Liu", "Xiaofen Xing", "Chong Sun", "Chen Li", "Nancy F. Chen", "Shuicheng Yan", "Xulei Yang", "Xun Xu"], "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs", "comment": "24 pages, 12 figures and 9 tables", "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT."}
{"id": "2510.01529", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01529", "abs": "https://arxiv.org/abs/2510.01529", "authors": ["Jaiden Fairoze", "Sanjam Garg", "Keewoo Lee", "Mingyuan Wang"], "title": "Bypassing Prompt Guards in Production with Controlled-Release Prompting", "comment": null, "summary": "As large language models (LLMs) advance, ensuring AI safety and alignment is\nparamount. One popular approach is prompt guards, lightweight mechanisms\ndesigned to filter malicious queries while being easy to implement and update.\nIn this work, we introduce a new attack that circumvents such prompt guards,\nhighlighting their limitations. Our method consistently jailbreaks production\nmodels while maintaining response quality, even under the highly protected chat\ninterfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok\n(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry\nbetween the prompt guard and the main LLM, encoding a jailbreak prompt that\nlightweight guards cannot decode but the main model can. This reveals an attack\nsurface inherent to lightweight prompt guards in modern LLM architectures and\nunderscores the need to shift defenses from blocking malicious inputs to\npreventing malicious outputs. We additionally identify other critical alignment\nissues, such as copyrighted data extraction, training data extraction, and\nmalicious response leakage during thinking."}
{"id": "2510.01549", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01549", "abs": "https://arxiv.org/abs/2510.01549", "authors": ["Kevin Zhai", "Utsav Singh", "Anirudh Thatipelli", "Souradip Chakraborty", "Anit Kumar Sahu", "Furong Huang", "Amrit Singh Bedi", "Mubarak Shah"], "title": "MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models", "comment": null, "summary": "Diffusion models excel at generating images conditioned on text prompts, but\nthe resulting images often do not satisfy user-specific criteria measured by\nscalar rewards such as Aesthetic Scores. This alignment typically requires\nfine-tuning, which is computationally demanding. Recently, inference-time\nalignment via noise optimization has emerged as an efficient alternative,\nmodifying initial input noise to steer the diffusion denoising process towards\ngenerating high-reward images. However, this approach suffers from reward\nhacking, where the model produces images that score highly, yet deviate\nsignificantly from the original prompt. We show that noise-space regularization\nis insufficient and that preventing reward hacking requires an explicit\nimage-space constraint. To this end, we propose MIRA (MItigating Reward\nhAcking), a training-free, inference-time alignment method. MIRA introduces an\nimage-space, score-based KL surrogate that regularizes the sampling trajectory\nwith a frozen backbone, constraining the output distribution so reward can\nincrease without off-distribution drift (reward hacking). We derive a tractable\napproximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple\nrewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,\nAnimal-Animal, HPDv2), MIRA achieves >60\\% win rate vs. strong baselines while\npreserving prompt adherence; mechanism plots show reward gains with near-zero\ndrift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,\nmapping preference optimization to inference time with a frozen backbone,\nextending MIRA to non-differentiable rewards without fine-tuning."}
{"id": "2510.01555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01555", "abs": "https://arxiv.org/abs/2510.01555", "authors": ["Kezhao Liu", "Jason Klein Liu", "Mingtao Chen", "Yiming Liu"], "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a\nKullback-Leibler (KL) divergence loss to stabilize training and prevent\noverfitting. However, in methods such as GRPO, its implementation may be guided\nby principles from numerical value estimation-a practice that overlooks the\nterm's functional role as an optimization loss. To analyze this issue, we\nestablish a unified framework that connects two seemingly distinct\nimplementation styles: using the mathematical term $k_n$ as a detached\ncoefficient for the policy's score function ('$k_n$ in reward') or as a direct\nloss function through which gradients are propagated ('$k_n$ as loss'). We show\nthat the latter can always be analyzed via an equivalent gradient coefficient\nin the former, unifying the two perspectives. Through this framework, we prove\nthat the conventional '$k_1$ in reward' (like in PPO) is the principled loss\nfor Reverse KL (RKL) regularization. We further establish a key finding: under\non-policy conditions, the '$k_2$ as loss' formulation is, in fact,\ngradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our\nwork, identifies both as the theoretically sound implementations of the RKL\nobjective. In contrast, we show that the recently adopted '$k_3$ as loss' (like\nin GRPO) is merely a first-order, biased approximation of the principled loss.\nFurthermore, we argue that common off-policy implementations of '$k_n$ as loss'\nmethods are biased due to neglected importance sampling, and we propose a\nprincipled correction. Our findings provide a comprehensive, gradient-based\nrationale for choosing and correctly implementing KL regularization, paving the\nway for more robust and effective RLHF systems."}
{"id": "2510.01237", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01237", "abs": "https://arxiv.org/abs/2510.01237", "authors": ["Nandakishor M"], "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation", "comment": null, "summary": "Large Language Models suffer from hallucination, generating plausible yet\nfactually incorrect content. Current mitigation strategies focus on\npost-generation correction, which is computationally expensive and fails to\nprevent unreliable content generation. We propose a confidence-aware routing\nsystem that proactively assesses model uncertainty before generation and\nredirects queries based on estimated reliability. Our approach combines three\ncomplementary signals: semantic alignment between internal representations and\nreference embeddings, internal convergence analysis across model layers, and\nlearned confidence estimation. The unified confidence score determines routing\nto four pathways: local generation for high confidence, retrieval-augmented\ngeneration for medium confidence, larger models for low confidence, and human\nreview for very low confidence. Evaluation on knowledge-intensive QA benchmarks\ndemonstrates significant improvements in hallucination detection (0.74 vs. 0.42\nbaseline) while reducing computational costs by 40% compared to post-hoc\nmethods. The F1 score improves from 0.61 to 0.82 with low false positive rates\n(0.09). This paradigm shift from reactive correction to proactive assessment\noffers a computationally efficient approach to LLM reliability enhancement."}
{"id": "2510.02262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02262", "abs": "https://arxiv.org/abs/2510.02262", "authors": ["Guangyu Sun", "Archit Singhal", "Burak Uzkent", "Mubarak Shah", "Chen Chen", "Garin Kessler"], "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding", "comment": null, "summary": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c ."}
{"id": "2510.01270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01270", "abs": "https://arxiv.org/abs/2510.01270", "authors": ["Hoang Phan", "Victor Li", "Qi Lei"], "title": "Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) have revolutionized natural language processing\nwith their ability to generate coherent and contextually relevant text.\nHowever, their deployment raises significant concerns about the potential for\ngenerating harmful or inappropriate content. In this paper, we introduce\nProgressive Self-Reflection (PSR), a novel inference-time technique that\nempowers LLMs to self-monitor and correct their outputs dynamically.\nExperimental results demonstrate that applying our proposed method to\nLlama-3.1-8B-Instruct reduces the attack success rate from 77.5\\% to 5.9\\%, to\nLlama-3.1-8B base from 89.7\\% to 5.6\\%, and to Qwen2.5-7B-Instruct from 44.4\\%\nto 3.8\\%, without additional training, while maintaining their original\nperformance on benign tasks. Our approach acts as a test-time scaling method,\nwhere additional self-reflection rounds enhance safety at the cost of inference\noverhead. To balance safety with computational efficiency, we introduce a\nlightweight self-reflection predictor that estimates the optimal number of\nreflection rounds based on input complexity. This adaptive mechanism prevents\nunnecessary self-assessment on benign inputs while ensuring thorough evaluation\nwhen encountering potentially harmful content. Our findings suggest that\nProgressive Self-Reflection serves as a scalable test-time approach, enhancing\nLLM safety by dynamically allocating computational resources in proportion to\nthe input's risk profile."}
{"id": "2510.02025", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02025", "abs": "https://arxiv.org/abs/2510.02025", "authors": ["Donghoon Jung", "Jiwoo Choi", "Songeun Chae", "Seohyon Jung"], "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models", "comment": null, "summary": "Evaluations of large language models (LLMs)' creativity have focused\nprimarily on the quality of their outputs rather than the processes that shape\nthem. This study takes a process-oriented approach, drawing on narratology to\nexamine LLMs as computational authors. We introduce constraint-based\ndecision-making as a lens for authorial creativity. Using controlled prompting\nto assign authorial personas, we analyze the creative preferences of the\nmodels. Our findings show that LLMs consistently emphasize Style over other\nelements, including Character, Event, and Setting. By also probing the\nreasoning the models provide for their choices, we show that distinctive\nprofiles emerge across models and argue that our approach provides a novel\nsystematic tool for analyzing AI's authorial creativity."}
{"id": "2510.01520", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01520", "abs": "https://arxiv.org/abs/2510.01520", "authors": ["Hossein Sholehrasa", "Xuan Xu", "Doina Caragea", "Jim E. Riviere", "Majid Jaberi-Douraki"], "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties", "comment": null, "summary": "The safe use of pharmaceuticals in food-producing animals is vital to protect\nanimal welfare and human food safety. Adverse events (AEs) may signal\nunexpected pharmacokinetic or toxicokinetic effects, increasing the risk of\nviolative residues in the food chain. This study introduces a predictive\nframework for classifying outcomes (Death vs. Recovery) using ~1.28 million\nreports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary\nMedicine. A preprocessing pipeline merged relational tables and standardized\nAEs through VeDDRA ontologies. Data were normalized, missing values imputed,\nand high-cardinality features reduced; physicochemical drug properties were\nintegrated to capture chemical-residue links. We evaluated supervised models,\nincluding Random Forest, CatBoost, XGBoost, ExcelFormer, and large language\nmodels (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as\nundersampling and oversampling, with a focus on prioritizing recall for fatal\noutcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,\nachieving precision, recall, and F1-scores of 0.95. Incorporating Average\nUncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved\nminority-class detection, particularly in ExcelFormer and XGBoost.\nInterpretability via SHAP identified biologically plausible predictors,\nincluding lung, heart, and bronchial disorders, animal demographics, and drug\nphysicochemical properties. These features were strongly linked to fatal\noutcomes. Overall, the framework shows that combining rigorous data\nengineering, advanced machine learning, and explainable AI enables accurate,\ninterpretable predictions of veterinary safety outcomes. The approach supports\nFARAD's mission by enabling early detection of high-risk drug-event profiles,\nstrengthening residue risk assessment, and informing regulatory and clinical\ndecision-making."}
{"id": "2510.01353", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01353", "abs": "https://arxiv.org/abs/2510.01353", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Anand Kannappan", "Rebecca Qian", "Peng Wang"], "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments", "comment": "Accepted to NeurIPS 2025 SEA Workshop", "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings"}
{"id": "2510.01555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01555", "abs": "https://arxiv.org/abs/2510.01555", "authors": ["Kezhao Liu", "Jason Klein Liu", "Mingtao Chen", "Yiming Liu"], "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a\nKullback-Leibler (KL) divergence loss to stabilize training and prevent\noverfitting. However, in methods such as GRPO, its implementation may be guided\nby principles from numerical value estimation-a practice that overlooks the\nterm's functional role as an optimization loss. To analyze this issue, we\nestablish a unified framework that connects two seemingly distinct\nimplementation styles: using the mathematical term $k_n$ as a detached\ncoefficient for the policy's score function ('$k_n$ in reward') or as a direct\nloss function through which gradients are propagated ('$k_n$ as loss'). We show\nthat the latter can always be analyzed via an equivalent gradient coefficient\nin the former, unifying the two perspectives. Through this framework, we prove\nthat the conventional '$k_1$ in reward' (like in PPO) is the principled loss\nfor Reverse KL (RKL) regularization. We further establish a key finding: under\non-policy conditions, the '$k_2$ as loss' formulation is, in fact,\ngradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our\nwork, identifies both as the theoretically sound implementations of the RKL\nobjective. In contrast, we show that the recently adopted '$k_3$ as loss' (like\nin GRPO) is merely a first-order, biased approximation of the principled loss.\nFurthermore, we argue that common off-policy implementations of '$k_n$ as loss'\nmethods are biased due to neglected importance sampling, and we propose a\nprincipled correction. Our findings provide a comprehensive, gradient-based\nrationale for choosing and correctly implementing KL regularization, paving the\nway for more robust and effective RLHF systems."}
{"id": "2510.02206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02206", "abs": "https://arxiv.org/abs/2510.02206", "authors": ["Daniel Gallo Fernández"], "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling", "comment": null, "summary": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos."}
{"id": "2510.01784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01784", "abs": "https://arxiv.org/abs/2510.01784", "authors": ["Xiaofei Wu", "Guozhen Zhang", "Zhiyong Xu", "Yuan Zhou", "Qinglin Lu", "Xuming He"], "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation", "comment": null, "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models."}
{"id": "2510.02265", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02265", "abs": "https://arxiv.org/abs/2510.02265", "authors": ["Yalin E. Sagduyu", "Tugba Erpek", "Kemal Davaslioglu", "Sastry Kompella"], "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning", "comment": null, "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime."}
{"id": "2510.02265", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02265", "abs": "https://arxiv.org/abs/2510.02265", "authors": ["Yalin E. Sagduyu", "Tugba Erpek", "Kemal Davaslioglu", "Sastry Kompella"], "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning", "comment": null, "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime."}
{"id": "2510.01528", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01528", "abs": "https://arxiv.org/abs/2510.01528", "authors": ["Daniel Zhao", "Abhilash Shankarampeta", "Lanxiang Hu", "Tajana Rosing", "Hao Zhang"], "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation", "comment": null, "summary": "We propose a novel method that leverages sparse autoencoders (SAEs) and\nclustering techniques to analyze the internal token representations of large\nlanguage models (LLMs) and guide generations in mathematical reasoning tasks.\nOur approach first trains an SAE to generate sparse vector representations for\ntraining tokens, then applies k-means clustering to construct a graph where\nvertices represent token clusters and weighted edges capture sequential token\ntransitions. Using this graph, we define an edge-weight based reward function\nto quantify adherence to established reasoning traces, thereby identifying\nexploitative reasoning trajectories. Additionally, we measure generation\ndiversity from clustering to assess the extent of exploration. Our findings\nindicate that balancing both exploitation and exploration is crucial for\nachieving high accuracy in mathematical reasoning tasks. During generation, the\nSAE can serve as a scalable reward model to guide generations, ensuring a\nbalanced trade-off between exploitation and exploration. This prevents extreme\nbehaviors in either direction, ultimately fostering a higher-quality reasoning\nprocess in LLMs."}
{"id": "2510.02060", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02060", "abs": "https://arxiv.org/abs/2510.02060", "authors": ["Sanghyu Yoon", "Dongmin Kim", "Suhee Yoon", "Ye Seul Sim", "Seungdong Yoa", "Hye-Seung Cho", "Soonyoung Lee", "Hankook Lee", "Woohyung Lim"], "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection", "comment": "9 pages, 4 figures", "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD."}
