{"id": "2602.06097", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06097", "abs": "https://arxiv.org/abs/2602.06097", "authors": ["Purbak Sengupta", "Sambeet Mishra", "Sonal Shreya"], "title": "Agentic Workflow Using RBA$_\u03b8$ for Event Prediction", "comment": null, "summary": "Wind power ramp events are difficult to forecast due to strong variability, multi-scale dynamics, and site-specific meteorological effects. This paper proposes an event-first, frequency-aware forecasting paradigm that directly predicts ramp events and reconstructs the power trajectory thereafter, rather than inferring events from dense forecasts. The framework is built on an enhanced Ramping Behaviour Analysis (RBA$_\u03b8$) method's event representation and progressively integrates statistical, machine-learning, and deep-learning models. Traditional forecasting models with post-hoc event extraction provides a strong interpretable baseline but exhibits limited generalisation across sites. Direct event prediction using Random Forests improves robustness over survival-based formulations, motivating fully event-aware modelling. To capture the multi-scale nature of wind ramps, we introduce an event-first deep architecture that integrates wavelet-based frequency decomposition, temporal excitation features, and adaptive feature selection. The resulting sequence models enable stable long-horizon event prediction, physically consistent trajectory reconstruction, and zero-shot transfer to previously unseen wind farms. Empirical analysis shows that ramp magnitude and duration are governed by distinct mid-frequency bands, allowing accurate signal reconstruction from sparse event forecasts. An agentic forecasting layer is proposed, in which specialised workflows are selected dynamically based on operational context. Together, the framework demonstrates that event-first, frequency-aware forecasting provides a transferable and operationally aligned alternative to trajectory-first wind-power prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e8b\u4ef6\u4f18\u5148\u3001\u9891\u7387\u611f\u77e5\u7684\u98ce\u7535\u722c\u5761\u4e8b\u4ef6\u9884\u6d4b\u6846\u67b6\uff0c\u76f4\u63a5\u9884\u6d4b\u722c\u5761\u4e8b\u4ef6\u5e76\u91cd\u5efa\u529f\u7387\u8f68\u8ff9\uff0c\u800c\u975e\u4ece\u5bc6\u96c6\u9884\u6d4b\u4e2d\u63a8\u65ad\u4e8b\u4ef6\uff0c\u5b9e\u73b0\u8de8\u98ce\u7535\u573a\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u98ce\u7535\u722c\u5761\u4e8b\u4ef6\u56e0\u5f3a\u53d8\u5f02\u6027\u3001\u591a\u5c3a\u5ea6\u52a8\u6001\u548c\u7ad9\u70b9\u7279\u5b9a\u6c14\u8c61\u6548\u5e94\u800c\u96be\u4ee5\u9884\u6d4b\u3002\u4f20\u7edf\u57fa\u4e8e\u5bc6\u96c6\u529f\u7387\u9884\u6d4b\u518d\u63d0\u53d6\u4e8b\u4ef6\u7684\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u3001\u53ef\u8fc1\u79fb\u7684\u4e8b\u4ef6\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u4e8b\u4ef6\u4f18\u5148\u9884\u6d4b\u8303\u5f0f\uff1a\u76f4\u63a5\u9884\u6d4b\u722c\u5761\u4e8b\u4ef6\uff0c\u7136\u540e\u91cd\u5efa\u529f\u7387\u8f68\u8ff9\uff1b2. \u57fa\u4e8e\u589e\u5f3a\u7684RBA\u03b8\u65b9\u6cd5\u7684\u4e8b\u4ef6\u8868\u793a\uff1b3. \u6e10\u8fdb\u96c6\u6210\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b4. \u5f15\u5165\u4e8b\u4ef6\u4f18\u5148\u6df1\u5ea6\u67b6\u6784\uff1a\u96c6\u6210\u5c0f\u6ce2\u9891\u7387\u5206\u89e3\u3001\u65f6\u95f4\u6fc0\u52b1\u7279\u5f81\u548c\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\uff1b5. \u63d0\u51fa\u667a\u80fd\u4f53\u9884\u6d4b\u5c42\uff1a\u6839\u636e\u8fd0\u884c\u4e0a\u4e0b\u6587\u52a8\u6001\u9009\u62e9\u4e13\u95e8\u5de5\u4f5c\u6d41\u3002", "result": "1. \u968f\u673a\u68ee\u6797\u76f4\u63a5\u4e8b\u4ef6\u9884\u6d4b\u6bd4\u57fa\u4e8e\u751f\u5b58\u5206\u6790\u7684\u516c\u5f0f\u66f4\u7a33\u5065\uff1b2. \u4e8b\u4ef6\u4f18\u5148\u6df1\u5ea6\u67b6\u6784\u5b9e\u73b0\u7a33\u5b9a\u957f\u65f6\u57df\u4e8b\u4ef6\u9884\u6d4b\u3001\u7269\u7406\u4e00\u81f4\u7684\u8f68\u8ff9\u91cd\u5efa\u548c\u96f6\u6837\u672c\u8fc1\u79fb\uff1b3. \u5b9e\u8bc1\u5206\u6790\u663e\u793a\u722c\u5761\u5e45\u5ea6\u548c\u6301\u7eed\u65f6\u95f4\u53d7\u4e0d\u540c\u4e2d\u9891\u5e26\u63a7\u5236\uff1b4. \u53ef\u4ece\u7a00\u758f\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u51c6\u786e\u91cd\u5efa\u4fe1\u53f7\u3002", "conclusion": "\u4e8b\u4ef6\u4f18\u5148\u3001\u9891\u7387\u611f\u77e5\u7684\u9884\u6d4b\u6846\u67b6\u4e3a\u98ce\u7535\u529f\u7387\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u8fc1\u79fb\u4e14\u4e0e\u8fd0\u884c\u5bf9\u9f50\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u8f68\u8ff9\u4f18\u5148\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8de8\u7ad9\u70b9\u6cdb\u5316\u548c\u8fd0\u884c\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2602.06051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06051", "abs": "https://arxiv.org/abs/2602.06051", "authors": ["Kexin Ma", "Bojun Li", "Yuhua Tang", "Ruochun Jin", "Liting Sun"], "title": "CAST: Character-and-Scene Episodic Memory for Agents", "comment": null, "summary": "Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u620f\u5267\u7406\u8bba\u7684\u89d2\u8272\u4e0e\u573a\u666f\u8bb0\u5fc6\u67b6\u6784(CAST)\uff0c\u901a\u8fc7\u6784\u5efa3D\u573a\u666f(\u65f6\u95f4/\u5730\u70b9/\u4e3b\u9898)\u5e76\u7ec4\u7ec7\u6210\u89d2\u8272\u6863\u6848\u6765\u8868\u793a\u60c5\u666f\u8bb0\u5fc6\uff0c\u7ed3\u5408\u56fe\u8bed\u4e49\u8bb0\u5fc6\u5f62\u6210\u53cc\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u56de\u5fc6\uff0c\u5c06\u7ecf\u9a8c\u8868\u793a\u4e3a\u952e\u503c\u5bf9\u3001\u5411\u91cf\u6216\u56fe\u7ed3\u6784\uff0c\u96be\u4ee5\u8868\u793a\u548c\u68c0\u7d22\u8fde\u8d2f\u7684\u4e8b\u4ef6\u3002\u4eba\u7c7b\u7684\u60c5\u666f\u8bb0\u5fc6\u80fd\u591f\u56de\u5fc6\u57fa\u4e8e\u8c01\u3001\u4f55\u65f6\u3001\u4f55\u5730\u7684\u8fde\u8d2f\u4e8b\u4ef6\uff0c\u9700\u8981\u65b0\u7684\u8bb0\u5fc6\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faCAST\u8bb0\u5fc6\u67b6\u6784\uff0c\u53d7\u620f\u5267\u7406\u8bba\u542f\u53d1\uff1a1) \u6784\u5efa3D\u573a\u666f(\u65f6\u95f4/\u5730\u70b9/\u4e3b\u9898)\u5e76\u7ec4\u7ec7\u6210\u89d2\u8272\u6863\u6848\u6765\u603b\u7ed3\u89d2\u8272\u4e8b\u4ef6\uff0c\u8868\u793a\u60c5\u666f\u8bb0\u5fc6\uff1b2) \u7ed3\u5408\u57fa\u4e8e\u56fe\u7684\u8bed\u4e49\u8bb0\u5fc6\uff0c\u5f62\u6210\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCAST\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53478.11% F1\u5206\u6570\u548c10.21% J(LLM-as-a-Judge)\u8bc4\u5206\uff0c\u5c24\u5176\u5728\u5f00\u653e\u6027\u548c\u65f6\u95f4\u654f\u611f\u7684\u5bf9\u8bdd\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CAST\u901a\u8fc7\u620f\u5267\u7406\u8bba\u542f\u53d1\u7684\u89d2\u8272\u4e0e\u573a\u666f\u67b6\u6784\u6709\u6548\u8868\u793a\u60c5\u666f\u8bb0\u5fc6\uff0c\u7ed3\u5408\u8bed\u4e49\u8bb0\u5fc6\u5f62\u6210\u53cc\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.06161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06161", "abs": "https://arxiv.org/abs/2602.06161", "authors": ["Yanzheng Xiang", "Lan Wei", "Yizhen Yao", "Qinglin Zhu", "Hanqi Yan", "Chen Jin", "Philip Alexander Teare", "Dandan Zhang", "Lin Gui", "Amrutha Saseendran", "Yulan He"], "title": "Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding", "comment": null, "summary": "Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.", "AI": {"tldr": "COVER\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u6269\u6563\u89e3\u7801\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7KV\u7f13\u5b58\u8986\u76d6\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u7559\u4e00\u9a8c\u8bc1\u548c\u7a33\u5b9a\u8349\u7a3f\u751f\u6210\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u4fee\u8ba2\u5e76\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u6269\u6563\u89e3\u7801\u65b9\u6cd5\u4e2d\uff0c\u53ef\u64a4\u9500\u89e3\u7801\u7684\u9a8c\u8bc1\u65b9\u6848\u7ecf\u5e38\u89e6\u53d1\"\u7ffb\u8f6c\u632f\u8361\"\u95ee\u9898\uff0c\u5373\u4ee4\u724c\u88ab\u91cd\u65b0\u63a9\u7801\u540e\u53c8\u6062\u590d\u539f\u72b6\uff0c\u8fd9\u65e2\u524a\u5f31\u4e86\u5e76\u884c\u8349\u7a3f\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\uff0c\u53c8\u6d6a\u8d39\u4e86\u4fee\u8ba2\u9884\u7b97\uff0c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u4e0b\u964d\u3002", "method": "COVER\u91c7\u7528KV\u7f13\u5b58\u8986\u76d6\u6280\u672f\uff1a1) \u901a\u8fc7\u63a9\u7801\u9009\u5b9a\u79cd\u5b50\u4f4d\u7f6e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u540c\u65f6\u5c06\u5176\u7f13\u5b58\u7684\u952e\u503c\u72b6\u6001\u6ce8\u5165\u6240\u6709\u5176\u4ed6\u67e5\u8be2\u4ee5\u4fdd\u7559\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b2) \u4f7f\u7528\u95ed\u5f0f\u5bf9\u89d2\u7ebf\u6821\u6b63\u9632\u6b62\u79cd\u5b50\u4f4d\u7f6e\u7684\u81ea\u6cc4\u6f0f\uff1b3) \u8bbe\u8ba1\u7a33\u5b9a\u6027\u611f\u77e5\u8bc4\u5206\u6765\u5e73\u8861\u4e0d\u786e\u5b9a\u6027\u3001\u4e0b\u6e38\u5f71\u54cd\u548c\u7f13\u5b58\u6f02\u79fb\uff1b4) \u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u6b65\u9a8c\u8bc1\u7684\u79cd\u5b50\u6570\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOVER\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u4fee\u8ba2\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u89e3\u7801\u901f\u5ea6\u3002", "conclusion": "COVER\u901a\u8fc7\u521b\u65b0\u7684KV\u7f13\u5b58\u8986\u76d6\u673a\u5236\u89e3\u51b3\u4e86\u5e76\u884c\u6269\u6563\u89e3\u7801\u4e2d\u7684\u7ffb\u8f6c\u632f\u8361\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u9a8c\u8bc1\u548c\u66f4\u7a33\u5b9a\u7684\u5e76\u884c\u8349\u7a3f\u751f\u6210\uff0c\u4e3a\u52a0\u901f\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.06268", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06268", "abs": "https://arxiv.org/abs/2602.06268", "authors": ["Junhyeok Lee", "Han Jang", "Kyu Sung Choi"], "title": "MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs", "comment": "13 pages, 7 figures", "summary": "Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).", "AI": {"tldr": "MPIB\u662f\u4e00\u4e2a\u533b\u7597\u63d0\u793a\u6ce8\u5165\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u548cRAG\u7cfb\u7edf\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u4e34\u5e8a\u4f24\u5bb3\u4e8b\u4ef6\u7387\uff08CHER\uff09\u548c\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u7684\u5dee\u5f02\u3002", "motivation": "\u968f\u7740LLM\u548cRAG\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u53ef\u80fd\u5bfc\u81f4\u4e34\u5e8a\u4e0d\u5b89\u5168\u6216\u8bef\u5bfc\u6027\u8f93\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e34\u5e8a\u5b89\u5168\u6027\u3002", "method": "\u521b\u5efaMPIB\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b9,697\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u5b9e\u4f8b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8d28\u91cf\u95e8\u63a7\u548c\u4e34\u5e8a\u5b89\u5168\u68c0\u67e5\u6784\u5efa\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u4e34\u5e8a\u4f24\u5bb3\u4e8b\u4ef6\u7387\uff08CHER\uff09\u548c\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u533a\u5206\u76f4\u63a5\u63d0\u793a\u6ce8\u5165\u548c\u95f4\u63a5RAG\u4ecb\u5bfc\u7684\u6ce8\u5165\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0ASR\u548cCHER\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9c81\u68d2\u6027\u5173\u952e\u53d6\u51b3\u4e8e\u5bf9\u6297\u6027\u6307\u4ee4\u51fa\u73b0\u5728\u7528\u6237\u67e5\u8be2\u4e2d\u8fd8\u662f\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e2d\u3002\u63d0\u4f9b\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u6587\u6863\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u3002", "conclusion": "MPIB\u4e3a\u4e34\u5e8a\u63d0\u793a\u6ce8\u5165\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u533a\u5206\u6307\u4ee4\u9075\u4ece\u6027\u548c\u4e0b\u6e38\u60a3\u8005\u98ce\u9669\u7684\u91cd\u8981\u6027\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u4e34\u5e8aAI\u7cfb\u7edf\u3002"}}
{"id": "2602.06275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06275", "abs": "https://arxiv.org/abs/2602.06275", "authors": ["Isaac Picov", "Ritesh Goru"], "title": "RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution", "comment": null, "summary": "Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.", "AI": {"tldr": "RoPE-LIME\uff1a\u4e00\u79cd\u7528\u4e8e\u89e3\u91ca\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5f00\u6e90\u66ff\u4ee3\u6a21\u578b\u548c\u4f18\u5316\u7684\u6270\u52a8\u7b56\u7565\uff0c\u5728\u51cf\u5c11API\u8c03\u7528\u6210\u672c\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u7279\u5f81\u5f52\u56e0\u3002", "motivation": "\u89e3\u91ca\u95ed\u6e90LLM\u8f93\u51fa\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3aAPI\u8bbf\u95ee\u963b\u6b62\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u800c\u57fa\u4e8e\u6270\u52a8\u7684\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u566a\u58f0\u5927\uff08\u4f9d\u8d56\u4e8e\u91cd\u65b0\u751f\u6210\u6587\u672c\uff09\u3002", "method": "RoPE-LIME\u6269\u5c55\u4e86gSMILE\u65b9\u6cd5\uff0c\u5c06\u63a8\u7406\u4e0e\u89e3\u91ca\u89e3\u8026\uff1a\u7ed9\u5b9a\u95ed\u6e90\u6a21\u578b\u7684\u56fa\u5b9a\u8f93\u51fa\uff0c\u4f7f\u7528\u8f83\u5c0f\u7684\u5f00\u6e90\u66ff\u4ee3\u6a21\u578b\u57fa\u4e8e\u6982\u7387\u76ee\u6807\uff08\u8d1f\u5bf9\u6570\u4f3c\u7136\u548c\u6563\u5ea6\u76ee\u6807\uff09\u8ba1\u7b97\u8f93\u5165\u6270\u52a8\u4e0b\u7684token\u7ea7\u5f52\u56e0\u3002\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(i) \u57fa\u4e8eRoPE\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u7684Relaxed Word Mover's Distance\u7684\u5c40\u90e8\u6027\u6838\u51fd\u6570\uff0c\u786e\u4fdd\u63a9\u7801\u4e0b\u7684\u7a33\u5b9a\u76f8\u4f3c\u6027\uff1b(ii) Sparse-K\u91c7\u6837\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u6270\u52a8\u7b56\u7565\uff0c\u5728\u6709\u9650\u9884\u7b97\u4e0b\u63d0\u9ad8\u4ea4\u4e92\u8986\u76d6\u7387\u3002", "result": "\u5728HotpotQA\uff08\u53e5\u5b50\u7279\u5f81\uff09\u548c\u624b\u5de5\u6807\u6ce8\u7684MMLU\u5b50\u96c6\uff08\u5355\u8bcd\u7279\u5f81\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRoPE-LIME\u6bd4\u7559\u4e00\u6cd5\u91c7\u6837\u4ea7\u751f\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u5f52\u56e0\uff0c\u76f8\u6bd4gSMILE\u6709\u6240\u6539\u8fdb\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u95ed\u6e90\u6a21\u578b\u7684API\u8c03\u7528\u3002", "conclusion": "RoPE-LIME\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u95ed\u6e90LLM\u8f93\u51fa\uff0c\u901a\u8fc7\u4f7f\u7528\u5f00\u6e90\u66ff\u4ee3\u6a21\u578b\u548c\u4f18\u5316\u7684\u6270\u52a8\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u89e3\u91ca\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2602.06575", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06575", "abs": "https://arxiv.org/abs/2602.06575", "authors": ["Fangyuan Wang", "Peng Zhou", "Jiaming Qi", "Shipeng Lyu", "David Navarro-Alarcon", "Guodong Guo"], "title": "Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation", "comment": null, "summary": "Vision-language-action (VLA) models typically inject proprioception only as a late conditioning signal, which prevents robot state from shaping instruction understanding and from influencing which visual tokens are attended throughout the policy. We introduce ThinkProprio, which converts proprioception into a sequence of text tokens in the VLM embedding space and fuses them with the task instruction at the input. This early fusion lets embodied state participate in subsequent visual reasoning and token selection, biasing computation toward action-critical evidence while suppressing redundant visual tokens. In a systematic ablation over proprioception encoding, state entry point, and action-head conditioning, we find that text tokenization is more effective than learned projectors, and that retaining roughly 15% of visual tokens can match the performance of using the full token set. Across CALVIN, LIBERO, and real-world manipulation, ThinkProprio matches or improves over strong baselines while reducing end-to-end inference latency over 50%.", "AI": {"tldr": "ThinkProprio \u662f\u4e00\u79cdVLA\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u672c\u4f53\u611f\u89c9\u8f6c\u6362\u4e3a\u6587\u672c\u6807\u8bb0\u5e76\u4e0e\u4efb\u52a1\u6307\u4ee4\u65e9\u671f\u878d\u5408\uff0c\u8ba9\u673a\u5668\u4eba\u72b6\u6001\u53c2\u4e0e\u89c6\u89c9\u63a8\u7406\u548c\u6807\u8bb0\u9009\u62e9\uff0c\u51cf\u5c11\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\uff0c\u63d0\u9ad8\u63a8\u7406\u901f\u5ea650%\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u901a\u5e38\u5c06\u672c\u4f53\u611f\u89c9\u4f5c\u4e3a\u540e\u671f\u6761\u4ef6\u4fe1\u53f7\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u72b6\u6001\u5bf9\u6307\u4ee4\u7406\u89e3\u548c\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u5e03\u7684\u5f71\u54cd\u3002\u9700\u8981\u8ba9\u673a\u5668\u4eba\u72b6\u6001\u66f4\u65e9\u5730\u53c2\u4e0e\u89c6\u89c9\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u5c06\u672c\u4f53\u611f\u89c9\u8f6c\u6362\u4e3aVLM\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6587\u672c\u6807\u8bb0\u5e8f\u5217\uff0c\u4e0e\u4efb\u52a1\u6307\u4ee4\u5728\u8f93\u5165\u5c42\u65e9\u671f\u878d\u5408\u3002\u8fd9\u79cd\u65b9\u6cd5\u8ba9\u673a\u5668\u4eba\u72b6\u6001\u53c2\u4e0e\u540e\u7eed\u7684\u89c6\u89c9\u63a8\u7406\u548c\u6807\u8bb0\u9009\u62e9\uff0c\u504f\u5411\u4e8e\u52a8\u4f5c\u5173\u952e\u8bc1\u636e\uff0c\u540c\u65f6\u6291\u5236\u5197\u4f59\u89c6\u89c9\u6807\u8bb0\u3002", "result": "\u6587\u672c\u6807\u8bb0\u5316\u6bd4\u5b66\u4e60\u6295\u5f71\u5668\u66f4\u6709\u6548\uff1b\u4fdd\u7559\u7ea615%\u7684\u89c6\u89c9\u6807\u8bb0\u5373\u53ef\u8fbe\u5230\u4f7f\u7528\u5b8c\u6574\u6807\u8bb0\u96c6\u7684\u6027\u80fd\uff1b\u5728CALVIN\u3001LIBERO\u548c\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4e2d\uff0cThinkProprio\u5339\u914d\u6216\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u5c06\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e50%\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u5c06\u672c\u4f53\u611f\u89c9\u8f6c\u6362\u4e3a\u6587\u672c\u6807\u8bb0\u5e76\u4e0e\u6307\u4ee4\u65e9\u671f\u878d\u5408\uff0cThinkProprio\u8ba9\u673a\u5668\u4eba\u72b6\u6001\u66f4\u6709\u6548\u5730\u53c2\u4e0e\u89c6\u89c9\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.06653", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06653", "abs": "https://arxiv.org/abs/2602.06653", "authors": ["Zi Yin", "Fanhong Li", "Shurui Zheng", "Jia Liu"], "title": "RAPID: Reconfigurable, Adaptive Platform for Iterative Design", "comment": null, "summary": "Developing robotic manipulation policies is iterative and hypothesis-driven: researchers test tactile sensing, gripper geometries, and sensor placements through real-world data collection and training. Yet even minor end-effector changes often require mechanical refitting and system re-integration, slowing iteration. We present RAPID, a full-stack reconfigurable platform designed to reduce this friction. RAPID is built around a tool-free, modular hardware architecture that unifies handheld data collection and robot deployment, and a matching software stack that maintains real-time awareness of the underlying hardware configuration through a driver-level Physical Mask derived from USB events. This modular hardware architecture reduces reconfiguration to seconds and makes systematic multi-modal ablation studies practical, allowing researchers to sweep diverse gripper and sensing configurations without repeated system bring-up. The Physical Mask exposes modality presence as an explicit runtime signal, enabling auto-configuration and graceful degradation under sensor hot-plug events, so policies can continue executing when sensors are physically added or removed. System-centric experiments show that RAPID reduces the setup time for multi-modal configurations by two orders of magnitude compared to traditional workflows and preserves policy execution under runtime sensor hot-unplug events. The hardware designs, drivers, and software stack are open-sourced at https://rapid-kit.github.io/ .", "AI": {"tldr": "RAPID\u662f\u4e00\u4e2a\u5168\u6808\u53ef\u91cd\u6784\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u786c\u4ef6\u67b6\u6784\u548c\u8f6f\u4ef6\u5806\u6808\uff0c\u5c06\u591a\u6a21\u6001\u914d\u7f6e\u8bbe\u7f6e\u65f6\u95f4\u51cf\u5c11\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u652f\u6301\u4f20\u611f\u5668\u70ed\u63d2\u62d4\u548c\u7b56\u7565\u6301\u7eed\u6267\u884c\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5f00\u53d1\u662f\u8fed\u4ee3\u548c\u5047\u8bbe\u9a71\u52a8\u7684\u8fc7\u7a0b\uff0c\u4f46\u5373\u4f7f\u5fae\u5c0f\u7684\u672b\u7aef\u6267\u884c\u5668\u66f4\u6539\u4e5f\u9700\u8981\u673a\u68b0\u91cd\u65b0\u88c5\u914d\u548c\u7cfb\u7edf\u91cd\u65b0\u96c6\u6210\uff0c\u8fd9\u4e25\u91cd\u62d6\u6162\u4e86\u8fed\u4ee3\u901f\u5ea6\u3002\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u5316\u591a\u6a21\u6001\u6d88\u878d\u7814\u7a76\u6765\u6d4b\u8bd5\u89e6\u89c9\u4f20\u611f\u3001\u5939\u6301\u5668\u51e0\u4f55\u5f62\u72b6\u548c\u4f20\u611f\u5668\u653e\u7f6e\uff0c\u4f46\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u3002", "method": "RAPID\u91c7\u7528\u514d\u5de5\u5177\u6a21\u5757\u5316\u786c\u4ef6\u67b6\u6784\uff0c\u7edf\u4e00\u624b\u6301\u6570\u636e\u6536\u96c6\u548c\u673a\u5668\u4eba\u90e8\u7f72\uff1b\u5339\u914d\u7684\u8f6f\u4ef6\u5806\u6808\u901a\u8fc7\u57fa\u4e8eUSB\u4e8b\u4ef6\u7684\u9a71\u52a8\u7ea7\u7269\u7406\u63a9\u7801\u4fdd\u6301\u5bf9\u5e95\u5c42\u786c\u4ef6\u914d\u7f6e\u7684\u5b9e\u65f6\u611f\u77e5\u3002\u7269\u7406\u63a9\u7801\u5c06\u6a21\u6001\u5b58\u5728\u4f5c\u4e3a\u663e\u5f0f\u8fd0\u884c\u65f6\u4fe1\u53f7\uff0c\u652f\u6301\u81ea\u52a8\u914d\u7f6e\u548c\u4f20\u611f\u5668\u70ed\u63d2\u62d4\u65f6\u7684\u4f18\u96c5\u964d\u7ea7\u3002", "result": "\u7cfb\u7edf\u4e2d\u5fc3\u5b9e\u9a8c\u663e\u793a\uff0cRAPID\u5c06\u591a\u6a21\u6001\u914d\u7f6e\u8bbe\u7f6e\u65f6\u95f4\u76f8\u6bd4\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u51cf\u5c11\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u4f20\u611f\u5668\u70ed\u62d4\u63d2\u4e8b\u4ef6\u4e0b\u4fdd\u6301\u7b56\u7565\u6267\u884c\u3002\u5e73\u53f0\u652f\u6301\u5728\u65e0\u9700\u91cd\u590d\u7cfb\u7edf\u542f\u52a8\u7684\u60c5\u51b5\u4e0b\u7cfb\u7edf\u5316\u626b\u63cf\u591a\u6837\u5316\u7684\u5939\u6301\u5668\u548c\u4f20\u611f\u914d\u7f6e\u3002", "conclusion": "RAPID\u901a\u8fc7\u6a21\u5757\u5316\u786c\u4ef6\u548c\u8f6f\u4ef6\u5806\u6808\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5f00\u53d1\u7684\u8fed\u4ee3\u6469\u64e6\uff0c\u4f7f\u7cfb\u7edf\u5316\u591a\u6a21\u6001\u6d88\u878d\u7814\u7a76\u53d8\u5f97\u5b9e\u7528\uff0c\u5e76\u652f\u6301\u4f20\u611f\u5668\u70ed\u63d2\u62d4\u4e0b\u7684\u7b56\u7565\u6301\u7eed\u6267\u884c\u3002\u786c\u4ef6\u8bbe\u8ba1\u3001\u9a71\u52a8\u7a0b\u5e8f\u548c\u8f6f\u4ef6\u5806\u6808\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.06343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06343", "abs": "https://arxiv.org/abs/2602.06343", "authors": ["Weiquan Wang", "Feifei Shao", "Lin Li", "Zhen Wang", "Jun Xiao", "Long Chen"], "title": "Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering", "comment": null, "summary": "High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.", "AI": {"tldr": "U-4DGS\uff1a\u4e00\u79cd\u7528\u4e8e\u5355\u76ee\u89c6\u9891\u4e2d\u52a8\u6001\u4eba\u4f53\u6e32\u67d3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u53d8\u5f62\u7f51\u7edc\u548c\u53cc\u91cd\u5149\u6805\u5316\u5904\u7406\u906e\u6321\u95ee\u9898\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u81ea\u9002\u5e94\u8c03\u8282\u68af\u5ea6\uff0c\u63d0\u9ad8\u6e32\u67d3\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u65f6\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u4f1a\u4ea7\u751f\u65f6\u95f4\u95ea\u70c1\uff0c\u800c\u57fa\u4e8e\u521a\u6027\u51e0\u4f55\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u591a\u6837\u5916\u89c2\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5904\u7406\u4e0d\u53ef\u9760\u89c2\u6d4b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u5f02\u65b9\u5dee\u89c2\u6d4b\u566a\u58f0\u4e0b\u7684\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\u95ee\u9898\u3002\u63d0\u51faU-4DGS\u6846\u67b6\uff0c\u5305\u542b\u6982\u7387\u53d8\u5f62\u7f51\u7edc\u548c\u53cc\u91cd\u5149\u6805\u5316\u6d41\u6c34\u7ebf\uff0c\u751f\u6210\u50cf\u7d20\u5bf9\u9f50\u7684\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u4f5c\u4e3a\u81ea\u9002\u5e94\u68af\u5ea6\u8c03\u8282\u5668\u3002\u8fd8\u5f15\u5165\u7f6e\u4fe1\u611f\u77e5\u6b63\u5219\u5316\uff0c\u9632\u6b62\u7f3a\u4e4f\u53ef\u9760\u89c6\u89c9\u7ebf\u7d22\u533a\u57df\u7684\u51e0\u4f55\u6f02\u79fb\u3002", "result": "\u5728ZJU-MoCap\u548cOcMotion\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cU-4DGS\u5728\u6e32\u67d3\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "U-4DGS\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u4eba\u4f53\u6e32\u67d3\u4e2d\u7684\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u9c81\u68d2\u7684\u6e32\u67d3\u6548\u679c\u3002"}}
{"id": "2602.06346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06346", "abs": "https://arxiv.org/abs/2602.06346", "authors": ["Tianyi Zhang", "Chengcheng Liu", "Jinwei Chen", "Chun-Le Guo", "Chongyi Li", "Ming-Ming Cheng", "Bo Li", "Peng-Tao Jiang"], "title": "FlowConsist: Make Your Flow Consistent with Real Trajectory", "comment": null, "summary": "Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\\times$256, achieving an FID of 1.52 with only 1 sampling step.", "AI": {"tldr": "FlowConsist\uff1a\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u9884\u6d4b\u7684\u8fb9\u9645\u901f\u5ea6\u66ff\u4ee3\u6761\u4ef6\u901f\u5ea6\uff0c\u5e76\u5f15\u5165\u8f68\u8ff9\u6821\u6b63\u7b56\u7565\uff0c\u89e3\u51b3\u5feb\u901f\u6d41\u6a21\u578b\u4e2d\u7684\u8f68\u8ff9\u6f02\u79fb\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5728ImageNet 256\u00d7256\u4e0a\u4ec5\u75281\u6b65\u91c7\u6837\u8fbe\u5230FID 1.52\u7684SOTA\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u5feb\u901f\u6d41\u6a21\u578b\u8bad\u7ec3\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u95ee\u9898\uff1a1\uff09\u968f\u673a\u914d\u5bf9\u566a\u58f0-\u6570\u636e\u6837\u672c\u6784\u5efa\u7684\u6761\u4ef6\u901f\u5ea6\u5f15\u5165\u7cfb\u7edf\u6027\u8f68\u8ff9\u6f02\u79fb\uff0c\u963b\u788d\u6a21\u578b\u9075\u5faa\u4e00\u81f4\u7684ODE\u8def\u5f84\uff1b2\uff09\u6a21\u578b\u8fd1\u4f3c\u8bef\u5dee\u968f\u65f6\u95f4\u6b65\u7d2f\u79ef\uff0c\u5bfc\u81f4\u957f\u65f6\u533a\u95f4\u4e25\u91cd\u504f\u5dee\u3002", "method": "\u63d0\u51faFlowConsist\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u7528\u6a21\u578b\u81ea\u8eab\u9884\u6d4b\u7684\u8fb9\u9645\u901f\u5ea6\u66ff\u4ee3\u6761\u4ef6\u901f\u5ea6\uff0c\u4f7f\u4f18\u5316\u4e0e\u771f\u5b9e\u8f68\u8ff9\u5bf9\u9f50\uff1b2\uff09\u5f15\u5165\u8f68\u8ff9\u6821\u6b63\u7b56\u7565\uff0c\u5728\u8f68\u8ff9\u7684\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5bf9\u9f50\u751f\u6210\u6837\u672c\u548c\u771f\u5b9e\u6837\u672c\u7684\u8fb9\u9645\u5206\u5e03\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4ec5\u75281\u6b65\u91c7\u6837\u83b7\u5f97FID 1.52\u3002", "conclusion": "FlowConsist\u901a\u8fc7\u89e3\u51b3\u8f68\u8ff9\u4e00\u81f4\u6027\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5feb\u901f\u6d41\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e00\u6b65\u751f\u6210\u3002"}}
{"id": "2602.06864", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06864", "abs": "https://arxiv.org/abs/2602.06864", "authors": ["Zhuocheng Zhang", "Haizhou Zhao", "Xudong Sun", "Aaron M. Johnson", "Majid Khadiv"], "title": "SURE: Safe Uncertainty-Aware Robot-Environment Interaction using Trajectory Optimization", "comment": null, "summary": "Robotic tasks involving contact interactions pose significant challenges for trajectory optimization due to discontinuous dynamics. Conventional formulations typically assume deterministic contact events, which limit robustness and adaptability in real-world settings. In this work, we propose SURE, a robust trajectory optimization framework that explicitly accounts for contact timing uncertainty. By allowing multiple trajectories to branch from possible pre-impact states and later rejoin a shared trajectory, SURE achieves both robustness and computational efficiency within a unified optimization framework. We evaluate SURE on two representative tasks with unknown impact times. In a cart-pole balancing task involving uncertain wall location, SURE achieves an average improvement of 21.6% in success rate when branch switching is enabled during control. In an egg-catching experiment using a robotic manipulator, SURE improves the success rate by 40%. These results demonstrate that SURE substantially enhances robustness compared to conventional nominal formulations.", "AI": {"tldr": "SURE\uff1a\u4e00\u79cd\u8003\u8651\u63a5\u89e6\u65f6\u5e8f\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u652f\u8f68\u8ff9\u8bbe\u8ba1\u63d0\u5347\u673a\u5668\u4eba\u63a5\u89e6\u4efb\u52a1\u7684\u9c81\u68d2\u6027", "motivation": "\u673a\u5668\u4eba\u63a5\u89e6\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u8f68\u8ff9\u4f18\u5316\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u786e\u5b9a\u6027\u63a5\u89e6\u4e8b\u4ef6\u9650\u5236\u4e86\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027", "method": "\u63d0\u51faSURE\u6846\u67b6\uff0c\u5141\u8bb8\u4ece\u53ef\u80fd\u7684\u9884\u78b0\u649e\u72b6\u6001\u5206\u652f\u591a\u6761\u8f68\u8ff9\uff0c\u968f\u540e\u91cd\u65b0\u6c47\u5408\u5230\u5171\u4eab\u8f68\u8ff9\uff0c\u5728\u7edf\u4e00\u4f18\u5316\u6846\u67b6\u4e2d\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387", "result": "\u5728\u4e0d\u786e\u5b9a\u5899\u58c1\u4f4d\u7f6e\u7684\u5012\u7acb\u6446\u5e73\u8861\u4efb\u52a1\u4e2d\uff0c\u5206\u652f\u5207\u6362\u4f7f\u6210\u529f\u7387\u5e73\u5747\u63d0\u534721.6%\uff1b\u5728\u673a\u5668\u4eba\u6293\u9e21\u86cb\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u534740%", "conclusion": "SURE\u6846\u67b6\u76f8\u6bd4\u4f20\u7edf\u540d\u4e49\u89c4\u5212\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a5\u89e6\u65f6\u5e8f\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027"}}
{"id": "2602.06269", "categories": ["cs.LG", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.06269", "abs": "https://arxiv.org/abs/2602.06269", "authors": ["Vinh Hoang", "Sebastian Krumscheid", "Holger Rauhut", "Ra\u00fal Tempone"], "title": "PurSAMERE: Reliable Adversarial Purification via Sharpness-Aware Minimization of Expected Reconstruction Error", "comment": null, "summary": "We propose a novel deterministic purification method to improve adversarial robustness by mapping a potentially adversarial sample toward a nearby sample that lies close to a mode of the data distribution, where classifiers are more reliable. We design the method to be deterministic to ensure reliable test accuracy and to prevent the degradation of effective robustness observed in stochastic purification approaches when the adversary has full knowledge of the system and its randomness. We employ a score model trained by minimizing the expected reconstruction error of noise-corrupted data, thereby learning the structural characteristics of the input data distribution. Given a potentially adversarial input, the method searches within its local neighborhood for a purified sample that minimizes the expected reconstruction error under noise corruption and then feeds this purified sample to the classifier. During purification, sharpness-aware minimization is used to guide the purified samples toward flat regions of the expected reconstruction error landscape, thereby enhancing robustness. We further show that, as the noise level decreases, minimizing the expected reconstruction error biases the purified sample toward local maximizers of the Gaussian-smoothed density; under additional local assumptions on the score model, we prove recovery of a local maximizer in the small-noise limit. Experimental results demonstrate significant gains in adversarial robustness over state-of-the-art methods under strong deterministic white-box attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u786e\u5b9a\u6027\u51c0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5bf9\u6297\u6837\u672c\u6620\u5c04\u5230\u6570\u636e\u5206\u5e03\u6a21\u5f0f\u9644\u8fd1\u7684\u6837\u672c\uff0c\u63d0\u9ad8\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u907f\u514d\u968f\u673a\u51c0\u5316\u65b9\u6cd5\u5728\u5b8c\u5168\u77e5\u8bc6\u653b\u51fb\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u968f\u673a\u51c0\u5316\u65b9\u6cd5\u5728\u653b\u51fb\u8005\u5b8c\u5168\u4e86\u89e3\u7cfb\u7edf\u53ca\u5176\u968f\u673a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u9c81\u68d2\u6027\u4f1a\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u786e\u5b9a\u6027\u65b9\u6cd5\u6765\u786e\u4fdd\u53ef\u9760\u7684\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06\u5bf9\u6297\u6837\u672c\u6620\u5c04\u5230\u6570\u636e\u5206\u5e03\u6a21\u5f0f\u9644\u8fd1\uff0c\u56e0\u4e3a\u5206\u7c7b\u5668\u5728\u8fd9\u4e9b\u533a\u57df\u66f4\u53ef\u9760\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u901a\u8fc7\u6700\u5c0f\u5316\u566a\u58f0\u6c61\u67d3\u6570\u636e\u671f\u671b\u91cd\u6784\u8bef\u5dee\u7684\u5f97\u5206\u6a21\u578b\uff0c\u5b66\u4e60\u8f93\u5165\u6570\u636e\u5206\u5e03\u7684\u7ed3\u6784\u7279\u5f81\u3002\u7ed9\u5b9a\u6f5c\u5728\u5bf9\u6297\u8f93\u5165\uff0c\u5728\u5176\u5c40\u90e8\u90bb\u57df\u5185\u641c\u7d22\u6700\u5c0f\u5316\u566a\u58f0\u6c61\u67d3\u4e0b\u671f\u671b\u91cd\u6784\u8bef\u5dee\u7684\u51c0\u5316\u6837\u672c\uff0c\u7136\u540e\u5c06\u5176\u8f93\u5165\u5206\u7c7b\u5668\u3002\u4f7f\u7528\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u5f15\u5bfc\u51c0\u5316\u6837\u672c\u671d\u5411\u671f\u671b\u91cd\u6784\u8bef\u5dee\u666f\u89c2\u7684\u5e73\u5766\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5f3a\u786e\u5b9a\u6027\u767d\u76d2\u653b\u51fb\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u968f\u7740\u566a\u58f0\u6c34\u5e73\u964d\u4f4e\uff0c\u6700\u5c0f\u5316\u671f\u671b\u91cd\u6784\u8bef\u5dee\u4f1a\u4f7f\u51c0\u5316\u6837\u672c\u504f\u5411\u9ad8\u65af\u5e73\u6ed1\u5bc6\u5ea6\u7684\u5c40\u90e8\u6700\u5927\u5316\u5668\u3002", "conclusion": "\u63d0\u51fa\u7684\u786e\u5b9a\u6027\u51c0\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u907f\u514d\u4e86\u968f\u673a\u65b9\u6cd5\u7684\u5f31\u70b9\uff0c\u901a\u8fc7\u5c06\u5bf9\u6297\u6837\u672c\u6620\u5c04\u5230\u6570\u636e\u5206\u5e03\u6a21\u5f0f\u9644\u8fd1\uff0c\u4f7f\u5206\u7c7b\u5668\u5728\u66f4\u53ef\u9760\u7684\u533a\u57df\u5de5\u4f5c\uff0c\u540c\u65f6\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u51c0\u5316\u6837\u672c\u4f1a\u6536\u655b\u5230\u5c40\u90e8\u5bc6\u5ea6\u6700\u5927\u5316\u5668\u3002"}}
{"id": "2602.06405", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.06405", "abs": "https://arxiv.org/abs/2602.06405", "authors": ["Adam D. Hines", "Karin Nordstr\u00f6m", "Andrew B. Barron"], "title": "A neuromorphic model of the insect visual system for natural image processing", "comment": "21 pages, 7 figures, under review", "summary": "Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u6606\u866b\u89c6\u89c9\u542f\u53d1\u7684\u751f\u7269\u542f\u53d1\u89c6\u89c9\u6a21\u578b\uff0c\u901a\u8fc7\u5b8c\u5168\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u751f\u6210\u7a00\u758f\u3001\u5224\u522b\u6027\u7f16\u7801\uff0c\u5728\u82b1\u6735\u8bc6\u522b\u548c\u81ea\u7136\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5b9e\u73b0\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e24\u79cd\u5b9e\u73b0\u3002", "motivation": "\u5f53\u524d\u8bb8\u591a\u89c6\u89c9\u6a21\u578b\u8fc7\u4e8e\u5173\u6ce8\u4efb\u52a1\u6027\u80fd\u800c\u5ffd\u89c6\u4e86\u751f\u7269\u771f\u5b9e\u7684\u5904\u7406\u901a\u8def\u3002\u6606\u866b\u89c6\u89c9\u652f\u6301\u590d\u6742\u884c\u4e3a\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u751f\u7269\u57fa\u7840\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u6355\u6349\u6606\u866b\u89c6\u89c9\u7cfb\u7edf\u539f\u7406\uff0c\u53c8\u80fd\u751f\u6210\u7a00\u758f\u5224\u522b\u6027\u7f16\u7801\u7684\u751f\u7269\u542f\u53d1\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86\u53d7\u6606\u866b\u89c6\u89c9\u542f\u53d1\u7684\u751f\u7269\u542f\u53d1\u89c6\u89c9\u6a21\u578b\uff0c\u91c7\u7528\u5b8c\u5168\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002\u6a21\u578b\u5c06\u5bc6\u96c6\u89c6\u89c9\u8f93\u5165\u8f6c\u6362\u4e3a\u7a00\u758f\u3001\u5224\u522b\u6027\u7f16\u7801\uff0c\u652f\u6301\u8de8\u4efb\u52a1\u91cd\u7528\u3002\u5b9e\u73b0\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e24\u79cd\u7248\u672c\u3002", "result": "\u6a21\u578b\u5728\u82b1\u6735\u8bc6\u522b\u4efb\u52a1\u548c\u81ea\u7136\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u751f\u6210\u53ef\u9760\u7684\u7a00\u758f\u7f16\u7801\uff0c\u80fd\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u7684\u8f93\u5165\u3002\u5728\u6a21\u62df\u5b9a\u4f4d\u8bbe\u7f6e\u4e2d\uff0c\u4f18\u4e8e\u7b80\u5355\u7684\u56fe\u50cf\u4e0b\u91c7\u6837\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u5904\u7406\u901a\u8def\u7684\u529f\u80fd\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u6606\u866b\u8ba1\u7b97\u5efa\u6a21\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u751f\u7269\u542f\u53d1\u89c6\u89c9\u6a21\u578b\uff0c\u80fd\u591f\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a00\u758f\u8ba1\u7b97\uff0c\u4e3a\u7406\u89e3\u751f\u7269\u89c6\u89c9\u5904\u7406\u548c\u5f00\u53d1\u9ad8\u6548\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.06353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06353", "abs": "https://arxiv.org/abs/2602.06353", "authors": ["Jia-Le Xu", "Shen-Huan Lyu", "Yu-Nian Wang", "Ning Chen", "Zhihao Qu", "Bin Tang", "Baoliu Ye"], "title": "Enhance and Reuse: A Dual-Mechanism Approach to Boost Deep Forest for Label Distribution Learning", "comment": null, "summary": "Label distribution learning (LDL) requires the learner to predict the degree of correlation between each sample and each label. To achieve this, a crucial task during learning is to leverage the correlation among labels. Deep Forest (DF) is a deep learning framework based on tree ensembles, whose training phase does not rely on backpropagation. DF performs in-model feature transform using the prediction of each layer and achieves competitive performance on many tasks. However, its exploration in the field of LDL is still in its infancy. The few existing methods that apply DF to the field of LDL do not have effective ways to utilize the correlation among labels. Therefore, we propose a method named Enhanced and Reused Feature Deep Forest (ERDF). It mainly contains two mechanisms: feature enhancement exploiting label correlation and measure-aware feature reuse. The first one is to utilize the correlation among labels to enhance the original features, enabling the samples to acquire more comprehensive information for the task of LDL. The second one performs a reuse operation on the features of samples that perform worse than the previous layer on the validation set, in order to ensure the stability of the training process. This kind of Enhance-Reuse pattern not only enables samples to enrich their features but also validates the effectiveness of their new features and conducts a reuse process to prevent the noise from spreading further. Experiments show that our method outperforms other comparison algorithms on six evaluation metrics.", "AI": {"tldr": "\u63d0\u51faERDF\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u7b7e\u76f8\u5173\u6027\u7279\u5f81\u589e\u5f3a\u548c\u5ea6\u91cf\u611f\u77e5\u7279\u5f81\u91cd\u7528\u673a\u5236\uff0c\u5c06\u6df1\u5ea6\u68ee\u6797\u5e94\u7528\u4e8e\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u9700\u8981\u5229\u7528\u6807\u7b7e\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u68ee\u6797\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u7f3a\u4e4f\u6709\u6548\u673a\u5236\u3002\u6df1\u5ea6\u68ee\u6797\u4f5c\u4e3a\u975e\u53cd\u5411\u4f20\u64ad\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5728LDL\u9886\u57df\u7684\u63a2\u7d22\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u63d0\u51faERDF\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u5229\u7528\u6807\u7b7e\u76f8\u5173\u6027\u589e\u5f3a\u539f\u59cb\u7279\u5f81\uff0c\u4f7f\u6837\u672c\u83b7\u5f97\u66f4\u5168\u9762\u7684\u4fe1\u606f\uff1b2\uff09\u5bf9\u9a8c\u8bc1\u96c6\u4e0a\u8868\u73b0\u4e0b\u964d\u7684\u6837\u672c\u7279\u5f81\u8fdb\u884c\u91cd\u7528\u64cd\u4f5c\uff0c\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u516d\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\uff0cERDF\u65b9\u6cd5\u5747\u4f18\u4e8e\u5176\u4ed6\u5bf9\u6bd4\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ERDF\u901a\u8fc7\u589e\u5f3a-\u91cd\u7528\u6a21\u5f0f\uff0c\u4e0d\u4ec5\u4e30\u5bcc\u4e86\u6837\u672c\u7279\u5f81\uff0c\u8fd8\u9a8c\u8bc1\u4e86\u65b0\u7279\u5f81\u7684\u6709\u6548\u6027\u5e76\u9632\u6b62\u566a\u58f0\u4f20\u64ad\uff0c\u4e3a\u6df1\u5ea6\u68ee\u6797\u5728\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.06429", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2602.06429", "abs": "https://arxiv.org/abs/2602.06429", "authors": ["Jasper A. Vrugt", "Jonathan M. Frame", "Ethan Bollman"], "title": "Reclaiming First Principles: A Differentiable Framework for Conceptual Hydrologic Models", "comment": "85 pages, 14 figures", "summary": "Conceptual hydrologic models remain the cornerstone of rainfall-runoff modeling, yet their calibration is often slow and numerically fragile. Most gradient-based parameter estimation methods rely on finite-difference approximations or automatic differentiation frameworks (e.g., JAX, PyTorch and TensorFlow), which are computationally demanding and introduce truncation errors, solver instabilities, and substantial overhead. These limitations are particularly acute for the ODE systems of conceptual watershed models. Here we introduce a fully analytic and computationally efficient framework for differentiable hydrologic modeling based on exact parameter sensitivities. By augmenting the governing ODE system with sensitivity equations, we jointly evolve the model states and the Jacobian matrix with respect to all parameters. This Jacobian then provides fully analytic gradient vectors for any differentiable loss function. These include classical objective functions such as the sum of absolute and squared residuals, widely used hydrologic performance metrics such as the Nash-Sutcliffe and Kling-Gupta efficiencies, robust loss functions that down-weight extreme events, and hydrograph-based functionals such as flow-duration and recession curves. The analytic sensitivities eliminate the step-size dependence and noise inherent to numerical differentiation, while avoiding the instability of adjoint methods and the overhead of modern machine-learning autodiff toolchains. The resulting gradients are deterministic, physically interpretable, and straightforward to embed in gradient-based optimizers. Overall, this work enables rapid, stable, and transparent gradient-based calibration of conceptual hydrologic models, unlocking the full potential of differentiable modeling without reliance on external, opaque, or CPU-intensive automatic-differentiation libraries.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7cbe\u786e\u53c2\u6570\u654f\u611f\u6027\u7684\u5168\u89e3\u6790\u53ef\u5fae\u6c34\u6587\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5e7fODE\u7cfb\u7edf\u8054\u5408\u6f14\u5316\u6a21\u578b\u72b6\u6001\u548c\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u4e3a\u6982\u5ff5\u6c34\u6587\u6a21\u578b\u63d0\u4f9b\u5feb\u901f\u3001\u7a33\u5b9a\u3001\u900f\u660e\u7684\u68af\u5ea6\u6821\u51c6\u65b9\u6cd5\u3002", "motivation": "\u6982\u5ff5\u6c34\u6587\u6a21\u578b\u7684\u6821\u51c6\u901a\u5e38\u7f13\u6162\u4e14\u6570\u503c\u8106\u5f31\uff0c\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff08\u6709\u9650\u5dee\u5206\u6216\u81ea\u52a8\u5fae\u5206\uff09\u8ba1\u7b97\u91cf\u5927\uff0c\u5b58\u5728\u622a\u65ad\u8bef\u5dee\u3001\u6c42\u89e3\u5668\u4e0d\u7a33\u5b9a\u548c\u663e\u8457\u5f00\u9500\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u5bf9\u4e8e\u6982\u5ff5\u6d41\u57df\u6a21\u578b\u7684ODE\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u589e\u5e7f\u63a7\u5236ODE\u7cfb\u7edf\uff0c\u52a0\u5165\u654f\u611f\u6027\u65b9\u7a0b\uff0c\u8054\u5408\u6f14\u5316\u6a21\u578b\u72b6\u6001\u548c\u5173\u4e8e\u6240\u6709\u53c2\u6570\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u3002\u8be5\u96c5\u53ef\u6bd4\u77e9\u9635\u4e3a\u4efb\u4f55\u53ef\u5fae\u635f\u5931\u51fd\u6570\u63d0\u4f9b\u5b8c\u5168\u89e3\u6790\u7684\u68af\u5ea6\u5411\u91cf\uff0c\u5305\u62ec\u7ecf\u5178\u76ee\u6807\u51fd\u6570\u3001\u6c34\u6587\u6027\u80fd\u6307\u6807\u3001\u9c81\u68d2\u635f\u5931\u51fd\u6570\u548c\u6c34\u6587\u8fc7\u7a0b\u7ebf\u51fd\u6570\u3002", "result": "\u89e3\u6790\u654f\u611f\u6027\u6d88\u9664\u4e86\u6570\u503c\u5fae\u5206\u7684\u6b65\u957f\u4f9d\u8d56\u6027\u548c\u566a\u58f0\uff0c\u907f\u514d\u4e86\u4f34\u968f\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u81ea\u52a8\u5fae\u5206\u5de5\u5177\u94fe\u7684\u5f00\u9500\u3002\u6240\u5f97\u68af\u5ea6\u5177\u6709\u786e\u5b9a\u6027\u3001\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u6613\u4e8e\u5d4c\u5165\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5668\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u6982\u5ff5\u6c34\u6587\u6a21\u578b\u7684\u5feb\u901f\u3001\u7a33\u5b9a\u3001\u900f\u660e\u7684\u68af\u5ea6\u6821\u51c6\uff0c\u91ca\u653e\u4e86\u53ef\u5fae\u5efa\u6a21\u7684\u5168\u90e8\u6f5c\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u3001\u4e0d\u900f\u660e\u6216CPU\u5bc6\u96c6\u7684\u81ea\u52a8\u5fae\u5206\u5e93\u3002"}}
{"id": "2602.06523", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.06523", "abs": "https://arxiv.org/abs/2602.06523", "authors": ["Mridankan Mandal"], "title": "MicroBi-ConvLSTM: An Ultra-Lightweight Efficient Model for Human Activity Recognition on Resource Constrained Devices", "comment": null, "summary": "Human Activity Recognition (HAR) on resource constrained wearables requires models that balance accuracy against strict memory and computational budgets. State of the art lightweight architectures such as TinierHAR (34K parameters) and TinyHAR (55K parameters) achieve strong accuracy, but exceed memory budgets of microcontrollers with limited SRAM once operating system overhead is considered. We present MicroBi-ConvLSTM, an ultra-lightweight convolutional-recurrent architecture achieving 11.4K parameters on average through two stage convolutional feature extraction with 4x temporal pooling and a single bidirectional LSTM layer. This represents 2.9x parameter reduction versus TinierHAR and 11.9x versus DeepConvLSTM while preserving linear O(N) complexity. Evaluation across eight diverse HAR benchmarks shows that MicroBi-ConvLSTM maintains competitive performance within the ultra-lightweight regime: 93.41% macro F1 on UCI-HAR, 94.46% on SKODA assembly gestures, and 88.98% on Daphnet gait freeze detection. Systematic ablation reveals task dependent component contributions where bidirectionality benefits episodic event detection, but provides marginal gains on periodic locomotion. INT8 post training quantization incurs only 0.21% average F1-score degradation, yielding a 23.0 KB average deployment footprint suitable for memory constrained edge devices.", "AI": {"tldr": "\u63d0\u51faMicroBi-ConvLSTM\uff0c\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u5377\u79ef-\u5faa\u73af\u67b6\u6784\uff0c\u5e73\u5747\u4ec511.4K\u53c2\u6570\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u8f7b\u91cf\u6a21\u578b\u51cf\u5c112.9-11.9\u500d\u53c2\u6570\uff0c\u57288\u4e2aHAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u91cf\u5316\u540e\u4ec523KB\u90e8\u7f72\u5927\u5c0f\uff0c\u9002\u5408\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8f7b\u91cf\u7ea7HAR\u6a21\u578b\uff08\u5982TinierHAR\u548cTinyHAR\uff09\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5728\u8003\u8651\u64cd\u4f5c\u7cfb\u7edf\u5f00\u9500\u540e\u4ecd\u8d85\u51fa\u5fae\u63a7\u5236\u5668\u7684\u5185\u5b58\u9650\u5236\u3002\u9700\u8981\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u8fdb\u4e00\u6b65\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u4ee5\u9002\u5e94\u8d44\u6e90\u53d7\u9650\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5377\u79ef\u7279\u5f81\u63d0\u53d6\uff08\u5e264\u500d\u65f6\u95f4\u6c60\u5316\uff09\u548c\u5355\u5c42\u53cc\u5411LSTM\u7684\u5377\u79ef-\u5faa\u73af\u67b6\u6784\uff0c\u5e73\u5747\u4ec511.4K\u53c2\u6570\u3002\u4fdd\u6301\u7ebf\u6027O(N)\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7INT8\u540e\u8bad\u7ec3\u91cf\u5316\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u3002", "result": "\u57288\u4e2a\u591a\u6837\u5316HAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aUCI-HAR\u4e0a93.41%\u5b8fF1\uff0cSKODA\u88c5\u914d\u624b\u52bf94.46%\uff0cDaphnet\u6b65\u6001\u51bb\u7ed3\u68c0\u6d4b88.98%\u3002\u91cf\u5316\u540e\u5e73\u5747F1\u5206\u6570\u4ec5\u4e0b\u964d0.21%\uff0c\u90e8\u7f72\u5927\u5c0f\u5e73\u574723.0KB\u3002", "conclusion": "MicroBi-ConvLSTM\u6210\u529f\u5b9e\u73b0\u4e86\u8d85\u8f7b\u91cf\u7ea7HAR\u6a21\u578b\u8bbe\u8ba1\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\uff082.9-11.9\u500d\uff09\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u7279\u522b\u9002\u5408\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\uff0c\u53cc\u5411\u6027\u5bf9\u4e8b\u4ef6\u68c0\u6d4b\u4efb\u52a1\u5c24\u5176\u6709\u76ca\u3002"}}
{"id": "2602.06456", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06456", "abs": "https://arxiv.org/abs/2602.06456", "authors": ["Brandon Gower-Winter", "Misja Groen", "Georg Krempl"], "title": "The Window Dilemma: Why Concept Drift Detection is Ill-Posed", "comment": "12 pages, 1 Figure, 5 Tables. Accepted to the 24th International Symposium on Intelligent Data Analysis (IDA) (2026)", "summary": "Non-stationarity of an underlying data generating process that leads to distributional changes over time is a key characteristic of Data Streams. This phenomenon, commonly referred to as Concept Drift, has been intensively studied, and Concept Drift Detectors have been established as a class of methods for detecting such changes (drifts). For the most part, Drift Detectors compare regions (windows) of the data stream and detect drift if those windows are sufficiently dissimilar.\n  In this work, we introduce the Window Dilemma, an observation that perceived drift is a product of windowing and not necessarily the underlying data generating process. Additionally, we highlight that drift detection is ill-posed, primarily because verification of drift events are implausible in practice. We demonstrate these contributions first by an illustrative example, followed by empirical comparisons of drift detectors against a variety of alternative adaptation strategies. Our main finding is that traditional batch learning techniques often perform better than their drift-aware counterparts further bringing into question the purpose of detectors in Stream Classification.", "AI": {"tldr": "\u4f20\u7edf\u6f02\u79fb\u68c0\u6d4b\u5668\u5b58\u5728\u7a97\u53e3\u56f0\u5883\u95ee\u9898\uff0c\u6f02\u79fb\u68c0\u6d4b\u672c\u8eab\u5b9a\u4e49\u4e0d\u660e\u786e\uff0c\u5b9e\u9a8c\u8868\u660e\u4f20\u7edf\u6279\u91cf\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f18\u4e8e\u6f02\u79fb\u611f\u77e5\u65b9\u6cd5", "motivation": "\u6570\u636e\u6d41\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u73b0\u8c61\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u6f02\u79fb\u68c0\u6d4b\u5668\u901a\u8fc7\u6bd4\u8f83\u6570\u636e\u7a97\u53e3\u7684\u5dee\u5f02\u6765\u68c0\u6d4b\u53d8\u5316\u3002\u4f46\u4f5c\u8005\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\uff1a\u68c0\u6d4b\u5230\u7684\u6f02\u79fb\u53ef\u80fd\u662f\u7a97\u53e3\u9009\u62e9\u7684\u4ea7\u7269\u800c\u975e\u771f\u5b9e\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u53d8\u5316\uff0c\u4e14\u6f02\u79fb\u68c0\u6d4b\u5728\u5b9e\u8df5\u4e2d\u96be\u4ee5\u9a8c\u8bc1\u3002", "method": "\u9996\u5148\u901a\u8fc7\u793a\u4f8b\u8bf4\u660e\u7a97\u53e3\u56f0\u5883\u95ee\u9898\uff0c\u7136\u540e\u5bf9\u591a\u79cd\u6f02\u79fb\u68c0\u6d4b\u5668\u4e0e\u66ff\u4ee3\u9002\u5e94\u7b56\u7565\u8fdb\u884c\u5b9e\u8bc1\u6bd4\u8f83\u3002\u7814\u7a76\u5c55\u793a\u4e86\u6f02\u79fb\u68c0\u6d4b\u7684\u56fa\u6709\u95ee\u9898\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u6d41\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u662f\u4f20\u7edf\u6279\u91cf\u5b66\u4e60\u6280\u672f\u901a\u5e38\u4f18\u4e8e\u6f02\u79fb\u611f\u77e5\u65b9\u6cd5\uff0c\u8fd9\u8fdb\u4e00\u6b65\u8d28\u7591\u4e86\u6f02\u79fb\u68c0\u6d4b\u5668\u5728\u6d41\u5206\u7c7b\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u7a97\u53e3\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u6f02\u79fb\u68c0\u6d4b\u7ed3\u679c\uff0c\u800c\u9a8c\u8bc1\u6f02\u79fb\u4e8b\u4ef6\u5728\u5b9e\u8df5\u4e2d\u51e0\u4e4e\u4e0d\u53ef\u80fd\u3002", "conclusion": "\u6f02\u79fb\u68c0\u6d4b\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff1a\u7a97\u53e3\u56f0\u5883\u4f7f\u5f97\u68c0\u6d4b\u7ed3\u679c\u4e0d\u53ef\u9760\uff0c\u4e14\u7f3a\u4e4f\u5b9e\u9645\u9a8c\u8bc1\u673a\u5236\u3002\u4f20\u7edf\u6279\u91cf\u5b66\u4e60\u65b9\u6cd5\u5728\u6d41\u5206\u7c7b\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u5efa\u8bae\u91cd\u65b0\u8bc4\u4f30\u6f02\u79fb\u68c0\u6d4b\u5668\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.06627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06627", "abs": "https://arxiv.org/abs/2602.06627", "authors": ["Gaurish Trivedi", "Alakh Sharma", "Kartikey Singh Bhandari", "Yash Sinha", "Pratik Narang", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "Trust Regions Sell, But Who's Buying? Overlap Geometry as an Alternative Trust Region for Policy Optimization", "comment": "Under Review", "summary": "Standard trust-region methods constrain policy updates via Kullback-Leibler (KL) divergence. However, KL controls only an average divergence and does not directly prevent rare, large likelihood-ratio excursions that destabilize training--precisely the failure mode that motivates heuristics such as PPO's clipping. We propose overlap geometry as an alternative trust region, constraining distributional overlap via the Bhattacharyya coefficient (closely related to the Hellinger/Renyi-1/2 geometry). This objective penalizes separation in the ratio tails, yielding tighter control over likelihood-ratio excursions without relying on total variation bounds that can be loose in tail regimes. We derive Bhattacharyya-TRPO (BTRPO) and Bhattacharyya-PPO (BPPO), enforcing overlap constraints via square-root ratio updates: BPPO clips the square-root ratio q = sqrt(r), and BTRPO applies a quadratic Hellinger/Bhattacharyya penalty. Empirically, overlap-based updates improve robustness and aggregate performance as measured by RLiable under matched training budgets, suggesting overlap constraints as a practical, principled alternative to KL for stable policy optimization.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5e03\u91cd\u53e0\u51e0\u4f55\u7684\u4fe1\u4efb\u533a\u57df\u65b9\u6cd5\uff0c\u7528Bhattacharyya\u7cfb\u6570\u66ff\u4ee3KL\u6563\u5ea6\uff0c\u901a\u8fc7\u63a7\u5236\u5e73\u65b9\u6839\u4f3c\u7136\u6bd4\u6765\u9632\u6b62\u8bad\u7ec3\u4e0d\u7a33\u5b9a", "motivation": "\u6807\u51c6KL\u4fe1\u4efb\u533a\u57df\u65b9\u6cd5\u53ea\u63a7\u5236\u5e73\u5747\u6563\u5ea6\uff0c\u65e0\u6cd5\u9632\u6b62\u7f55\u89c1\u4f46\u7834\u574f\u6027\u5927\u7684\u4f3c\u7136\u6bd4\u504f\u79fb\uff0c\u8fd9\u6b63\u662fPPO\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\u8bd5\u56fe\u89e3\u51b3\u7684\u95ee\u9898", "method": "\u63d0\u51faBhattacharyya-TRPO\u548cBhattacharyya-PPO\uff0c\u901a\u8fc7\u5e73\u65b9\u6839\u4f3c\u7136\u6bd4\u66f4\u65b0\u6765\u7ea6\u675f\u5206\u5e03\u91cd\u53e0\uff0cBPPO\u5bf9\u5e73\u65b9\u6839\u6bd4\u8fdb\u884c\u88c1\u526a\uff0cBTRPO\u5e94\u7528\u4e8c\u6b21Hellinger/Bhattacharyya\u60e9\u7f5a", "result": "\u5728\u76f8\u540c\u8bad\u7ec3\u9884\u7b97\u4e0b\uff0c\u57fa\u4e8e\u91cd\u53e0\u7684\u66f4\u65b0\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u805a\u5408\u6027\u80fd\uff08\u901a\u8fc7RLiable\u8bc4\u4f30\uff09", "conclusion": "\u91cd\u53e0\u7ea6\u675f\u662fKL\u6563\u5ea6\u7684\u4e00\u4e2a\u5b9e\u7528\u4e14\u6709\u7406\u8bba\u4f9d\u636e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u7a33\u5b9a\u7b56\u7565\u4f18\u5316"}}
{"id": "2602.06788", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06788", "abs": "https://arxiv.org/abs/2602.06788", "authors": ["Idan Pipano", "Shoham Sabach", "Kavosh Asadi", "Mohammad Ghavamzadeh"], "title": "Displacement-Resistant Extensions of DPO with Nonconvex $f$-Divergences", "comment": "Published as a conference paper at ICLR 2026", "summary": "DPO and related algorithms align language models by directly optimizing the RLHF objective: find a policy that maximizes the Bradley-Terry reward while staying close to a reference policy through a KL divergence penalty. Previous work showed that this approach could be further generalized: the original problem remains tractable even if the KL divergence is replaced by a family of $f$-divergence with a convex generating function $f$. Our first contribution is to show that convexity of $f$ is not essential. Instead, we identify a more general condition, referred to as DPO-inducing, that precisely characterizes when the RLHF problem remains tractable. Our next contribution is to establish a second condition on $f$ that is necessary to prevent probability displacement, a known empirical phenomenon in which the probabilities of the winner and the loser responses approach zero. We refer to any $f$ that satisfies this condition as displacement-resistant. We finally focus on a specific DPO-inducing and displacement-resistant $f$, leading to our novel SquaredPO loss. Compared to DPO, this new loss offers stronger theoretical guarantees while performing competitively in practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5e7f\u4e49\u7684RLHF\u4f18\u5316\u6846\u67b6\uff0c\u653e\u5bbd\u4e86f-\u6563\u5ea6\u7684\u51f8\u6027\u8981\u6c42\uff0c\u5b9a\u4e49\u4e86DPO-inducing\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u6b62\u6982\u7387\u4f4d\u79fb\u7684displacement-resistant\u6761\u4ef6\uff0c\u6700\u7ec8\u57fa\u4e8e\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u6761\u4ef6\u7684f\u51fd\u6570\u5f00\u53d1\u4e86\u65b0\u7684SquaredPO\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u73b0\u6709DPO\u53ca\u76f8\u5173\u7b97\u6cd5\u901a\u8fc7KL\u6563\u5ea6\u60e9\u7f5a\u6765\u5bf9\u9f50\u8bed\u8a00\u6a21\u578b\uff0c\u4f46KL\u6563\u5ea6\u53ea\u662ff-\u6563\u5ea6\u7684\u4e00\u79cd\u7279\u4f8b\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u5c06KL\u63a8\u5e7f\u5230\u51f8f-\u6563\u5ea6\uff0c\u4f46\u51f8\u6027\u9650\u5236\u53ef\u80fd\u8fc7\u4e8e\u4e25\u683c\u3002\u672c\u6587\u65e8\u5728\u5bfb\u627e\u66f4\u4e00\u822c\u7684\u6761\u4ef6\uff0c\u4f7fRLHF\u95ee\u9898\u4fdd\u6301\u53ef\u89e3\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u6982\u7387\u4f4d\u79fb\u8fd9\u4e00\u5b9e\u9645\u95ee\u9898\u3002", "method": "1. \u63d0\u51faDPO-inducing\u6761\u4ef6\uff0c\u7cbe\u786e\u523b\u753bRLHF\u95ee\u9898\u4fdd\u6301\u53ef\u89e3\u6027\u7684f\u51fd\u6570\u7279\u5f81\uff0c\u653e\u5bbd\u4e86\u51f8\u6027\u8981\u6c42\u30022. \u5efa\u7acbdisplacement-resistant\u6761\u4ef6\uff0c\u9632\u6b62\u6982\u7387\u4f4d\u79fb\u73b0\u8c61\u30023. \u57fa\u4e8e\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u6761\u4ef6\u7684\u7279\u5b9af\u51fd\u6570\uff0c\u5f00\u53d1\u65b0\u7684SquaredPO\u635f\u5931\u51fd\u6570\u3002", "result": "1. \u8bc1\u660e\u4e86\u51f8\u6027\u4e0d\u662fRLHF\u95ee\u9898\u53ef\u89e3\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u63d0\u51fa\u4e86\u66f4\u4e00\u822c\u7684DPO-inducing\u6761\u4ef6\u30022. \u8bc6\u522b\u4e86\u9632\u6b62\u6982\u7387\u4f4d\u79fb\u7684displacement-resistant\u6761\u4ef6\u30023. SquaredPO\u635f\u5931\u5728\u7406\u8bba\u4e0a\u6bd4DPO\u6709\u66f4\u5f3a\u7684\u4fdd\u8bc1\uff0c\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u672c\u6587\u6269\u5c55\u4e86RLHF\u4f18\u5316\u6846\u67b6\u7684\u7406\u8bba\u57fa\u7840\uff0c\u653e\u5bbd\u4e86f-\u6563\u5ea6\u7684\u51f8\u6027\u8981\u6c42\uff0c\u63d0\u51fa\u4e86\u9632\u6b62\u6982\u7387\u4f4d\u79fb\u7684\u6761\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86\u5177\u6709\u66f4\u5f3a\u7406\u8bba\u4fdd\u8bc1\u7684SquaredPO\u635f\u5931\u51fd\u6570\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u7a33\u5065\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.06791", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.06791", "abs": "https://arxiv.org/abs/2602.06791", "authors": ["Jake McAllister Dorman", "Edward Gillman", "Dominic C. Rose", "Jamie F. Mair", "Juan P. Garrahan"], "title": "Rare Event Analysis of Large Language Models", "comment": null, "summary": "Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7f55\u89c1\u4e8b\u4ef6\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u62ec\u7406\u8bba\u3001\u9ad8\u6548\u751f\u6210\u7b56\u7565\u3001\u6982\u7387\u4f30\u8ba1\u548c\u8bef\u5dee\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86\u5177\u4f53\u5e94\u7528\u793a\u4f8b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6982\u7387\u6a21\u578b\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u7f55\u89c1\u4e8b\u4ef6\u2014\u2014\u8fd9\u4e9b\u884c\u4e3a\u8fdc\u79bb\u5178\u578b\u4f46\u975e\u5e38\u91cd\u8981\u3002\u7531\u4e8e\u7f55\u89c1\u4e8b\u4ef6\u7684\u5b9a\u4e49\u7279\u6027\uff0c\u5b83\u4eec\u5f88\u96be\u88ab\u89c2\u5bdf\u5230\uff0c\u4f46LLM\u7684\u5de8\u5927\u4f7f\u7528\u89c4\u6a21\u610f\u5473\u7740\u5728\u5f00\u53d1\u9636\u6bb5\u5b8c\u5168\u672a\u89c2\u5bdf\u5230\u7684\u4e8b\u4ef6\u5f88\u53ef\u80fd\u5728\u90e8\u7f72\u540e\u53d8\u5f97\u663e\u8457\u3002\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u7f55\u89c1\u4e8b\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u7406\u8bba\u57fa\u7840\u3001\u9ad8\u6548\u751f\u6210\u7b56\u7565\u3001\u6982\u7387\u4f30\u8ba1\u65b9\u6cd5\u548c\u8bef\u5dee\u5206\u6790\u3002\u63d0\u4f9b\u4e86\u5b9e\u9645\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u5177\u4f53\u793a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u3001\u751f\u6210\u548c\u8bc4\u4f30LLM\u4e2d\u7684\u7f55\u89c1\u4e8b\u4ef6\uff0c\u4e3a\u7406\u89e3\u548c\u5904\u7406\u8fd9\u4e9b\u91cd\u8981\u4f46\u96be\u4ee5\u89c2\u5bdf\u7684\u73b0\u8c61\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u6a21\u578b\u548c\u4e0a\u4e0b\u6587\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u6982\u5ff5\u548c\u6280\u672f\u7684\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3aLLM\u7f55\u89c1\u4e8b\u4ef6\u5206\u6790\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u8bba\u3002"}}
