{"id": "2507.12553", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12553", "abs": "https://arxiv.org/abs/2507.12553", "authors": ["Michael A. Lepori", "Jennifer Hu", "Ishita Dasgupta", "Roma Patel", "Thomas Serre", "Ellie Pavlick"], "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility", "comment": null, "summary": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u80fd\u591f\u533a\u5206\u6a21\u6001\u7c7b\u522b\uff08\u53ef\u80fd\u3001\u4e0d\u53ef\u80fd\u3001\u65e0\u610f\u4e49\u7b49\uff09\u7684\u7ebf\u6027\u8868\u793a\uff08\u6a21\u6001\u5dee\u5f02\u5411\u91cf\uff09\uff0c\u8fd9\u4e9b\u5411\u91cf\u80fd\u591f\u9884\u6d4b\u4eba\u7c7b\u7684\u6a21\u6001\u5206\u7c7b\u884c\u4e3a\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u7684\u6a21\u6001\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8d28\u7591\u8bed\u8a00\u6a21\u578b\u6309\u6a21\u6001\u5206\u7c7b\u53e5\u5b50\u7684\u80fd\u529b\uff0c\u4f46\u8bed\u8a00\u6a21\u578b\u9700\u8981\u80fd\u591f\u8fa8\u522b\u53e5\u5b50\u7684\u6a21\u6001\u7c7b\u522b\uff08\u53ef\u80fd\u3001\u4e0d\u53ef\u80fd\u3001\u5b8c\u5168\u65e0\u610f\u4e49\u7b49\uff09\u624d\u80fd\u53ef\u9760\u5730\u5b8c\u6210\u5404\u79cd\u4efb\u52a1\u3002\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u6001\u5206\u7c7b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u5728\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u4e2d\u8bc6\u522b\u80fd\u591f\u533a\u5206\u6a21\u6001\u7c7b\u522b\u7684\u7ebf\u6027\u8868\u793a\uff08\u6a21\u6001\u5dee\u5f02\u5411\u91cf\uff09\uff0c\u5206\u6790\u8fd9\u4e9b\u5411\u91cf\u5728\u4e0d\u540c\u8bad\u7ec3\u6b65\u9aa4\u3001\u5c42\u6570\u548c\u53c2\u6570\u89c4\u6a21\u4e0b\u7684\u51fa\u73b0\u6a21\u5f0f\uff0c\u5e76\u5c06\u5411\u91cf\u6295\u5f71\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u5bf9\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u8bc4\u5206\u8fdb\u884c\u5173\u8054\u5206\u6790\u3002", "result": "\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5177\u6709\u6bd4\u4e4b\u524d\u62a5\u544a\u66f4\u53ef\u9760\u7684\u6a21\u6001\u5206\u7c7b\u5224\u65ad\u80fd\u529b\uff1b\u6a21\u6001\u5dee\u5f02\u5411\u91cf\u968f\u7740\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff08\u8bad\u7ec3\u6b65\u9aa4\u3001\u5c42\u6570\u3001\u53c2\u6570\u6570\u91cf\uff09\u4ee5\u4e00\u81f4\u7684\u987a\u5e8f\u51fa\u73b0\uff1b\u6a21\u6001\u5dee\u5f02\u5411\u91cf\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u7ec6\u7c92\u5ea6\u7684\u5206\u7c7b\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u83b7\u5f97\u4e86\u8bed\u8a00\u6a21\u578b\u6a21\u6001\u5206\u7c7b\u7684\u65b0\u89c1\u89e3\uff0c\u6a21\u6001\u5dee\u5f02\u5411\u91cf\u4e0d\u4ec5\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7684\u6a21\u6001\u8868\u793a\u673a\u5236\uff0c\u8fd8\u6709\u6f5c\u529b\u5e2e\u52a9\u7406\u89e3\u4eba\u7c7b\u7684\u6a21\u6001\u5206\u7c7b\u8fc7\u7a0b\u3002"}}
{"id": "2507.12663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12663", "abs": "https://arxiv.org/abs/2507.12663", "authors": ["Inamullah", "Ernesto Elias Vidal Rosas", "Imran Razzak", "Shoaib Jameel"], "title": "Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort", "comment": null, "summary": "Cardiovascular disease (CVD) remains the leading global cause of mortality,\nyet current risk stratification methods often fail to detect early, subclinical\nchanges. Previous studies have generally not integrated retinal\nmicrovasculature characteristics with comprehensive serum lipidomic profiles as\npotential indicators of CVD risk. In this study, an innovative imaging omics\nframework was introduced, combining retinal microvascular traits derived\nthrough deep learning based image processing with serum lipidomic data to\nhighlight asymptomatic biomarkers of cardiovascular risk beyond the\nconventional lipid panel. This represents the first large scale, covariate\nadjusted and stratified correlation analysis conducted in a healthy population,\nwhich is essential for identifying early indicators of disease. Retinal\nphenotypes were quantified using automated image analysis tools, while serum\nlipid profiling was performed by Ultra High Performance Liquid Chromatography\nElectrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).\nStrong, age- and sex-independent correlations were established, particularly\nbetween average artery width, vessel density, and lipid subclasses such as\ntriacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These\nassociations suggest a converging mechanism of microvascular remodeling under\nmetabolic stress. By linking detailed\n  vascular structural phenotypes to specific lipid species, this study fills a\ncritical gap in the understanding of early CVD pathogenesis. This integration\nnot only offers a novel perspective on microvascular metabolic associations but\nalso presents a significant opportunity for the identification of robust,\nnon-invasive biomarkers. Ultimately, these findings may support improved early\ndetection, targeted prevention, and personalized approaches in cardiovascular\nhealthcare.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u7f51\u819c\u5fae\u8840\u7ba1\u7279\u5f81\u4e0e\u8840\u6e05\u8102\u8d28\u7ec4\u5b66\u6570\u636e\u7684\u521b\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u65e9\u671f\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVD\uff09\u98ce\u9669\u7684\u65e0\u75c7\u72b6\u751f\u7269\u6807\u5fd7\u7269\u8bc6\u522b\u3002", "motivation": "\u5f53\u524d\u5fc3\u8840\u7ba1\u75be\u75c5\u98ce\u9669\u5206\u5c42\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u65e9\u671f\u4e9a\u4e34\u5e8a\u53d8\u5316\uff0c\u4e14\u7f3a\u4e4f\u89c6\u7f51\u819c\u5fae\u8840\u7ba1\u7279\u5f81\u4e0e\u8840\u6e05\u8102\u8d28\u7ec4\u5b66\u6570\u636e\u7684\u6574\u5408\u7814\u7a76\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u5904\u7406\u6280\u672f\u91cf\u5316\u89c6\u7f51\u819c\u5fae\u8840\u7ba1\u7279\u5f81\uff0c\u7ed3\u5408\u8d85\u9ad8\u6548\u6db2\u76f8\u8272\u8c31\u7535\u55b7\u96fe\u9ad8\u5206\u8fa8\u8d28\u8c31\uff08UHPLC ESI HRMS\uff09\u5206\u6790\u8840\u6e05\u8102\u8d28\u7ec4\u5b66\u6570\u636e\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u534f\u53d8\u91cf\u8c03\u6574\u548c\u5206\u5c42\u76f8\u5173\u5206\u6790\u3002", "result": "\u53d1\u73b0\u89c6\u7f51\u819c\u5fae\u8840\u7ba1\u7279\u5f81\uff08\u5982\u5e73\u5747\u52a8\u8109\u5bbd\u5ea6\u3001\u8840\u7ba1\u5bc6\u5ea6\uff09\u4e0e\u7279\u5b9a\u8102\u8d28\u4e9a\u7c7b\uff08\u5982TAGs\u3001DAGs\u3001Cers\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\uff0c\u63d0\u793a\u4ee3\u8c22\u538b\u529b\u4e0b\u7684\u5fae\u8840\u7ba1\u91cd\u5851\u673a\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u65e9\u671fCVD\u53d1\u75c5\u673a\u5236\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u4e3a\u65e0\u521b\u751f\u7269\u6807\u5fd7\u7269\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u671b\u6539\u5584\u65e9\u671f\u68c0\u6d4b\u548c\u4e2a\u6027\u5316\u9884\u9632\u7b56\u7565\u3002"}}
{"id": "2507.12989", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.12989", "abs": "https://arxiv.org/abs/2507.12989", "authors": ["Lyris Xu", "Fabio Aurelio D'Asaro", "Luke Dickens"], "title": "A Translation of Probabilistic Event Calculus into Markov Decision Processes", "comment": null, "summary": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about\nactions and their effects in uncertain environments, which enables the\nrepresentation of probabilistic narratives and computation of temporal\nprojections. The PEC formalism offers significant advantages in\ninterpretability and expressiveness for narrative reasoning. However, it lacks\nmechanisms for goal-directed reasoning. This paper bridges this gap by\ndeveloping a formal translation of PEC domains into Markov Decision Processes\n(MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's\nflexible action semantics. The resulting PEC-MDP formalism enables the\nextensive collection of algorithms and theoretical tools developed for MDPs to\nbe applied to PEC's interpretable narrative domains. We demonstrate how the\ntranslation supports both temporal reasoning tasks and objective-driven\nplanning, with methods for mapping learned policies back into human-readable\nPEC representations, maintaining interpretability while extending PEC's\ncapabilities.", "AI": {"tldr": "\u5c06\u6982\u7387\u4e8b\u4ef6\u6f14\u7b97\uff08PEC\uff09\u8f6c\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u4ee5\u652f\u6301\u76ee\u6807\u5bfc\u5411\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301PEC\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "PEC\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u673a\u5236\uff0c\u9700\u6269\u5c55\u5176\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u201c\u52a8\u4f5c\u6267\u884c\u60c5\u5883\u201d\u5c06PEC\u9886\u57df\u5f62\u5f0f\u5316\u8f6c\u5316\u4e3aMDP\uff0c\u4fdd\u7559PEC\u7684\u7075\u6d3b\u8bed\u4e49\u3002", "result": "PEC-MDP\u5f62\u5f0f\u5316\u652f\u6301\u65f6\u95f4\u63a8\u7406\u548c\u76ee\u6807\u9a71\u52a8\u89c4\u5212\uff0c\u5e76\u80fd\u5c06\u5b66\u4e60\u7b56\u7565\u6620\u5c04\u56de\u53ef\u8bfb\u7684PEC\u8868\u793a\u3002", "conclusion": "PEC-MDP\u7ed3\u5408\u4e86PEC\u7684\u53ef\u89e3\u91ca\u6027\u4e0eMDP\u7684\u7b97\u6cd5\u5de5\u5177\uff0c\u6269\u5c55\u4e86PEC\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.12755", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12755", "abs": "https://arxiv.org/abs/2507.12755", "authors": ["Yanchen Guan", "Haicheng Liao", "Chengyue Wang", "Bonan Wang", "Jiaxun Zhang", "Jia Hu", "Zhenning Li"], "title": "Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation", "comment": null, "summary": "Developing precise and computationally efficient traffic accident\nanticipation system is crucial for contemporary autonomous driving\ntechnologies, enabling timely intervention and loss prevention. In this paper,\nwe propose an accident anticipation framework employing a dual-branch\narchitecture that effectively integrates visual information from dashcam videos\nwith structured textual data derived from accident reports. Furthermore, we\nintroduce a feature aggregation method that facilitates seamless integration of\nmultimodal inputs through large models (GPT-4o, Long-CLIP), complemented by\ntargeted prompt engineering strategies to produce actionable feedback and\nstandardized accident archives. Comprehensive evaluations conducted on\nbenchmark datasets (DAD, CCD, and A3D) validate the superior predictive\naccuracy, enhanced responsiveness, reduced computational overhead, and improved\ninterpretability of our approach, thus establishing a new benchmark for\nstate-of-the-art performance in traffic accident anticipation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5206\u652f\u67b6\u6784\u7684\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u5229\u7528\u5927\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u7cbe\u786e\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53ca\u65f6\u5e72\u9884\u548c\u635f\u5931\u9884\u9632\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u7ed3\u5408\u5927\u6a21\u578b\uff08GPT-4o\u3001Long-CLIP\uff09\u548c\u63d0\u793a\u5de5\u7a0b\u8fdb\u884c\u7279\u5f81\u805a\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3001\u54cd\u5e94\u901f\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13142", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13142", "abs": "https://arxiv.org/abs/2507.13142", "authors": ["Ahmed Bahloul", "Simon Malberg"], "title": "From Roots to Rewards: Dynamic Tree Reasoning with RL", "comment": null, "summary": "Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6539\u8fdb\u9759\u6001\u6811\u7ed3\u6784\u63a8\u7406\u65b9\u6cd5\uff0c\u63d0\u5347\u95ee\u9898\u89e3\u7b54\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u89e3\u7b54\u4e2d\u9762\u4e34\u9519\u8bef\u4f20\u64ad\u548c\u77e5\u8bc6\u6574\u5408\u7684\u6311\u6218\uff0c\u73b0\u6709\u6811\u7ed3\u6784\u63a8\u7406\u65b9\u6cd5\uff08\u5982ProbTree\uff09\u56e0\u9759\u6001\u5b9e\u73b0\u548c\u8ba1\u7b97\u6548\u7387\u4e0d\u8db3\u800c\u53d7\u9650\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u65f6\u6784\u5efa\u63a8\u7406\u6811\u5e76\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u9009\u62e9\u6027\u6269\u5c55\u548c\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5728\u4fdd\u6301\u6982\u7387\u4e25\u8c28\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u89e3\u7b54\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6811\u7ed3\u6784\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e73\u8861\u4e86\u6982\u7387\u6846\u67b6\u7684\u53ef\u9760\u6027\u548c\u73b0\u5b9e\u95ee\u9898\u89e3\u7b54\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.12816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12816", "abs": "https://arxiv.org/abs/2507.12816", "authors": ["Ju-Young Oh", "Ho-Joong Kim", "Seong-Whan Lee"], "title": "FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering", "comment": "SMC 2025", "summary": "Video question answering (VQA) is a multimodal task that requires the\ninterpretation of a video to answer a given question. Existing VQA methods\nprimarily utilize question and answer (Q&A) pairs to learn the spatio-temporal\ncharacteristics of video content. However, these annotations are typically\nevent-centric, which is not enough to capture the broader context of each\nvideo. The absence of essential details such as object types, spatial layouts,\nand descriptive attributes restricts the model to learning only a fragmented\nscene representation. This issue limits the model's capacity for generalization\nand higher-level reasoning. In this paper, we propose a fundamental question\ngeneration with the integration of question embeddings for video question\nanswering (FIQ), a novel approach designed to strengthen the reasoning ability\nof the model by enhancing the fundamental understanding of videos. FIQ\ngenerates Q&A pairs based on descriptions extracted from videos, enriching the\ntraining data with fundamental scene information. Generated Q&A pairs enable\nthe model to understand the primary context, leading to enhanced\ngeneralizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign\nmodule that assists task-specific question embeddings with visual features,\nensuring that essential domain-specific details are preserved to increase the\nadaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate\nthat our FIQ achieves state-of-the-art performance compared to existing\nbaseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFIQ\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u57fa\u4e8e\u89c6\u9891\u63cf\u8ff0\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u589e\u5f3a\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8b\u4ef6\u4e2d\u5fc3\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u9891\u4e2d\u5bf9\u8c61\u7c7b\u578b\u3001\u7a7a\u95f4\u5e03\u5c40\u7b49\u57fa\u7840\u4fe1\u606f\u7684\u6355\u6349\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "FIQ\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u57fa\u4e8e\u89c6\u9891\u63cf\u8ff0\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u7ed3\u5408VQ-CAlign\u6a21\u5757\u5bf9\u9f50\u89c6\u89c9\u7279\u5f81\u4e0e\u95ee\u9898\u5d4c\u5165\u3002", "result": "\u5728SUTD-TrafficQA\u6570\u636e\u96c6\u4e0a\uff0cFIQ\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FIQ\u901a\u8fc7\u589e\u5f3a\u5bf9\u89c6\u9891\u57fa\u7840\u4fe1\u606f\u7684\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.13152", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13152", "abs": "https://arxiv.org/abs/2507.13152", "authors": ["Xiangyu Dong", "Haoran Zhao", "Jiang Gao", "Haozhou Li", "Xiaoguang Ma", "Yaoming Zhou", "Fuhai Chen", "Juan Liu"], "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models", "comment": null, "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u8fdb\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff08SE-VLN\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u3001\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u548c\u53cd\u601d\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5728\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8fdb\u5316\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u77e5\u8bc6\u5e93\u548c\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u7ecf\u9a8c\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u8fdb\u5316\u80fd\u529b\u3002", "method": "SE-VLN\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u5206\u5c42\u8bb0\u5fc6\u6a21\u5757\uff08\u5b58\u50a8\u6210\u529f\u548c\u5931\u8d25\u6848\u4f8b\uff09\u3001\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6a21\u5757\uff08\u591a\u6b65\u51b3\u7b56\uff09\u548c\u53cd\u601d\u6a21\u5757\uff08\u6301\u7eed\u8fdb\u5316\uff09\u3002", "result": "\u5728R2R\u548cREVERSE\u6570\u636e\u96c6\u4e0a\uff0c\u5bfc\u822a\u6210\u529f\u7387\u5206\u522b\u8fbe\u523057%\u548c35.2%\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8623.9%\u548c15.0%\u3002", "conclusion": "SE-VLN\u5c55\u793a\u4e86\u4f5c\u4e3a\u81ea\u8fdb\u5316\u6846\u67b6\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6027\u80fd\u968f\u7ecf\u9a8c\u5e93\u589e\u957f\u800c\u63d0\u5347\u3002"}}
{"id": "2507.12948", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12948", "abs": "https://arxiv.org/abs/2507.12948", "authors": ["Weiqiu You", "Anton Xue", "Shreya Havaldar", "Delip Rao", "Helen Jin", "Chris Callison-Burch", "Eric Wong"], "title": "Probabilistic Soundness Guarantees in LLM Reasoning Chains", "comment": null, "summary": "In reasoning chains generated by large language models (LLMs), initial errors\noften propagate and undermine the reliability of the final conclusion. Current\nLLM-based error detection methods often fail to detect propagated errors\nbecause they do not properly account for how earlier errors might corrupt\njudgments of downstream reasoning. To better detect such propagated errors, we\nintroduce Autoregressive Reasoning Entailment Stability (ARES), a novel\nprobabilistic framework that prevents error propagation by judging each claim\nbased only on previously-assessed sound premises. This inductive method yields\na nuanced score for each step and provides certified statistical guarantees of\nits soundness, rather than a brittle binary label. ARES achieves\nstate-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2\npoints) and demonstrates superior robustness on very long synthetic reasoning\nchains, where it excels at detecting propagated errors (90.3% F1, +27.6\npoints).", "AI": {"tldr": "ARES\u662f\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u7684\u524d\u63d0\u5224\u65ad\u6bcf\u4e2a\u4e3b\u5f20\uff0c\u6709\u6548\u68c0\u6d4b\u5e76\u9632\u6b62LLM\u63a8\u7406\u94fe\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u3002", "motivation": "\u5f53\u524dLLM\u7684\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u521d\u59cb\u9519\u8bef\u5728\u63a8\u7406\u94fe\u4e2d\u7684\u4f20\u64ad\u95ee\u9898\uff0c\u5bfc\u81f4\u6700\u7ec8\u7ed3\u8bba\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faAutoregressive Reasoning Entailment Stability (ARES)\u6846\u67b6\uff0c\u91c7\u7528\u5f52\u7eb3\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u751f\u6210\u7ec6\u7c92\u5ea6\u8bc4\u5206\uff0c\u5e76\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\u3002", "result": "ARES\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0872.1% Macro-F1\uff0c\u63d0\u53478.2\u70b9\uff09\uff0c\u5e76\u5728\u957f\u63a8\u7406\u94fe\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0890.3% F1\uff0c\u63d0\u534727.6\u70b9\uff09\u3002", "conclusion": "ARES\u901a\u8fc7\u9632\u6b62\u9519\u8bef\u4f20\u64ad\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u94fe\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.13335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13335", "abs": "https://arxiv.org/abs/2507.13335", "authors": ["Tyler Loakman", "William Thorne", "Chenghua Lin"], "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes", "comment": null, "summary": "Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u91ca\u4e0d\u540c\u7c7b\u578b\u5e7d\u9ed8\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u53ef\u9760\u89e3\u91ca\u590d\u6742\u5e7d\u9ed8\u5f62\u5f0f\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u5e7d\u9ed8\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u53cc\u5173\u7b11\u8bdd\uff0c\u5ffd\u7565\u4e86\u590d\u6742\u5e7d\u9ed8\u5f62\u5f0f\u3002", "method": "\u6784\u5efa\u5305\u542b4\u79cd\u7b11\u8bdd\u7c7b\u578b\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u591a\u79cdLLMs\u7684\u96f6\u6837\u672c\u89e3\u91ca\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5\u6a21\u578b\uff08\u5305\u62ec\u63a8\u7406\u6a21\u578b\uff09\u5747\u65e0\u6cd5\u53ef\u9760\u89e3\u91ca\u6240\u6709\u7b11\u8bdd\u7c7b\u578b\u3002", "conclusion": "\u73b0\u6709\u8ba1\u7b97\u5e7d\u9ed8\u7814\u7a76\u5bf9\u590d\u6742\u5e7d\u9ed8\u5f62\u5f0f\u7684\u5173\u6ce8\u4e0d\u8db3\u3002"}}
{"id": "2507.12905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12905", "abs": "https://arxiv.org/abs/2507.12905", "authors": ["Tomohiro Suzuki", "Ryota Tanaka", "Calvin Yeung", "Keisuke Fujii"], "title": "AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and Evaluation of Monocular 3D Pose Estimation Ability", "comment": "9 pages, 5 figures, 5 tables", "summary": "Monocular 3D pose estimation is a promising, flexible alternative to costly\nmotion capture systems for sports analysis. However, its practical application\nis hindered by two factors: a lack of realistic sports datasets and unclear\nreliability for sports tasks. To address these challenges, we introduce the\nAthleticsPose dataset, a new public dataset featuring ``real'' motions captured\nfrom 23 athletes performing various athletics events on an athletic field.\nUsing this dataset, we trained a representative 3D pose estimation model and\nperformed a comprehensive evaluation. Our results show that the model trained\non AthleticsPose significantly outperforms a baseline model trained on an\nimitated sports motion dataset, reducing MPJPE by approximately 75 %. These\nresults show the importance of training on authentic sports motion data, as\nmodels based on imitated motions do not effectively transfer to real-world\nmotions. Further analysis reveals that estimation accuracy is sensitive to\ncamera view and subject scale. In case studies of kinematic indicators, the\nmodel demonstrated the potential to capture individual differences in knee\nangles but struggled with higher-speed metrics, such as knee-drive velocity,\ndue to prediction biases. This work provides the research community with a\nvaluable dataset and clarifies the potential and practical limitations of using\nmonocular 3D pose estimation for sports motion analysis. Our dataset, code, and\ncheckpoints are available at https://github.com/SZucchini/AthleticsPose.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AthleticsPose\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5355\u76ee3D\u59ff\u6001\u4f30\u8ba1\u5728\u4f53\u80b2\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f53\u80b2\u5206\u6790\u4e2d\u7f3a\u4e4f\u771f\u5b9e\u8fd0\u52a8\u6570\u636e\u96c6\u548c\u59ff\u6001\u4f30\u8ba1\u53ef\u9760\u6027\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528AthleticsPose\u6570\u636e\u96c6\u8bad\u7ec33D\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff0c\u5e76\u4e0e\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "\u6a21\u578b\u5728\u771f\u5b9e\u8fd0\u52a8\u6570\u636e\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0cMPJPE\u964d\u4f4e\u7ea675%\uff0c\u4f46\u53d7\u76f8\u673a\u89c6\u89d2\u548c\u4e3b\u4f53\u5c3a\u5ea6\u5f71\u54cd\u3002", "conclusion": "\u771f\u5b9e\u8fd0\u52a8\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u578b\u5728\u9ad8\u901f\u8fd0\u52a8\u6307\u6807\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.12956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12956", "abs": "https://arxiv.org/abs/2507.12956", "authors": ["Qiang Wang", "Mengchao Wang", "Fan Jiang", "Yaqi Fan", "Yonggang Qi", "Mu Xu"], "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers", "comment": "https://fantasy-amap.github.io/fantasy-portrait/", "summary": "Producing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, a diffusion transformer\nbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces an\nexpression-augmented learning strategy that utilizes implicit representations\nto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose the Multi-Expr dataset and ExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelling particularly in\nchallenging cross reenactment and multi-character contexts. Our project page is\nhttps://fantasy-amap.github.io/fantasy-portrait/.", "AI": {"tldr": "FantasyPortrait\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9759\u6001\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u60c5\u611f\u4e30\u5bcc\u7684\u9762\u90e8\u52a8\u753b\uff0c\u652f\u6301\u5355\u4eba\u548c\u591a\u89d2\u8272\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\uff0c\u5bb9\u6613\u4ea7\u751f\u4f2a\u5f71\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u60c5\u611f\uff0c\u540c\u65f6\u7f3a\u4e4f\u591a\u89d2\u8272\u52a8\u753b\u652f\u6301\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u6846\u67b6\uff0c\u5f15\u5165\u8868\u8fbe\u589e\u5f3a\u5b66\u4e60\u7b56\u7565\u548c\u63a9\u7801\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u9690\u5f0f\u8868\u793a\u6355\u6349\u9762\u90e8\u52a8\u6001\u5e76\u9632\u6b62\u7279\u5f81\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFantasyPortrait\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8de8\u91cd\u6f14\u548c\u591a\u89d2\u8272\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FantasyPortrait\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e13\u7528\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2507.13224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13224", "abs": "https://arxiv.org/abs/2507.13224", "authors": ["Keerthi Veeramachaneni", "Praveen Tirupattur", "Amrit Singh Bedi", "Mubarak Shah"], "title": "Leveraging Pre-Trained Visual Models for AI-Generated Video Detection", "comment": null, "summary": "Recent advances in Generative AI (GenAI) have led to significant improvements\nin the quality of generated visual content. As AI-generated visual content\nbecomes increasingly indistinguishable from real content, the challenge of\ndetecting the generated content becomes critical in combating misinformation,\nensuring privacy, and preventing security threats. Although there has been\nsubstantial progress in detecting AI-generated images, current methods for\nvideo detection are largely focused on deepfakes, which primarily involve human\nfaces. However, the field of video generation has advanced beyond DeepFakes,\ncreating an urgent need for methods capable of detecting AI-generated videos\nwith generic content. To address this gap, we propose a novel approach that\nleverages pre-trained visual models to distinguish between real and generated\nvideos. The features extracted from these pre-trained models, which have been\ntrained on extensive real visual content, contain inherent signals that can\nhelp distinguish real from generated videos. Using these extracted features, we\nachieve high detection performance without requiring additional model training,\nand we further improve performance by training a simple linear classification\nlayer on top of the extracted features. We validated our method on a dataset we\ncompiled (VID-AID), which includes around 10,000 AI-generated videos produced\nby 9 different text-to-video models, along with 4,000 real videos, totaling\nover 7 hours of video content. Our evaluation shows that our approach achieves\nhigh detection accuracy, above 90% on average, underscoring its effectiveness.\nUpon acceptance, we plan to publicly release the code, the pre-trained models,\nand our dataset to support ongoing research in this critical area.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u68c0\u6d4bAI\u751f\u6210\u89c6\u9891\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u89c6\u9891\u8d28\u91cf\u7684\u63d0\u5347\uff0c\u68c0\u6d4b\u8fd9\u7c7b\u5185\u5bb9\u4ee5\u9632\u6b62\u865a\u5047\u4fe1\u606f\u548c\u9690\u79c1\u6cc4\u9732\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u5206\u7c7b\u5c42\u533a\u5206\u771f\u5b9e\u4e0e\u751f\u6210\u89c6\u9891\u3002", "result": "\u5728\u5305\u542b10,000\u4e2aAI\u751f\u6210\u89c6\u9891\u548c4,000\u4e2a\u771f\u5b9e\u89c6\u9891\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc790%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u8ba1\u5212\u516c\u5f00\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2507.13353", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13353", "abs": "https://arxiv.org/abs/2507.13353", "authors": ["Shihao Wang", "Guo Chen", "De-an Huang", "Zhiqi Li", "Minghan Li", "Guilin Li", "Jose M. Alvarez", "Lei Zhang", "Zhiding Yu"], "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding", "comment": "Technical Report", "summary": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVideoITG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u6307\u4ee4\u5b9a\u5236\u5316\u5e27\u91c7\u6837\uff0c\u7ed3\u5408VidThinker\u7ba1\u9053\u548cVideoITG-40K\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u96be\u4ee5\u5904\u7406\u590d\u6742\u573a\u666f\uff0c\u9700\u6539\u8fdb\u5e27\u9009\u62e9\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faVidThinker\u7ba1\u9053\uff0c\u5206\u4e09\u6b65\uff1a\u751f\u6210\u6307\u4ee4\u76f8\u5173\u7684\u526a\u8f91\u7ea7\u63cf\u8ff0\u3001\u68c0\u7d22\u76f8\u5173\u89c6\u9891\u7247\u6bb5\u3001\u7cbe\u7ec6\u9009\u62e9\u5173\u952e\u5e27\u3002", "result": "\u6784\u5efaVideoITG-40K\u6570\u636e\u96c6\uff0cVideoITG\u6a21\u578b\u5728\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VideoITG\u901a\u8fc7\u6307\u4ee4\u9a71\u52a8\u7684\u5e27\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
