{"id": "2509.23154", "categories": ["cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23154", "abs": "https://arxiv.org/abs/2509.23154", "authors": ["Jinzhe Pan", "Jingqing Wang", "Yuehui Ouyang", "Wenchi Cheng", "Wei Zhang"], "title": "AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8", "comment": "6 pages,6 figures, accepted by Globalcom 2025", "summary": "The exponential growth of wireless devices and stringent reliability\nrequirements of emerging applications demand fundamental improvements in\ndistributed channel access mechanisms for unlicensed bands. Current Wi-Fi\nsystems, which rely on binary exponential backoff (BEB), suffer from suboptimal\ncollision resolution in dense deployments and persistent fairness challenges\ndue to inherent randomness. This paper introduces a multi-agent reinforcement\nlearning framework that integrates artificial intelligence (AI) optimization\nwith legacy device coexistence. We first develop a dynamic backoff selection\nmechanism that adapts to real-time channel conditions through access deferral\nevents while maintaining full compatibility with conventional CSMA/CA\noperations. Second, we introduce a fairness quantification metric aligned with\nenhanced distributed channel access (EDCA) principles to ensure equitable\nmedium access opportunities. Finally, we propose a centralized training\ndecentralized execution (CTDE) architecture incorporating neighborhood activity\npatterns as observational inputs, optimized via constrained multi-agent\nproximal policy optimization (MAPPO) to jointly minimize collisions and\nguarantee fairness. Experimental results demonstrate that our solution\nsignificantly reduces collision probability compared to conventional BEB while\npreserving backward compatibility with commercial Wi-Fi devices. The proposed\nfairness metric effectively eliminates starvation risks in heterogeneous\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684Wi-Fi\u4fe1\u9053\u63a5\u5165\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9000\u907f\u9009\u62e9\u548c\u516c\u5e73\u6027\u91cf\u5316\u6307\u6807\uff0c\u5728\u4fdd\u6301\u4e0e\u4f20\u7edf\u8bbe\u5907\u517c\u5bb9\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u78b0\u649e\u6982\u7387\u5e76\u4fdd\u8bc1\u516c\u5e73\u6027\u3002", "motivation": "\u5f53\u524dWi-Fi\u7cfb\u7edf\u4f7f\u7528\u7684\u4e8c\u8fdb\u5236\u6307\u6570\u9000\u907f\u673a\u5236\u5728\u5bc6\u96c6\u90e8\u7f72\u4e2d\u78b0\u649e\u89e3\u51b3\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u5b58\u5728\u516c\u5e73\u6027\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u5206\u5e03\u5f0f\u4fe1\u9053\u63a5\u5165\u673a\u5236\u4ee5\u6ee1\u8db3\u65b0\u5174\u5e94\u7528\u7684\u9ad8\u53ef\u9760\u6027\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u52a8\u6001\u9000\u907f\u9009\u62e9\u673a\u5236\uff0c\u7ed3\u5408\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u67b6\u6784\uff0c\u4f7f\u7528\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u6574\u5408\u90bb\u57df\u6d3b\u52a8\u6a21\u5f0f\u4f5c\u4e3a\u89c2\u6d4b\u8f93\u5165\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8eEDCA\u539f\u5219\u7684\u516c\u5e73\u6027\u91cf\u5316\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edfBEB\u673a\u5236\uff0c\u8be5\u65b9\u6848\u663e\u8457\u964d\u4f4e\u78b0\u649e\u6982\u7387\uff0c\u4fdd\u6301\u4e0e\u5546\u7528Wi-Fi\u8bbe\u5907\u7684\u5411\u540e\u517c\u5bb9\u6027\uff0c\u516c\u5e73\u6027\u6307\u6807\u6709\u6548\u6d88\u9664\u4e86\u5f02\u6784\u573a\u666f\u4e2d\u7684\u9965\u997f\u98ce\u9669\u3002", "conclusion": "\u8be5AI\u4f18\u5316\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86Wi-Fi\u5bc6\u96c6\u90e8\u7f72\u4e2d\u7684\u78b0\u649e\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u667a\u80fd\u4fe1\u9053\u63a5\u5165\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.23112", "abs": "https://arxiv.org/abs/2509.23112", "authors": ["Ryo Watanabe", "Maxime Alvarez", "Pablo Ferreiro", "Pavel Savkin", "Genki Sano"], "title": "FTACT: Force Torque aware Action Chunking Transformer for Pick-and-Reorient Bottle Task", "comment": null, "summary": "Manipulator robots are increasingly being deployed in retail environments,\nyet contact rich edge cases still trigger costly human teleoperation. A\nprominent example is upright lying beverage bottles, where purely visual cues\nare often insufficient to resolve subtle contact events required for precise\nmanipulation. We present a multimodal Imitation Learning policy that augments\nthe Action Chunking Transformer with force and torque sensing, enabling\nend-to-end learning over images, joint states, and forces and torques. Deployed\non Ghost, single-arm platform by Telexistence Inc, our approach improves\nPick-and-Reorient bottle task by detecting and exploiting contact transitions\nduring pressing and placement. Hardware experiments demonstrate greater task\nsuccess compared to baseline matching the observation space of ACT as an\nablation and experiments indicate that force and torque signals are beneficial\nin the press and place phases where visual observability is limited, supporting\nthe use of interaction forces as a complementary modality for contact rich\nskills. The results suggest a practical path to scaling retail manipulation by\ncombining modern imitation learning architectures with lightweight force and\ntorque sensing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u529b/\u529b\u77e9\u4f20\u611f\u4e0eAction Chunking Transformer\u7ed3\u5408\uff0c\u6539\u8fdb\u96f6\u552e\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bf9\u76f4\u7acb\u996e\u6599\u74f6\u7684\u6293\u53d6\u548c\u91cd\u5b9a\u5411\u4efb\u52a1\u3002", "motivation": "\u96f6\u552e\u73af\u5883\u4e2d\u673a\u68b0\u81c2\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u8fb9\u7f18\u6848\u4f8b\uff08\u5982\u76f4\u7acb\u996e\u6599\u74f6\u64cd\u4f5c\uff09\u4e2d\u4ecd\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u9065\u63a7\u64cd\u4f5c\uff0c\u7eaf\u89c6\u89c9\u7ebf\u7d22\u4e0d\u8db3\u4ee5\u89e3\u51b3\u7cbe\u786e\u64cd\u4f5c\u6240\u9700\u7684\u7ec6\u5fae\u63a5\u89e6\u4e8b\u4ef6\u3002", "method": "\u5f00\u53d1\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u529b/\u529b\u77e9\u4f20\u611f\u96c6\u6210\u5230Action Chunking Transformer\u4e2d\uff0c\u5b9e\u73b0\u56fe\u50cf\u3001\u5173\u8282\u72b6\u6001\u3001\u529b/\u529b\u77e9\u7684\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u5728Telexistence\u516c\u53f8\u7684Ghost\u5355\u81c2\u5e73\u53f0\u4e0a\u90e8\u7f72\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u4ec5\u5339\u914dACT\u89c2\u6d4b\u7a7a\u95f4\uff09\u6709\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u529b/\u529b\u77e9\u4fe1\u53f7\u5728\u89c6\u89c9\u53ef\u89c2\u6d4b\u6027\u53d7\u9650\u7684\u6309\u538b\u548c\u653e\u7f6e\u9636\u6bb5\u7279\u522b\u6709\u76ca\u3002", "conclusion": "\u901a\u8fc7\u5c06\u73b0\u4ee3\u6a21\u4eff\u5b66\u4e60\u67b6\u6784\u4e0e\u8f7b\u91cf\u7ea7\u529b/\u529b\u77e9\u4f20\u611f\u76f8\u7ed3\u5408\uff0c\u4e3a\u6269\u5c55\u96f6\u552e\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u652f\u6301\u5c06\u4ea4\u4e92\u529b\u4f5c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u6280\u80fd\u7684\u8865\u5145\u6a21\u6001\u3002"}}
{"id": "2509.22979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22979", "abs": "https://arxiv.org/abs/2509.22979", "authors": ["Zeyi Chen", "Xinzhi Zhang", "Humishka Zope", "Hugo Barbalho", "Konstantina Mellou", "Marco Molinaro", "Janardhan Kulkarni", "Ishai Menache", "Sirui Li"], "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts", "comment": null, "summary": "Mathematical programming -- the task of expressing operations and\ndecision-making problems in precise mathematical language -- is fundamental\nacross domains, yet remains a skill-intensive process requiring operations\nresearch expertise. Recent advances in large language models for complex\nreasoning have spurred interest in automating this task, translating natural\nlanguage into executable optimization models. Current approaches, however,\nachieve limited accuracy, hindered by scarce and noisy training data without\nleveraging domain knowledge. In this work, we systematically integrate\noptimization expertise to improve formulation accuracy for mixed-integer linear\nprogramming, a key family of mathematical programs. Our approach first cleans\ntraining data through class-based error analysis to explicitly prevent common\nmistakes within each optimization class. We then develop multi-turn inference\nstrategies that guide LLMs with class-specific error summaries and solver\nfeedback, enabling iterative refinement. Experiments across multiple base LLMs\ndemonstrate that combining cleaned data with domain-informed prompting and\nfeedback improves formulation accuracy by 14 percentage points on average,\nenabling further progress toward robust LLM-assisted optimization formulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684LLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u548c\u591a\u8f6e\u63a8\u7406\u7b56\u7565\u6765\u63d0\u5347\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u7684\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u6267\u884c\u6a21\u578b\u7684\u8f6c\u6362\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6570\u5b66\u7f16\u7a0b\u81ea\u52a8\u5316\u65b9\u6cd5\u51c6\u786e\u7387\u6709\u9650\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u566a\u58f0\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u4f18\u5316\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "1. \u57fa\u4e8e\u7c7b\u522b\u7684\u9519\u8bef\u5206\u6790\u6e05\u6d17\u8bad\u7ec3\u6570\u636e\uff0c\u9632\u6b62\u5e38\u89c1\u9519\u8bef\uff1b2. \u5f00\u53d1\u591a\u8f6e\u63a8\u7406\u7b56\u7565\uff0c\u5229\u7528\u7c7b\u522b\u7279\u5b9a\u9519\u8bef\u603b\u7ed3\u548c\u6c42\u89e3\u5668\u53cd\u9988\u6307\u5bfcLLM\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6e05\u6d17\u6570\u636e\u548c\u9886\u57df\u77e5\u8bc6\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u53ef\u5c06\u516c\u5f0f\u5316\u51c6\u786e\u7387\u63d0\u534714\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u7840LLM\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u4e3a\u5b9e\u73b0\u7a33\u5065\u7684LLM\u8f85\u52a9\u4f18\u5316\u516c\u5f0f\u5316\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.22864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22864", "abs": "https://arxiv.org/abs/2509.22864", "authors": ["Yixuan Hu", "Yuxuan Xue", "Simon Klenk", "Daniel Cremers", "Gerard Pons-Moll"], "title": "ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models", "comment": null, "summary": "In recent years, event cameras have gained significant attention due to their\nbio-inspired properties, such as high temporal resolution and high dynamic\nrange. However, obtaining large-scale labeled ground-truth data for event-based\nvision tasks remains challenging and costly. In this paper, we present\nControlEvents, a diffusion-based generative model designed to synthesize\nhigh-quality event data guided by diverse control signals such as class text\nlabels, 2D skeletons, and 3D body poses. Our key insight is to leverage the\ndiffusion prior from foundation models, such as Stable Diffusion, enabling\nhigh-quality event data generation with minimal fine-tuning and limited labeled\ndata. Our method streamlines the data generation process and significantly\nreduces the cost of producing labeled event datasets. We demonstrate the\neffectiveness of our approach by synthesizing event data for visual\nrecognition, 2D skeleton estimation, and 3D body pose estimation. Our\nexperiments show that the synthesized labeled event data enhances model\nperformance in all tasks. Additionally, our approach can generate events based\non unseen text labels during training, illustrating the powerful text-based\ngeneration capabilities inherited from foundation models.", "AI": {"tldr": "ControlEvents\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u901a\u8fc7\u6587\u672c\u6807\u7b7e\u30012D\u9aa8\u67b6\u548c3D\u8eab\u4f53\u59ff\u6001\u7b49\u63a7\u5236\u4fe1\u53f7\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4e8b\u4ef6\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u751f\u7269\u542f\u53d1\u7279\u6027\uff0c\u4f46\u83b7\u53d6\u5927\u89c4\u6a21\u6807\u6ce8\u7684\u4e8b\u4ef6\u6570\u636e\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5229\u7528Stable Diffusion\u7b49\u57fa\u7840\u6a21\u578b\u7684\u6269\u6563\u5148\u9a8c\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5fae\u8c03\u548c\u6709\u9650\u6807\u6ce8\u6570\u636e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4e8b\u4ef6\u6570\u636e\u751f\u6210\uff0c\u652f\u6301\u6587\u672c\u6807\u7b7e\u30012D\u9aa8\u67b6\u548c3D\u59ff\u6001\u7b49\u591a\u79cd\u63a7\u5236\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5408\u6210\u7684\u6807\u6ce8\u4e8b\u4ef6\u6570\u636e\u5728\u89c6\u89c9\u8bc6\u522b\u30012D\u9aa8\u67b6\u4f30\u8ba1\u548c3D\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u90fd\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u57fa\u4e8e\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u6587\u672c\u6807\u7b7e\u751f\u6210\u4e8b\u4ef6\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u4e8b\u4ef6\u6570\u636e\u96c6\u7684\u6210\u672c\uff0c\u7ee7\u627f\u4e86\u57fa\u7840\u6a21\u578b\u5f3a\u5927\u7684\u57fa\u4e8e\u6587\u672c\u7684\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2509.22925", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22925", "abs": "https://arxiv.org/abs/2509.22925", "authors": ["Yuanzhi Zhu", "Xi Wang", "St\u00e9phane Lathuili\u00e8re", "Vicky Kalogeiton"], "title": "Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings", "comment": null, "summary": "One-step generators distilled from Masked Diffusion Models (MDMs) compress\nmultiple sampling steps into a single forward pass, enabling efficient text and\nimage synthesis. However, they suffer two key limitations: they inherit\nmodeling bias from the teacher, and their discrete token outputs block gradient\nflow, preventing post-distillation refinements such as adversarial training,\nreward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this\nwork, we introduce soft embeddings, a simple relaxation that replaces discrete\ntokens with the expected embeddings under the generator's output distribution.\nSoft embeddings preserve representation fidelity for one-step discrete\ngenerator while providing a fully differentiable continuous surrogate that is\ncompatible with teacher backbones and tokenizer decoders. Integrating soft\nembeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes\none-step generators end-to-end trainable and enables straightforward\napplication of GAN-based refinement, differentiable reward fine-tuning, and\nTTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen),\nSoft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image\nperformance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement,\nalong with higher GenEval and HPS scores on text-to-image with reward\nfine-tuning, and further gains from TTEO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8f6f\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u79bb\u6563\u4ee4\u724c\u66ff\u6362\u4e3a\u751f\u6210\u5668\u8f93\u51fa\u5206\u5e03\u4e0b\u7684\u671f\u671b\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u5355\u6b65\u751f\u6210\u5668\u7684\u68af\u5ea6\u6d41\u963b\u585e\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u3001\u5956\u52b1\u5fae\u8c03\u7b49\u540e\u84b8\u998f\u4f18\u5316\u3002", "motivation": "\u5355\u6b65\u751f\u6210\u5668\u867d\u7136\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6587\u672c\u548c\u56fe\u50cf\u5408\u6210\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7ee7\u627f\u6559\u5e08\u6a21\u578b\u7684\u5efa\u6a21\u504f\u5dee\uff0c\u4ee5\u53ca\u79bb\u6563\u4ee4\u724c\u8f93\u51fa\u963b\u585e\u68af\u5ea6\u6d41\uff0c\u65e0\u6cd5\u8fdb\u884c\u540e\u84b8\u998f\u4f18\u5316\u3002", "method": "\u5f15\u5165\u8f6f\u5d4c\u5165\u65b9\u6cd5\uff0c\u7528\u751f\u6210\u5668\u8f93\u51fa\u5206\u5e03\u4e0b\u7684\u671f\u671b\u5d4c\u5165\u66ff\u4ee3\u79bb\u6563\u4ee4\u724c\uff0c\u521b\u5efa\u5b8c\u5168\u53ef\u5fae\u7684\u8fde\u7eed\u4ee3\u7406\uff0c\u4e0e\u6559\u5e08\u4e3b\u5e72\u548c\u4ee4\u724c\u89e3\u7801\u5668\u517c\u5bb9\u3002", "result": "\u5728\u591a\u4e2aMDM\u6559\u5e08\u6a21\u578b\u4e0a\uff0cSoft-Di[M]O\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5355\u6b65\u7ed3\u679c\uff1a\u6539\u8fdb\u7684\u7c7b\u522b\u5230\u56fe\u50cf\u6027\u80fd\uff0c\u5728ImageNet-256\u4e0a\u901a\u8fc7GAN\u4f18\u5316\u8fbe\u5230FID 1.56\uff0c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u83b7\u5f97\u66f4\u9ad8\u7684GenEval\u548cHPS\u5206\u6570\uff0c\u5e76\u901a\u8fc7TTEO\u83b7\u5f97\u8fdb\u4e00\u6b65\u589e\u76ca\u3002", "conclusion": "\u8f6f\u5d4c\u5165\u4f7f\u5355\u6b65\u751f\u6210\u5668\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\uff0c\u652f\u6301GAN\u4f18\u5316\u3001\u53ef\u5fae\u5956\u52b1\u5fae\u8c03\u548cTTEO\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.23281", "categories": ["cs.RO", "I.2.6; I.2.9"], "pdf": "https://arxiv.org/pdf/2509.23281", "abs": "https://arxiv.org/abs/2509.23281", "authors": ["Francesco Marchiori", "Rohan Sinha", "Christopher Agia", "Alexander Robey", "George J. Pappas", "Mauro Conti", "Marco Pavone"], "title": "Preventing Robotic Jailbreaking via Multimodal Domain Adaptation", "comment": "Project page: https://j-dapt.github.io/. 9 pages, 6 figures", "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) are\nincreasingly deployed in robotic environments but remain vulnerable to\njailbreaking attacks that bypass safety mechanisms and drive unsafe or\nphysically harmful behaviors in the real world. Data-driven defenses such as\njailbreak classifiers show promise, yet they struggle to generalize in domains\nwhere specialized datasets are scarce, limiting their effectiveness in robotics\nand other safety-critical contexts. To address this gap, we introduce J-DAPT, a\nlightweight framework for multimodal jailbreak detection through\nattention-based fusion and domain adaptation. J-DAPT integrates textual and\nvisual embeddings to capture both semantic intent and environmental grounding,\nwhile aligning general-purpose jailbreak datasets with domain-specific\nreference data. Evaluations across autonomous driving, maritime robotics, and\nquadruped navigation show that J-DAPT boosts detection accuracy to nearly 100%\nwith minimal overhead. These results demonstrate that J-DAPT provides a\npractical defense for securing VLMs in robotic applications. Additional\nmaterials are made available at: https://j-dapt.github.io.", "AI": {"tldr": "J-DAPT\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u8d8a\u72f1\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u548c\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\uff0c\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u6709\u6548\u9632\u5fa1LLM\u548cVLM\u7684\u5b89\u5168\u6f0f\u6d1e\u653b\u51fb\u3002", "motivation": "LLM\u548cVLM\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u5bfc\u81f4\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u9632\u5fa1\u65b9\u6cd5\u5728\u4e13\u4e1a\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "J-DAPT\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\u6765\u6355\u6349\u8bed\u4e49\u610f\u56fe\u548c\u73af\u5883\u80cc\u666f\uff0c\u540c\u65f6\u5c06\u901a\u7528\u8d8a\u72f1\u6570\u636e\u96c6\u4e0e\u9886\u57df\u7279\u5b9a\u53c2\u8003\u6570\u636e\u8fdb\u884c\u5bf9\u9f50\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u878d\u5408\u548c\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u6d77\u6d0b\u673a\u5668\u4eba\u548c\u56db\u8db3\u5bfc\u822a\u7b49\u9886\u57df\u7684\u8bc4\u4f30\u663e\u793a\uff0cJ-DAPT\u5c06\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u81f3\u63a5\u8fd1100%\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "J-DAPT\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4fdd\u62a4VLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9632\u5fa1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23449", "categories": ["cs.AI", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23449", "abs": "https://arxiv.org/abs/2509.23449", "authors": ["Charles E. Gagnon", "Steven H. H. Ding", "Philippe Charland", "Benjamin C. M. Fung"], "title": "Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity", "comment": "17 pages, 7 figures, submitted to USENIX Security '26", "summary": "Binary code similarity detection is a core task in reverse engineering. It\nsupports malware analysis and vulnerability discovery by identifying\nsemantically similar code in different contexts. Modern methods have progressed\nfrom manually engineered features to vector representations. Hand-crafted\nstatistics (e.g., operation ratios) are interpretable, but shallow and fail to\ngeneralize. Embedding-based methods overcome this by learning robust\ncross-setting representations, but these representations are opaque vectors\nthat prevent rapid verification. They also face a scalability-accuracy\ntrade-off, since high-dimensional nearest-neighbor search requires\napproximations that reduce precision. Current approaches thus force a\ncompromise between interpretability, generalizability, and scalability.\n  We bridge these gaps using a language model-based agent to conduct structured\nreasoning analysis of assembly code and generate features such as input/output\ntypes, side effects, notable constants, and algorithmic intent. Unlike\nhand-crafted features, they are richer and adaptive. Unlike embeddings, they\nare human-readable, maintainable, and directly searchable with inverted or\nrelational indexes. Without any matching training, our method respectively\nachieves 42% and 62% for recall@1 in cross-architecture and cross-optimization\ntasks, comparable to embedding methods with training (39% and 34%). Combined\nwith embeddings, it significantly outperforms the state-of-the-art,\ndemonstrating that accuracy, scalability, and interpretability can coexist.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u5206\u6790\u6c47\u7f16\u4ee3\u7801\u751f\u6210\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u624b\u5de5\u7279\u5f81\u53ef\u89e3\u91ca\u4f46\u6cdb\u5316\u6027\u5dee\uff0c\u5d4c\u5165\u65b9\u6cd5\u6cdb\u5316\u6027\u597d\u4f46\u4e0d\u53ef\u89e3\u91ca\u4e14\u9762\u4e34\u53ef\u6269\u5c55\u6027-\u7cbe\u5ea6\u6743\u8861\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5bf9\u6c47\u7f16\u4ee3\u7801\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\u5206\u6790\uff0c\u751f\u6210\u8f93\u5165/\u8f93\u51fa\u7c7b\u578b\u3001\u526f\u4f5c\u7528\u3001\u663e\u8457\u5e38\u6570\u548c\u7b97\u6cd5\u610f\u56fe\u7b49\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u53ef\u76f4\u63a5\u7528\u4e8e\u5012\u6392\u6216\u5173\u7cfb\u7d22\u5f15\u641c\u7d22\u3002", "result": "\u65e0\u9700\u5339\u914d\u8bad\u7ec3\uff0c\u5728\u8de8\u67b6\u6784\u548c\u8de8\u4f18\u5316\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523042%\u548c62%\u7684recall@1\uff0c\u4e0e\u9700\u8981\u8bad\u7ec3\u7684\u5d4c\u5165\u65b9\u6cd5\u76f8\u5f53\uff0839%\u548c34%\uff09\u3002\u4e0e\u5d4c\u5165\u65b9\u6cd5\u7ed3\u5408\u540e\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u53ef\u4ee5\u5171\u5b58\uff0c\u4e3a\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23038", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23038", "abs": "https://arxiv.org/abs/2509.23038", "authors": ["Jingxing Li", "Yongjae Lee", "Deliang Fan"], "title": "GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization", "comment": null, "summary": "Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and\nstate-of-the-art regression accuracy, yet our analysis reveals subtle geometric\ninconsistencies in its internal representations that prevent reaching the\nprecision ceiling of correspondence-based methods like MASt3R (which require\n300ms per pair). In this work, we present GeLoc3r, a novel approach to relative\ncamera pose estimation that enhances pose regression methods through Geometric\nConsistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma\nby training regression networks to produce geometrically consistent poses\nwithout inference-time geometric computation. During training, GeLoc3r\nleverages ground-truth depth to generate dense 3D-2D correspondences, weights\nthem using a FusionTransformer that learns correspondence importance, and\ncomputes geometrically-consistent poses via weighted RANSAC. This creates a\nconsistency loss that transfers geometric knowledge into the regression\nnetwork. Unlike FAR method which requires both regression and geometric solving\nat inference, GeLoc3r only uses the enhanced regression head at test time,\nmaintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On\nchallenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving\nsignificant improvements including 40.45% vs. 34.85% AUC@5{\\deg} on the CO3Dv2\ndataset (16% relative improvement), 68.66% vs. 66.70% AUC@5{\\deg} on\nRealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric\nconsistency during training rather than enforcing it at inference, GeLoc3r\nrepresents a paradigm shift in how neural networks learn camera geometry,\nachieving both the speed of regression and the geometric understanding of\ncorrespondence methods.", "AI": {"tldr": "GeLoc3r\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u6027\u6b63\u5219\u5316\u589e\u5f3a\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u56de\u5f52\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301ReLoc3R\u5feb\u901f\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u63a5\u8fd1MASt3R\u7684\u9ad8\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\u95ee\u9898\u3002", "motivation": "ReLoc3R\u867d\u7136\u5177\u6709\u5feb\u901f\u63a8\u7406\u548c\u5148\u8fdb\u56de\u5f52\u7cbe\u5ea6\uff0c\u4f46\u5176\u5185\u90e8\u8868\u793a\u5b58\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u8fbe\u5230\u57fa\u4e8e\u5bf9\u5e94\u5173\u7cfb\u65b9\u6cd5\u7684\u7cbe\u5ea6\u4e0a\u9650\u3002", "method": "\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u771f\u5b9e\u6df1\u5ea6\u751f\u6210\u5bc6\u96c63D-2D\u5bf9\u5e94\u5173\u7cfb\uff0c\u901a\u8fc7FusionTransformer\u5b66\u4e60\u5bf9\u5e94\u5173\u7cfb\u91cd\u8981\u6027\uff0c\u4f7f\u7528\u52a0\u6743RANSAC\u8ba1\u7b97\u51e0\u4f55\u4e00\u81f4\u59ff\u6001\uff0c\u521b\u5efa\u4e00\u81f4\u6027\u635f\u5931\u5c06\u51e0\u4f55\u77e5\u8bc6\u8f6c\u79fb\u5230\u56de\u5f52\u7f51\u7edc\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eReLoc3R\uff0cCO3Dv2\u6570\u636e\u96c6AUC@5\u00b0\u4ece34.85%\u63d0\u5347\u81f340.45%(\u76f8\u5bf9\u63d0\u534716%)\uff0cRealEstate10K\u4ece66.70%\u63d0\u5347\u81f368.66%\uff0cMegaDepth1500\u4ece49.60%\u63d0\u5347\u81f350.45%\u3002", "conclusion": "\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u6559\u6388\u51e0\u4f55\u4e00\u81f4\u6027\u800c\u975e\u5728\u63a8\u7406\u65f6\u5f3a\u5236\u6267\u884c\uff0cGeLoc3r\u4ee3\u8868\u4e86\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u76f8\u673a\u51e0\u4f55\u7684\u65b0\u8303\u5f0f\uff0c\u540c\u65f6\u5177\u5907\u56de\u5f52\u65b9\u6cd5\u7684\u901f\u5ea6\u548c\u5bf9\u5e94\u5173\u7cfb\u65b9\u6cd5\u7684\u51e0\u4f55\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.23286", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23286", "abs": "https://arxiv.org/abs/2509.23286", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Yoonjun Cho", "Dongjae Jeon", "Sangwoo Shin", "Hyesoo Hong", "Albert No"], "title": "A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models", "comment": "Code and models are available at https://ai-isl.github.io/A2D", "summary": "Diffusion large language models (dLLMs) enable any-order generation, but this\nflexibility enlarges the attack surface: harmful spans may appear at arbitrary\npositions, and template-based prefilling attacks such as DIJA bypass\nresponse-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a\ntoken-level alignment method that aligns dLLMs to emit an [EOS] refusal signal\nwhenever harmful content arises. By aligning safety directly at the token-level\nunder randomized masking, A2D achieves robustness to both any-decoding-order\nand any-step prefilling attacks under various conditions. It also enables\nreal-time monitoring: dLLMs may begin a response but automatically terminate if\nunsafe continuation emerges. On safety benchmarks, A2D consistently prevents\nthe generation of harmful outputs, slashing DIJA success rates from over 80% to\nnear-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and\nthresholded [EOS] probabilities allow early rejection, yielding up to 19.3x\nfaster safe termination.", "AI": {"tldr": "A2D\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee4\u724c\u7ea7\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4efb\u610f\u4f4d\u7f6e\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u5e76\u53d1\u51fa[EOS]\u62d2\u7edd\u4fe1\u53f7\uff0c\u6709\u6548\u9632\u5fa1\u4efb\u610f\u89e3\u7801\u987a\u5e8f\u548c\u9884\u586b\u5145\u653b\u51fb\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u610f\u987a\u5e8f\u751f\u6210\u7279\u6027\u6269\u5927\u4e86\u653b\u51fb\u9762\uff0c\u6709\u5bb3\u5185\u5bb9\u53ef\u80fd\u51fa\u73b0\u5728\u4efb\u610f\u4f4d\u7f6e\uff0c\u4e14\u6a21\u677f\u9884\u586b\u5145\u653b\u51fb\u5982DIJA\u80fd\u591f\u7ed5\u8fc7\u54cd\u5e94\u7ea7\u62d2\u7edd\u673a\u5236\u3002", "method": "\u91c7\u7528\u4ee4\u724c\u7ea7\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u968f\u673a\u63a9\u7801\u6761\u4ef6\u4e0b\u8bad\u7ec3\u6a21\u578b\u5728\u68c0\u6d4b\u5230\u6709\u5bb3\u5185\u5bb9\u65f6\u53d1\u51fa[EOS]\u62d2\u7edd\u4fe1\u53f7\uff0c\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\u548c\u81ea\u52a8\u7ec8\u6b62\u4e0d\u5b89\u5168\u751f\u6210\u3002", "result": "\u5728\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cA2D\u5c06DIJA\u653b\u51fb\u6210\u529f\u7387\u4ece80%\u4ee5\u4e0a\u964d\u81f3\u63a5\u8fd1\u96f6\uff08LLaDA-8B-Instruct\u4e3a1.3%\uff0cDream-v0-Instruct-7B\u4e3a0.0%\uff09\uff0c\u5e76\u901a\u8fc7\u9608\u503c[EOS]\u6982\u7387\u5b9e\u73b0\u65e9\u671f\u62d2\u7edd\uff0c\u5b89\u5168\u7ec8\u6b62\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe19.3\u500d\u3002", "conclusion": "A2D\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5bf9\u4efb\u610f\u987a\u5e8f\u548c\u4efb\u610f\u6b65\u9aa4\u653b\u51fb\u7684\u9c81\u68d2\u9632\u5fa1\uff0c\u540c\u65f6\u652f\u6301\u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u3002"}}
{"id": "2509.23129", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23129", "abs": "https://arxiv.org/abs/2509.23129", "authors": ["Haotian Liu", "Shuo Wang", "Hongteng Xu"], "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning", "comment": null, "summary": "Reinforcement Learning (RL) methods, exemplified by Group Relative Policy\nOptimization (GRPO) and its variants, play a central role in developing\nreasoning models. However, these methods often suffer from a critical\noverconfidence issue, which prevents them from achieving self-aware reasoning\nmodels. In this study, we propose a simple yet effective confidence-calibration\ngroup sequence policy gradient method, called C$^2$GSPG, which simultaneously\nenhances reasoning performance while suppressing overconfidence. In principle,\nwe propose a Group Sequence Policy Gradient (GSPG) framework for learning\nreasoning models, which eliminates the token-level bias commonly appearing in\nGRPO and its variants. In this framework, we define the model confidence for\neach reasoning problem using the normalized sequence-level probability, and\nthen apply a cross-entropy regularizer to calibrate the model confidence to the\nsequence's reward. We demonstrate that the confidence calibration regularizer\nand GSPG are collaborative for binary rewards, as their objectives always share\nthe same gradient direction. For non-binary rewards, we apply nonlinear reward\nnormalization and adaptive regularizer clipping, mitigating the potential\nconflict between the two objectives. Applying C$^2$GSPG to post-train large\nlanguage models in logical and mathematical reasoning tasks, we show its\nsuperiority over state-of-the-art methods in both reasoning accuracy and\nconfidence calibration. The code of C$^2$GSPG is available at\nhttps://github.com/HaotianLiu123/CCGSPG.", "AI": {"tldr": "\u63d0\u51fa\u4e86C\u00b2GSPG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u7ec4\u5e8f\u5217\u7b56\u7565\u68af\u5ea6\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u7406\u6027\u80fd\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\u53ca\u5176\u53d8\u4f53\uff09\u5b58\u5728\u4e25\u91cd\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u963b\u788d\u4e86\u81ea\u611f\u77e5\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u7ec4\u5e8f\u5217\u7b56\u7565\u68af\u5ea6\uff08GSPG\uff09\u6846\u67b6\u6d88\u9664token\u7ea7\u504f\u5dee\uff0c\u5b9a\u4e49\u57fa\u4e8e\u5f52\u4e00\u5316\u5e8f\u5217\u7ea7\u6982\u7387\u7684\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u5e94\u7528\u4ea4\u53c9\u71b5\u6b63\u5219\u5668\u5c06\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5230\u5e8f\u5217\u5956\u52b1\u3002", "result": "\u5728\u903b\u8f91\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cC\u00b2GSPG\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "C\u00b2GSPG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u6027\u80fd\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u53cc\u91cd\u63d0\u5347\u3002"}}
{"id": "2509.23122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23122", "abs": "https://arxiv.org/abs/2509.23122", "authors": ["Chenrui Ma", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Yanning Shen"], "title": "Stochastic Interpolants via Conditional Dependent Coupling", "comment": null, "summary": "Existing image generation models face critical challenges regarding the\ntrade-off between computation and fidelity. Specifically, models relying on a\npretrained Variational Autoencoder (VAE) suffer from information loss, limited\ndetail, and the inability to support end-to-end training. In contrast, models\noperating directly in the pixel space incur prohibitive computational cost.\nAlthough cascade models can mitigate computational cost, stage-wise separation\nprevents effective end-to-end optimization, hampers knowledge sharing, and\noften results in inaccurate distribution learning within each stage. To address\nthese challenges, we introduce a unified multistage generative framework based\non our proposed Conditional Dependent Coupling strategy. It decomposes the\ngenerative process into interpolant trajectories at multiple stages, ensuring\naccurate distribution learning while enabling end-to-end optimization.\nImportantly, the entire process is modeled as a single unified Diffusion\nTransformer, eliminating the need for disjoint modules and also enabling\nknowledge sharing. Extensive experiments demonstrate that our method achieves\nboth high fidelity and efficiency across multiple resolutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u4f9d\u8d56\u8026\u5408\u7b56\u7565\u7684\u7edf\u4e00\u591a\u9636\u6bb5\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u591a\u9636\u6bb5\u7684\u63d2\u503c\u8f68\u8ff9\uff0c\u5728\u786e\u4fdd\u51c6\u786e\u5206\u5e03\u5b66\u4e60\u7684\u540c\u65f6\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u8ba1\u7b97\u6210\u672c\u4e0e\u4fdd\u771f\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u95ee\u9898\uff1a\u57fa\u4e8eVAE\u7684\u6a21\u578b\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u7ec6\u8282\u6709\u9650\u95ee\u9898\uff0c\u800c\u50cf\u7d20\u7a7a\u95f4\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u7ea7\u8054\u6a21\u578b\u5219\u65e0\u6cd5\u6709\u6548\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u4f9d\u8d56\u8026\u5408\u7b56\u7565\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u591a\u9636\u6bb5\u63d2\u503c\u8f68\u8ff9\uff0c\u6574\u4e2a\u6d41\u7a0b\u5efa\u6a21\u4e3a\u5355\u4e00\u7edf\u4e00\u7684\u6269\u6563\u53d8\u6362\u5668\uff0c\u907f\u514d\u6a21\u5757\u5206\u79bb\u5e76\u5b9e\u73b0\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5206\u8fa8\u7387\u4e0b\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u9ad8\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u591a\u9636\u6bb5\u751f\u6210\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u4f18\u5316\u548c\u77e5\u8bc6\u5171\u4eab\u3002"}}
{"id": "2509.24094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24094", "abs": "https://arxiv.org/abs/2509.24094", "authors": ["Vignesh Ramanathan", "Michael Milford", "Tobias Fischer"], "title": "Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras", "comment": null, "summary": "Visual Place Recognition (VPR) enables systems to identify previously visited\nlocations within a map, a fundamental task for autonomous navigation. Prior\nworks have developed VPR solutions using event cameras, which asynchronously\nmeasure per-pixel brightness changes with microsecond temporal resolution.\nHowever, these approaches rely on dense representations of the inherently\nsparse camera output and require tens to hundreds of milliseconds of event data\nto predict a place. Here, we break this paradigm with Flash, a lightweight VPR\nsystem that predicts places using sub-millisecond slices of event data. Our\nmethod is based on the observation that active pixel locations provide strong\ndiscriminative features for VPR. Flash encodes these active pixel locations\nusing efficient binary frames and computes similarities via fast bitwise\noperations, which are then normalized based on the relative event activity in\nthe query and reference frames. Flash improves Recall@1 for sub-millisecond VPR\nover existing baselines by 11.33x on the indoor QCR-Event-Dataset and 5.92x on\nthe 8 km Brisbane-Event-VPR dataset. Moreover, our approach reduces the\nduration for which the robot must operate without awareness of its position, as\nevidenced by a localization latency metric we term Time to Correct Match (TCM).\nTo the best of our knowledge, this is the first work to demonstrate\nsub-millisecond VPR using event cameras.", "AI": {"tldr": "Flash\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e9a\u6beb\u79d2\u7ea7\u4e8b\u4ef6\u6570\u636e\u901a\u8fc7\u4e8c\u8fdb\u5236\u5e27\u7f16\u7801\u548c\u5feb\u901f\u4f4d\u8fd0\u7b97\u5b9e\u73b0\u5feb\u901f\u4f4d\u7f6e\u8bc6\u522b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684VPR\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u8868\u793a\u4e14\u9700\u8981\u6570\u5341\u5230\u6570\u767e\u6beb\u79d2\u7684\u4e8b\u4ef6\u6570\u636e\uff0cFlash\u65e8\u5728\u6253\u7834\u8fd9\u4e00\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e9a\u6beb\u79d2\u7ea7\u7684\u4f4d\u7f6e\u8bc6\u522b", "method": "\u5229\u7528\u6d3b\u8dc3\u50cf\u7d20\u4f4d\u7f6e\u4f5c\u4e3a\u5224\u522b\u7279\u5f81\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u5e27\u7f16\u7801\u8fd9\u4e9b\u4f4d\u7f6e\uff0c\u4f7f\u7528\u5feb\u901f\u4f4d\u8fd0\u7b97\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u5e76\u6839\u636e\u67e5\u8be2\u548c\u53c2\u8003\u5e27\u7684\u76f8\u5bf9\u4e8b\u4ef6\u6d3b\u52a8\u8fdb\u884c\u5f52\u4e00\u5316", "result": "\u5728\u5ba4\u5185QCR-Event-Dataset\u4e0aRecall@1\u63d0\u534711.33\u500d\uff0c\u57288\u516c\u91ccBrisbane-Event-VPR\u6570\u636e\u96c6\u4e0a\u63d0\u53475.92\u500d\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u65e0\u4f4d\u7f6e\u611f\u77e5\u7684\u8fd0\u884c\u65f6\u95f4", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c55\u793a\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u5b9e\u73b0\u4e9a\u6beb\u79d2\u7ea7VPR\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4f4d\u7f6e\u8bc6\u522b\u901f\u5ea6\u548c\u51c6\u786e\u6027"}}
{"id": "2509.24124", "categories": ["cs.RO", "cs.AI", "cs.LG", "F.2.2; G.3; I.5.3; F.2.2; I.2.9; G.3; I.5.3"], "pdf": "https://arxiv.org/pdf/2509.24124", "abs": "https://arxiv.org/abs/2509.24124", "authors": ["Ilari Vallivaara", "Bingnan Duan", "Yinhuan Dong", "Tughrul Arslan"], "title": "Ancestry Tree Clustering for Particle Filter Diversity Maintenance", "comment": "15th International Conference on Indoor Positioning and Indoor\n  Navigation, 15-18 September 2025, Tampere, Finland Originally 8 pages. The\n  online version with appendices is 14 pages", "summary": "We propose a method for linear-time diversity maintenance in particle\nfiltering. It clusters particles based on ancestry tree topology: closely\nrelated particles in sufficiently large subtrees are grouped together. The main\nidea is that the tree structure implicitly encodes similarity without the need\nfor spatial or other domain-specific metrics. This approach, when combined with\nintra-cluster fitness sharing and the protection of particles not included in a\ncluster, effectively prevents premature convergence in multimodal environments\nwhile maintaining estimate compactness. We validate our approach in a\nmultimodal robotics simulation and a real-world multimodal indoor environment.\nWe compare the performance to several diversity maintenance algorithms from the\nliterature, including Deterministic Resampling and Particle Gaussian Mixtures.\nOur algorithm achieves high success rates with little to no negative effect on\ncompactness, showing particular robustness to different domains and challenging\ninitial conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7956\u5148\u6811\u62d3\u6251\u7ed3\u6784\u7684\u7ebf\u6027\u65f6\u95f4\u591a\u6837\u6027\u7ef4\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u76f8\u5173\u7c92\u5b50\u6765\u9632\u6b62\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u8fc7\u65e9\u6536\u655b\u3002", "motivation": "\u89e3\u51b3\u7c92\u5b50\u6ee4\u6ce2\u4e2d\u591a\u6837\u6027\u635f\u5931\u95ee\u9898\uff0c\u907f\u514d\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8fc7\u65e9\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u4f30\u8ba1\u7684\u7d27\u51d1\u6027\u3002", "method": "\u57fa\u4e8e\u7c92\u5b50\u7956\u5148\u6811\u62d3\u6251\u7ed3\u6784\u8fdb\u884c\u805a\u7c7b\uff1a\u5c06\u5bc6\u5207\u76f8\u5173\u7684\u7c92\u5b50\u5728\u8db3\u591f\u5927\u7684\u5b50\u6811\u4e2d\u5206\u7ec4\uff0c\u7ed3\u5408\u7c07\u5185\u9002\u5e94\u5ea6\u5171\u4eab\u548c\u4fdd\u62a4\u672a\u805a\u7c7b\u7c92\u5b50\u3002", "result": "\u5728\u591a\u6a21\u6001\u673a\u5668\u4eba\u4eff\u771f\u548c\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u786e\u5b9a\u6027\u91cd\u91c7\u6837\u548c\u7c92\u5b50\u9ad8\u65af\u6df7\u5408\u7b49\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u4e14\u5bf9\u7d27\u51d1\u6027\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u548c\u6311\u6218\u6027\u521d\u59cb\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7279\u522b\u9c81\u68d2\u6027\uff0c\u80fd\u6709\u6548\u7ef4\u6301\u591a\u6837\u6027\u800c\u4e0d\u663e\u8457\u5f71\u54cd\u4f30\u8ba1\u8d28\u91cf\u3002"}}
{"id": "2509.23183", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23183", "abs": "https://arxiv.org/abs/2509.23183", "authors": ["Guohao Chen", "Shuaicheng Niu", "Deyu Chen", "Jiahao Yang", "Zitian Zhang", "Mingkui Tan", "Pengcheng Wu", "Zhiqi Shen"], "title": "ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse", "comment": null, "summary": "Test-time entropy minimization helps adapt a model to novel environments and\nincentivize its reasoning capability, unleashing the model's potential during\ninference by allowing it to evolve and improve in real-time using its own\npredictions, achieving promising performance. However, pure entropy\nminimization can favor non-generalizable shortcuts, such as inflating the logit\nnorm and driving all predictions to a dominant class to reduce entropy, risking\ncollapsed solutions (e.g., constant one-hot outputs) that trivially minimize\nthe objective without meaningful learning. In this paper, we introduce\nZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time\nentropy minimization. ZeroSiam prevents collapse through asymmetric divergence\nalignment, which is efficiently achieved by a learnable predictor and a\nstop-gradient operator before the classifier. We provide empirical and\ntheoretical evidence that ZeroSiam not only prevents collapse solutions, but\nalso absorbs and regularizes biased learning signals, enhancing performance\neven when no collapse occurs. Despite its simplicity, extensive results show\nthat ZeroSiam performs more stably over prior methods using negligible\noverhead, demonstrating efficacy on both vision adaptation and large language\nmodel reasoning tasks across challenging test scenarios and diverse models,\nincluding tiny models that are particularly collapse-prone.", "AI": {"tldr": "ZeroSiam\u662f\u4e00\u79cd\u7528\u4e8e\u6d4b\u8bd5\u65f6\u71b5\u6700\u5c0f\u5316\u7684\u975e\u5bf9\u79f0\u5b6a\u751f\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u6563\u5ea6\u5bf9\u9f50\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u5728\u89c6\u89c9\u9002\u5e94\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u9ad8\u6548\u3002", "motivation": "\u7eaf\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\uff0c\u6bd4\u5982\u901a\u8fc7\u81a8\u80c0logit\u8303\u6570\u6216\u5c06\u6240\u6709\u9884\u6d4b\u63a8\u5411\u4e3b\u5bfc\u7c7b\u522b\u6765\u51cf\u5c11\u71b5\uff0c\u8fd9\u4f1a\u5f62\u6210\u65e0\u610f\u4e49\u7684\u5e38\u6570\u8f93\u51fa\uff0c\u800c\u975e\u771f\u6b63\u7684\u5b66\u4e60\u3002", "method": "\u63d0\u51faZeroSiam\u975e\u5bf9\u79f0\u5b6a\u751f\u67b6\u6784\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u9884\u6d4b\u5668\u548c\u5206\u7c7b\u5668\u524d\u7684\u505c\u6b62\u68af\u5ea6\u64cd\u4f5c\u6765\u5b9e\u73b0\u975e\u5bf9\u79f0\u6563\u5ea6\u5bf9\u9f50\uff0c\u9632\u6b62\u5d29\u6e83\u3002", "result": "ZeroSiam\u4e0d\u4ec5\u80fd\u9632\u6b62\u5d29\u6e83\uff0c\u8fd8\u80fd\u5438\u6536\u548c\u6b63\u5219\u5316\u6709\u504f\u5b66\u4e60\u4fe1\u53f7\uff0c\u5728\u5404\u79cd\u6d4b\u8bd5\u573a\u666f\u548c\u4e0d\u540c\u6a21\u578b\uff08\u5305\u62ec\u6613\u5d29\u6e83\u7684\u5c0f\u6a21\u578b\uff09\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "ZeroSiam\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u8bbe\u8ba1\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.23235", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23235", "abs": "https://arxiv.org/abs/2509.23235", "authors": ["Seongsoo Heo", "Dong-Wan Choi"], "title": "Patch Rebirth: Toward Fast and Transferable Model Inversion of Vision Transformers", "comment": "22 pages, 8 figures", "summary": "Model inversion is a widely adopted technique in data-free learning that\nreconstructs synthetic inputs from a pretrained model through iterative\noptimization, without access to original training data. Unfortunately, its\napplication to state-of-the-art Vision Transformers (ViTs) poses a major\ncomputational challenge, due to their expensive self-attention mechanisms. To\naddress this, Sparse Model Inversion (SMI) was proposed to improve efficiency\nby pruning and discarding seemingly unimportant patches, which were even\nclaimed to be obstacles to knowledge transfer. However, our empirical findings\nsuggest the opposite: even randomly selected patches can eventually acquire\ntransferable knowledge through continued inversion. This reveals that\ndiscarding any prematurely inverted patches is inefficient, as it suppresses\nthe extraction of class-agnostic features essential for knowledge transfer,\nalong with class-specific features. In this paper, we propose Patch Rebirth\nInversion (PRI), a novel approach that incrementally detaches the most\nimportant patches during the inversion process to construct sparse synthetic\nimages, while allowing the remaining patches to continue evolving for future\nselection. This progressive strategy not only improves efficiency, but also\nencourages initially less informative patches to gradually accumulate more\nclass-relevant knowledge, a phenomenon we refer to as the Re-Birth effect,\nthereby effectively balancing class-agnostic and class-specific knowledge.\nExperimental results show that PRI achieves up to 10x faster inversion than\nstandard Dense Model Inversion (DMI) and 2x faster than SMI, while consistently\noutperforming SMI in accuracy and matching the performance of DMI.", "AI": {"tldr": "\u63d0\u51fa\u4e86Patch Rebirth Inversion (PRI)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5206\u79bb\u91cd\u8981\u8865\u4e01\u6765\u6784\u5efa\u7a00\u758f\u5408\u6210\u56fe\u50cf\uff0c\u540c\u65f6\u5141\u8bb8\u5269\u4f59\u8865\u4e01\u7ee7\u7eed\u6f14\u5316\uff0c\u6bd4\u6807\u51c6\u5bc6\u96c6\u6a21\u578b\u53cd\u6f14\u5feb10\u500d\uff0c\u6bd4\u7a00\u758f\u6a21\u578b\u53cd\u6f14\u5feb2\u500d\uff0c\u4e14\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6a21\u578b\u53cd\u6f14(SMI)\u65b9\u6cd5\u8fc7\u65e9\u4e22\u5f03\u770b\u4f3c\u4e0d\u91cd\u8981\u7684\u8865\u4e01\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u662f\u968f\u673a\u9009\u62e9\u7684\u8865\u4e01\u4e5f\u80fd\u901a\u8fc7\u6301\u7eed\u53cd\u6f14\u83b7\u5f97\u53ef\u8fc1\u79fb\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b56\u7565\u6765\u5e73\u8861\u7c7b\u65e0\u5173\u548c\u7c7b\u7279\u5b9a\u7279\u5f81\u3002", "method": "PRI\u5728\u53cd\u6f14\u8fc7\u7a0b\u4e2d\u9010\u6b65\u5206\u79bb\u6700\u91cd\u8981\u7684\u8865\u4e01\u6765\u6784\u5efa\u7a00\u758f\u56fe\u50cf\uff0c\u540c\u65f6\u8ba9\u5269\u4f59\u8865\u4e01\u7ee7\u7eed\u6f14\u5316\u4ee5\u4f9b\u672a\u6765\u9009\u62e9\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u77e5\u8bc6\u79ef\u7d2f\u3002", "result": "PRI\u6bd4\u6807\u51c6\u5bc6\u96c6\u6a21\u578b\u53cd\u6f14\u5feb10\u500d\uff0c\u6bd4\u7a00\u758f\u6a21\u578b\u53cd\u6f14\u5feb2\u500d\uff0c\u5728\u51c6\u786e\u7387\u4e0a\u6301\u7eed\u4f18\u4e8eSMI\u5e76\u5339\u914dDMI\u6027\u80fd\u3002", "conclusion": "PRI\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8865\u4e01\u5206\u79bb\u7b56\u7565\u6709\u6548\u5e73\u8861\u4e86\u6548\u7387\u548c\u77e5\u8bc6\u63d0\u53d6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u6a21\u578b\u53cd\u6f14\u3002"}}
{"id": "2509.23213", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23213", "abs": "https://arxiv.org/abs/2509.23213", "authors": ["Hugo Math", "Robin Sch\u00f6n", "Rainer Lienhart"], "title": "One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences", "comment": "Accepted at NeuRIPS2025 Workshop CauScien: Discovering Causality in\n  Science. arXiv admin note: substantial text overlap with arXiv:2509.19112", "summary": "Understanding causality in event sequences with thousands of sparse event\ntypes is critical in domains such as healthcare, cybersecurity, or vehicle\ndiagnostics, yet current methods fail to scale. We present OSCAR, a one-shot\ncausal autoregressive method that infers per-sequence Markov Boundaries using\ntwo pretrained Transformers as density estimators. This enables efficient,\nparallel causal discovery without costly global CI testing. On a real-world\nautomotive dataset with 29,100 events and 474 labels, OSCAR recovers\ninterpretable causal structures in minutes, while classical methods fail to\nscale, enabling practical scientific diagnostics at production scale.", "AI": {"tldr": "OSCAR\u662f\u4e00\u79cd\u4e00\u6b21\u6027\u56e0\u679c\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3Transformer\u4f5c\u4e3a\u5bc6\u5ea6\u4f30\u8ba1\u5668\u6765\u63a8\u65ad\u6bcf\u4e2a\u5e8f\u5217\u7684\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5e76\u884c\u7684\u56e0\u679c\u53d1\u73b0\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5168\u5c40\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u3002", "motivation": "\u5728\u533b\u7597\u3001\u7f51\u7edc\u5b89\u5168\u6216\u8f66\u8f86\u8bca\u65ad\u7b49\u9886\u57df\uff0c\u7406\u89e3\u5177\u6709\u6570\u5343\u4e2a\u7a00\u758f\u4e8b\u4ef6\u7c7b\u578b\u7684\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u5982\u6b64\u5927\u89c4\u6a21\u7684\u6570\u636e\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3Transformer\u4f5c\u4e3a\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u4e00\u6b21\u6027\u56e0\u679c\u81ea\u56de\u5f52\u65b9\u6cd5\u63a8\u65ad\u6bcf\u4e2a\u5e8f\u5217\u7684\u9a6c\u5c14\u53ef\u592b\u8fb9\u754c\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u5168\u5c40\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u3002", "result": "\u5728\u5305\u542b29,100\u4e2a\u4e8b\u4ef6\u548c474\u4e2a\u6807\u7b7e\u7684\u771f\u5b9e\u4e16\u754c\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\uff0cOSCAR\u5728\u51e0\u5206\u949f\u5185\u6062\u590d\u4e86\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u800c\u7ecf\u5178\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u3002", "conclusion": "OSCAR\u80fd\u591f\u5728\u751f\u4ea7\u89c4\u6a21\u4e0b\u5b9e\u73b0\u5b9e\u7528\u7684\u79d1\u5b66\u8bca\u65ad\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u4e8b\u4ef6\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002"}}
{"id": "2509.23864", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23864", "abs": "https://arxiv.org/abs/2509.23864", "authors": ["Roham Koohestani"], "title": "AgentGuard: Runtime Verification of AI Agents", "comment": "Accepted for publication in the proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering, ASE 2025, in the\n  1st international workshop on Agentic Software Engineering (AgenticSE)", "summary": "The rapid evolution to autonomous, agentic AI systems introduces significant\nrisks due to their inherent unpredictability and emergent behaviors; this also\nrenders traditional verification methods inadequate and necessitates a shift\ntowards probabilistic guarantees where the question is no longer if a system\nwill fail, but the probability of its failure within given constraints. This\npaper presents AgentGuard, a framework for runtime verification of Agentic AI\nsystems that provides continuous, quantitative assurance through a new paradigm\ncalled Dynamic Probabilistic Assurance. AgentGuard operates as an inspection\nlayer that observes an agent's raw I/O and abstracts it into formal events\ncorresponding to transitions in a state model. It then uses online learning to\ndynamically build and update a Markov Decision Process (MDP) that formally\nmodels the agent's emergent behavior. Using probabilistic model checking, the\nframework then verifies quantitative properties in real-time.", "AI": {"tldr": "AgentGuard\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3bAI\u7cfb\u7edf\u8fd0\u884c\u65f6\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6982\u7387\u4fdd\u8bc1\u63d0\u4f9b\u8fde\u7eed\u5b9a\u91cf\u4fdd\u8bc1\uff0c\u5c06\u4ee3\u7406\u7684\u539f\u59cbI/O\u62bd\u8c61\u4e3a\u72b6\u6001\u6a21\u578b\u4e2d\u7684\u5f62\u5f0f\u5316\u4e8b\u4ef6\uff0c\u5e76\u4f7f\u7528\u5728\u7ebf\u5b66\u4e60\u6784\u5efa\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6765\u5efa\u6a21\u4ee3\u7406\u7684\u6d8c\u73b0\u884c\u4e3a\u3002", "motivation": "\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5feb\u901f\u6f14\u8fdb\u5e26\u6765\u4e86\u663e\u8457\u98ce\u9669\uff0c\u5176\u56fa\u6709\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u6d8c\u73b0\u884c\u4e3a\u4f7f\u5f97\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u4e0d\u8db3\uff0c\u9700\u8981\u8f6c\u5411\u6982\u7387\u4fdd\u8bc1\u6765\u8bc4\u4f30\u7cfb\u7edf\u5728\u7ed9\u5b9a\u7ea6\u675f\u4e0b\u7684\u5931\u8d25\u6982\u7387\u3002", "method": "AgentGuard\u4f5c\u4e3a\u68c0\u67e5\u5c42\u89c2\u5bdf\u4ee3\u7406\u7684\u539f\u59cbI/O\uff0c\u5c06\u5176\u62bd\u8c61\u4e3a\u72b6\u6001\u6a21\u578b\u8f6c\u6362\u7684\u5f62\u5f0f\u5316\u4e8b\u4ef6\uff0c\u4f7f\u7528\u5728\u7ebf\u5b66\u4e60\u52a8\u6001\u6784\u5efa\u548c\u66f4\u65b0\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6765\u5f62\u5f0f\u5316\u5efa\u6a21\u4ee3\u7406\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u6982\u7387\u6a21\u578b\u68c0\u67e5\u5b9e\u65f6\u9a8c\u8bc1\u5b9a\u91cf\u5c5e\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u8fde\u7eed\u3001\u5b9a\u91cf\u7684\u8fd0\u884c\u65f6\u4fdd\u8bc1\uff0c\u901a\u8fc7\u52a8\u6001\u6982\u7387\u4fdd\u8bc1\u8303\u5f0f\u6709\u6548\u5e94\u5bf9\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u3002", "conclusion": "AgentGuard\u4e3a\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u8fd0\u884c\u65f6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6982\u7387\u4fdd\u8bc1\u89e3\u51b3\u4e86\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u5728\u5904\u7406\u6d8c\u73b0\u884c\u4e3a\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.23304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23304", "abs": "https://arxiv.org/abs/2509.23304", "authors": ["Liwen Hu", "Yang Li", "Mianzhi Liu", "Yijia Guo", "Shenghao Xie", "Ziluo Ding", "Tiejun Huang", "Lei Ma"], "title": "Seeing the Unseen in Low-light Spike Streams", "comment": null, "summary": "Spike camera, a type of neuromorphic sensor with high-temporal resolution,\nshows great promise for high-speed visual tasks. Unlike traditional cameras,\nspike camera continuously accumulates photons and fires asynchronous spike\nstreams. Due to unique data modality, spike streams require reconstruction\nmethods to become perceptible to the human eye.\n  However, lots of methods struggle to handle spike streams in low-light\nhigh-speed scenarios due to severe noise and sparse information. In this work,\nwe propose Diff-SPK, the first diffusion-based reconstruction method for spike\ncamera. Diff-SPK effectively leverages generative priors to supplement texture\ninformation in low-light conditions. Specifically, it first employs an\n\\textbf{E}nhanced \\textbf{T}exture \\textbf{f}rom Inter-spike \\textbf{I}nterval\n(ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI\nserves as a conditioning input for ControlNet to generate the high-speed\nscenes. To improve the quality of results, we introduce an ETFI-based feature\nfusion module during the generation process.\n  Moreover, we establish the first bona fide benchmark for the low-light spike\nstream reconstruction task. It significantly surpasses existing reconstruction\ndatasets in scale and provides quantitative illumination information. The\nperformance on real low-light spike streams demonstrates the superiority of\nDiff-SPK.", "AI": {"tldr": "Diff-SPK\u662f\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8109\u51b2\u76f8\u673a\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u7eb9\u7406\u63d0\u53d6\u548cControlNet\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4f4e\u5149\u7167\u9ad8\u901f\u573a\u666f\u4e0b\u8109\u51b2\u6d41\u91cd\u5efa\u7684\u566a\u58f0\u548c\u4fe1\u606f\u7a00\u758f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8109\u51b2\u76f8\u673a\u91cd\u5efa\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u9ad8\u901f\u573a\u666f\u4e0b\u96be\u4ee5\u5904\u7406\u4e25\u91cd\u566a\u58f0\u548c\u7a00\u758f\u4fe1\u606f\uff0c\u9700\u8981\u65b0\u7684\u91cd\u5efa\u6280\u672f\u6765\u751f\u6210\u4eba\u773c\u53ef\u611f\u77e5\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "method": "\u9996\u5148\u4f7f\u7528ETFI\u4ece\u4f4e\u5149\u7167\u8109\u51b2\u6d41\u4e2d\u805a\u5408\u7a00\u758f\u4fe1\u606f\uff0c\u7136\u540e\u5c06\u5176\u4f5c\u4e3aControlNet\u7684\u6761\u4ef6\u8f93\u5165\u6765\u751f\u6210\u9ad8\u901f\u573a\u666f\uff0c\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165ETFI\u7279\u5f81\u878d\u5408\u6a21\u5757\u63d0\u5347\u8d28\u91cf\u3002", "result": "\u5728\u771f\u5b9e\u4f4e\u5149\u7167\u8109\u51b2\u6d41\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u8bc1\u660e\u4e86Diff-SPK\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u4f4e\u5149\u7167\u8109\u51b2\u6d41\u91cd\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "Diff-SPK\u6210\u529f\u5229\u7528\u751f\u6210\u5148\u9a8c\u8865\u5145\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u7eb9\u7406\u4fe1\u606f\uff0c\u4e3a\u8109\u51b2\u76f8\u673a\u5728\u4f4e\u5149\u7167\u9ad8\u901f\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23311", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23311", "abs": "https://arxiv.org/abs/2509.23311", "authors": ["Haorui Yu", "Qiufeng Yi", "Yijia Chu", "Yang Zhao"], "title": "Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning", "comment": "8 pages, 5 figures, 4 tables. Submitted to WiNLP 2025 Workshop at\n  COLING 2025", "summary": "Vision-Language Models (VLMs) often appear culturally competent but rely on\nsuperficial pattern matching rather than genuine cultural understanding. We\nintroduce a diagnostic framework to probe VLM reasoning on fire-themed cultural\nimagery through both classification and explanation analysis. Testing multiple\nmodels on Western festivals, non-Western traditions, and emergency scenes\nreveals systematic biases: models correctly identify prominent Western\nfestivals but struggle with underrepresented cultural events, frequently\noffering vague labels or dangerously misclassifying emergencies as\ncelebrations. These failures expose the risks of symbolic shortcuts and\nhighlight the need for cultural evaluation beyond accuracy metrics to ensure\ninterpretable and fair multimodal systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\u6765\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u56fe\u50cf\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u80fd\u8bc6\u522b\u4e3b\u6d41\u897f\u65b9\u8282\u65e5\u4f46\u96be\u4ee5\u7406\u89e3\u975e\u897f\u65b9\u6587\u5316\u4e8b\u4ef6\uff0c\u751a\u81f3\u53ef\u80fd\u5c06\u7d27\u6025\u573a\u666f\u8bef\u5224\u4e3a\u5e86\u795d\u6d3b\u52a8\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8868\u9762\u4e0a\u8868\u73b0\u51fa\u6587\u5316\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u4e0a\u4f9d\u8d56\u80a4\u6d45\u7684\u6a21\u5f0f\u5339\u914d\u800c\u975e\u771f\u6b63\u7684\u6587\u5316\u7406\u89e3\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u8fd9\u79cd\u8868\u9762\u80fd\u529b\u80cc\u540e\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u51c6\u786e\u7387\u6307\u6807\u7684\u6587\u5316\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7c7b\u548c\u89e3\u91ca\u5206\u6790\u6765\u63a2\u6d4b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u706b\u4e3b\u9898\u6587\u5316\u56fe\u50cf\u7684\u7406\u89e3\u80fd\u529b\u3002\u6d4b\u8bd5\u4e86\u591a\u4e2a\u6a21\u578b\u5728\u897f\u65b9\u8282\u65e5\u3001\u975e\u897f\u65b9\u4f20\u7edf\u548c\u7d27\u6025\u573a\u666f\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff1a\u80fd\u6b63\u786e\u8bc6\u522b\u4e3b\u6d41\u897f\u65b9\u8282\u65e5\uff0c\u4f46\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6587\u5316\u4e8b\u4ef6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7ecf\u5e38\u63d0\u4f9b\u6a21\u7cca\u6807\u7b7e\u6216\u5371\u9669\u5730\u5c06\u7d27\u6025\u60c5\u51b5\u8bef\u5206\u7c7b\u4e3a\u5e86\u795d\u6d3b\u52a8\u3002", "conclusion": "\u8fd9\u4e9b\u5931\u8d25\u66b4\u9732\u4e86\u7b26\u53f7\u6377\u5f84\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u51c6\u786e\u7387\u6307\u6807\u7684\u6587\u5316\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u53ef\u89e3\u91ca\u548c\u516c\u5e73\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u3002"}}
{"id": "2509.23316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23316", "abs": "https://arxiv.org/abs/2509.23316", "authors": ["Siheng Wang", "Zhengdao Li", "Yanshu Li", "Canran Xiao", "Haibo Zhan", "Zhengtao Yao", "Xuzhi Zhang", "Jiale Kang", "Linshan Li", "Weiming Liu", "Zhikang Dong", "Jifeng Shen", "Junhao Dong", "Qiang Sun", "Piotr Koniusz"], "title": "C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection", "comment": null, "summary": "Object detection has advanced significantly in the closed-set setting, but\nreal-world deployment remains limited by two challenges: poor generalization to\nunseen categories and insufficient robustness under adverse conditions. Prior\nresearch has explored these issues separately: visible-infrared detection\nimproves robustness but lacks generalization, while open-world detection\nleverages vision-language alignment strategy for category diversity but\nstruggles under extreme environments. This trade-off leaves robustness and\ndiversity difficult to achieve simultaneously. To mitigate these issues, we\npropose \\textbf{C3-OWD}, a curriculum cross-modal contrastive learning\nframework that unifies both strengths. Stage~1 enhances robustness by\npretraining with RGBT data, while Stage~2 improves generalization via\nvision-language alignment. To prevent catastrophic forgetting between two\nstages, we introduce an Exponential Moving Average (EMA) mechanism that\ntheoretically guarantees preservation of pre-stage performance with bounded\nparameter lag and function consistency. Experiments on FLIR, OV-COCO, and\nOV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$\nAP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\\text{Novel}}$ on OV-COCO, and $35.7$\nmAP$_r$ on OV-LVIS, establishing competitive performance across both robustness\nand diversity evaluations. Code available at:\nhttps://github.com/justin-herry/C3-OWD.git.", "AI": {"tldr": "C3-OWD\u662f\u4e00\u4e2a\u8bfe\u7a0b\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u540c\u65f6\u89e3\u51b3\u76ee\u6807\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u5728RGB-T\u6570\u636e\u548c\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u4e0a\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4ee5\u53ca\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u73b0\u6709\u7814\u7a76\u5206\u522b\u5904\u7406\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4f46\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528RGBT\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff1b\u5f15\u5165\u6307\u6570\u79fb\u52a8\u5e73\u5747\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728FLIR\u6570\u636e\u96c6\u4e0a\u8fbe\u523080.1 AP50\uff0c\u5728OV-COCO\u4e0a\u8fbe\u523048.6 AP50_Novel\uff0c\u5728OV-LVIS\u4e0a\u8fbe\u523035.7 mAPr\uff0c\u5728\u9c81\u68d2\u6027\u548c\u591a\u6837\u6027\u8bc4\u4f30\u4e2d\u90fd\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "C3-OWD\u6210\u529f\u7edf\u4e00\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8bfe\u7a0b\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.23325", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23325", "abs": "https://arxiv.org/abs/2509.23325", "authors": ["Jonas Ngnaw\u00e9", "Maxime Heuillet", "Sabyasachi Sahoo", "Yann Pequignot", "Ola Ahmad", "Audrey Durand", "Fr\u00e9d\u00e9ric Precioso", "Christian Gagn\u00e9"], "title": "Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling", "comment": null, "summary": "Fine-tuning pretrained models is a standard and effective workflow in modern\nmachine learning. However, robust fine-tuning (RFT), which aims to\nsimultaneously achieve adaptation to a downstream task and robustness to\nadversarial examples, remains challenging. Despite the abundance of non-robust\npretrained models in open-source repositories, their potential for RFT is less\nunderstood. We address this knowledge gap by systematically examining RFT from\nsuch non-robust models. Our experiments reveal that fine-tuning non-robust\nmodels with a robust objective, even under small perturbations, can lead to\npoor performance, a phenomenon that we dub \\emph{suboptimal transfer}. In\nchallenging scenarios (eg, difficult tasks, high perturbation), the resulting\nperformance can be so low that it may be considered a transfer failure. We find\nthat fine-tuning using a robust objective impedes task adaptation at the\nbeginning of training and eventually prevents optimal transfer. However, we\npropose a novel heuristic, \\emph{Epsilon-Scheduling}, a schedule over\nperturbation strength used during training that promotes optimal transfer.\nAdditionally, we introduce \\emph{expected robustness}, a metric that captures\nperformance across a range of perturbations, providing a more comprehensive\nevaluation of the accuracy-robustness trade-off for diverse models at test\ntime. Extensive experiments on a wide range of configurations (six pretrained\nmodels and five datasets) show that \\emph{Epsilon-Scheduling} successfully\nprevents \\emph{suboptimal transfer} and consistently improves expected\nrobustness.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u9c81\u68d2\u76ee\u6807\u5bf9\u975e\u9c81\u68d2\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4f1a\u5bfc\u81f4\u6b21\u4f18\u8fc1\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u53ef\u80fd\u5b8c\u5168\u5931\u8d25\u3002\u4f5c\u8005\u63d0\u51fa\u4e86Epsilon-Scheduling\u65b9\u6cd5\u548cexpected robustness\u6307\u6807\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5f00\u6e90\u5e93\u4e2d\u6709\u5927\u91cf\u975e\u9c81\u68d2\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5b83\u4eec\u5728\u9c81\u68d2\u5fae\u8c03\uff08RFT\uff09\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4ece\u975e\u9c81\u68d2\u6a21\u578b\u8fdb\u884cRFT\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Epsilon-Scheduling\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u6270\u52a8\u5f3a\u5ea6\u8fdb\u884c\u8c03\u5ea6\u7684\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u4ee5\u4fc3\u8fdb\u6700\u4f18\u8fc1\u79fb\u3002\u540c\u65f6\u5f15\u5165\u4e86expected robustness\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u6743\u8861\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u914d\u7f6e\uff086\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u548c5\u4e2a\u6570\u636e\u96c6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEpsilon-Scheduling\u6210\u529f\u9632\u6b62\u4e86\u6b21\u4f18\u8fc1\u79fb\uff0c\u5e76\u6301\u7eed\u63d0\u9ad8\u4e86\u9884\u671f\u9c81\u68d2\u6027\u3002", "conclusion": "Epsilon-Scheduling\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u80fd\u591f\u89e3\u51b3\u4ece\u975e\u9c81\u68d2\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9c81\u68d2\u5fae\u8c03\u65f6\u7684\u6b21\u4f18\u8fc1\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.24763", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24763", "abs": "https://arxiv.org/abs/2509.24763", "authors": ["Xiangyi Meng", "Delun Li", "Zihao Mao", "Yi Yang", "Wenjie Song"], "title": "SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations within a Hierarchical Exploration Framework", "comment": null, "summary": "Zero-shot object navigation in unknown environments presents significant\nchallenges, mainly due to two key limitations: insufficient semantic guidance\nleads to inefficient exploration, while limited spatial memory resulting from\nenvironmental structure causes entrapment in local regions. To address these\nissues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object\nnavigation method based on the TARE hierarchical exploration framework,\nintegrating a viewpoint generation strategy balancing spatial coverage and\nsemantic density with an LLM-based global guidance mechanism. The performance\nimprovement of the proposed method is due to two key innovations. First, the\nviewpoint generation strategy prioritizes areas of high semantic density within\ntraversable sub-regions to maximize spatial coverage and minimize invalid\nexploration. Second, coupled with an LLM-based global guidance mechanism, it\nassesses semantic associations to direct navigation toward high-value spaces,\npreventing local entrapment and ensuring efficient exploration. Deployed on\nhybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves\nreal-time operation and superior performance. On Matterport3D and\nHabitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\\% and\n11.2\\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140,\nrespectively, over state-of-the-art methods.", "AI": {"tldr": "SSR-ZSON\u662f\u4e00\u79cd\u57fa\u4e8eTARE\u5206\u5c42\u63a2\u7d22\u6846\u67b6\u7684\u7a7a\u95f4\u8bed\u4e49\u76f8\u5bf9\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u7a7a\u95f4\u8986\u76d6\u548c\u8bed\u4e49\u5bc6\u5ea6\u7684\u89c6\u70b9\u751f\u6210\u7b56\u7565\u4e0e\u57fa\u4e8eLLM\u7684\u5168\u5c40\u5f15\u5bfc\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\u4e2d\u7684\u8bed\u4e49\u5f15\u5bfc\u4e0d\u8db3\u548c\u7a7a\u95f4\u8bb0\u5fc6\u53d7\u9650\u95ee\u9898\u3002", "motivation": "\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8bed\u4e49\u5f15\u5bfc\u4e0d\u8db3\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u73af\u5883\u7ed3\u6784\u9020\u6210\u7684\u7a7a\u95f4\u8bb0\u5fc6\u53d7\u9650\u5bfc\u81f4\u5c40\u90e8\u533a\u57df\u56f0\u9677\u3002", "method": "\u63d0\u51faSSR-ZSON\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u89c6\u70b9\u751f\u6210\u7b56\u7565\u4f18\u5148\u8003\u8651\u53ef\u904d\u5386\u5b50\u533a\u57df\u5185\u9ad8\u8bed\u4e49\u5bc6\u5ea6\u533a\u57df\u4ee5\u6700\u5927\u5316\u7a7a\u95f4\u8986\u76d6\uff1b2\uff09\u7ed3\u5408\u57fa\u4e8eLLM\u7684\u5168\u5c40\u5f15\u5bfc\u673a\u5236\u8bc4\u4f30\u8bed\u4e49\u5173\u8054\uff0c\u5f15\u5bfc\u5bfc\u822a\u671d\u5411\u9ad8\u4ef7\u503c\u7a7a\u95f4\u3002", "result": "\u5728Matterport3D\u548cHabitat-Matterport3D\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u5206\u522b\u63d0\u534718.5%\u548c11.2%\uff0c\u8def\u5f84\u957f\u5ea6\u52a0\u6743\u6210\u529f\u7387\u5206\u522b\u63d0\u53470.181\u548c0.140\u3002", "conclusion": "SSR-ZSON\u5b9e\u73b0\u4e86\u5b9e\u65f6\u64cd\u4f5c\u548c\u4f18\u8d8a\u6027\u80fd\uff0c\u901a\u8fc7\u5e73\u8861\u7a7a\u95f4\u8986\u76d6\u548c\u8bed\u4e49\u5bc6\u5ea6\u7684\u89c6\u70b9\u751f\u6210\u7b56\u7565\u4e0eLLM\u5168\u5c40\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u5c40\u90e8\u56f0\u9677\u95ee\u9898\u3002"}}
{"id": "2509.23410", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.23410", "abs": "https://arxiv.org/abs/2509.23410", "authors": ["Younes Hourri", "Mohammad Mozaffari", "Maryam Mehri Dehnavi"], "title": "PATCH: Learnable Tile-level Hybrid Sparsity for LLMs", "comment": null, "summary": "Large language models (LLMs) deliver impressive performance but incur\nprohibitive memory and compute costs at deployment. Model pruning is an\neffective way to reduce these overheads, yet existing approaches face\nchallenges: unstructured sparsity, where nonzeros can appear anywhere,\npreserves accuracy but yields irregular access patterns that prevent GPU\nacceleration, while semi-structured 2:4 sparsity is hardware-friendly but\nenforces a rigid 50% pattern that degrades model quality. To bridge this gap,\nwe introduce PATCH, a hybrid sparsity framework that enables a continuous\nsparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles,\nassigning each tile to be either dense or 2:4 sparse via a learnable mask\nselection mechanism. This design provides fine-grained control over\naccuracy-acceleration tradeoffs and supports non-uniform sparsity across\nlayers, leading to superior overall quality. Across models from 0.5B to 8B\nparameters, PATCH consistently narrows the gap to dense accuracy while\ndelivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU,\nPATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while\nimproving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning\nmethod, MaskLLM.", "AI": {"tldr": "PATCH\u662f\u4e00\u79cd\u6df7\u5408\u7a00\u758f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u5212\u5206\u4e3a\u5757\u5e76\u5b66\u4e60\u9009\u62e9\u5bc6\u96c6\u5757\u62162:4\u7a00\u758f\u5757\uff0c\u5b9e\u73b0\u4e860%-50%\u7684\u8fde\u7eed\u7a00\u758f\u7387\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u5b9e\u9645\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u526a\u679d\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u975e\u7ed3\u6784\u5316\u7a00\u758f\u4fdd\u6301\u51c6\u786e\u6027\u4f46\u65e0\u6cd5GPU\u52a0\u901f\uff0c\u800c2:4\u7a00\u758f\u786c\u4ef6\u53cb\u597d\u4f46\u5f3a\u523650%\u6a21\u5f0f\u4f1a\u964d\u4f4e\u6a21\u578b\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u52a0\u901f\u7684\u7075\u6d3b\u7a00\u758f\u65b9\u6cd5\u3002", "method": "\u5c06\u6743\u91cd\u77e9\u9635\u5212\u5206\u4e3a\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u9009\u62e9\u673a\u5236\u4e3a\u6bcf\u4e2a\u5757\u5206\u914d\u5bc6\u96c6\u62162:4\u7a00\u758f\u6a21\u5f0f\uff0c\u652f\u6301\u8de8\u5c42\u975e\u5747\u5300\u7a00\u758f\u5206\u5e03\u548c\u7ec6\u7c92\u5ea6\u7cbe\u5ea6-\u52a0\u901f\u6743\u8861\u63a7\u5236\u3002", "result": "\u57280.5B\u52308B\u53c2\u6570\u6a21\u578b\u4e0a\uff0cPATCH\u6301\u7eed\u7f29\u5c0f\u4e0e\u5bc6\u96c6\u6a21\u578b\u7684\u51c6\u786e\u6027\u5dee\u8ddd\u5e76\u63d0\u4f9b\u5b9e\u9645\u52a0\u901f\u3002\u5728LLaMA-2 7B\u4e0a\uff0c\u76f8\u6bd4\u5bc6\u96c6\u57fa\u7ebf\u5b9e\u73b01.18x-1.38x\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u76842:4\u526a\u679d\u65b9\u6cd5MaskLLM\u51c6\u786e\u6027\u63d0\u53470.37%-2.96%\u3002", "conclusion": "PATCH\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u7a00\u758f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6df7\u5408\u7a00\u758f\u7b56\u7565\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u52a0\u901f\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23433", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23433", "abs": "https://arxiv.org/abs/2509.23433", "authors": ["Sahithya Ravi", "Aditya Chinchure", "Raymond T. Ng", "Leonid Sigal", "Vered Shwartz"], "title": "SPIKE-RL: Video-LLMs meet Bayesian Surprise", "comment": "10 pages, 4 figures, Code: https://github.com/sahithyaravi/SPIKE-RL", "summary": "Real-world videos often show routine activities punctuated by memorable,\nsurprising events. However, most Video-LLMs process videos by sampling frames\nuniformly, likely missing critical moments that define a video's narrative. We\nintroduce SPIKE, an inference-time framework that quantifies Bayesian Surprise\nas the belief update triggered by new visual evidence in the video stream,\nidentifying moments where new visual evidence conflicts with prior beliefs.\nSPIKE effectively localizes surprise in videos, strongly correlated with humans\non positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs\nof zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which\nleverages GRPO to optimize belief hypotheses based on a reward signal from the\nvideo caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame\nsampling, which allocates more frames to interesting moments in the video. With\nthis strategy, we achieve consistent performance gains on five downstream\nbenchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and\nregister surprise, our work paves the way for more robust models that can\nrevise their understanding in response to new information.", "AI": {"tldr": "SPIKE\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u8d1d\u53f6\u65af\u60ca\u559c\u6765\u8bc6\u522b\u89c6\u9891\u4e2d\u7684\u5173\u952e\u65f6\u523b\uff0c\u5e76\u57fa\u4e8e\u60ca\u559c\u6743\u91cd\u8fdb\u884c\u5e27\u91c7\u6837\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5747\u5300\u91c7\u6837\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u901a\u5e38\u5305\u542b\u5e38\u89c4\u6d3b\u52a8\u548c\u4ee4\u4eba\u96be\u5fd8\u7684\u60ca\u559c\u4e8b\u4ef6\uff0c\u4f46\u73b0\u6709\u7684Video-LLMs\u901a\u8fc7\u5747\u5300\u91c7\u6837\u5e27\u53ef\u80fd\u9519\u8fc7\u5b9a\u4e49\u89c6\u9891\u53d9\u4e8b\u7684\u5173\u952e\u65f6\u523b\u3002", "method": "\u5f15\u5165SPIKE\u6846\u67b6\u91cf\u5316\u8d1d\u53f6\u65af\u60ca\u559c\uff08\u65b0\u89c6\u89c9\u8bc1\u636e\u5f15\u53d1\u7684\u4fe1\u5ff5\u66f4\u65b0\uff09\uff0c\u5e76\u5f00\u53d1SPIKE-RL\u4f7f\u7528GRPO\u4f18\u5316\u4fe1\u5ff5\u5047\u8bbe\uff0c\u57fa\u4e8e\u89c6\u9891\u6807\u9898\u7684\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u60ca\u559c\u52a0\u6743\u5e27\u91c7\u6837\u3002", "result": "SPIKE\u80fd\u6709\u6548\u5b9a\u4f4d\u89c6\u9891\u4e2d\u7684\u60ca\u559c\u65f6\u523b\uff0c\u4e0e\u4eba\u7c7b\u5728\u6b63\u8d1f\u60ca\u559c\u57fa\u51c6\u4e0a\u9ad8\u5ea6\u76f8\u5173\u3002\u60ca\u559c\u52a0\u6743\u5e27\u91c7\u6837\u5728\u4e94\u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5747\u5300\u91c7\u6837\u3002", "conclusion": "\u901a\u8fc7\u8ba9Video-LLMs\u8ddf\u8e2a\u4fe1\u5ff5\u5e76\u6ce8\u518c\u60ca\u559c\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u6839\u636e\u65b0\u4fe1\u606f\u4fee\u6b63\u7406\u89e3\u3002"}}
{"id": "2509.23936", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23936", "abs": "https://arxiv.org/abs/2509.23936", "authors": ["Zhangdie Yuan", "Zifeng Ding", "Andreas Vlachos"], "title": "Assessing Large Language Models in Updating Their Forecasts with New Information", "comment": null, "summary": "Prior work has largely treated future event prediction as a static task,\nfailing to consider how forecasts and the confidence in them should evolve as\nnew evidence emerges. To address this gap, we introduce EVOLVECAST, a framework\nfor evaluating whether large language models appropriately revise their\npredictions in response to new information. In particular, EVOLVECAST assesses\nwhether LLMs adjust their forecasts when presented with information released\nafter their training cutoff. We use human forecasters as a comparative\nreference to analyze prediction shifts and confidence calibration under updated\ncontexts. While LLMs demonstrate some responsiveness to new information, their\nupdates are often inconsistent or overly conservative. We further find that\nneither verbalized nor logits-based confidence estimates consistently\noutperform the other, and both remain far from the human reference standard.\nAcross settings, models tend to express conservative bias, underscoring the\nneed for more robust approaches to belief updating.", "AI": {"tldr": "EVOLVECAST\u6846\u67b6\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u4fe1\u606f\u51fa\u73b0\u65f6\u662f\u5426\u9002\u5f53\u8c03\u6574\u9884\u6d4b\uff0c\u53d1\u73b0LLM\u7684\u9884\u6d4b\u66f4\u65b0\u5f80\u5f80\u4e0d\u4e00\u81f4\u6216\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u8fdc\u672a\u8fbe\u5230\u4eba\u7c7b\u6807\u51c6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u89c6\u4e3a\u9759\u6001\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u968f\u7740\u65b0\u8bc1\u636e\u51fa\u73b0\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u5e94\u5982\u4f55\u6f14\u53d8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528EVOLVECAST\u6846\u67b6\uff0c\u4ee5\u4eba\u7c7b\u9884\u6d4b\u8005\u4e3a\u53c2\u8003\u57fa\u51c6\uff0c\u5206\u6790LLM\u5728\u66f4\u65b0\u4e0a\u4e0b\u6587\u4e0b\u7684\u9884\u6d4b\u504f\u79fb\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "result": "LLM\u5bf9\u65b0\u4fe1\u606f\u6709\u4e00\u5b9a\u54cd\u5e94\u6027\uff0c\u4f46\u66f4\u65b0\u5f80\u5f80\u4e0d\u4e00\u81f4\u6216\u8fc7\u4e8e\u4fdd\u5b88\uff1b\u53e3\u5934\u8868\u8fbe\u548c\u57fa\u4e8elogits\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5747\u672a\u663e\u8457\u4f18\u4e8e\u5bf9\u65b9\uff0c\u4e14\u90fd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6807\u51c6\u3002", "conclusion": "\u6a21\u578b\u666e\u904d\u8868\u73b0\u51fa\u4fdd\u5b88\u504f\u5dee\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u4fe1\u5ff5\u66f4\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.23475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23475", "abs": "https://arxiv.org/abs/2509.23475", "authors": ["Ming-Tsung Hsu", "Fang-Yu Hsu", "Yi-Ting Lin", "Kai-Heng Chien", "Jun-Ren Chen", "Cheng-Hsiang Su", "Yi-Chen Ou", "Chiou-Ting Hsu", "Pei-Kai Huang"], "title": "Robust Multi-Modal Face Anti-Spoofing with Domain Adaptation: Tackling Missing Modalities, Noisy Pseudo-Labels, and Model Degradation", "comment": null, "summary": "Recent multi-modal face anti-spoofing (FAS) methods have investigated the\npotential of leveraging multiple modalities to distinguish live and spoof\nfaces. However, pre-adapted multi-modal FAS models often fail to detect unseen\nattacks from new target domains. Although a more realistic domain adaptation\n(DA) scenario has been proposed for single-modal FAS to learn specific spoof\nattacks during inference, DA remains unexplored in multi-modal FAS methods. In\nthis paper, we propose a novel framework, MFAS-DANet, to address three major\nchallenges in multi-modal FAS under the DA scenario: missing modalities, noisy\npseudo labels, and model degradation. First, to tackle the issue of missing\nmodalities, we propose extracting complementary features from other modalities\nto substitute missing modality features or enhance existing ones. Next, to\nreduce the impact of noisy pseudo labels during model adaptation, we propose\nderiving reliable pseudo labels by leveraging prediction uncertainty across\ndifferent modalities. Finally, to prevent model degradation, we design an\nadaptive mechanism that decreases the loss weight during unstable adaptations\nand increasing it during stable ones. Extensive experiments demonstrate the\neffectiveness and state-of-the-art performance of our proposed MFAS-DANet.", "AI": {"tldr": "\u63d0\u51fa\u4e86MFAS-DANet\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u4eba\u8138\u53cd\u6b3a\u8bc8\u5728\u57df\u81ea\u9002\u5e94\u573a\u666f\u4e0b\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u6a21\u6001\u7f3a\u5931\u3001\u566a\u58f0\u4f2a\u6807\u7b7e\u548c\u6a21\u578b\u9000\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u4eba\u8138\u53cd\u6b3a\u8bc8\u6a21\u578b\u96be\u4ee5\u68c0\u6d4b\u6765\u81ea\u65b0\u76ee\u6807\u57df\u7684\u672a\u77e5\u653b\u51fb\uff0c\u4e14\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "1) \u4ece\u5176\u4ed6\u6a21\u6001\u63d0\u53d6\u4e92\u8865\u7279\u5f81\u6765\u66ff\u4ee3\u7f3a\u5931\u6a21\u6001\u6216\u589e\u5f3a\u73b0\u6709\u7279\u5f81\uff1b2) \u5229\u7528\u591a\u6a21\u6001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u53ef\u9760\u4f2a\u6807\u7b7e\uff1b3) \u8bbe\u8ba1\u81ea\u9002\u5e94\u673a\u5236\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eMFAS-DANet\u7684\u6709\u6548\u6027\u548c\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u4eba\u8138\u53cd\u6b3a\u8bc8\u5728\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.23982", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.23982", "abs": "https://arxiv.org/abs/2509.23982", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering", "comment": null, "summary": "Preference alignment is a critical step in making Large Language Models\n(LLMs) useful and aligned with (human) preferences. Existing approaches such as\nReinforcement Learning from Human Feedback or Direct Preference Optimization\ntypically require curated data and expensive optimization over billions of\nparameters, and eventually lead to persistent task-specific models. In this\nwork, we introduce Preference alignment of Large Language Models via Residual\nSteering (PaLRS), a training-free method that exploits preference signals\nencoded in the residual streams of LLMs. From as few as one hundred preference\npairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be\napplied at inference time to push models toward preferred behaviors. We\nevaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that\nPaLRS-aligned models achieve consistent gains on mathematical reasoning and\ncode generation benchmarks while preserving baseline general-purpose\nperformance. Moreover, when compared to DPO-aligned models, they perform better\nwith huge time savings. Our findings highlight that PaLRS offers an effective,\nmuch more efficient and flexible alternative to standard preference\noptimization pipelines, offering a training-free, plug-and-play mechanism for\nalignment with minimal data.", "AI": {"tldr": "PaLRS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u6b8b\u5dee\u6d41\u4e2d\u7684\u504f\u597d\u4fe1\u53f7\u6765\u521b\u5efa\u8f7b\u91cf\u7ea7\u8f6c\u5411\u5411\u91cf\uff0c\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u6a21\u578b\u4ea7\u751f\u504f\u597d\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\u3001DPO\uff09\u9700\u8981\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u548c\u6602\u8d35\u7684\u53c2\u6570\u4f18\u5316\uff0c\u5bfc\u81f4\u4efb\u52a1\u7279\u5b9a\u7684\u6a21\u578b\u3002PaLRS\u65e8\u5728\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u8bad\u7ec3\u514d\u8d39\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4ece\u5c11\u91cf\u504f\u597d\u5bf9\u4e2d\u63d0\u53d6\u6b8b\u5dee\u6d41\u4e2d\u7684\u504f\u597d\u4fe1\u53f7\uff0c\u521b\u5efa\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u8f6c\u5411\u5411\u91cf\uff0c\u5728\u63a8\u7406\u65f6\u5e94\u7528\u8fd9\u4e9b\u5411\u91cf\u6765\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\u3002", "result": "PaLRS\u5bf9\u9f50\u7684\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e0a\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7ebf\u901a\u7528\u6027\u80fd\u3002\u4e0eDPO\u5bf9\u9f50\u6a21\u578b\u76f8\u6bd4\u8868\u73b0\u66f4\u597d\u4e14\u8282\u7701\u5927\u91cf\u65f6\u95f4\u3002", "conclusion": "PaLRS\u4e3a\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u3001\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ec5\u9700\u6700\u5c11\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8bad\u7ec3\u514d\u8d39\u7684\u5373\u63d2\u5373\u7528\u5bf9\u9f50\u673a\u5236\u3002"}}
{"id": "2509.24443", "categories": ["cs.AI", "cs.ET", "cs.SE", "68T01, 68T09, 68M14, 68W10, 68W15", "C.2.4; C.4; C.5; D.2.2; D.2.11; I.2.5; I.2.6; I.2.11; J.0; J.7"], "pdf": "https://arxiv.org/pdf/2509.24443", "abs": "https://arxiv.org/abs/2509.24443", "authors": ["Leila Ismail", "Abdelmoneim Abdelmoti", "Arkaprabha Basu", "Aymen Dia Eddine Berini", "Mohammad Naouss"], "title": "A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions", "comment": null, "summary": "With the increasing complexity of industrial systems, there is a pressing\nneed for predictive maintenance to avoid costly downtime and disastrous\noutcomes that could be life-threatening in certain domains. With the growing\npopularity of the Internet of Things, Artificial Intelligence, machine\nlearning, and real-time big data analytics, there is a unique opportunity for\nefficient predictive maintenance to forecast equipment failures for real-time\nintervention and optimize maintenance actions, as traditional reactive and\npreventive maintenance practices are often inadequate to meet the requirements\nfor the industry to provide quality-of-services of operations. Central to this\nevolution is digital twin technology, an adaptive virtual replica that\ncontinuously monitors and integrates sensor data to simulate and improve asset\nperformance. Despite remarkable progress in digital twin implementations, such\nas considering DT in predictive maintenance for industrial engineering. This\npaper aims to address this void. We perform a retrospective analysis of the\ntemporal evolution of the digital twin in predictive maintenance for industrial\nengineering to capture the applications, middleware, and technological\nrequirements that led to the development of the digital twin from its inception\nto the AI-enabled digital twin and its self-learning models. We provide a\nlayered architecture of the digital twin technology, as well as a taxonomy of\nthe technology-enabled industrial engineering applications systems, middleware,\nand the used Artificial Intelligence algorithms. We provide insights into these\nsystems for the realization of a trustworthy and efficient smart digital-twin\nindustrial engineering ecosystem. We discuss future research directions in\ndigital twin for predictive maintenance in industrial engineering.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u5de5\u4e1a\u5de5\u7a0b\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5206\u6790\u4e86\u5176\u5e94\u7528\u3001\u4e2d\u95f4\u4ef6\u548c\u6280\u672f\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u5206\u5c42\u67b6\u6784\u548c\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u53cd\u5e94\u5f0f\u548c\u9884\u9632\u6027\u7ef4\u62a4\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u5229\u7528\u7269\u8054\u7f51\u3001\u4eba\u5de5\u667a\u80fd\u548c\u5927\u6570\u636e\u5206\u6790\u7b49\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u800c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u5176\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\u3002", "method": "\u91c7\u7528\u56de\u987e\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u68b3\u7406\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u5de5\u4e1a\u5de5\u7a0b\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u7684\u65f6\u95f4\u6f14\u5316\u8fc7\u7a0b\uff0c\u5305\u62ec\u5e94\u7528\u3001\u4e2d\u95f4\u4ef6\u548c\u6280\u672f\u9700\u6c42\uff0c\u63d0\u51fa\u5206\u5c42\u67b6\u6784\u548c\u6280\u672f\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u6784\u5efa\u4e86\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u5206\u5c42\u67b6\u6784\uff0c\u5bf9\u6280\u672f\u9a71\u52a8\u7684\u5de5\u4e1a\u5de5\u7a0b\u5e94\u7528\u7cfb\u7edf\u3001\u4e2d\u95f4\u4ef6\u548c\u4f7f\u7528\u7684AI\u7b97\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u4e3a\u5b9e\u73b0\u53ef\u4fe1\u9ad8\u6548\u7684\u667a\u80fd\u6570\u5b57\u5b6a\u751f\u5de5\u4e1a\u5de5\u7a0b\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u5de5\u4e1a\u5de5\u7a0b\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u672a\u6765\u9700\u8981\u5728\u6570\u5b57\u5b6a\u751f\u81ea\u5b66\u4e60\u6a21\u578b\u3001\u53ef\u4fe1\u5ea6\u7b49\u65b9\u9762\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u4ee5\u63a8\u52a8\u667a\u80fd\u5de5\u4e1a\u5de5\u7a0b\u751f\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.23535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23535", "abs": "https://arxiv.org/abs/2509.23535", "authors": ["Ibne Farabi Shihab", "Weiheng Chai", "Jiyang Wang", "Sanjeda Akter", "Senem Velipasalar Gursoy", "Anuj Sharma"], "title": "Calibrated and Resource-Aware Super-Resolution for Reliable Driver Behavior Analysis", "comment": null, "summary": "Driver monitoring systems require not just high accuracy but reliable,\nwell-calibrated confidence scores for safety-critical deployment. While direct\nlow-resolution training yields high overall accuracy, it produces poorly\ncalibrated predictions that can be dangerous in safety-critical scenarios. We\npropose a resource-aware adaptive super-resolution framework that optimizes for\nmodel calibration and high precision-recall on critical events. Our approach\nachieves state-of-the-art performance on safety-centric metrics: best\ncalibration (ECE of 5.8\\% vs 6.2\\% for LR-trained baselines), highest AUPR for\ndrowsiness detection (0.78 vs 0.74), and superior precision-recall for phone\nuse detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters,\n5.2ms overhead) provides additional safety by filtering SR-induced\nhallucinations. While LR-trained video models serve as strong general-purpose\nbaselines, our adaptive framework represents the state-of-the-art solution for\nsafety-critical applications where reliability is paramount.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\u4e2d\u7684\u6a21\u578b\u6821\u51c6\u548c\u5173\u952e\u4e8b\u4ef6\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5728\u5b89\u5168\u5173\u952e\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u8fd8\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u4ee5\u786e\u4fdd\u5b89\u5168\u5173\u952e\u90e8\u7f72\u3002\u76f4\u63a5\u4f4e\u5206\u8fa8\u7387\u8bad\u7ec3\u867d\u7136\u6574\u4f53\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u9884\u6d4b\u6821\u51c6\u5dee\uff0c\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u53ef\u80fd\u9020\u6210\u5371\u9669\u3002", "method": "\u91c7\u7528\u8d44\u6e90\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u4f2a\u5f71\u68c0\u6d4b\u5668\uff080.3M\u53c2\u6570\uff0c5.2ms\u5f00\u9500\uff09\u6765\u8fc7\u6ee4\u8d85\u5206\u8fa8\u7387\u5f15\u8d77\u7684\u5e7b\u89c9\u3002", "result": "\u5728\u5b89\u5168\u4e2d\u5fc3\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u6700\u4f73\u6821\u51c6\uff08ECE 5.8% vs \u57fa\u7ebf6.2%\uff09\u3001\u6700\u9ad8AUPR\u7528\u4e8e\u56f0\u5026\u68c0\u6d4b\uff080.78 vs 0.74\uff09\u3001\u624b\u673a\u4f7f\u7528\u68c0\u6d4b\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u66f4\u4f18\uff080.74 vs 0.71\uff09\u3002", "conclusion": "\u867d\u7136\u4f4e\u5206\u8fa8\u7387\u8bad\u7ec3\u7684\u89c6\u9891\u6a21\u578b\u662f\u5f3a\u5927\u7684\u901a\u7528\u57fa\u7ebf\uff0c\u4f46\u6211\u4eec\u7684\u81ea\u9002\u5e94\u6846\u67b6\u5728\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u4ee3\u8868\u4e86\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24014", "abs": "https://arxiv.org/abs/2509.24014", "authors": ["Zeqing Wang", "Gongfan Fang", "Xinyin Ma", "Xingyi Yang", "Xinchao Wang"], "title": "SparseD: Sparse Attention for Diffusion Language Models", "comment": "The code is available at https://github.com/INV-WZQ/SparseD", "summary": "While diffusion language models (DLMs) offer a promising alternative to\nautoregressive models (ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to the attention's quadratic\ncomplexity with respect to context length in computing all query-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrict\nattention to sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, where attention follows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1) attention patterns vary across heads, (2) attention patterns in\neach head remain highly similar across denoising steps, and (3) early denoising\nsteps are critical for generation. These findings render sparse attention\nmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapture head-specific structures and risk degrading generation when applied in\nearly denoising steps. To address these challenges, we propose SparseD, a novel\nsparse attention method for DLMs. Leveraging the observations, SparseD only\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile, SparseD uses full attention in the early steps, then switches\nto sparse attention later to maintain generation quality. Together, these\nestablish SparseD as a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate that SparseD\nachieves lossless acceleration, delivering up to $1.50\\times$ speedup over\nFlashAttention at a 64k context length with 1,024 denoising steps.", "AI": {"tldr": "\u63d0\u51fa\u4e86SparseD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u7279\u5b9a\u7279\u6027\uff08\u5934\u90e8\u7279\u5f02\u6027\u3001\u8de8\u6b65\u76f8\u4f3c\u6027\u3001\u65e9\u671f\u6b65\u9aa4\u5173\u952e\u6027\uff09\uff0c\u5b9e\u73b0\u4e86\u65e0\u635f\u52a0\u901f\uff0c\u572864k\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u6bd4FlashAttention\u5feb1.5\u500d\u3002", "motivation": "\u73b0\u6709\u7684\u5f00\u6e90\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9ad8\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u3002\u867d\u7136\u7a00\u758f\u6ce8\u610f\u529b\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u5df2\u6709\u5e94\u7528\uff0c\u4f46\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u5177\u6709\u4e0d\u540c\u7684\u7a00\u758f\u7279\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u3002", "method": "SparseD\u65b9\u6cd5\uff1a1\uff09\u9884\u8ba1\u7b97\u5934\u90e8\u7279\u5b9a\u7684\u7a00\u758f\u6a21\u5f0f\u5e76\u8de8\u6b65\u91cd\u7528\uff1b2\uff09\u65e9\u671f\u6b65\u9aa4\u4f7f\u7528\u5168\u6ce8\u610f\u529b\uff0c\u540e\u671f\u5207\u6362\u4e3a\u7a00\u758f\u6ce8\u610f\u529b\u4ee5\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSparseD\u5b9e\u73b0\u4e86\u65e0\u635f\u52a0\u901f\uff0c\u572864k\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c1024\u53bb\u566a\u6b65\u9aa4\u4e0b\uff0c\u6bd4FlashAttention\u5feb1.5\u500d\u3002", "conclusion": "SparseD\u662f\u4e00\u4e2a\u5b9e\u7528\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u5728\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u4e2d\u90e8\u7f72\u6269\u6563\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2509.23541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23541", "abs": "https://arxiv.org/abs/2509.23541", "authors": ["Hongyang Li", "Jinyuan Qu", "Lei Zhang"], "title": "OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction", "comment": null, "summary": "In this paper, we propose a training scheme called OVSeg3R to learn\nopen-vocabulary 3D instance segmentation from well-studied 2D perception models\nwith the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes\nfrom 2D videos as input, avoiding costly manual adjustment while aligning input\nwith real-world applications. By exploiting the 2D to 3D correspondences\nprovided by 3D reconstruction models, OVSeg3R projects each view's 2D instance\nmask predictions, obtained from an open-vocabulary 2D model, onto 3D to\ngenerate annotations for the view's corresponding sub-scene. To avoid\nincorrectly introduced false positives as supervision due to partial\nannotations from 2D to 3D, we propose a View-wise Instance Partition algorithm,\nwhich partitions predictions to their respective views for supervision,\nstabilizing the training process. Furthermore, since 3D reconstruction models\ntend to over-smooth geometric details, clustering reconstructed points into\nrepresentative super-points based solely on geometry, as commonly done in\nmainstream 3D segmentation methods, may overlook geometrically non-salient\nobjects. We therefore introduce 2D Instance Boundary-aware Superpoint, which\nleverages 2D masks to constrain the superpoint clustering, preventing\nsuperpoints from violating instance boundaries. With these designs, OVSeg3R not\nonly extends a state-of-the-art closed-vocabulary 3D instance segmentation\nmodel to open-vocabulary, but also substantially narrows the performance gap\nbetween tail and head classes, ultimately leading to an overall improvement of\n+2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard\nopen-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP\non the novel classes, further validating its effectiveness.", "AI": {"tldr": "OVSeg3R\u662f\u4e00\u79cd\u4ece2D\u611f\u77e5\u6a21\u578b\u5b66\u4e60\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5206\u5272\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5229\u75283D\u91cd\u5efa\u6280\u672f\u5c062D\u5b9e\u4f8b\u63a9\u7801\u6295\u5f71\u52303D\u7a7a\u95f4\u751f\u6210\u6807\u6ce8\uff0c\u901a\u8fc7\u89c6\u56fe\u7ea7\u5b9e\u4f8b\u5206\u533a\u548c2D\u5b9e\u4f8b\u8fb9\u754c\u611f\u77e5\u8d85\u70b9\u805a\u7c7b\u7b49\u6280\u672f\uff0c\u5728ScanNet200\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86+2.3 mAP\u7684\u6574\u4f53\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u4ece\u6210\u719f\u76842D\u611f\u77e5\u6a21\u578b\u4e2d\u5b66\u4e60\u5f00\u653e\u8bcd\u6c47\u76843D\u5b9e\u4f8b\u5206\u5272\uff0c\u907f\u514d\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\uff0c\u5e76\u4f7f\u8f93\u5165\u4e0e\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u5bf9\u9f50\u3002", "method": "\u5229\u75283D\u91cd\u5efa\u6a21\u578b\u63d0\u4f9b\u76842D\u52303D\u5bf9\u5e94\u5173\u7cfb\uff0c\u5c062D\u5f00\u653e\u8bcd\u6c47\u6a21\u578b\u7684\u5b9e\u4f8b\u63a9\u7801\u9884\u6d4b\u6295\u5f71\u52303D\u7a7a\u95f4\u751f\u6210\u6807\u6ce8\uff1b\u63d0\u51fa\u89c6\u56fe\u7ea7\u5b9e\u4f8b\u5206\u533a\u7b97\u6cd5\u907f\u514d\u9519\u8bef\u76d1\u7763\uff1b\u5f15\u51652D\u5b9e\u4f8b\u8fb9\u754c\u611f\u77e5\u8d85\u70b9\u805a\u7c7b\u9632\u6b62\u8d85\u70b9\u8de8\u8d8a\u5b9e\u4f8b\u8fb9\u754c\u3002", "result": "\u5728ScanNet200\u57fa\u51c6\u4e0a\u6574\u4f53\u63d0\u5347+2.3 mAP\uff0c\u5c3e\u7c7b\u548c\u5934\u7c7b\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u7f29\u5c0f\uff1b\u5728\u6807\u51c6\u5f00\u653e\u8bcd\u6c47\u8bbe\u7f6e\u4e0b\uff0c\u65b0\u7c7b\u6027\u80fd\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u5347\u7ea6+7.1 mAP\u3002", "conclusion": "OVSeg3R\u6210\u529f\u5c06\u6700\u5148\u8fdb\u7684\u5c01\u95ed\u8bcd\u6c473D\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u6269\u5c55\u5230\u5f00\u653e\u8bcd\u6c47\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u548c\u65b0\u7c7b\u8bc6\u522b\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.24803", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24803", "abs": "https://arxiv.org/abs/2509.24803", "authors": ["Tong Guan", "Zijie Meng", "Dianqi Li", "Shiyu Wang", "Chao-Han Huck Yang", "Qingsong Wen", "Zuozhu Liu", "Sabato Marco Siniscalchi", "Ming Jin", "Shirui Pan"], "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models", "comment": null, "summary": "Recent advances in multimodal time series learning underscore a paradigm\nshift from analytics centered on basic patterns toward advanced time series\nunderstanding and reasoning. However, existing multimodal time series datasets\nmostly remain at the level of surface alignment and question answering, without\nreaching the depth of genuine reasoning. The absence of well-defined tasks that\ngenuinely require time series reasoning, along with the scarcity of\nhigh-quality data, has limited progress in building practical time series\nreasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite\n(TSR-Suite), which formalizes four atomic tasks that span three fundamental\ncapabilities for reasoning with time series: (1) perception, acquired through\nscenario understanding and causality discovery; (2) extrapolation, realized via\nevent-aware forecasting; and (3) decision-making, developed through\ndeliberation over perception and extrapolation. TSR-Suite is the first\ncomprehensive time series reasoning suite that supports not only thorough\nevaluation but also the data pipeline and training of TSRMs. It contains more\nthan 23K samples, of which 2.3K are carefully curated through a human-guided\nhierarchical annotation process. Building on this foundation, we introduce\nTimeOmni-1, the first unified reasoning model designed to address diverse\nreal-world problems demanding time series reasoning. The model is trained in\nmultiple stages, integrating a mixture of task scenarios, novel reward\nfunctions, and tailored optimizations. Experiments show that TimeOmni-1\ndelivers strong out-of-distribution generalization across all tasks and\nachieves a high rate of valid responses. It significantly improves causality\ndiscovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response\nrate by over 6% compared to GPT-4.1 on the event-aware forecasting task.", "AI": {"tldr": "\u63d0\u51fa\u4e86TSR-Suite\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u5957\u4ef6\uff0c\u5305\u542b\u56db\u4e2a\u539f\u5b50\u4efb\u52a1\u548c\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff08\u611f\u77e5\u3001\u5916\u63a8\u3001\u51b3\u7b56\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u9996\u4e2a\u7edf\u4e00\u63a8\u7406\u6a21\u578bTimeOmni-1\uff0c\u5728\u56e0\u679c\u53d1\u73b0\u548c\u4e8b\u4ef6\u611f\u77e5\u9884\u6d4b\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4.1\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u5927\u591a\u505c\u7559\u5728\u8868\u9762\u5bf9\u9f50\u548c\u95ee\u7b54\u5c42\u9762\uff0c\u7f3a\u4e4f\u771f\u6b63\u9700\u8981\u63a8\u7406\u7684\u4efb\u52a1\u5b9a\u4e49\u548c\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u9650\u5236\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efaTSR-Suite\u5957\u4ef6\uff0c\u5305\u542b23K+\u6837\u672c\uff08\u5176\u4e2d2.3K\u7ecf\u8fc7\u4eba\u5de5\u6807\u6ce8\uff09\uff0c\u5b9a\u4e49\u56db\u4e2a\u539f\u5b50\u4efb\u52a1\u548c\u4e09\u4e2a\u63a8\u7406\u80fd\u529b\uff1b\u5f00\u53d1TimeOmni-1\u6a21\u578b\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u7ed3\u5408\u4efb\u52a1\u573a\u666f\u6df7\u5408\u3001\u65b0\u5956\u52b1\u51fd\u6570\u548c\u5b9a\u5236\u4f18\u5316\u3002", "result": "TimeOmni-1\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u679c\u53d1\u73b0\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0864.0% vs 35.9%\uff09\uff0c\u4e8b\u4ef6\u611f\u77e5\u9884\u6d4b\u4efb\u52a1\u7684\u6709\u6548\u54cd\u5e94\u7387\u6bd4GPT-4.1\u63d0\u9ad8\u8d85\u8fc76%\u3002", "conclusion": "TSR-Suite\u662f\u9996\u4e2a\u5168\u9762\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u5957\u4ef6\uff0cTimeOmni-1\u4f5c\u4e3a\u9996\u4e2a\u7edf\u4e00\u63a8\u7406\u6a21\u578b\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.23665", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2509.23665", "abs": "https://arxiv.org/abs/2509.23665", "authors": ["Kristina P. Sinaga", "Arjun S. Nair"], "title": "Calibration Meets Reality: Making Machine Learning Predictions Trustworthy", "comment": "30 pages, 7 figures, 5 tables", "summary": "Post-hoc calibration methods are widely used to improve the reliability of\nprobabilistic predictions from machine learning models. Despite their\nprevalence, a comprehensive theoretical understanding of these methods remains\nelusive, particularly regarding their performance across different datasets and\nmodel architectures. Input features play a crucial role in shaping model\npredictions and, consequently, their calibration. However, the interplay\nbetween feature quality and calibration performance has not been thoroughly\ninvestigated. In this work, we present a rigorous theoretical analysis of\npost-hoc calibration methods, focusing on Platt scaling and isotonic\nregression. We derive convergence guarantees, computational complexity bounds,\nand finite-sample performance metrics for these methods. Furthermore, we\nexplore the impact of feature informativeness on calibration performance\nthrough controlled synthetic experiments. Our empirical evaluation spans a\ndiverse set of real-world datasets and model architectures, demonstrating\nconsistent improvements in calibration metrics across various scenarios. By\nexamining calibration performance under varying feature conditions utilizing\nonly informative features versus complete feature spaces including noise\ndimensions, we provide fundamental insights into the robustness and reliability\nof different calibration approaches. Our findings offer practical guidelines\nfor selecting appropriate calibration methods based on dataset characteristics\nand computational constraints, bridging the gap between theoretical\nunderstanding and practical implementation in uncertainty quantification. Code\nand experimental data are available at:\nhttps://github.com/Ajwebdevs/calibration-analysis-experiments.", "AI": {"tldr": "\u672c\u6587\u5bf9\u540e\u9a8c\u6821\u51c6\u65b9\u6cd5\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u91cd\u70b9\u7814\u7a76\u4e86Platt\u7f29\u653e\u548c\u4fdd\u5e8f\u56de\u5f52\uff0c\u63a8\u5bfc\u4e86\u6536\u655b\u4fdd\u8bc1\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7279\u5f81\u4fe1\u606f\u91cf\u5bf9\u6821\u51c6\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u540e\u9a8c\u6821\u51c6\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u6027\u80fd\u7f3a\u4e4f\u5168\u9762\u7684\u7406\u8bba\u7406\u89e3\uff0c\u7279\u522b\u662f\u7279\u5f81\u8d28\u91cf\u4e0e\u6821\u51c6\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u6df1\u5165\u7814\u7a76\u3002", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u63a8\u5bfcPlatt\u7f29\u653e\u548c\u4fdd\u5e8f\u56de\u5f52\u7684\u6536\u655b\u4fdd\u8bc1\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5408\u6210\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8bc4\u4f30\u7279\u5f81\u4fe1\u606f\u91cf\u5bf9\u6821\u51c6\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6821\u51c6\u65b9\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e0b\u90fd\u80fd\u6301\u7eed\u6539\u8fdb\u6821\u51c6\u6307\u6807\uff0c\u901a\u8fc7\u6bd4\u8f83\u4ec5\u4f7f\u7528\u4fe1\u606f\u7279\u5f81\u4e0e\u5305\u542b\u566a\u58f0\u7ef4\u5ea6\u7684\u5b8c\u6574\u7279\u5f81\u7a7a\u95f4\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6821\u51c6\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8e\u6570\u636e\u96c6\u7279\u5f81\u548c\u8ba1\u7b97\u7ea6\u675f\u9009\u62e9\u9002\u5f53\u7684\u6821\u51c6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5f25\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7406\u8bba\u7406\u89e3\u4e0e\u5b9e\u9645\u5b9e\u65bd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.25146", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25146", "abs": "https://arxiv.org/abs/2509.25146", "authors": ["Richeek Das", "Kostas Daniilidis", "Pratik Chaudhari"], "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events", "comment": "39 pages, 9 figures", "summary": "This paper develops a mathematical argument and algorithms for building\nrepresentations of data from event-based cameras, that we call Fast Feature\nField ($\\text{F}^3$). We learn this representation by predicting future events\nfrom past events and show that it preserves scene structure and motion\ninformation. $\\text{F}^3$ exploits the sparsity of event data and is robust to\nnoise and variations in event rates. It can be computed efficiently using ideas\nfrom multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and\n440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous\nspatiotemporal volume as a multi-channel image, enabling a range of downstream\ntasks. We obtain state-of-the-art performance on optical flow estimation,\nsemantic segmentation, and monocular metric depth estimation, on data from\nthree robotic platforms (a car, a quadruped robot and a flying platform),\nacross different lighting conditions (daytime, nighttime), environments\n(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors\n(resolutions and event rates). Our implementations can predict these tasks at\n25-75 Hz at HD resolution.", "AI": {"tldr": "\u63d0\u51faF\u00b3\uff08Fast Feature Field\uff09\u8868\u793a\u65b9\u6cd5\uff0c\u4ece\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u5b66\u4e60\u7279\u5f81\u8868\u793a\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u6765\u4fdd\u7559\u573a\u666f\u7ed3\u6784\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u7a00\u758f\u6570\u636e\u3001\u5bf9\u566a\u58f0\u548c\u4e8b\u4ef6\u7387\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u5229\u7528\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f16\u7801\u548c\u6df1\u5ea6\u96c6\u5408\u601d\u60f3\uff0c\u5c06\u4e8b\u4ef6\u6570\u636e\u8868\u793a\u4e3a\u8fde\u7eed\u65f6\u7a7a\u4f53\u79ef\u5185\u7684\u591a\u901a\u9053\u56fe\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u6765\u5b66\u4e60\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728HD\u5206\u8fa8\u7387\u4e0b\u8fbe\u5230120Hz\uff0cVGA\u5206\u8fa8\u7387\u4e0b\u8fbe\u5230440Hz\u7684\u8ba1\u7b97\u6548\u7387\uff1b\u5728\u5149\u6d41\u4f30\u8ba1\u3001\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e0a\u83b7\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u630125-75Hz\u7684\u5b9e\u65f6\u9884\u6d4b\u3002", "conclusion": "F\u00b3\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u4e8b\u4ef6\u6570\u636e\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u3001\u5149\u7167\u6761\u4ef6\u548c\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u89c6\u89c9\u4efb\u52a1\u5904\u7406\u3002"}}
{"id": "2509.23712", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23712", "abs": "https://arxiv.org/abs/2509.23712", "authors": ["Gholamali Aminian", "Andrew Elliott", "Tiger Li", "Timothy Cheuk Hin Wong", "Victor Claude Dehon", "Lukasz Szpruch", "Carsten Maple", "Christopher Read", "Martin Brown", "Gesine Reinert", "Mo Mamouei"], "title": "FraudTransformer: Time-Aware GPT for Transaction Fraud Detection", "comment": "Pre-print", "summary": "Detecting payment fraud in real-world banking streams requires models that\ncan exploit both the order of events and the irregular time gaps between them.\nWe introduce FraudTransformer, a sequence model that augments a vanilla\nGPT-style architecture with (i) a dedicated time encoder that embeds either\nabsolute timestamps or inter-event values, and (ii) a learned positional\nencoder that preserves relative order. Experiments on a large industrial\ndataset -- tens of millions of transactions and auxiliary events -- show that\nFraudTransformer surpasses four strong classical baselines (Logistic\nRegression, XGBoost and LightGBM) as well as transformer ablations that omit\neither the time or positional component. On the held-out test set it delivers\nthe highest AUROC and PRAUC.", "AI": {"tldr": "FraudTransformer\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u7684\u5e8f\u5217\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u7f16\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u6765\u5229\u7528\u4e8b\u4ef6\u987a\u5e8f\u548c\u65f6\u95f4\u95f4\u9694\u4fe1\u606f\uff0c\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u7b80\u5316\u7248transformer\u3002", "motivation": "\u73b0\u5b9e\u94f6\u884c\u6d41\u4e2d\u7684\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u9700\u8981\u80fd\u591f\u540c\u65f6\u5229\u7528\u4e8b\u4ef6\u987a\u5e8f\u548c\u4e8b\u4ef6\u95f4\u4e0d\u89c4\u5219\u65f6\u95f4\u95f4\u9694\u7684\u6a21\u578b\u3002", "method": "\u5728\u6807\u51c6GPT\u67b6\u6784\u57fa\u7840\u4e0a\u589e\u52a0\uff1a(i)\u4e13\u7528\u65f6\u95f4\u7f16\u7801\u5668\uff0c\u5d4c\u5165\u7edd\u5bf9\u65f6\u95f4\u6233\u6216\u4e8b\u4ef6\u95f4\u65f6\u95f4\u503c\uff1b(ii)\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u4fdd\u6301\u76f8\u5bf9\u987a\u5e8f\u3002", "result": "\u5728\u5305\u542b\u6570\u5343\u4e07\u4ea4\u6613\u548c\u8f85\u52a9\u4e8b\u4ef6\u7684\u5927\u578b\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cFraudTransformer\u8d85\u8d8a\u4e86\u56db\u4e2a\u5f3a\u57fa\u7ebf\uff08\u903b\u8f91\u56de\u5f52\u3001XGBoost\u3001LightGBM\uff09\u4ee5\u53ca\u7701\u7565\u65f6\u95f4\u6216\u4f4d\u7f6e\u7ec4\u4ef6\u7684transformer\u53d8\u4f53\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u6700\u9ad8\u7684AUROC\u548cPRAUC\u3002", "conclusion": "FraudTransformer\u901a\u8fc7\u6709\u6548\u5229\u7528\u5e8f\u5217\u987a\u5e8f\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u5728\u652f\u4ed8\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u65f6\u95f4\u611f\u77e5transformer\u67b6\u6784\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.25148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25148", "abs": "https://arxiv.org/abs/2509.25148", "authors": ["FaQiang Qian", "WeiKun Zhang", "Ziliang Wang", "Kang An", "Xuhui Zheng", "Liangjian Wen", "Mengya Gao", "Yong Dai", "Yichao Wu"], "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following", "comment": null, "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.", "AI": {"tldr": "UniAPL\u662f\u4e00\u4e2a\u7edf\u4e00\u5bf9\u6297\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u8bad\u7ec3\u540c\u65f6\u5229\u7528\u6f14\u793a\u504f\u597d\u548c\u6bd4\u8f83\u504f\u597d\u6570\u636e\uff0c\u89e3\u51b3\u4f20\u7edfSFT+RL\u6d41\u7a0b\u4e2d\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u884c\u4e3a\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edfSFT\u540e\u63a5RL\u7684\u6d41\u7a0b\u5b58\u5728\u5173\u952e\u95ee\u9898\uff1aSFT\u4f7f\u7528\u9759\u6001\u4e13\u5bb6\u6570\u636e\uff0c\u4f46\u7b56\u7565\u6f14\u5316\u65f6\u751f\u6210\u5206\u5e03\u4f1a\u6f02\u79fb\uff0c\u5bfc\u81f4SFT\u77e5\u8bc6\u8106\u5f31\uff1b\u540e\u7eedRL\u7f3a\u4e4f\u5bf9\u4e13\u5bb6\u6f14\u793a\u4e2d\u4e30\u5bcc\u771f\u5b9e\u77e5\u8bc6\u7684\u76f4\u63a5\u8bbf\u95ee\uff0c\u9020\u6210\u4f4e\u6548\u3001\u65e0\u57fa\u7840\u7684\u66f4\u65b0\u3002\u8fd9\u79cd\u5206\u79bb\u963b\u788d\u4e86\u6570\u636e\u6e90\u4e4b\u95f4\u7684\u76f8\u4e92\u6b63\u5219\u5316\u3002", "method": "\u5c06\u5bf9\u9f50\u95ee\u9898\u91cd\u6784\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51faUniAPL\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u7edf\u4e00\u8bad\u7ec3\u76ee\u6807\uff0c\u4ece\u6df7\u5408\u7684SFT\u548c\u504f\u597d\u6570\u636e\u6279\u6b21\u4e2d\u8054\u5408\u5b66\u4e60\u3002\u5728\u6bcf\u4e2a\u68af\u5ea6\u6b65\u9aa4\u4e2d\uff0c\u5bc6\u96c6\u7684\u4e13\u5bb6\u6f14\u793a\u76f4\u63a5\u57fa\u7840\u548c\u6b63\u5219\u5316\u5728\u7ebf\u63a2\u7d22\uff0c\u56fa\u6709\u5730\u89e3\u51b3\u5206\u5e03\u4e0d\u5339\u914d\u5e76\u6700\u5927\u5316\u6570\u636e\u534f\u540c\u3002", "result": "\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u8bc4\u4f30UniAPL\uff0c\u4f7f\u7528Qwen3-235B-Instruct-2507\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u3002UniAPL\u6a21\u578b\u5339\u914d\u6216\u8d85\u8fc7\u5f3aGRPO\u57fa\u7ebf\uff1aQwen3-0.6B\u4e0a\u63d0\u53475.77%\uff08\u5339\u914d32B\u6a21\u578b\u6027\u80fd\uff09\uff0cQwen3-4B\u4e0a\u63d0\u53473.75%\uff0c\u751a\u81f3\u8d85\u8fc7\u6559\u5e08\u6a21\u578b\u3002\u54cd\u5e94\u957f\u5ea6\u548c\u5bf9\u6570\u6982\u7387\u5206\u5e03\u5206\u6790\u786e\u8ba4UniAPL\u8f93\u51fa\u7d27\u5bc6\u6a21\u4eff\u4e13\u5bb6\u6f14\u793a\u3002", "conclusion": "UniAPL\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u540c\u65f6\u5229\u7528\u4e24\u79cd\u504f\u597d\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u6027\u80fd\u548c\u66f4\u597d\u7684\u884c\u4e3a\u5bf9\u9f50\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.23809", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23809", "abs": "https://arxiv.org/abs/2509.23809", "authors": ["Hong Huang", "Decheng Wu", "Rui Cen", "Guanghua Yu", "Zonghang Li", "Kai Liu", "Jianchen Zhu", "Peng Chen", "Xue Liu", "Dapeng Wu"], "title": "Tequila: Trapping-free Ternary Quantization for Large Language Models", "comment": null, "summary": "Quantization techniques are essential for the deployment of Large Language\nModels (LLMs) on edge devices. However, prevailing methods often rely on\nmixed-precision multiplication that lacks efficient hardware support, making it\nnot feasible. Ternary weight quantization addresses this by constraining\nweights to {-1, 0, 1}, replacing expensive multiplications with\nhardware-efficient additions. However, such aggressive compression leads to\nsignificant accuracy degradation, even after costly quantization-aware training\nwith massive data. We identify the core issue as deadzone trapping: a large\nnumber of weights are trapped at the deadzone boundary. This occurs because\nthese weights receive only noisy, uninformative gradients, preventing stable\nescape from the deadzone and severely impeding model capacity and optimization.\nTo address this issue, we propose Tequila, a trapping-free quantization\noptimization method that reactivates deadzone-trapped weights by repurposing\nthem as dynamic biases. This allows the repurposed weights to provide a\ncontinuous signal in the forward pass and, critically, receive direct,\nmeaningful gradient signals during backpropagation, thereby enhancing model\ncapacity and optimization with nearly zero inference overhead. Extensive\nevaluations demonstrate that Tequila outperforms state-of-the-art (SOTA)\nternary quantization methods across five benchmarks. Specifically, on the ARC\nbenchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly\nmatching full-precision performance (within <1% gap) with a 3.0x inference\nspeedup. Consequently, Tequila offers a highly practical and efficient\nimplementation for the deployment of advanced LLMs in resource-constrained\nenvironments. The code is available at https://github.com/Tencent/AngelSlim.", "AI": {"tldr": "Tequila\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u5143\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6b7b\u533a\u9677\u9631\u6743\u91cd\u91cd\u65b0\u7528\u4f5c\u52a8\u6001\u504f\u7f6e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e09\u5143\u91cf\u5316\u5bfc\u81f4\u7684\u7cbe\u5ea6\u635f\u5931\u95ee\u9898\uff0c\u5728\u51e0\u4e4e\u4e0d\u589e\u52a0\u63a8\u7406\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e09\u5143\u91cf\u5316\u4e2d\u6743\u91cd\u88ab\u56f0\u5728\u6b7b\u533a\u8fb9\u754c\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u6b7b\u533a\u9677\u9631\u5bfc\u81f4\u5927\u91cf\u6743\u91cd\u53ea\u80fd\u63a5\u6536\u566a\u58f0\u68af\u5ea6\u4fe1\u53f7\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6a21\u578b\u5bb9\u91cf\u548c\u4f18\u5316\u6548\u679c\u3002", "method": "\u63d0\u51faTequila\u65b9\u6cd5\uff0c\u5c06\u6b7b\u533a\u9677\u9631\u6743\u91cd\u91cd\u65b0\u7528\u4f5c\u52a8\u6001\u504f\u7f6e\uff0c\u4f7f\u8fd9\u4e9b\u6743\u91cd\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u63d0\u4f9b\u8fde\u7eed\u4fe1\u53f7\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u63a5\u6536\u6709\u610f\u4e49\u7684\u68af\u5ea6\u4fe1\u53f7\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e09\u5143\u91cf\u5316\u65b9\u6cd5\uff0c\u5728ARC\u57fa\u51c6\u4e0a\u6bd4SOTA\u57fa\u7ebf\u63d0\u5347>4%\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6027\u80fd\uff08\u5dee\u8ddd<1%\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53473.0\u500d\u3002", "conclusion": "Tequila\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5148\u8fdbLLMs\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u5b9e\u7528\u548c\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2509.23828", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23828", "abs": "https://arxiv.org/abs/2509.23828", "authors": ["Hanyu Zhou", "Gim Hee Lee"], "title": "Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and Generation", "comment": null, "summary": "Vision-language models (VLMs) have demonstrated strong performance in 2D\nscene understanding and generation, but extending this unification to the\nphysical world remains an open challenge. Existing 3D and 4D approaches\ntypically embed scene geometry into autoregressive model for semantic\nunderstanding and diffusion model for content generation. This paradigm gap\nprevents a single model from jointly handling both tasks, especially in dynamic\n4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM,\nthe first unified VLM framework with spatiotemporal awareness for 4D scene\nunderstanding and generation. Our design is guided by two key insights: 1)\nUnification requires a shared representation. We extract semantic features for\nunderstanding and noisy-injected appearance features for generation,\nincorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual\nrepresentation through adaptive cross-attention. 2) Unification requires a\nshared architecture. Both autoregression and diffusion are built on Transformer\nbackbones, and this enables integration into a single LLM with task-specific\nheads. By aligning visual and linguistic representations, our Uni4D-LLM\nproduces predictions for both understanding and generation within one\nTransformer-based framework. We further apply instruction fine-tuning on\ndiverse 4D vision-language datasets to improve generalization across tasks.\nExtensive experiments on multiple benchmarks demonstrate that Uni4D-LLM\nachieves competitive or superior results compared to state-of-the-art models\nand offers the first true unification of 4D scene understanding and generation.", "AI": {"tldr": "Uni4D-LLM\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u8868\u793a\u548c\u67b6\u6784\uff0c\u5728\u5355\u4e00Transformer\u6846\u67b6\u4e2d\u540c\u65f6\u5904\u74064D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u73b0\u67093D\u548c4D\u65b9\u6cd5\u901a\u5e38\u5c06\u573a\u666f\u51e0\u4f55\u5206\u522b\u5d4c\u5165\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u7406\u89e3\u548c\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5185\u5bb9\u751f\u6210\uff0c\u8fd9\u79cd\u8303\u5f0f\u5dee\u8ddd\u963b\u788d\u4e86\u5355\u4e00\u6a21\u578b\u540c\u65f6\u5904\u7406\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u65f6\u7a7a\u5efa\u6a21\u7684\u52a8\u60014D\u573a\u666f\u4e2d\u3002", "method": "1) \u4f7f\u7528\u5171\u4eab\u8868\u793a\uff1a\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u7528\u4e8e\u7406\u89e3\uff0c\u566a\u58f0\u6ce8\u5165\u7684\u5916\u89c2\u7279\u5f81\u7528\u4e8e\u751f\u6210\uff0c\u7ed3\u54084D\u51e0\u4f55\u7ebf\u7d22\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u4e3a\u65f6\u7a7a\u611f\u77e5\u7684\u89c6\u89c9\u8868\u793a\uff1b2) \u4f7f\u7528\u5171\u4eab\u67b6\u6784\uff1a\u5c06\u81ea\u56de\u5f52\u548c\u6269\u6563\u90fd\u6784\u5efa\u5728Transformer\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0c\u96c6\u6210\u5230\u5355\u4e00LLM\u4e2d\uff0c\u914d\u5907\u4efb\u52a1\u7279\u5b9a\u5934\u90e8\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUni4D-LLM\u76f8\u6bd4\u6700\u5148\u8fdb\u6a21\u578b\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u7ed3\u679c\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e864D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u7684\u771f\u6b63\u7edf\u4e00\u3002", "conclusion": "Uni4D-LLM\u901a\u8fc7\u5171\u4eab\u8868\u793a\u548c\u67b6\u6784\u7684\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6307\u4ee4\u5fae\u8c03\uff0c\u6210\u529f\u5b9e\u73b0\u4e864D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u7684\u7edf\u4e00\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u7684\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24816", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24816", "abs": "https://arxiv.org/abs/2509.24816", "authors": ["Xilin Dang", "Kexin Chen", "Xiaorui Su", "Ayush Noori", "I\u00f1aki Arango", "Lucas Vittor", "Xinyi Long", "Yuyang Du", "Marinka Zitnik", "Pheng Ann Heng"], "title": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning", "comment": null, "summary": "In clinical practice, physicians refrain from making decisions when patient\ninformation is insufficient. This behavior, known as abstention, is a critical\nsafety mechanism preventing potentially harmful misdiagnoses. Recent\ninvestigations have reported the application of large language models (LLMs) in\nmedical scenarios. However, existing LLMs struggle with the abstentions,\nfrequently providing overconfident responses despite incomplete information.\nThis limitation stems from conventional abstention methods relying solely on\nmodel self-assessments, which lack systematic strategies to identify knowledge\nboundaries with external medical evidences. To address this, we propose\n\\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that\nintegrates systematic knowledge graph exploration for clinical decision-making.\nOur approach consists of two key stages operating on a shared contextualized\nevidence pool: 1) an evidence discovery stage that systematically explores the\nmedical knowledge space through graph expansion and direct retrieval, and 2) an\nevidence evaluation stage that ranks evidence using multiple factors to adapt\nexploration based on patient context and conversation history. This two-stage\napproach enables systematic knowledge graph exploration, allowing models to\ntrace structured reasoning paths and recognize insufficient medical evidence.\nWe evaluate our abstention approach using open-ended multi-round clinical\nbenchmarks that mimic realistic diagnostic scenarios, assessing abstention\nquality through accuracy-efficiency trade-offs beyond existing closed-form\nevaluations. Experimental evidences clearly demonstrate that KnowGuard\noutperforms state-of-the-art abstention approaches, improving diagnostic\naccuracy by 3.93\\% while reducing unnecessary interaction by 7.27 turns on\naverage.", "AI": {"tldr": "KnowGuard\u662f\u4e00\u4e2a\u65b0\u9896\u7684\"\u8c03\u67e5-\u518d\u5f03\u6743\"\u8303\u5f0f\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u77e5\u8bc6\u56fe\u8c31\u63a2\u7d22\u6765\u6539\u8fdb\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5f03\u6743\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u4ea4\u4e92\u3002", "motivation": "\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u533b\u751f\u4f1a\u5728\u60a3\u8005\u4fe1\u606f\u4e0d\u8db3\u65f6\u9009\u62e9\u5f03\u6743\u4ee5\u907f\u514d\u8bef\u8bca\uff0c\u4f46\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7684\u5f03\u6743\u673a\u5236\uff0c\u7ecf\u5e38\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u65f6\u7ed9\u51fa\u8fc7\u5ea6\u81ea\u4fe1\u7684\u56de\u7b54\u3002", "method": "\u63d0\u51faKnowGuard\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff1a1)\u8bc1\u636e\u53d1\u73b0\u9636\u6bb5\u901a\u8fc7\u56fe\u8c31\u6269\u5c55\u548c\u76f4\u63a5\u68c0\u7d22\u7cfb\u7edf\u63a2\u7d22\u533b\u5b66\u77e5\u8bc6\u7a7a\u95f4\uff1b2)\u8bc1\u636e\u8bc4\u4f30\u9636\u6bb5\u4f7f\u7528\u591a\u56e0\u7d20\u5bf9\u8bc1\u636e\u8fdb\u884c\u6392\u5e8f\uff0c\u6839\u636e\u60a3\u8005\u4e0a\u4e0b\u6587\u548c\u5bf9\u8bdd\u5386\u53f2\u81ea\u9002\u5e94\u8c03\u6574\u63a2\u7d22\u7b56\u7565\u3002", "result": "KnowGuard\u5728\u5f00\u653e\u5f0f\u591a\u8f6e\u4e34\u5e8a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5f03\u6743\u65b9\u6cd5\uff0c\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.93%\uff0c\u5e73\u5747\u51cf\u5c11\u4e867.27\u6b21\u4e0d\u5fc5\u8981\u7684\u4ea4\u4e92\u3002", "conclusion": "KnowGuard\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u77e5\u8bc6\u56fe\u8c31\u63a2\u7d22\u6709\u6548\u6539\u8fdb\u4e86LLMs\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5f03\u6743\u80fd\u529b\uff0c\u4e3a\u533b\u7597AI\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.24068", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24068", "abs": "https://arxiv.org/abs/2509.24068", "authors": ["Roussel Rahman", "Jeff Shrager"], "title": "A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture", "comment": null, "summary": "Strategy Choice Theory (SCT)\\footnote{``Strategy Choice Theory'',\n``Distributions of Associations'', and ``Overlapping Wave Theory'' have been\nused to refer to this line of work, emphasizing different\naspects.}\\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth}\nexplains important aspects of children's arithmetic learning based upon\nprinciples including learning from developmentally naturalistic data,\nprobabilistic representation, confidence-based retrieval, and the phase-like\nimportance of scaffolding strategies, such as finger-counting. Here we recast\nSCT as a ``Small Math Model'' (SMM), employing a neural-network-based\narchitecture analogous to LLMs. The SMM extends SCT to include counting\npractice\\footnote{The original SCT model was pre-biased in accordance with the\nsupposed experience of counting.}, symbol (number) embedding, and gated\nattention. Similar to earlier work, the SMM demonstrates constructive and\ndestructive interference between counting and addition, and the ``wave-like''\nuse of finger-counting as sum recall improves. We plan to extend the SMM to\nlater aspects of the decades-long SCT program, including adaptive strategy\nchoice and eventually strategy discovery, providing a unified platform to\ninvestigate the understanding of numerical characteristics and relationships\nessential for mathematical reasoning -- as it can emerge in LLM-based agents.", "AI": {"tldr": "\u5c06\u7b56\u7565\u9009\u62e9\u7406\u8bba\u91cd\u65b0\u6784\u5efa\u4e3a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5c0f\u6570\u5b66\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u539f\u7406\u8bba\u5e76\u5c55\u793a\u4e86\u8ba1\u6570\u4e0e\u52a0\u6cd5\u4e4b\u95f4\u7684\u5efa\u8bbe\u6027\u548c\u7834\u574f\u6027\u5e72\u6270\u3002", "motivation": "\u5c06\u4f20\u7edf\u7684\u7b56\u7565\u9009\u62e9\u7406\u8bba\u73b0\u4ee3\u5316\uff0c\u4f7f\u7528\u7c7b\u4f3c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6765\u7814\u7a76\u513f\u7ae5\u7b97\u672f\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u5c0f\u6570\u5b66\u6a21\u578b\uff0c\u5305\u542b\u8ba1\u6570\u7ec3\u4e60\u3001\u6570\u5b57\u7b26\u53f7\u5d4c\u5165\u548c\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u6a21\u578b\u5c55\u793a\u4e86\u8ba1\u6570\u4e0e\u52a0\u6cd5\u4e4b\u95f4\u7684\u5efa\u8bbe\u6027\u548c\u7834\u574f\u6027\u5e72\u6270\uff0c\u4ee5\u53ca\u968f\u7740\u6c42\u548c\u56de\u5fc6\u80fd\u529b\u63d0\u9ad8\u800c\u51fa\u73b0\u7684\u7c7b\u4f3c\u6ce2\u6d6a\u72b6\u7684\u624b\u6307\u8ba1\u6570\u4f7f\u7528\u6a21\u5f0f\u3002", "conclusion": "\u5c0f\u6570\u5b66\u6a21\u578b\u4e3a\u7814\u7a76\u6570\u5b66\u63a8\u7406\u4e2d\u6570\u503c\u7279\u5f81\u548c\u5173\u7cfb\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u5e73\u53f0\uff0c\u8ba1\u5212\u6269\u5c55\u5230\u7b56\u7565\u9009\u62e9\u548c\u53d1\u73b0\u7b49\u66f4\u9ad8\u7ea7\u529f\u80fd\u3002"}}
{"id": "2509.24069", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "62M10, 68T45, 62P35, 92C40, 65C20, 60G35, 92C42, 92C35, 93E10", "I.2.6; C.2.4; H.3.4; I.2.4; H.3.5; C.2.4; C.3; I.4.8; I.5.1; J.3;\n  K.6.1; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.24069", "abs": "https://arxiv.org/abs/2509.24069", "authors": ["Youssef Sabiri", "Walid Houmaidi", "Ouail El Maadi", "Yousra Chtouki"], "title": "AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring", "comment": "6 pages, 6 figures, 3 tables. Accepted at the 9th IEEE Global\n  Conference on Artificial Intelligence & Internet of Things (IEEE GCAIoT)\n  2025. Final camera-ready manuscript. Math expressions in this field are\n  rendered via MathJax", "summary": "Smart aquaculture systems depend on rich environmental data streams to\nprotect fish welfare, optimize feeding, and reduce energy use. Yet public\ndatasets that describe the air surrounding indoor tanks remain scarce, limiting\nthe development of forecasting and anomaly-detection tools that couple\nhead-space conditions with water-quality dynamics. We therefore introduce\nAQUAIR, an open-access public dataset that logs six Indoor Environmental\nQuality (IEQ) variables--air temperature, relative humidity, carbon dioxide,\ntotal volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture\nfacility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every\nfive minutes from 14 October 2024 to 9 January 2025, producing more than 23,000\ntime-stamped observations that are fully quality-controlled and publicly\narchived on Figshare. We describe the sensor placement, ISO-compliant mounting\nheight, calibration checks against reference instruments, and an open-source\nprocessing pipeline that normalizes timestamps, interpolates short gaps, and\nexports analysis-ready tables. Exploratory statistics show stable conditions\n(median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time\npeaks, offering rich structure for short-horizon forecasting, event detection,\nand sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture\ninformatics and provides a reproducible benchmark for data-centric machine\nlearning curricula and environmental sensing research focused on head-space\ndynamics in recirculating aquaculture systems.", "AI": {"tldr": "AQUAIR\u662f\u4e00\u4e2a\u5f00\u653e\u83b7\u53d6\u7684\u5ba4\u5185\u6c34\u4ea7\u517b\u6b96\u73af\u5883\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b6\u4e2aIEQ\u53d8\u91cf\uff0c\u5728\u6469\u6d1b\u54e5\u6c34\u4ea7\u517b\u6b96\u8bbe\u65bd\u4e2d\u6bcf5\u5206\u949f\u91c7\u6837\u4e00\u6b21\uff0c\u4e3a\u667a\u80fd\u6c34\u4ea7\u517b\u6b96\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\u3002", "motivation": "\u7531\u4e8e\u63cf\u8ff0\u5ba4\u5185\u517b\u6b96\u6c60\u5468\u56f4\u7a7a\u6c14\u73af\u5883\u7684\u516c\u5171\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u7ed3\u5408\u7a7a\u6c14\u6761\u4ef6\u4e0e\u6c34\u8d28\u52a8\u6001\u7684\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u5de5\u5177\u7684\u5f00\u53d1\u3002", "method": "\u4f7f\u7528\u5355\u4e2aAwair HOME\u76d1\u6d4b\u5668\u5728\u6469\u6d1b\u54e5\u6c34\u4ea7\u517b\u6b96\u8bbe\u65bd\u4e2d\u6bcf5\u5206\u949f\u91c7\u6837\uff0c\u8bb0\u5f556\u4e2aIEQ\u53d8\u91cf\uff0c\u91c7\u7528ISO\u6807\u51c6\u5b89\u88c5\u9ad8\u5ea6\uff0c\u901a\u8fc7\u5f00\u6e90\u5904\u7406\u7ba1\u9053\u8fdb\u884c\u8d28\u91cf\u63a7\u5236\u3002", "result": "\u83b7\u5f97\u4e86\u8d85\u8fc723,000\u4e2a\u65f6\u95f4\u6233\u89c2\u6d4b\u6570\u636e\uff0c\u663e\u793a\u7a33\u5b9a\u73af\u5883\u6761\u4ef6\uff08\u4e2d\u4f4dCO2=758 ppm\uff1bPM2.5=12\u5fae\u514b/\u7acb\u65b9\u7c73\uff09\u548c\u660e\u663e\u7684\u5582\u98df\u65f6\u95f4\u5cf0\u503c\u3002", "conclusion": "AQUAIR\u586b\u8865\u4e86\u667a\u80fd\u6c34\u4ea7\u517b\u6b96\u4fe1\u606f\u5b66\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u548c\u73af\u5883\u4f20\u611f\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.24122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24122", "abs": "https://arxiv.org/abs/2509.24122", "authors": ["Hongbo Liu", "Jia Xu"], "title": "Echo Flow Networks", "comment": "Under Review", "summary": "At the heart of time-series forecasting (TSF) lies a fundamental challenge:\nhow can models efficiently and effectively capture long-range temporal\ndependencies across ever-growing sequences? While deep learning has brought\nnotable progress, conventional architectures often face a trade-off between\ncomputational complexity and their ability to retain accumulative information\nover extended horizons.\n  Echo State Networks (ESNs), a class of reservoir computing models, have\nrecently regained attention for their exceptional efficiency, offering constant\nmemory usage and per-step training complexity regardless of input length. This\nmakes them particularly attractive for modeling extremely long-term event\nhistory in TSF. However, traditional ESNs fall short of state-of-the-art\nperformance due to their limited nonlinear capacity, which constrains both\ntheir expressiveness and stability.\n  We introduce Echo Flow Networks (EFNs), a framework composed of a group of\nextended Echo State Networks (X-ESNs) with MLP readouts, enhanced by our novel\nMatrix-Gated Composite Random Activation (MCRA), which enables complex,\nneuron-specific temporal dynamics, significantly expanding the network's\nrepresentational capacity without compromising computational efficiency. In\naddition, we propose a dual-stream architecture in which recent input history\ndynamically selects signature reservoir features from an infinite-horizon\nmemory, leading to improved prediction accuracy and long-term stability.\n  Extensive evaluations on five benchmarks demonstrate that EFNs achieve up to\n4x faster training and 3x smaller model size compared to leading methods like\nPatchTST, reducing forecasting error from 43% to 35%, a 20% relative\nimprovement. One instantiation of our framework, EchoFormer, consistently\nachieves new state-of-the-art performance across five benchmark datasets: ETTh,\nETTm, DMV, Weather, and Air Quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86Echo Flow Networks (EFNs)\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u56de\u58f0\u72b6\u6001\u7f51\u7edc(X-ESNs)\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u578b\u77e9\u9635\u95e8\u63a7\u590d\u5408\u968f\u673a\u6fc0\u6d3b(MCRA)\u548c\u53cc\u6d41\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u6355\u83b7\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u4fe1\u606f\u4fdd\u7559\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u800c\u4f20\u7edfESNs\u867d\u7136\u9ad8\u6548\u4f46\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002", "method": "\u4f7f\u7528\u6269\u5c55\u56de\u58f0\u72b6\u6001\u7f51\u7edc(X-ESNs)\u4e0eMLP\u8bfb\u51fa\u5c42\uff0c\u5f15\u5165\u77e9\u9635\u95e8\u63a7\u590d\u5408\u968f\u673a\u6fc0\u6d3b(MCRA)\u5b9e\u73b0\u590d\u6742\u7684\u795e\u7ecf\u5143\u7279\u5b9a\u65f6\u95f4\u52a8\u6001\uff0c\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u8ba9\u8fd1\u671f\u8f93\u5165\u5386\u53f2\u52a8\u6001\u9009\u62e9\u6765\u81ea\u65e0\u9650\u65f6\u57df\u8bb0\u5fc6\u7684\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cEFNs\u76f8\u6bd4PatchTST\u7b49\u65b9\u6cd5\u8bad\u7ec3\u901f\u5ea6\u5feb4\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u5c0f3\u500d\uff0c\u9884\u6d4b\u8bef\u5dee\u4ece43%\u964d\u81f335%(\u76f8\u5bf9\u6539\u8fdb20%)\u3002EchoFormer\u5728ETTh\u3001ETTm\u3001DMV\u3001Weather\u548cAir Quality\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "EFNs\u6846\u67b6\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u957f\u671f\u7a33\u5b9a\u6027\uff0c\u4e3a\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22613", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22613", "abs": "https://arxiv.org/abs/2509.22613", "authors": ["Siwei Wang", "Yifei Shen", "Haoran Sun", "Shi Feng", "Shang-Hua Teng", "Li Dong", "Yaru Hao", "Wei Chen"], "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective", "comment": null, "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u56fe\u62bd\u8c61\u5206\u6790\u5f3a\u5316\u5b66\u4e60\u5728LLM\u89c4\u5212\u4e2d\u7684\u7406\u8bba\u673a\u5236\uff0c\u53d1\u73b0SFT\u4f1a\u4ea7\u751f\u865a\u5047\u89e3\uff0cRL\u901a\u8fc7\u63a2\u7d22\u5b9e\u73b0\u6b63\u786e\u89c4\u5212\uff0c\u4f46PG\u5b58\u5728\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u800cQ\u5b66\u4e60\u80fd\u4fdd\u6301\u591a\u6837\u6027\u4e14\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u3002", "motivation": "\u5f53\u524dRL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u5176\u7406\u8bba\u6709\u6548\u6027\u57fa\u7840\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4ece\u7406\u8bba\u5c42\u9762\u5206\u6790RL\u7684\u76ca\u5904\u548c\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u53ef\u5904\u7406\u7684\u56fe\u62bd\u8c61\u65b9\u6cd5\uff0c\u91cd\u70b9\u5206\u6790\u7b56\u7565\u68af\u5ea6(PG)\u548cQ\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728Blocksworld\u89c4\u5212\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1aSFT\u5f15\u5165\u57fa\u4e8e\u5171\u73b0\u7684\u865a\u5047\u89e3\uff1bRL\u901a\u8fc7\u63a2\u7d22\u5b9e\u73b0\u6b63\u786e\u89c4\u5212\uff1bPG\u5b58\u5728\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff1bQ\u5b66\u4e60\u5177\u6709\u79bb\u7b56\u7565\u5b66\u4e60\u548c\u591a\u6837\u6027\u4fdd\u6301\u4f18\u52bf\uff1b\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u3002", "conclusion": "\u63a2\u7d22\u5728RL\u89c4\u5212\u4e2d\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0cQ\u5b66\u4e60\u76f8\u6bd4PG\u5728\u4fdd\u6301\u591a\u6837\u6027\u65b9\u9762\u66f4\u5177\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u8c28\u614e\u7684\u5956\u52b1\u8bbe\u8ba1\u6765\u786e\u4fdd\u6709\u6548\u6027\u3002"}}
{"id": "2509.24317", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24317", "abs": "https://arxiv.org/abs/2509.24317", "authors": ["Xianhang Li", "Chen Huang", "Chun-Liang Li", "Eran Malach", "Josh Susskind", "Vimal Thilak", "Etai Littwin"], "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers", "comment": "Technical Report", "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable\noff-the-shelf video representation by predicting masked regions in latent space\nwith an exponential moving average (EMA)-updated teacher. While EMA prevents\nrepresentation collapse, it complicates scalable model selection and couples\nteacher and student architectures. We revisit masked-latent prediction and show\nthat a frozen teacher suffices. Concretely, we (i) train a target encoder with\na simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze\nit and train a student to predict the teacher's latents on masked regions. This\nleads to a two-stage, unregularized scheme that we refer to as SALT\n(Static-teacher Asymmetric Latent Training). SALT decouples optimization into\npixel reconstruction (teacher) and masked latent prediction (student),\nincreasing transparency, efficiency, and scalability while preserving the\nability of representation to generalize under frozen evaluation. Empirically,\nour student models outperform recently proposed V-JEPA 2 encoders under frozen\nbackbone evaluation across diverse benchmarks. They are also more\ncompute-optimal: at matched pretraining FLOPs, our method achieves higher\nprobing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs\nPareto frontier. Finally, we find that student quality is remarkably robust to\nteacher quality: high-performing students emerge even with small, sub-optimal\nteachers. This points to a compute budget allocation that should overwhelmingly\nfavor the student. These results position SALT as a simple, scalable, and\ncompute-efficient alternative to EMA-based self-distillation for video\nrepresentation learning.", "AI": {"tldr": "\u63d0\u51faSALT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u50cf\u7d20\u91cd\u5efa\u6559\u5e08+\u63a9\u7801\u6f5c\u5728\u9884\u6d4b\u5b66\u751f\uff09\u66ff\u4ee3EMA\u81ea\u84b8\u998f\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89c6\u9891\u8868\u793a\u5b66\u4e60", "motivation": "V-JEPA\u4f7f\u7528EMA\u66f4\u65b0\u7684\u6559\u5e08\u9632\u6b62\u8868\u793a\u5d29\u6e83\uff0c\u4f46\u589e\u52a0\u4e86\u6a21\u578b\u9009\u62e9\u590d\u6742\u6027\u5e76\u8026\u5408\u4e86\u5e08\u751f\u67b6\u6784\u3002\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u63a9\u7801\u6f5c\u5728\u9884\u6d4b\uff0c\u53d1\u73b0\u51bb\u7ed3\u6559\u5e08\u5df2\u8db3\u591f", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u7528\u50cf\u7d20\u91cd\u5efa\u76ee\u6807\u8bad\u7ec3\u6559\u5e08\u7f16\u7801\u5668\uff1b2) \u51bb\u7ed3\u6559\u5e08\uff0c\u8bad\u7ec3\u5b66\u751f\u9884\u6d4b\u6559\u5e08\u5bf9\u63a9\u7801\u533a\u57df\u7684\u6f5c\u5728\u8868\u793a", "result": "SALT\u5b66\u751f\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eV-JEPA 2\u7f16\u7801\u5668\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5728\u76f8\u540cFLOPs\u4e0b\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u4e14\u5bf9\u6559\u5e08\u8d28\u91cf\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "SALT\u662fEMA\u81ea\u84b8\u998f\u7684\u7b80\u5355\u3001\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5efa\u8bae\u5c06\u8ba1\u7b97\u9884\u7b97\u4e3b\u8981\u5206\u914d\u7ed9\u5b66\u751f\u800c\u975e\u6559\u5e08"}}
{"id": "2509.24136", "categories": ["cs.CV", "cs.AI", "cs.LG", "60G35, 62M10, 62P35, 65C20, 68T45, 68U10, 92C35, 92C40, 92C42, 93E10", "I.4; I.4.8; I.4.9; I.4.10; I.2; I.2.6; I.2.10; J.3; C.2.4; C.3;\n  H.2.8; H.3.4; H.3.5; I.2.4; I.5; I.5.1; I.5.4; K.6.1"], "pdf": "https://arxiv.org/pdf/2509.24136", "abs": "https://arxiv.org/abs/2509.24136", "authors": ["Youssef Sabiri", "Walid Houmaidi", "Amine Abouaomar"], "title": "EYE-DEX: Eye Disease Detection and EXplanation System", "comment": "6 pages, 4 figures, 3 tables. Accepted at the 12th International\n  Conference on Wireless Networks and Mobile Communications 2025 (WINCOM 2025)", "summary": "Retinal disease diagnosis is critical in preventing vision loss and reducing\nsocioeconomic burdens. Globally, over 2.2 billion people are affected by some\nform of vision impairment, resulting in annual productivity losses estimated at\n$411 billion. Traditional manual grading of retinal fundus images by\nophthalmologists is time-consuming and subjective. In contrast, deep learning\nhas revolutionized medical diagnostics by automating retinal image analysis and\nachieving expert-level performance. In this study, we present EYE-DEX, an\nautomated framework for classifying 10 retinal conditions using the large-scale\nRetinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three\npre-trained Convolutional Neural Network (CNN) models--VGG16, VGG19, and\nResNet50--with our finetuned VGG16 achieving a state-of-the-art global\nbenchmark test accuracy of 92.36%. To enhance transparency and explainability,\nwe integrate the Gradient-weighted Class Activation Mapping (Grad-CAM)\ntechnique to generate visual explanations highlighting disease-specific\nregions, thereby fostering clinician trust and reliability in AI-assisted\ndiagnostics.", "AI": {"tldr": "EYE-DEX\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u7f51\u819c\u75be\u75c5\u81ea\u52a8\u8bca\u65ad\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3CNN\u6a21\u578b\u572821,577\u5f20\u773c\u5e95\u56fe\u50cf\u4e0a\u5bf910\u79cd\u89c6\u7f51\u819c\u75be\u75c5\u8fdb\u884c\u5206\u7c7b\uff0c\u8fbe\u5230\u4e8692.36%\u7684\u6700\u5148\u8fdb\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7Grad-CAM\u63d0\u4f9b\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "motivation": "\u89c6\u7f51\u819c\u75be\u75c5\u8bca\u65ad\u5bf9\u9884\u9632\u89c6\u529b\u4e27\u5931\u548c\u51cf\u8f7b\u793e\u4f1a\u7ecf\u6d4e\u8d1f\u62c5\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u624b\u52a8\u5206\u7ea7\u65b9\u6cd5\u8017\u65f6\u4e14\u4e3b\u89c2\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u53ef\u4ee5\u81ea\u52a8\u5316\u89c6\u7f51\u819c\u56fe\u50cf\u5206\u6790\u5e76\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6027\u80fd\u3002", "method": "\u4f7f\u7528VGG16\u3001VGG19\u548cResNet50\u4e09\u79cd\u9884\u8bad\u7ec3CNN\u6a21\u578b\u5728\u5927\u89c4\u6a21\u89c6\u7f51\u819c\u75be\u75c5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u96c6\u6210Grad-CAM\u6280\u672f\u751f\u6210\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "result": "\u5fae\u8c03\u540e\u7684VGG16\u6a21\u578b\u8fbe\u5230\u4e8692.36%\u7684\u5168\u7403\u57fa\u51c6\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "EYE-DEX\u6846\u67b6\u5728\u89c6\u7f51\u819c\u75be\u75c5\u5206\u7c7b\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u89e3\u91ca\u589e\u5f3a\u4e86AI\u8f85\u52a9\u8bca\u65ad\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u6709\u52a9\u4e8e\u5efa\u7acb\u4e34\u5e8a\u533b\u751f\u7684\u4fe1\u4efb\u3002"}}
{"id": "2509.24406", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24406", "abs": "https://arxiv.org/abs/2509.24406", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Muon: Training and Trade-offs with Latent Attention and MoE", "comment": null, "summary": "We present a comprehensive theoretical and empirical study of the Muon\noptimizer for training transformers only with a small to medium decoder (30M -\n200M parameters), with an emphasis on its mathematical foundations, convergence\nproperties and synergistic interactions with modern architectural\noptimizations. Building on recent work showing Muon's scalability, we provide\nrigorous theoretical analysis including: (i)showing the convergence rate under\nstandard assumptions, (ii) spectral regularization properties that prevent\ngradient explosion, (iii) connection to natural gradient descent on the Stiefel\nmanifold, and (iv) equivalence to steepest gradient descent under the spectral\nnorm. Crucially, we demonstrate that Muon expands the Pareto frontier in the\ncompute-time trade-off by maintaining superior data efficiency at large batch\nsizes, a key finding of~\\cite{essentialai2025muon} that we validate across our\nmodel scales. Empirically, Muon reaches the target loss with 48-52\\% of the\ntraining calculated by AdamW while maintaining or improving the final\nperplexity, consistent with larger-scale results. When combined with Multi-Head\nLatent Attention (MLA) and Mixture-of-Experts (MoE), we observe multiplicative\nefficiency gains: MLA+MoE+Muon achieves 68\\% memory reduction and 3.2$\\times$\ninference speedup, while improving perplexity by 8-12\\%. We provide detailed\nprocedures on 15 architectural and optimizer components, stability analyzes\nacross 100+ training runs, and practical implementation guidelines including\nNewton-Schulz coefficients $(3.4445, -4.7750, 2.0315)$ optimized\nby~\\cite{su2024muonblog}. Our theoretical analysis and comprehensive\nexperiments establish Muon as a principled, robust alternative to AdamW that\nparticularly excels when combined with modern efficiency techniques and\nlarge-batch training regimes.", "AI": {"tldr": "Muon\u4f18\u5316\u5668\u5728\u8bad\u7ec3\u5c0f\u578b\u5230\u4e2d\u578bTransformer\u89e3\u7801\u5668\u65f6\uff0c\u76f8\u6bd4AdamW\u8282\u770148-52%\u7684\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u6539\u5584\u56f0\u60d1\u5ea6\u3002\u4e0eMLA\u548cMoE\u7ed3\u5408\u65f6\uff0c\u80fd\u5b9e\u73b068%\u5185\u5b58\u51cf\u5c11\u548c3.2\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u7814\u7a76Muon\u4f18\u5316\u5668\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u9a8c\u8bc1\u5176\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u4e0e\u73b0\u4ee3\u5316\u67b6\u6784\u4f18\u5316\u6280\u672f\u7ed3\u5408\u65f6\u7684\u534f\u540c\u6548\u5e94\u3002", "method": "\u63d0\u4f9b\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u5305\u62ec\u6536\u655b\u7387\u8bc1\u660e\u3001\u8c31\u6b63\u5219\u5316\u7279\u6027\u3001\u4e0eStiefel\u6d41\u5f62\u4e0a\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u7684\u8054\u7cfb\uff0c\u4ee5\u53ca\u5728\u8c31\u8303\u6570\u4e0b\u4e0e\u6700\u901f\u68af\u5ea6\u4e0b\u964d\u7684\u7b49\u4ef7\u6027\u3002\u901a\u8fc7100+\u8bad\u7ec3\u8fd0\u884c\u8fdb\u884c\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "Muon\u5728\u4fdd\u6301\u6216\u6539\u5584\u6700\u7ec8\u56f0\u60d1\u5ea6\u7684\u540c\u65f6\uff0c\u4ec5\u9700AdamW 48-52%\u7684\u8bad\u7ec3\u8ba1\u7b97\u91cf\u3002\u4e0eMLA+MoE\u7ed3\u5408\u65f6\uff0c\u5b9e\u73b068%\u5185\u5b58\u51cf\u5c11\u30013.2\u500d\u63a8\u7406\u52a0\u901f\u548c8-12%\u56f0\u60d1\u5ea6\u6539\u5584\u3002", "conclusion": "Muon\u662f\u4e00\u4e2a\u6709\u7406\u8bba\u57fa\u7840\u3001\u7a33\u5065\u7684AdamW\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u5728\u4e0e\u73b0\u4ee3\u5316\u6548\u7387\u6280\u672f\u548c\u5927\u6279\u91cf\u8bad\u7ec3\u673a\u5236\u7ed3\u5408\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.24547", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24547", "abs": "https://arxiv.org/abs/2509.24547", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Linh Ngo Van"], "title": "LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection", "comment": null, "summary": "Few-shot Continual Event Detection (FCED) poses the dual challenges of\nlearning from limited data and mitigating catastrophic forgetting across\nsequential tasks. Existing approaches often suffer from severe forgetting due\nto the full fine-tuning of a shared base model, which leads to knowledge\ninterference between tasks. Moreover, they frequently rely on data augmentation\nstrategies that can introduce unnatural or semantically distorted inputs. To\naddress these limitations, we propose LEAF, a novel and robust expert-based\nframework for FCED. LEAF integrates a specialized mixture of experts\narchitecture into the base model, where each expert is parameterized with\nlow-rank adaptation (LoRA) matrices. A semantic-aware expert selection\nmechanism dynamically routes instances to the most relevant experts, enabling\nexpert specialization and reducing knowledge interference. To improve\ngeneralization in limited-data settings, LEAF incorporates a contrastive\nlearning objective guided by label descriptions, which capture high-level\nsemantic information about event types. Furthermore, to prevent overfitting on\nthe memory buffer, our framework employs a knowledge distillation strategy that\ntransfers knowledge from previous models to the current one. Extensive\nexperiments on multiple FCED benchmarks demonstrate that LEAF consistently\nachieves state-of-the-art performance.", "AI": {"tldr": "LEAF\u662f\u4e00\u4e2a\u7528\u4e8e\u5c11\u6837\u672c\u6301\u7eed\u4e8b\u4ef6\u68c0\u6d4b\u7684\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u53c2\u6570\u5316\u4e13\u5bb6\u3001\u8bed\u4e49\u611f\u77e5\u4e13\u5bb6\u9009\u62e9\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u6765\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u548c\u63d0\u5347\u6709\u9650\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u6301\u7eed\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7531\u4e8e\u5b8c\u5168\u5fae\u8c03\u5171\u4eab\u57fa\u7840\u6a21\u578b\u5bfc\u81f4\u7684\u77e5\u8bc6\u5e72\u6270\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4ee5\u53ca\u6570\u636e\u589e\u5f3a\u7b56\u7565\u53ef\u80fd\u5f15\u5165\u4e0d\u81ea\u7136\u6216\u8bed\u4e49\u626d\u66f2\u8f93\u5165\u7684\u95ee\u9898\u3002", "method": "1) \u5728\u57fa\u7840\u6a21\u578b\u4e2d\u96c6\u6210\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4f7f\u7528LoRA\u77e9\u9635\u53c2\u6570\u5316\uff1b2) \u8bed\u4e49\u611f\u77e5\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u52a8\u6001\u8def\u7531\u5b9e\u4f8b\u5230\u6700\u76f8\u5173\u4e13\u5bb6\uff1b3) \u57fa\u4e8e\u6807\u7b7e\u63cf\u8ff0\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff1b4) \u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u9632\u6b62\u5185\u5b58\u7f13\u51b2\u533a\u8fc7\u62df\u5408\u3002", "result": "\u5728\u591a\u4e2aFCED\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLEAF\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LEAF\u901a\u8fc7\u4e13\u5bb6\u4e13\u4e1a\u5316\u3001\u8bed\u4e49\u5f15\u5bfc\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u6301\u7eed\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u77e5\u8bc6\u5e72\u6270\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2509.24350", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24350", "abs": "https://arxiv.org/abs/2509.24350", "authors": ["Yan Ke", "Xin Yu", "Heming Du", "Scott Chapman", "Helen Huang"], "title": "Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA", "comment": "13 pages, 2 figures, 2 tables", "summary": "Agricultural visual question answering is essential for providing farmers and\nresearchers with accurate and timely knowledge. However, many existing\napproaches are predominantly developed for evidence-constrained settings such\nas text-only queries or single-image cases. This design prevents them from\ncoping with real-world agricultural scenarios that often require multi-image\ninputs with complementary views across spatial scales, and growth stages.\nMoreover, limited access to up-to-date external agricultural context makes\nthese systems struggle to adapt when evidence is incomplete. In addition, rigid\npipelines often lack systematic quality control. To address this gap, we\npropose a self-reflective and self-improving multi-agent framework that\nintegrates four roles, the Retriever, the Reflector, the Answerer, and the\nImprover. They collaborate to enable context enrichment, reflective reasoning,\nanswer drafting, and iterative improvement.\n  A Retriever formulates queries and gathers external information, while a\nReflector assesses adequacy and triggers sequential reformulation and renewed\nretrieval. Two Answerers draft candidate responses in parallel to reduce bias.\nThe Improver refines them through iterative checks while ensuring that\ninformation from multiple images is effectively aligned and utilized.\nExperiments on the AgMMU benchmark show that our framework achieves competitive\nperformance on multi-image agricultural QA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u53cd\u601d\u81ea\u6539\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u519c\u4e1a\u89c6\u89c9\u95ee\u7b54\u4e2d\u591a\u56fe\u50cf\u8f93\u5165\u548c\u5916\u90e8\u77e5\u8bc6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728AgMMU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u89c6\u89c9\u95ee\u7b54\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u67e5\u8be2\u6216\u5355\u56fe\u50cf\u573a\u666f\uff0c\u65e0\u6cd5\u5904\u7406\u9700\u8981\u591a\u56fe\u50cf\u4e92\u8865\u89c6\u89d2\u548c\u751f\u957f\u9636\u6bb5\u4fe1\u606f\u7684\u771f\u5b9e\u519c\u4e1a\u573a\u666f\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u8d28\u91cf\u63a7\u5236\u548c\u5916\u90e8\u77e5\u8bc6\u66f4\u65b0\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u68c0\u7d22\u5668\u3001\u53cd\u601d\u5668\u3001\u56de\u7b54\u5668\u548c\u6539\u8fdb\u5668\u7684\u56db\u89d2\u8272\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u4e30\u5bcc\u3001\u53cd\u601d\u63a8\u7406\u3001\u7b54\u6848\u8349\u62df\u548c\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u5728AgMMU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u591a\u56fe\u50cf\u519c\u4e1a\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u81ea\u53cd\u601d\u81ea\u6539\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u519c\u4e1a\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u591a\u56fe\u50cf\u8f93\u5165\u548c\u77e5\u8bc6\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u4e3a\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24713", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24713", "abs": "https://arxiv.org/abs/2509.24713", "authors": ["Jing Liu"], "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit\nsystematic failures on longtail distributions, leading to reward hacking and\nmisalignment. We propose a mechanistic interpretability framework that\nidentifies specialized neural circuits responsible for rare-event processing in\nreward models. Drawing from recent advances showing distributed specialization\nfor rare tokens in language models\\citep{liu2025no, liu2025emergent}, we\nhypothesize that reward models also develop functionally distinct circuits for\nlongtail scenarios. Our theoretical framework establishes formal connections\nbetween circuit specialization, reward generalization bounds, and longtail\nperformance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which\nuses circuit analysis to guide data augmentation, regularization, and ensemble\nstrategies. This approach provides both theoretical insights into reward model\nfailures and practical interventions for improving longtail robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u6765\u8bc6\u522b\u5956\u52b1\u6a21\u578b\u4e2d\u8d1f\u8d23\u7f55\u89c1\u4e8b\u4ef6\u5904\u7406\u7684\u4e13\u95e8\u795e\u7ecf\u56de\u8def\uff0c\u5e76\u5f15\u5165Circuit-Aware Reward Training (CART)\u65b9\u6cd5\u6765\u63d0\u9ad8\u957f\u5c3e\u5206\u5e03\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60(RLHF)\u5956\u52b1\u6a21\u578b\u5728\u957f\u5c3e\u5206\u5e03\u4e0a\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u548c\u9519\u4f4d\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u8bc6\u522b\u4e13\u95e8\u795e\u7ecf\u56de\u8def\uff0c\u63d0\u51faCART\u65b9\u6cd5\uff0c\u901a\u8fc7\u56de\u8def\u5206\u6790\u6307\u5bfc\u6570\u636e\u589e\u5f3a\u3001\u6b63\u5219\u5316\u548c\u96c6\u6210\u7b56\u7565\u3002", "result": "\u5efa\u7acb\u4e86\u56de\u8def\u4e13\u95e8\u5316\u3001\u5956\u52b1\u6cdb\u5316\u8fb9\u754c\u548c\u957f\u5c3e\u6027\u80fd\u4e4b\u95f4\u7684\u5f62\u5f0f\u5316\u8054\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u5956\u52b1\u6a21\u578b\u5931\u8d25\u63d0\u4f9b\u4e86\u7406\u8bba\u6d1e\u89c1\uff0c\u5e76\u4e3a\u63d0\u9ad8\u957f\u5c3e\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2509.24367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24367", "abs": "https://arxiv.org/abs/2509.24367", "authors": ["Jinhee Park", "Guisik Kim", "Choongsang Cho", "Junseok Kwon"], "title": "Real-Aware Residual Model Merging for Deepfake Detection", "comment": null, "summary": "Deepfake generators evolve quickly, making exhaustive data collection and\nrepeated retraining impractical. We argue that model merging is a natural fit\nfor deepfake detection: unlike generic multi-task settings with disjoint\nlabels, deepfake specialists share the same binary decision and differ in\ngenerator-specific artifacts. Empirically, we show that simple weight averaging\npreserves Real representations while attenuating Fake-specific cues. Building\nupon these findings, we propose Real-aware Residual Model Merging (R$^2$M), a\ntraining-free parameter-space merging framework. R$^2$M estimates a shared Real\ncomponent via a low-rank factorization of task vectors, decomposes each\nspecialist into a Real-aligned part and a Fake residual, denoises residuals\nwith layerwise rank truncation, and aggregates them with per-task norm matching\nto prevent any single generator from dominating. A concise rationale explains\nwhy a simple head suffices: the Real component induces a common separation\ndirection in feature space, while truncated residuals contribute only minor\noff-axis variations. Across in-distribution, cross-dataset, and unseen-dataset,\nR$^2$M outperforms joint training and other merging baselines. Importantly,\nR$^2$M is also composable: when a new forgery family appears, we fine-tune one\nspecialist and re-merge, eliminating the need for retraining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u578b\u878d\u5408\u6846\u67b6R\u00b2M\uff0c\u7528\u4e8e\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff0c\u901a\u8fc7\u5206\u89e3\u4e13\u5bb6\u6a21\u578b\u7684\u53c2\u6570\u7a7a\u95f4\u6765\u5171\u4eab\u771f\u5b9e\u56fe\u50cf\u7279\u5f81\u5e76\u805a\u5408\u4f2a\u9020\u7279\u5f81\u6b8b\u5dee\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u5668\u5feb\u901f\u6f14\u53d8\u5e26\u6765\u7684\u91cd\u590d\u8bad\u7ec3\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u5668\u5feb\u901f\u6f14\u53d8\uff0c\u4f7f\u5f97\u6536\u96c6\u6570\u636e\u548c\u91cd\u590d\u8bad\u7ec3\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u6a21\u578b\u878d\u5408\u662f\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u81ea\u7136\u9009\u62e9\uff0c\u56e0\u4e3a\u4e13\u5bb6\u6a21\u578b\u5171\u4eab\u76f8\u540c\u7684\u4e8c\u5143\u51b3\u7b56\uff0c\u53ea\u5728\u751f\u6210\u5668\u7279\u5b9a\u4f2a\u5f71\u4e0a\u6709\u6240\u4e0d\u540c\u3002", "method": "\u63d0\u51faReal-aware Residual Model Merging (R\u00b2M)\u6846\u67b6\uff1a\u901a\u8fc7\u4efb\u52a1\u5411\u91cf\u7684\u4f4e\u79e9\u5206\u89e3\u4f30\u8ba1\u5171\u4eab\u7684\u771f\u5b9e\u7ec4\u4ef6\uff0c\u5c06\u6bcf\u4e2a\u4e13\u5bb6\u6a21\u578b\u5206\u89e3\u4e3a\u771f\u5b9e\u5bf9\u9f50\u90e8\u5206\u548c\u4f2a\u9020\u6b8b\u5dee\uff0c\u4f7f\u7528\u5c42\u95f4\u79e9\u622a\u65ad\u5bf9\u6b8b\u5dee\u53bb\u566a\uff0c\u5e76\u901a\u8fc7\u6bcf\u4efb\u52a1\u8303\u6570\u5339\u914d\u805a\u5408\u4ee5\u9632\u6b62\u5355\u4e2a\u751f\u6210\u5668\u4e3b\u5bfc\u3002", "result": "\u5728\u5206\u5e03\u5185\u3001\u8de8\u6570\u636e\u96c6\u548c\u672a\u89c1\u6570\u636e\u96c6\u4e0a\uff0cR\u00b2M\u4f18\u4e8e\u8054\u5408\u8bad\u7ec3\u548c\u5176\u4ed6\u878d\u5408\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "R\u00b2M\u662f\u53ef\u7ec4\u5408\u7684\uff1a\u5f53\u51fa\u73b0\u65b0\u7684\u4f2a\u9020\u5bb6\u65cf\u65f6\uff0c\u53ea\u9700\u5fae\u8c03\u4e00\u4e2a\u4e13\u5bb6\u6a21\u578b\u5e76\u91cd\u65b0\u878d\u5408\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u7cfb\u7edf\u3002"}}
{"id": "2509.24734", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24734", "abs": "https://arxiv.org/abs/2509.24734", "authors": ["Giordano Cicchetti", "Eleonora Grassucci", "Danilo Comminiello"], "title": "A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity", "comment": "NeurIPS 2025", "summary": "Multimodal learning plays a pivotal role in advancing artificial intelligence\nsystems by incorporating information from multiple modalities to build a more\ncomprehensive representation. Despite its importance, current state-of-the-art\nmodels still suffer from severe limitations that prevent the successful\ndevelopment of a fully multimodal model. Such methods may not provide\nindicators that all the involved modalities are effectively aligned. As a\nresult, some modalities may not be aligned, undermining the effectiveness of\nthe model in downstream tasks where multiple modalities should provide\nadditional information that the model fails to exploit. In this paper, we\npresent TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed\nsimilarity measure that is directly computed in the higher-dimensional space\nspanned by the modality embeddings. TRIANGLE improves the joint alignment of\nthree modalities via a triangle-area similarity, avoiding additional fusion\nlayers or pairwise similarities. When incorporated in contrastive losses\nreplacing cosine similarity, TRIANGLE significantly boosts the performance of\nmultimodal modeling, while yielding interpretable alignment rationales.\nExtensive evaluation in three-modal tasks such as video-text and audio-text\nretrieval or audio-video classification, demonstrates that TRIANGLE achieves\nstate-of-the-art results across different datasets improving the performance of\ncosine-based methods up to 9 points of Recall@1.", "AI": {"tldr": "\u63d0\u51faTRIANGLE\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u89d2\u5f62\u9762\u79ef\u76f8\u4f3c\u6027\u6539\u8fdb\u4e09\u79cd\u6a21\u6001\u7684\u8054\u5408\u5bf9\u9f50\uff0c\u5728\u5bf9\u6bd4\u635f\u5931\u4e2d\u66ff\u4ee3\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5efa\u6a21\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u67d0\u4e9b\u6a21\u6001\u53ef\u80fd\u672a\u88ab\u6709\u6548\u5bf9\u9f50\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "TRIANGLE\uff1a\u76f4\u63a5\u5728\u6a21\u6001\u5d4c\u5165\u7684\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u4e09\u89d2\u5f62\u9762\u79ef\u76f8\u4f3c\u6027\uff0c\u907f\u514d\u989d\u5916\u7684\u878d\u5408\u5c42\u6216\u6210\u5bf9\u76f8\u4f3c\u6027\u8ba1\u7b97\uff0c\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u5b9e\u73b0\u4e09\u79cd\u6a21\u6001\u7684\u8054\u5408\u5bf9\u9f50\u3002", "result": "\u5728\u89c6\u9891-\u6587\u672c\u3001\u97f3\u9891-\u6587\u672c\u68c0\u7d22\u548c\u97f3\u9891-\u89c6\u9891\u5206\u7c7b\u7b49\u4e09\u6a21\u6001\u4efb\u52a1\u4e2d\uff0cTRIANGLE\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u6bd4\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5Recall@1\u63d0\u5347\u9ad8\u8fbe9\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "TRIANGLE\u76f8\u4f3c\u6027\u5ea6\u91cf\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5bf9\u9f50\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5bf9\u9f50\u539f\u7406\uff0c\u662f\u591a\u6a21\u6001\u5b66\u4e60\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2509.24445", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24445", "abs": "https://arxiv.org/abs/2509.24445", "authors": ["Jianxin Liang", "Tan Yue", "Yuxuan Wang", "Yueqian Wang", "Zhihan Yin", "Huishuai Zhang", "Dongyan Zhao"], "title": "Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA", "comment": null, "summary": "The performance of Video Question Answering (VideoQA) models is fundamentally\nconstrained by the nature of their supervision, which typically consists of\nisolated, factual question-answer pairs. This \"bag-of-facts\" approach fails to\ncapture the underlying narrative and causal structure of events, limiting\nmodels to a shallow understanding of video content. To move beyond this\nparadigm, we introduce a framework to synthesize richer supervisory signals. We\npropose two complementary strategies: Question-Based Paraphrasing (QBP), which\nsynthesizes the diverse inquiries (what, how, why) from a video's existing set\nof question-answer pairs into a holistic narrative paragraph that reconstructs\nthe video's event structure; and Question-Based Captioning (QBC), which\ngenerates fine-grained visual rationales, grounding the answer to each question\nin specific, relevant evidence. Leveraging powerful generative models, we use\nthis synthetic data to train VideoQA models under a unified next-token\nprediction objective. Extensive experiments on STAR and NExT-QA validate our\napproach, demonstrating significant accuracy gains and establishing new\nstate-of-the-art results, such as improving a 3B model to 72.5\\% on STAR\n(+4.9\\%) and a 7B model to 80.8\\% on NExT-QA. Beyond accuracy, our analysis\nreveals that both QBP and QBC substantially enhance cross-dataset\ngeneralization, with QBP additionally accelerating model convergence by over\n2.5x. These results demonstrate that shifting data synthesis from isolated\nfacts to narrative coherence and grounded rationales yields a more accurate,\nefficient, and generalizable training paradigm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684VideoQA\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u8f6c\u8ff0\u548c\u95ee\u9898\u63cf\u8ff0\u4e24\u79cd\u7b56\u7565\uff0c\u4ece\u5b64\u7acb\u7684\u4e8b\u5b9e\u95ee\u7b54\u5bf9\u4e2d\u5408\u6210\u66f4\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfVideoQA\u6a21\u578b\u7684\u76d1\u7763\u4fe1\u53f7\u5c40\u9650\u4e8e\u5b64\u7acb\u7684\u4e8b\u5b9e\u95ee\u7b54\u5bf9\uff0c\u65e0\u6cd5\u6355\u6349\u89c6\u9891\u7684\u53d9\u4e8b\u7ed3\u6784\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u89c6\u9891\u5185\u5bb9\u7684\u6df1\u5ea6\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u95ee\u9898\u8f6c\u8ff0\u5c06\u591a\u6837\u5316\u95ee\u9898\u5408\u6210\u4e3a\u6574\u4f53\u53d9\u4e8b\u6bb5\u843d\u91cd\u6784\u4e8b\u4ef6\u7ed3\u6784\uff1b\u95ee\u9898\u63cf\u8ff0\u4e3a\u6bcf\u4e2a\u95ee\u9898\u751f\u6210\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4f9d\u636e\uff0c\u5c06\u7b54\u6848\u951a\u5b9a\u5728\u5177\u4f53\u8bc1\u636e\u4e0a\u3002\u5229\u7528\u751f\u6210\u6a21\u578b\u5408\u6210\u6570\u636e\uff0c\u5728\u7edf\u4e00\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u76ee\u6807\u4e0b\u8bad\u7ec3VideoQA\u6a21\u578b\u3002", "result": "\u5728STAR\u548cNExT-QA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u5e76\u5efa\u7acb\u65b0\u7684SOTA\u7ed3\u679c\uff0c\u59823B\u6a21\u578b\u5728STAR\u4e0a\u8fbe\u523072.5%\uff08+4.9%\uff09\uff0c7B\u6a21\u578b\u5728NExT-QA\u4e0a\u8fbe\u523080.8%\u3002QBP\u548cQBC\u90fd\u5927\u5e45\u589e\u5f3a\u4e86\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0cQBP\u8fd8\u52a0\u901f\u6a21\u578b\u6536\u655b2.5\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u5c06\u6570\u636e\u5408\u6210\u4ece\u5b64\u7acb\u4e8b\u5b9e\u8f6c\u5411\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u6709\u4f9d\u636e\u7684\u63a8\u7406\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u548c\u53ef\u6cdb\u5316\u7684\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2509.24762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24762", "abs": "https://arxiv.org/abs/2509.24762", "authors": ["David Berghaus", "Patrick Seifner", "Kostadin Cvejoski", "C\u00e9sar Ojeda", "Rams\u00e9s J. S\u00e1nchez"], "title": "In-Context Learning of Temporal Point Processes with Foundation Inference Models", "comment": null, "summary": "Modeling event sequences of multiple event types with marked temporal point\nprocesses (MTPPs) provides a principled way to uncover governing dynamical\nrules and predict future events. Current neural network approaches to MTPP\ninference rely on training separate, specialized models for each target system.\nWe pursue a radically different approach: drawing on amortized inference and\nin-context learning, we pretrain a deep neural network to infer, in-context,\nthe conditional intensity functions of event histories from a context defined\nby sets of event sequences. Pretraining is performed on a large synthetic\ndataset of MTPPs sampled from a broad distribution of Hawkes processes. Once\npretrained, our Foundation Inference Model for Point Processes (FIM-PP) can\nestimate MTPPs from real-world data without any additional training, or be\nrapidly finetuned to target systems. Experiments show that this amortized\napproach matches the performance of specialized models on next-event prediction\nacross common benchmark datasets.\n  Our pretrained model, repository and tutorials will soon be available online", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edcFIM-PP\uff0c\u80fd\u591f\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a8\u65ad\u4e8b\u4ef6\u5e8f\u5217\u7684\u6761\u4ef6\u5f3a\u5ea6\u51fd\u6570\uff0c\u65e0\u9700\u4e3a\u76ee\u6807\u7cfb\u7edf\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u76ee\u6807\u7cfb\u7edf\u8bad\u7ec3\u4e13\u95e8\u7684MTPP\u6a21\u578b\uff0c\u6211\u4eec\u8ffd\u6c42\u4e00\u79cd\u5b8c\u5168\u4e0d\u540c\u7684\u65b9\u6cd5\uff1a\u5229\u7528\u644a\u9500\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u9884\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u6a21\u578b\u3002", "method": "\u5728\u5e7f\u6cdb\u7684Hawkes\u8fc7\u7a0b\u5206\u5e03\u4e0a\u751f\u6210\u7684\u5927\u578b\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u5176\u80fd\u591f\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a8\u65ad\u4e8b\u4ef6\u5386\u53f2\u7684\u6761\u4ef6\u5f3a\u5ea6\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u644a\u9500\u65b9\u6cd5\u5728\u5e38\u89c1\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e2d\u4e0e\u4e13\u95e8\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "FIM-PP\u80fd\u591f\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u4f30\u8ba1\u771f\u5b9e\u6570\u636e\u7684MTPP\uff0c\u6216\u5feb\u901f\u5fae\u8c03\u5230\u76ee\u6807\u7cfb\u7edf\uff0c\u4e3a\u4e8b\u4ef6\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u901a\u7528\u65b9\u6cd5\u3002"}}
{"id": "2509.24382", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24382", "abs": "https://arxiv.org/abs/2509.24382", "authors": ["Soumyadeep Chandra", "Kaushik Roy"], "title": "REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport", "comment": "10 pages, 4 figures, 6 tables", "summary": "Learning from procedural videos remains a core challenge in self-supervised\nrepresentation learning, as real-world instructional data often contains\nbackground segments, repeated actions, and steps presented out of order. Such\nvariability violates the strong monotonicity assumptions underlying many\nalignment methods. Prior state-of-the-art approaches, such as OPEL, leverage\nKantorovich Optimal Transport (KOT) to build frame-to-frame correspondences,\nbut rely solely on feature similarity and fail to capture the higher-order\ntemporal structure of a task. In this paper, we introduce REALIGN, a\nself-supervised framework for procedure learning based on Regularized Fused\nPartial Gromov-Wasserstein Optimal Transport (R-FPGWOT). In contrast to KOT,\nour formulation jointly models visual correspondences and temporal relations\nunder a partial alignment scheme, enabling robust handling of irrelevant\nframes, repeated actions, and non-monotonic step orders common in instructional\nvideos. To stabilize training, we integrate FPGWOT distances with\ninter-sequence contrastive learning, avoiding the need for multiple\nregularizers and preventing collapse to degenerate solutions. Across egocentric\n(EgoProceL) and third-person (ProceL, CrossTask) benchmarks, REALIGN achieves\nup to 18.9% average F1-score improvements and over 30% temporal IoU gains,\nwhile producing more interpretable transport maps that preserve key-step\norderings and filter out noise.", "AI": {"tldr": "REALIGN\u662f\u4e00\u4e2a\u57fa\u4e8e\u6b63\u5219\u5316\u878d\u5408\u90e8\u5206Gromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u7684\u81ea\u76d1\u7763\u7a0b\u5e8f\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u7a0b\u5e8f\u89c6\u9891\u4e2d\u7684\u80cc\u666f\u7247\u6bb5\u3001\u91cd\u590d\u52a8\u4f5c\u548c\u975e\u5355\u8c03\u6b65\u9aa4\u987a\u5e8f\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6559\u5b66\u89c6\u9891\u901a\u5e38\u5305\u542b\u80cc\u666f\u7247\u6bb5\u3001\u91cd\u590d\u52a8\u4f5c\u548c\u4e71\u5e8f\u6b65\u9aa4\uff0c\u8fd9\u79cd\u53d8\u5f02\u6027\u8fdd\u53cd\u4e86\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u7684\u5f3a\u5355\u8c03\u6027\u5047\u8bbe\u3002\u73b0\u6709\u65b9\u6cd5\u5982OPEL\u4ec5\u4f9d\u8d56\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u4efb\u52a1\u7684\u9ad8\u9636\u65f6\u95f4\u7ed3\u6784\u3002", "method": "\u63d0\u51faR-FPGWOT\uff08\u6b63\u5219\u5316\u878d\u5408\u90e8\u5206Gromov-Wasserstein\u6700\u4f18\u4f20\u8f93\uff09\u65b9\u6cd5\uff0c\u8054\u5408\u5efa\u6a21\u89c6\u89c9\u5bf9\u5e94\u5173\u7cfb\u548c\u65f6\u95f4\u5173\u7cfb\uff0c\u901a\u8fc7\u90e8\u5206\u5bf9\u9f50\u65b9\u6848\u5904\u7406\u65e0\u5173\u5e27\u3001\u91cd\u590d\u52a8\u4f5c\u548c\u975e\u5355\u8c03\u6b65\u9aa4\u987a\u5e8f\u3002\u7ed3\u5408\u5e8f\u5217\u95f4\u5bf9\u6bd4\u5b66\u4e60\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728EgoProceL\u3001ProceL\u548cCrossTask\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREALIGN\u5b9e\u73b0\u4e86\u9ad8\u8fbe18.9%\u7684\u5e73\u5747F1\u5206\u6570\u63d0\u5347\u548c\u8d85\u8fc730%\u7684\u65f6\u95f4IoU\u589e\u76ca\uff0c\u540c\u65f6\u751f\u6210\u66f4\u53ef\u89e3\u91ca\u7684\u4f20\u8f93\u6620\u5c04\u3002", "conclusion": "REALIGN\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u89c6\u89c9\u548c\u65f6\u95f4\u5173\u7cfb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a0b\u5e8f\u89c6\u9891\u5b66\u4e60\u4e2d\u7684\u5bf9\u9f50\u6311\u6218\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.24779", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.24779", "abs": "https://arxiv.org/abs/2509.24779", "authors": ["Kacper Kapu\u015bniak", "Cristian Gabellini", "Michael Bronstein", "Prudencio Tossou", "Francesco Di Giovanni"], "title": "MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models", "comment": null, "summary": "Molecular Dynamics (MD) is a powerful computational microscope for probing\nprotein functions. However, the need for fine-grained integration and the long\ntimescales of biomolecular events make MD computationally expensive. To address\nthis, several generative models have been proposed to generate surrogate\ntrajectories at lower cost. Yet, these models typically learn a fixed-lag\ntransition density, causing the training signal to be dominated by frequent but\nuninformative transitions. We introduce a new class of generative models, MSM\nEmulators, which instead learn to sample transitions across discrete states\ndefined by an underlying Markov State Model (MSM). We instantiate this class\nwith Markov Space Flow Matching (MarS-FM), whose sampling offers more than two\norders of magnitude speedup compared to implicit- or explicit-solvent MD\nsimulations. We benchmark Mars-FM ability to reproduce MD statistics through\nstructural observables such as RMSD, radius of gyration, and secondary\nstructure content. Our evaluation spans protein domains (up to 500 residues)\nwith significant chemical and structural diversity, including unfolding events,\nand enforces strict sequence dissimilarity between training and test sets to\nassess generalization. Across all metrics, MarS-FM outperforms existing\nmethods, often by a substantial margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578bMSM Emulators\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u6a21\u578b\u5b66\u4e60\u79bb\u6563\u72b6\u6001\u95f4\u7684\u8f6c\u79fb\uff0c\u76f8\u6bd4\u4f20\u7edf\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5b9e\u73b0\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5b66\u4e60\u56fa\u5b9a\u6ede\u540e\u8f6c\u79fb\u5bc6\u5ea6\uff0c\u8bad\u7ec3\u4fe1\u53f7\u88ab\u9891\u7e41\u4f46\u65e0\u4fe1\u606f\u7684\u8f6c\u79fb\u4e3b\u5bfc\u3002", "method": "\u5f15\u5165MSM Emulators\u7c7b\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u6a21\u578b\u5b66\u4e60\u79bb\u6563\u72b6\u6001\u95f4\u7684\u8f6c\u79fb\uff0c\u5177\u4f53\u5b9e\u73b0\u4e3aMarkov Space Flow Matching (MarS-FM)\u3002", "result": "MarS-FM\u5728\u7ed3\u6784\u89c2\u6d4b\u6307\u6807\uff08RMSD\u3001\u56de\u8f6c\u534a\u5f84\u3001\u4e8c\u7ea7\u7ed3\u6784\u542b\u91cf\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728500\u4e2a\u6b8b\u57fa\u4ee5\u5185\u7684\u86cb\u767d\u8d28\u57df\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u6d4b\u8bd5\u96c6\u5e8f\u5217\u5dee\u5f02\u5927\u65f6\u4ecd\u80fd\u826f\u597d\u6cdb\u5316\u3002", "conclusion": "MSM Emulators\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0cMarS-FM\u5728\u751f\u6210\u8f68\u8ff9\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.24788", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2509.24788", "abs": "https://arxiv.org/abs/2509.24788", "authors": ["Felix Strnad", "Jonathan Schmidt", "Fabian Mockert", "Philipp Hennig", "Nicole Ludwig"], "title": "Assessing the risk of future Dunkelflaute events for Germany using generative deep learning", "comment": null, "summary": "The European electricity power grid is transitioning towards renewable energy\nsources, characterized by an increasing share of off- and onshore wind and\nsolar power. However, the weather dependency of these energy sources poses a\nchallenge to grid stability, with so-called Dunkelflaute events -- periods of\nlow wind and solar power generation -- being of particular concern due to their\npotential to cause electricity supply shortages. In this study, we investigate\nthe impact of these events on the German electricity production in the years\nand decades to come. For this purpose, we adapt a recently developed generative\ndeep learning framework to downscale climate simulations from the CMIP6\nensemble. We first compare their statistics to the historical record taken from\nERA5 data. Next, we use these downscaled simulations to assess plausible future\noccurrences of Dunkelflaute events in Germany under the optimistic low\n(SSP2-4.5) and high (SSP5-8.5) emission scenarios. Our analysis indicates that\nboth the frequency and duration of Dunkelflaute events in Germany in the\nensemble mean are projected to remain largely unchanged compared to the\nhistorical period. This suggests that, under the considered climate scenarios,\nthe associated risk is expected to remain stable throughout the century.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5bf9CMIP6\u6c14\u5019\u6a21\u62df\u8fdb\u884c\u964d\u5c3a\u5ea6\uff0c\u5206\u6790\u5fb7\u56fd\u672a\u6765Dunkelflaute\u4e8b\u4ef6\uff08\u4f4e\u98ce\u80fd\u548c\u592a\u9633\u80fd\u53d1\u7535\u671f\uff09\u7684\u53d8\u5316\u8d8b\u52bf\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728SSP2-4.5\u548cSSP5-8.5\u6392\u653e\u60c5\u666f\u4e0b\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u6301\u7eed\u65f6\u95f4\u4e0e\u5386\u53f2\u65f6\u671f\u76f8\u6bd4\u57fa\u672c\u4fdd\u6301\u4e0d\u53d8\u3002", "motivation": "\u6b27\u6d32\u7535\u7f51\u5411\u53ef\u518d\u751f\u80fd\u6e90\u8f6c\u578b\uff0c\u4f46\u98ce\u80fd\u548c\u592a\u9633\u80fd\u7684\u5929\u6c14\u4f9d\u8d56\u6027\u5bf9\u7535\u7f51\u7a33\u5b9a\u6027\u6784\u6210\u6311\u6218\uff0c\u7279\u522b\u662fDunkelflaute\u4e8b\u4ef6\u53ef\u80fd\u5bfc\u81f4\u7535\u529b\u4f9b\u5e94\u77ed\u7f3a\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u4e8b\u4ef6\u5bf9\u672a\u6765\u5fb7\u56fd\u7535\u529b\u751f\u4ea7\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5bf9CMIP6\u6c14\u5019\u6a21\u62df\u8fdb\u884c\u964d\u5c3a\u5ea6\uff0c\u9996\u5148\u4e0eERA5\u5386\u53f2\u6570\u636e\u7edf\u8ba1\u6bd4\u8f83\uff0c\u7136\u540e\u4f7f\u7528\u964d\u5c3a\u5ea6\u6a21\u62df\u8bc4\u4f30\u5fb7\u56fd\u672a\u6765Dunkelflaute\u4e8b\u4ef6\u7684\u53d1\u751f\u60c5\u51b5\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5728SSP2-4.5\u548cSSP5-8.5\u6392\u653e\u60c5\u666f\u4e0b\uff0c\u5fb7\u56fdDunkelflaute\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u6301\u7eed\u65f6\u95f4\u5728\u96c6\u5408\u5e73\u5747\u4e2d\u4e0e\u5386\u53f2\u65f6\u671f\u76f8\u6bd4\u57fa\u672c\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u5728\u6240\u8003\u8651\u7684\u6c14\u5019\u60c5\u666f\u4e0b\uff0cDunkelflaute\u4e8b\u4ef6\u76f8\u5173\u7684\u98ce\u9669\u9884\u8ba1\u5728\u6574\u4e2a\u4e16\u7eaa\u5185\u4fdd\u6301\u7a33\u5b9a\u3002"}}
{"id": "2509.24640", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24640", "abs": "https://arxiv.org/abs/2509.24640", "authors": ["Mohamad Ballout", "Okajevo Wilfred", "Seyedalireza Yaghoubi", "Nohayr Muhammad Abdelmoneim", "Julius Mayer", "Elia Bruni"], "title": "Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs", "comment": null, "summary": "In this work, we introduce SPLICE, a human-curated benchmark derived from the\nCOIN instructional video dataset, designed to probe event-based reasoning\nacross multiple dimensions: temporal, causal, spatial, contextual, and general\nknowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories\nand 180 sub-categories, such as sports, engineering, and housework. These\nvideos are segmented into a total of 11,423 event clips. We evaluate both human\nparticipants and state-of-the-art vision-language models (VLMs) on the task of\nrearranging these clips into coherent event sequences to assess visual\nreasoning capabilities. Results reveal a significant gap: VLMs struggle to\nmatch human performance. While human-annotated textual descriptions improve\nmodel accuracy, they do not affect human performance, suggesting that models\nrely more on language priors than on visual understanding. Even with\nannotations, VLMs fall short of human-level reasoning, underscoring persistent\nchallenges in visual reasoning. A deeper analysis across sub-categories shows\nthat VLMs perform relatively better on videos where temporal and causal\nreasoning are dominant, compared to those where contextual and spatial\nreasoning are dominant. They also perform better on everyday tasks than on\nspecialized ones.", "AI": {"tldr": "SPLICE\u662f\u4e00\u4e2a\u57fa\u4e8eCOIN\u6559\u5b66\u89c6\u9891\u6570\u636e\u96c6\u7684\u4eba\u7c7b\u6807\u6ce8\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u7ef4\u5ea6\u4e8b\u4ef6\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0VLMs\u5728\u4e8b\u4ef6\u6392\u5e8f\u4efb\u52a1\u4e2d\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u4e14\u66f4\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u975e\u89c6\u89c9\u7406\u89e3\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u4e8b\u4ef6\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u65f6\u95f4\u3001\u56e0\u679c\u3001\u7a7a\u95f4\u3001\u4e0a\u4e0b\u6587\u548c\u5e38\u8bc6\u63a8\u7406\u7b49\u591a\u4e2a\u7ef4\u5ea6\uff0c\u63ed\u793a\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u4eceCOIN\u6570\u636e\u96c6\u4e2d\u7b5b\u90093,381\u4e2a\u89c6\u9891\uff0c\u5206\u4e3a12\u4e2a\u7c7b\u522b\u548c180\u4e2a\u5b50\u7c7b\u522b\uff0c\u5206\u5272\u621011,423\u4e2a\u4e8b\u4ef6\u7247\u6bb5\u3002\u901a\u8fc7\u91cd\u65b0\u6392\u5217\u8fd9\u4e9b\u7247\u6bb5\u6765\u8bc4\u4f30\u4eba\u7c7b\u548cVLMs\u7684\u4e8b\u4ef6\u63a8\u7406\u80fd\u529b\u3002", "result": "\u4eba\u7c7b\u8868\u73b0\u663e\u8457\u4f18\u4e8eVLMs\u3002\u867d\u7136\u4eba\u7c7b\u6807\u6ce8\u7684\u6587\u672c\u63cf\u8ff0\u80fd\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u7387\uff0c\u4f46\u4e0d\u5f71\u54cd\u4eba\u7c7b\u8868\u73b0\uff0c\u8868\u660e\u6a21\u578b\u66f4\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u3002VLMs\u5728\u65f6\u95f4\u56e0\u679c\u63a8\u7406\u4e3b\u5bfc\u7684\u89c6\u9891\u4e2d\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\uff0c\u5728\u4e13\u4e1a\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5f53\u524dVLMs\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u4e0a\u4e0b\u6587\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u3002\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u975e\u771f\u6b63\u7684\u89c6\u89c9\u7406\u89e3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.24659", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24659", "abs": "https://arxiv.org/abs/2509.24659", "authors": ["Siddharth Roheda", "Aniruddha Bala", "Rohit Chowdhury", "Rohan Jaiswal"], "title": "VNODE: A Piecewise Continuous Volterra Neural Network", "comment": "5 pages", "summary": "This paper introduces Volterra Neural Ordinary Differential Equations\n(VNODE), a piecewise continuous Volterra Neural Network that integrates\nnonlinear Volterra filtering with continuous time neural ordinary differential\nequations for image classification. Drawing inspiration from the visual cortex,\nwhere discrete event processing is interleaved with continuous integration,\nVNODE alternates between discrete Volterra feature extraction and ODE driven\nstate evolution. This hybrid formulation captures complex patterns while\nrequiring substantially fewer parameters than conventional deep architectures.\nVNODE consistently outperforms state of the art models with improved\ncomputational complexity as exemplified on benchmark datasets like CIFAR10 and\nImagenet1K.", "AI": {"tldr": "VNODE\u662f\u4e00\u79cd\u7ed3\u5408\u975e\u7ebf\u6027Volterra\u6ee4\u6ce2\u548c\u795e\u7ecfODE\u7684\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u66ff\u79bb\u6563\u7279\u5f81\u63d0\u53d6\u548c\u8fde\u7eed\u72b6\u6001\u6f14\u5316\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u53d7\u89c6\u89c9\u76ae\u5c42\u4e2d\u79bb\u6563\u4e8b\u4ef6\u5904\u7406\u4e0e\u8fde\u7eed\u79ef\u5206\u4ea4\u66ff\u7684\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u6355\u6349\u590d\u6742\u6a21\u5f0f\u53c8\u53c2\u6570\u66f4\u5c11\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002", "method": "\u5c06\u975e\u7ebf\u6027Volterra\u6ee4\u6ce2\u4e0e\u8fde\u7eed\u65f6\u95f4\u795e\u7ecfODE\u7ed3\u5408\uff0c\u4ea4\u66ff\u8fdb\u884c\u79bb\u6563Volterra\u7279\u5f81\u63d0\u53d6\u548cODE\u9a71\u52a8\u7684\u72b6\u6001\u6f14\u5316\u3002", "result": "\u5728CIFAR10\u548cImagenet1K\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "VNODE\u901a\u8fc7\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u5efa\u6a21\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u9009\u62e9\u3002"}}
{"id": "2509.24709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24709", "abs": "https://arxiv.org/abs/2509.24709", "authors": ["Yang Chen", "Minghao Liu", "Yufan Shen", "Yunwen Li", "Tianyuan Huang", "Xinyu Fang", "Tianyu Zheng", "Wenxuan Huang", "Cheng Yang", "Daocheng Fu", "Jianbiao Mei", "Rong Wu", "Licheng Wen", "Xuemeng Yang", "Song Mao", "Qunshu Lin", "Zhi Yu", "Yongliang Shen", "Yu Qiao", "Botian Shi"], "title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?", "comment": null, "summary": "The webpage-to-code task requires models to understand visual representations\nof webpages and generate corresponding code. However, existing benchmarks\nprimarily focus on static screenshot-to-code tasks, thereby overlooking the\ndynamic interactions fundamental to real-world web applications. To address\nthis limitation, this paper introduces IWR-Bench, a novel benchmark for\nevaluating the capabilities of Large Vision-Language Models (LVLMs) in\ninteractive webpage reconstruction from video. IWR-Bench comprises 113\nmeticulously curated tasks from 100 real-world websites, with 1,001 actions and\nfeaturing diverse interaction complexities (e.g., web games), visual styles,\nand domains. Aligning with standard web development practices, each task\nincludes not only user interaction videos but also all crawled static assets\n(e.g., images, videos). This benchmark evaluates models on two fundamental\nchallenges: comprehensive multi-modal reasoning to infer interaction logic from\nvideo and assets, and advanced code generation to translate this logic into\nfunctional code. An agent-as-a-judge framework with a comprehensive metric\nsystem automatically assesses the functional correctness and visual fidelity of\ngenerated webpages. Extensive experiments on 28 LVLMs reveal a significant\nchallenge: the best model achieves an overall score of only 36.35%, as\nfunctional correctness (24.39% IFS) lags significantly behind visual fidelity\n(64.25% VFS). These results highlight critical limitations in current models'\nability to reason about temporal dynamics and synthesize event-driven logic,\nestablishing IWR-Bench as a challenging frontier for vision-language research.\nThe benchmark and evaluation code will be made publicly available. Code is\navailable at https://github.com/L-O-I/IWR-Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86IWR-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u89c6\u9891\u91cd\u5efa\u4ea4\u4e92\u5f0f\u7f51\u9875\u7684\u80fd\u529b\uff0c\u5305\u542b113\u4e2a\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u529f\u80fd\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u622a\u56fe\u5230\u4ee3\u7801\u7684\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u771f\u5b9e\u7f51\u9875\u5e94\u7528\u4e2d\u7684\u52a8\u6001\u4ea4\u4e92\u7279\u6027\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u4ea4\u4e92\u903b\u8f91\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b113\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u771f\u5b9e\u7f51\u7ad9\u7684\u4ea4\u4e92\u89c6\u9891\u548c\u9759\u6001\u8d44\u6e90\uff0c\u4f7f\u7528\u4ee3\u7406\u4f5c\u4e3a\u8bc4\u5224\u6846\u67b6\u81ea\u52a8\u8bc4\u4f30\u751f\u6210\u7f51\u9875\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u5bf928\u4e2aLVLM\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u6700\u4f73\u6a21\u578b\u603b\u4f53\u5f97\u5206\u4ec536.35%\uff0c\u529f\u80fd\u6b63\u786e\u6027\u5f97\u5206(24.39%)\u8fdc\u4f4e\u4e8e\u89c6\u89c9\u4fdd\u771f\u5ea6\u5f97\u5206(64.25%)\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u7406\u89e3\u65f6\u95f4\u52a8\u6001\u548c\u5408\u6210\u4e8b\u4ef6\u9a71\u52a8\u903b\u8f91\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5c40\u9650\uff0cIWR-Bench\u4e3a\u89c6\u89c9\u8bed\u8a00\u7814\u7a76\u8bbe\u7acb\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u524d\u6cbf\u57fa\u51c6\u3002"}}
{"id": "2509.24791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24791", "abs": "https://arxiv.org/abs/2509.24791", "authors": ["Cheng Shi", "Yizhou Yu", "Sibei Yang"], "title": "Vision Function Layer in Multimodal LLMs", "comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)", "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u76f8\u5173\u529f\u80fd\u89e3\u7801\u5206\u5e03\u5728\u4e0d\u540c\u7684\u89e3\u7801\u5668\u5c42\uff0c\u6bcf\u79cd\u89c6\u89c9\u529f\u80fd\uff08\u5982\u8ba1\u6570\u3001\u5b9a\u4f4d\u3001OCR\u8bc6\u522b\uff09\u96c6\u4e2d\u57282-3\u4e2a\u7279\u5b9a\u5c42\uff08Vision Function Layers\uff09\u3002\u8fd9\u4e9bVFL\u7684\u6df1\u5ea6\u548c\u987a\u5e8f\u5728\u4e0d\u540cMLLMs\u4e2d\u5448\u73b0\u4e00\u81f4\u6a21\u5f0f\uff0c\u4e0e\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u76f8\u7b26\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86VFL-LoRA\u548cVFL-select\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u529f\u80fd\u7684\u5177\u4f53\u5206\u5e03\u673a\u5236\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fd9\u79cd\u5206\u5e03\u6a21\u5f0f\u6765\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u548c\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u4ee4\u724c\u4ea4\u6362\uff08Visual Token Swapping\uff09\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u7279\u5b9aKV\u7f13\u5b58\u6761\u76ee\u6765\u7cbe\u786e\u63ed\u793a\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5404\u5c42\u7684\u529f\u80fd\u3002\u57fa\u4e8eVFL\u53d1\u73b0\u5f00\u53d1\u4e86VFL-LoRA\uff08\u9009\u62e9\u6027\u8bad\u7ec3\u76f8\u5173\u529f\u80fd\u5c42\uff09\u548cVFL-select\uff08\u81ea\u52a8\u6570\u636e\u5206\u7c7b\u9009\u62e9\uff09\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u89c6\u89c9\u529f\u80fd\u96c6\u4e2d\u5728\u7279\u5b9a2-3\u4e2a\u89e3\u7801\u5c42\uff0c\u4e14\u5c42\u95f4\u987a\u5e8f\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u4e00\u81f4\uff08\u8bc6\u522b\u2192\u8ba1\u6570\u2192\u5b9a\u4f4d\uff09\u3002VFL-LoRA\u4f18\u4e8e\u5168\u53c2\u6570LoRA\u8bad\u7ec3\uff0c\u9632\u6b62\u529f\u80fd\u9057\u5fd8\u3002VFL-select\u4ec5\u752820%\u6570\u636e\u5373\u53ef\u8fbe\u523098%\u5168\u6570\u636e\u6027\u80fd\uff0c\u8d85\u8d8a\u4eba\u5de5\u6570\u636e\u9009\u62e9\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5316\u4e86\u5bf9MLLM\u89c6\u89c9\u5904\u7406\u673a\u5236\u7684\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2509.24871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24871", "abs": "https://arxiv.org/abs/2509.24871", "authors": ["Xiangyu Zeng", "Kefan Qiu", "Qingyu Zhang", "Xinhao Li", "Jing Wang", "Jiaxin Li", "Ziang Yan", "Kun Tian", "Meng Tian", "Xinhai Zhao", "Yi Wang", "Limin Wang"], "title": "StreamForest: Efficient Online Video Understanding with Persistent Event Memory", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable\nprogress in video understanding. However, their effectiveness in real-time\nstreaming scenarios remains limited due to storage constraints of historical\nvisual features and insufficient real-time spatiotemporal reasoning. To address\nthese challenges, we propose StreamForest, a novel architecture specifically\ndesigned for streaming video understanding. Central to StreamForest is the\nPersistent Event Memory Forest, a memory mechanism that adaptively organizes\nvideo frames into multiple event-level tree structures. This process is guided\nby penalty functions based on temporal distance, content similarity, and merge\nfrequency, enabling efficient long-term memory retention under limited\ncomputational resources. To enhance real-time perception, we introduce a\nFine-grained Spatiotemporal Window, which captures detailed short-term visual\ncues to improve current scene perception. Additionally, we present OnlineIT, an\ninstruction-tuning dataset tailored for streaming video tasks. OnlineIT\nsignificantly boosts MLLM performance in both real-time perception and future\nprediction. To evaluate generalization in practical applications, we introduce\nODV-Bench, a new benchmark focused on real-time streaming video understanding\nin autonomous driving scenarios. Experimental results demonstrate that\nStreamForest achieves the state-of-the-art performance, with accuracies of\n77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In\nparticular, even under extreme visual token compression (limited to 1024\ntokens), the model retains 96.8% of its average accuracy in eight benchmarks\nrelative to the default setting. These results underscore the robustness,\nefficiency, and generalizability of StreamForest for streaming video\nunderstanding.", "AI": {"tldr": "StreamForest\u662f\u4e00\u4e2a\u4e13\u4e3a\u6d41\u5a92\u4f53\u89c6\u9891\u7406\u89e3\u8bbe\u8ba1\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u6301\u4e45\u4e8b\u4ef6\u8bb0\u5fc6\u68ee\u6797\u548c\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u7a97\u53e3\u89e3\u51b3\u5386\u53f2\u89c6\u89c9\u7279\u5f81\u5b58\u50a8\u548c\u5b9e\u65f6\u65f6\u7a7a\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5b9e\u65f6\u6d41\u5a92\u4f53\u573a\u666f\u4e2d\u7531\u4e8e\u5386\u53f2\u89c6\u89c9\u7279\u5f81\u5b58\u50a8\u9650\u5236\u548c\u5b9e\u65f6\u65f6\u7a7a\u63a8\u7406\u4e0d\u8db3\u800c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faStreamForest\u67b6\u6784\uff0c\u6838\u5fc3\u662f\u6301\u4e45\u4e8b\u4ef6\u8bb0\u5fc6\u68ee\u6797\u673a\u5236\uff0c\u901a\u8fc7\u57fa\u4e8e\u65f6\u95f4\u8ddd\u79bb\u3001\u5185\u5bb9\u76f8\u4f3c\u6027\u548c\u5408\u5e76\u9891\u7387\u7684\u60e9\u7f5a\u51fd\u6570\u81ea\u9002\u5e94\u5730\u5c06\u89c6\u9891\u5e27\u7ec4\u7ec7\u6210\u591a\u4e2a\u4e8b\u4ef6\u7ea7\u6811\u7ed3\u6784\uff1b\u5f15\u5165\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u7a97\u53e3\u6355\u83b7\u8be6\u7ec6\u7684\u77ed\u671f\u89c6\u89c9\u7ebf\u7d22\uff1b\u8fd8\u63d0\u51fa\u4e86\u4e13\u95e8\u7528\u4e8e\u6d41\u5a92\u4f53\u89c6\u9891\u4efb\u52a1\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6OnlineIT\u3002", "result": "\u5728StreamingBench\u4e0a\u8fbe\u523077.3%\u51c6\u786e\u7387\uff0cOVBench\u4e0a60.5%\uff0cOVO-Bench\u4e0a55.6%\uff1b\u5373\u4f7f\u5728\u6781\u7aef\u89c6\u89c9token\u538b\u7f29\uff08\u9650\u5236\u4e3a1024\u4e2atoken\uff09\u4e0b\uff0c\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u9ed8\u8ba4\u8bbe\u7f6e\u4e0b\u5e73\u5747\u51c6\u786e\u7387\u768496.8%\u3002", "conclusion": "StreamForest\u5728\u6d41\u5a92\u4f53\u89c6\u9891\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3001\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24968", "abs": "https://arxiv.org/abs/2509.24968", "authors": ["Donghwa Kang", "Junho Kim", "Dongwoo Kang"], "title": "Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning", "comment": "11 pages, 7 figures", "summary": "Event cameras offer unique advantages for facial keypoint alignment under\nchallenging conditions, such as low light and rapid motion, due to their high\ntemporal resolution and robustness to varying illumination. However, existing\nRGB facial keypoint alignment methods do not perform well on event data, and\ntraining solely on event data often leads to suboptimal performance because of\nits limited spatial information. Moreover, the lack of comprehensive labeled\nevent datasets further hinders progress in this area. To address these issues,\nwe propose a novel framework based on cross-modal fusion attention (CMFA) and\nself-supervised multi-event representation learning (SSMER) for event-based\nfacial keypoint alignment. Our framework employs CMFA to integrate\ncorresponding RGB data, guiding the model to extract robust facial features\nfrom event input images. In parallel, SSMER enables effective feature learning\nfrom unlabeled event data, overcoming spatial limitations. Extensive\nexperiments on our real-event E-SIE dataset and a synthetic-event version of\nthe public WFLW-V benchmark show that our approach consistently surpasses\nstate-of-the-art methods across multiple evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8de8\u6a21\u6001\u878d\u5408\u6ce8\u610f\u529b\u548c\u81ea\u76d1\u7763\u591a\u4e8b\u4ef6\u8868\u793a\u5b66\u4e60\u7684\u4e8b\u4ef6\u76f8\u673a\u9762\u90e8\u5173\u952e\u70b9\u5bf9\u9f50\u6846\u67b6\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u4f4e\u5149\u7167\u548c\u5feb\u901f\u8fd0\u52a8\u6761\u4ef6\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709RGB\u65b9\u6cd5\u5728\u4e8b\u4ef6\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4e8b\u4ef6\u6570\u636e\u7a7a\u95f4\u4fe1\u606f\u6709\u9650\uff0c\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6", "method": "\u4f7f\u7528\u8de8\u6a21\u6001\u878d\u5408\u6ce8\u610f\u529b\u6574\u5408RGB\u6570\u636e\u6307\u5bfc\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u591a\u4e8b\u4ef6\u8868\u793a\u5b66\u4e60\u4ece\u65e0\u6807\u7b7e\u4e8b\u4ef6\u6570\u636e\u4e2d\u5b66\u4e60\u7279\u5f81", "result": "\u5728\u771f\u5b9eE-SIE\u6570\u636e\u96c6\u548c\u5408\u6210WFLW-V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6570\u636e\u7a7a\u95f4\u4fe1\u606f\u6709\u9650\u548c\u6807\u6ce8\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9762\u90e8\u5173\u952e\u70b9\u5bf9\u9f50\u6027\u80fd"}}
{"id": "2509.24980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24980", "abs": "https://arxiv.org/abs/2509.24980", "authors": ["Shuang Liang", "Jing He", "Chuanmeizhi Wang", "Lejun Liao", "Guo Zhang", "Yingcong Chen", "Yuan Yuan"], "title": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation", "comment": "18 pages, 9 figures, 9 tables", "summary": "Pre-trained diffusion models provide rich multi-scale latent features and are\nemerging as powerful vision backbones. While recent works such as\nMarigold~\\citep{ke2024repurposing} and Lotus~\\citep{he2024lotus} adapt\ndiffusion priors for dense prediction with strong cross-domain generalization,\ntheir potential for structured outputs (e.g., human pose estimation) remains\nunderexplored. In this paper, we propose \\textbf{SDPose}, a fine-tuning\nframework built upon Stable Diffusion to fully exploit pre-trained diffusion\npriors for human pose estimation. First, rather than modifying cross-attention\nmodules or introducing learnable embeddings, we directly predict keypoint\nheatmaps in the SD U-Net's image latent space to preserve the original\ngenerative priors. Second, we map these latent features into keypoint heatmaps\nthrough a lightweight convolutional pose head, which avoids disrupting the\npre-trained backbone. Finally, to prevent overfitting and enhance\nout-of-distribution robustness, we incorporate an auxiliary RGB reconstruction\nbranch that preserves domain-transferable generative semantics. To evaluate\nrobustness under domain shift, we further construct \\textbf{COCO-OOD}, a\nstyle-transferred variant of COCO with preserved annotations. With just\none-fifth of the training schedule used by Sapiens on COCO, SDPose attains\nparity with Sapiens-1B/2B on the COCO validation set and establishes a new\nstate of the art on the cross-domain benchmarks HumanArt and COCO-OOD.\nFurthermore, we showcase SDPose as a zero-shot pose annotator for downstream\ncontrollable generation tasks, including ControlNet-based image synthesis and\nvideo generation, where it delivers qualitatively superior pose guidance.", "AI": {"tldr": "SDPose\u662f\u4e00\u4e2a\u57fa\u4e8eStable Diffusion\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u5148\u9a8c\u8fdb\u884c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u5728\u5c11\u91cf\u8bad\u7ec3\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5177\u6709\u51fa\u8272\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u591a\u5c3a\u5ea6\u6f5c\u5728\u7279\u5f81\uff0c\u4f46\u5b83\u4eec\u5728\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5982\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff09\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5982Marigold\u548cLotus\u4e3b\u8981\u5173\u6ce8\u5bc6\u96c6\u9884\u6d4b\uff0c\u800cSDPose\u65e8\u5728\u5145\u5206\u5229\u7528\u6269\u6563\u5148\u9a8c\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u3002", "method": "1. \u76f4\u63a5\u5728SD U-Net\u7684\u56fe\u50cf\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9884\u6d4b\u5173\u952e\u70b9\u70ed\u56fe\uff0c\u4fdd\u7559\u539f\u59cb\u751f\u6210\u5148\u9a8c\uff1b2. \u901a\u8fc7\u8f7b\u91cf\u7ea7\u5377\u79ef\u59ff\u6001\u5934\u5c06\u6f5c\u5728\u7279\u5f81\u6620\u5c04\u5230\u5173\u952e\u70b9\u70ed\u56fe\uff1b3. \u52a0\u5165\u8f85\u52a9RGB\u91cd\u5efa\u5206\u652f\u4ee5\u9632\u6b62\u8fc7\u62df\u5408\u5e76\u589e\u5f3a\u8de8\u57df\u9c81\u68d2\u6027\u3002", "result": "\u4ec5\u4f7f\u7528Sapiens\u4e94\u5206\u4e4b\u4e00\u8bad\u7ec3\u65f6\u95f4\uff0c\u5728COCO\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230Sapiens-1B/2B\u540c\u7b49\u6c34\u5e73\uff0c\u5728HumanArt\u548cCOCO-OOD\u8de8\u57df\u57fa\u51c6\u4e0a\u521b\u4e0b\u65b0SOTA\u3002\u8fd8\u80fd\u4f5c\u4e3a\u96f6\u6837\u672c\u59ff\u6001\u6807\u6ce8\u5668\u7528\u4e8e\u53ef\u63a7\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "SDPose\u8bc1\u660e\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u9aa8\u5e72\u5728\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u4fdd\u7559\u751f\u6210\u5148\u9a8c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.25151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25151", "abs": "https://arxiv.org/abs/2509.25151", "authors": ["Zhaozhi Wang", "Tong Zhang", "Mingyue Guo", "Yaowei Wang", "Qixiang Ye"], "title": "VideoAnchor: Reinforcing Subspace-Structured Visual Cues for Coherent Visual-Spatial Reasoning", "comment": "16 pages, 6 figures", "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in\nvision-language alignment, yet they remain limited in visual-spatial reasoning.\nWe first identify that this limitation arises from the attention mechanism:\nvisual tokens are overshadowed by language tokens, preventing the model from\nconsistently recognizing the same visual cues across frames. To address this\nchallenge, we draw a novel connection between the self-expressiveness property\nin sparse subspace clustering and the attention mechanism in Transformers.\nBuilding on this insight, we propose VideoAnchor, a plug-and-play module that\nleverages subspace affinities to reinforce visual cues across frames without\nretraining, effectively anchoring attention to shared visual structures.\nExtensive experiments across benchmarks and backbone models show consistent\nperformance gains -- $e.g.$, 3.2% and 4.6% improvements on VSI-Bench and\nVideo-MME (spatial-related tasks) with InternVL2-8B and Qwen2.5VL-72B -- while\nqualitative analyses demonstrate more coherent subspace partitions and stronger\nvisual grounding. Our codes will be made public available at\nhttps://github.com/feufhd/VideoAnchor.", "AI": {"tldr": "VideoAnchor\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u901a\u8fc7\u5229\u7528\u5b50\u7a7a\u95f4\u4eb2\u548c\u6027\u6765\u589e\u5f3a\u8de8\u5e27\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u951a\u5b9a\u6ce8\u610f\u529b\u5230\u5171\u4eab\u7684\u89c6\u89c9\u7ed3\u6784\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002\u8fd9\u79cd\u9650\u5236\u6e90\u4e8e\u6ce8\u610f\u529b\u673a\u5236\uff1a\u89c6\u89c9\u6807\u8bb0\u88ab\u8bed\u8a00\u6807\u8bb0\u6240\u63a9\u76d6\uff0c\u963b\u6b62\u6a21\u578b\u8de8\u5e27\u4e00\u81f4\u5730\u8bc6\u522b\u76f8\u540c\u7684\u89c6\u89c9\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u4e86VideoAnchor\u6a21\u5757\uff0c\u5c06\u7a00\u758f\u5b50\u7a7a\u95f4\u805a\u7c7b\u4e2d\u7684\u81ea\u8868\u8fbe\u7279\u6027\u4e0eTransformer\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u8054\u7cfb\u8d77\u6765\uff0c\u5229\u7528\u5b50\u7a7a\u95f4\u4eb2\u548c\u6027\u6765\u5f3a\u5316\u8de8\u5e27\u7684\u89c6\u89c9\u7ebf\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f8b\u5982\u5728VSI-Bench\u548cVideo-MME\uff08\u7a7a\u95f4\u76f8\u5173\u4efb\u52a1\uff09\u4e0a\u5206\u522b\u5b9e\u73b0\u4e863.2%\u548c4.6%\u7684\u6539\u8fdb\u3002\u5b9a\u6027\u5206\u6790\u5c55\u793a\u4e86\u66f4\u8fde\u8d2f\u7684\u5b50\u7a7a\u95f4\u5206\u533a\u548c\u66f4\u5f3a\u7684\u89c6\u89c9\u57fa\u7840\u3002", "conclusion": "VideoAnchor\u901a\u8fc7\u5b50\u7a7a\u95f4\u4eb2\u548c\u6027\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u539f\u6709\u7ed3\u6784\u3002"}}
{"id": "2509.25187", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25187", "abs": "https://arxiv.org/abs/2509.25187", "authors": ["Yunyang Ge", "Xinhua Cheng", "Chengshu Zhao", "Xianyi He", "Shenghai Yuan", "Bin Lin", "Bin Zhu", "Li Yuan"], "title": "FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation", "comment": null, "summary": "In Image-to-Video (I2V) generation, a video is created using an input image\nas the first-frame condition. Existing I2V methods concatenate the full\ninformation of the conditional image with noisy latents to achieve high\nfidelity. However, the denoisers in these methods tend to shortcut the\nconditional image, which is known as conditional image leakage, leading to\nperformance degradation issues such as slow motion and color inconsistency. In\nthis work, we further clarify that conditional image leakage leads to\noverfitting to in-domain data and decreases the performance in out-of-domain\nscenarios. Moreover, we introduce Fourier-Guided Latent Shifting I2V, named\nFlashI2V, to prevent conditional image leakage. Concretely, FlashI2V consists\nof: (1) Latent Shifting. We modify the source and target distributions of flow\nmatching by subtracting the conditional image information from the noisy\nlatents, thereby incorporating the condition implicitly. (2) Fourier Guidance.\nWe use high-frequency magnitude features obtained by the Fourier Transform to\naccelerate convergence and enable the adjustment of detail levels in the\ngenerated video. Experimental results show that our method effectively\novercomes conditional image leakage and achieves the best generalization and\nperformance on out-of-domain data among various I2V paradigms. With only 1.3B\nparameters, FlashI2V achieves a dynamic degree score of 53.01 on Vbench-I2V,\nsurpassing CogVideoX1.5-5B-I2V and Wan2.1-I2V-14B-480P. Github page:\nhttps://pku-yuangroup.github.io/FlashI2V/", "AI": {"tldr": "FlashI2V\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u504f\u79fb\u548c\u5085\u91cc\u53f6\u5f15\u5bfc\u6765\u89e3\u51b3\u6761\u4ef6\u56fe\u50cf\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728\u57df\u5916\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684I2V\u65b9\u6cd5\u5b58\u5728\u6761\u4ef6\u56fe\u50cf\u6cc4\u6f0f\u95ee\u9898\uff0c\u5bfc\u81f4\u8fd0\u52a8\u7f13\u6162\u3001\u989c\u8272\u4e0d\u4e00\u81f4\u7b49\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u4e14\u5728\u57df\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFlashI2V\uff0c\u5305\u542b\uff1a(1) \u6f5c\u5728\u504f\u79fb\uff1a\u901a\u8fc7\u4ece\u566a\u58f0\u6f5c\u5728\u4e2d\u51cf\u53bb\u6761\u4ef6\u56fe\u50cf\u4fe1\u606f\u6765\u4fee\u6539\u6d41\u5339\u914d\u7684\u6e90\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\uff1b(2) \u5085\u91cc\u53f6\u5f15\u5bfc\uff1a\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\u83b7\u5f97\u7684\u9ad8\u9891\u5e45\u5ea6\u7279\u5f81\u6765\u52a0\u901f\u6536\u655b\u5e76\u8c03\u6574\u751f\u6210\u89c6\u9891\u7684\u7ec6\u8282\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u6761\u4ef6\u56fe\u50cf\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728Vbench-I2V\u4e0a\u52a8\u6001\u5ea6\u5f97\u5206\u8fbe\u523053.01\uff0c\u8d85\u8d8a\u4e86CogVideoX1.5-5B-I2V\u548cWan2.1-I2V-14B-480P\u3002", "conclusion": "FlashI2V\u4ec5\u75281.3B\u53c2\u6570\u5c31\u5728\u5404\u79cdI2V\u8303\u5f0f\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u57df\u5916\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.10202", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10202", "abs": "https://arxiv.org/abs/2506.10202", "authors": ["Shubhashis Roy Dipta", "Francis Ferraro"], "title": "Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval", "comment": null, "summary": "Recent approaches have shown impressive proficiency in extracting and\nleveraging parametric knowledge from Large-Language Models (LLMs) and\nVision-Language Models (VLMs). In this work, we consider how we can improve the\nidentification and retrieval of videos related to complex real-world events by\nautomatically extracting latent parametric knowledge about those events. We\npresent Q2E: a Query-to-Event decomposition method for zero-shot multilingual\ntext-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our\napproach demonstrates that we can enhance the understanding of otherwise overly\nsimplified human queries by decomposing the query using the knowledge embedded\nin LLMs and VLMs. We additionally show how to apply our approach to both visual\nand speech-based inputs. To combine this varied multimodal knowledge, we adopt\nentropy-based fusion scoring for zero-shot fusion. Through evaluations on two\ndiverse datasets and multiple retrieval metrics, we demonstrate that Q2E\noutperforms several state-of-the-art baselines. Our evaluation also shows that\nintegrating audio information can significantly improve text-to-video\nretrieval. We have released code and data for future research.", "AI": {"tldr": "Q2E\u662f\u4e00\u79cd\u67e5\u8be2\u5230\u4e8b\u4ef6\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u591a\u8bed\u8a00\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\uff0c\u901a\u8fc7\u5206\u89e3\u67e5\u8be2\u6765\u589e\u5f3a\u5bf9\u7b80\u5316\u4eba\u7c7b\u67e5\u8be2\u7684\u7406\u89e3\uff0c\u5e76\u6574\u5408\u89c6\u89c9\u548c\u8bed\u97f3\u4fe1\u606f\u3002", "motivation": "\u6539\u8fdb\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u76f8\u5173\u89c6\u9891\u7684\u8bc6\u522b\u548c\u68c0\u7d22\uff0c\u901a\u8fc7\u81ea\u52a8\u63d0\u53d6\u5173\u4e8e\u8fd9\u4e9b\u4e8b\u4ef6\u7684\u6f5c\u5728\u53c2\u6570\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u5d4c\u5165\u5728LLMs\u548cVLMs\u4e2d\u7684\u77e5\u8bc6\u5206\u89e3\u67e5\u8be2\uff0c\u91c7\u7528\u57fa\u4e8e\u71b5\u7684\u878d\u5408\u8bc4\u5206\u8fdb\u884c\u96f6\u6837\u672c\u878d\u5408\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u548c\u8bed\u97f3\u8f93\u5165\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u591a\u4e2a\u68c0\u7d22\u6307\u6807\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cQ2E\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u97f3\u9891\u4fe1\u606f\u7684\u6574\u5408\u663e\u8457\u6539\u5584\u4e86\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u3002", "conclusion": "Q2E\u65b9\u6cd5\u901a\u8fc7\u5206\u89e3\u67e5\u8be2\u548c\u6574\u5408\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u96f6\u6837\u672c\u591a\u8bed\u8a00\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
