<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification](https://arxiv.org/abs/2511.08711)
*Abhipsa Basu,Aviral Gupta,Abhijnya Bhat,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 该论文研究了使用扩散模型微调技术生成平衡训练数据以解决图像分类中的群体偏见问题，通过LoRA和DreamBooth等方法生成更准确的群体表示图像，并采用聚类策略处理组内变异。


<details>
  <summary>Details</summary>
Motivation: 图像分类系统常因训练数据中群体分布不均而继承偏见，例如人脸数据集中金发与女性的过度关联会强化刻板印象。现有方法使用Stable Diffusion生成平衡数据但难以保持原始数据分布。

Method: 探索多种扩散模型微调技术（LoRA和DreamBooth），直接从样本学习以生成更准确的群体表示图像；为防止单个DreamBooth模型被组内变异淹没，对每个群体内的图像进行聚类并为每个聚类训练一个DreamBooth模型；使用这些模型生成群体平衡数据进行预训练，然后在真实数据上微调。

Result: 在多个基准测试中，所研究的微调方法平均优于原始Stable Diffusion，且与最先进的去偏技术如Group-DRO效果相当，随着数据集偏见严重程度增加，表现更优。

Conclusion: 扩散模型微调技术能有效生成平衡训练数据以缓解图像分类中的群体偏见问题，特别是在偏见严重的数据集上表现突出。

Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.

</details>


### [2] [WiCV at CVPR 2025: The Women in Computer Vision Workshop](https://arxiv.org/abs/2511.08748)
*Estefania Talavera,Deblina Bhattacharjee,Himangi Mittal,Mengwei Ren,Karen Sanchez,Carla Muntean,JungEun Kim,Mona Jalal*

Main category: cs.CV

TL;DR: WiCV@CVPR 2025是第16届计算机视觉女性研讨会，旨在提升女性及少数群体在计算机视觉领域的可见度、包容性和专业发展。活动包括论文展示、导师计划和网络交流，吸引了100多名现场参与者。


<details>
  <summary>Details</summary>
Motivation: 记录WiCV研讨会的影响和演变，为未来版本和其他促进AI和计算机视觉社区多样性、公平性和包容性的倡议提供参考。

Method: 通过论文展示（14篇全文论文，其中5篇口头报告，36篇扩展摘要海报）、导师计划（80名学员与37名导师匹配）和网络活动来促进专业发展。

Result: 研讨会成功吸引了100多名现场参与者，获得了10家赞助商支持，提供了约44,000美元的旅行补助和多样性奖项。

Conclusion: WiCV@CVPR 2025继续其使命，赋能新兴研究人员，并在计算机视觉领域放大多元声音，为促进社区多样性做出了重要贡献。

Abstract: The Women in Computer Vision Workshop (WiCV@CVPR 2025) was held in conjunction with the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025) in Nashville, Tennessee, United States. This report presents an overview of the workshop program, participation statistics, mentorship outcomes, and historical trends from previous WiCV editions. The goal is to document the impact and evolution of WiCV as a reference for future editions and for other initiatives aimed at advancing diversity, equity, and inclusion within the AI and computer vision communities. WiCV@CVPR 2025 marked the 16th edition of this long-standing event dedicated to increasing the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community. This year's workshop featured 14 accepted papers in the CVPR Workshop Proceedings out of 32 full-paper submissions. Five of these were selected for oral presentations, while all 14 were also presented as posters, along with 36 extended abstract posters accepted from 62 short-paper submissions, which are not included in the proceedings. The mentoring program matched 80 mentees with 37 mentors from both academia and industry. The 2025 edition attracted over 100 onsite participants, fostering rich technical and networking interactions across all career stages. Supported by 10 sponsors and approximately $44,000 USD in travel grants and diversity awards, WiCV continued its mission to empower emerging researchers and amplify diverse voices in computer vision.

</details>


### [3] [Ultra-Light Test-Time Adaptation for Vision--Language Models](https://arxiv.org/abs/2511.09101)
*Byunghyun Kim*

Main category: cs.CV

TL;DR: UL-TTA是一种无需训练和反向传播的轻量级测试时适应方法，通过在线EM风格过程调整logit级参数（类别原型、先验概率和温度），在领域偏移下显著提升CLIP等视觉语言模型的准确性和校准性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法需要反向传播、协方差估计或大量内存，不适用于流式和边缘场景。在领域偏移下，视觉语言模型存在特征漂移、类别先验不匹配和严重校准错误的问题。

Method: 冻结主干网络，仅在线调整logit级参数：使用选择性样本过滤、基于文本和Dirichlet先验的闭式贝叶斯更新、解耦的温度参数，以及轻量级防护机制防止长期流中的漂移。

Result: 在多个大规模跨域和OOD基准测试中，UL-TTA相比零样本CLIP平均提升4.7个百分点的top-1准确率，同时将ECE降低20-30%，延迟开销小于8%。在长达20万样本的流式测试中无崩溃。

Conclusion: logit级贝叶斯适应足以在领域偏移下为视觉语言模型获得最先进的准确率-校准权衡，无需更新任何主干网络参数。

Abstract: Vision-Language Models (VLMs) such as CLIP achieve strong zero-shot recognition by comparing image embeddings to text-derived class prototypes. However, under domain shift, they suffer from feature drift, class-prior mismatch, and severe miscalibration. Existing test-time adaptation (TTA) methods often require backpropagation through large backbones, covariance estimation, or heavy memory/state, which is problematic for streaming and edge scenarios. We propose Ultra-Light Test-Time Adaptation (UL-TTA), a fully training-free and backprop-free framework that freezes the backbone and adapts only logit-level parameters: class prototypes, class priors, and temperature. UL-TTA performs an online EM-style procedure with (i) selective sample filtering to use only confident predictions, (ii) closed-form Bayesian updates for prototypes and priors anchored by text and Dirichlet priors, (iii) decoupled temperatures for prediction vs. calibration, and (iv) lightweight guards (norm clipping, prior KL constraints, smoothed temperature) to prevent drift in long streams. Across large-scale cross-domain and OOD benchmarks (PACS, Office-Home, DomainNet, Terra Incognita, ImageNet-R/A/V2/Sketch; ~726K test samples) and strong TTA baselines including Tent, T3A, CoTTA, SAR, Tip-Adapter, and FreeTTA, UL-TTA consistently improves top-1 accuracy (e.g., +4.7 points over zero-shot CLIP on average) while reducing ECE by 20-30%, with less than 8% latency overhead. Long-stream experiments up to 200K samples show no collapse. Our results demonstrate that logit-level Bayesian adaptation is sufficient to obtain state-of-the-art accuracy-calibration trade-offs for VLMs under domain shift, without updating any backbone parameters.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion](https://arxiv.org/abs/2511.08653)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.LG

TL;DR: CGAR是一种新的递归推理模型训练方法，通过渐进深度课程学习和分层监督加权，在保持性能的同时显著降低训练成本，在Sudoku-Extreme数据集上实现1.71倍训练加速和42%成本减少。


<details>
  <summary>Details</summary>
Motivation: 递归推理模型虽然性能优异，但训练成本高昂（约36 GPU小时/数据集），限制了广泛应用和研究。

Method: 提出CGAR方法：渐进深度课程学习动态调整递归深度，分层监督加权对监督步骤应用指数衰减权重，与梯度幅度衰减对齐。

Result: 在Sudoku-Extreme数据集上，CGAR实现1.71倍训练加速（10.93到6.38小时），准确率仅下降0.63%（86.65%到86.02%），推理效率提升，100%停止准确率和11%更少推理步骤。

Conclusion: 基于架构深度的原则性课程学习能够在适度硬件上高效训练递归推理模型，实现了训练效率和解决方案质量的帕累托改进。

Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku

</details>


### [5] [Transformer-Based Sleep Stage Classification Enhanced by Clinical Information](https://arxiv.org/abs/2511.08864)
*Woosuk Chung,Seokwoo Hong,Wonhyeok Lee,Sangyoon Bae*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer编码器和CNN聚合器的两阶段架构，通过融合临床元数据和专家事件标注等上下文信息，显著提升了自动睡眠分期的准确性。


<details>
  <summary>Details</summary>
Motivation: 手动睡眠分期劳动密集且存在评分者间差异，现有深度学习模型大多只依赖原始PSG信号，忽略了人类专家使用的上下文线索。

Method: 使用两阶段架构：Transformer-based每时段编码器+1D CNN聚合器，系统性地融合受试者临床元数据（年龄、性别、BMI）和每时段专家事件标注（呼吸暂停、血氧下降、觉醒、周期性呼吸）。

Result: 在SHHS队列（n=8,357）上，相比仅使用PSG的基线（macro-F1 0.7745，micro-F1 0.8774），最终模型达到macro-F1 0.8031和micro-F1 0.9051，事件标注贡献最大增益。特征融合优于多任务替代方案。

Conclusion: 将临床有意义特征与学习表示结合，在不改变PSG配置或需要额外传感器的情况下，既提升了性能又增强了可解释性，为实现上下文感知、专家对齐的睡眠分期系统提供了实用且可扩展的路径。

Abstract: Manual sleep staging from polysomnography (PSG) is labor-intensive and prone to inter-scorer variability. While recent deep learning models have advanced automated staging, most rely solely on raw PSG signals and neglect contextual cues used by human experts. We propose a two-stage architecture that combines a Transformer-based per-epoch encoder with a 1D CNN aggregator, and systematically investigates the effect of incorporating explicit context: subject-level clinical metadata (age, sex, BMI) and per-epoch expert event annotations (apneas, desaturations, arousals, periodic breathing). Using the Sleep Heart Health Study (SHHS) cohort (n=8,357), we demonstrate that contextual fusion substantially improves staging accuracy. Compared to a PSG-only baseline (macro-F1 0.7745, micro-F1 0.8774), our final model achieves macro-F1 0.8031 and micro-F1 0.9051, with event annotations contributing the largest gains. Notably, feature fusion outperforms multi-task alternatives that predict the same auxiliary labels. These results highlight that augmenting learned representations with clinically meaningful features enhances both performance and interpretability, without modifying the PSG montage or requiring additional sensors. Our findings support a practical and scalable path toward context-aware, expert-aligned sleep staging systems.

</details>


### [6] [Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks](https://arxiv.org/abs/2511.09114)
*Tim Dudman,Martyn Bull*

Main category: cs.LG

TL;DR: 本文提出了TERLA（拓扑扩展强化学习代理），使网络防御代理能够适应不同拓扑和大小的网络而无需重新训练，通过图神经网络和固定大小动作空间实现泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习网络防御代理在拓扑或规模变化时需要重新训练，难以适应真实网络中不断变化的网络结构。

Method: 使用异构图神经网络层生成固定大小的网络状态潜在嵌入，结合固定大小、语义明确且可解释的动作空间，基于PPO算法构建TERLA代理。

Result: TERLA代理在保持标准PPO代理防御性能的同时提高了动作效率，能够部署到不同拓扑和大小的网络段上，展现了良好的泛化能力。

Conclusion: TERLA方法成功解决了网络防御代理的泛化问题，为真实网络环境中的自适应防御提供了可行方案。

Abstract: Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.

</details>


### [7] [Sure! Here's a short and concise title for your paper: "Contamination in Generated Text Detection Benchmarks"](https://arxiv.org/abs/2511.09200)
*Philipp Dingfelder,Christian Riess*

Main category: cs.LG

TL;DR: 本文分析了DetectRL基准数据集中存在的AI生成文本模式问题，通过数据清洗提高了检测器的鲁棒性，并发布了重新处理的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的AI文本检测基准数据集存在质量问题，特别是DetectRL数据集中98.5%的Claude-LLM数据包含简单的AI生成模式，这些模式可能被检测器作为捷径利用，导致易受欺骗攻击。

Method: 对DetectRL数据集进行多种清洗操作，移除AI生成文本中的特定模式（如开头语"Sure! Here is the academic article abstract:"和任务拒绝实例），重新处理数据集。

Result: 实验表明，数据清洗使得直接攻击检测器变得更加困难，提高了检测器的鲁棒性。

Conclusion: 数据清洗是提高AI文本检测器性能的有效方法，重新处理的数据集已公开可用，有助于更可靠的检测器训练和评估。

Abstract: Large language models are increasingly used for many applications. To prevent illicit use, it is desirable to be able to detect AI-generated text. Training and evaluation of such detectors critically depend on suitable benchmark datasets. Several groups took on the tedious work of collecting, curating, and publishing large and diverse datasets for this task. However, it remains an open challenge to ensure high quality in all relevant aspects of such a dataset. For example, the DetectRL benchmark exhibits relatively simple patterns of AI-generation in 98.5% of the Claude-LLM data. These patterns may include introductory words such as "Sure! Here is the academic article abstract:", or instances where the LLM rejects the prompted task. In this work, we demonstrate that detectors trained on such data use such patterns as shortcuts, which facilitates spoofing attacks on the trained detectors. We consequently reprocessed the DetectRL dataset with several cleansing operations. Experiments show that such data cleansing makes direct attacks more difficult. The reprocessed dataset is publicly available.

</details>


### [8] [A Tensor Residual Circuit Neural Network Factorized with Matrix Product Operation](https://arxiv.org/abs/2511.09315)
*Andi Chen*

Main category: cs.LG

TL;DR: 提出了一种新颖的张量电路神经网络（TCNN），结合张量神经网络和残差电路模型的优势，在低复杂度下实现出色的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在降低神经网络复杂度的同时难以保持泛化能力和鲁棒性，特别是量子启发的神经网络和混合张量神经网络中的全连接层在泛化能力和鲁棒性方面不如量子计算中的电路模型。

Method: 提出TCNN模型，利用复数域中的激活操作和电路并行性提高非线性特征学习效率，引入信息融合层合并实部和虚部参数中的特征信息以增强泛化能力。

Result: 实验结果表明，TCNN在各种数据集上的平均准确率比最先进的对比模型高出2%-3%，在噪声参数攻击下仍能保持显著的学习能力，且参数量和CPU运行时间与对比模型相当。

Conclusion: TCNN通过激活操作、并行架构和信息融合层的结合，在保持低复杂度的同时实现了优异的泛化能力和鲁棒性。

Abstract: It is challenging to reduce the complexity of neural networks while maintaining their generalization ability and robustness, especially for practical applications. Conventional solutions for this problem incorporate quantum-inspired neural networks with Kronecker products and hybrid tensor neural networks with MPO factorization and fully-connected layers. Nonetheless, the generalization power and robustness of the fully-connected layers are not as outstanding as circuit models in quantum computing. In this paper, we propose a novel tensor circuit neural network (TCNN) that takes advantage of the characteristics of tensor neural networks and residual circuit models to achieve generalization ability and robustness with low complexity. The proposed activation operation and parallelism of the circuit in complex number field improves its non-linearity and efficiency for feature learning. Moreover, since the feature information exists in the parameters in both the real and imaginary parts in TCNN, an information fusion layer is proposed for merging features stored in those parameters to enhance the generalization capability. Experimental results confirm that TCNN showcases more outstanding generalization and robustness with its average accuracies on various datasets 2\%-3\% higher than those of the state-of-the-art compared models. More significantly, while other models fail to learn features under noise parameter attacking, TCNN still showcases prominent learning capability owing to its ability to prevent gradient explosion. Furthermore, it is comparable to the compared models on the number of trainable parameters and the CPU running time. An ablation study also indicates the advantage of the activation operation, the parallelism architecture and the information fusion layer.

</details>


### [9] [AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting](https://arxiv.org/abs/2511.09478)
*Renda Li,Hailang Huang,Fei Wei,Feng Xiong,Yong Wang,Xiangxiang Chu*

Main category: cs.LG

TL;DR: AdaCuRL是一个自适应课程强化学习框架，通过粗到细的难度估计和自适应课程调度来解决强化学习在语言模型推理训练中的梯度饥饿和策略退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在混合难度样本上训练时会出现梯度饥饿和策略退化问题，而构建高质量思维链数据成本高昂，传统课程学习方法存在难度不匹配、依赖人工设计和灾难性遗忘等挑战。

Method: 提出AdaCuRL框架，整合粗到细难度估计与自适应课程调度，动态对齐数据难度与模型能力，包含数据重访机制防止灾难性遗忘，并采用自适应参考和稀疏KL策略防止策略退化。

Result: 在多种推理基准测试中，AdaCuRL在大型语言模型和多模态语言模型上都实现了显著的性能提升。

Conclusion: AdaCuRL通过自适应课程强化学习有效解决了推理训练中的关键问题，为语言模型的推理能力提升提供了有效解决方案。

Abstract: Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.

</details>


### [10] [Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach](https://arxiv.org/abs/2511.09475)
*Anli Ji,Pranjal Patil,Chetraj Pandey,Manolis K. Georgoulis,Berkay Aydin*

Main category: cs.LG

TL;DR: 提出一个结合全局解释和特设特征映射的新框架，用于提高太阳高能粒子事件预测模型的透明度和可解释性，帮助理解物理机制而不仅仅是获得预测结果。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的SEP预测方法多为黑盒模型，难以解释结果和理解物理原因，需要提高模型透明度以支持太阳物理研究。

Method: 集成全局解释和特设特征映射的框架，使用341个SEP事件数据集（包括244个重要质子事件）进行验证，并进行了可解释性案例研究。

Result: 方法提高了模型的可解释性，促进了基于物理的SEP事件预测理解，验证了框架的有效性。

Conclusion: 该框架成功增强了SEP预测模型的透明度，为太阳物理学家提供了更深入理解事件物理机制的途径，推动了可解释AI在空间天气预测中的应用。

Abstract: Solar energetic particle (SEP) events, as one of the most prominent manifestations of solar activity, can generate severe hazardous radiation when accelerated by solar flares or shock waves formed aside from coronal mass ejections (CMEs). However, most existing data-driven methods used for SEP predictions are operated as black-box models, making it challenging for solar physicists to interpret the results and understand the underlying physical causes of such events rather than just obtain a prediction. To address this challenge, we propose a novel framework that integrates global explanations and ad-hoc feature mapping to enhance model transparency and provide deeper insights into the decision-making process. We validate our approach using a dataset of 341 SEP events, including 244 significant (>=10 MeV) proton events exceeding the Space Weather Prediction Center S1 threshold, spanning solar cycles 22, 23, and 24. Furthermore, we present an explainability-focused case study of major SEP events, demonstrating how our method improves explainability and facilitates a more physics-informed understanding of SEP event prediction.

</details>


### [11] [Event-Driven Digital-Time-Domain Inference Architectures for Tsetlin Machines](https://arxiv.org/abs/2511.09527)
*Tian Lan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: 提出了一种基于数字时域计算的Tsetlin机器推理方法，通过延迟累积机制和胜者全得方案来减少算术运算，显著提升了能效和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在推理过程中通常需要大量算术计算，这会增加延迟和功耗。为了解决这个问题，需要开发更高效的推理架构。

Method: 采用数字时域计算方法，使用延迟累积机制替代昂贵的类别算术和，用胜者全得方案替换传统的幅度比较器。对于多类TM实现汉明距离驱动的时域方案，对于融合TM使用差分延迟路径和领先一检测器对数延迟压缩方案。

Result: 与功能等效的后实现数字TM架构基线相比，所提出的架构在能效和吞吐量方面实现了数量级的提升。

Conclusion: 数字时域计算方法为Tsetlin机器推理提供了一种高效的解决方案，显著降低了计算成本和功耗。

Abstract: Machine learning fits model parameters to approximate input-output mappings, predicting unknown samples. However, these models often require extensive arithmetic computations during inference, increasing latency and power consumption. This paper proposes a digital-time-domain computing approach for Tsetlin machine (TM) inference process to address these challenges. This approach leverages a delay accumulation mechanism to mitigate the costly arithmetic sums of classes and employs a Winner-Takes-All scheme to replace conventional magnitude comparators. Specifically, a Hamming distance-driven time-domain scheme is implemented for multi-class TMs. Furthermore, differential delay paths, combined with a leading-ones-detector logarithmic delay compression digital-time-domain scheme, are utilised for the coalesced TMs, accommodating both binary-signed and exponential-scale delay accumulation issues. Compared to the functionally equivalent, post-implementation digital TM architecture baseline, the proposed architecture demonstrates orders-of-magnitude improvements in energy efficiency and throughput.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030)
*Elliot Meyerson,Giuseppe Paolo,Roberto Dailey,Hormoz Shahrzad,Olivier Francon,Conor F. Hayes,Xin Qiu,Babak Hodjat,Risto Miikkulainen*

Main category: cs.AI

TL;DR: MAKER系统首次实现了超过100万步LLM推理的零错误任务执行，通过极端任务分解和微代理投票机制解决了LLM在长程任务中的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理、洞察和工具使用方面取得突破，但在执行类似人类组织和社会规模的扩展过程时存在持续错误率问题，无法实现大规模任务链式执行。

Method: 采用极端任务分解策略，将任务分解为可由专注微代理处理的子任务，通过高效的多代理投票方案在每一步进行错误校正。

Result: 成功解决了超过100万步LLM推理的任务且零错误，理论上可扩展到更高水平。

Conclusion: 相比持续改进当前LLMs，大规模分解代理过程（MDAPs）可能为组织和社会层面的问题提供高效解决方案。

Abstract: LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.

</details>


### [13] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: 提出了一个名为Argus的运行时韧性框架，用于增强端到端自动驾驶系统的安全性，通过持续监控轨迹和危险缓解机制来防止安全违规。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在公共道路上部署时面临各种驾驶危险，需要具备持续监控和自适应响应能力来维持稳健的驾驶行为。

Method: Argus框架持续监控ADS生成的轨迹，当检测到不安全状态时通过危险缓解器无缝接管控制。

Result: 与TCP、UniAD和VAD三个先进ADS集成测试，驾驶评分平均提升150.30%，防止了64.38%的违规，时间开销很小。

Conclusion: Argus能有效提升自动驾驶系统的韧性，显著改善驾驶性能并预防安全违规。

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [14] [HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting](https://arxiv.org/abs/2511.09275)
*Minlan Shao,Zijian Zhang,Yili Wang,Yiwei Dai,Xu Shen,Xin Wang*

Main category: cs.AI

TL;DR: HyperD是一个用于交通预测的新框架，通过将交通数据分解为周期性和残差分量来应对复杂的时空依赖性和多尺度周期模式。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临两大挑战：复杂的空间依赖性和多尺度周期模式与不规则波动的共存。需要有效分离周期性模式和非周期性波动以提高预测准确性。

Method: 提出HyperD框架，包含混合周期表示模块（处理周期性分量）和频率感知残差表示模块（处理非周期性波动），并引入双视图对齐损失来强制语义分离。

Result: 在四个真实交通数据集上的实验表明，HyperD实现了最先进的预测精度，在干扰下具有更好的鲁棒性，并且计算效率优于现有方法。

Conclusion: HyperD通过解耦周期性和残差分量，有效解决了交通预测中的关键挑战，为智能交通系统提供了准确可靠的预测解决方案。

Abstract: Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Structured Uncertainty guided Clarification for LLM Agents](https://arxiv.org/abs/2511.08798)
*Manan Suri,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi,Dinesh Manocha*

Main category: cs.CL

TL;DR: 提出SAGE-Agent，通过结构化不确定性建模工具调用参数，使用POMDP和EVPI优化问题选择，在模糊任务中提高覆盖率7-39%，同时减少澄清问题1.5-2.7倍。


<details>
  <summary>Details</summary>
Motivation: LLM代理在工具调用时，模糊的用户指令会导致错误调用和任务失败，需要更有效的澄清机制。

Method: 将工具参数的结构化不确定性建模为POMDP，使用EVPI目标进行最优问题选择，采用基于方面的成本建模防止冗余。

Result: 在ClarifyBench基准测试中表现优异，提高模糊任务覆盖率7-39%，减少澄清问题1.5-2.7倍。通过不确定性加权GRPO训练，将When2Call准确率从36.5%提升至65.2%。

Conclusion: 结构化不确定性为工具增强代理提供了原则性、高效的方法，在现实场景中同时提高任务成功率和交互效率。

Abstract: LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\% while reducing clarification questions by 1.5-2.7$\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\% to 65.2\% (3B model) and 36.7\% to 62.9\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.

</details>


### [16] [Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional Support in Language Models](https://arxiv.org/abs/2511.09003)
*Zhouxing Tan,Ruochong Xiong,Yulong Wan,Jinlong Ma,Hanlin Xue,Qichun Deng,Haifeng Jing,Zhengtong Zhang,Depei Liu,Shiyuan Luo,Junfei Liu*

Main category: cs.CL

TL;DR: 本文提出了一个基于轨迹的情感支持评估框架，从静态对话评估转向动态长期评估，通过模拟328个情感情境和1152个扰动事件来评估LLMs在长期情感支持中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型情感支持评估多依赖短时静态对话，无法捕捉情感支持的动态性和长期性，需要建立更贴近真实交互的评估方法。

Method: 采用用户中心视角，构建大规模基准测试，使用验证的情绪调节策略约束模型输出，将用户情感轨迹建模为一阶马尔可夫过程，并应用因果调整的情感估计方法。

Result: 提出了三个轨迹级指标（BEL、ETV、ECP）来捕捉用户情感动态，并在多样化LLMs上进行了广泛评估，揭示了情感支持能力的显著差异。

Conclusion: 该框架为LLMs的长期情感支持能力提供了全面评估方法，并为模型开发提供了可操作的见解。

Abstract: Emotional support is a core capability in human-AI interaction, with applications including psychological counseling, role play, and companionship. However, existing evaluations of large language models (LLMs) often rely on short, static dialogues and fail to capture the dynamic and long-term nature of emotional support. To overcome this limitation, we shift from snapshot-based evaluation to trajectory-based assessment, adopting a user-centered perspective that evaluates models based on their ability to improve and stabilize user emotional states over time. Our framework constructs a large-scale benchmark consisting of 328 emotional contexts and 1,152 disturbance events, simulating realistic emotional shifts under evolving dialogue scenarios. To encourage psychologically grounded responses, we constrain model outputs using validated emotion regulation strategies such as situation selection and cognitive reappraisal. User emotional trajectories are modeled as a first-order Markov process, and we apply causally-adjusted emotion estimation to obtain unbiased emotional state tracking. Based on this framework, we introduce three trajectory-level metrics: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). These metrics collectively capture user emotional dynamics over time and support comprehensive evaluation of long-term emotional support performance of LLMs. Extensive evaluations across a diverse set of LLMs reveal significant disparities in emotional support capabilities and provide actionable insights for model development.

</details>


### [17] [Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning](https://arxiv.org/abs/2511.09222)
*Jiarui Liu,Kaustubh Dhole,Yingheng Wang,Haoyang Wen,Sarah Zhang,Haitao Mao,Gaotang Li,Neeraj Varshney,Jingguo Liu,Xiaoman Pan*

Main category: cs.CL

TL;DR: 本文提出Anchor方法，通过将真实轨迹注入rollouts来防止早期训练崩溃，显著提升语言模型在可验证推理任务中的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在优化最终任务结果时，容易在早期训练阶段因负奖励主导而崩溃，特别是在诚实对齐任务中，模型需要既能回答可解答查询，又能识别无法从前提得出结论的情况。

Method: 提出Anchor强化学习方法，将真实轨迹注入rollouts以防止早期训练崩溃；使用从图结构构建的两个多步演绎推理数据集（线性代数和逻辑推理），其中一半实例通过随机扰动边来创建不可回答案例。

Result: Anchor方法稳定了学习过程，显著提高了整体推理性能；而GRPO方法（无论是否使用监督微调初始化）在这些任务上表现不佳；课程学习虽有一定帮助但需要精心设计的分布内数据集。

Conclusion: 训练动态对于使对齐语言模型实现可靠演绎推理至关重要，Anchor方法通过防止早期训练崩溃有效解决了诚实对齐中的稳定性问题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently emerged as a promising framework for aligning language models with complex reasoning objectives. However, most existing methods optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. This challenge is especially pronounced in honesty alignment, where models must not only solve answerable queries but also identify when conclusions cannot be drawn from the given premises. Deductive reasoning provides an ideal testbed because it isolates reasoning capability from reliance on external factual knowledge. To investigate honesty alignment, we curate two multi-step deductive reasoning datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that GRPO, with or without supervised fine tuning initialization, struggles on these tasks. Through extensive experiments across three models, we evaluate stabilization strategies and show that curriculum learning provides some benefit but requires carefully designed in distribution datasets with controllable difficulty. To address these limitations, we propose Anchor, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling reliable deductive reasoning in aligned language models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [18] [Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning](https://arxiv.org/abs/2511.08942)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 该论文提出了一个框架，将视觉语言模型从被动观察者转变为主动策略制定者，通过结构化思维链提示、动态动作历史集成和障碍物地图解释等技术，显著提升了机器人导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用视觉语言模型的推理能力，需要将其角色从被动观察者转变为导航过程中的主动策略制定者，以释放其在机器人领域的全部潜力。

Method: 采用结构化思维链提示激发逐步逻辑推理，动态集成智能体近期动作历史防止陷入循环，以及让VLM能够解释俯视障碍物地图和第一人称视图以增强空间感知能力。

Result: 在HM3D、Gibson和MP3D等挑战性基准测试中，该方法产生了极其直接和逻辑的轨迹，导航效率相比现有方法有显著提升。

Conclusion: 该方法为开发更强大的具身智能体开辟了道路，通过将VLM作为高级规划器，实现了更智能的机器人导航。

Abstract: While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.

</details>
