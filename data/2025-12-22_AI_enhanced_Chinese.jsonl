{"id": "2512.16929", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.16929", "abs": "https://arxiv.org/abs/2512.16929", "authors": ["Pranesh Sathish Kumar"], "title": "BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control", "comment": "12 pages, 8 figures", "summary": "Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.", "AI": {"tldr": "\u4f4e\u6210\u672c\u53cc\u6a21\u5f0f\u795e\u7ecf\u808c\u8089\u63a7\u5236\u7cfb\u7edf\uff0c\u7ed3\u5408EEG\u548cEMG\u5b9e\u73b0\u5047\u80a2\u591a\u81ea\u7531\u5ea6\u5b9e\u65f6\u63a7\u5236", "motivation": "\u4f20\u7edf\u4f4e\u6210\u672c\u4e0a\u80a2\u5047\u80a2\u7f3a\u4e4f\u76f4\u89c2\u63a7\u5236\u7cfb\u7edf\uff0c\u9650\u5236\u4e86\u622a\u80a2\u8005\u5728\u8d44\u6e90\u532e\u4e4f\u73af\u5883\u4e0b\u7684\u529f\u80fd\u6027\u548c\u53ef\u53ca\u6027", "method": "\u4f7f\u7528NeuroSky MindWave Mobile 2\u91c7\u96c6EEG\u4fe1\u53f7\uff0c\u901a\u8fc7ThinkGear\u84dd\u7259\u4f20\u8f93\u5230ESP32\u8fd0\u884c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u6a21\u578b\uff1b\u4f7f\u7528MyoWare 2.0\u4f20\u611f\u5668\u91c7\u96c6EMG\u4fe1\u53f7\uff0c\u901a\u8fc7SparkFun\u65e0\u7ebf\u6a21\u5757\u4f20\u8f93\u5230\u7b2c\u4e8c\u4e2aESP32\u8fdb\u884c\u9608\u503c\u68c0\u6d4b\u3002EEG\u63a7\u5236\u624b\u6307\u5f00\u5408\uff0cEMG\u63a7\u5236\u8098\u90e8\u8fd0\u52a8", "result": "\u6784\u5efa\u4e86\u529f\u80fd\u539f\u578b\uff08\u603b\u6210\u672c\u7ea6240\u7f8e\u5143\uff09\uff0cEEG\u901a\u8fc7\u7728\u773c\u68c0\u6d4b\u63a7\u5236\u624b\u6307\uff0cEMG\u901a\u8fc7\u4e09\u4e2a\u6fc0\u6d3b\u5e26\u63a7\u5236\u8098\u90e8\uff0c\u7cfb\u7edf\u7a33\u5b9a\u53ef\u9760", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u3001\u751f\u7269\u76f4\u89c2\u5047\u80a2\u63a7\u5236\u7684\u53ef\u884c\u9014\u5f84\uff0c\u9002\u5408\u8d44\u6e90\u532e\u4e4f\u548c\u5168\u7403\u5065\u5eb7\u5e94\u7528\uff0c\u672a\u6765\u53ef\u6539\u8fdb3D\u6253\u5370\u5916\u58f3\u3001\u964d\u4f4eEMG\u5ef6\u8fdf\u3001\u63d0\u5347\u4f3a\u670d\u626d\u77e9"}}
{"id": "2512.16967", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16967", "abs": "https://arxiv.org/abs/2512.16967", "authors": ["Marcelo Cerda Castillo"], "title": "Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes", "comment": "12 pages, 5 tables, 1 figure. Uses publicly available METAR surface observations and TAF forecast data for benchmarking", "summary": "Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.\n  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing", "AI": {"tldr": "\u57fa\u4e8eXGBoost\u7684\u8f7b\u91cf\u7ea7\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u5229\u7528\u5730\u9762\u89c2\u6d4b\u6570\u636e\u548c\u7269\u7406\u5f15\u5bfc\u7279\u5f81\u5de5\u7a0b\uff0c\u5b9e\u73b0\u4f4e\u80fd\u89c1\u5ea6\u548c\u964d\u6c34\u4e8b\u4ef6\u7684\u77ed\u671f\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u56fd\u9645\u673a\u573a\u9a8c\u8bc1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfTAF\u9884\u62a5\u3002", "motivation": "\u5f53\u524d\u822a\u7a7a\u6c14\u8c61\u4e1a\u52a1\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\u548c\u4eba\u5de5TAF\u4ea7\u54c1\uff0c\u5b58\u5728\u4fdd\u5b88\u504f\u5dee\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u77ed\u671f\u9884\u6d4b\u65b9\u6cd5\u4fdd\u969c\u822a\u7a7a\u5b89\u5168\u3002", "method": "\u91c7\u7528XGBoost\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5730\u9762\u89c2\u6d4b\u6570\u636e\uff08METAR\uff09\uff0c\u901a\u8fc7\u70ed\u529b\u5b66\u539f\u7406\u8fdb\u884c\u7269\u7406\u5f15\u5bfc\u7279\u5f81\u5de5\u7a0b\uff0c\u572811\u4e2a\u4e0d\u540c\u6c14\u5019\u533a\u7684\u56fd\u9645\u673a\u573a\uff082000-2024\u5e74\u6570\u636e\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u57283\u5c0f\u65f6\u6218\u672f\u9884\u6d4b\u4e2d\uff0c\u6a21\u578b\u76f8\u6bd4\u4e1a\u52a1TAF\u9884\u62a5\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7387\uff0c\u53ec\u56de\u7387\u63d0\u53472.5-4.0\u500d\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8bef\u62a5\u7387\u3002SHAP\u5206\u6790\u663e\u793a\u6a21\u578b\u80fd\u9690\u5f0f\u91cd\u5efa\u5c40\u5730\u7269\u7406\u9a71\u52a8\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u5c40\u5730\u7269\u7406\u8fc7\u7a0b\uff0c\u65e0\u9700\u4eba\u5de5\u914d\u7f6e\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\uff0c\u4e3a\u822a\u7a7a\u6c14\u8c61\u77ed\u671f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.17266", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17266", "abs": "https://arxiv.org/abs/2512.17266", "authors": ["Miru Hong", "Minho Lee", "Geonhee Jo", "Jae-Hee So", "Pascal Bauer", "Sang-Ki Ko"], "title": "ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework", "comment": "8 pages, 2 figures, 7 tables. To appear in Hudl Performance Insights 2025", "summary": "Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.", "AI": {"tldr": "EventGPT\uff1a\u57fa\u4e8eGPT\u67b6\u6784\u7684\u7403\u5458\u6761\u4ef6\u5316\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6a21\u62df\u8bc4\u4f30\u7403\u5458\u8f6c\u4f1a\u9002\u5e94\u6027", "motivation": "\u73b0\u6709\u8f6c\u4f1a\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u7edf\u8ba1\u6570\u636e\u6216\u540e\u9a8c\u4ef7\u503c\u6a21\u578b\uff0c\u65e0\u6cd5\u6355\u6349\u7403\u5458\u5728\u65b0\u6218\u672f\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u53d8\u5316\uff0c\u9700\u8981\u80fd\u6a21\u62df\u7403\u5458\u5728\u4e0d\u540c\u7403\u961f\u4e2d\u8868\u73b0\u7684\u65b9\u6cd5", "method": "\u57fa\u4e8eGPT\u98ce\u683c\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u5c06\u6bd4\u8d5b\u5904\u7406\u4e3a\u79bb\u6563token\u5e8f\u5217\uff0c\u8054\u5408\u9884\u6d4b\u4e0b\u4e00\u52a8\u4f5c\u7c7b\u578b\u3001\u4f4d\u7f6e\u3001\u65f6\u95f4\u548c\u6b8b\u5dee\u63a7\u7403\u4ef7\u503c\uff0c\u901a\u8fc7\u66ff\u6362\u7403\u5458\u5d4c\u5165\u8fdb\u884c\u53cd\u4e8b\u5b9e\u6a21\u62df", "result": "\u5728\u82f1\u8d855\u4e2a\u8d5b\u5b63\u6570\u636e\u4e0a\uff0cEventGPT\u5728\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a7a\u95f4\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u5e8f\u5217\u57fa\u7ebf\uff0c\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5728\u8f6c\u4f1a\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c", "conclusion": "EventGPT\u4e3a\u8f6c\u4f1a\u9002\u5e94\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u6a21\u62df\u7403\u5458\u5728\u4e0d\u540c\u6218\u672f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u53d8\u5316\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.17373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17373", "abs": "https://arxiv.org/abs/2512.17373", "authors": ["Zhengmian Hu"], "title": "Dialectics for Artificial Intelligence", "comment": null, "summary": "Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of \"concept\" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents \"concepts\" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ece\u7b97\u6cd5\u4fe1\u606f\u8bba\u89c6\u89d2\u5b9a\u4e49\"\u6982\u5ff5\"\uff0c\u5c06\u6982\u5ff5\u89c6\u4e3a\u4e0e\u667a\u80fd\u4f53\u6574\u4f53\u7ecf\u9a8c\u76f8\u5173\u7684\u4fe1\u606f\u5bf9\u8c61\uff0c\u901a\u8fc7\u53ef\u9006\u4e00\u81f4\u6027\u5173\u7cfb\u548c\u5197\u4f59\u4fe1\u606f\u5ea6\u91cf\u6765\u5f62\u5f0f\u5316\u6982\u5ff5\u53d1\u73b0\u4e0e\u6f14\u5316\uff0c\u5e76\u5efa\u7acb\u591a\u667a\u80fd\u4f53\u6982\u5ff5\u5bf9\u9f50\u7684\u901a\u4fe1\u6846\u67b6\u3002", "motivation": "\u4eba\u7c7b\u6982\u5ff5\u5177\u6709\u6d41\u52a8\u6027\uff08\u5982\u51a5\u738b\u661f\u4e0d\u518d\u88ab\u89c6\u4e3a\u884c\u661f\uff09\uff0c\u4f20\u7edf\u57fa\u4e8e\u8bcd\u5178\u6807\u7b7e\u7684\u6982\u5ff5\u5b9a\u4e49\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u52a8\u6001\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fee\u8ba2\u3001\u6bd4\u8f83\u548c\u5728\u667a\u80fd\u4f53\u95f4\u5bf9\u9f50\u7684\u6982\u5ff5\u5b9a\u4e49\u65b9\u6cd5\uff0c\u4ee5\u63a2\u7d22AI\u662f\u5426\u80fd\u5728\u65e0\u76d1\u7763\u4e0b\u4ece\u539f\u59cb\u7ecf\u9a8c\u4e2d\u53d1\u73b0\u4eba\u7c7b\u6982\u5ff5\u3002", "method": "\u91c7\u7528\u7b97\u6cd5\u4fe1\u606f\u8bba\u89c6\u89d2\uff0c\u5c06\u6982\u5ff5\u5b9a\u4e49\u4e3a\u901a\u8fc7\u53ef\u9006\u4e00\u81f4\u6027\u5173\u7cfb\u4e0e\u667a\u80fd\u4f53\u7ecf\u9a8c\u7ed3\u6784\u76f8\u5173\u7684\u4fe1\u606f\u5bf9\u8c61\u3002\u63d0\u51fa\"\u5197\u4f59\u4fe1\u606f\"\u5ea6\u91cf\u5206\u89e3\u7684\u81ea\u7136\u6027\uff0c\u5efa\u7acb\u8fa9\u8bc1\u6cd5\u4f18\u5316\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u5f62\u5f0f\u5316\u57fa\u4e8e\u5171\u4eab\u534f\u8bae\u7684\u4f4e\u6210\u672c\u6982\u5ff5\u4f20\u8f93\u548c\u591a\u667a\u80fd\u4f53\u5bf9\u9f50\u673a\u5236\u3002", "result": "\u5efa\u7acb\u4e86\u6982\u5ff5\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u6846\u67b6\uff0c\u4f7f\u6982\u5ff5\u5b58\u5728\u6027\u6210\u4e3a\u53ef\u68c0\u9a8c\u7684\u7ed3\u6784\u6027\u4e3b\u5f20\u3002\u63d0\u51fa\u4e86\u6982\u5ff5\u6f14\u5316\uff08\u6269\u5c55\u3001\u6536\u7f29\u3001\u5206\u88c2\u3001\u5408\u5e76\uff09\u7684\u4f18\u5316\u52a8\u6001\u6a21\u578b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8ba1\u7b97-\u6bd4\u7279\u6743\u8861\u7684\u591a\u667a\u80fd\u4f53\u6982\u5ff5\u5bf9\u9f50\u901a\u4fe1\u534f\u8bae\u3002", "conclusion": "\u4ece\u7b97\u6cd5\u4fe1\u606f\u8bba\u51fa\u53d1\u4e3a\u6982\u5ff5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u4f7f\u6982\u5ff5\u53d1\u73b0\u3001\u6f14\u5316\u548c\u591a\u667a\u80fd\u4f53\u5bf9\u9f50\u6210\u4e3a\u53ef\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u4e3aAI\u65e0\u76d1\u7763\u6982\u5ff5\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.17352", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.17352", "abs": "https://arxiv.org/abs/2512.17352", "authors": ["Ivan Kralj", "Lodovico Giaretta", "Gordan Je\u017ei\u0107", "Ivana Podnar \u017darko", "\u0160ar\u016bnas Girdzijauskas"], "title": "Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs", "comment": "19 pages, 6 figures, 5 tables, journal", "summary": "Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u526a\u679d\u7b97\u6cd5\u51cf\u5c11ST-GNN\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u5f15\u5165SEPA\u65b0\u6307\u6807\u8bc4\u4f30\u4ea4\u901a\u4e8b\u4ef6\u9884\u6d4b\u80fd\u529b", "motivation": "ST-GNN\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5904\u7406\u9ad8\u9891\u4f20\u611f\u5668\u6570\u636e\u65f6\uff0c\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8282\u70b9\u90e8\u7f72\u4f1a\u4ea7\u751f\u5927\u91cf\u901a\u4fe1\u5f00\u9500\uff0c\u7279\u522b\u662f\u76f8\u90bb\u8282\u70b9\u95f4\u91cd\u590d\u4f20\u8f93\u91cd\u53e0\u7279\u5f81\u6570\u636e", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u526a\u679d\u7b97\u6cd5\u52a8\u6001\u8fc7\u6ee4\u5197\u4f59\u90bb\u5c45\u7279\u5f81\uff0c\u57fa\u4e8e\u8fd1\u671f\u6a21\u578b\u6027\u80fd\u8c03\u6574\u526a\u679d\u7387\uff1b\u5f15\u5165SEPA\u65b0\u6307\u6807\u4e13\u95e8\u8bc4\u4f30\u4ea4\u901a\u51cf\u901f\u548c\u6062\u590d\u4e8b\u4ef6\u7684\u9884\u6d4b\u80fd\u529b", "result": "\u5728PeMS-BAY\u548cPeMSD7-M\u6570\u636e\u96c6\u4e0a\uff0c\u81ea\u9002\u5e94\u526a\u679d\u7b97\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0cSEPA\u6307\u6807\u80fd\u6709\u6548\u63ed\u793a\u7a7a\u95f4\u8fde\u63a5\u6027\u5bf9\u52a8\u6001\u4ea4\u901a\u9884\u6d4b\u7684\u771f\u6b63\u4ef7\u503c", "conclusion": "\u81ea\u9002\u5e94\u526a\u679d\u7b97\u6cd5\u53ef\u5728\u4e0d\u635f\u5bb3\u5173\u952e\u4ea4\u901a\u4e8b\u4ef6\u54cd\u5e94\u80fd\u529b\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0cSEPA\u6307\u6807\u6bd4\u6807\u51c6\u8bef\u5dee\u6307\u6807\u66f4\u80fd\u8bc4\u4f30\u4ea4\u901a\u4e8b\u4ef6\u9884\u6d4b\u6027\u80fd"}}
{"id": "2512.17531", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17531", "abs": "https://arxiv.org/abs/2512.17531", "authors": ["Salar Beigzad"], "title": "NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks", "comment": "Conference paper, IEEE, 2025", "summary": "The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.", "AI": {"tldr": "CFF\u7b97\u6cd5\u901a\u8fc7\u5c42\u95f4\u534f\u4f5c\u673a\u5236\u6539\u8fdbForward-Forward\u7b97\u6cd5\uff0c\u89e3\u51b3\u5c42\u95f4\u9694\u79bb\u95ee\u9898\uff0c\u63d0\u5347\u6df1\u5c42\u67b6\u6784\u7684\u6536\u655b\u6548\u7387\u548c\u8868\u793a\u534f\u8c03\u6027", "motivation": "\u4f20\u7edfForward-Forward\u7b97\u6cd5\u5b58\u5728\u5c42\u95f4\u9694\u79bb\u95ee\u9898\uff0c\u5404\u5c42\u72ec\u7acb\u4f18\u5316goodness\u51fd\u6570\uff0c\u7f3a\u4e4f\u96c6\u4f53\u5b66\u4e60\u52a8\u6001\uff0c\u9650\u5236\u4e86\u8868\u793a\u534f\u8c03\u548c\u6df1\u5c42\u67b6\u6784\u7684\u6536\u655b\u6548\u7387", "method": "\u63d0\u51faCollaborative Forward-Forward\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u534f\u4f5c\u8303\u5f0f\uff1a\u56fa\u5b9a\u8026\u5408\u7684F-CFF\u548c\u53ef\u5b66\u4e60\u534f\u4f5c\u53c2\u6570\u7684A-CFF\uff0c\u901a\u8fc7\u52a0\u6743\u6240\u6709\u5c42\u7684\u8d21\u732e\u5b9e\u73b0\u534f\u8c03\u7279\u5f81\u5b66\u4e60", "result": "\u5728MNIST\u548cFashion-MNIST\u4e0a\u76f8\u6bd4\u57fa\u7ebfForward-Forward\u5b9e\u73b0\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "\u5c42\u95f4\u534f\u4f5c\u662fForward-Forward\u5b66\u4e60\u7684\u57fa\u672c\u589e\u5f3a\u673a\u5236\uff0c\u5bf9\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u67b6\u6784\u548c\u80fd\u6e90\u53d7\u9650AI\u7cfb\u7edf\u6709\u76f4\u63a5\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.17323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.17323", "abs": "https://arxiv.org/abs/2512.17323", "authors": ["Jiyun Kong", "Jun-Hyuk Kim", "Jong-Seok Lee"], "title": "DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training", "comment": null, "summary": "Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.", "AI": {"tldr": "DESSERT\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e8b\u4ef6\u9a71\u52a8\u5355\u5e27\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u8bad\u7ec3\u5b9e\u73b0\u66f4\u6e05\u6670\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u89c6\u9891\u5e27\u9884\u6d4b", "motivation": "\u4f20\u7edf\u89c6\u9891\u5e27\u9884\u6d4b\u5728\u52a8\u6001\u573a\u666f\u4e2d\u56e0\u7f3a\u4e4f\u4e0b\u4e00\u5e27\u4fe1\u606f\u800c\u5b58\u5728\u9884\u6d4b\u8bef\u5dee\uff0c\u4e8b\u4ef6\u76f8\u673a\u901a\u8fc7\u5f02\u6b65\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u6355\u83b7\u4eae\u5ea6\u53d8\u5316\u53ef\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\u901a\u8fc7\u5149\u6d41\u9884\u6d4b\u548c\u50cf\u7d20\u626d\u66f2\u4f1a\u5f15\u5165\u7a7a\u6d1e\u548c\u6a21\u7cca\u95ee\u9898", "method": "\u63d0\u51faDESSERT\u6846\u67b6\uff1a1\uff09ER-VAE\u5bf9\u9f50\u4e8b\u4ef6\u5e27\u4e0e\u6b8b\u5dee\uff1b2\uff09\u6269\u6563\u6a21\u578b\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u53bb\u566a\u6b8b\u5dee\u6f5c\u5728\u8868\u793a\uff1b3\uff09\u5f15\u5165DLT\u589e\u5f3a\u8bad\u7ec3\u4e0d\u540c\u65f6\u95f4\u957f\u5ea6\u5e27\u6bb5\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e8b\u4ef6\u91cd\u5efa\u3001\u56fe\u50cf\u89c6\u9891\u5e27\u9884\u6d4b\u3001\u4e8b\u4ef6\u89c6\u9891\u5e27\u9884\u6d4b\u548c\u5355\u8fb9\u4e8b\u4ef6\u89c6\u9891\u5e27\u63d2\u503c\u7b49\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u66f4\u6e05\u6670\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u5e27\u5408\u6210", "conclusion": "DESSERT\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u6b8b\u5dee\u8bad\u7ec3\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u9a71\u52a8\u89c6\u9891\u5e27\u9884\u6d4b\u4e2d\u7684\u7a7a\u6d1e\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5e27\u5408\u6210"}}
{"id": "2512.17577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17577", "abs": "https://arxiv.org/abs/2512.17577", "authors": ["Nikolaos Nakis"], "title": "Machine Learning for Static and Single-Event Dynamic Complex Network Analysis", "comment": null, "summary": "The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8e\u9759\u6001\u548c\u5355\u4e8b\u4ef6\u52a8\u6001\u7f51\u7edc\u7684\u56fe\u8868\u793a\u5b66\u4e60\u65b0\u7b97\u6cd5\uff0c\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\uff0c\u521b\u5efa\u7ed3\u6784\u611f\u77e5\u7684\u7f51\u7edc\u8868\u793a\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u5d4c\u5165\u5b66\u4e60\u8fc7\u7a0b", "motivation": "\u5f00\u53d1\u80fd\u591f\u81ea\u7136\u6355\u6349\u7f51\u7edc\u91cd\u8981\u7279\u6027\uff08\u5982\u540c\u8d28\u6027\u3001\u4f20\u9012\u6027\u3001\u5e73\u8861\u7406\u8bba\uff09\u7684\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u521b\u5efa\u7ed3\u6784\u611f\u77e5\u7684\u7f51\u7edc\u8868\u793a\uff0c\u907f\u514d\u542f\u53d1\u5f0f\u548c\u591a\u9636\u6bb5\u5904\u7406\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u7f51\u7edc\u5d4c\u5165\u5b66\u4e60", "method": "\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u5bb6\u65cf\uff0c\u7279\u522b\u662f\u6f5c\u5728\u8ddd\u79bb\u6a21\u578b\uff0c\u5f00\u53d1\u7edf\u4e00\u7684\u56fe\u8868\u793a\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u9759\u6001\u548c\u5355\u4e8b\u4ef6\u52a8\u6001\u7f51\u7edc\uff0c\u751f\u6210\u7ed3\u6784\u611f\u77e5\u7684\u5c42\u6b21\u5316\u7f51\u7edc\u8868\u793a", "result": "\u63d0\u51fa\u4e86\u80fd\u591f\u81ea\u7136\u8868\u8fbe\u7f51\u7edc\u7279\u6027\u3001\u521b\u5efa\u5c42\u6b21\u5316\u7ed3\u6784\u8868\u8fbe\u3001\u793e\u533a\u7279\u5f81\u8bc6\u522b\u3001\u6781\u7aef\u8f6e\u5ed3\u8bc6\u522b\u548c\u65f6\u95f4\u7f51\u7edc\u5f71\u54cd\u52a8\u6001\u91cf\u5316\u7684\u7edf\u4e00\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5", "conclusion": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u7684\u7edf\u4e00\u56fe\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5168\u9762\u8868\u5f81\u7f51\u7edc\u7ed3\u6784\uff0c\u5904\u7406\u591a\u6837\u5316\u7684\u56fe\u5206\u6790\u4efb\u52a1\uff0c\u4e3a\u9759\u6001\u548c\u52a8\u6001\u7f51\u7edc\u63d0\u4f9b\u5f3a\u5927\u7684\u5d4c\u5165\u8868\u793a"}}
{"id": "2512.17629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17629", "abs": "https://arxiv.org/abs/2512.17629", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "title": "SCOPE: Sequential Causal Optimization of Process Interventions", "comment": null, "summary": "Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.", "AI": {"tldr": "SCOPE\u662f\u4e00\u79cd\u65b0\u7684\u89c4\u8303\u6027\u8fc7\u7a0b\u76d1\u63a7\u65b9\u6cd5\uff0c\u4f7f\u7528\u53cd\u5411\u5f52\u7eb3\u548c\u56e0\u679c\u5b66\u4e60\u6765\u63a8\u8350\u5bf9\u9f50\u7684\u987a\u5e8f\u5e72\u9884\uff0c\u76f4\u63a5\u5229\u7528\u89c2\u6d4b\u6570\u636e\u4f18\u5316KPI\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c4\u8303\u6027\u8fc7\u7a0b\u76d1\u63a7\u65b9\u6cd5\u5728\u5904\u7406\u987a\u5e8f\u5e72\u9884\u65f6\u5b58\u5728\u5c40\u9650\uff1a\u8981\u4e48\u53ea\u5173\u6ce8\u5355\u6b21\u5e72\u9884\u51b3\u7b56\uff0c\u8981\u4e48\u5c06\u591a\u6b21\u5e72\u9884\u89c6\u4e3a\u72ec\u7acb\u4e8b\u4ef6\uff0c\u5ffd\u7565\u4e86\u5e72\u9884\u4e4b\u95f4\u7684\u65f6\u95f4\u4ea4\u4e92\u4f5c\u7528\u3002\u90a3\u4e9b\u8003\u8651\u4f9d\u8d56\u5173\u7cfb\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6a21\u62df\u6216\u6570\u636e\u589e\u5f3a\u6765\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u8fd9\u4f1a\u9020\u6210\u73b0\u5b9e\u5dee\u8ddd\u5e76\u5f15\u5165\u504f\u5dee\u3002", "method": "SCOPE\u91c7\u7528\u53cd\u5411\u5f52\u7eb3\u6cd5\u4f30\u8ba1\u6bcf\u4e2a\u5019\u9009\u5e72\u9884\u884c\u52a8\u7684\u6548\u679c\uff0c\u5c06\u5176\u5f71\u54cd\u4ece\u6700\u7ec8\u51b3\u7b56\u70b9\u4f20\u64ad\u56de\u7b2c\u4e00\u4e2a\u51b3\u7b56\u70b9\u3002\u901a\u8fc7\u5229\u7528\u56e0\u679c\u5b66\u4e60\u5668\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u89c2\u6d4b\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u6784\u5efa\u8fc7\u7a0b\u8fd1\u4f3c\u6765\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u548c\u65b0\u7684\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCOPE\u5728\u4f18\u5316\u5173\u952e\u7ee9\u6548\u6307\u6807\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c4\u8303\u6027\u8fc7\u7a0b\u76d1\u63a7\u6280\u672f\u3002\u57fa\u4e8e\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u65b0\u534a\u5408\u6210\u8bbe\u7f6e\u53ef\u4f5c\u4e3a\u672a\u6765\u987a\u5e8f\u89c4\u8303\u6027\u8fc7\u7a0b\u76d1\u63a7\u7814\u7a76\u7684\u53ef\u91cd\u7528\u57fa\u51c6\u3002", "conclusion": "SCOPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89c4\u8303\u6027\u8fc7\u7a0b\u76d1\u63a7\u65b9\u6cd5\uff0c\u80fd\u591f\u5b66\u4e60\u5bf9\u9f50\u7684\u987a\u5e8f\u5e72\u9884\u63a8\u8350\uff0c\u76f4\u63a5\u5229\u7528\u89c2\u6d4b\u6570\u636e\uff0c\u907f\u514d\u4e86\u6a21\u62df\u65b9\u6cd5\u5e26\u6765\u7684\u73b0\u5b9e\u5dee\u8ddd\u548c\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u987a\u5e8f\u5e72\u9884\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.17432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.17432", "abs": "https://arxiv.org/abs/2512.17432", "authors": ["Georgios Simantiris", "Konstantinos Bacharidis", "Apostolos Papanikolaou", "Petros Giannakakis", "Costas Panagiotakis"], "title": "AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments", "comment": "36 pages, 19 figures, 8 tables", "summary": "Accurate flood detection from visual data is a critical step toward improving disaster response and risk assessment, yet datasets for flood segmentation remain scarce due to the challenges of collecting and annotating large-scale imagery. Existing resources are often limited in geographic scope and annotation detail, hindering the development of robust, generalized computer vision methods. To bridge this gap, we introduce AIFloodSense, a comprehensive, publicly available aerial imagery dataset comprising 470 high-resolution images from 230 distinct flood events across 64 countries and six continents. Unlike prior benchmarks, AIFloodSense ensures global diversity and temporal relevance (2022-2024), supporting three complementary tasks: (i) Image Classification with novel sub-tasks for environment type, camera angle, and continent recognition; (ii) Semantic Segmentation providing precise pixel-level masks for flood, sky, and buildings; and (iii) Visual Question Answering (VQA) to enable natural language reasoning for disaster assessment. We establish baseline benchmarks for all tasks using state-of-the-art architectures, demonstrating the dataset's complexity and its value in advancing domain-generalized AI tools for climate resilience.", "AI": {"tldr": "AIFloodSense\uff1a\u4e00\u4e2a\u5305\u542b470\u5f20\u9ad8\u5206\u8fa8\u7387\u822a\u62cd\u56fe\u50cf\u3001\u8986\u76d664\u4e2a\u56fd\u5bb6230\u6b21\u6d2a\u6c34\u4e8b\u4ef6\u7684\u5168\u7403\u6027\u6d2a\u6c34\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u652f\u6301\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u89c6\u89c9\u95ee\u7b54\u4e09\u79cd\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6d2a\u6c34\u68c0\u6d4b\u6570\u636e\u96c6\u5b58\u5728\u5730\u7406\u8303\u56f4\u6709\u9650\u3001\u6807\u6ce8\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u3001\u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u9700\u8981\u66f4\u5168\u9762\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u707e\u5bb3\u54cd\u5e94\u548c\u98ce\u9669\u8bc4\u4f30\u7684\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u4e86AIFloodSense\u6570\u636e\u96c6\uff0c\u5305\u542b2022-2024\u5e74\u671f\u95f4\u6765\u81ea64\u4e2a\u56fd\u5bb6\u3001230\u6b21\u6d2a\u6c34\u4e8b\u4ef6\u7684470\u5f20\u9ad8\u5206\u8fa8\u7387\u822a\u62cd\u56fe\u50cf\u3002\u6570\u636e\u96c6\u652f\u6301\u4e09\u79cd\u4efb\u52a1\uff1a\u56fe\u50cf\u5206\u7c7b\uff08\u73af\u5883\u7c7b\u578b\u3001\u76f8\u673a\u89d2\u5ea6\u3001\u5927\u9646\u8bc6\u522b\uff09\u3001\u8bed\u4e49\u5206\u5272\uff08\u6d2a\u6c34\u3001\u5929\u7a7a\u3001\u5efa\u7b51\u7269\u7684\u50cf\u7d20\u7ea7\u63a9\u7801\uff09\u548c\u89c6\u89c9\u95ee\u7b54\u3002", "result": "\u5efa\u7acb\u4e86\u6240\u6709\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u67b6\u6784\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u548c\u5728\u63a8\u8fdb\u6c14\u5019\u97e7\u6027\u9886\u57df\u901a\u7528AI\u5de5\u5177\u65b9\u9762\u7684\u4ef7\u503c\u3002\u6570\u636e\u96c6\u5177\u6709\u5168\u7403\u591a\u6837\u6027\u548c\u65f6\u95f4\u76f8\u5173\u6027\u3002", "conclusion": "AIFloodSense\u586b\u8865\u4e86\u6d2a\u6c34\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u3001\u516c\u5f00\u53ef\u7528\u7684\u5168\u7403\u6027\u6570\u636e\u96c6\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u901a\u7528\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u7528\u4e8e\u707e\u5bb3\u8bc4\u4f30\u548c\u6c14\u5019\u97e7\u6027\u5efa\u8bbe\u3002"}}
{"id": "2512.17601", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.17601", "abs": "https://arxiv.org/abs/2512.17601", "authors": ["Zhaolin Cai", "Fan Li", "Ziwei Zheng", "Haixia Bi", "Lijun He"], "title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.", "AI": {"tldr": "HeadHunt-VAD\uff1a\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u8bc6\u522bMLLM\u4e2d\u5f02\u5e38\u654f\u611f\u7684\u6ce8\u610f\u529b\u5934\uff0c\u907f\u514d\u6587\u672c\u751f\u6210\u7684\u4fe1\u606f\u635f\u5931\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u514d\u8c03\u4f18\u65b9\u6cd5\u867d\u7136\u524d\u666f\u597d\uff0c\u4f46\u4f9d\u8d56\u6587\u672c\u8f93\u51fa\u4f1a\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u3001\u6b63\u5e38\u6027\u504f\u5dee\u548c\u63d0\u793a\u654f\u611f\u6027\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u5f02\u5e38\u7ebf\u7d22\u3002", "method": "\u63d0\u51faHeadHunt-VAD\u8303\u5f0f\uff0c\u7ed5\u8fc7\u6587\u672c\u751f\u6210\uff0c\u76f4\u63a5\u8bc6\u522b\u51bb\u7ed3MLLM\u4e2d\u5f02\u5e38\u654f\u611f\u7684\u6ce8\u610f\u529b\u5934\u3002\u6838\u5fc3\u662f\u9c81\u68d2\u5934\u8bc6\u522b\u6a21\u5757\uff0c\u901a\u8fc7\u663e\u8457\u6027\u548c\u7a33\u5b9a\u6027\u7684\u591a\u6807\u51c6\u5206\u6790\uff0c\u7b5b\u9009\u51fa\u5bf9\u591a\u6837\u63d0\u793a\u4e00\u81f4\u7684\u5224\u522b\u6027\u7a00\u758f\u5934\u5b50\u96c6\u3002\u8fd9\u4e9b\u4e13\u5bb6\u5934\u7684\u7279\u5f81\u8f93\u5165\u8f7b\u91cf\u7ea7\u5f02\u5e38\u8bc4\u5206\u5668\u548c\u65f6\u5e8f\u5b9a\u4f4d\u5668\u3002", "result": "\u5728\u4e24\u4e2a\u4e3b\u8981VAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHeadHunt-VAD\u5728\u514d\u8c03\u4f18\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5934\u7ea7\u63a2\u6d4b\u5728MLLM\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "MLLM\u4e2d\u7684\u5934\u7ea7\u63a2\u6d4b\u662f\u73b0\u5b9e\u4e16\u754c\u5f02\u5e38\u68c0\u6d4b\u7684\u5f3a\u5927\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0cHeadHunt-VAD\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u6ce8\u610f\u529b\u5934\u7684\u5185\u90e8\u8868\u793a\uff0c\u514b\u670d\u4e86\u6587\u672c\u751f\u6210\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2512.17908", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17908", "abs": "https://arxiv.org/abs/2512.17908", "authors": ["Ananta R. Bhattarai", "Helge Rhodin"], "title": "Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting", "comment": null, "summary": "Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.", "AI": {"tldr": "Re-Depth Anything\uff1a\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408Depth Anything V2\u4e0e\u5927\u89c4\u6a212D\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u89e3\u51b3\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e2d\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5982Depth Anything V2\u5728\u5904\u7406\u4e0e\u8bad\u7ec3\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u6765\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\u3002", "method": "\u63d0\u51fa\u6d4b\u8bd5\u65f6\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u7167\u660e\u9884\u6d4b\u6df1\u5ea6\u56fe\u5e76\u589e\u5f3a\u8f93\u5165\u56fe\u50cf\uff0c\u5229\u7528\u5f62\u72b6\u4ece\u9634\u5f71\u7ebf\u7d22\u5728\u751f\u6210\u5f0f\u73af\u5883\u4e2d\u8fdb\u884c\u91cd\u5408\u6210\u3002\u91c7\u7528\u76ee\u6807\u4f18\u5316\u7b56\u7565\uff1a\u51bb\u7ed3\u7f16\u7801\u5668\uff0c\u4ec5\u66f4\u65b0\u4e2d\u95f4\u5d4c\u5165\u5e76\u5fae\u8c03\u89e3\u7801\u5668\uff0c\u907f\u514d\u4f18\u5316\u5d29\u6e83\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRe-Depth Anything\u76f8\u6bd4Depth Anything V2\u5728\u6df1\u5ea6\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u589e\u5f3a\u51e0\u4f55\u63a8\u7406\u5b9e\u73b0\u81ea\u76d1\u7763\u7684\u65b0\u9014\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u878d\u5408\u4e86\u6df1\u5ea6\u4f30\u8ba1\u57fa\u7840\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u5148\u9a8c\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d4b\u8bd5\u65f6\u81ea\u76d1\u7763\u6846\u67b6\u6709\u6548\u7f29\u5c0f\u4e86\u9886\u57df\u5dee\u8ddd\uff0c\u4e3a\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u76d1\u7763\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.17724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.17724", "abs": "https://arxiv.org/abs/2512.17724", "authors": ["Shaoyan Zhai", "Mohamed Abdel-Aty", "Chenzhu Wang", "Rodrigo Vena Garcia"], "title": "SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses", "comment": null, "summary": "The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.", "AI": {"tldr": "SAVeD\u662f\u4e00\u4e2a\u4ece\u793e\u4ea4\u5a92\u4f53\u6536\u96c6\u7684\u5927\u89c4\u6a21ADAS\u8f66\u8f86\u4e8b\u6545\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b\u78b0\u649e\u3001\u9669\u60c5\u548c\u7cfb\u7edf\u5931\u6548\u4e8b\u4ef6\uff0c\u7528\u4e8e\u5b89\u5168\u5173\u952e\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u57fa\u4e8e\u6a21\u62df\u73af\u5883\u6216\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff0c\u7f3a\u4e4f\u771f\u5b9eADAS\u8f66\u8f86\u5728\u98ce\u9669\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b89\u5168\u5173\u952e\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u4ece\u793e\u4ea4\u5a92\u4f53\u6536\u96c62,119\u4e2a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\uff0c\u8fdb\u884c\u5e27\u7ea7\u6807\u6ce8\uff0c\u5e76\u63d0\u51faTTC\u8ba1\u7b97\u6846\u67b6\u3001GEV\u98ce\u9669\u91cf\u5316\u65b9\u6cd5\u548cVLLM\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u591a\u6837\u573a\u666f\u7684ADAS\u8f66\u8f86\u4e8b\u6545\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86TTC\u8ba1\u7b97\u548c\u98ce\u9669\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u8be6\u7ec6\u6807\u6ce8\u80fd\u663e\u8457\u63d0\u5347VLLM\u5728\u590d\u6742\u9669\u60c5\u573a\u666f\u7684\u6027\u80fd\u3002", "conclusion": "SAVeD\u586b\u8865\u4e86\u771f\u5b9eADAS\u8f66\u8f86\u98ce\u9669\u884c\u4e3a\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u98ce\u9669\u5206\u6790\u548c\u6a21\u578b\u6539\u8fdb\u65b9\u9762\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
