{"id": "2509.04490", "categories": ["cs.CV", "J.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.04490", "abs": "https://arxiv.org/abs/2509.04490", "authors": ["Abel van Elburg", "Konstantinos Gkentsidis", "Mathieu Sarrazin", "Sarah Barendswaard", "Varun Kotian", "Riender Happee"], "title": "Facial Emotion Recognition does not detect feeling unsafe in automated driving", "comment": null, "summary": "Trust and perceived safety play a crucial role in the public acceptance of\nautomated vehicles. To understand perceived risk, an experiment was conducted\nusing a driving simulator under two automated driving styles and optionally\nintroducing a crossing pedestrian. Data was collected from 32 participants,\nconsisting of continuous subjective comfort ratings, motion, webcam footage for\nfacial expression, skin conductance, heart rate, and eye tracking. The\ncontinuous subjective perceived risk ratings showed significant discomfort\nassociated with perceived risk during cornering and braking followed by relief\nor even positive comfort on continuing the ride. The dynamic driving style\ninduced a stronger discomfort as compared to the calm driving style. The\ncrossing pedestrian did not affect discomfort with the calm driving style but\ndoubled the comfort decrement with the dynamic driving style. This illustrates\nthe importance of consequences of critical interactions in risk perception.\nFacial expression was successfully analyzed for 24 participants but most\n(15/24) did not show any detectable facial reaction to the critical event.\nAmong the 9 participants who did, 8 showed a Happy expression, and only 4\nshowed a Surprise expression. Fear was never dominant. This indicates that\nfacial expression recognition is not a reliable method for assessing perceived\nrisk in automated vehicles. To predict perceived risk a neural network model\nwas implemented using vehicle motion and skin conductance. The model correlated\nwell with reported perceived risk, demonstrating its potential for objective\nperceived risk assessment in automated vehicles, reducing subjective bias and\nhighlighting areas for future research.", "AI": {"tldr": "\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5668\u5b9e\u9a8c\u7814\u7a76\u81ea\u52a8\u9a7e\u9a76\u98ce\u9669\u611f\u77e5\uff0c\u53d1\u73b0\u8f66\u8f86\u8fd0\u52a8\u548c\u76ae\u7535\u5bfc\u53ef\u4ee5\u9884\u6d4b\u4e3b\u89c2\u98ce\u9669\u611f\u77e5\uff0c\u800c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\u4e0d\u53ef\u9760", "motivation": "\u7814\u7a76\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u98ce\u9669\u611f\u77e5\u5bf9\u516c\u4f17\u63a5\u53d7\u5ea6\u7684\u91cd\u8981\u5f71\u54cd", "method": "\u4f7f\u752832\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u9a7e\u9a76\u6a21\u62df\u5668\u5b9e\u9a8c\uff0c\u6536\u96c6\u4e3b\u89c2\u8212\u9002\u5ea6\u8bc4\u5206\u3001\u8fd0\u52a8\u6570\u636e\u3001\u9762\u90e8\u8868\u60c5\u3001\u76ae\u7535\u5bfc\u3001\u5fc3\u7387\u548c\u773c\u52a8\u8ddf\u8e2a\u6570\u636e", "result": "\u52a8\u6001\u9a7e\u9a76\u98ce\u683c\u5bfc\u81f4\u66f4\u5f3a\u7684\u4e0d\u8212\u9002\u611f\uff0c\u884c\u4eba\u8de8\u8def\u4e8b\u4ef6\u589e\u52a0\u4e86\u98ce\u9669\u611f\u77e5\uff1b\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\u4e0d\u53ef\u9760\uff0c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u4e3b\u89c2\u98ce\u9669\u611f\u77e5", "conclusion": "\u8f66\u8f86\u8fd0\u52a8\u548c\u76ae\u7535\u5bfc\u6570\u636e\u53ef\u4ee5\u5bf9\u8c61\u5730\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u98ce\u9669\u611f\u77e5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411"}}
{"id": "2509.04602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04602", "abs": "https://arxiv.org/abs/2509.04602", "authors": ["MinJu Jeon", "Si-Woo Kim", "Ye-Chan Kim", "HyunGee Kim", "Dong-Jin Kim"], "title": "Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning", "comment": "Accepted in EMNLP 2025", "summary": "Dense video captioning aims to temporally localize events in video and\ngenerate captions for each event. While recent works propose end-to-end models,\nthey suffer from two limitations: (1) applying timestamp supervision only to\ntext while treating all video frames equally, and (2) retrieving captions from\nfixed-size video chunks, overlooking scene transitions. To address these, we\npropose Sali4Vid, a simple yet effective saliency-aware framework. We introduce\nSaliency-aware Video Reweighting, which converts timestamp annotations into\nsigmoid-based frame importance weights, and Semantic-based Adaptive Caption\nRetrieval, which segments videos by frame similarity to capture scene\ntransitions and improve caption retrieval. Sali4Vid achieves state-of-the-art\nresults on YouCook2 and ViTT, demonstrating the benefit of jointly improving\nvideo weighting and retrieval for dense video captioning", "AI": {"tldr": "Sali4Vid\u662f\u4e00\u4e2a\u7528\u4e8e\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u8457\u6027\u611f\u77e5\u89c6\u9891\u91cd\u52a0\u6743\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u81ea\u9002\u5e94\u5b57\u5e55\u68c0\u7d22\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728YouCook2\u548cViTT\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1)\u53ea\u5bf9\u6587\u672c\u5e94\u7528\u65f6\u95f4\u6233\u76d1\u7763\uff0c\u800c\u5c06\u6240\u6709\u89c6\u9891\u5e27\u540c\u7b49\u5bf9\u5f85\uff1b(2)\u4ece\u56fa\u5b9a\u5927\u5c0f\u7684\u89c6\u9891\u5757\u4e2d\u68c0\u7d22\u5b57\u5e55\uff0c\u5ffd\u7565\u4e86\u573a\u666f\u8f6c\u6362\u3002", "method": "\u63d0\u51fa\u4e86Sali4Vid\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u663e\u8457\u6027\u611f\u77e5\u89c6\u9891\u91cd\u52a0\u6743\uff08\u5c06\u65f6\u95f4\u6233\u6807\u6ce8\u8f6c\u6362\u4e3a\u57fa\u4e8esigmoid\u7684\u5e27\u91cd\u8981\u6027\u6743\u91cd\uff09\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u81ea\u9002\u5e94\u5b57\u5e55\u68c0\u7d22\uff08\u901a\u8fc7\u5e27\u76f8\u4f3c\u6027\u5206\u5272\u89c6\u9891\u4ee5\u6355\u6349\u573a\u666f\u8f6c\u6362\u5e76\u6539\u8fdb\u5b57\u5e55\u68c0\u7d22\uff09\u3002", "result": "\u5728YouCook2\u548cViTT\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u6539\u8fdb\u89c6\u9891\u52a0\u6743\u548c\u68c0\u7d22\u5bf9\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u7684\u6709\u6548\u6027\u3002", "conclusion": "Sali4Vid\u901a\u8fc7\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u663e\u8457\u6027\u611f\u77e5\u6846\u67b6\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.04736", "categories": ["cs.CV", "I.2.10; H.5.2"], "pdf": "https://arxiv.org/pdf/2509.04736", "abs": "https://arxiv.org/abs/2509.04736", "authors": ["Taeyoung Yeon", "Vasco Xu", "Henry Hoffmann", "Karan Ahuja"], "title": "WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches", "comment": "8 pages, 4 figures, ICMI '25 (27th International Conference on\n  Multimodal Interaction), October 13-17, 2025, Canberra, ACT, Australia", "summary": "Despite advances in practical and multimodal fine-grained Human Activity\nRecognition (HAR), a system that runs entirely on smartwatches in unconstrained\nenvironments remains elusive. We present WatchHAR, an audio and inertial-based\nHAR system that operates fully on smartwatches, addressing privacy and latency\nissues associated with external data processing. By optimizing each component\nof the pipeline, WatchHAR achieves compounding performance gains. We introduce\na novel architecture that unifies sensor data preprocessing and inference into\nan end-to-end trainable module, achieving 5x faster processing while\nmaintaining over 90% accuracy across more than 25 activity classes. WatchHAR\noutperforms state-of-the-art models for event detection and activity\nclassification while running directly on the smartwatch, achieving 9.3 ms\nprocessing time for activity event detection and 11.8 ms for multimodal\nactivity classification. This research advances on-device activity recognition,\nrealizing smartwatches' potential as standalone, privacy-aware, and\nminimally-invasive continuous activity tracking devices.", "AI": {"tldr": "WatchHAR\u662f\u4e00\u4e2a\u5b8c\u5168\u5728\u667a\u80fd\u624b\u8868\u4e0a\u8fd0\u884c\u7684\u97f3\u9891\u548c\u60ef\u6027\u4f20\u611f\u5668\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5b9e\u73b0\u4e865\u500d\u5904\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u572825+\u6d3b\u52a8\u7c7b\u522b\u4e0a\u4fdd\u630190%\u4ee5\u4e0a\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u624b\u8868\u5728\u65e0\u7ea6\u675f\u73af\u5883\u4e2d\u5b8c\u5168\u672c\u5730\u5316\u8fd0\u884c\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\u96be\u9898\uff0c\u5904\u7406\u9690\u79c1\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u5f15\u5165\u65b0\u9896\u67b6\u6784\uff0c\u5c06\u4f20\u611f\u5668\u6570\u636e\u9884\u5904\u7406\u548c\u63a8\u7406\u7edf\u4e00\u4e3a\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6a21\u5757\uff0c\u4f18\u5316\u6d41\u6c34\u7ebf\u6bcf\u4e2a\u7ec4\u4ef6\u3002", "result": "\u5904\u7406\u65f6\u95f4\uff1a\u6d3b\u52a8\u4e8b\u4ef6\u68c0\u6d4b9.3\u6beb\u79d2\uff0c\u591a\u6a21\u6001\u6d3b\u52a8\u5206\u7c7b11.8\u6beb\u79d2\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u8bbe\u5907\u7aef\u6d3b\u52a8\u8bc6\u522b\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u624b\u8868\u4f5c\u4e3a\u72ec\u7acb\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u6700\u5c0f\u4fb5\u5165\u6027\u8fde\u7eed\u6d3b\u52a8\u8ddf\u8e2a\u8bbe\u5907\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.05201", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.05201", "abs": "https://arxiv.org/abs/2509.05201", "authors": ["Nariman Niknejad", "Gokul S. Sankar", "Bahare Kiumarsi", "Hamidreza Modares"], "title": "Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers", "comment": null, "summary": "This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u52a8\u6027\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u91c7\u7528\u96c6\u5408\u57fa\u72b6\u6001\u4f30\u8ba1\u548c\u7ebf\u6027\u89c4\u5212\u6765\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u7684\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u56e0\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u7684\u566a\u58f0\u901a\u5e38\u5177\u6709\u504f\u5dee\u3001\u91cd\u5c3e\u7b49\u975e\u9ad8\u65af\u7279\u5f81\uff0c\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5f71\u54cd\u63a7\u5236\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u7ea6\u675f\u96c6\u5408\u5149\u7ebf\u6765\u5f04\u6355\u504f\u5dee\u3001\u91cd\u5c3e\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u6d41\u52a8\u6027MPC\u91cd\u6784\u4e3a\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u91c7\u7528Minkowski-Lyapunov\u6210\u672c\u51fd\u6570\u548c\u677e\u5f1b\u53d8\u91cf\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u901a\u8fc7\u96c6\u5408\u5149\u7ebf\u7684\u692d\u7403\u8fd1\u4f3c\u6c42\u89e3\u6700\u5927\u7a33\u5b9a\u7ec8\u7aef\u96c6\u548c\u53cd\u9988\u589e\u76ca\u3002", "result": "\u5728\u5168\u5411\u8fd0\u52a8\u79fb\u52a8\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u611f\u77e5\u654f\u611f\u6027MPC\u5728\u91cd\u5c3e\u566a\u58f0\u6761\u4ef6\u4e0b\u80fd\u591f\u63d0\u4f9b\u7a33\u5b9a\u51c6\u786e\u7684\u63a7\u5236\u6027\u80fd\uff0c\u5728\u72b6\u6001\u4f30\u8ba1\u9519\u8bef\u8fb9\u754c\u548c\u6574\u4f53\u63a7\u5236\u6027\u80fd\u65b9\u9762\u663e\u8457\u8d85\u8fc7\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u7684\u975e\u9ad8\u65af\u566a\u58f0\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u95ed\u73af\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u63a7\u5236\u7cfb\u7edf\u7684\u5f3a\u9540\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.04921", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04921", "abs": "https://arxiv.org/abs/2509.04921", "authors": ["Yuki Takemoto"], "title": "Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series", "comment": "Patent pending", "summary": "Time series forecasting plays a critical role in decision-making processes\nacross diverse fields including meteorology, traffic, electricity, economics,\nfinance, and so on. Especially, predicting returns on financial instruments is\na challenging problem. Some researchers have proposed time series foundation\nmodels applicable to various forecasting tasks. Simultaneously, based on the\nrecognition that real-world time series exhibit chaotic properties, methods\nhave been developed to artificially generate synthetic chaotic time series,\nconstruct diverse datasets and train models. In this study, we propose a\nmethodology for modeling financial time series by generating artificial chaotic\ntime series and applying resampling techniques to simulate financial time\nseries data, which we then use as training samples. Increasing the resampling\ninterval to extend predictive horizons, we conducted large-scale pre-training\nusing 10 billion training samples for each case. We subsequently created test\ndatasets for multiple timeframes using actual Bitcoin trade data and performed\nzero-shot prediction without re-training the pre-trained model. The results of\nevaluating the profitability of a simple trading strategy based on these\npredictions demonstrated significant performance improvements over\nautocorrelation models. During the large-scale pre-training process, we\nobserved a scaling law-like phenomenon that we can achieve predictive\nperformance at a certain level with extended predictive horizons for chaotic\ntime series by increasing the number of training samples exponentially. If this\nscaling law proves robust and holds true across various chaotic models, it\nsuggests the potential to predict near-future events by investing substantial\ncomputational resources. Future research should focus on further large-scale\ntraining and verifying the applicability of this scaling law to diverse chaotic\nmodels.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u4eba\u5de5\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u548c\u91cd\u91c7\u6837\u6280\u672f\u6765\u6a21\u62df\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528100\u4ebf\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u6bd4\u7279\u5e01\u4ea4\u6613\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u4ea4\u6613\u7b56\u7565\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u81ea\u76f8\u5173\u6a21\u578b\u3002", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u9700\u8981\u9002\u5e94\u5404\u79cd\u9884\u6d4b\u4efb\u52a1\uff0c\u540c\u65f6\u8ba4\u8bc6\u5230\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u5177\u6709\u6df7\u6c8c\u7279\u6027\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u6765\u5efa\u6a21\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u3002", "method": "\u751f\u6210\u4eba\u5de5\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\uff0c\u5e94\u7528\u91cd\u91c7\u6837\u6280\u672f\u6a21\u62df\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u6837\u672c\uff0c\u589e\u52a0\u91cd\u91c7\u6837\u95f4\u9694\u6269\u5c55\u9884\u6d4b\u8303\u56f4\uff0c\u4f7f\u7528100\u4ebf\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5728\u771f\u5b9e\u6bd4\u7279\u5e01\u4ea4\u6613\u6570\u636e\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4b\u3002", "result": "\u57fa\u4e8e\u9884\u6d4b\u7ed3\u679c\u7684\u7b80\u5355\u4ea4\u6613\u7b56\u7565\u5728\u76c8\u5229\u80fd\u529b\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u81ea\u76f8\u5173\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u7c7b\u4f3c\u7f29\u653e\u5b9a\u5f8b\u7684\u73b0\u8c61\u3002", "conclusion": "\u5982\u679c\u8fd9\u79cd\u7f29\u653e\u5b9a\u5f8b\u5728\u5404\u79cd\u6df7\u6c8c\u6a21\u578b\u4e2d\u90fd\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8868\u660e\u901a\u8fc7\u6295\u5165\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u53ef\u4ee5\u9884\u6d4b\u8fd1\u672a\u6765\u4e8b\u4ef6\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u66f4\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u9a8c\u8bc1\u8be5\u7f29\u653e\u5b9a\u5f8b\u5728\u4e0d\u540c\u6df7\u6c8c\u6a21\u578b\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.04848", "categories": ["cs.CV", "physics.bio-ph", "physics.optics", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.04848", "abs": "https://arxiv.org/abs/2509.04848", "authors": ["Enze Ye", "Wei Lin", "Shaochi Ren", "Yakun Liu", "Xiaoping Li", "Hao Wang", "He Sun", "Feng Pan"], "title": "Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations", "comment": "16 pages, 5 figures", "summary": "High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables\nlabel-free, volumetric characterization of individual cells by reconstructing\ntheir refractive index (RI) distributions from multiple viewing angles during\nflow through microfluidic channels. However, current imaging methods assume\nthat cells undergo uniform, single-axis rotation, which require their poses to\nbe known at each frame. This assumption restricts applicability to\nnear-spherical cells and prevents accurate imaging of irregularly shaped cells\nwith complex rotations. As a result, only a subset of the cellular population\ncan be analyzed, limiting the ability of flow-based assays to perform robust\nstatistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction\nframework that leverages the Fourier diffraction theorem and implicit neural\nrepresentations (INRs) for high-throughput flow cytometry tomographic imaging.\nBy jointly optimizing each cell's unknown rotational trajectory and volumetric\nstructure under weak scattering assumptions, OmniFHT supports arbitrary cell\ngeometries and multi-axis rotations. Its continuous representation also allows\naccurate reconstruction from sparsely sampled projections and restricted\nangular coverage, producing high-fidelity results with as few as 10 views or\nonly 120 degrees of angular range. OmniFHT enables, for the first time, in\nsitu, high-throughput tomographic imaging of entire flowing cell populations,\nproviding a scalable and unbiased solution for label-free morphometric analysis\nin flow cytometry platforms.", "AI": {"tldr": "OmniFHT\u662f\u4e00\u79cd\u65e0\u9700\u59ff\u6001\u4fe1\u606f\u76843D\u6298\u5c04\u7387\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u884d\u5c04\u5b9a\u7406\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u901a\u91cf\u6d41\u5f0f\u7ec6\u80de\u672f\u4e2d\u5bf9\u4efb\u610f\u51e0\u4f55\u5f62\u72b6\u548c\u591a\u8f74\u65cb\u8f6c\u7ec6\u80de\u7684\u9ad8\u4fdd\u771f\u6210\u50cf\u3002", "motivation": "\u73b0\u67093D\u5b9a\u91cf\u76f8\u4f4d\u6210\u50cf\u65b9\u6cd5\u5047\u8bbe\u7ec6\u80de\u8fdb\u884c\u5747\u5300\u5355\u8f74\u65cb\u8f6c\u4e14\u9700\u8981\u5df2\u77e5\u6bcf\u5e27\u59ff\u6001\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u5bf9\u8fd1\u7403\u5f62\u7ec6\u80de\u7684\u9002\u7528\u6027\uff0c\u65e0\u6cd5\u51c6\u786e\u6210\u50cf\u5177\u6709\u590d\u6742\u65cb\u8f6c\u7684\u4e0d\u89c4\u5219\u5f62\u72b6\u7ec6\u80de\uff0c\u5bfc\u81f4\u53ea\u80fd\u5206\u6790\u7ec6\u80de\u7fa4\u4f53\u7684\u5b50\u96c6\u3002", "method": "\u91c7\u7528\u5085\u91cc\u53f6\u884d\u5c04\u5b9a\u7406\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INRs)\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6bcf\u4e2a\u7ec6\u80de\u7684\u672a\u77e5\u65cb\u8f6c\u8f68\u8ff9\u548c\u4f53\u79ef\u7ed3\u6784\uff0c\u5728\u5f31\u6563\u5c04\u5047\u8bbe\u4e0b\u652f\u6301\u4efb\u610f\u7ec6\u80de\u51e0\u4f55\u5f62\u72b6\u548c\u591a\u8f74\u65cb\u8f6c\u3002\u8fde\u7eed\u8868\u793a\u5141\u8bb8\u4ece\u7a00\u758f\u91c7\u6837\u6295\u5f71\u548c\u6709\u9650\u89d2\u5ea6\u8986\u76d6\u8fdb\u884c\u51c6\u786e\u91cd\u5efa\u3002", "result": "OmniFHT\u80fd\u591f\u4f7f\u7528\u5c11\u81f310\u4e2a\u89c6\u56fe\u6216\u4ec5120\u5ea6\u89d2\u5ea6\u8303\u56f4\u4ea7\u751f\u9ad8\u4fdd\u771f\u7ed3\u679c\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u6574\u4e2a\u6d41\u52a8\u7ec6\u80de\u7fa4\u4f53\u7684\u539f\u4f4d\u9ad8\u901a\u91cf\u65ad\u5c42\u6210\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d41\u5f0f\u7ec6\u80de\u672f\u5e73\u53f0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u65e0\u504f\u89c1\u7684\u65e0\u6807\u8bb0\u5f62\u6001\u8ba1\u91cf\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u6280\u672f\u7684\u9650\u5236\u3002"}}
{"id": "2509.04485", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04485", "abs": "https://arxiv.org/abs/2509.04485", "authors": ["Chris Sainsbury", "Andreas Karwath"], "title": "ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records", "comment": null, "summary": "We present ASCENDgpt, a transformer-based model specifically designed for\ncardiovascular risk prediction from longitudinal electronic health records\n(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme\nthat maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,\nachieving 99.6\\% consolidation of diagnosis codes while preserving semantic\ninformation. This phenotype mapping contributes to a total vocabulary of 10,442\ntokens - a 77.9\\% reduction when compared with using raw ICD codes directly. We\npretrain ASCENDgpt on sequences derived from 19402 unique individuals using a\nmasked language modeling objective, then fine-tune for time-to-event prediction\nof five cardiovascular outcomes: myocardial infarction (MI), stroke, major\nadverse cardiovascular events (MACE), cardiovascular death, and all-cause\nmortality. Our model achieves excellent discrimination on the held-out test set\nwith an average C-index of 0.816, demonstrating strong performance across all\noutcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,\nall-cause mortality: 0.824). The phenotype-based approach enables clinically\ninterpretable predictions while maintaining computational efficiency. Our work\ndemonstrates the effectiveness of domain-specific tokenization and pretraining\nfor EHR-based risk prediction tasks.", "AI": {"tldr": "ASCENDgpt\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u5fc3\u8840\u7ba1\u98ce\u9669\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8868\u578b\u611f\u77e5\u6807\u8bb0\u5316\u65b9\u6848\u5c0647,155\u4e2a\u539f\u59cbICD\u4ee3\u7801\u6620\u5c04\u5230176\u4e2a\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u8868\u578b\u6807\u8bb0\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4fe1\u606f\u7684\u540c\u65f6\u5b9e\u73b0\u4e8699.6%\u7684\u4ee3\u7801\u538b\u7f29\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u4e2dICD\u4ee3\u7801\u6570\u91cf\u5e9e\u5927\u4e14\u8bed\u4e49\u590d\u6742\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u4fdd\u6301\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u53c8\u80fd\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u5fc3\u8840\u7ba1\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u8868\u578b\u611f\u77e5\u6807\u8bb0\u5316\u65b9\u6848\u538b\u7f29ICD\u4ee3\u7801\uff0c\u4f7f\u7528\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u5bf919,402\u540d\u4e2a\u4f53\u7684\u5e8f\u5217\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9488\u5bf9\u4e94\u79cd\u5fc3\u8840\u7ba1\u7ed3\u5c40\u8fdb\u884c\u65f6\u95f4\u5230\u4e8b\u4ef6\u9884\u6d4b\u7684\u5fae\u8c03\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u533a\u5206\u6027\u80fd\uff0c\u5e73\u5747C-index\u4e3a0.816\uff08\u5fc3\u808c\u6897\u6b7b:0.792\uff0c\u5352\u4e2d:0.824\uff0cMACE:0.800\uff0c\u5fc3\u8840\u7ba1\u6b7b\u4ea1:0.842\uff0c\u5168\u56e0\u6b7b\u4ea1\u7387:0.824\uff09\u3002", "conclusion": "\u57fa\u4e8e\u8868\u578b\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u8bc1\u660e\u4e86\u9886\u57df\u7279\u5b9a\u6807\u8bb0\u5316\u548c\u9884\u8bad\u7ec3\u5728EHR\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.04977", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04977", "abs": "https://arxiv.org/abs/2509.04977", "authors": ["Shuaicheng Niu", "Guohao Chen", "Deyu Chen", "Yifan Zhang", "Jiaxiang Wu", "Zhiquan Wen", "Yaofo Chen", "Peilin Zhao", "Chunyan Miao", "Mingkui Tan"], "title": "Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization", "comment": "25 pages, 27 tables, 14 figures. arXiv admin note: substantial text\n  overlap with arXiv:2302.12400", "summary": "Test-time adaptation (TTA) may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, 3) online imbalanced label distribution shifts. This is often a key\nobstacle preventing existing TTA methods from being deployed in the real world.\nIn this paper, we investigate the unstable reasons and find that the batch norm\nlayer is a crucial factor hindering TTA stability. Conversely, TTA can perform\nmore stably with batch-agnostic norm layers, i.e., group or layer norm.\nHowever, we observe that TTA with group and layer norms does not always succeed\nand still suffers many failure cases, i.e., the model collapses into trivial\nsolutions by assigning the same class label for all samples. By digging into\nthis, we find that, during the collapse process: 1) the model gradients often\nundergo an initial explosion followed by rapid degradation, suggesting that\ncertain noisy test samples with large gradients may disrupt adaptation; and 2)\nthe model representations tend to exhibit high correlations and classification\nbias. To address this, we first propose a sharpness-aware and reliable entropy\nminimization method, called SAR, for stabilizing TTA from two aspects: 1)\nremove partial noisy samples with large gradients, 2) encourage model weights\nto go to a flat minimum so that the model is robust to the remaining noisy\nsamples. Based on SAR, we further introduce SAR^2 to prevent representation\ncollapse with two regularizers: 1) a redundancy regularizer to reduce\ninter-dimensional correlations among centroid-invariant features; and 2) an\ninequity regularizer to maximize the prediction entropy of a prototype\ncentroid, thereby penalizing biased representations toward any specific class.\nPromising results demonstrate that our methods perform more stably over prior\nmethods and are computationally efficient under the above wild test scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAR\u548cSAR^2\u65b9\u6cd5\u89e3\u51b3\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94(TTA)\u5728\u6df7\u5408\u5206\u5e03\u504f\u79fb\u3001\u5c0f\u6279\u91cf\u6570\u636e\u548c\u5728\u7ebf\u4e0d\u5e73\u8861\u6807\u7b7e\u5206\u5e03\u7b49\u573a\u666f\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u611f\u77e5\u548c\u8868\u793a\u6b63\u5219\u5316\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684TTA\u3002", "motivation": "\u73b0\u6709TTA\u65b9\u6cd5\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6df7\u5408\u5206\u5e03\u504f\u79fb\u3001\u5c0f\u6279\u91cf\u6570\u636e\u548c\u5728\u7ebf\u4e0d\u5e73\u8861\u6807\u7b7e\u5206\u5e03\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u751a\u81f3\u5d29\u6e83\u3002\u7814\u7a76\u53d1\u73b0\u6279\u5f52\u4e00\u5316\u5c42\u662f\u963b\u788dTTA\u7a33\u5b9a\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800c\u4f7f\u7528\u7ec4\u5f52\u4e00\u5316\u6216\u5c42\u5f52\u4e00\u5316\u540e\u4ecd\u5b58\u5728\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51faSAR\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u68af\u5ea6\u611f\u77e5\u79fb\u9664\u566a\u58f0\u6837\u672c\uff1b2) \u4f7f\u7528\u5e73\u5766\u6700\u5c0f\u5316\u4f7f\u6a21\u578b\u5bf9\u5269\u4f59\u566a\u58f0\u6837\u672c\u9c81\u68d2\u3002\u8fdb\u4e00\u6b65\u63d0\u51faSAR^2\u65b9\u6cd5\uff1a1) \u5197\u4f59\u6b63\u5219\u5668\u964d\u4f4e\u7279\u5f81\u7ef4\u5ea6\u95f4\u76f8\u5173\u6027\uff1b2) \u4e0d\u516c\u5e73\u6b63\u5219\u5668\u6700\u5927\u5316\u539f\u578b\u8d28\u5fc3\u7684\u9884\u6d4b\u71b5\uff0c\u60e9\u7f5a\u504f\u5411\u7279\u5b9a\u7c7b\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u79cd\u91ce\u751f\u6d4b\u8bd5\u573a\u666f\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "SAR\u548cSAR^2\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86TTA\u5728\u6311\u6218\u6027\u6d4b\u8bd5\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u7b5b\u9009\u548c\u5e73\u5766\u4f18\u5316\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u901a\u8fc7\u8868\u793a\u6b63\u5219\u5316\u4fdd\u6301\u7279\u5f81\u591a\u6837\u6027\u548c\u516c\u5e73\u6027\uff0c\u4e3aTTA\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05037", "abs": "https://arxiv.org/abs/2509.05037", "authors": ["Noorul Wahab", "Ethar Alzaid", "Jiaqi Lv", "Adam Shephard", "Shan E Ahmed Raza"], "title": "MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer", "comment": "6 pages, 1 figure, 2 tables", "summary": "Accurate prediction of time-to-event outcomes is a central challenge in\noncology, with significant implications for treatment planning and patient\nmanagement. In this work, we present MultiSurv, a multimodal deep survival\nmodel utilising DeepHit with a projection layer and inter-modality\ncross-attention, which integrates heterogeneous patient data, including\nclinical, MRI, RNA-seq and whole-slide pathology features. The model is\ndesigned to capture complementary prognostic signals across modalities and\nestimate individualised time-to-biochemical recurrence in prostate cancer and\ntime-to-cancer recurrence in bladder cancer. Our approach was evaluated in the\ncontext of the CHIMERA Grand Challenge, across two of the three provided tasks.\nFor Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed\nframework achieved a concordance index (C-index) of 0.843 on 5-folds\ncross-validation and 0.818 on CHIMERA development set, demonstrating robust\ndiscriminatory ability. For Task 3 (bladder cancer recurrence prediction), the\nmodel obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on\ndevelopment set, highlighting its adaptability and potential for clinical\ntranslation. These results suggest that leveraging multimodal integration with\ndeep survival learning provides a promising pathway toward personalised risk\nstratification in prostate and bladder cancer. Beyond the challenge setting,\nour framework is broadly applicable to survival prediction tasks involving\nheterogeneous biomedical data.", "AI": {"tldr": "MultiSurv\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u751f\u5b58\u6a21\u578b\uff0c\u4f7f\u7528DeepHit\u67b6\u6784\u7ed3\u5408\u6295\u5f71\u5c42\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6574\u5408\u4e34\u5e8a\u3001MRI\u3001RNA-seq\u548c\u75c5\u7406\u5b66\u6570\u636e\uff0c\u7528\u4e8e\u524d\u5217\u817a\u764c\u548c\u8180\u80f1\u764c\u7684\u590d\u53d1\u65f6\u95f4\u9884\u6d4b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u764c\u75c7\u60a3\u8005\u7684\u65f6\u95f4-\u4e8b\u4ef6\u7ed3\u679c\u5bf9\u6cbb\u7597\u89c4\u5212\u548c\u60a3\u8005\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u6574\u5408\u591a\u6a21\u6001\u5f02\u8d28\u6570\u636e\u6765\u6355\u6349\u4e92\u8865\u7684\u9884\u540e\u4fe1\u53f7\u3002", "method": "\u91c7\u7528DeepHit\u751f\u5b58\u6a21\u578b\uff0c\u52a0\u5165\u6295\u5f71\u5c42\u548c\u8de8\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6574\u5408\u4e34\u5e8a\u6570\u636e\u3001MRI\u5f71\u50cf\u3001RNA-seq\u548c\u5168\u5207\u7247\u75c5\u7406\u7279\u5f81\u7b49\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728\u524d\u5217\u817a\u764c\u751f\u5316\u590d\u53d1\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c5\u6298\u4ea4\u53c9\u9a8c\u8bc1C-index\u8fbe\u52300.843\uff0c\u5f00\u53d1\u96c6\u8fbe\u52300.818\uff1b\u5728\u8180\u80f1\u764c\u590d\u53d1\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c5\u6298\u4ea4\u53c9\u9a8c\u8bc1C-index\u4e3a0.662\uff0c\u5f00\u53d1\u96c6\u4e3a0.457\u3002", "conclusion": "\u591a\u6a21\u6001\u6574\u5408\u4e0e\u6df1\u5ea6\u751f\u5b58\u5b66\u4e60\u76f8\u7ed3\u5408\u4e3a\u524d\u5217\u817a\u764c\u548c\u8180\u80f1\u764c\u7684\u4e2a\u6027\u5316\u98ce\u9669\u5206\u5c42\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u8be5\u6846\u67b6\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6d89\u53ca\u5f02\u8d28\u751f\u7269\u533b\u5b66\u6570\u636e\u7684\u751f\u5b58\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2509.05276", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05276", "abs": "https://arxiv.org/abs/2509.05276", "authors": ["Yuqi Pan", "Yupeng Feng", "Jinghao Zhuang", "Siyu Ding", "Zehao Liu", "Bohan Sun", "Yuhong Chou", "Han Xu", "Xuerui Qiu", "Anlin Deng", "Anjie Hu", "Peng Zhou", "Man Yao", "Jibin Wu", "Jian Yang", "Guoliang Sun", "Bo Xu", "Guoqi Li"], "title": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models", "comment": null, "summary": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design.", "AI": {"tldr": "SpikingBrain\u662f\u4e00\u4e2a\u53d7\u5927\u8111\u542f\u53d1\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u7ebf\u6027/\u6df7\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\u3001\u8109\u51b2\u795e\u7ecf\u5143\u548c\u4e13\u7528\u786c\u4ef6\u4f18\u5316\uff0c\u89e3\u51b3\u4e86Transformer\u6a21\u578b\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7ebf\u6027\u5185\u5b58\u589e\u957f\u95ee\u9898\uff0c\u5728\u975eNVIDIA\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u4e3b\u6d41Transformer\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u8bad\u7ec3\u8ba1\u7b97\u968f\u5e8f\u5217\u957f\u5ea6\u4e8c\u6b21\u589e\u957f\u3001\u63a8\u7406\u5185\u5b58\u7ebf\u6027\u589e\u957f\u7684\u6548\u7387\u74f6\u9888\uff0c\u4e14\u5728\u975eNVIDIA\u5e73\u53f0\u4e0a\u6784\u5efa\u5927\u6a21\u578b\u5b58\u5728\u7a33\u5b9a\u6027\u548c\u6548\u7387\u6311\u6218\u3002", "method": "\u91c7\u7528\u7ebf\u6027/\u6df7\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\u548c\u81ea\u9002\u5e94\u8109\u51b2\u795e\u7ecf\u5143\uff1b\u5f00\u53d1\u9ad8\u6548\u7684\u57fa\u4e8e\u8f6c\u6362\u7684\u8bad\u7ec3\u6d41\u6c34\u7ebf\u548c\u4e13\u7528\u8109\u51b2\u7f16\u7801\u6846\u67b6\uff1b\u9488\u5bf9MetaX\u786c\u4ef6\u5b9a\u5236\u8bad\u7ec3\u6846\u67b6\u3001\u7b97\u5b50\u5e93\u548c\u5e76\u884c\u7b56\u7565\u3002", "result": "\u5f00\u53d1\u4e867B\u7ebf\u6027LLM\u548c76B\u6df7\u5408\u7ebf\u6027MoE LLM\uff0c\u4ec5\u7528\u7ea6150B token\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u5373\u8fbe\u5230\u5f00\u6e90Transformer\u57fa\u7ebf\u6027\u80fd\uff0c4M token\u5e8f\u5217\u7684\u9996token\u65f6\u95f4\u52a0\u901f100\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u7a33\u5b9a\uff0c\u6a21\u578bFLOPs\u5229\u7528\u7387\u8fbe23.4%\uff0c\u8109\u51b2\u7a00\u758f\u5ea6\u8fbe69.15%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5927\u8111\u542f\u53d1\u673a\u5236\u5728\u63a8\u52a8\u4e0b\u4e00\u4ee3\u9ad8\u6548\u53ef\u6269\u5c55\u5927\u6a21\u578b\u8bbe\u8ba1\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u5728\u975eNVIDIA\u5e73\u53f0\u4e0a\u5f00\u53d1\u5927\u89c4\u6a21LLM\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.04069", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04069", "abs": "https://arxiv.org/abs/2509.04069", "authors": ["Chengyandan Shen", "Christoffer Sloth"], "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning", "comment": null, "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u52b3\u52a8\u9009\u62e9\u6a21\u5757\u63d0\u4f9b\u68c0\u67e5\u540e\u7684Q\u503c\uff0cDRLR\u6846\u67b6\u51cf\u5c11\u8f85\u52a9\u5b66\u4e60\u9519\u8bef\uff0c\u7ed3\u5408SAC\u7b56\u7565\u907f\u514d\u6b21\u4f18\u5316\uff0c\u5728\u6a21\u62df\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u8f85\u52a9\u5b66\u4e60\u9519\u8bef\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u80fd\u6c10\u5408\u5230\u6b21\u4f18\u5316\u7b56\u7565\u7684\u95ee\u9898", "method": "\u57fa\u4e8eIBRL\u7b97\u6cd5\uff0c\u6539\u8fdb\u52b3\u52a8\u9009\u62e9\u6a21\u5757\u63d0\u4f9b\u68c0\u67e5\u540e\u7684Q\u503c\uff0c\u4f7f\u7528SAC\u4ee3\u66ffTD3\u4f5c\u4e3aRL\u7b56\u7565\uff0c\u5728\u6876\u88c5\u8f7d\u548c\u6253\u5f00\u62bd\u5c49\u7b49\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1", "result": "\u5728\u4e0d\u540c\u72b6\u6001-\u52b3\u52a8\u7ef4\u5ea6\u548c\u793a\u8303\u8d28\u91cf\u7684\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u6210\u529f\u5728\u771f\u5b9e\u8f6e\u5f0f\u88c5\u8f7d\u673a\u4e0a\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "DRLR\u6846\u67b6\u901a\u8fc7\u7ef4\u62a4\u68c0\u67e5\u540e\u7684Q\u503c\u548c\u4f7f\u7528SAC\u7b56\u7565\uff0c\u6709\u6548\u51cf\u5c11\u8f85\u52a9\u5b66\u4e60\u9519\u8bef\u548c\u907f\u514d\u6b21\u4f18\u5316\uff0c\u5728\u5404\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u548c\u53ef\u90e8\u7f72\u6027"}}
