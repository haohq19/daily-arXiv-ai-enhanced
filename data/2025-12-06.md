<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning](https://arxiv.org/abs/2512.04219)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur\\*

Main category: cs.CV

TL;DR: PARSE是一个无监督学习框架，通过多尺度递归预测器从流式视频中学习层次化事件结构，利用预测误差峰值检测事件边界，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 人类自然地将连续经验感知为时间嵌套事件的层次结构（细粒度动作嵌入较粗的日常活动中）。在计算机视觉中复制这种结构需要能够前瞻性和层次性地分割视频的模型。

Method: PARSE框架通过层次化递归预测器学习多尺度事件结构：低层建模短期动态，高层通过基于注意力的反馈整合长期上下文。事件边界自然地从预测误差的瞬态峰值中涌现。

Result: 在Breakfast Actions、50 Salads和Assembly 101三个基准测试中，PARSE在流式方法中达到SOTA性能，在时间对齐（H-GEBD）和结构一致性（TED, hF1）方面媲美离线基线方法。

Conclusion: 不确定性下的预测学习为人类式时间抽象和组合事件理解提供了可扩展的路径，证明了预测学习能够产生与人类事件感知相似的层次化事件结构。

Abstract: Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.

</details>


### [2] [DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision](https://arxiv.org/abs/2512.04314)
*Jiashu Liao,Pietro Liò,Marc de Kamps,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出DisentangleFormer，通过解耦空间和通道维度来改进Vision Transformers，在超光谱图像处理中实现更好的结构-语义依赖建模，并在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 标准Vision Transformers中的自注意力机制将空间和通道维度联合处理，导致表示纠缠，无法独立建模结构依赖和语义依赖。这个问题在超光谱成像中尤为突出，因为不同通道捕获不同的生物物理或生化线索。

Method: 提出DisentangleFormer架构，基于信息论的去相关表示学习原理，采用并行设计实现空间-通道解耦。包含三个核心组件：1) 并行解耦：独立处理空间token和通道token流；2) 压缩token增强器：自适应校准模块动态融合空间和通道流；3) 多尺度FFN：补充全局注意力，捕获细粒度结构-语义依赖。

Result: 在超光谱基准测试（Indian Pine、Pavia University、Houston、BigEarthNet遥感数据集和红外病理数据集）上达到最先进性能。在ImageNet上保持竞争性准确率的同时，计算成本降低17.8% FLOPs。

Conclusion: DisentangleFormer通过空间-通道解耦实现了鲁棒的多通道视觉表示，解决了标准Vision Transformers中的表示纠缠问题，在超光谱成像任务中表现出色，同时计算效率更高。

Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.

</details>


### [3] [StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios](https://arxiv.org/abs/2512.04451)
*Yifei Wang,Zhenkai Li,Tianwen Qian,Huanran Zheng,Zheng Wang,Yuqian Fu,Xiaoling Wang*

Main category: cs.CV

TL;DR: StreamEQA是首个面向具身智能场景的流式视频问答基准，评估模型在具身（感知、交互、规划）和流式（后向、实时、前向推理）两个维度上的能力，包含156个长视频、42个任务和约21K问答对。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能向实际部署发展，需要模型能够持续感知和理解流式视觉输入，维持环境态势感知，理解与周围实体的交互，并根据过去观察、当前上下文和预期未来事件动态规划行动。

Method: 构建StreamEQA基准：1) 具身维度分为感知、交互、规划三个层次；2) 流式维度分为后向、实时、前向推理三种模式；3) 基于156个独立长视频，通过自动化生成和人工精炼的混合流程创建约21K带精确时间戳的问答对。

Result: 评估13个最先进的视频LLM发现，尽管在传统基准上表现良好，但这些模型在具身场景的流式视频理解方面仍然存在困难。

Conclusion: StreamEQA将推动具身应用中的流式视频理解研究，填补了现有基准在评估模型持续感知和推理能力方面的空白。

Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.

</details>


### [4] [SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding](https://arxiv.org/abs/2512.04643)
*Chang-Hsun Wu,Kai-Po Chang,Yu-Yang Sheng,Hung-Kai Chung,Kuei-Chun Wang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: SEASON是一种无需训练的方法，通过自适应对比解码增强VideoLLMs的时空一致性，减少视频理解中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs在处理视频时难以有效感知和利用丰富的时序信息，导致生成的事件描述存在时序不一致或因果关系不合理的问题，产生严重的幻觉问题。虽然先前研究主要关注空间幻觉，但时序推理在视频理解中仍相对未被充分探索。

Method: 提出Self-Diagnostic Contrastive Decoding (SEASON)，这是一种无需训练的方法，通过动态诊断每个输出token的幻觉倾向，并对其对应的时空负样本应用自适应对比解码，从而自适应地增强每个输出token的时空忠实度。

Result: 在三个幻觉检测基准测试中，SEASON优于所有现有的无需训练幻觉缓解方法，同时在四个通用视频理解基准测试中进一步提升了VideoLLMs的性能。

Conclusion: SEASON通过自适应对比解码有效解决了VideoLLMs中的时空幻觉问题，无需额外训练即可显著提升模型的时空一致性和视频理解能力。

Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.

</details>


### [5] [Controllable Long-term Motion Generation with Extended Joint Targets](https://arxiv.org/abs/2512.04487)
*Eunjong Lee,Eunhee Kim,Sanghoon Hong,Eunho Jung,Jihoon Kim*

Main category: cs.CV

TL;DR: COMET：实时角色动画生成框架，通过自回归Transformer条件VAE实现精确关节控制，引入参考引导反馈机制确保长期稳定性，支持实时风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有实时角色动画方法存在两个主要问题：1）无法提供细粒度控制，2）长序列运动质量会随时间退化，限制了在交互应用中的使用。

Method: 提出COMET自回归框架，采用高效的Transformer条件VAE，支持对任意用户指定关节的精确控制（如目标到达和插值）。引入新颖的参考引导反馈机制防止误差累积，该机制也可作为即插即用风格化模块实现实时风格迁移。

Result: COMET能够以实时速度稳健生成高质量运动，在复杂运动控制任务中显著优于现有最先进方法，证实了其在要求严格的交互应用中的可用性。

Conclusion: COMET解决了实时角色动画中的细粒度控制和长期稳定性问题，通过创新的参考引导反馈机制实现了高质量、可控的实时运动生成，为交互应用提供了实用解决方案。

Abstract: Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.

</details>


### [6] [Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation](https://arxiv.org/abs/2512.04678)
*Yunhong Lu,Yanhong Zeng,Haobo Li,Hao Ouyang,Qiuyu Wang,Ka Leong Cheng,Jiapeng Zhu,Hengyuan Cao,Zhipeng Zhang,Xing Zhu,Yujun Shen,Min Zhang*

Main category: cs.CV

TL;DR: 提出Reward Forcing框架，包含EMA-Sink和Re-DMD两个创新设计，解决视频流生成中初始帧复制和运动动态不足的问题，实现高效高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频流生成方法使用滑动窗口注意力，以初始帧作为sink tokens来维持注意力性能并减少误差累积，但这导致视频帧过度依赖静态token，造成初始帧复制和运动动态减弱的问题。

Method: 1. EMA-Sink：维护固定大小的token，从初始帧初始化，通过指数移动平均融合被淘汰的token来持续更新，在不增加计算成本的情况下捕获长期上下文和近期动态。2. Re-DMD（奖励分布匹配蒸馏）：通过视觉语言模型评估样本动态性，优先处理高奖励样本，将模型输出分布偏向高动态区域，提升运动质量同时保持数据保真度。

Result: Reward Forcing在标准基准测试中达到最先进性能，在单个H100 GPU上实现23.1 FPS的高质量视频流生成，有效防止初始帧复制，显著提升运动动态质量。

Conclusion: Reward Forcing框架通过EMA-Sink和Re-DMD的创新设计，成功解决了视频流生成中的关键问题，实现了高效、高质量的视频生成，为交互式动态世界模拟提供了有效解决方案。

Abstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [SIMA 2: A Generalist Embodied Agent for Virtual Worlds](https://arxiv.org/abs/2512.04797)
*SIMA team,Adrian Bolton,Alexander Lerchner,Alexandra Cordell,Alexandre Moufarek,Andrew Bolt,Andrew Lampinen,Anna Mitenkova,Arne Olav Hallingstad,Bojan Vujatovic,Bonnie Li,Cong Lu,Daan Wierstra,Daniel P. Sawyer,Daniel Slater,David Reichert,Davide Vercelli,Demis Hassabis,Drew A. Hudson,Duncan Williams,Ed Hirst,Fabio Pardo,Felix Hill,Frederic Besse,Hannah Openshaw,Harris Chan,Hubert Soyer,Jane X. Wang,Jeff Clune,John Agapiou,John Reid,Joseph Marino,Junkyung Kim,Karol Gregor,Kaustubh Sridhar,Kay McKinney,Laura Kampis,Lei M. Zhang,Loic Matthey,Luyu Wang,Maria Abi Raad,Maria Loks-Thompson,Martin Engelcke,Matija Kecman,Matthew Jackson,Maxime Gazeau,Ollie Purkiss,Oscar Knagg,Peter Stys,Piermaria Mendolicchio,Raia Hadsell,Rosemary Ke,Ryan Faulkner,Sarah Chakera,Satinder Singh Baveja,Shane Legg,Sheleem Kashem,Tayfun Terzi,Thomas Keck,Tim Harley,Tim Scholtes,Tyson Roberts,Volodymyr Mnih,Yulan Liu,Zhengdong Wang,Zoubin Ghahramani*

Main category: cs.AI

TL;DR: SIMA 2是基于Gemini基础模型构建的通用具身智能体，能够在多种3D虚拟世界中理解和行动，支持复杂语言和图像指令，接近人类表现，并能通过自我改进学习新技能。


<details>
  <summary>Details</summary>
Motivation: 构建能够主动、目标导向地在具身环境中交互的通用智能体，超越之前仅支持简单语言命令的限制，实现与用户的交互式合作。

Method: 基于Gemini基础模型构建，支持语言和图像输入，能够推理高级目标、与用户对话，并通过Gemini生成任务和奖励实现自主学习和技能获取。

Result: 在多种游戏中显著缩小与人类表现的差距，在未见环境中展现强大泛化能力，同时保持基础模型的核心推理能力，并能通过自我改进在新环境中从零学习新技能。

Conclusion: SIMA 2验证了创建多功能、持续学习的智能体的可行路径，为虚拟和未来物理世界的通用具身智能体发展奠定了基础。

Abstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

</details>


### [8] [Sequential Enumeration in Large Language Models](https://arxiv.org/abs/2512.04727)
*Kuinan Hou,Marco Zorzi,Alberto Testolin*

Main category: cs.AI

TL;DR: LLMs在序列计数任务上表现有限，虽然能在明确提示下执行计数，但无法自发采用系统性的计数策略，显示神经与符号系统在组合泛化上的差距。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够像基于规则的符号系统那样可靠地执行序列计数任务，填补关于现代深度学习系统能否对离散符号序列部署系统性计数程序的知识空白。

Method: 评估5个最先进的LLM（包括专有、开源和推理模型）在序列命名和生产任务中的表现，使用不同提示策略探索思维链的作用，分析模型大小对计数能力的影响，并研究嵌入动态。

Result: 部分LLM在明确提示下能够执行计数，但没有模型在被简单要求枚举序列项目数量时会自发进行计数。模型大小增加并未带来计数原则的掌握。

Conclusion: 尽管LLM具有令人印象深刻的涌现能力，但仍无法稳健且系统地部署计数程序，突显了神经方法与符号方法在组合泛化方面的持续差距。

Abstract: Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.

</details>


### [9] [The AI Consumer Index (ACE)](https://arxiv.org/abs/2512.04921)
*Julien Benchek,Rohit Shetty,Benjamin Hunsberger,Ajay Arun,Zach Richards,Brendan Foody,Osvald Nitski,Bertie Vidgen*

Main category: cs.AI

TL;DR: AI Consumer Index (ACE) 基准测试评估前沿AI模型在消费者任务上的表现，结果显示即使最佳模型（GPT 5）也仅达到56.1%的准确率，与消费者需求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估AI模型在消费者日常任务中实际表现的基准，需要了解前沿模型能否满足消费者的真实需求，特别是在购物、饮食、游戏和DIY等关键领域。

Method: 创建包含400个隐藏测试用例的ACE基准，涵盖购物、饮食、游戏、DIY四大领域；开源80个开发集案例；采用新颖的评分方法，动态检查模型回答是否基于检索的网页来源；评估10个前沿模型（开启网页搜索功能）。

Result: GPT 5（思考模式=高）表现最佳，得分56.1%；o3 Pro（思考模式=开）55.2%；GPT 5.1（思考模式=高）55.1%。各领域表现差异明显，购物领域最佳模型得分不足50%。模型在价格和链接等具体信息上容易产生幻觉。

Conclusion: 即使最佳AI模型与消费者实际需求之间仍存在显著差距，特别是在购物等关键领域表现不足，模型在提供准确价格和工作链接方面容易产生幻觉。

Abstract: We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.

</details>


### [10] [Detecting Perspective Shifts in Multi-agent Systems](https://arxiv.org/abs/2512.05013)
*Eric Bridgeford,Hayden Helm*

Main category: cs.AI

TL;DR: 提出TDKPS框架，用于在时间维度上联合嵌入多智能体，并开发统计检验来检测黑盒多智能体系统中的行为变化


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体应用的普及，动态多智能体系统自然涌现，但缺乏监控其行为动态变化的系统化方法

Method: 提出Temporal Data Kernel Perspective Space (TDKPS)，在时间维度上联合嵌入智能体，并开发智能体级别和群体级别的行为变化检测统计检验

Result: 通过模拟实验验证了检验方法的敏感性，并通过自然实验证明该方法能检测到与真实外部事件显著相关的行为变化

Conclusion: TDKPS是首个用于监控黑盒多智能体系统行为动态的原则性框架，对于规模化部署生成式智能体具有关键意义

Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral](https://arxiv.org/abs/2512.04220)
*Wenlong Deng,Yushu Li,Boying Gong,Yi Ren,Christos Thrampoulidis,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 论文分析了GRPO在工具集成RL中的训练崩溃问题，提出了LLD死亡螺旋机制，并设计了轻量级正则化方法LLDS来稳定训练。


<details>
  <summary>Details</summary>
Motivation: GRPO（如Search-R1）在工具集成RL中具有快速收敛和无价值函数等优势，但存在训练崩溃问题。作者旨在探究崩溃的根本原因并提供解决方案。

Method: 识别了LLD（懒惰似然位移）作为GRPO失败的核心机制，提出了LLDS正则化方法，仅在轨迹似然下降时激活，并仅正则化相关token。

Result: 在7个开放域和多跳QA基准测试中，LLDS方法稳定了训练，防止梯度爆炸，显著提升性能（Qwen2.5-3B提升37.8%，Qwen2.5-7B提升32.0%）。

Conclusion: LLD是GRPO基工具集成RL的根本瓶颈，LLDS提供了一条稳定、可扩展的训练路径，为工具集成LLM的实用化奠定了基础。

Abstract: Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.

</details>


### [12] [MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection](https://arxiv.org/abs/2512.04492)
*Yuanshuo Zhang,Aohua Li,Bo Chen,Jingbo Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: MSME：一种用于零样本立场检测的多阶段多专家框架，通过知识准备、专家推理和决策聚合三阶段解决复杂现实场景中的立场理解挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的零样本立场检测方法在复杂现实场景中仍面临挑战：需要动态背景知识、目标定义涉及需要明确链接到立场标签的复合实体或事件、以及讽刺等修辞手法常常掩盖作者真实意图。

Method: 提出MSME多阶段多专家框架：1) 知识准备阶段检索相关背景知识并澄清立场标签；2) 专家推理阶段包含三个专门模块：知识专家从知识角度提炼关键事实和推理，标签专家精炼立场标签并相应推理，语用专家检测讽刺等修辞线索从语用角度推断意图；3) 决策聚合阶段由元法官整合所有专家分析产生最终立场预测。

Result: 在三个公共数据集上的实验表明，MSME在所有数据集上都达到了最先进的性能。

Conclusion: MSME框架通过多阶段多专家方法有效解决了复杂现实场景中的零样本立场检测挑战，显著提升了性能。

Abstract: LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.

</details>


### [13] [UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction](https://arxiv.org/abs/2512.04518)
*Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 该论文介绍了ChemoTimelines共享任务中从癌症患者电子健康记录构建系统抗癌治疗时间线的方法，重点研究了从原始临床笔记生成患者化疗时间线的子任务2，通过多种LLM策略取得了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 从电子健康记录中准确提取化疗时间线对于癌症治疗监测和研究至关重要，但现有方法在从非结构化临床笔记中提取和标准化治疗事件方面存在挑战，需要开发更有效的时间线构建方法。

Method: 采用两步工作流程：1) 使用LLM从单个临床笔记中提取化疗事件；2) 通过算法将事件标准化并聚合成患者级时间线。评估了思维链、监督微调、直接偏好优化和基于字典查找等多种策略，不同方法主要在LLM的使用和训练方式上有所区别。

Result: 多种方法在测试集排行榜上表现出竞争性性能，其中微调的Qwen3-14B模型获得了最佳官方分数0.678。研究结果和分析为未来类似任务提供了有价值的见解。

Conclusion: 该研究证明了使用LLM从临床笔记中提取化疗时间线的可行性，多种策略都能取得良好效果，为医疗时间线提取任务提供了实用的方法框架和设计参考。

Abstract: The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions](https://arxiv.org/abs/2512.04773)
*Giorgos Polychronis,Foivos Pournaropoulos,Christos D. Antonopoulos,Spyros Lalis*

Main category: cs.RO

TL;DR: 无人机在数据驱动任务中需要实时处理数据以决定是否采取后续行动，本文提出基于分支预测和强化学习的机器学习方法，显著优化任务执行时间。


<details>
  <summary>Details</summary>
Motivation: 无人机在数据采集任务中面临两难：如果原地等待处理结果可能浪费时间，如果提前离开而需要返回则增加飞行时间。现有方法无法有效平衡这种权衡。

Method: 提出基于分支预测和强化学习的机器学习方法，根据事件发生概率随时间变化的情况，智能决策无人机是否应该等待处理结果还是立即前往下一个兴趣点。

Result: 提出的方法在多种场景下均优于文献中的回归方法，最坏情况任务时间最多提升4.1倍，中位任务时间仅比完美知识方法高2.7%。

Conclusion: 基于分支预测和强化学习的方法能有效解决无人机在数据驱动任务中的决策问题，显著优化任务执行效率，接近理想情况下的性能。

Abstract: Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.

</details>
