{"id": "2509.02851", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02851", "abs": "https://arxiv.org/abs/2509.02851", "authors": ["Sadra Saremi", "Amirhossein Ahmadkhan Kordbacheh"], "title": "Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach", "comment": null, "summary": "Colon cancer also known as Colorectal cancer, is one of the most malignant\ntypes of cancer worldwide. Early-stage detection of colon cancer is highly\ncrucial to prevent its deterioration. This research presents a hybrid\nmulti-scale deep learning architecture that synergizes capsule networks, graph\nattention mechanisms, transformer modules, and residual learning to advance\ncolon cancer classification on the Lung and Colon Cancer Histopathological\nImage Dataset (LC25000) dataset. The proposed model in this paper utilizes the\nHG-TNet model that introduces a hybrid architecture that joins strength points\nin transformers and convolutional neural networks to capture multi-scale\nfeatures in histopathological images. Mainly, a transformer branch extracts\nglobal contextual bonds by partitioning the image into patches by\nconvolution-based patch embedding and then processing these patches through a\ntransformer encoder. Analogously, a dedicated CNN branch captures fine-grained,\nlocal details through successive Incorporation these diverse features, combined\nwith a self-supervised rotation prediction objective, produce a robust\ndiagnostic representation that surpasses standard architectures in performance.\nResults show better performance not only in accuracy or loss function but also\nin these algorithms by utilizing capsule networks to preserve spatial orders\nand realize how each element individually combines and forms whole structures.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eHG-TNet\u7684\u6df7\u5408\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u80f6\u56ca\u7f51\u7edc\u3001\u56fe\u6ce8\u610f\u529b\u673a\u5236\u3001Transformer\u6a21\u5757\u548c\u6b8b\u5dee\u5b66\u4e60\uff0c\u7528\u4e8e\u7ed3\u80a0\u764c\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\uff0c\u5728LC25000\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7ed3\u80a0\u764c\u662f\u5168\u7403\u6700\u6076\u6027\u7684\u764c\u75c7\u7c7b\u578b\u4e4b\u4e00\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u9884\u9632\u75c5\u60c5\u6076\u5316\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5f00\u53d1\u66f4\u51c6\u786e\u7684\u5206\u7c7b\u6a21\u578b\u6765\u63d0\u5347\u7ed3\u80a0\u764c\u75c5\u7406\u56fe\u50cf\u7684\u8bca\u65ad\u6027\u80fd\u3002", "method": "\u91c7\u7528HG-TNet\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408Transformer\u548cCNN\u7684\u4f18\u52bf\uff1aTransformer\u5206\u652f\u901a\u8fc7\u5377\u79ef\u8865\u4e01\u5d4c\u5165\u63d0\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0cCNN\u5206\u652f\u6355\u83b7\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ec6\u8282\uff0c\u5e76\u96c6\u6210\u80f6\u56ca\u7f51\u7edc\u4fdd\u6301\u7a7a\u95f4\u987a\u5e8f\u5173\u7cfb\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u65cb\u8f6c\u9884\u6d4b\u76ee\u6807\u3002", "result": "\u6a21\u578b\u5728\u51c6\u786e\u7387\u548c\u635f\u5931\u51fd\u6570\u65b9\u9762\u5747\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u80f6\u56ca\u7f51\u7edc\u6709\u6548\u4fdd\u6301\u4e86\u7a7a\u95f4\u987a\u5e8f\u5e76\u7406\u89e3\u4e86\u5404\u5143\u7d20\u5982\u4f55\u7ec4\u5408\u5f62\u6210\u6574\u4f53\u7ed3\u6784\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u7ed3\u80a0\u764c\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u6807\u51c6\u67b6\u6784\uff0c\u4e3a\u7ed3\u80a0\u764c\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2509.02808", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.02808", "abs": "https://arxiv.org/abs/2509.02808", "authors": ["Isaac Ronald Ward", "Mark Paral", "Kristopher Riordan", "Mykel J. Kochenderfer"], "title": "Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers", "comment": "Accepted and awarded best paper at the 11th International Conference\n  on Control, Decision and Information Technologies (CoDIT 2025 -\n  https://codit2025.org/)", "summary": "Autonomously controlling quadrotors in large-scale subterranean environments\nis applicable to many areas such as environmental surveying, mining operations,\nand search and rescue. Learning-based controllers represent an appealing\napproach to autonomy, but are known to not generalize well to\n`out-of-distribution' environments not encountered during training. In this\nwork, we train a normalizing flow-based prior over the environment, which\nprovides a measure of how far out-of-distribution the quadrotor is at any given\ntime. We use this measure as a runtime monitor, allowing us to switch between a\nlearning-based controller and a safe controller when we are sufficiently\nout-of-distribution. Our methods are benchmarked on a point-to-point navigation\ntask in a simulated 3D cave environment based on real-world point cloud data\nfrom the DARPA Subterranean Challenge Final Event Dataset. Our experimental\nresults show that our combined controller simultaneously possesses the liveness\nof the learning-based controller (completing the task quickly) and the safety\nof the safety controller (avoiding collision).", "AI": {"tldr": "\u901a\u8fc7\u6b63\u89c4\u5316\u6d41\u73af\u5883\u524d\u7f6e\u5206\u5e03\u7684\u8fd0\u884c\u76d1\u63a7\u5668\uff0c\u5728\u5730\u4e0b\u73af\u5883\u4e2d\u81ea\u4e3b\u5207\u6362\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\uff0c\u4ee5\u5e73\u8861\u901f\u5ea6\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u5b66\u4e60\u57fa\u4e8e\u63a7\u5236\u5668\u5728\u5927\u89c4\u6a21\u5730\u4e0b\u73af\u5883\u4e2d\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7ecf\u5e38\u65e0\u6cd5\u826f\u597d\u5730\u6cdb\u5316\u5230\u8bad\u7ec3\u5206\u5e03\u5916\u7684\u73af\u5883\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u786e\u4fdd\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u6b63\u89c4\u5316\u6d41\u7684\u73af\u5883\u524d\u7f6e\u5206\u5e03\uff0c\u7528\u4e8e\u8bc4\u4f30\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u5206\u5e03\u504f\u79bb\u7a0b\u5ea6\u3002\u5c06\u8fd9\u4e2a\u8bc4\u4f30\u4f5c\u4e3a\u8fd0\u884c\u65f6\u76d1\u63a7\u5668\uff0c\u5f53\u5206\u5e03\u504f\u79bb\u8d85\u8fc7\u9608\u503c\u65f6\u5207\u6362\u5230\u5b89\u5168\u63a7\u5236\u5668\u3002", "result": "\u5728\u57fa\u4e8eDARPA\u5730\u4e0b\u6311\u6218\u8d5b\u771f\u5b9e\u70b9\u4e91\u6570\u636e\u7684\u6a21\u62df\u6d1e\u7a74\u73af\u5883\u4e2d\uff0c\u7ed3\u5408\u63a7\u5236\u5668\u65e2\u4fdd\u6301\u4e86\u5b66\u4e60\u63a7\u5236\u5668\u7684\u9ad8\u6548\u6027\uff0c\u53c8\u786e\u4fdd\u4e86\u5b89\u5168\u63a7\u5236\u5668\u7684\u907f\u6495\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u73af\u5883\u524d\u7f6e\u5206\u5e03\u7684\u8fd0\u884c\u65f6\u76d1\u63a7\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\u7684\u52a8\u6001\u5207\u6362\uff0c\u5728\u5730\u4e0b\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\u4e2d\u540c\u65f6\u4f53\u73b0\u901f\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2509.03060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03060", "abs": "https://arxiv.org/abs/2509.03060", "authors": ["Md. Jahidul Islam Razin", "Md. Abdul Karim", "M. F. Mridha", "S M Rafiuddin", "Tahira Alam"], "title": "A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network", "comment": "11 pages, 9 figures, 3 tables, published in Sustainable Communication\n  Networks and Application: Proceedings of ICSCN 2020 (2021). Paper presents an\n  LSTM-based business sentiment analysis model with 91.33% accuracy, compares\n  against KNN, SVM, and Naive Bayes, and discusses methodology, dataset,\n  training/testing, results, and implementation tools", "summary": "Business sentiment analysis (BSA) is one of the significant and popular\ntopics of natural language processing. It is one kind of sentiment analysis\ntechniques for business purposes. Different categories of sentiment analysis\ntechniques like lexicon-based techniques and different types of machine\nlearning algorithms are applied for sentiment analysis on different languages\nlike English, Hindi, Spanish, etc. In this paper, long short-term memory (LSTM)\nis applied for business sentiment analysis, where a recurrent neural network is\nused. An LSTM model is used in a modified approach to prevent the vanishing\ngradient problem rather than applying the conventional recurrent neural network\n(RNN). To apply the modified RNN model, product review dataset is used. In this\nexperiment, 70\\% of the data is trained for the LSTM and the rest 30\\% of the\ndata is used for testing. The result of this modified RNN model is compared\nwith other conventional RNN models, and a comparison is made among the results.\nIt is noted that the proposed model performs better than the other conventional\nRNN models. Here, the proposed model, i.e., the modified RNN model approach has\nachieved around 91.33\\% of accuracy. By applying this model, any business\ncompany or e-commerce business site can identify the feedback from their\ncustomers about different types of products that customers like or dislike.\nBased on the customer reviews, a business company or e-commerce platform can\nevaluate its marketing strategy.", "AI": {"tldr": "\u672c\u6587\u5e94\u7528\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc(LSTM)\u8fdb\u884c\u5546\u4e1a\u60c5\u611f\u5206\u6790\uff0c\u901a\u8fc7\u6539\u8fdbRNN\u6a21\u578b\u89e3\u51b3\u6e10\u6d88\u68af\u5ea6\u95ee\u9898\uff0c\u5728\u4ea7\u54c1\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.33%\u7684\u51c6\u786e\u7387\uff0c\u6027\u80fd\u8d85\u8fc7\u4f20\u7edfRNN\u6a21\u578b\u3002", "motivation": "\u5546\u4e1a\u60c5\u611f\u5206\u6790\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u91cd\u8981\u9886\u57df\uff0c\u4f20\u7edfRNN\u6a21\u578b\u5b58\u5728\u6e10\u6d88\u68af\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u63d0\u9ad8\u5546\u4e1a\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u679c\u3002", "method": "\u91c7\u7528LSTM\u7f51\u7edc\u7ed3\u6784\u5bf9\u4f20\u7edfRNN\u8fdb\u884c\u6539\u8fdb\uff0c\u4f7f\u7528\u4ea7\u54c1\u8bc4\u8bba\u6570\u636e\u96c6\uff0c70%\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\uff0c30%\u7528\u4e8e\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u5176\u4ed6\u4f20\u7edfRNN\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u6539\u8fdb\u7684LSTM\u6a21\u578b\u8fbe\u5230\u4e8691.33%\u7684\u51c6\u786e\u7387\uff0c\u8868\u73b0\u8d85\u8fc7\u4e86\u6240\u6709\u4f20\u7edfRNN\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6e10\u6d88\u68af\u5ea6\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdbLSTM\u6a21\u578b\u5728\u5546\u4e1a\u60c5\u611f\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u5e2e\u52a9\u4f01\u4e1a\u5206\u6790\u5ba2\u6237\u8bc4\u4ef7\u3001\u4f18\u5316\u8425\u9500\u7b56\u7565\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.03122", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03122", "abs": "https://arxiv.org/abs/2509.03122", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Yongyi Cui", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "title": "From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models", "comment": "preprint", "summary": "The intellectual property (IP) protection of Large Language Models (LLMs) is\nincreasingly critical. Injecting specialized fingerprints into LLMs through\ninstruction tuning is a common IP protection technique. However, this may\nsignificantly degrade model performance, requires substantial computational\nresources, and exhibits poor persistence under model modifications. We argue\nthat knowledge editing offers a lightweight alternative that is more suitable\nfor fingerprint injection. Accordingly, we apply knowledge editing to\nfingerprint injection for the first time and demonstrate its strong capability.\nDespite using scrambled text as fingerprints to prevent them from being\noverwritten during fine-tuning, degradation still occurs under large-scale\nfine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which reduces fingerprint degradation by constraining the update of the\nfingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even\nin the worst-case scenario. Additionally, we observe that the\nfingerprint-injected models struggle to distinguish between fingerprints and\nsimilar texts due to the high similarity of their features. This finding\nunderscores the urgent need for more robust and fine-grained fingerprinting\ninjection methods for LLMs.", "AI": {"tldr": "\u901a\u8fc7\u77e5\u8bc6\u7f16\u8f91\u6ce8\u5165\u6307\u7eb9\u4fdd\u62a4LLM\u77e5\u8bc6\u4ea7\u6743\uff0c\u63d0\u51faFSFT\u65b9\u6cd5\u51cf\u5c11\u7ec6\u8c03\u65f6\u6307\u7eb9\u9000\u5316\uff0c\u6027\u80fd\u63d0\u534710%", "motivation": "\u4f20\u7edf\u6307\u4ee4\u5faa\u73af\u6ce8\u5165\u6307\u7eb9\u65b9\u6cd5\u5b58\u5728\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3001\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3001\u6307\u7eb9\u6301\u7eed\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848", "method": "\u9996\u6b21\u5e94\u7528\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u8fdb\u884c\u6307\u7eb9\u6ce8\u5165\uff0c\u4f7f\u7528\u7c7b\u4f3c\u52a8\u6001\u7f16\u7801\u7684\u7c7b\u4f3c\u6587\u672c\u4f5c\u4e3a\u6307\u7eb9\uff0c\u5e76\u63d0\u51faFingerprint Subspace-aware Fine-Tuning (FSFT)\u65b9\u6cd5\u6765\u9650\u5236\u6307\u7eb9\u5b50\u7a7a\u95f4\u7684\u66f4\u65b0", "result": "FSFT\u65b9\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8d85\u8fc7\u666e\u901a\u7ec6\u8c03\u65b9\u6cd510%\uff0c\u4f46\u53d1\u73b0\u6ce8\u5165\u6307\u7eb9\u7684\u6a21\u578b\u56e0\u7279\u5f81\u76f8\u4f3c\u5ea6\u9ad8\u800c\u96be\u4ee5\u533a\u5206\u6307\u7eb9\u548c\u76f8\u4f3c\u6587\u672c", "conclusion": "\u77e5\u8bc6\u7f16\u8f91\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684LLM\u6307\u7eb9\u6ce8\u5165\u65b9\u6cd5\uff0cFSFT\u6709\u6548\u51cf\u5c11\u7ec6\u8c03\u65f6\u6307\u7eb9\u9000\u5316\uff0c\u4f46\u4ecd\u9700\u66f4\u7a33\u5065\u548c\u7ec6\u7c92\u5ea6\u7684\u6307\u7eb9\u6ce8\u5165\u65b9\u6cd5"}}
{"id": "2509.03161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03161", "abs": "https://arxiv.org/abs/2509.03161", "authors": ["Rafael Seidi Oyamada", "Jari Peeperkorn", "Jochen De Weerdt", "Johannes De Smedt"], "title": "Domain Adaptation of LLMs for Process Data", "comment": null, "summary": "In recent years, Large Language Models (LLMs) have emerged as a prominent\narea of interest across various research domains, including Process Mining\n(PM). Current applications in PM have predominantly centered on prompt\nengineering strategies or the transformation of event logs into narrative-style\ndatasets, thereby exploiting the semantic capabilities of LLMs to address\ndiverse tasks. In contrast, this study investigates the direct adaptation of\npretrained LLMs to process data without natural language reformulation,\nmotivated by the fact that these models excel in generating sequences of\ntokens, similar to the objective in PM. More specifically, we focus on\nparameter-efficient fine-tuning techniques to mitigate the computational\noverhead typically associated with such models. Our experimental setup focuses\non Predictive Process Monitoring (PPM), and considers both single- and\nmulti-task predictions. The results demonstrate a potential improvement in\npredictive performance over state-of-the-art recurrent neural network (RNN)\napproaches and recent narrative-style-based solutions, particularly in the\nmulti-task setting. Additionally, our fine-tuned models exhibit faster\nconvergence and require significantly less hyperparameter optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u8fc7\u7a0b\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u5728\u9884\u6d4b\u6027\u8fc7\u7a0b\u76d1\u63a7\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8eRNN\u548c\u57fa\u4e8e\u53d9\u4e8b\u98ce\u683c\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63d0\u793a\u5de5\u7a0b\u6216\u5c06\u4e8b\u4ef6\u65e5\u5fd7\u8f6c\u6362\u4e3a\u53d9\u4e8b\u6570\u636e\u96c6\u6765\u5229\u7528LLMs\u7684\u8bed\u4e49\u80fd\u529b\uff0c\u4f46\u672c\u7814\u7a76\u5e0c\u671b\u76f4\u63a5\u5229\u7528LLMs\u751f\u6210token\u5e8f\u5217\u7684\u80fd\u529b\u6765\u5904\u7406\u8fc7\u7a0b\u6570\u636e\uff0c\u907f\u514d\u81ea\u7136\u8bed\u8a00\u91cd\u6784\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u6765\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u4e13\u6ce8\u4e8e\u9884\u6d4b\u6027\u8fc7\u7a0b\u76d1\u63a7\u4efb\u52a1\uff0c\u5305\u62ec\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u9884\u6d4b\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684RNN\u65b9\u6cd5\u548c\u8fd1\u671f\u57fa\u4e8e\u53d9\u4e8b\u98ce\u683c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8868\u73b0\u66f4\u4f73\u3002\u5fae\u8c03\u6a21\u578b\u6536\u655b\u66f4\u5feb\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8d85\u53c2\u6570\u4f18\u5316\u3002", "conclusion": "\u76f4\u63a5\u5fae\u8c03\u9884\u8bad\u7ec3LLMs\u5904\u7406\u8fc7\u7a0b\u6570\u636e\u662f\u53ef\u884c\u7684\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u591a\u4efb\u52a1\u9884\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.03500", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03500", "abs": "https://arxiv.org/abs/2509.03500", "authors": ["Itai Zilberstein", "Alberto Candela", "Steve Chien"], "title": "Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena", "comment": "Appears in Proceedings of 18th Symposium on Advanced Space\n  Technologies in Robotics and Automation", "summary": "Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u8f68\u8ff9\u89c4\u5212\u6d41\u7a0b\uff0c\u5b9e\u73b0\u536b\u661f\u5bf9\u706b\u5c71\u70df\u67f1\u7b49\u52a8\u6001\u73b0\u8c61\u7684\u9ad8\u5206\u8fa8\u7387\u8fdf\u8e2a\u89c2\u6d4b\uff0c\u4f7f\u89c2\u6d4b\u6548\u679c\u63d0\u534710\u500d", "motivation": "\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u80fd\u529b\u5b9e\u73b0\u5bf9\u7a00\u6709\u3001\u77ac\u6001\u3001\u7c92\u5ea6\u7ec6\u5c0f\u7684\u52a8\u6001\u79d1\u5b66\u73b0\u8c61\u7684\u81ea\u4e3b\u68c0\u6d4b\u4e0e\u89c2\u6d4b", "method": "\u7efc\u5408\u4f7f\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\uff0c\u5f00\u53d1\u591a\u79cd\u8f68\u8ff9\u89c4\u5212\u7b97\u6cd5\u8ddf\u8e2a\u70df\u67f1\u5f62\u6001\u7279\u5f81\uff0c\u5e76\u4e0e\u5206\u7c7b\u5668\u96c6\u6210", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0c\u9ad8\u5206\u8fa8\u7387\u4eea\u5668\u7684\u5229\u7528\u6548\u679c\u7c97\u5bf9\u57fa\u7ebf\u63d0\u5347\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u8fd0\u884c\u65f6\u95f4", "conclusion": "\u8be5\u81ea\u52a8\u5316\u6d41\u7a0b\u80fd\u591f\u6709\u6548\u63d0\u5347\u536b\u661f\u5bf9\u52a8\u6001\u73b0\u8c61\u7684\u89c2\u6d4b\u80fd\u529b\uff0c\u4e3a\u706b\u5c71\u76d1\u6d4b\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u624b\u6bb5"}}
{"id": "2509.02920", "categories": ["cs.LG", "cs.CY", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.02920", "abs": "https://arxiv.org/abs/2509.02920", "authors": ["Jaliya L. Wijayaraja", "Janaka L. Wijekoon", "Malitha Wijesundara"], "title": "Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal", "comment": "This article has been accepted for publication in IEEE Access", "summary": "Detecting elephants through seismic signals is an emerging research topic\naimed at developing solutions for Human-Elephant Conflict (HEC). Despite the\npromising results, such solutions heavily rely on manual classification of\nelephant footfalls, which limits their applicability for real-time\nclassification in natural settings. To address this limitation and build on our\nprevious work, this study introduces a classification framework targeting\nresource-constrained implementations, prioritizing both accuracy and\ncomputational efficiency. As part of this framework, a novel event detection\ntechnique named Contextually Customized Windowing (CCW), tailored specifically\nfor detecting elephant footfalls, was introduced, and evaluations were\nconducted by comparing it with the Short-Term Average/Long-Term Average\n(STA/LTA) method. The yielded results show that the maximum validated detection\nrange was 155.6 m in controlled conditions and 140 m in natural environments.\nElephant footfall classification using Support Vector Machine (SVM) with a\nRadial Basis Function (RBF) kernel demonstrated superior performance across\nmultiple settings, achieving an accuracy of 99% in controlled environments, 73%\nin natural elephant habitats, and 70% in HEC-prone human habitats, the most\nchallenging scenario. Furthermore, feature impact analysis using explainable AI\nidentified the number of Zero Crossings and Dynamic Time Warping (DTW)\nAlignment Cost as the most influential factors in all experiments, while\nPredominant Frequency exhibited significant influence in controlled settings.", "AI": {"tldr": "\u901a\u8fc7\u5730\u9707\u4fe1\u53f7\u68c0\u6d4b\u5927\u8c61\u811a\u6b65\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7a97\u53e3\u6280\u672f\u548cSVM\u5206\u7c7b\u5668\uff0c\u5728\u81ea\u7136\u73af\u5883\u4e2d\u8fbe\u5230140\u7c73\u68c0\u6d4b\u8303\u56f4\u548c70-99%\u7684\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u4eba\u8c61\u51b2\u7a81\u95ee\u9898\uff0c\u514b\u670d\u4f9d\u8d56\u4eba\u5de5\u5206\u7c7b\u7684\u9650\u5236\uff0c\u5f00\u53d1\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u7684\u5b9e\u65f6\u5927\u8c61\u811a\u6b65\u68c0\u6d4b\u65b9\u6848", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7a97\u53e3(CCW)\u4e8b\u4ef6\u68c0\u6d4b\u6280\u672f\uff0c\u4f7f\u7528\u652f\u6301\u5411\u91cf\u673a(SVM)\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e0eSTA/LTA\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30", "result": "\u68c0\u6d4b\u8303\u56f4\u63a7\u5236\u73af\u5883155.6\u7c73\uff0c\u81ea\u7136\u73af\u5883140\u7c73\uff1bSVM\u5206\u7c7b\u51c6\u786e\u7387\u63a7\u5236\u73af\u588399%\uff0c\u81ea\u7136\u751f\u5883\u4e2d73%\uff0c\u4eba\u8c61\u51b2\u7a81\u533a\u57df70%\uff1b\u96f6\u70b9\u4ea4\u53c9\u6570\u548cDTW\u5bf9\u9f50\u6210\u672c\u662f\u6700\u91cd\u8981\u7279\u5f81", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5927\u8c61\u811a\u6b65\u68c0\u6d4b\uff0c\u4e3a\u4eba\u8c61\u51b2\u7a81\u89e3\u51b3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6491"}}
{"id": "2509.02923", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02923", "abs": "https://arxiv.org/abs/2509.02923", "authors": ["Kunal Kumar", "Muhammad Ashad Kabir", "Luke Donnan", "Sayed Ahmed"], "title": "A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers", "comment": "44 pages, 2 figures, and 3 tables", "summary": "Offloading footwear helps prevent and treat diabetic foot ulcers (DFUs) by\nlowering plantar pressure (PP), yet prescription decisions remain fragmented:\nfeature selection varies, personalization is limited, and evaluation practices\ndiffer. We performed a narrative review of 45 studies (12 guidelines/protocols,\n25 knowledge-based systems, 8 machine-learning applications) published to Aug\n2025. We thematically analyzed knowledge type, decision logic, evaluation\nmethods, and enabling technologies. Guidelines emphasize PP thresholds (<=200\nkPa or >=25--30\\% reduction) but rarely yield actionable, feature-level\noutputs. Knowledge-based systems use rule- and sensor-driven logic, integrating\nPP monitoring, adherence tracking, and usability testing. ML work introduces\npredictive, optimization, and generative models with high computational\naccuracy but limited explainability and clinical validation. Evaluation remains\nfragmented: protocols prioritize biomechanical tests; knowledge-based systems\nassess usability/adherence; ML studies focus on technical accuracy with weak\nlinkage to long-term outcomes. From this synthesis we propose a five-part CDSS\nframework: (1) a minimum viable dataset; (2) a hybrid architecture combining\nrules, optimization, and explainable ML; (3) structured feature-level outputs;\n(4) continuous validation and evaluation; and (5) integration with clinical and\ntelehealth workflows. This framework aims to enable scalable, patient-centered\nCDSSs for DFU care; prioritizing interoperable datasets, explainable models,\nand outcome-focused evaluation will be key to clinical adoption.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u56de\u987e45\u7bc7\u5173\u4e8e\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u51cf\u538b\u978b\u5177\u5904\u65b9\u51b3\u7b56\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u90e8\u5206\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u51b3\u7b56\u788e\u7247\u5316\u3001\u4e2a\u6027\u5316\u6709\u9650\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u7edf\u4e00\u7684\u95ee\u9898\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u51cf\u538b\u978b\u5177\u7684\u5904\u65b9\u51b3\u7b56\u5b58\u5728\u7279\u5f81\u9009\u62e9\u4e0d\u4e00\u81f4\u3001\u4e2a\u6027\u5316\u7a0b\u5ea6\u6709\u9650\u3001\u8bc4\u4f30\u65b9\u6cd5\u788e\u7247\u5316\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u6846\u67b6\u6765\u6539\u5584\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u5bf9\u622a\u81f32025\u5e748\u6708\u768445\u9879\u7814\u7a76\u8fdb\u884c\u53d9\u8ff0\u6027\u7efc\u8ff0\uff0c\u5305\u62ec12\u4e2a\u6307\u5357/\u534f\u8bae\u300125\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684\u7cfb\u7edf\u548c8\u4e2a\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u901a\u8fc7\u4e3b\u9898\u5206\u6790\u77e5\u8bc6\u7c7b\u578b\u3001\u51b3\u7b56\u903b\u8f91\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u4f7f\u80fd\u6280\u672f\u3002", "result": "\u53d1\u73b0\u6307\u5357\u5f3a\u8c03\u8db3\u5e95\u538b\u529b\u9608\u503c\u4f46\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u7279\u5f81\u7ea7\u8f93\u51fa\uff1b\u57fa\u4e8e\u77e5\u8bc6\u7684\u7cfb\u7edf\u4f7f\u7528\u89c4\u5219\u548c\u4f20\u611f\u5668\u9a71\u52a8\u903b\u8f91\uff1b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u7cbe\u5ea6\u9ad8\u4f46\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u9a8c\u8bc1\u6709\u9650\uff1b\u8bc4\u4f30\u65b9\u6cd5\u788e\u7247\u5316\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5305\u542b\u6700\u5c0f\u53ef\u884c\u6570\u636e\u96c6\u3001\u6df7\u5408\u67b6\u6784\u3001\u7ed3\u6784\u5316\u7279\u5f81\u7ea7\u8f93\u51fa\u3001\u6301\u7eed\u9a8c\u8bc1\u8bc4\u4f30\u4ee5\u53ca\u4e34\u5e8a\u5de5\u4f5c\u6d41\u6574\u5408\u7684\u4e94\u90e8\u5206CDSS\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u62a4\u7406\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.03108", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03108", "abs": "https://arxiv.org/abs/2509.03108", "authors": ["Shota Iwamatsu", "Koichi Ito", "Takafumi Aoki"], "title": "Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods", "comment": "2025 Asia Pacific Signal and Information Processing Association\n  Annual Summit and Conference (APSIPA ASC)", "summary": "Face recognition systems are robust against environmental changes and noise,\nand thus may be vulnerable to illegal authentication attempts using user face\nphotos, such as spoofing attacks. To prevent such spoofing attacks, it is\ncrucial to discriminate whether the input image is a live user image or a\nspoofed image prior to the face recognition process. Most existing spoofing\nattack detection methods utilize deep learning, which necessitates a\nsubstantial amount of training data. Consequently, if malicious data is\ninjected into a portion of the training dataset, a specific spoofing attack may\nbe erroneously classified as live, leading to false positives.In this paper, we\npropose a novel backdoor poisoning attack method to demonstrate the latent\nthreat of backdoor poisoning within face anti-spoofing detection. The proposed\nmethod enables certain spoofing attacks to bypass detection by embedding\nfeatures extracted from the spoofing attack's face image into a live face image\nwithout inducing any perceptible visual alterations.Through experiments\nconducted on public datasets, we demonstrate that the proposed method\nconstitutes a realistic threat to existing spoofing attack detection systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4eba\u8138\u53cd\u6b3a\u9a97\u68c0\u6d4b\u7cfb\u7edf\u7684\u540e\u95e8\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6d3b\u4f53\u4eba\u8138\u56fe\u50cf\u4e2d\u5d4c\u5165\u6b3a\u9a97\u653b\u51fb\u7279\u5f81\uff0c\u4f7f\u7279\u5b9a\u6b3a\u9a97\u653b\u51fb\u80fd\u591f\u7ed5\u8fc7\u68c0\u6d4b\u800c\u4e0d\u5f15\u8d77\u89c6\u89c9\u53d8\u5316\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u4f7f\u7528\u7528\u6237\u7167\u7247\u7684\u6b3a\u9a97\u653b\u51fb\uff0c\u73b0\u6709\u53cd\u6b3a\u9a97\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5982\u679c\u8bad\u7ec3\u6570\u636e\u88ab\u6076\u610f\u6ce8\u5165\uff0c\u53ef\u80fd\u5bfc\u81f4\u7279\u5b9a\u6b3a\u9a97\u653b\u51fb\u88ab\u8bef\u5206\u7c7b\u4e3a\u6d3b\u4f53\u3002", "method": "\u63d0\u51fa\u540e\u95e8\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u5c06\u6b3a\u9a97\u653b\u51fb\u7684\u4eba\u8138\u56fe\u50cf\u7279\u5f81\u5d4c\u5165\u5230\u6d3b\u4f53\u4eba\u8138\u56fe\u50cf\u4e2d\uff0c\u4e0d\u4ea7\u751f\u53ef\u5bdf\u89c9\u7684\u89c6\u89c9\u53d8\u5316\uff0c\u4ece\u800c\u4f7f\u7279\u5b9a\u6b3a\u9a97\u653b\u51fb\u80fd\u591f\u7ed5\u8fc7\u68c0\u6d4b\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5bf9\u73b0\u6709\u6b3a\u9a97\u653b\u51fb\u68c0\u6d4b\u7cfb\u7edf\u6784\u6210\u73b0\u5b9e\u5a01\u80c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4eba\u8138\u53cd\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u540e\u95e8\u6295\u6bd2\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u9700\u8981\u52a0\u5f3a\u5bf9\u6b64\u7c7b\u653b\u51fb\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2509.03240", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.03240", "abs": "https://arxiv.org/abs/2509.03240", "authors": ["Harald Vilhelm Skat-R\u00f8rdam", "Sneha Das", "Kathrine Sofie Rasmussen", "Nicole Nadine L\u00f8nfeldt", "Line Clemmensen"], "title": "Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric", "comment": "15 pages, 6 figures", "summary": "Accurate evaluation of event detection in time series is essential for\napplications such as stress monitoring with wearable devices, where ground\ntruth is typically annotated as single-point events, even though the underlying\nphenomena are gradual and temporally diffused. Standard metrics like F1 and\npoint-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such\nreal-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)\nthat incorporates temporal tolerance, enabling a more robust assessment of\nevent detection when exact alignment is unrealistic. Empirical analysis in\nthree physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one\nexperimental (ROAD), indicates that F1$_w$ reveals meaningful model performance\npatterns invisible to conventional metrics, while its window size can be\nadapted to domain knowledge to avoid overestimation. We show that the choice of\nevaluation metric strongly influences the interpretation of model performance:\nusing predictions from TimesFM, only our temporally tolerant metrics reveal\nstatistically significant improvements over random and null baselines in the\ntwo in-the-wild use cases. This work addresses key gaps in time series\nevaluation and provides practical guidance for healthcare applications where\nrequirements for temporal precision vary by context.", "AI": {"tldr": "\u63d0\u51fa\u7a97\u53e3\u57fa\u4e8eF1\u6307\u6807(F1$_w$)\uff0c\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u56e0\u5730\u9760\u6807\u7b7e\u4e3a\u5355\u70b9\u4e8b\u4ef6\u800c\u5bfc\u81f4\u7684\u8bc4\u4f30\u504f\u5dee\u95ee\u9898", "motivation": "\u73b0\u6709F1\u548c\u70b9\u8c03\u6574F1\u6307\u6807\u5728\u65f6\u95f4\u5e8f\u5217\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u5b58\u5728\u9650\u5236\uff0c\u7279\u522b\u662f\u5f53\u5730\u9760\u6807\u7b7e\u4e3a\u5355\u70b9\u4e8b\u4ef6\u800c\u5b9e\u9645\u73b0\u8c61\u662f\u6e10\u8fdb\u6027\u65f6\uff0c\u6807\u51c6\u6307\u6807\u5bb9\u6613\u8bef\u5bfc\u6a21\u578b\u6027\u80fd\u8bc4\u4f30", "method": "\u8bbe\u8ba1\u4e86\u7a97\u53e3\u57fa\u4e8eF1\u6307\u6807(F1$_w$)\uff0c\u7ed9\u4e88\u65f6\u95f4\u5bb9\u5bcd\u5ea6\uff0c\u5728\u4e09\u4e2a\u751f\u7406\u6570\u636e\u96c6(ADARP\u3001Wrist Angel\u3001ROAD)\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790", "result": "F1$_w$\u80fd\u591f\u663e\u793a\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u53d1\u73b0\u7684\u6709\u610f\u4e49\u6a21\u578b\u6027\u80fd\u6a21\u5f0f\uff0c\u5728\u91ce\u5916\u5e94\u7528\u573a\u666f\u4e2d\u663e\u793a\u51fa\u7edf\u8ba1\u4e0a\u663e\u8457\u7684\u6539\u8fdb", "conclusion": "\u8bc4\u4f30\u6307\u6807\u7684\u9009\u62e9\u5f3a\u70c8\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u89e3\u91ca\uff0cF1$_w$\u4e3a\u5065\u5eb7\u517b\u62a4\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5065\u58ee\u7684\u65f6\u95f4\u5e8f\u5217\u8bc4\u4f30\u65b9\u6cd5"}}
{"id": "2509.03260", "categories": ["cs.LG", "cs.AI", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.03260", "abs": "https://arxiv.org/abs/2509.03260", "authors": ["Minjung Park", "Gyuyeon Na", "Soyoun Kim", "Sunyoung Moon", "HyeonJeong Cha", "Sangmi Chai"], "title": "HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling", "comment": null, "summary": "Abnormal cryptocurrency transactions - such as mixing services, fraudulent\ntransfers, and pump-and-dump operations -- pose escalating risks to financial\nintegrity but remain notoriously difficult to detect due to class imbalance,\ntemporal volatility, and complex network dependencies. Existing approaches are\npredominantly model-centric and post hoc, flagging anomalies only after they\noccur and thus offering limited preventive value. This paper introduces\nHyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a\ndata-driven early-warning framework that explicitly incorporates lead time into\nanomaly detection. Unlike prior methods, HyPV-LEAD integrates three\ninnovations: (1) window-horizon modeling to guarantee actionable lead-time\nalerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while\npreserving temporal continuity, and (3) hyperbolic embedding to capture the\nhierarchical and scale-free properties of blockchain transaction networks.\nEmpirical evaluation on large-scale Bitcoin transaction data demonstrates that\nHyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a\nPR-AUC of 0.9624 with significant gains in precision and recall. Ablation\nstudies further confirm that each component - PV sampling, hyperbolic\nembedding, and structural-temporal modeling - provides complementary benefits,\nwith the full framework delivering the highest performance. By shifting anomaly\ndetection from reactive classification to proactive early-warning, HyPV-LEAD\nestablishes a robust foundation for real-time risk management, anti-money\nlaundering (AML) compliance, and financial security in dynamic blockchain\nenvironments.", "AI": {"tldr": "HyPV-LEAD\u662f\u4e00\u4e2a\u7528\u4e8e\u52a0\u5bc6\u8d27\u5e01\u5f02\u5e38\u4ea4\u6613\u68c0\u6d4b\u7684\u8d85\u65e9\u671f\u9884\u8b66\u6846\u67b6\uff0c\u901a\u8fc7\u7a97\u53e3-\u65f6\u95f4\u5efa\u6a21\u3001\u5cf0\u503c-\u8c37\u503c\u91c7\u6837\u548c\u53cc\u66f2\u5d4c\u5165\u6280\u672f\uff0c\u5728\u6bd4\u7279\u5e01\u4ea4\u6613\u6570\u636e\u4e0a\u5b9e\u73b0\u4e860.9624\u7684PR-AUC\u6027\u80fd\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5f02\u5e38\u4ea4\u6613\uff08\u5982\u6df7\u5e01\u670d\u52a1\u3001\u6b3a\u8bc8\u8f6c\u8d26\u548c\u62c9\u76d8\u7838\u76d8\uff09\u5bf9\u91d1\u878d\u5b8c\u6574\u6027\u6784\u6210\u65e5\u76ca\u589e\u957f\u7684\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u662f\u6a21\u578b\u4e2d\u5fc3\u5316\u548c\u4e8b\u540e\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u9884\u9632\u4ef7\u503c\u3002", "method": "HyPV-LEAD\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a(1)\u7a97\u53e3-\u65f6\u95f4\u5efa\u6a21\u786e\u4fdd\u53ef\u64cd\u4f5c\u7684\u63d0\u524d\u9884\u8b66\uff1b(2)\u5cf0\u503c-\u8c37\u503c\u91c7\u6837\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u8fde\u7eed\u6027\uff1b(3)\u53cc\u66f2\u5d4c\u5165\u6355\u6349\u533a\u5757\u94fe\u4ea4\u6613\u7f51\u7edc\u7684\u5c42\u6b21\u548c\u65e0\u6807\u5ea6\u7279\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6bd4\u7279\u5e01\u4ea4\u6613\u6570\u636e\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cHyPV-LEAD\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e860.9624\u7684PR-AUC\uff0c\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u63d0\u4f9b\u4e92\u8865\u6548\u76ca\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5f02\u5e38\u68c0\u6d4b\u4ece\u88ab\u52a8\u5206\u7c7b\u8f6c\u5411\u4e3b\u52a8\u9884\u8b66\uff0cHyPV-LEAD\u4e3a\u5b9e\u65f6\u98ce\u9669\u7ba1\u7406\u3001\u53cd\u6d17\u94b1\u5408\u89c4\u548c\u52a8\u6001\u533a\u5757\u94fe\u73af\u5883\u4e2d\u7684\u91d1\u878d\u5b89\u5168\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2509.03426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03426", "abs": "https://arxiv.org/abs/2509.03426", "authors": ["AJ Piergiovanni", "Ganesh Satish Mallya", "Dahun Kim", "Anelia Angelova"], "title": "Time-Scaling State-Space Models for Dense Video Captioning", "comment": "BMVC 2025", "summary": "Dense video captioning is a challenging video understanding task which aims\nto simultaneously segment the video into a sequence of meaningful consecutive\nevents and to generate detailed captions to accurately describe each event.\nExisting methods often encounter difficulties when working with the long videos\nassociated with dense video captioning, due to the computational complexity and\nmemory limitations. Furthermore, traditional approaches require the entire\nvideo as input, in order to produce an answer, which precludes online\nprocessing of the video. We address these challenges by time-scaling\nState-Space Models (SSMs) to even longer sequences than before. Our approach,\nState-Space Models with Transfer State, combines both the long-sequence and\nrecurrent properties of SSMs and addresses the main limitation of SSMs which\nare otherwise not able to sustain their state for very long contexts,\neffectively scaling SSMs further in time. The proposed model is particularly\nsuitable for generating captions on-the-fly, in an online or streaming manner,\nwithout having to wait for the full video to be processed, which is more\nbeneficial in practice. When applied to dense video captioning, our approach\nscales well with video lengths and uses 7x fewer FLOPs.", "AI": {"tldr": "\u65f6\u95f4\u7f29\u653e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u6765\u5904\u7406\u957f\u89c6\u9891\u7684\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\uff0c\u901a\u8fc7\u8f6c\u79fb\u72b6\u6001\u6280\u672f\u6269\u5c55SSM\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u652f\u6301\u5728\u7ebf\u6d41\u5f0f\u5904\u7406\u4e14\u8ba1\u7b97\u6548\u7387\u63d0\u53477\u500d", "motivation": "\u73b0\u6709\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u9762\u4e34\u957f\u89c6\u9891\u5904\u7406\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u9650\u5236\uff0c\u4e14\u9700\u8981\u6574\u4e2a\u89c6\u9891\u4f5c\u4e3a\u8f93\u5165\u65e0\u6cd5\u5b9e\u73b0\u5728\u7ebf\u5904\u7406", "method": "\u63d0\u51fa\u5e26\u6709\u8f6c\u79fb\u72b6\u6001\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSMs)\uff0c\u7ed3\u5408SSM\u7684\u957f\u5e8f\u5217\u548c\u9012\u5f52\u7279\u6027\uff0c\u89e3\u51b3SSM\u5728\u5f88\u957f\u4e0a\u4e0b\u6587\u4e2d\u65e0\u6cd5\u7ef4\u6301\u72b6\u6001\u7684\u9650\u5236", "result": "\u65b9\u6cd5\u9002\u5408\u5728\u7ebf\u6d41\u5f0f\u751f\u6210\u63cf\u8ff0\uff0c\u65e0\u9700\u7b49\u5f85\u6574\u4e2a\u89c6\u9891\u5904\u7406\u5b8c\u6210\uff0c\u5728\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u4f7f\u7528\u8ba1\u7b97\u91cf\u51cf\u5c117\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55SSM\u7684\u65f6\u95f4\u7f29\u653e\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u5bc6\u96c6\u63cf\u8ff0\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5728\u7ebf\u5904\u7406\u80fd\u529b"}}
{"id": "2509.03501", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03501", "abs": "https://arxiv.org/abs/2509.03501", "authors": ["Honglu Zhou", "Xiangyu Peng", "Shrikant Kendre", "Michael S. Ryoo", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "title": "Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data", "comment": "This technical report serves as the archival version of our paper\n  accepted at the ICCV 2025 Workshop. For more information, please visit our\n  project website: https://strefer.github.io/", "summary": "Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs.", "AI": {"tldr": "Strefer\u662f\u4e00\u4e2a\u5408\u6210\u6307\u4ee4\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u65f6\u7a7a\u5f15\u7528\u548c\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4f7f\u7528\u4e13\u6709\u6a21\u578b\u6216\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u57fa\u4e8e\u65f6\u95f4\u7684\u4e8b\u4ef6\u5f15\u7528\u548c\u624b\u52bf\u7ebf\u7d22\u7684\u7a7a\u95f4\u951a\u5b9a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u9650\u5236\u4e86\u4e0b\u4e00\u4ee3AI\u4f34\u4fa3\u5728\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u6570\u636e\u5f15\u64ce\u4f2a\u6807\u6ce8\u65f6\u95f4\u5bc6\u96c6\u7684\u7ec6\u7c92\u5ea6\u89c6\u9891\u5143\u6570\u636e\uff0c\u4ee5\u7ed3\u6784\u5316\u65b9\u5f0f\u6355\u83b7\u4e30\u5bcc\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u5305\u62ec\u4e3b\u4f53\u3001\u5bf9\u8c61\u3001\u4f4d\u7f6e\uff08masklets\uff09\u4ee5\u53ca\u52a8\u4f5c\u63cf\u8ff0\u548c\u65f6\u95f4\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u4f7f\u7528Strefer\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u9700\u8981\u65f6\u7a7a\u6d88\u6b67\u7684\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u589e\u5f3a\u7684\u65f6\u7a7a\u611f\u77e5\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Strefer\u4e3a\u611f\u77e5\u57fa\u7840\u3001\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u66f4\u901a\u7528\u3001\u65f6\u7a7a\u611f\u77e5\u7684\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002"}}
