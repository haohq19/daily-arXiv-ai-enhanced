{"id": "2511.10707", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10707", "abs": "https://arxiv.org/abs/2511.10707", "authors": ["Sirui Liang", "Pengfei Cao", "Jian Zhao", "Cong Huang", "Jun Zhao", "Kang Liu"], "title": "Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning", "comment": "accepted by aaai2026", "summary": "Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.", "AI": {"tldr": "\u63d0\u51fa\u4e86BREP ReFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u524d\u7f00\u751f\u6210\u3001\u65e9\u671f\u5e72\u9884\u548c\u7ea6\u675f\u5e72\u9884\u5411\u91cf\u5e45\u5ea6\uff0c\u89e3\u51b3\u4e86ReFT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "ReFT\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u96be\u4ee5\u751f\u6210\u6709\u6548\u7684\u63a8\u7406\u524d\u7f00\uff0c\u4ee5\u53ca\u5e72\u6270\u6570\u503c\u7f16\u7801\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u3002", "method": "BREP ReFT\u901a\u8fc7\u622a\u65ad\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u521d\u59cb\u63a8\u7406\u524d\u7f00\u751f\u6210\uff0c\u5728\u65e9\u671f\u63a8\u7406\u9636\u6bb5\u8fdb\u884c\u5e72\u9884\u9632\u6b62\u9519\u8bef\u7d2f\u79ef\uff0c\u5e76\u7ea6\u675f\u5e72\u9884\u5411\u91cf\u5e45\u5ea6\u907f\u514d\u5e72\u6270\u6570\u503c\u7f16\u7801\u3002", "result": "\u5728\u5404\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBREP\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6807\u51c6ReFT\u548c\u57fa\u4e8e\u6743\u91cd\u7684PEFT\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u6548\u679c\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BREP ReFT\u6709\u6548\u63d0\u5347\u4e86ReFT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.10810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10810", "abs": "https://arxiv.org/abs/2511.10810", "authors": ["Ran Elgedawy", "Sanjay Das", "Ethan Seefried", "Gavin Wiggins", "Ryan Burchfield", "Dana Hewit", "Sudarshan Srinivasan", "Todd Thomas", "Prasanna Balaprakash", "Tirthankar Ghosal"], "title": "HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments", "comment": null, "summary": "Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.", "AI": {"tldr": "HARNESS\u662f\u4e00\u4e2a\u6a21\u5757\u5316AI\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u548c\u5206\u6790\u7f8e\u56fd\u80fd\u6e90\u90e8\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u98ce\u9669\uff0c\u901a\u8fc7\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u3001\u7ed3\u6784\u5316\u5de5\u4f5c\u6570\u636e\u3001\u5386\u53f2\u4e8b\u4ef6\u68c0\u7d22\u548c\u98ce\u9669\u5206\u6790\uff0c\u7ed3\u5408\u4e13\u5bb6\u53c2\u4e0e\u5f62\u6210\u81ea\u9002\u5e94\u5b66\u4e60\u5faa\u73af\u3002", "motivation": "\u5173\u952e\u4efb\u52a1\u5de5\u4f5c\u573a\u6240\u7684\u64cd\u4f5c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5e94\u5bf9\u590d\u6742\u5371\u9669\u7684\u4efb\u52a1\u73af\u5883\uff0c\u5f00\u53d1\u80fd\u591f\u4e3b\u52a8\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u7684\u7cfb\u7edf\u3002", "method": "\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u7ed3\u6784\u5316\u5de5\u4f5c\u6570\u636e\u3001\u5386\u53f2\u4e8b\u4ef6\u68c0\u7d22\u548c\u98ce\u9669\u5206\u6790\uff0c\u91c7\u7528\u4eba\u5728\u56de\u8def\u673a\u5236\u8ba9\u9886\u57df\u4e13\u5bb6\u4f18\u5316\u9884\u6d4b\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u5b66\u4e60\u5faa\u73af\u3002", "result": "\u521d\u6b65\u90e8\u7f72\u663e\u793a\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u5b89\u5168\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "conclusion": "HARNESS\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u534f\u4f5c\u548c\u8fed\u4ee3\u63a8\u7406\uff0c\u6539\u8fdb\u4e86\u9884\u6d4b\u5b89\u5168\u7cfb\u7edf\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u5173\u6ce8\u51c6\u786e\u6027\u3001\u4e13\u5bb6\u4e00\u81f4\u6027\u548c\u51b3\u7b56\u5ef6\u8fdf\u7684\u5b9a\u91cf\u8bc4\u4f30\u3002"}}
{"id": "2511.10866", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10866", "abs": "https://arxiv.org/abs/2511.10866", "authors": ["Seoik Jung", "Taekyung Song", "Yangro Lee", "Sungjun Lee"], "title": "Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling", "comment": "5 pages, 2 figures. Accepted paper for the IEIE (Institute of Electronics and Information Engineers) Fall Conference 2025. Presentation on Nov 27, 2025", "summary": "This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u7a97\u53e3\u6ed1\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eCCTV\u89c6\u9891\u4e2d\u7684\u5b9e\u65f6\u66b4\u529b\u68c0\u6d4b\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u5206\u5272\u4e3a1-2\u79d2\u7247\u6bb5\u5e76\u5e94\u7528LLM\u81ea\u52a8\u6807\u6ce8\u6765\u6784\u5efa\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u957f\u89c6\u9891\u8bad\u7ec3\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u8bc6\u522b\u5feb\u901f\u66b4\u529b\u4e8b\u4ef6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u65f6\u95f4\u8fde\u7eed\u6027\u5e76\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u89c6\u9891\u5206\u5272\u4e3a1-2\u79d2\u77ed\u7247\u6bb5\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5145\u5206\u5229\u7528\u6240\u6709\u5e27\u4fdd\u6301\u65f6\u95f4\u8fde\u7eed\u6027\u3002", "result": "\u5728RWF-2000\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.25%\u51c6\u786e\u7387\uff0c\u5728UCF-Crime\u957f\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fbe\u523083.25%\u51c6\u786e\u7387\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u66b4\u529b\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u3002"}}
{"id": "2511.10853", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10853", "abs": "https://arxiv.org/abs/2511.10853", "authors": ["Gerui Xu", "Boyou Chen", "Huizhong Guo", "Dave LeBlanc", "Ananna Ahmed", "Zhaonan Sun", "Shan Bao"], "title": "Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction", "comment": "26 pages, 10 figures", "summary": "Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u788e\u7247\u5316\u7684\u78b0\u649e\u6570\u636e\u4e2d\u91cd\u5efa\u4e8b\u6545\u524d\u573a\u666f\u5e76\u63a8\u65ad\u8f66\u8f86\u884c\u4e3a\uff0c\u5728\u590d\u6742\u4e8b\u6545\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86100%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u4e13\u5bb6\u768492%\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u4e8b\u6545\u91cd\u5efa\u4f9d\u8d56\u4eba\u5de5\u7ecf\u9a8c\uff0c\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u6570\u636e\u65f6\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u534f\u4f5c\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4ece\u591a\u6a21\u6001\u8f93\u5165\u751f\u6210\u81ea\u7136\u8bed\u8a00\u4e8b\u6545\u91cd\u5efa\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7ed3\u5408\u65f6\u95f4\u4e8b\u4ef6\u6570\u636e\u8bb0\u5f55\u5668\u8fdb\u884c\u6df1\u5ea6\u4e8b\u6545\u63a8\u7406\u3002", "result": "\u572839\u4e2a\u590d\u6742\u8ffd\u5c3e\u4e8b\u6545\u6848\u4f8b\u4e2d\uff0c\u6846\u67b6\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u51c6\u786e\u7387\uff0c\u6210\u529f\u8bc6\u522b\u6700\u76f8\u5173EDR\u4e8b\u4ef6\u5e76\u6b63\u786e\u533a\u5206\u649e\u51fb\u4e0e\u88ab\u649e\u8f66\u8f86\uff0c\u5373\u4f7f\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u6570\u636e\u65f6\u4e5f\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86AI\u5728\u5904\u7406\u5f02\u6784\u78b0\u649e\u6570\u636e\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5728\u91cd\u5efa\u78b0\u649e\u52a8\u529b\u5b66\u548c\u8868\u5f81\u4e8b\u6545\u524d\u884c\u4e3a\u65b9\u9762\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u7cbe\u786e\u5ea6\u3002"}}
{"id": "2511.10857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10857", "abs": "https://arxiv.org/abs/2511.10857", "authors": ["Seyedeh Mobina Noorani", "Shangde Gao", "Changjie Chen", "Karla Saldana Ochoa"], "title": "Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning", "comment": "Accepted by NeurIPS 2025 UrbanAI Workshop as poster", "summary": "Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fdAI\u7684\u89c4\u5212\u652f\u6301\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u9762\u5411\u9700\u6c42\u7684\u52a8\u6001\u89c4\u5212\u5355\u5143\uff0c\u4ee5\u6539\u8fdb\u707e\u5bb3\u89c4\u5212\u3002\u7cfb\u7edf\u7ed3\u5408\u4eba\u7c7b\u53c2\u4e0e\u539f\u5219\uff0c\u901a\u8fc7\u6269\u5c55\u7684\u81ea\u7ec4\u7ec7\u5730\u56fe\u548cAI\u4ee3\u7406\u6765\u6307\u5bfc\u533a\u57df\u5212\u5206\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u7684\u89c4\u5212\u5355\u5143\uff08\u5982\u4eba\u53e3\u666e\u67e5\u533a\u3001\u90ae\u653f\u7f16\u7801\u533a\u7b49\uff09\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5f53\u5730\u793e\u533a\u7684\u7279\u5b9a\u9700\u6c42\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u6765\u5b9e\u65bd\u6709\u6548\u7684\u707e\u5bb3\u9884\u9632\u6216\u5e94\u5bf9\u7b56\u7565\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u8868\u6027\u521d\u59cb\u5316\u7a7a\u95f4\u7ea6\u675f\u81ea\u7ec4\u7ec7\u5730\u56fe\uff08RepSC-SOM\uff09\u7684\u5e73\u53f0\uff0c\u6269\u5c55\u4e86\u4f20\u7edfSOM\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5730\u7406\u8fc7\u6ee4\u548c\u533a\u57df\u589e\u957f\u7ec6\u5316\u3002AI\u4ee3\u7406\u80fd\u591f\u63a8\u7406\u3001\u89c4\u5212\u548c\u884c\u52a8\uff0c\u6307\u5bfc\u8f93\u5165\u7279\u5f81\u9009\u62e9\u3001\u7a7a\u95f4\u7ea6\u675f\u8bbe\u7f6e\u548c\u4ea4\u4e92\u5f0f\u63a2\u7d22\u3002", "result": "\u901a\u8fc7\u4f5b\u7f57\u91cc\u8fbe\u5dde\u6770\u514b\u900a\u7ef4\u5c14\u5e02\u7684\u6d2a\u6c34\u76f8\u5173\u98ce\u9669\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5e73\u53f0\u5141\u8bb8\u7528\u6237\u4ea4\u4e92\u5f0f\u63a2\u7d22\u3001\u751f\u6210\u548c\u8bc4\u4f30\u533a\u57df\u5212\u5206\u7684\u80fd\u529b\uff0c\u5c06\u8ba1\u7b97\u4e25\u8c28\u6027\u4e0e\u7528\u6237\u9a71\u52a8\u51b3\u7b56\u76f8\u7ed3\u5408\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u5730\u5c06\u8ba1\u7b97\u65b9\u6cd5\u548c\u4eba\u7c7b\u51b3\u7b56\u76f8\u7ed3\u5408\uff0c\u4e3a\u707e\u5bb3\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9700\u6c42\u5bfc\u5411\u7684\u52a8\u6001\u89c4\u5212\u5355\u5143\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2511.10831", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.10831", "abs": "https://arxiv.org/abs/2511.10831", "authors": ["Yuhan Jiang", "Matthew Otten"], "title": "Benchmarking Quantum Kernels Across Diverse and Complex Data", "comment": null, "summary": "Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ans\u00e4tze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d8\u5206\u91cf\u5b50\u6838\u6846\u67b6\uff0c\u57288\u4e2a\u9ad8\u7ef4\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u91cf\u5b50\u6838\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u7ecf\u5178RBF\u6838\u7684\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u6838\u65b9\u6cd5\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u4f4e\u7ef4\u6216\u5408\u6210\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5176\u5728\u771f\u5b9e\u4e16\u754c\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u5b9e\u9645\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4f7f\u7528\u8d44\u6e90\u9ad8\u6548ansatz\u7684\u53d8\u5206\u91cf\u5b50\u6838\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u53c2\u6570\u7f29\u653e\u6280\u672f\u6765\u52a0\u901f\u6536\u655b\u3002", "result": "\u57288\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u7684\u7ecf\u5178\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u91cf\u5b50\u6838\u6bd4\u6807\u51c6\u7ecf\u5178\u6838\uff08\u5982RBF\u6838\uff09\u5177\u6709\u660e\u663e\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u9002\u5f53\u8bbe\u8ba1\u7684\u91cf\u5b50\u6838\u53ef\u4ee5\u4f5c\u4e3a\u591a\u529f\u80fd\u3001\u9ad8\u6027\u80fd\u7684\u5de5\u5177\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u91cf\u5b50\u589e\u5f3a\u5e94\u7528\u5960\u5b9a\u57fa\u7840\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u5168\u9762\u8bc4\u4f30\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\u3002"}}
{"id": "2511.10948", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10948", "abs": "https://arxiv.org/abs/2511.10948", "authors": ["Ren Zhang", "Huilai Li", "Chao qi", "Guoliang Xu", "Tianyu Zhou", "Wei wei", "Jianqin Yin"], "title": "DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition", "comment": null, "summary": "Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.", "AI": {"tldr": "\u63d0\u51faDEFT-LLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u89e3\u8026\u5b9e\u73b0\u8fd0\u52a8\u8bed\u4e49\u5bf9\u9f50\uff0c\u89e3\u51b3\u5fae\u8868\u60c5\u8bc6\u522b\u4e2d\u9759\u6001\u5916\u89c2\u4e0e\u52a8\u6001\u8fd0\u52a8\u7ea0\u7f20\u4ee5\u53ca\u6587\u672c\u6807\u7b7e\u4e0e\u9762\u90e8\u808c\u8089\u8fd0\u52a8\u8bed\u4e49\u9e3f\u6c9f\u7684\u95ee\u9898\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\u4e2d\uff0c\u9759\u6001\u5916\u89c2\u4e0e\u52a8\u6001\u8fd0\u52a8\u7ebf\u7d22\u7684\u7ea0\u7f20\u963b\u788d\u6a21\u578b\u5173\u6ce8\u7ec6\u5fae\u8fd0\u52a8\uff0c\u540c\u65f6\u73b0\u6709\u6570\u636e\u96c6\u7684\u6587\u672c\u6807\u7b7e\u4e0e\u9762\u90e8\u808c\u8089\u8fd0\u52a8\u4e0d\u5b8c\u5168\u5bf9\u5e94\uff0c\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u6784\u5efaUni-MER\u8fd0\u52a8\u9a71\u52a8\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5229\u7528\u5149\u6d41\u548c\u52a8\u4f5c\u5355\u5143\u53cc\u91cd\u7ea6\u675f\u786e\u4fdd\u65f6\u7a7a\u4e00\u81f4\u6027\uff1b\u8bbe\u8ba1\u4e09\u4e13\u5bb6\u67b6\u6784\u89e3\u8026\u9762\u90e8\u52a8\u6001\u4e3a\u7ed3\u6784\u3001\u52a8\u6001\u7eb9\u7406\u548c\u8fd0\u52a8\u8bed\u4e49\u7684\u72ec\u7acb\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027MER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5c40\u90e8\u9762\u90e8\u8fd0\u52a8\u7684\u53ef\u89e3\u91ca\u5efa\u6a21\u65b9\u9762\u5177\u6709\u7279\u522b\u4f18\u52bf\u3002", "conclusion": "DEFT-LLM\u901a\u8fc7\u8fd0\u52a8\u8bed\u4e49\u5bf9\u9f50\u548c\u591a\u4e13\u5bb6\u89e3\u8026\uff0c\u6709\u6548\u6ce8\u5165\u7269\u7406\u5148\u9a8c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u7cbe\u786e\u6355\u6349\u7ec6\u5fae\u60c5\u611f\u7ebf\u7d22\u3002"}}
{"id": "2511.10898", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10898", "abs": "https://arxiv.org/abs/2511.10898", "authors": ["Chenghao Duan", "Chuanyi Ji"], "title": "Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters", "comment": null, "summary": "Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\\% - 15\\%$ in both the overall performance and class-wise accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(GAT)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u6076\u52a3\u5929\u6c14\u5bfc\u81f4\u7684\u505c\u7535\u6301\u7eed\u65f6\u95f4\uff0c\u5728\u56db\u4e2a\u98d3\u98ce\u5f71\u54cd\u4e0b\u7684501\u4e2a\u53bf\u6570\u636e\u4e0a\u53d6\u5f97\u4e8693%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u5bfc\u81f4\u7684\u5927\u89c4\u6a21\u505c\u7535\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u548c\u793e\u4f1a\u5f71\u54cd\uff0c\u51c6\u786e\u9884\u6d4b\u505c\u7535\u6062\u590d\u65f6\u95f4\u5bf9\u7535\u7f51\u97e7\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7a7a\u95f4\u4f9d\u8d56\u6027\u3001\u7a7a\u95f4\u5f02\u8d28\u6027\u548c\u4e8b\u4ef6\u6570\u636e\u6709\u9650\u4e09\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(GAT)\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u534a\u76d1\u7763\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5730\u7406\u7a7a\u95f4\u548c\u5929\u6c14\u6570\u636e\u6765\u4f30\u8ba1\u505c\u7535\u6301\u7eed\u65f6\u95f4\u3002", "result": "\u6a21\u578b\u5728\u56db\u4e2a\u98d3\u98ce\u5f71\u54cd\u4e0b\u7684501\u4e2a\u53bf\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc793%\uff0c\u6bd4XGBoost\u3001\u968f\u673a\u68ee\u6797\u3001GCN\u548c\u7b80\u5355GAT\u65b9\u6cd5\u9ad8\u51fa2%-15%\u3002", "conclusion": "\u63d0\u51fa\u7684GAT\u65b9\u6cd5\u5728\u9884\u6d4b\u6076\u52a3\u5929\u6c14\u5bfc\u81f4\u7684\u505c\u7535\u6301\u7eed\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u5f02\u8d28\u6027\u6311\u6218\uff0c\u4e3a\u7535\u7f51\u97e7\u6027\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.11301", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11301", "abs": "https://arxiv.org/abs/2511.11301", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Teng Ma", "Hongyi Zhang"], "title": "EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment", "comment": null, "summary": "Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.", "AI": {"tldr": "EcoAlign\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u5c06\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ecf\u6d4e\u7406\u6027\u7684\u641c\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u524d\u77bb\u6027\u51fd\u6570\u52a8\u6001\u6743\u8861\u5b89\u5168\u6027\u3001\u6548\u7528\u548c\u6210\u672c\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u5f3a\u5927\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u5b9e\u7528\u6027\u548c\u8fd0\u8425\u6210\u672c\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u56f0\u96be\uff0c\u4e14\u4ec5\u5173\u6ce8\u6700\u7ec8\u8f93\u51fa\u7684\u8fc7\u7a0b\u76f2\u76ee\u6027\u4f1a\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u9884\u7b97\u5728\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u4e0a\uff0c\u5141\u8bb8\u6709\u5bb3\u63a8\u7406\u901a\u8fc7\u826f\u6027\u7406\u7531\u4f2a\u88c5\u6765\u89c4\u907f\u7b80\u5355\u7684\u5b89\u5168\u8bc4\u5206\u3002", "method": "\u5c06LVLM\u89c6\u4e3a\u6709\u9650\u7406\u6027\u4ee3\u7406\uff0c\u9010\u6b65\u6269\u5c55\u601d\u7ef4\u56fe\uff0c\u4f7f\u7528\u524d\u77bb\u6027\u51fd\u6570\uff08\u7c7b\u4f3c\u51c0\u73b0\u503c\uff09\u5bf9\u884c\u52a8\u8fdb\u884c\u8bc4\u5206\uff0c\u52a8\u6001\u6743\u8861\u9884\u671f\u5b89\u5168\u6027\u3001\u6548\u7528\u548c\u6210\u672c\u4e0e\u5269\u4f59\u9884\u7b97\uff0c\u5e76\u901a\u8fc7\u6700\u8584\u5f31\u73af\u8282\u539f\u5219\u5f3a\u5236\u6267\u884c\u8def\u5f84\u5b89\u5168\u6027\u3002", "result": "\u57283\u4e2a\u95ed\u6e90\u548c2\u4e2a\u5f00\u6e90\u6a21\u578b\u30016\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEcoAlign\u5728\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u4e0b\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "EcoAlign\u4e3a\u5f3a\u5927\u7684LVLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u6761\u539f\u5219\u6027\u3001\u7ecf\u6d4e\u6027\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u5b89\u5168\u4e0e\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2511.11015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11015", "abs": "https://arxiv.org/abs/2511.11015", "authors": ["Siheon Joo", "Hongjo Kim"], "title": "SUPER Decoder Block for Reconstruction-Aware U-Net Variants", "comment": "8 pages. Under review", "summary": "Skip-connected encoder-decoder architectures (U-Net variants) are widely adopted for inverse problems but still suffer from information loss, limiting recovery of fine high-frequency details. We present Selectively Suppressed Perfect Reconstruction (SUPER), which exploits the perfect reconstruction (PR) property of wavelets to prevent information degradation while selectively suppressing (SS) redundant features. Free from rigid framelet constraints, SUPER serves as a plug-and-play decoder block for diverse U-Net variants, eliminating their intrinsic reconstruction bottlenecks and enhancing representational richness. Experiments across diverse crack benchmarks, including state-of-the-art (SOTA) models, demonstrate the structural potential of the proposed SUPER Decoder Block. Maintaining comparable computational cost, SUPER enriches representational diversity through increased parameterization. In small-scale in-domain experiments on the CrackVision12K dataset, SUPER markedly improves thin-crack segmentation performance, particularly for cracks narrower than 4 px, underscoring its advantage in high-frequency dominant settings. In smartphone image denoising on SIDD, where low-frequency components prevail, SUPER still achieves a moderate gain in PSNR, confirming its robustness across low- and high-frequency regimes. These results validate its plug-and-play generality across U-Net variants, achieving high-frequency fidelity and global coherence within a unified, reconstruction-aware framework.", "AI": {"tldr": "\u63d0\u51faSUPER\u89e3\u7801\u5668\u5757\uff0c\u5229\u7528\u5c0f\u6ce2\u5b8c\u7f8e\u91cd\u6784\u7279\u6027\u9632\u6b62\u4fe1\u606f\u4e22\u5931\uff0c\u540c\u65f6\u9009\u62e9\u6027\u6291\u5236\u5197\u4f59\u7279\u5f81\uff0c\u53ef\u5373\u63d2\u5373\u7528\u5730\u589e\u5f3a\u5404\u79cdU-Net\u53d8\u4f53\u7684\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u8df3\u8dc3\u8fde\u63a5\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08U-Net\u53d8\u4f53\uff09\u5728\u9006\u95ee\u9898\u4e2d\u4ecd\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u9ad8\u9891\u7ec6\u8282\u7684\u6062\u590d\u3002", "method": "\u5229\u7528\u5c0f\u6ce2\u7684\u5b8c\u7f8e\u91cd\u6784\u7279\u6027\u9632\u6b62\u4fe1\u606f\u9000\u5316\uff0c\u540c\u65f6\u9009\u62e9\u6027\u6291\u5236\u5197\u4f59\u7279\u5f81\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u89e3\u7801\u5668\u5757\u96c6\u6210\u5230\u5404\u79cdU-Net\u53d8\u4f53\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u88c2\u7f1d\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u88c2\u7f1d\uff08\u5bbd\u5ea6\u5c0f\u4e8e4\u50cf\u7d20\uff09\u7684\u5206\u5272\u6027\u80fd\uff0c\u5728\u667a\u80fd\u624b\u673a\u56fe\u50cf\u53bb\u566a\u4efb\u52a1\u4e2d\u4e5f\u83b7\u5f97\u4e86PSNR\u7684\u9002\u5ea6\u63d0\u5347\u3002", "conclusion": "SUPER\u89e3\u7801\u5668\u5757\u5177\u6709\u5373\u63d2\u5373\u7528\u7684\u901a\u7528\u6027\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u7684\u91cd\u6784\u611f\u77e5\u6846\u67b6\u5185\u5b9e\u73b0\u9ad8\u9891\u4fdd\u771f\u5ea6\u548c\u5168\u5c40\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.11025", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11025", "abs": "https://arxiv.org/abs/2511.11025", "authors": ["Jirong Zha", "Yuxuan Fan", "Tianyu Zhang", "Geng Chen", "Yingfeng Chen", "Chen Gao", "Xinlei Chen"], "title": "AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.", "AI": {"tldr": "AirCopBench\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u7a7a\u4e2d\u534f\u4f5c\u611f\u77e5\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b14.6k+\u95ee\u9898\uff0c\u6db5\u76d64\u4e2a\u5173\u952e\u4efb\u52a1\u7ef4\u5ea6\uff0c\u572840\u4e2aMLLMs\u4e0a\u8bc4\u4f30\u663e\u793a\u534f\u4f5c\u611f\u77e5\u4efb\u52a1\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u4f7f\u7528\u9ad8\u8d28\u91cf\u5355\u667a\u80fd\u4f53\u56fe\u50cf\u7684\u57fa\u7840\u611f\u77e5\u4efb\u52a1\uff0c\u65e0\u6cd5\u8bc4\u4f30MLLMs\u5728\u66f4\u590d\u6742\u7684\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u9000\u5316\u611f\u77e5\u6761\u4ef6\u4e0b\u3002", "method": "\u6784\u5efa\u5305\u542b\u6a21\u62df\u5668\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6a21\u578b\u3001\u89c4\u5219\u548c\u4eba\u5de5\u65b9\u6cd5\u5728\u4e25\u683c\u8d28\u91cf\u63a7\u5236\u4e0b\u751f\u6210\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u6db5\u76d6\u573a\u666f\u7406\u89e3\u3001\u7269\u4f53\u7406\u89e3\u3001\u611f\u77e5\u8bc4\u4f30\u548c\u534f\u4f5c\u51b3\u7b56\u56db\u4e2a\u7ef4\u5ea6\u3002", "result": "\u572840\u4e2aMLLMs\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u534f\u4f5c\u611f\u77e5\u4efb\u52a1\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u6700\u4f73\u6a21\u578b\u5e73\u5747\u843d\u540e\u4eba\u7c7b24.38%\uff0c\u4e14\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u8868\u73b0\u4e0d\u4e00\u81f4\u3002\u5fae\u8c03\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u7a7a\u4e2d\u534f\u4f5c\u611f\u77e5\u548c\u63a8\u7406\u4e2d\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u7684\u53ef\u884c\u6027\u3002", "conclusion": "AirCopBench\u586b\u8865\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u8bc4\u4f30\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86MLLMs\u5728\u590d\u6742\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2511.11111", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11111", "abs": "https://arxiv.org/abs/2511.11111", "authors": ["Xin Wang", "Pietro Lodi Rizzini", "Sourav Medya", "Zhiling Lan"], "title": "SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems", "comment": "Accepted at AAAI 2026", "summary": "The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \\ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \\ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4bDragonfly\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u7a0b\u5e8f\u8fd0\u884c\u65f6\u95f4\uff0c\u652f\u6301\u9ad8\u6548\u7684\u6df7\u5408\u4eff\u771f\u3002", "motivation": "Dragonfly\u7f51\u7edc\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5171\u4eab\u7f51\u7edc\u94fe\u8def\u4e0a\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5e72\u6270\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u4f20\u7edf\u7684\u5e76\u884c\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u6216\u5b9e\u65f6\u573a\u666f\u3002", "method": "\u5f00\u53d1\u4e86\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u4ece\u7aef\u53e3\u7ea7\u8def\u7531\u5668\u6570\u636e\u4e2d\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u6a21\u5f0f\u3002", "result": "\u8be5\u6a21\u578b\u5728\u8fd0\u884c\u65f6\u95f4\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u8fd0\u884c\u65f6\u95f4\uff0c\u652f\u6301Dragonfly\u7f51\u7edc\u7684\u9ad8\u6548\u6df7\u5408\u4eff\u771f\u3002"}}
{"id": "2511.11159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11159", "abs": "https://arxiv.org/abs/2511.11159", "authors": ["Omri Ben-Dov", "Luiz F. O. Chamon"], "title": "Adaptive Symmetrization of the KL Divergence", "comment": null, "summary": "Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u6700\u5c0f\u5316Jeffreys\u6563\u5ea6\uff0c\u901a\u8fc7\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u8054\u5408\u8bad\u7ec3\uff0c\u7ed3\u5408\u5f52\u4e00\u5316\u6d41\u548c\u80fd\u91cf\u6a21\u578b\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u524d\u5411KL\u6563\u5ea6\u867d\u7136\u6613\u4e8e\u5904\u7406\uff0c\u4f46\u5176\u4e0d\u5bf9\u79f0\u6027\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u76ee\u6807\u5206\u5e03\u7684\u6240\u6709\u7279\u6027\u3002\u5bf9\u79f0\u66ff\u4ee3\u65b9\u6848\u5982Jeffreys\u6563\u5ea6\u8ba1\u7b97\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u8054\u5408\u8bad\u7ec3\uff0c\u5c06\u4efb\u52a1\u8868\u8ff0\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u578b\u4f18\u5148\u7ea7\u6765\u4f18\u5316Jeffreys\u6563\u5ea6\u3002", "result": "\u5f00\u53d1\u51fa\u5b9e\u7528\u7b97\u6cd5\uff0c\u80fd\u591f\u7ed3\u5408\u5f52\u4e00\u5316\u6d41\u548c\u80fd\u91cf\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5728\u5bc6\u5ea6\u4f30\u8ba1\u3001\u56fe\u50cf\u751f\u6210\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u8bad\u7ec3\u6846\u67b6\u4e3a\u6700\u5c0f\u5316Jeffreys\u6563\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u4e0d\u540c\u6982\u7387\u6a21\u578b\u7684\u4f18\u70b9\u3002"}}
{"id": "2511.11170", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.11170", "abs": "https://arxiv.org/abs/2511.11170", "authors": ["Julien Collard", "Pierre Gentine", "Tian Zheng"], "title": "Power Ensemble Aggregation for Improved Extreme Event AI Prediction", "comment": "Accepted for the NeurIPS 2025 ML4PS workshop", "summary": "This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u70ed\u6d6a\u7b49\u6c14\u5019\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\uff0c\u901a\u8fc7\u529f\u7387\u5747\u503c\u805a\u5408\u96c6\u6210\u9884\u6d4b\u663e\u8457\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u5728\u9884\u6d4b\u6781\u7aef\u9ad8\u6e29\u4e8b\u4ef6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5747\u503c\u9884\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6c14\u5019\u6781\u7aef\u4e8b\u4ef6\uff08\u7279\u522b\u662f\u70ed\u6d6a\uff09\u9884\u6d4b\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5c06\u95ee\u9898\u6784\u5efa\u4e3a\u5206\u7c7b\u95ee\u9898\uff0c\u9884\u6d4b\u5730\u8868\u6c14\u6e29\u662f\u5426\u4f1a\u5728\u6307\u5b9a\u65f6\u95f4\u5185\u8d85\u8fc7\u5c40\u90e8q\u5206\u4f4d\u6570\uff1b\u901a\u8fc7\u529f\u7387\u5747\u503c\u805a\u5408\u96c6\u6210\u9884\u6d4b\uff0c\u4f7f\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5177\u6709\u751f\u6210\u6027\u3002", "result": "\u529f\u7387\u805a\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u5728\u9884\u6d4b\u6781\u7aef\u9ad8\u6e29\u4e8b\u4ef6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5747\u503c\u9884\u6d4b\uff0c\u4e14\u5bf9\u66f4\u9ad8\u6781\u7aef\u4e8b\u4ef6\u7684\u9884\u6d4b\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u529f\u7387\u5747\u503c\u805a\u5408\u65b9\u6cd5\u5728\u6c14\u5019\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u826f\u597d\u524d\u666f\u548c\u9002\u5e94\u6027\uff0c\u5176\u6700\u4f18\u6027\u80fd\u968f\u6240\u9009\u5206\u4f4d\u6570\u9608\u503c\u53d8\u5316\uff0c\u5bf9\u66f4\u9ad8\u6781\u7aef\u4e8b\u4ef6\u7684\u9884\u6d4b\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2511.11048", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11048", "abs": "https://arxiv.org/abs/2511.11048", "authors": ["Sun Jo", "Seok Young Hong", "JinHyun Kim", "Seungmin Kang", "Ahjin Choi", "Don-Gwan An", "Simon Song", "Je Hyeong Hong"], "title": "PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI", "comment": "Accepted at AAAI 2026. Supplementary material included after references. 27 pages, 21 figures, 11 tables", "summary": "4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.", "AI": {"tldr": "PINGS-X\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f74\u5bf9\u9f50\u65f6\u7a7a\u9ad8\u65af\u8868\u793a\u76844D\u6d41MRI\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u9ad8\u65af\u6e85\u5c04\u3001\u8f74\u5bf9\u9f50\u9ad8\u65af\u548c\u5408\u5e76\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u63d0\u9ad8\u8d85\u5206\u8fa8\u7387\u7cbe\u5ea6\u3002", "motivation": "4D\u6d41MRI\u9700\u8981\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u6765\u65e9\u671f\u68c0\u6d4b\u5fc3\u8840\u7ba1\u75be\u75c5\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u83b7\u53d6\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u73b0\u6709PINNs\u65b9\u6cd5\u8bad\u7ec3\u8fc7\u7a0b\u7f13\u6162\u4e14\u9700\u4e3a\u6bcf\u4e2a\u60a3\u8005\u5355\u72ec\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faPINGS-X\u6846\u67b6\uff0c\u91c7\u7528\u8f74\u5bf9\u9f50\u65f6\u7a7a\u9ad8\u65af\u8868\u793a\uff1a1) \u5177\u6709\u5f62\u5f0f\u6536\u655b\u4fdd\u8bc1\u7684\u5f52\u4e00\u5316\u9ad8\u65af\u6e85\u5c04\uff1b2) \u8f74\u5bf9\u9f50\u9ad8\u65af\u7b80\u5316\u9ad8\u7ef4\u6570\u636e\u8bad\u7ec3\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u548c\u6536\u655b\u6027\uff1b3) \u9ad8\u65af\u5408\u5e76\u8fc7\u7a0b\u9632\u6b62\u9000\u5316\u89e3\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u771f\u5b9e4D\u6d41MRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPINGS-X\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u5b9e\u73b0\u4f18\u8d8a\u7684\u8d85\u5206\u8fa8\u7387\u7cbe\u5ea6\u3002", "conclusion": "PINGS-X\u901a\u8fc7\u521b\u65b0\u7684\u9ad8\u65af\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e864D\u6d41MRI\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5fc3\u8840\u7ba1\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11051", "abs": "https://arxiv.org/abs/2511.11051", "authors": ["Chuheng Chen", "Xiaofei Zhou", "Geyuan Zhang", "Yong Huang"], "title": "NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion", "comment": null, "summary": "Low-Rank Adaptation (LoRA) fusion has emerged as a key technique for reusing and composing learned subject and style representations for controllable generation without costly retraining. However, existing methods rely on weight-based merging, where one LoRA often dominates the other, leading to interference and degraded fidelity. This interference is structural: separately trained LoRAs occupy low-rank high-dimensional subspaces, leading to non-orthogonal and overlapping representations. In this work, we analyze the internal structure of LoRAs and find their generative behavior is dominated by a few principal directions in the low-rank subspace, which should remain free from interference during fusion. To achieve this, we propose Null Space Projection LoRA (NP-LoRA), a projection-based framework for LoRA fusion that enforces subspace separation to prevent structural interference among principal directions. Specifically, we first extract principal style directions via singular value decomposition (SVD) and then project the subject LoRA into its orthogonal null space. Furthermore, we introduce a soft projection mechanism that enables smooth control over the trade-off between subject fidelity and style consistency. Experiments show NP-LoRA consistently improves fusion quality over strong baselines (e.g., DINO and CLIP-based metrics, with human and LLM preference scores), and applies broadly across backbones and LoRA pairs without retraining.", "AI": {"tldr": "\u63d0\u51faNP-LoRA\u65b9\u6cd5\u89e3\u51b3LoRA\u878d\u5408\u4e2d\u7684\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u96f6\u7a7a\u95f4\u6295\u5f71\u5b9e\u73b0\u5b50\u7a7a\u95f4\u5206\u79bb\uff0c\u63d0\u5347\u878d\u5408\u8d28\u91cf", "motivation": "\u73b0\u6709LoRA\u878d\u5408\u65b9\u6cd5\u57fa\u4e8e\u6743\u91cd\u5408\u5e76\uff0c\u5bfc\u81f4\u4e00\u4e2aLoRA\u4e3b\u5bfc\u53e6\u4e00\u4e2a\uff0c\u4ea7\u751f\u5e72\u6270\u548c\u4fdd\u771f\u5ea6\u4e0b\u964d\u95ee\u9898", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3\u63d0\u53d6\u4e3b\u98ce\u683c\u65b9\u5411\uff0c\u5c06\u4e3b\u9898LoRA\u6295\u5f71\u5230\u6b63\u4ea4\u96f6\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u8f6f\u6295\u5f71\u673a\u5236\u5e73\u8861\u4fdd\u771f\u5ea6\u4e0e\u4e00\u81f4\u6027", "result": "\u5b9e\u9a8c\u8868\u660eNP-LoRA\u5728DINO\u3001CLIP\u6307\u6807\u548c\u4eba\u7c7b/LLM\u504f\u597d\u5f97\u5206\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "NP-LoRA\u80fd\u6709\u6548\u9632\u6b62\u7ed3\u6784\u5e72\u6270\uff0c\u63d0\u5347LoRA\u878d\u5408\u8d28\u91cf\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u5404\u79cd\u9aa8\u5e72\u7f51\u7edc\u548cLoRA\u5bf9"}}
{"id": "2511.11065", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11065", "abs": "https://arxiv.org/abs/2511.11065", "authors": ["Muskaan Chopra", "Lorenz Sparrenberg", "Armin Berger", "Sarthak Khanna", "Jan H. Terheyden", "Rafet Sifa"], "title": "From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening", "comment": "Accepted in IEEE BigData 2025", "summary": "Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e862016-2025\u5e74\u95f4\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8(DR)\u68c0\u6d4b\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u8fdb\u5c55\uff0c\u6574\u5408\u4e8650\u591a\u9879\u7814\u7a76\u548c20\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u5173\u6ce8\u65b9\u6cd5\u521b\u65b0\u3001\u8bc4\u4f30\u6807\u51c6\u548c\u4e34\u5e8a\u8f6c\u5316\u6311\u6218\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5168\u7403\u53ef\u9884\u9632\u6027\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u51cf\u5c11\u89c6\u529b\u4e27\u5931\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u5f7b\u5e95\u6539\u53d8\u4e86DR\u7b5b\u67e5\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u603b\u7ed3\u6280\u672f\u8fdb\u5c55\u548c\u4e34\u5e8a\u8f6c\u5316\u969c\u788d\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u534a\u76d1\u7763\u5b66\u4e60\u3001\u9886\u57df\u6cdb\u5316\u3001\u8054\u90a6\u8bad\u7ec3\u548c\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u7b49\u6280\u672f\u8fdb\u5c55\uff0c\u540c\u65f6\u8003\u5bdf\u8bc4\u4f30\u534f\u8bae\u3001\u62a5\u544a\u6807\u51c6\u548c\u53ef\u91cd\u590d\u6027\u6311\u6218\u3002", "result": "\u5efa\u7acb\u4e86\u6db5\u76d6\u591a\u4e2a\u6570\u636e\u96c6\u7684\u57fa\u51c6\u6027\u80fd\u8868\uff0c\u8bc6\u522b\u4e86\u591a\u4e2d\u5fc3\u9a8c\u8bc1\u548c\u4e34\u5e8a\u4fe1\u4efb\u65b9\u9762\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5c06\u6280\u672f\u8fdb\u6b65\u4e0e\u8f6c\u5316\u969c\u788d\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "\u4e3a\u53ef\u91cd\u590d\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u4e34\u5e8a\u53ef\u90e8\u7f72\u7684DR AI\u5236\u5b9a\u4e86\u5b9e\u7528\u8bae\u7a0b\uff0c\u540c\u65f6\u6307\u51fa\u6240\u8c03\u67e5\u7684\u521b\u65b0\u65b9\u6cd5\u5e7f\u6cdb\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u533b\u5b66\u6210\u50cf\u3002"}}
{"id": "2511.11221", "categories": ["cs.LG", "nucl-ex"], "pdf": "https://arxiv.org/pdf/2511.11221", "abs": "https://arxiv.org/abs/2511.11221", "authors": ["Tyler Wheeler", "Michelle P. Kuchera", "Raghuram Ramanujan", "Ryan Krupp", "Chris Wrede", "Saiprasad Ravishankar", "Connor L. Cross", "Hoi Yan Ian Heung", "Andrew J. Jones", "Benjamin Votaw"], "title": "Sparse Methods for Vector Embeddings of TPC Data", "comment": "NeurIPS Machine Learning and the Physical Sciences Workshop 2025", "summary": "Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $\u03b2$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u5728\u65f6\u95f4\u6295\u5f71\u5ba4(TPC)\u6570\u636e\u4e0a\u4f7f\u7528\u7a00\u758f\u5377\u79ef\u7f51\u7edc\u8fdb\u884c\u8868\u793a\u5b66\u4e60\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u968f\u673a\u6743\u91cd\u7684\u7a00\u758fResNet\u67b6\u6784\u4e5f\u80fd\u63d0\u4f9b\u6709\u7528\u7684\u4e8b\u4ef6\u5411\u91cf\u5d4c\u5165\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u5d4c\u5165\u8d28\u91cf\u3002", "motivation": "TPC\u63a2\u6d4b\u5668\u5728\u6838\u7269\u7406\u5b9e\u9a8c\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u5176\u590d\u6742\u6570\u636e\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u901a\u7528\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u9ad8TPC\u6570\u636e\u5206\u6790\u6548\u7387\u3002", "method": "\u5c06\u539f\u59cbpad\u7ea7\u4fe1\u53f7\u8868\u793a\u4e3a\u7a00\u758f\u5f20\u91cf\uff0c\u4f7f\u7528Minkowski Engine ResNet\u6a21\u578b\u8bad\u7ec3\uff0c\u5728GADGET II TPC\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728AT-TPC\u6570\u636e\u4e0a\u8fdb\u884c\u8de8\u63a2\u6d4b\u5668\u6d4b\u8bd5\u3002", "result": "\u672a\u8bad\u7ec3\u7684\u7a00\u758fResNet\u6a21\u578b\u5df2\u80fd\u63d0\u4f9b\u6709\u7528\u7684AT-TPC\u6570\u636e\u5d4c\u5165\uff0c\u5728GADGET\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\u5d4c\u5165\u8d28\u91cf\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u63ed\u793a\u4e86\u4e30\u5bcc\u7684\u4e8b\u4ef6\u7ed3\u6784\u3002", "conclusion": "\u7a00\u758f\u5377\u79ef\u6280\u672f\u6709\u6f5c\u529b\u6210\u4e3aTPC\u5b9e\u9a8c\u4e2d\u901a\u7528\u7684\u8868\u793a\u5b66\u4e60\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u63a2\u6d4b\u5668\u6570\u636e\u3002"}}
{"id": "2511.11093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11093", "abs": "https://arxiv.org/abs/2511.11093", "authors": ["Dylan Saeed", "Ramtin Gharleghi", "Susann Bier", "Sonit Singh"], "title": "Machine-Learning Based Detection of Coronary Artery Calcification Using Synthetic Chest X-Rays", "comment": "10 pages, 5 figures. Under review for MIDL 2026", "summary": "Coronary artery calcification (CAC) is a strong predictor of cardiovascular events, with CT-based Agatston scoring widely regarded as the clinical gold standard. However, CT is costly and impractical for large-scale screening, while chest X-rays (CXRs) are inexpensive but lack reliable ground truth labels, constraining deep learning development. Digitally reconstructed radiographs (DRRs) offer a scalable alternative by projecting CT volumes into CXR-like images while inheriting precise labels. In this work, we provide the first systematic evaluation of DRRs as a surrogate training domain for CAC detection. Using 667 CT scans from the COCA dataset, we generate synthetic DRRs and assess model capacity, super-resolution fidelity enhancement, preprocessing, and training strategies. Lightweight CNNs trained from scratch outperform large pretrained networks; pairing super-resolution with contrast enhancement yields significant gains; and curriculum learning stabilises training under weak supervision. Our best configuration achieves a mean AUC of 0.754, comparable to or exceeding prior CXR-based studies. These results establish DRRs as a scalable, label-rich foundation for CAC detection, while laying the foundation for future transfer learning and domain adaptation to real CXRs.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6570\u5b57\u91cd\u5efa\u653e\u5c04\u5f71\u50cf\uff08DRR\uff09\u4f5c\u4e3a\u51a0\u72b6\u52a8\u8109\u9499\u5316\u68c0\u6d4b\u7684\u66ff\u4ee3\u8bad\u7ec3\u57df\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210DRR\u56fe\u50cf\u5e76\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709CXR\u7814\u7a76\u76f8\u5f53\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "CT\u662f\u51a0\u72b6\u52a8\u8109\u9499\u5316\u68c0\u6d4b\u7684\u91d1\u6807\u51c6\u4f46\u6210\u672c\u9ad8\uff0c\u80f8\u7247\u6210\u672c\u4f4e\u4f46\u7f3a\u4e4f\u53ef\u9760\u6807\u7b7e\uff0cDRR\u901a\u8fc7\u6295\u5f71CT\u4f53\u79ef\u751f\u6210\u7c7b\u4f3c\u80f8\u7247\u7684\u56fe\u50cf\u5e76\u7ee7\u627f\u7cbe\u786e\u6807\u7b7e\uff0c\u4e3a\u5927\u89c4\u6a21\u7b5b\u67e5\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002", "method": "\u4f7f\u7528667\u4e2aCT\u626b\u63cf\u751f\u6210\u5408\u6210DRR\uff0c\u8bc4\u4f30\u6a21\u578b\u5bb9\u91cf\u3001\u8d85\u5206\u8fa8\u7387\u4fdd\u771f\u5ea6\u589e\u5f3a\u3001\u9884\u5904\u7406\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u8f7b\u91cfCNN\u4ece\u5934\u8bad\u7ec3\u3001\u8d85\u5206\u8fa8\u7387\u4e0e\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u914d\u5bf9\u3001\u8bfe\u7a0b\u5b66\u4e60\u7b49\u3002", "result": "\u6700\u4f73\u914d\u7f6e\u8fbe\u5230\u5e73\u5747AUC 0.754\uff0c\u4e0e\u73b0\u6709\u57fa\u4e8e\u80f8\u7247\u7684\u7814\u7a76\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u8f7b\u91cfCNN\u4ece\u5934\u8bad\u7ec3\u4f18\u4e8e\u5927\u578b\u9884\u8bad\u7ec3\u7f51\u7edc\uff0c\u8d85\u5206\u8fa8\u7387\u4e0e\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u7ed3\u5408\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "DRR\u4e3a\u51a0\u72b6\u52a8\u8109\u9499\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6807\u7b7e\u4e30\u5bcc\u7684\u8bad\u7ec3\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u5411\u771f\u5b9e\u80f8\u7247\u7684\u8fc1\u79fb\u5b66\u4e60\u548c\u57df\u9002\u5e94\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11380", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11380", "abs": "https://arxiv.org/abs/2511.11380", "authors": ["Jiangkai Long", "Yanran Zhu", "Chang Tang", "Kun Sun", "Yuanyuan Liu", "Xuesong Yan"], "title": "When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering", "comment": "AAAI'2026 poster paper. 12 pages, 8 figures", "summary": "Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to \"speak\" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.", "AI": {"tldr": "SemST\u662f\u4e00\u4e2a\u8bed\u4e49\u5f15\u5bfc\u7684\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u805a\u7c7b\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u57fa\u56e0\u7b26\u53f7\u8f6c\u5316\u4e3a\u751f\u7269\u8bed\u4e49\u5d4c\u5165\uff0c\u5e76\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u83b7\u7684\u7a7a\u95f4\u5173\u7cfb\u878d\u5408\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u8c03\u5236\u6a21\u5757\u52a8\u6001\u6ce8\u5165\u9ad8\u9636\u751f\u7269\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u6a21\u578b\u5c06\u57fa\u56e0\u89c6\u4e3a\u5b64\u7acb\u6570\u503c\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u57fa\u56e0\u7b26\u53f7\u4e2d\u7f16\u7801\u7684\u4e30\u5bcc\u751f\u7269\u8bed\u4e49\uff0c\u963b\u788d\u4e86\u5bf9\u5173\u952e\u751f\u7269\u7279\u5f81\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u6bcf\u4e2a\u7ec4\u7ec7\u4f4d\u70b9\u7684\u57fa\u56e0\u96c6\u8f6c\u5316\u4e3a\u751f\u7269\u8bed\u4e49\u5d4c\u5165\uff0c\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u83b7\u7684\u7a7a\u95f4\u90bb\u57df\u5173\u7cfb\u878d\u5408\uff0c\u5e76\u5f15\u5165\u7ec6\u7c92\u5ea6\u8bed\u4e49\u8c03\u5236\u6a21\u5757\u8fdb\u884c\u7279\u5f81\u6821\u51c6\u3002", "result": "\u5728\u516c\u5171\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSemST\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u6027\u80fd\uff0c\u4e14FSM\u6a21\u5757\u5177\u6709\u5373\u63d2\u5373\u7528\u7684\u901a\u7528\u6027\u3002", "conclusion": "SemST\u901a\u8fc7\u6574\u5408\u751f\u7269\u8bed\u4e49\u548c\u7a7a\u95f4\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u7684\u805a\u7c7b\u6027\u80fd\uff0c\u4e3a\u7406\u89e3\u7ec4\u7ec7\u5fae\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.11164", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11164", "abs": "https://arxiv.org/abs/2511.11164", "authors": ["Conghao Wong", "Ziqian Zou", "Beihao Xia", "Xinge You"], "title": "Reverberation: Learning the Latencies Before Forecasting Trajectories", "comment": null, "summary": "Bridging the past to the future, connecting agents both spatially and temporally, lies at the core of the trajectory prediction task. Despite great efforts, it remains challenging to explicitly learn and predict latencies, the temporal delays with which agents respond to different trajectory-changing events and adjust their future paths, whether on their own or interactively. Different agents may exhibit distinct latency preferences for noticing, processing, and reacting to any specific trajectory-changing event. The lack of consideration of such latencies may undermine the causal continuity of the forecasting system and also lead to implausible or unintended trajectories. Inspired by the reverberation curves in acoustics, we propose a new reverberation transform and the corresponding Reverberation (short for Rev) trajectory prediction model, which simulates and predicts different latency preferences of each agent as well as their stochasticity by using two explicit and learnable reverberation kernels, allowing for the controllable trajectory prediction based on these forecasted latencies. Experiments on multiple datasets, whether pedestrians or vehicles, demonstrate that Rev achieves competitive accuracy while revealing interpretable latency dynamics across agents and scenarios. Qualitative analyses further verify the properties of the proposed reverberation transform, highlighting its potential as a general latency modeling approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u54cd\u53d8\u6362\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578bRev\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6df7\u54cd\u6838\u663e\u5f0f\u5efa\u6a21\u548c\u9884\u6d4b\u667a\u80fd\u4f53\u5bf9\u8f68\u8ff9\u53d8\u5316\u4e8b\u4ef6\u7684\u5ef6\u8fdf\u54cd\u5e94\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u663e\u5f0f\u5b66\u4e60\u548c\u9884\u6d4b\u667a\u80fd\u4f53\u5bf9\u8f68\u8ff9\u53d8\u5316\u4e8b\u4ef6\u7684\u5ef6\u8fdf\u54cd\u5e94\uff08latencies\uff09\uff0c\u8fd9\u4f1a\u5f71\u54cd\u9884\u6d4b\u7cfb\u7edf\u7684\u56e0\u679c\u8fde\u7eed\u6027\u5e76\u5bfc\u81f4\u4e0d\u5408\u7406\u7684\u8f68\u8ff9\u3002", "method": "\u53d7\u58f0\u5b66\u4e2d\u6df7\u54cd\u66f2\u7ebf\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u6df7\u54cd\u53d8\u6362\u548cRev\u6a21\u578b\uff0c\u4f7f\u7528\u4e24\u4e2a\u663e\u5f0f\u53ef\u5b66\u4e60\u7684\u6df7\u54cd\u6838\u6765\u6a21\u62df\u548c\u9884\u6d4b\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u4e0d\u540c\u5ef6\u8fdf\u504f\u597d\u53ca\u5176\u968f\u673a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRev\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u63ed\u793a\u8de8\u667a\u80fd\u4f53\u548c\u573a\u666f\u7684\u53ef\u89e3\u91ca\u5ef6\u8fdf\u52a8\u6001\u3002", "conclusion": "\u6df7\u54cd\u53d8\u6362\u5177\u6709\u4f5c\u4e3a\u901a\u7528\u5ef6\u8fdf\u5efa\u6a21\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8e\u9884\u6d4b\u5ef6\u8fdf\u7684\u53ef\u63a7\u8f68\u8ff9\u9884\u6d4b\u3002"}}
{"id": "2511.11214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11214", "abs": "https://arxiv.org/abs/2511.11214", "authors": ["Jooyoung Lee", "Jader Martins Camboim de S\u00e1"], "title": "Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy", "comment": null, "summary": "WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86WordNet\u4e2d\u526f\u8bcd\u7684\u7cfb\u7edf\u5316\u8d85\u4e49\u5206\u7c7b\u6cd5\uff0c\u586b\u8865\u4e86\u526f\u8bcd\u8bed\u4e49\u5206\u7c7b\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027", "motivation": "WordNet\u4e3a\u540d\u8bcd\u548c\u52a8\u8bcd\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8d85\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u4f46\u526f\u8bcd\u7f3a\u4e4f\u7cfb\u7edf\u7684\u8bed\u4e49\u5206\u7c7b\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5e94\u7528", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u526f\u8bcd\u8d85\u4e49\u7c7b\u578b\u5b66\uff0c\u5305\u62ec\u65b9\u5f0f\u3001\u65f6\u95f4\u3001\u9891\u7387\u3001\u7a0b\u5ea6\u3001\u9886\u57df\u3001\u8bf4\u8bdd\u8005\u5bfc\u5411\u548c\u4e3b\u8bed\u5bfc\u5411\u7b49\u8bed\u4e49\u57df\uff0c\u5e76\u901a\u8fc7\u6807\u6ce8\u7814\u7a76\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1", "result": "\u8bd5\u70b9\u6807\u6ce8\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u7c7b\u522b\u80fd\u591f\u5e7f\u6cdb\u8986\u76d6\u81ea\u7136\u6587\u672c\u4e2d\u7684\u526f\u8bcd\uff0c\u5e76\u4e14\u4eba\u7c7b\u6807\u6ce8\u8005\u80fd\u591f\u53ef\u9760\u5730\u5206\u914d\u8fd9\u4e9b\u7c7b\u522b", "conclusion": "\u8be5\u7c7b\u578b\u5b66\u6269\u5c55\u4e86WordNet\u7684\u8986\u76d6\u8303\u56f4\uff0c\u4f7f\u5176\u66f4\u8d34\u8fd1\u8bed\u8a00\u5b66\u7406\u8bba\uff0c\u5e76\u4fc3\u8fdb\u8bcd\u4e49\u6d88\u6b67\u3001\u4e8b\u4ef6\u62bd\u53d6\u3001\u60c5\u611f\u5206\u6790\u548c\u8bdd\u8bed\u5efa\u6a21\u7b49\u4e0b\u6e38NLP\u5e94\u7528"}}
{"id": "2511.11197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11197", "abs": "https://arxiv.org/abs/2511.11197", "authors": ["Anushree Bhuskute", "Kaushik Gopalan", "Jeet Shah"], "title": "Computationally-efficient deep learning models for nowcasting of precipitation: A solution for the Weather4cast 2025 challenge", "comment": null, "summary": "This study presents a transfer-learning framework based on Convolutional Gated Recurrent Units (ConvGRU) for short-term rainfall prediction in the Weather4Cast 2025 competition. A single SEVIRI infrared channel (10.8 \u03bcm wavelength) is used as input, which consists of four observations over a one-hour period. A two-stage training strategy is applied to generate rainfall estimates up to four hours ahead. In the first stage, ConvGRU is trained to forecast the brightness temperatures from SEVIRI, enabling the model to capture relevant spatiotemporal patterns. In the second stage, an empirically derived nonlinear transformation maps the predicted fields to OPERA-compatible rainfall rates.\n  For the event-prediction task, the transformed rainfall forecasts are processed using 3D event detection followed by spatiotemporal feature extraction to identify and characterize precipitation events. Our submission achieved 2nd place in the cumulative rainfall task. Further, the same model was used out-of-the-box for the event prediction task, and resulted in similar scores as the baseline model to the competition.", "AI": {"tldr": "\u57fa\u4e8eConvGRU\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u77ed\u671f\u964d\u96e8\u9884\u6d4b\uff0c\u4f7f\u7528SEVIRI\u7ea2\u5916\u901a\u9053\u6570\u636e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u9884\u6d4b\u672a\u67654\u5c0f\u65f6\u964d\u96e8\u91cf\uff0c\u5728Weather4Cast 2025\u7ade\u8d5b\u4e2d\u7d2f\u8ba1\u964d\u96e8\u4efb\u52a1\u83b7\u5f97\u7b2c\u4e8c\u540d\u3002", "motivation": "\u5f00\u53d1\u6709\u6548\u7684\u77ed\u671f\u964d\u96e8\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u5929\u6c14\u9884\u62a5\u63d0\u4f9b\u51c6\u786e\u7684\u6280\u672f\u652f\u6301\uff0c\u7279\u522b\u662f\u5728Weather4Cast\u7ade\u8d5b\u80cc\u666f\u4e0b\u63d0\u5347\u964d\u96e8\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u4f7f\u7528ConvGRU\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u9884\u6d4bSEVIRI\u4eae\u5ea6\u6e29\u5ea6\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u975e\u7ebf\u6027\u53d8\u6362\u5c06\u9884\u6d4b\u573a\u6620\u5c04\u4e3aOPERA\u517c\u5bb9\u7684\u964d\u96e8\u7387\u3002\u5bf9\u4e8e\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\uff0c\u4f7f\u75283D\u4e8b\u4ef6\u68c0\u6d4b\u548c\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728\u7d2f\u8ba1\u964d\u96e8\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u8868\u73b0\u4e0e\u57fa\u51c6\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "ConvGRU\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u5728\u77ed\u671f\u964d\u96e8\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65f6\u7a7a\u6a21\u5f0f\u548c\u964d\u96e8\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u3002"}}
{"id": "2511.11344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11344", "abs": "https://arxiv.org/abs/2511.11344", "authors": ["Pavel Rojtberg", "Julius K\u00fchn"], "title": "YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation", "comment": null, "summary": "We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation. While synthetic data has become fundamental in frame-based computer vision, event-based vision lacks comparable comprehensive resources. Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology. Our generation framework employs simulated linear camera motion to ensure complete scene coverage, including background activity.\n  Through systematic evaluation of event representations for CNN-based inference, we demonstrate that time-surfaces with linear decay and dual-channel polarity encoding achieve superior pose estimation performance, outperforming exponential decay and single-channel alternatives by significant margins. Our analysis reveals that polarity information contributes most substantially to performance gains, while linear temporal encoding preserves critical motion information more effectively than exponential decay. The dataset is provided in a structured format with both raw event streams and precomputed optimal representations to facilitate immediate research use and reproducible benchmarking.\n  The dataset is publicly available at https://huggingface.co/datasets/paroj/ycbev_sd.", "AI": {"tldr": "YCB-Ev SD\u662f\u4e00\u4e2a\u7528\u4e8e6DoF\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6807\u51c6\u5206\u8fa8\u7387\u4e8b\u4ef6\u76f8\u673a\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b50,000\u4e2a\u4e8b\u4ef6\u5e8f\u5217\uff0c\u91c7\u7528\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\u548c\u7ebf\u6027\u76f8\u673a\u8fd0\u52a8\u6a21\u62df\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u7f3a\u4e4f\u4e0e\u5e27\u57fa\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5ab2\u7f8e\u7684\u7efc\u5408\u6027\u5408\u6210\u6570\u636e\u8d44\u6e90\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3(PBR)\u573a\u666f\uff0c\u9075\u5faaBOP\u65b9\u6cd5\u5b66\uff0c\u901a\u8fc7\u6a21\u62df\u7ebf\u6027\u76f8\u673a\u8fd0\u52a8\u751f\u6210\u4e8b\u4ef6\u5e8f\u5217\uff0c\u786e\u4fdd\u5b8c\u6574\u573a\u666f\u8986\u76d6\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff0c\u5177\u6709\u7ebf\u6027\u8870\u51cf\u548c\u53cc\u901a\u9053\u6781\u6027\u7f16\u7801\u7684\u65f6\u95f4\u8868\u9762\u5728\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u663e\u8457\u4f18\u4e8e\u6307\u6570\u8870\u51cf\u548c\u5355\u901a\u9053\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u6781\u6027\u4fe1\u606f\u5bf9\u6027\u80fd\u63d0\u5347\u8d21\u732e\u6700\u5927\uff0c\u7ebf\u6027\u65f6\u95f4\u7f16\u7801\u6bd4\u6307\u6570\u8870\u51cf\u66f4\u6709\u6548\u5730\u4fdd\u7559\u5173\u952e\u8fd0\u52a8\u4fe1\u606f\uff0c\u6570\u636e\u96c6\u4ee5\u7ed3\u6784\u5316\u683c\u5f0f\u63d0\u4f9b\uff0c\u4fbf\u4e8e\u7acb\u5373\u4f7f\u7528\u548c\u53ef\u91cd\u590d\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2511.11421", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11421", "abs": "https://arxiv.org/abs/2511.11421", "authors": ["Lan Li", "Tao Hu", "Da-Wei Zhou", "Han-Jia Ye", "De-Chuan Zhan"], "title": "BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning", "comment": "Accepted by AAAI 2026", "summary": "Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace\" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.", "AI": {"tldr": "BOFA\u662f\u4e00\u4e2a\u7528\u4e8e\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u8c03\u6574CLIP\u7684\u8de8\u6a21\u6001\u6865\u63a5\u5c42\u5b9e\u73b0\u65e0\u989d\u5916\u53c2\u6570\u7684\u9ad8\u6548\u9002\u5e94\uff0c\u5229\u7528\u6b63\u4ea4\u4f4e\u79e9\u878d\u5408\u9632\u6b62\u9057\u5fd8\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u6df7\u5408\u539f\u578b\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c06CLIP\u5e94\u7528\u4e8e\u7c7b\u589e\u91cf\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u9700\u8981\u989d\u5916\u6a21\u5757\u589e\u52a0\u590d\u6742\u5ea6\u548c\u9057\u5fd8\u98ce\u9669\uff1b\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u8868\u793a\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "BOFA\u6846\u67b6\uff1a1) \u4ec5\u8c03\u6574CLIP\u7684\u8de8\u6a21\u6001\u6865\u63a5\u5c42\uff0c\u4e0d\u589e\u52a0\u53c2\u6570\uff1b2) \u6b63\u4ea4\u4f4e\u79e9\u878d\u5408\u673a\u5236\uff0c\u5728\u4f4e\u79e9\u5b89\u5168\u5b50\u7a7a\u95f4\u66f4\u65b0\u53c2\u6570\u9632\u6b62\u9057\u5fd8\uff1b3) \u8de8\u6a21\u6001\u6df7\u5408\u539f\u578b\u878d\u5408\u7a33\u5b9a\u7684\u6587\u672c\u539f\u578b\u548c\u89c6\u89c9\u5bf9\u5e94\u7269\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBOFA\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BOFA\u901a\u8fc7\u9650\u5236\u6027\u53c2\u6570\u66f4\u65b0\u548c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6297\u9057\u5fd8\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\uff0c\u65e0\u9700\u6570\u636e\u56de\u653e\u3002"}}
