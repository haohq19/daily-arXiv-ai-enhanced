<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出了一种无监督、无需专家标注的医学影像异常检测框架，通过增量扩展正常样本集，结合轻量级适配器更新和不确定性门控样本准入机制，在多个医学影像数据集上显著提升了异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中未知异常检测面临标注异常样本稀缺和专家监督成本高昂的挑战，需要开发无需异常标签的无监督方法。

Method: 使用冻结的预训练视觉主干网络，添加微小卷积适配器进行快速域适应。通过k近邻异常评分和双概率门控机制（距离z-score阈值和SWAG认知不确定性边界）来安全扩展正常样本集。

Result: 在COVID-CXR数据集上ROC-AUC从0.9489提升至0.9982，F1从0.8048提升至0.9746；在Pneumonia CXR上ROC-AUC从0.6834提升至0.8968；在Brain MRI ND-5上ROC-AUC从0.6041提升至0.7269，PR-AUC从0.7539提升至0.8211。

Conclusion: 该框架在真实世界标注稀缺的医学影像应用中展现出高效性和有效性，能够持续优化正常性概念定义。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [2] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 提出了一种基于体素稀疏化和子流形稀疏卷积网络的两阶段方法，用于医学CT图像中的肿瘤自动分割，在保持高分辨率输入的同时显著降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 放射图像中的肿瘤精确分割是专业且耗时的任务，目前是临床环境中定量分析的瓶颈。传统方法需要降采样或使用图像块来处理3D扫描，这限制了分割精度。

Method: 使用两阶段方法：体素稀疏化和子流形稀疏卷积网络。该方法允许在高分辨率输入下进行分割，采用原生3D模型架构。

Result: 在KiTS23挑战赛的肾癌CT图像上，获得了与挑战赛优胜者竞争的结果：肾脏+肿块Dice相似系数95.8%，肿瘤+囊肿85.7%，单独肿瘤80.3%。计算效率显著提升，推理时间减少60%，VRAM使用减少75%。

Conclusion: 该方法在保持最先进分割精度的同时，显著减少了GPU内存和时间的计算资源需求，为临床环境中的自动肿瘤分割提供了实用解决方案。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [3] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 论文提出从反应式任务驱动系统转向空间超感知范式，包括语义感知、流式事件认知、隐式3D空间认知和预测性世界建模四个阶段。开发了VSI-SUPER基准测试，证明仅靠数据扩展不足以实现空间超感知，需要预测性感知方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能系统主要关注反应式任务驱动方法，缺乏真正的空间认知和世界建模能力。需要从纯语言理解转向更全面的空间超感知范式。

Method: 提出VSI-SUPER基准测试（包含VSR和VSC任务），构建VSI-590K数据集训练Cambrian-S模型，并开发基于预测误差的自监督下一潜在帧预测器方法。

Result: 在VSI-Bench上获得+30%绝对改进，但在VSI-SUPER上表现仍有限。预测性感知方法显著优于领先的专有基线模型。

Conclusion: 仅靠数据扩展无法实现空间超感知，需要能够预见、选择和组织经验的预测性感知模型。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 本文综述了如何利用连续血糖监测和可穿戴技术，结合机器学习模型，从静态血糖阈值转向动态代谢表型分析，实现糖尿病的精准预防和个性化干预。


<details>
  <summary>Details</summary>
Motivation: 传统的糖尿病和糖尿病前期分类使用静态血糖阈值，掩盖了由胰岛素抵抗、β细胞功能障碍和肠促胰岛素缺乏引起的病理生理异质性。需要更精准的方法来识别不同的代谢亚型。

Method: 使用连续血糖监测和可穿戴技术收集高分辨率血糖数据，结合机器学习模型分析家庭口服葡萄糖耐量测试数据，预测肌肉胰岛素抵抗和β细胞功能。整合饮食、睡眠和身体活动模式数据。

Result: 研究表明机器学习模型能够准确预测金标准测量的肌肉胰岛素抵抗和β细胞功能。个体对标准化餐食的餐后血糖反应可作为代谢亚型的生物标志物。饮食缓解剂对餐后血糖反应的衰减效果具有表型依赖性。

Conclusion: 连续血糖监测能够将早期血糖异常的复杂性分解为不同的、可操作的亚表型，超越简单的血糖控制，为针对个体核心代谢缺陷的精准营养、行为和药物策略开辟了新途径。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [5] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 提出了首个面向边缘设备LLM部署的自回归感知分割计算框架，通过混合精度量化、中间压缩和联合优化，在保持精度的同时显著减少通信开销和内存需求。


<details>
  <summary>Details</summary>
Motivation: LLM在资源受限的IoT设备上部署不切实际，现有分割计算方法无法解决自回归推理的独特挑战，特别是迭代token生成和扩展的KV缓存需求。

Method: 开发了三点关键贡献：1) 单点分割压缩(OPSC)混合精度量化方案；2) 两阶段中间压缩管道(TS+TAB-Q)；3) 统一优化框架联合选择分割点、量化设置和序列长度。

Result: 在多样化LLM和硬件平台上评估显示，相比SmoothQuant、OmniQuant和Atom等先进量化方法，框架实现了1.49倍推理加速和显著通信开销减少，同时保持或提高模型精度。

Conclusion: 该框架为边缘设备上的LLM部署提供了有效的解决方案，成功解决了自回归推理的挑战，在资源受限环境下实现了高性能部署。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [6] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: EPO是一个解决半无限安全强化学习问题的算法框架，通过迭代求解有限约束集的安全RL子问题，并自适应调整活动约束集来实现最优策略性能和确定性有界安全。


<details>
  <summary>Details</summary>
Motivation: 实际应用中安全强化学习常涉及无限约束问题（半无限安全RL），如需要在连续参数空间上强制执行安全条件。现有方法难以处理这类问题。

Method: EPO通过迭代求解有限约束集的安全RL子问题，采用约束扩展和删除机制自适应调整活动集：违反预定义容差的约束被添加，拉格朗日乘子为零的约束被移除。

Result: 理论分析表明，在温和假设下，通过EPO训练的策略实现与最优解相当的性能，且全局约束违反严格保持在预定界限内。

Conclusion: EPO为半无限安全RL问题提供了有效的解决方案，能够控制工作集的增长并支持有效的策略训练。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [7] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出基于Transformer的纵向建模方法，解决异构电子健康记录数据中的临床风险分类挑战，包括不规则时间模式、大模态差异和复杂语义结构。


<details>
  <summary>Details</summary>
Motivation: 解决异构电子健康记录数据在临床风险分类中的挑战，包括不规则时间模式、大模态差异和复杂语义结构。

Method: 采用特征嵌入层统一表示结构化和非结构化数据，引入可学习时间编码机制捕获不均匀采样间隔下的动态演变，使用多头自注意力结构进行纵向序列的全局依赖建模，设计语义加权池化模块自适应分配关键医疗事件的重要性。

Result: 实验结果表明，该模型在准确性、召回率、精确率和F1分数上优于传统机器学习和时序深度学习模型。

Conclusion: 该方法在多源异构电子健康记录环境中实现了稳定精确的风险识别，为临床智能决策提供了高效可靠的框架。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [8] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 提出RGP模型，通过时间子图采样和跨注意力潜在瓶颈来整合时空依赖，支持多任务预测，在多个数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注空间结构，将时间信息仅作为过滤约束而非建模信号，且通常只支持单任务预测，无法满足医疗、金融等领域对长程时空依赖和多任务预测的需求

Method: 提出时间子图采样器增强全局上下文，引入RGP图变换器架构，使用跨注意力潜在瓶颈整合结构和时间信息，支持多任务学习的灵活解码器

Result: 在RelBench、SALT和CTU数据集上实现最先进的性能

Conclusion: RGP为关系深度学习提供了通用且可扩展的解决方案，支持多样化的预测任务

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [9] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 本文提出在无线时间触发联邦学习系统中引入自适应模型剪枝，通过联合优化剪枝率和带宽分配来最小化训练损失并保证学习延迟，可减少40%通信成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习网络中的用户设备数量不断增加且带宽有限，导致掉队者问题和通信开销加剧。时间触发联邦学习作为通用形式，需要解决这些通信效率问题。

Method: 基于模型剪枝进行TT-Fed模型的梯度收敛分析，建立收敛上界，然后使用KKT条件推导剪枝率和带宽分配的闭式解。

Result: 模拟结果显示模型剪枝可减少40%的通信成本，同时保持模型性能不变。

Conclusion: 自适应模型剪枝能有效解决无线TT-Fed系统中的通信效率问题，在保证性能的同时显著降低通信开销。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [10] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种灰盒三维临近预报框架，结合物理约束神经网络和数据驱动学习，直接处理三维雷达反射率数据，实现更准确的三小时极端降水预报。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在时空分辨率不足、误差累积、过度平滑以及忽略垂直信息等问题，无法准确捕捉快速演变的对流过程。

Method: 使用三维雷达反射率数据，结合物理约束的神经算子学习垂直变化的3D平流场，参数化空间变化的扩散，引入布朗运动启发的随机项表示未解析运动，并使用残差分支捕捉小尺度对流启动。

Result: 在三小时预报期内对不同降水类型均实现更准确预报，在160名气象学家的盲评中57%情况下排名第一。

Conclusion: 通过恢复完整三维动力学并保持物理一致性，该框架为极端降水的准确可靠临近预报提供了可扩展且稳健的途径。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: SSPO提出句子级别重要性比率，平衡GRPO和GSPO的优缺点，避免训练崩溃和采样数据利用率低的问题，在五个数据集上平均得分46.57，优于GRPO和GSPO。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO存在策略更新不稳定问题，GSPO存在采样数据利用率低的问题，需要一种平衡的解决方案。

Method: SSPO采用句子级别重要性比率，结合句子熵调整PPO-CLIP的裁剪边界，鼓励高熵token探索，限制低熵token的裁剪范围。

Result: SSPO在五个数据集上平均得分46.57，超过GRPO的43.01和GSPO的44.42，在三个数据集上达到最先进性能。

Conclusion: SSPO通过采用GSPO的优点但避免其缺点，有效利用了生成数据，在推理任务中表现出色。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [12] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: LLM代码助手生成的安全漏洞问题严重，现有安全基准和改进方法效果有限，最新开源模型仍存在早期报告的安全漏洞。作者提出了新的严重性度量Prompt Exposure(PE)和Model Exposure(ME)来评估漏洞风险和模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的代码助手在软件开发中作用日益重要，其生成的漏洞对网络安全的影响也愈发关键。现有安全基准和改进方法对主流编码LLM的影响程度尚不明确。

Method: 引入新的严重性度量Prompt Exposure(PE)，综合考虑漏洞严重性、生成概率和诱导漏洞代码生成的提示词表述；基于PE定义Model Exposure(ME)评分，指示模型生成漏洞的严重性和普遍性。

Result: 研究发现即使最新的开源模型在现实使用场景中仍然容易受到早期报告漏洞的影响，表明安全性与功能性之间的权衡阻碍了漏洞的有效修复。

Conclusion: 需要新的严重性评估方法来鼓励缓解最严重和普遍的漏洞，提出的PE和ME评分有助于更好地评估和改善LLM代码生成的安全性。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>
