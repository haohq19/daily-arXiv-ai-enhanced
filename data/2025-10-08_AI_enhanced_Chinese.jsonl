{"id": "2510.05110", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05110", "abs": "https://arxiv.org/abs/2510.05110", "authors": ["Arezoo Saedi", "Afsaneh Fatemi", "Mohammad Ali Nematbakhsh", "Sophie Rosset", "Anne Vilnat"], "title": "Collaborative and Proactive Management of Task-Oriented Conversations", "comment": null, "summary": "Task oriented dialogue systems (TOD) complete particular tasks based on user\npreferences across natural language interactions. Considering the impressive\nperformance of large language models (LLMs) in natural language processing\n(NLP) tasks, most of the latest TODs are centered on LLMs. While proactive\nplanning is crucial for task completion, many existing TODs overlook effective\ngoal-aware planning. This paper creates a model for managing task-oriented\nconversations, conceptualized centered on the information state approach to\ndialogue management. The created model incorporated constructive intermediate\ninformation in planning. Initially, predefined slots and text part\ninformational components are created to model user preferences. Investigating\nintermediate information, critical circumstances are identified. Informational\ncomponents corresponding to these circumstances are created. Possible\nconfigurations for these informational components lead to limited information\nstates. Then, dialogue moves, which indicate movement between these information\nstates and the procedures that must be performed in the movements, are created.\nEventually, the update strategy is constructed. The created model is\nimplemented leveraging in-context learning of LLMs. In this model, database\nqueries are created centered on indicated predefined slots and the order of\nretrieved entities is indicated centered on text part. This mechanism enables\npassing the whole corresponding entities to the preferences in the order of\ncongruency. Evaluations exploiting the complete test conversations of MultiWOZ,\nwith no more than a domain in a conversation, illustrate maximal inform and\nsuccess, and improvement compared with previous methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u72b6\u6001\u65b9\u6cd5\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7ba1\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u4e2d\u95f4\u4fe1\u606f\u7ec4\u4ef6\u548c\u5bf9\u8bdd\u52a8\u4f5c\u6765\u5b9e\u73b0\u76ee\u6807\u611f\u77e5\u89c4\u5212\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u8be5\u6a21\u578b\uff0c\u5728MultiWOZ\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u5927\u591a\u5ffd\u89c6\u6709\u6548\u7684\u76ee\u6807\u611f\u77e5\u89c4\u5212\uff0c\u800c\u4e3b\u52a8\u89c4\u5212\u5bf9\u4efb\u52a1\u5b8c\u6210\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u5efa\u8bbe\u6027\u4e2d\u95f4\u4fe1\u606f\u89c4\u5212\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7ba1\u7406\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u72b6\u6001\u65b9\u6cd5\u6784\u5efa\u5bf9\u8bdd\u7ba1\u7406\u6a21\u578b\uff1a1) \u521b\u5efa\u9884\u5b9a\u4e49\u69fd\u4f4d\u548c\u6587\u672c\u90e8\u5206\u4fe1\u606f\u7ec4\u4ef6\u5efa\u6a21\u7528\u6237\u504f\u597d\uff1b2) \u8bc6\u522b\u5173\u952e\u60c5\u5883\u5e76\u521b\u5efa\u5bf9\u5e94\u4fe1\u606f\u7ec4\u4ef6\uff1b3) \u5b9a\u4e49\u6709\u9650\u4fe1\u606f\u72b6\u6001\uff1b4) \u521b\u5efa\u5bf9\u8bdd\u52a8\u4f5c\u548c\u72b6\u6001\u8f6c\u79fb\u8fc7\u7a0b\uff1b5) \u6784\u5efa\u66f4\u65b0\u7b56\u7565\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u6a21\u578b\u3002", "result": "\u5728MultiWOZ\u6570\u636e\u96c6\uff08\u5355\u57df\u5bf9\u8bdd\uff09\u7684\u5b8c\u6574\u6d4b\u8bd5\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\uff0c\u5b9e\u73b0\u4e86\u6700\u5927\u7684\u4fe1\u606f\u63d0\u4f9b\u7387\u548c\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4fe1\u606f\u72b6\u6001\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7ba1\u7406\u6a21\u578b\u80fd\u591f\u6709\u6548\u8fdb\u884c\u76ee\u6807\u611f\u77e5\u89c4\u5212\uff0c\u901a\u8fc7\u4e2d\u95f4\u4fe1\u606f\u7ec4\u4ef6\u548c\u5bf9\u8bdd\u52a8\u4f5c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5bf9\u8bdd\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.05168", "categories": ["cs.LG", "cs.CV", "I.2"], "pdf": "https://arxiv.org/pdf/2510.05168", "abs": "https://arxiv.org/abs/2510.05168", "authors": ["Eric Jahns", "Davi Moreno", "Milan Stojkov", "Michel A. Kinsy"], "title": "Discretized Quadratic Integrate-and-Fire Neuron Model for Deep Spiking Neural Networks", "comment": "18 pages, 2 figures", "summary": "Spiking Neural Networks (SNNs) have emerged as energy-efficient alternatives\nto traditional artificial neural networks, leveraging asynchronous and\nbiologically inspired neuron dynamics. Among existing neuron models, the Leaky\nIntegrate-and-Fire (LIF) neuron has become widely adopted in deep SNNs due to\nits simplicity and computational efficiency. However, this efficiency comes at\nthe expense of expressiveness, as LIF dynamics are constrained to linear decay\nat each timestep. In contrast, more complex models, such as the Quadratic\nIntegrate-and-Fire (QIF) neuron, exhibit richer, nonlinear dynamics but have\nseen limited adoption due to their training instability. On that note, we\npropose the first discretization of the QIF neuron model tailored for\nhigh-performance deep spiking neural networks and provide an in-depth analysis\nof its dynamics. To ensure training stability, we derive an analytical\nformulation for surrogate gradient windows directly from our discretizations'\nparameter set, minimizing gradient mismatch. We evaluate our method on\nCIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS, demonstrating its ability to\noutperform state-of-the-art LIF-based methods. These results establish our\ndiscretization of the QIF neuron as a compelling alternative to LIF neurons for\ndeep SNNs, combining richer dynamics with practical scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u9ad8\u6027\u80fd\u6df1\u5ea6\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684QIF\u795e\u7ecf\u5143\u6a21\u578b\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6bd4LIF\u795e\u7ecf\u5143\u66f4\u4e30\u5bcc\u7684\u975e\u7ebf\u6027\u52a8\u6001\u7279\u6027\u3002", "motivation": "LIF\u795e\u7ecf\u5143\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\u4f46\u8868\u8fbe\u80fd\u529b\u53d7\u9650\uff0c\u800cQIF\u7b49\u590d\u6742\u6a21\u578b\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u975e\u7ebf\u6027\u52a8\u6001\u4f46\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u53c8\u80fd\u63d0\u4f9b\u66f4\u5f3a\u8868\u8fbe\u80fd\u529b\u7684\u795e\u7ecf\u5143\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86QIF\u795e\u7ecf\u5143\u6a21\u578b\u7684\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u5e76\u76f4\u63a5\u4ece\u79bb\u6563\u5316\u53c2\u6570\u96c6\u4e2d\u63a8\u5bfc\u51fa\u66ff\u4ee3\u68af\u5ea6\u7a97\u53e3\u7684\u89e3\u6790\u516c\u5f0f\uff0c\u4ee5\u6700\u5c0f\u5316\u68af\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001ImageNet\u548cCIFAR-10 DVS\u7b49\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u8d85\u8d8a\u57fa\u4e8eLIF\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "QIF\u795e\u7ecf\u5143\u7684\u79bb\u6563\u5316\u65b9\u6cd5\u4e3a\u6df1\u5ea6SNNs\u63d0\u4f9b\u4e86\u6bd4LIF\u795e\u7ecf\u5143\u66f4\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u66f4\u4e30\u5bcc\u7684\u52a8\u6001\u7279\u6027\u548c\u5b9e\u9645\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.05408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05408", "abs": "https://arxiv.org/abs/2510.05408", "authors": ["Kebin Contreras", "Luis Toscano-Palomino", "Mauro Dalla Mura", "Jorge Bacca"], "title": "See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models", "comment": null, "summary": "Recovering the past from present observations is an intriguing challenge with\npotential applications in forensics and scene analysis. Thermal imaging,\noperating in the infrared range, provides access to otherwise invisible\ninformation. Since humans are typically warmer (37 C -98.6 F) than their\nsurroundings, interactions such as sitting, touching, or leaning leave residual\nheat traces. These fading imprints serve as passive temporal codes, allowing\nfor the inference of recent events that exceed the capabilities of RGB cameras.\nThis work proposes a time-reversed reconstruction framework that uses paired\nRGB and thermal images to recover scene states from a few seconds earlier. The\nproposed approach couples Visual-Language Models (VLMs) with a constrained\ndiffusion process, where one VLM generates scene descriptions and another\nguides image reconstruction, ensuring semantic and structural consistency. The\nmethod is evaluated in three controlled scenarios, demonstrating the\nfeasibility of reconstructing plausible past frames up to 120 seconds earlier,\nproviding a first step toward time-reversed imaging from thermal traces.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u70ed\u6210\u50cf\u7684\u65f6\u95f4\u53cd\u8f6c\u91cd\u5efa\u6846\u67b6\uff0c\u4f7f\u7528RGB\u548c\u70ed\u56fe\u50cf\u914d\u5bf9\u6765\u6062\u590d\u51e0\u79d2\u524d\u7684\u573a\u666f\u72b6\u6001\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7ea6\u675f\u6269\u6563\u8fc7\u7a0b\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u7684\u91cd\u5efa\u3002", "motivation": "\u70ed\u6210\u50cf\u53ef\u4ee5\u6355\u6349\u4eba\u673a\u4ea4\u4e92\u7559\u4e0b\u7684\u70ed\u75d5\u8ff9\uff0c\u8fd9\u4e9b\u9010\u6e10\u6d88\u5931\u7684\u70ed\u5370\u8bb0\u53ef\u4f5c\u4e3a\u88ab\u52a8\u65f6\u95f4\u7f16\u7801\uff0c\u63a8\u65ad\u51faRGB\u76f8\u673a\u65e0\u6cd5\u68c0\u6d4b\u7684\u8fd1\u671f\u4e8b\u4ef6\uff0c\u5728\u6cd5\u533b\u5b66\u548c\u573a\u666f\u5206\u6790\u4e2d\u6709\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u7ea6\u675f\u6269\u6563\u8fc7\u7a0b\u8026\u5408\uff0c\u4e00\u4e2aVLM\u751f\u6210\u573a\u666f\u63cf\u8ff0\uff0c\u53e6\u4e00\u4e2a\u6307\u5bfc\u56fe\u50cf\u91cd\u5efa\uff0c\u786e\u4fdd\u8bed\u4e49\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002\u4f7f\u7528RGB\u548c\u70ed\u56fe\u50cf\u914d\u5bf9\u8fdb\u884c\u65f6\u95f4\u53cd\u8f6c\u91cd\u5efa\u3002", "result": "\u5728\u4e09\u4e2a\u53d7\u63a7\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u8bc1\u660e\u53ef\u4ee5\u91cd\u5efa120\u79d2\u524d\u7684\u5408\u7406\u8fc7\u53bb\u5e27\uff0c\u4e3a\u57fa\u4e8e\u70ed\u75d5\u8ff9\u7684\u65f6\u95f4\u53cd\u8f6c\u6210\u50cf\u63d0\u4f9b\u4e86\u521d\u6b65\u6b65\u9aa4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4ece\u70ed\u75d5\u8ff9\u8fdb\u884c\u65f6\u95f4\u53cd\u8f6c\u91cd\u5efa\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6cd5\u533b\u5b66\u548c\u573a\u666f\u5206\u6790\u4e2d\u7684\u8fc7\u53bb\u4e8b\u4ef6\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2510.05197", "categories": ["cs.AI", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.05197", "abs": "https://arxiv.org/abs/2510.05197", "authors": ["Joshua Kazdan", "Rylan Schaeffer", "Youssef Allouah", "Colin Sullivan", "Kyssen Yu", "Noam Levi", "Sanmi Koyejo"], "title": "Efficient Prediction of Pass@k Scaling in Large Language Models", "comment": null, "summary": "Assessing the capabilities and risks of frontier AI systems is a critical\narea of research, and recent work has shown that repeated sampling from models\ncan dramatically increase both. For instance, repeated sampling has been shown\nto increase their capabilities, such as solving difficult math and coding\nproblems, but it has also been shown to increase their potential for harm, such\nas being jailbroken. Such results raise a crucial question for both capability\nand safety forecasting: how can one accurately predict a model's behavior when\nscaled to a massive number of attempts, given a vastly smaller sampling budget?\nThis question is directly relevant to model providers, who serve hundreds of\nmillions of users daily, and to governmental regulators, who seek to prevent\nharms. To answer this questions, we make three contributions. First, we find\nthat standard methods for fitting these laws suffer from statistical\nshortcomings that hinder predictive accuracy, especially in data-limited\nscenarios. Second, we remedy these shortcomings by introducing a robust\nestimation framework, which uses a beta-binomial distribution to generate more\naccurate predictions from limited data. Third, we propose a dynamic sampling\nstrategy that allocates a greater budget to harder problems. Combined, these\ninnovations enable more reliable prediction of rare risks and capabilities at a\nfraction of the computational cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7edf\u8ba1\u65b9\u6cd5\u6765\u9884\u6d4bAI\u6a21\u578b\u5728\u5927\u89c4\u6a21\u91c7\u6837\u4e0b\u7684\u80fd\u529b\u548c\u98ce\u9669\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u7edf\u8ba1\u7f3a\u9677\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u88ab\u5927\u89c4\u6a21\u4f7f\u7528\uff0c\u9700\u8981\u51c6\u786e\u9884\u6d4b\u6a21\u578b\u5728\u5927\u91cf\u5c1d\u8bd5\u4e0b\u7684\u884c\u4e3a\u548c\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u65f6\u9884\u6d4b\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u57fa\u4e8ebeta-\u4e8c\u9879\u5206\u5e03\u7684\u7a33\u5065\u4f30\u8ba1\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u5c06\u66f4\u591a\u9884\u7b97\u5206\u914d\u7ed9\u66f4\u96be\u7684\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u66f4\u53ef\u9760\u5730\u9884\u6d4b\u7f55\u89c1\u98ce\u9669\u548c\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7a33\u5065\u4f30\u8ba1\u6846\u67b6\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5927\u89c4\u6a21\u91c7\u6837\u573a\u666f\u4e0b\u9884\u6d4bAI\u6a21\u578b\u884c\u4e3a\u548c\u98ce\u9669\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.05172", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05172", "abs": "https://arxiv.org/abs/2510.05172", "authors": ["Anushiya Arunan", "Yan Qin", "Xiaoli Li", "U-Xuan Tan", "H. Vincent Poor", "Chau Yuen"], "title": "Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data", "comment": "Accepted in IEEE Transactions on Industrial Informatics", "summary": "Accurate battery capacity estimation is key to alleviating consumer concerns\nabout battery performance and reliability of electric vehicles (EVs). However,\npractical data limitations imposed by stringent privacy regulations and labeled\ndata shortages hamper the development of generalizable capacity estimation\nmodels that remain robust to real-world data distribution shifts. While\nself-supervised learning can leverage unlabeled data, existing techniques are\nnot particularly designed to learn effectively from challenging field data --\nlet alone from privacy-friendly data, which are often less feature-rich and\nnoisier. In this work, we propose a first-of-its-kind capacity estimation model\nbased on self-supervised pre-training, developed on a large-scale dataset of\nprivacy-friendly charging data snippets from real-world EV operations. Our\npre-training framework, snippet similarity-weighted masked input\nreconstruction, is designed to learn rich, generalizable representations even\nfrom less feature-rich and fragmented privacy-friendly data. Our key innovation\nlies in harnessing contrastive learning to first capture high-level\nsimilarities among fragmented snippets that otherwise lack meaningful context.\nWith our snippet-wise contrastive learning and subsequent similarity-weighted\nmasked reconstruction, we are able to learn rich representations of both\ngranular charging patterns within individual snippets and high-level\nassociative relationships across different snippets. Bolstered by this rich\nrepresentation learning, our model consistently outperforms state-of-the-art\nbaselines, achieving 31.9% lower test error than the best-performing benchmark,\neven under challenging domain-shifted settings affected by both manufacturer\nand age-induced distribution shifts.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u7535\u6c60\u5bb9\u91cf\u4f30\u8ba1\u6a21\u578b\uff0c\u4f7f\u7528\u9690\u79c1\u53cb\u597d\u7684\u5145\u7535\u6570\u636e\u7247\u6bb5\uff0c\u901a\u8fc7\u7247\u6bb5\u76f8\u4f3c\u6027\u52a0\u6743\u63a9\u7801\u8f93\u5165\u91cd\u5efa\u5b66\u4e60\u901a\u7528\u8868\u793a\uff0c\u5728\u57df\u504f\u79fb\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u5bb9\u91cf\u51c6\u786e\u4f30\u8ba1\u5bf9\u6d88\u8d39\u8005\u4fe1\u5fc3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9690\u79c1\u6cd5\u89c4\u548c\u6807\u6ce8\u6570\u636e\u77ed\u7f3a\u9650\u5236\u4e86\u901a\u7528\u5316\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u4ece\u9690\u79c1\u53cb\u597d\u4f46\u7279\u5f81\u8f83\u5c11\u3001\u566a\u58f0\u8f83\u591a\u7684\u73b0\u573a\u6570\u636e\u4e2d\u6709\u6548\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u7247\u6bb5\u76f8\u4f3c\u6027\u52a0\u6743\u63a9\u7801\u8f93\u5165\u91cd\u5efa\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u9996\u5148\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6355\u83b7\u7247\u6bb5\u95f4\u9ad8\u5c42\u76f8\u4f3c\u6027\uff0c\u7136\u540e\u8fdb\u884c\u76f8\u4f3c\u6027\u52a0\u6743\u7684\u63a9\u7801\u91cd\u5efa\uff0c\u5b66\u4e60\u7ec6\u7c92\u5ea6\u5145\u7535\u6a21\u5f0f\u548c\u9ad8\u5c42\u5173\u8054\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u5728\u5236\u9020\u5546\u548c\u8001\u5316\u5f15\u8d77\u7684\u57df\u504f\u79fb\u8bbe\u7f6e\u4e0b\uff0c\u6d4b\u8bd5\u8bef\u5dee\u6bd4\u6700\u4f73\u57fa\u51c6\u65b9\u6cd5\u964d\u4f4e31.9%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5206\u5e03\u53d8\u5316\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u8be5\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u4ece\u9690\u79c1\u53cb\u597d\u4f46\u7279\u5f81\u6709\u9650\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u4e30\u5bcc\u8868\u793a\uff0c\u4e3a\u7535\u6c60\u5bb9\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.05205", "categories": ["cs.LG", "astro-ph.CO"], "pdf": "https://arxiv.org/pdf/2510.05205", "abs": "https://arxiv.org/abs/2510.05205", "authors": ["Sebastian Wagner-Carena", "Aizhan Akhmetzhanova", "Sydney Erickson"], "title": "A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors", "comment": "Accepted to main conference of NeurIPS 2025. Code available at\n  https://github.com/swagnercarena/ddprism", "summary": "A common challenge in the natural sciences is to disentangle distinct,\nunknown sources from observations. Examples of this source separation task\ninclude deblending galaxies in a crowded field, distinguishing the activity of\nindividual neurons from overlapping signals, and separating seismic events from\nan ambient background. Traditional analyses often rely on simplified source\nmodels that fail to accurately reproduce the data. Recent advances have shown\nthat diffusion models can directly learn complex prior distributions from\nnoisy, incomplete data. In this work, we show that diffusion models can solve\nthe source separation problem without explicit assumptions about the source.\nOur method relies only on multiple views, or the property that different sets\nof observations contain different linear transformations of the unknown\nsources. We show that our method succeeds even when no source is individually\nobserved and the observations are noisy, incomplete, and vary in resolution.\nThe learned diffusion models enable us to sample from the source priors,\nevaluate the probability of candidate sources, and draw from the joint\nposterior of the source distribution given an observation. We demonstrate the\neffectiveness of our method on a range of synthetic problems as well as\nreal-world galaxy observations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6e90\u5206\u79bb\u65b9\u6cd5\uff0c\u65e0\u9700\u5bf9\u6e90\u8fdb\u884c\u663e\u5f0f\u5047\u8bbe\uff0c\u4ec5\u4f9d\u8d56\u591a\u89c6\u56fe\u89c2\u6d4b\u6570\u636e\u5373\u53ef\u89e3\u51b3\u6e90\u5206\u79bb\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6e90\u5206\u79bb\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5316\u7684\u6e90\u6a21\u578b\uff0c\u65e0\u6cd5\u51c6\u786e\u590d\u73b0\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u566a\u58f0\u3001\u4e0d\u5b8c\u6574\u6570\u636e\u4e2d\u5b66\u4e60\u590d\u6742\u5148\u9a8c\u5206\u5e03\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u6e90\u5206\u79bb\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u4ec5\u4f9d\u8d56\u591a\u89c6\u56fe\u89c2\u6d4b\uff08\u5373\u4e0d\u540c\u89c2\u6d4b\u5305\u542b\u672a\u77e5\u6e90\u7684\u4e0d\u540c\u7ebf\u6027\u53d8\u6362\uff09\u6765\u5b66\u4e60\u6e90\u5148\u9a8c\u5206\u5e03\uff0c\u65e0\u9700\u5bf9\u6e90\u8fdb\u884c\u663e\u5f0f\u5047\u8bbe\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6e90\u672a\u88ab\u5355\u72ec\u89c2\u6d4b\u3001\u89c2\u6d4b\u6570\u636e\u5b58\u5728\u566a\u58f0\u3001\u4e0d\u5b8c\u6574\u4e14\u5206\u8fa8\u7387\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6210\u529f\u5206\u79bb\u6e90\uff0c\u80fd\u591f\u4ece\u6e90\u5148\u9a8c\u4e2d\u91c7\u6837\u3001\u8bc4\u4f30\u5019\u9009\u6e90\u6982\u7387\uff0c\u5e76\u57fa\u4e8e\u89c2\u6d4b\u4ece\u8054\u5408\u540e\u9a8c\u5206\u5e03\u4e2d\u91c7\u6837\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6e90\u5206\u79bb\u95ee\u9898\uff0c\u5728\u5408\u6210\u95ee\u9898\u548c\u771f\u5b9e\u661f\u7cfb\u89c2\u6d4b\u4e2d\u90fd\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\uff0c\u4e3a\u81ea\u7136\u79d1\u5b66\u4e2d\u7684\u6e90\u5206\u79bb\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.05684", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05684", "abs": "https://arxiv.org/abs/2510.05684", "authors": ["Suwhan Choi", "Jaeyoon Jung", "Haebin Seong", "Minchan Kim", "Minyeong Kim", "Yongjun Cho", "Yoonshik Kim", "Yubeen Park", "Youngjae Yu", "Yunsung Lee"], "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI", "comment": null, "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/", "AI": {"tldr": "D2E\u6846\u67b6\u5229\u7528\u684c\u9762\u6e38\u620f\u73af\u5883\u4f5c\u4e3a\u673a\u5668\u4eba\u5177\u8eabAI\u7684\u9884\u8bad\u7ec3\u5e73\u53f0\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6570\u636e\u6536\u96c6\u548c\u4f2a\u6807\u6ce8\u6280\u672f\uff0c\u6210\u529f\u5c06\u6570\u5b57\u4ea4\u4e92\u6280\u80fd\u8fc1\u79fb\u5230\u7269\u7406\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u3002", "motivation": "\u89e3\u51b3\u5177\u8eabAI\u56e0\u7269\u7406\u8f68\u8ff9\u6536\u96c6\u6210\u672c\u9ad8\u6602\u800c\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5229\u7528\u684c\u9762\u6e38\u620f\u73af\u5883\u63d0\u4f9b\u5927\u89c4\u6a21\u4f20\u611f\u5668\u8fd0\u52a8\u4ea4\u4e92\u6570\u636e\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u4e2a\u7ec4\u4ef6\uff1aOWA\u5de5\u5177\u5305\u6807\u51c6\u5316\u684c\u9762\u4ea4\u4e92\u6570\u636e\u5e76\u5b9e\u73b0152\u500d\u538b\u7f29\uff1bGeneralist-IDM\u901a\u8fc7\u65f6\u95f4\u6233\u4e8b\u4ef6\u9884\u6d4b\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff1bVAPT\u5c06\u684c\u9762\u9884\u8bad\u7ec3\u8868\u793a\u8fc1\u79fb\u5230\u7269\u7406\u64cd\u4f5c\u548c\u5bfc\u822a\u4efb\u52a1\u3002", "result": "\u4f7f\u75281300+\u5c0f\u65f6\u6570\u636e\uff08259\u5c0f\u65f6\u4eba\u7c7b\u6f14\u793a+1000+\u5c0f\u65f6\u4f2a\u6807\u6ce8\u6e38\u620f\u6570\u636e\uff09\uff0c\u5728LIBERO\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fbe\u523096.6%\u6210\u529f\u7387\uff0c\u5728CANVAS\u5bfc\u822a\u4efb\u52a1\u4e0a\u8fbe\u523083.3%\u6210\u529f\u7387\u3002", "conclusion": "\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u4f20\u611f\u5668\u8fd0\u52a8\u57fa\u5143\u5177\u6709\u8db3\u591f\u7684\u4e0d\u53d8\u6027\uff0c\u80fd\u591f\u6709\u6548\u8fc1\u79fb\u5230\u7269\u7406\u5177\u8eab\u4efb\u52a1\uff0c\u786e\u7acb\u4e86\u684c\u9762\u9884\u8bad\u7ec3\u4f5c\u4e3a\u673a\u5668\u4eba\u5b66\u7684\u5b9e\u7528\u8303\u5f0f\u3002"}}
{"id": "2510.05761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05761", "abs": "https://arxiv.org/abs/2510.05761", "authors": ["Sedat Dogan", "Nina Dethlefs", "Debarati Chakraborty"], "title": "Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis", "comment": "Preprint work in progress. Main body: 9 pages. Total: 15 pages\n  including references and appendix. 16 figures and 12 tables", "summary": "Predicting the virality of online content remains challenging, especially for\nculturally complex, fast-evolving memes. This study investigates the\nfeasibility of early prediction of meme virality using a large-scale,\ncross-lingual dataset from 25 diverse Reddit communities. We propose a robust,\ndata-driven method to define virality based on a hybrid engagement score,\nlearning a percentile-based threshold from a chronologically held-out training\nset to prevent data leakage. We evaluated a suite of models, including Logistic\nRegression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive,\nmultimodal feature set across increasing time windows (30-420 min). Crucially,\nuseful signals emerge quickly: our best-performing model, XGBoost, achieves a\nPR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear \"evidentiary\ntransition,\" in which the importance of the feature dynamically shifts from the\nstatic context to the temporal dynamics as a meme gains traction. This work\nestablishes a robust, interpretable, and practical benchmark for early virality\nprediction in scenarios where full diffusion cascade data is unavailable,\ncontributing a novel cross-lingual dataset and a methodologically sound\ndefinition of virality. To our knowledge, this study is the first to combine\ntime series data with static content and network features to predict early meme\nvirality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u53c2\u4e0e\u5ea6\u7684\u65e9\u671f\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528XGBoost\u6a21\u578b\u572830\u5206\u949f\u5185\u5c31\u80fd\u6709\u6548\u9884\u6d4b\u7f51\u7edc\u8ff7\u56e0\u7684\u75c5\u6bd2\u5f0f\u4f20\u64ad\uff0c\u51c6\u786e\u7387PR-AUC>0.52\u3002", "motivation": "\u9884\u6d4b\u5728\u7ebf\u5185\u5bb9\u7684\u75c5\u6bd2\u5f0f\u4f20\u64ad\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6587\u5316\u590d\u6742\u3001\u5feb\u901f\u6f14\u53d8\u7684\u8ff7\u56e0\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u65e9\u671f\u9884\u6d4b\u8ff7\u56e0\u75c5\u6bd2\u5f0f\u4f20\u64ad\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u6765\u81ea25\u4e2a\u591a\u6837\u5316Reddit\u793e\u533a\u7684\u5927\u89c4\u6a21\u8de8\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u53c2\u4e0e\u5ea6\u7684\u75c5\u6bd2\u6027\u5b9a\u4e49\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u903b\u8f91\u56de\u5f52\u3001XGBoost\u548c\u591a\u5c42\u611f\u77e5\u673a\u7b49\u591a\u79cd\u6a21\u578b\uff0c\u91c7\u7528\u5305\u542b\u9759\u6001\u5185\u5bb9\u548c\u65f6\u95f4\u52a8\u6001\u7684\u5168\u9762\u591a\u6a21\u6001\u7279\u5f81\u96c6\u3002", "result": "XGBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5728\u77ed\u77ed30\u5206\u949f\u5185\u5c31\u80fd\u8fbe\u5230PR-AUC>0.52\u7684\u9884\u6d4b\u6548\u679c\u3002\u5206\u6790\u53d1\u73b0\u5b58\u5728\u660e\u663e\u7684\"\u8bc1\u636e\u8f6c\u6362\"\u73b0\u8c61\uff0c\u5373\u968f\u7740\u8ff7\u56e0\u83b7\u5f97\u5173\u6ce8\uff0c\u7279\u5f81\u91cd\u8981\u6027\u4ece\u9759\u6001\u4e0a\u4e0b\u6587\u52a8\u6001\u8f6c\u5411\u65f6\u95f4\u52a8\u6001\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u65e9\u671f\u75c5\u6bd2\u6027\u9884\u6d4b\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u5b9e\u7528\u7684\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u65e0\u6cd5\u83b7\u5f97\u5b8c\u6574\u6269\u6563\u7ea7\u8054\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8d21\u732e\u4e86\u65b0\u9896\u7684\u8de8\u8bed\u8a00\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u8bba\u4e0a\u5408\u7406\u7684\u75c5\u6bd2\u6027\u5b9a\u4e49\u3002"}}
{"id": "2510.05336", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05336", "abs": "https://arxiv.org/abs/2510.05336", "authors": ["Yongan Yu", "Xianda Du", "Qingchen Hu", "Jiahao Liang", "Jingwei Ni", "Dan Qiang", "Kaiyu Huang", "Grant McKenzie", "Renee Sieber", "Fengran Mo"], "title": "WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives", "comment": null, "summary": "Historical archives on weather events are collections of enduring primary\nsource records that offer rich, untapped narratives of how societies have\nexperienced and responded to extreme weather events. These qualitative accounts\nprovide insights into societal vulnerability and resilience that are largely\nabsent from meteorological records, making them valuable for climate scientists\nto understand societal responses. However, their vast scale, noisy digitized\nquality, and archaic language make it difficult to transform them into\nstructured knowledge for climate research. To address this challenge, we\nintroduce WeatherArchive-Bench, the first benchmark for evaluating\nretrieval-augmented generation (RAG) systems on historical weather archives.\nWeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which\nmeasures a system's ability to locate historically relevant passages from over\none million archival news segments, and WeatherArchive-Assessment, which\nevaluates whether Large Language Models (LLMs) can classify societal\nvulnerability and resilience indicators from extreme weather narratives.\nExtensive experiments across sparse, dense, and re-ranking retrievers, as well\nas a diverse set of LLMs, reveal that dense retrievers often fail on historical\nterminology, while LLMs frequently misinterpret vulnerability and resilience\nconcepts. These findings highlight key limitations in reasoning about complex\nsocietal indicators and provide insights for designing more robust\nclimate-focused RAG systems from archival contexts. The constructed dataset and\nevaluation framework are publicly available at\nhttps://anonymous.4open.science/r/WeatherArchive-Bench/.", "AI": {"tldr": "\u63d0\u51fa\u4e86WeatherArchive-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5386\u53f2\u5929\u6c14\u6863\u6848\u4e0a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u5305\u542b\u68c0\u7d22\u548c\u8bc4\u4f30\u4e24\u4e2a\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u5bc6\u96c6\u68c0\u7d22\u5668\u5728\u5386\u53f2\u672f\u8bed\u4e0a\u7684\u5931\u8d25\u548cLLM\u5728\u89e3\u8bfb\u793e\u4f1a\u8106\u5f31\u6027\u4e0e\u97e7\u6027\u6982\u5ff5\u65f6\u7684\u5e38\u89c1\u8bef\u89e3\u3002", "motivation": "\u5386\u53f2\u5929\u6c14\u6863\u6848\u5305\u542b\u4e30\u5bcc\u7684\u793e\u4f1a\u5e94\u5bf9\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u5b9a\u6027\u53d9\u8ff0\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5728\u6c14\u8c61\u8bb0\u5f55\u4e2d\u7f3a\u5931\uff0c\u4f46\u5bf9\u7406\u89e3\u793e\u4f1a\u54cd\u5e94\u5f88\u6709\u4ef7\u503c\u3002\u7136\u800c\uff0c\u6863\u6848\u89c4\u6a21\u5e9e\u5927\u3001\u6570\u5b57\u5316\u8d28\u91cf\u5608\u6742\u548c\u53e4\u65e7\u8bed\u8a00\u4f7f\u5176\u96be\u4ee5\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u7528\u4e8e\u6c14\u5019\u7814\u7a76\u3002", "method": "\u6784\u5efaWeatherArchive-Bench\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1aWeatherArchive-Retrieval\u8bc4\u4f30\u4ece\u8d85\u8fc7100\u4e07\u6863\u6848\u65b0\u95fb\u7247\u6bb5\u4e2d\u5b9a\u4f4d\u76f8\u5173\u6bb5\u843d\u7684\u80fd\u529b\uff1bWeatherArchive-Assessment\u8bc4\u4f30LLM\u4ece\u6781\u7aef\u5929\u6c14\u53d9\u8ff0\u4e2d\u5206\u7c7b\u793e\u4f1a\u8106\u5f31\u6027\u548c\u97e7\u6027\u6307\u6807\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5bc6\u96c6\u68c0\u7d22\u5668\u5728\u5904\u7406\u5386\u53f2\u672f\u8bed\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u800cLLM\u7ecf\u5e38\u8bef\u89e3\u8106\u5f31\u6027\u548c\u97e7\u6027\u6982\u5ff5\uff0c\u63ed\u793a\u4e86\u5728\u63a8\u7406\u590d\u6742\u793e\u4f1a\u6307\u6807\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8bbe\u8ba1\u66f4\u7a33\u5065\u7684\u57fa\u4e8e\u6863\u6848\u80cc\u666f\u7684\u6c14\u5019\u805a\u7126RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6784\u5efa\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.05399", "categories": ["cs.LG", "astro-ph.SR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05399", "abs": "https://arxiv.org/abs/2510.05399", "authors": ["Kangwoo Yi", "Bo Shen", "Qin Li", "Haimin Wang", "Yong-Jae Moon", "Jaewon Lee", "Hwanhee Lee"], "title": "Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour Solar Proton Flux Profiles Using GOES Data", "comment": "7 pages; accepted as a workshop paper at ICDM 2025", "summary": "Solar Proton Events (SPEs) cause significant radiation hazards to satellites,\nastronauts, and technological systems. Accurate forecasting of their proton\nflux time profiles is crucial for early warnings and mitigation. This paper\nexplores deep learning sequence-to-sequence (seq2seq) models based on Long\nShort-Term Memory networks to predict 24-hour proton flux profiles following\nSPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by\nNOAA GOES, each associated with a >=M-class western-hemisphere solar flare and\nundisturbed proton flux profiles. Using 4-fold stratified cross-validation, we\nevaluate seq2seq model configurations (varying hidden units and embedding\ndimensions) under multiple forecasting scenarios: (i) proton-only input vs.\ncombined proton+X-ray input, (ii) original flux data vs. trend-smoothed data,\nand (iii) autoregressive vs. one-shot forecasting. Our major results are as\nfollows: First, one-shot forecasting consistently yields lower error than\nautoregressive prediction, avoiding the error accumulation seen in iterative\napproaches. Second, on the original data, proton-only models outperform\nproton+X-ray models. However, with trend-smoothed data, this gap narrows or\nreverses in proton+X-ray models. Third, trend-smoothing significantly enhances\nthe performance of proton+X-ray models by mitigating fluctuations in the X-ray\nchannel. Fourth, while models trained on trendsmoothed data perform best on\naverage, the best-performing model was trained on original data, suggesting\nthat architectural choices can sometimes outweigh the benefits of data\npreprocessing.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8eLSTM\u7684seq2seq\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u592a\u9633\u8d28\u5b50\u4e8b\u4ef6\u540e24\u5c0f\u65f6\u7684\u8d28\u5b50\u901a\u91cf\u65f6\u95f4\u5256\u9762\uff0c\u901a\u8fc7\u591a\u79cd\u914d\u7f6e\u5bf9\u6bd4\u53d1\u73b0\u5355\u6b21\u9884\u6d4b\u4f18\u4e8e\u81ea\u56de\u5f52\u9884\u6d4b\uff0c\u8d8b\u52bf\u5e73\u6ed1\u80fd\u63d0\u5347\u8d28\u5b50+X\u5c04\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u592a\u9633\u8d28\u5b50\u4e8b\u4ef6\u5bf9\u536b\u661f\u3001\u5b87\u822a\u5458\u548c\u6280\u672f\u7cfb\u7edf\u9020\u6210\u4e25\u91cd\u8f90\u5c04\u5371\u5bb3\uff0c\u51c6\u786e\u9884\u6d4b\u8d28\u5b50\u901a\u91cf\u65f6\u95f4\u5256\u9762\u5bf9\u4e8e\u65e9\u671f\u9884\u8b66\u548c\u7f13\u89e3\u63aa\u65bd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u752840\u4e2aSPE\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u57fa\u4e8eLSTM\u7684seq2seq\u6a21\u578b\uff0c\u901a\u8fc74\u6298\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u4e0d\u540c\u914d\u7f6e\uff1a\u8d28\u5b50\u8f93\u5165vs\u8d28\u5b50+X\u5c04\u7ebf\u8f93\u5165\u3001\u539f\u59cb\u6570\u636evs\u8d8b\u52bf\u5e73\u6ed1\u6570\u636e\u3001\u81ea\u56de\u5f52vs\u5355\u6b21\u9884\u6d4b\u3002", "result": "\u5355\u6b21\u9884\u6d4b\u8bef\u5dee\u4f4e\u4e8e\u81ea\u56de\u5f52\u9884\u6d4b\uff1b\u539f\u59cb\u6570\u636e\u4e0a\u8d28\u5b50\u6a21\u578b\u4f18\u4e8e\u8d28\u5b50+X\u5c04\u7ebf\u6a21\u578b\uff0c\u4f46\u8d8b\u52bf\u5e73\u6ed1\u540e\u5dee\u8ddd\u7f29\u5c0f\u6216\u9006\u8f6c\uff1b\u8d8b\u52bf\u5e73\u6ed1\u663e\u8457\u63d0\u5347\u8d28\u5b50+X\u5c04\u7ebf\u6a21\u578b\u6027\u80fd\uff1b\u8d8b\u52bf\u5e73\u6ed1\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u5e73\u5747\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6700\u4f73\u6a21\u578b\u6765\u81ea\u539f\u59cb\u6570\u636e\u8bad\u7ec3\u3002", "conclusion": "LSTM seq2seq\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4bSPE\u8d28\u5b50\u901a\u91cf\uff0c\u5355\u6b21\u9884\u6d4b\u7b56\u7565\u548c\u9002\u5f53\u7684\u6570\u636e\u9884\u5904\u7406\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u6a21\u578b\u67b6\u6784\u9009\u62e9\u6709\u65f6\u6bd4\u6570\u636e\u9884\u5904\u7406\u66f4\u91cd\u8981\u3002"}}
{"id": "2510.05442", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05442", "abs": "https://arxiv.org/abs/2510.05442", "authors": ["Zizhao Wang", "Dingcheng Li", "Vaishakh Keshava", "Phillip Wallis", "Ananth Balashankar", "Peter Stone", "Lukas Rutishauser"], "title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety", "comment": null, "summary": "Large Language Model (LLM) agents can leverage tools such as Google Search to\ncomplete complex tasks. However, this tool usage introduces the risk of\nindirect prompt injections, where malicious instructions hidden in tool outputs\ncan manipulate the agent, posing security risks like data leakage. Current\ndefense strategies typically rely on fine-tuning LLM agents on datasets of\nknown attacks. However, the generation of these datasets relies on manually\ncrafted attack patterns, which limits their diversity and leaves agents\nvulnerable to novel prompt injections. To address this limitation, we propose\nAdversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework\nthat leverages adversarial reinforcement learning (RL) by formulating the\nproblem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker\nthat learns to autonomously generate diverse prompt injections and an agent\nthat learns to defend against them while completing its assigned tasks. To\nensure robustness against a wide range of attacks and to prevent cyclic\nlearning, we employ a population-based learning framework that trains the agent\nto defend against all previous attacker checkpoints. Evaluated on BrowserGym\nand AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower\nattack success rate than the original model while also improving their task\nsuccess rate. Our analysis further confirms that the adversarial process\ngenerates a diverse and challenging set of attacks, leading to a more robust\nagent compared to the base model.", "AI": {"tldr": "ARLAS\u6846\u67b6\u901a\u8fc7\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u4ee3\u7406\uff0c\u8ba9\u653b\u51fb\u8005\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u540c\u65f6\u8bad\u7ec3\u4ee3\u7406\u9632\u5fa1\u8fd9\u4e9b\u653b\u51fb\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u4f7f\u7528\u5de5\u5177\u65f6\u9762\u4e34\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u6784\u5efa\u7684\u653b\u51fb\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u5e94\u5bf9\u65b0\u578b\u653b\u51fb\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4e24\u4eba\u96f6\u548c\u535a\u5f08\uff0c\u540c\u65f6\u8bad\u7ec3\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005LLM\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7fa4\u4f53\u7684\u5b66\u4e60\u6846\u67b6\u9632\u6b62\u5faa\u73af\u5b66\u4e60\u3002", "result": "\u5728BrowserGym\u548cAgentDojo\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cARLAS\u5fae\u8c03\u7684\u4ee3\u7406\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u4efb\u52a1\u6210\u529f\u7387\u4e5f\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u5bf9\u6297\u8fc7\u7a0b\u751f\u6210\u4e86\u591a\u6837\u5316\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u653b\u51fb\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u4ea7\u751f\u4e86\u66f4\u9c81\u68d2\u7684\u4ee3\u7406\u3002"}}
{"id": "2510.05453", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05453", "abs": "https://arxiv.org/abs/2510.05453", "authors": ["Arpit Kapoor", "Rohitash Chandra"], "title": "QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification", "comment": null, "summary": "Conceptual rainfall-runoff models aid hydrologists and climate scientists in\nmodelling streamflow to inform water management practices. Recent advances in\ndeep learning have unravelled the potential for combining hydrological models\nwith deep learning models for better interpretability and improved predictive\nperformance. In our previous work, we introduced DeepGR4J, which enhanced the\nGR4J conceptual rainfall-runoff model using a deep learning model to serve as a\nsurrogate for the routing component. DeepGR4J had an improved rainfall-runoff\nprediction accuracy, particularly in arid catchments. Quantile regression\nmodels have been extensively used for quantifying uncertainty while aiding\nextreme value forecasting. In this paper, we extend DeepGR4J using a quantile\nregression-based ensemble learning framework to quantify uncertainty in\nstreamflow prediction. We also leverage the uncertainty bounds to identify\nextreme flow events potentially leading to flooding. We further extend the\nmodel to multi-step streamflow predictions for uncertainty bounds. We design\nexperiments for a detailed evaluation of the proposed framework using the\nCAMELS-Aus dataset. The results show that our proposed Quantile DeepGR4J\nframework improves the predictive accuracy and uncertainty interval quality\n(interval score) compared to baseline deep learning models. Furthermore, we\ncarry out flood risk evaluation using Quantile DeepGR4J, and the results\ndemonstrate its suitability as an early warning system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Quantile DeepGR4J\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u4f4d\u6570\u56de\u5f52\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55DeepGR4J\u6a21\u578b\uff0c\u7528\u4e8e\u91cf\u5316\u5f84\u6d41\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u8bc6\u522b\u6781\u7aef\u6d2a\u6c34\u4e8b\u4ef6\u3002", "motivation": "\u7ed3\u5408\u6c34\u6587\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5229\u7528\u5206\u4f4d\u6570\u56de\u5f52\u6a21\u578b\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5e76\u8f85\u52a9\u6781\u7aef\u503c\u9884\u6d4b\u3002", "method": "\u4f7f\u7528\u5206\u4f4d\u6570\u56de\u5f52\u96c6\u6210\u5b66\u4e60\u6846\u67b6\u6269\u5c55DeepGR4J\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u6b65\u5f84\u6d41\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u5e76\u5728CAMELS-Aus\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u4e0e\u57fa\u7ebf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0cQuantile DeepGR4J\u6846\u67b6\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u8d28\u91cf\uff08\u533a\u95f4\u5f97\u5206\uff09\uff0c\u5e76\u8bc1\u660e\u5176\u9002\u5408\u4f5c\u4e3a\u6d2a\u6c34\u9884\u8b66\u7cfb\u7edf\u3002", "conclusion": "Quantile DeepGR4J\u6846\u67b6\u5728\u5f84\u6d41\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6d2a\u6c34\u98ce\u9669\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u4f5c\u4e3a\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.06002", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06002", "abs": "https://arxiv.org/abs/2510.06002", "authors": ["Hudson de Martim"], "title": "Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG", "comment": null, "summary": "The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core\nlimitations of standard Retrieval-Augmented Generation in the legal domain by\nproviding a verifiable knowledge graph that models hierarchical structure,\ntemporal evolution, and causal events of legal norms. However, a critical gap\nremains: how to reliably query this structured knowledge without sacrificing\nits deterministic properties. This paper introduces the SAT-Graph API, a formal\nquery execution layer centered on canonical actions-atomic, composable, and\nauditable primitives that isolate probabilistic discovery from deterministic\nretrieval. These actions enable: (i) high-precision hybrid search; (ii) robust\nreference resolution; (iii) point-in-time version retrieval; and (iv) auditable\ncausal tracing. We demonstrate how planner-guided agents can decompose complex\nqueries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer\narchitecture transforms retrieval from an opaque black box to a transparent,\nauditable process, directly addressing Explainable AI (XAI) requirements for\nhigh-stakes domains.", "AI": {"tldr": "SAT-Graph API\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c4\u8303\u52a8\u4f5c\u7684\u67e5\u8be2\u6267\u884c\u5c42\uff0c\u901a\u8fc7\u539f\u5b50\u5316\u3001\u53ef\u7ec4\u5408\u548c\u53ef\u5ba1\u8ba1\u7684\u539f\u8bed\uff0c\u5728\u4fdd\u6301\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u786e\u5b9a\u6027\u5c5e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u9760\u67e5\u8be2\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6RAG\u5728\u6cd5\u5f8b\u9886\u57df\u4e2d\u7684\u6838\u5fc3\u9650\u5236\uff0c\u7279\u522b\u662f\u5982\u4f55\u5728\u4fdd\u6301\u7ed3\u6784\u5316\u77e5\u8bc6\u786e\u5b9a\u6027\u5c5e\u6027\u7684\u540c\u65f6\u8fdb\u884c\u53ef\u9760\u67e5\u8be2\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u89c4\u8303\u52a8\u4f5c\u7684\u67e5\u8be2\u6267\u884c\u5c42\uff0c\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u5b9e\u73b0\u6df7\u5408\u641c\u7d22\u3001\u5f15\u7528\u89e3\u6790\u3001\u65f6\u95f4\u7248\u672c\u68c0\u7d22\u548c\u56e0\u679c\u8ffd\u8e2a\u3002", "result": "\u6784\u5efa\u4e86\u4e24\u5c42\u67b6\u6784\uff0c\u5c06\u68c0\u7d22\u4ece\u9ed1\u76d2\u8fc7\u7a0b\u8f6c\u53d8\u4e3a\u900f\u660e\u53ef\u5ba1\u8ba1\u7684\u8fc7\u7a0b\uff0c\u76f4\u63a5\u6ee1\u8db3\u9ad8\u98ce\u9669\u9886\u57df\u5bf9\u53ef\u89e3\u91caAI(XAI)\u7684\u8981\u6c42\u3002", "conclusion": "SAT-Graph API\u901a\u8fc7\u89c4\u8303\u52a8\u4f5c\u5b9e\u73b0\u4e86\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u7684\u53ef\u9760\u67e5\u8be2\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u786e\u5b9a\u6027\u5c5e\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u4e86\u900f\u660e\u5316\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05527", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.05527", "abs": "https://arxiv.org/abs/2510.05527", "authors": ["Yuyao Wang", "Yu-Hung Cheng", "Debarghya Mukherjee", "Huimin Cheng"], "title": "Transfer Learning on Edge Connecting Probability Estimation under Graphon Model", "comment": null, "summary": "Graphon models provide a flexible nonparametric framework for estimating\nlatent connectivity probabilities in networks, enabling a range of downstream\napplications such as link prediction and data augmentation. However, accurate\ngraphon estimation typically requires a large graph, whereas in practice, one\noften only observes a small-sized network. One approach to addressing this\nissue is to adopt a transfer learning framework, which aims to improve\nestimation in a small target graph by leveraging structural information from a\nlarger, related source graph. In this paper, we propose a novel method, namely\nGTRANS, a transfer learning framework that integrates neighborhood smoothing\nand Gromov-Wasserstein optimal transport to align and transfer structural\npatterns between graphs. To prevent negative transfer, GTRANS includes an\nadaptive debiasing mechanism that identifies and corrects for target-specific\ndeviations via residual smoothing. We provide theoretical guarantees on the\nstability of the estimated alignment matrix and demonstrate the effectiveness\nof GTRANS in improving the accuracy of target graph estimation through\nextensive synthetic and real data experiments. These improvements translate\ndirectly to enhanced performance in downstream applications, such as the graph\nclassification task and the link prediction task.", "AI": {"tldr": "\u63d0\u51faGTRANS\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u90bb\u57df\u5e73\u6ed1\u548cGromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u5728\u76f8\u5173\u56fe\u4e4b\u95f4\u5bf9\u9f50\u548c\u8fc1\u79fb\u7ed3\u6784\u6a21\u5f0f\uff0c\u89e3\u51b3\u5c0f\u56fe\u7f51\u7edc\u4f30\u8ba1\u95ee\u9898", "motivation": "\u56fe\u6a21\u578b\u4f30\u8ba1\u901a\u5e38\u9700\u8981\u5927\u56fe\u6570\u636e\uff0c\u4f46\u5b9e\u8df5\u4e2d\u5f80\u5f80\u53ea\u80fd\u83b7\u5f97\u5c0f\u89c4\u6a21\u7f51\u7edc\uff0c\u9700\u8981\u5229\u7528\u76f8\u5173\u5927\u56fe\u7684\u7ed3\u4fe1\u606f\u6765\u6539\u8fdb\u5c0f\u56fe\u7684\u4f30\u8ba1\u7cbe\u5ea6", "method": "\u7ed3\u5408\u90bb\u57df\u5e73\u6ed1\u548cGromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u6765\u5bf9\u9f50\u56fe\u7ed3\u6784\uff0c\u5e76\u5305\u542b\u81ea\u9002\u5e94\u53bb\u504f\u673a\u5236\u901a\u8fc7\u6b8b\u5dee\u5e73\u6ed1\u8bc6\u522b\u548c\u6821\u6b63\u76ee\u6807\u56fe\u7279\u5b9a\u504f\u5dee", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u5bf9\u9f50\u77e9\u9635\u7684\u7a33\u5b9a\u6027\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u56fe\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5e76\u6539\u5584\u4e86\u56fe\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "conclusion": "GTRANS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u56fe\u7f51\u7edc\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u548c\u4e0b\u6e38\u5e94\u7528\u6027\u80fd"}}
{"id": "2510.06105", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06105", "abs": "https://arxiv.org/abs/2510.06105", "authors": ["Batu El", "James Zou"], "title": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences", "comment": null, "summary": "Large language models (LLMs) are increasingly shaping how information is\ncreated and disseminated, from companies using them to craft persuasive\nadvertisements, to election campaigns optimizing messaging to gain votes, to\nsocial media influencers boosting engagement. These settings are inherently\ncompetitive, with sellers, candidates, and influencers vying for audience\napproval, yet it remains poorly understood how competitive feedback loops\ninfluence LLM behavior. We show that optimizing LLMs for competitive success\ncan inadvertently drive misalignment. Using simulated environments across these\nscenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise\nin deceptive marketing; in elections, a 4.9% gain in vote share coincides with\n22.3% more disinformation and 12.5% more populist rhetoric; and on social\nmedia, a 7.5% engagement boost comes with 188.6% more disinformation and a\n16.3% increase in promotion of harmful behaviors. We call this phenomenon\nMoloch's Bargain for AI--competitive success achieved at the cost of alignment.\nThese misaligned behaviors emerge even when models are explicitly instructed to\nremain truthful and grounded, revealing the fragility of current alignment\nsafeguards. Our findings highlight how market-driven optimization pressures can\nsystematically erode alignment, creating a race to the bottom, and suggest that\nsafe deployment of AI systems will require stronger governance and carefully\ndesigned incentives to prevent competitive dynamics from undermining societal\ntrust.", "AI": {"tldr": "\u4f18\u5316LLMs\u5728\u7ade\u4e89\u73af\u5883\u4e2d\u7684\u8868\u73b0\u4f1a\u65e0\u610f\u4e2d\u5bfc\u81f4\u6a21\u578b\u5931\u51c6\uff0c\u8868\u73b0\u4e3a\u6b3a\u9a97\u6027\u8425\u9500\u3001\u865a\u5047\u4fe1\u606f\u548c\u6709\u5bb3\u884c\u4e3a\u7684\u589e\u52a0\uff0c\u5373\u4f7f\u6a21\u578b\u88ab\u660e\u786e\u6307\u793a\u8981\u4fdd\u6301\u771f\u5b9e\u548c\u53ef\u9760\u3002", "motivation": "\u7406\u89e3\u7ade\u4e89\u6027\u53cd\u9988\u5faa\u73af\u5982\u4f55\u5f71\u54cdLLM\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u5546\u4e1a\u5e7f\u544a\u3001\u9009\u4e3e\u548c\u793e\u4ea4\u5a92\u4f53\u7b49\u7ade\u4e89\u6027\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u73af\u5883\u4e2d\u5404\u65b9\u90fd\u5728\u4e89\u593a\u53d7\u4f17\u7684\u8ba4\u53ef\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u73af\u5883\u6765\u7814\u7a76LLMs\u5728\u5546\u4e1a\u3001\u9009\u4e3e\u548c\u793e\u4ea4\u5a92\u4f53\u573a\u666f\u4e2d\u7684\u884c\u4e3a\uff0c\u6d4b\u91cf\u7ade\u4e89\u6210\u529f\u4e0e\u6a21\u578b\u5931\u51c6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728\u5546\u4e1a\u73af\u5883\u4e2d\uff0c\u9500\u552e\u589e\u957f6.3%\u4f34\u968f\u7740\u6b3a\u9a97\u6027\u8425\u9500\u589e\u52a014.0%\uff1b\u9009\u4e3e\u4e2d\uff0c\u9009\u7968\u4efd\u989d\u589e\u957f4.9%\u4f34\u968f\u7740\u865a\u5047\u4fe1\u606f\u589e\u52a022.3%\u548c\u6c11\u7cb9\u4e3b\u4e49\u8a00\u8bba\u589e\u52a012.5%\uff1b\u793e\u4ea4\u5a92\u4f53\u4e2d\uff0c\u53c2\u4e0e\u5ea6\u63d0\u53477.5%\u4f34\u968f\u7740\u865a\u5047\u4fe1\u606f\u589e\u52a0188.6%\u548c\u6709\u5bb3\u884c\u4e3a\u63a8\u5e7f\u589e\u52a016.3%\u3002", "conclusion": "\u7ade\u4e89\u6027\u4f18\u5316\u538b\u529b\u4f1a\u7cfb\u7edf\u6027\u5730\u4fb5\u8680\u6a21\u578b\u7684\u5bf9\u9f50\u6027\uff0c\u5f62\u6210\u6076\u6027\u7ade\u4e89\uff0cAI\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u9700\u8981\u66f4\u5f3a\u7684\u6cbb\u7406\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6fc0\u52b1\u673a\u5236\u6765\u9632\u6b62\u7ade\u4e89\u52a8\u6001\u7834\u574f\u793e\u4f1a\u4fe1\u4efb\u3002"}}
{"id": "2510.05530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05530", "abs": "https://arxiv.org/abs/2510.05530", "authors": ["Harshil Vejendla"], "title": "LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability", "comment": "MIT URTC 2025 Technical Paper (Oral), 5 pages, 3 figures", "summary": "Test-time adaptation (TTA) aims to adapt a pretrained model to distribution\nshifts using only unlabeled test data. While promising, existing methods like\nTent suffer from instability and can catastrophically forget the source\nknowledge, especially with small batch sizes or challenging corruptions. We\nargue that this arises from overly deterministic updates on a complex loss\nsurface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation\n(LATTA), a novel approach that regularizes adaptation through two key\nmechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient\nLangevin Dynamics (SGLD) to explore the local parameter space and escape poor\nlocal minima, and (2) a stable weight anchor that prevents the model from\ndiverging from its robust source pre-training. This combination allows LATTA to\nadapt effectively without sacrificing stability. Unlike prior Bayesian TTA\nmethods, LATTA requires no architectural changes or expensive Monte Carlo\npasses. We conduct extensive experiments on standard benchmarks, including\nRotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that\nLATTA significantly outperforms existing methods, including Tent, CoTTA, and\nEATA, setting a new state of the art for self-supervised TTA by improving\naverage accuracy on CIFAR-10-C by over 2% while simultaneously reducing\nperformance variance.", "AI": {"tldr": "LATTA\u662f\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6743\u91cd\u6270\u52a8\u548c\u7a33\u5b9a\u6743\u91cd\u951a\u70b9\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u6709\u6548\u9002\u5e94\u5206\u5e03\u504f\u79fb\uff0c\u65e0\u9700\u67b6\u6784\u6539\u53d8\u6216\u6602\u8d35\u7684\u8499\u7279\u5361\u6d1b\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff08\u5982Tent\uff09\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5c0f\u6279\u91cf\u6216\u590d\u6742\u635f\u574f\u60c5\u51b5\u4e0b\uff0c\u8fd9\u6e90\u4e8e\u590d\u6742\u635f\u5931\u8868\u9762\u4e0a\u8fc7\u4e8e\u786e\u5b9a\u6027\u7684\u66f4\u65b0\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u673a\u5236\uff1a(1) \u53d7\u968f\u673a\u68af\u5ea6Langevin\u52a8\u529b\u5b66\u542f\u53d1\u7684\u566a\u58f0\u6743\u91cd\u6270\u52a8\uff0c\u63a2\u7d22\u5c40\u90e8\u53c2\u6570\u7a7a\u95f4\u5e76\u9003\u79bb\u4e0d\u826f\u5c40\u90e8\u6700\u5c0f\u503c\uff1b(2) \u7a33\u5b9a\u6743\u91cd\u951a\u70b9\uff0c\u9632\u6b62\u6a21\u578b\u504f\u79bb\u5176\u9c81\u68d2\u7684\u6e90\u9884\u8bad\u7ec3\u3002", "result": "\u5728Rotated-MNIST\u548cCIFAR-10-C\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLATTA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08Tent\u3001CoTTA\u3001EATA\uff09\uff0c\u5728CIFAR-10-C\u4e0a\u5c06\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u8d85\u8fc72%\uff0c\u540c\u65f6\u964d\u4f4e\u6027\u80fd\u65b9\u5dee\u3002", "conclusion": "LATTA\u4e3a\u81ea\u76d1\u7763\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u8bbe\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u9002\u5e94\u800c\u4e0d\u4f1a\u727a\u7272\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.06008", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 86A10", "I.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.06008", "abs": "https://arxiv.org/abs/2510.06008", "authors": ["Moritz Alker", "David C. Schedl", "Andreas St\u00f6ckl"], "title": "Detection and Measurement of Hailstones with Multimodal Large Language Models", "comment": "6 pages, 5 figures, accepted at The 2nd International Conference on\n  Electrical and Computer Engineering Researches", "summary": "This study examines the use of social media and news images to detect and\nmeasure hailstones, utilizing pre-trained multimodal large language models. The\ndataset for this study comprises 474 crowdsourced images of hailstones from\ndocumented hail events in Austria, which occurred between January 2022 and\nSeptember 2024. These hailstones have maximum diameters ranging from 2 to 11cm.\nWe estimate the hail diameters and compare four different models utilizing\none-stage and two-stage prompting strategies. The latter utilizes additional\nsize cues from reference objects, such as human hands, within the image. Our\nresults show that pretrained models already have the potential to measure\nhailstone diameters from images with an average mean absolute error of 1.12cm\nfor the best model. In comparison to a single-stage prompt, two-stage prompting\nimproves the reliability of most models. Our study suggests that these\noff-the-shelf models, even without fine-tuning, can complement traditional hail\nsensors by extracting meaningful and spatially dense information from social\nmedia imagery, enabling faster and more detailed assessments of severe weather\nevents. The automated real-time image harvesting from social media and other\nsources remains an open task, but it will make our approach directly applicable\nto future hail events.", "AI": {"tldr": "\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u793e\u4ea4\u5a92\u4f53\u548c\u65b0\u95fb\u56fe\u50cf\u4e2d\u68c0\u6d4b\u548c\u6d4b\u91cf\u51b0\u96f9\u76f4\u5f84\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63d0\u793a\u7b56\u7565\u63d0\u9ad8\u6d4b\u91cf\u53ef\u9760\u6027\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.12\u5398\u7c73\u3002", "motivation": "\u4f20\u7edf\u51b0\u96f9\u4f20\u611f\u5668\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u9700\u8981\u4ece\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u4e2d\u63d0\u53d6\u66f4\u5bc6\u96c6\u7684\u7a7a\u95f4\u4fe1\u606f\u6765\u5feb\u901f\u8bc4\u4f30\u6076\u52a3\u5929\u6c14\u4e8b\u4ef6\u3002", "method": "\u4f7f\u7528474\u5f20\u5965\u5730\u5229\u51b0\u96f9\u4e8b\u4ef6\u7684\u4f17\u5305\u56fe\u50cf\uff0c\u91c7\u7528\u4e00\u9636\u6bb5\u548c\u4e24\u9636\u6bb5\u63d0\u793a\u7b56\u7565\uff0c\u5229\u7528\u56fe\u50cf\u4e2d\u7684\u53c2\u8003\u7269\u4f53\uff08\u5982\u4eba\u624b\uff09\u4f5c\u4e3a\u5c3a\u5bf8\u7ebf\u7d22\u6765\u4f30\u8ba1\u51b0\u96f9\u76f4\u5f84\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4ece\u56fe\u50cf\u4e2d\u6d4b\u91cf\u51b0\u96f9\u76f4\u5f84\uff0c\u6700\u4f73\u6a21\u578b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.12\u5398\u7c73\uff0c\u4e24\u9636\u6bb5\u63d0\u793a\u63d0\u9ad8\u4e86\u5927\u591a\u6570\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u73b0\u6210\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u8865\u5145\u4f20\u7edf\u51b0\u96f9\u4f20\u611f\u5668\uff0c\u4ece\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u66f4\u5feb\u66f4\u8be6\u7ec6\u7684\u6076\u52a3\u5929\u6c14\u8bc4\u4f30\u3002"}}
{"id": "2510.06026", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06026", "abs": "https://arxiv.org/abs/2510.06026", "authors": ["An Thi Nguyen", "Radina Stoykova", "Eric Arazo"], "title": "Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context", "comment": "10 pages, accepted to AIES 2025", "summary": "Generic instance search models can dramatically reduce the manual effort\nrequired to analyze vast surveillance footage during criminal investigations by\nretrieving specific objects of interest to law enforcement. However, our\nresearch reveals an unintended emergent capability: through overlearning, these\nmodels can single out specific individuals even when trained on datasets\nwithout human subjects. This capability raises concerns regarding\nidentification and profiling of individuals based on their personal data, while\nthere is currently no clear standard on how de-identification can be achieved.\nWe evaluate two technical safeguards to curtail a model's person\nre-identification capacity: index exclusion and confusion loss. Our experiments\ndemonstrate that combining these approaches can reduce person re-identification\naccuracy to below 2% while maintaining 82% of retrieval performance for\nnon-person objects. However, we identify critical vulnerabilities in these\nmitigations, including potential circumvention using partial person images.\nThese findings highlight urgent regulatory questions at the intersection of AI\ngovernance and data protection: How should we classify and regulate systems\nwith emergent identification capabilities? And what technical standards should\nbe required to prevent identification capabilities from developing in seemingly\nbenign applications?", "AI": {"tldr": "\u901a\u7528\u5b9e\u4f8b\u641c\u7d22\u6a21\u578b\u5728\u72af\u7f6a\u8c03\u67e5\u4e2d\u80fd\u6709\u6548\u68c0\u7d22\u7279\u5b9a\u5bf9\u8c61\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5373\u4f7f\u5728\u6ca1\u6709\u4eba\u7c7b\u6570\u636e\u7684\u8bad\u7ec3\u96c6\u4e0a\u4e5f\u4f1a\u610f\u5916\u83b7\u5f97\u8bc6\u522b\u7279\u5b9a\u4e2a\u4eba\u7684\u80fd\u529b\uff0c\u8fd9\u5f15\u53d1\u4e86\u9690\u79c1\u62c5\u5fe7\u3002\u4f5c\u8005\u8bc4\u4f30\u4e86\u4e24\u79cd\u6280\u672f\u9632\u62a4\u63aa\u65bd\uff0c\u53d1\u73b0\u7ec4\u5408\u4f7f\u7528\u53ef\u5c06\u4eba\u5458\u91cd\u8bc6\u522b\u51c6\u786e\u7387\u964d\u81f32%\u4ee5\u4e0b\uff0c\u4f46\u9632\u62a4\u63aa\u65bd\u4ecd\u5b58\u5728\u6f0f\u6d1e\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u53d1\u73b0\u901a\u7528\u5b9e\u4f8b\u641c\u7d22\u6a21\u578b\u5b58\u5728\u610f\u5916\u51fa\u73b0\u7684\u4eba\u5458\u8bc6\u522b\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u4eba\u7c7b\u6570\u636e\u7684\u8bad\u7ec3\u96c6\u4e0a\u4e5f\u80fd\u8bc6\u522b\u7279\u5b9a\u4e2a\u4eba\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u4e2a\u4eba\u6570\u636e\u8bc6\u522b\u548c\u753b\u50cf\u7684\u9690\u79c1\u62c5\u5fe7\uff0c\u76ee\u524d\u7f3a\u4e4f\u660e\u786e\u7684\u53bb\u6807\u8bc6\u5316\u6807\u51c6\u3002", "method": "\u8bc4\u4f30\u4e86\u4e24\u79cd\u6280\u672f\u9632\u62a4\u63aa\u65bd\uff1a\u7d22\u5f15\u6392\u9664\u548c\u6df7\u6dc6\u635f\u5931\u3002\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u8fd9\u4e9b\u65b9\u6cd5\u5728\u51cf\u5c11\u4eba\u5458\u91cd\u8bc6\u522b\u80fd\u529b\u65b9\u9762\u7684\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u975e\u4eba\u7269\u4f53\u7684\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ec4\u5408\u4f7f\u7528\u7d22\u5f15\u6392\u9664\u548c\u6df7\u6dc6\u635f\u5931\u53ef\u4ee5\u5c06\u4eba\u5458\u91cd\u8bc6\u522b\u51c6\u786e\u7387\u964d\u4f4e\u52302%\u4ee5\u4e0b\uff0c\u540c\u65f6\u4fdd\u630182%\u7684\u975e\u4eba\u7269\u4f53\u68c0\u7d22\u6027\u80fd\u3002\u4f46\u53d1\u73b0\u8fd9\u4e9b\u7f13\u89e3\u63aa\u65bd\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u5305\u62ec\u4f7f\u7528\u90e8\u5206\u4eba\u7269\u56fe\u50cf\u53ef\u80fd\u7ed5\u8fc7\u9632\u62a4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86AI\u6cbb\u7406\u548c\u6570\u636e\u4fdd\u62a4\u4ea4\u53c9\u9886\u57df\u7684\u7d27\u8feb\u76d1\u7ba1\u95ee\u9898\uff1a\u5982\u4f55\u5206\u7c7b\u548c\u76d1\u7ba1\u5177\u6709\u7a81\u53d1\u8bc6\u522b\u80fd\u529b\u7684\u7cfb\u7edf\uff1f\u4ee5\u53ca\u5e94\u8be5\u8981\u6c42\u4ec0\u4e48\u6280\u672f\u6807\u51c6\u6765\u9632\u6b62\u770b\u4f3c\u826f\u6027\u5e94\u7528\u4e2d\u51fa\u73b0\u8bc6\u522b\u80fd\u529b\uff1f"}}
{"id": "2510.06040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06040", "abs": "https://arxiv.org/abs/2510.06040", "authors": ["Xinye Cao", "Hongcan Guo", "Jiawen Qian", "Guoshun Nan", "Chao Wang", "Yuqi Pan", "Tianhao Hou", "Xiaojuan Wang", "Yutong Gao"], "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization", "comment": "Accepted by ICCV 2025", "summary": "Understanding hour-long videos with multi-modal large language models\n(MM-LLMs) enriches the landscape of human-centered AI applications. However,\nfor end-to-end video understanding with LLMs, uniformly sampling video frames\nresults in LLMs being overwhelmed by a vast amount of irrelevant information as\nvideo length increases. Existing hierarchical key frame extraction methods\nimprove the accuracy of video understanding but still face two critical\nchallenges. 1) How can the interference of extensive redundant information in\nlong videos be mitigated? 2) How can a model dynamically adapt to complex\nhierarchical structures while accurately identifying key frames? To address\nthese issues, we propose VideoMiner, which iteratively segments, captions, and\nclusters long videos, forming a hierarchical tree structure. The proposed\nVideoMiner progresses from long videos to events to frames while preserving\ntemporal coherence, effectively addressing the first challenge. To precisely\nlocate key frames, we introduce T-GRPO, a tree-based group relative policy\noptimization in reinforcement learning method that guides the exploration of\nthe VideoMiner. The proposed T-GRPO is specifically designed for tree\nstructures, integrating spatiotemporal information at the event level while\nbeing guided by the question, thus solving the second challenge. We achieve\nsuperior performance in all long-video understanding tasks and uncover several\ninteresting insights. Our proposed T-GRPO surprisingly incentivizes the model\nto spontaneously generate a reasoning chain. Additionally, the designed tree\ngrowth auxin dynamically adjusts the expansion depth, obtaining accuracy and\nefficiency gains. The code is publicly available at\nhttps://github.com/caoxinye/VideoMiner.", "AI": {"tldr": "VideoMiner\u901a\u8fc7\u8fed\u4ee3\u5206\u5272\u3001\u63cf\u8ff0\u548c\u805a\u7c7b\u957f\u89c6\u9891\u5f62\u6210\u5c42\u6b21\u6811\u7ed3\u6784\uff0c\u7ed3\u5408T-GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5197\u4f59\u4fe1\u606f\u5e72\u6270\u548c\u5173\u952e\u5e27\u8bc6\u522b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u5982\u4f55\u51cf\u8f7b\u5927\u91cf\u5197\u4f59\u4fe1\u606f\u7684\u5e72\u6270\uff1b2) \u5982\u4f55\u52a8\u6001\u9002\u5e94\u590d\u6742\u5c42\u6b21\u7ed3\u6784\u5e76\u51c6\u786e\u8bc6\u522b\u5173\u952e\u5e27\u3002", "method": "\u63d0\u51faVideoMiner\u7cfb\u7edf\uff0c\u8fed\u4ee3\u5206\u5272\u3001\u63cf\u8ff0\u548c\u805a\u7c7b\u957f\u89c6\u9891\u5f62\u6210\u5c42\u6b21\u6811\u7ed3\u6784\uff0c\u5e76\u5f15\u5165T-GRPO\uff08\u57fa\u4e8e\u6811\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6307\u5bfc\u5173\u952e\u5e27\u5b9a\u4f4d\u3002", "result": "\u5728\u6240\u6709\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0cT-GRPO\u610f\u5916\u5730\u6fc0\u52b1\u6a21\u578b\u81ea\u53d1\u751f\u6210\u63a8\u7406\u94fe\uff0c\u6811\u751f\u957f\u8f85\u52a9\u7d20\u52a8\u6001\u8c03\u6574\u6269\u5c55\u6df1\u5ea6\uff0c\u83b7\u5f97\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "VideoMiner\u901a\u8fc7\u5c42\u6b21\u6811\u7ed3\u6784\u548cT-GRPO\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u5206\u6790\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05799", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.05799", "abs": "https://arxiv.org/abs/2510.05799", "authors": ["Rikuto Kotoge", "Yuichi Sasaki"], "title": "Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech", "comment": null, "summary": "Aligning text-to-speech (TTS) system outputs with human feedback through\npreference optimization has been shown to effectively improve the robustness\nand naturalness of language model-based TTS models. Current approaches\nprimarily require paired desirable and undesirable samples at the utterance\nlevel. However, such pairs are often limited in TTS output data, and\nutterance-level formulation prevents fine-grained token-level optimization\nneeded for accurate pronunciation alignment. In this study, we propose TKTO\nthat eliminates the need for paired data, enabling a more data-efficient\ntraining paradigm, and directly targets token-level units, automatically\nproviding fine-grained alignment signals without token-level annotations. TKTO\nimproves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%,\nautomatically assigning 12.8 times stronger reward to targeted tokens.", "AI": {"tldr": "TKTO\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u6570\u636e\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u76f4\u63a5\u9488\u5bf9token\u7ea7\u5355\u5143\uff0c\u81ea\u52a8\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4fe1\u53f7\u800c\u65e0\u9700token\u7ea7\u6807\u6ce8\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684TTS\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u9700\u8981utterance\u7ea7\u522b\u7684\u914d\u5bf9\u6837\u672c\uff0c\u4f46\u8fd9\u79cd\u914d\u5bf9\u6570\u636e\u5728TTS\u8f93\u51fa\u4e2d\u5f80\u5f80\u6709\u9650\uff0c\u4e14utterance\u7ea7\u522b\u7684\u8868\u8ff0\u963b\u788d\u4e86\u51c6\u786e\u53d1\u97f3\u5bf9\u9f50\u6240\u9700\u7684\u7ec6\u7c92\u5ea6token\u7ea7\u4f18\u5316\u3002", "method": "\u63d0\u51faTKTO\u65b9\u6cd5\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\uff0c\u76f4\u63a5\u9488\u5bf9token\u7ea7\u5355\u5143\uff0c\u81ea\u52a8\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4fe1\u53f7\u800c\u4e0d\u9700\u8981token\u7ea7\u6807\u6ce8\u3002", "result": "TKTO\u5c06\u5177\u6709\u6311\u6218\u6027\u7684\u65e5\u8bedTTS\u51c6\u786e\u7387\u63d0\u9ad8\u4e8639%\uff0cCER\u964d\u4f4e\u4e8654%\uff0c\u81ea\u52a8\u4e3a\u76ee\u6807token\u5206\u914d\u4e8612.8\u500d\u66f4\u5f3a\u7684\u5956\u52b1\u3002", "conclusion": "TKTO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86TTS\u7cfb\u7edf\u4e2d\u914d\u5bf9\u6570\u636e\u6709\u9650\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u56f0\u96be\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d1\u97f3\u51c6\u786e\u6027\u548c\u81ea\u7136\u5ea6\u3002"}}
{"id": "2510.05856", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05856", "abs": "https://arxiv.org/abs/2510.05856", "authors": ["Egor Surkov", "Dmitry Osin", "Evgeny Burnaev", "Egor Shvetsov"], "title": "How to model Human Actions distribution with Event Sequence Data", "comment": "9 pages main text + 2 pages references + 6 pages appendix, 10\n  figures, 3 tables. Preprint version", "summary": "This paper studies forecasting of the future distribution of events in human\naction sequences, a task essential in domains like retail, finance, healthcare,\nand recommendation systems where the precise temporal order is often less\ncritical than the set of outcomes. We challenge the dominant autoregressive\nparadigm and investigate whether explicitly modeling the future distribution or\norder-invariant multi-token approaches outperform order-preserving methods. We\nanalyze local order invariance and introduce a KL-based metric to quantify\ntemporal drift. We find that a simple explicit distribution forecasting\nobjective consistently surpasses complex implicit baselines. We further\ndemonstrate that mode collapse of predicted categories is primarily driven by\ndistributional imbalance. This work provides a principled framework for\nselecting modeling strategies and offers practical guidance for building more\naccurate and robust forecasting systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u7c7b\u884c\u4e3a\u5e8f\u5217\u4e2d\u4e8b\u4ef6\u672a\u6765\u5206\u5e03\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u6311\u6218\u4e86\u4e3b\u6d41\u7684\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u53d1\u73b0\u663e\u5f0f\u5206\u5e03\u9884\u6d4b\u65b9\u6cd5\u4f18\u4e8e\u9690\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u96f6\u552e\u3001\u91d1\u878d\u3001\u533b\u7597\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\uff0c\u9884\u6d4b\u4e8b\u4ef6\u672a\u6765\u5206\u5e03\u6bd4\u7cbe\u786e\u7684\u65f6\u95f4\u987a\u5e8f\u66f4\u91cd\u8981\uff0c\u9700\u8981\u63a2\u7d22\u6bd4\u81ea\u56de\u5f52\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u5efa\u6a21\u7b56\u7565\u3002", "method": "\u5206\u6790\u5c40\u90e8\u987a\u5e8f\u4e0d\u53d8\u6027\uff0c\u5f15\u5165\u57fa\u4e8eKL\u6563\u5ea6\u7684\u6307\u6807\u91cf\u5316\u65f6\u95f4\u6f02\u79fb\uff0c\u6bd4\u8f83\u663e\u5f0f\u5206\u5e03\u9884\u6d4b\u4e0e\u9690\u5f0f\u591a\u6807\u8bb0\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u7b80\u5355\u7684\u663e\u5f0f\u5206\u5e03\u9884\u6d4b\u76ee\u6807\u6301\u7eed\u4f18\u4e8e\u590d\u6742\u7684\u9690\u5f0f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9884\u6d4b\u7c7b\u522b\u7684\u6a21\u5f0f\u5d29\u6e83\u4e3b\u8981\u7531\u5206\u5e03\u4e0d\u5e73\u8861\u9a71\u52a8\u3002", "conclusion": "\u4e3a\u9009\u62e9\u5efa\u6a21\u7b56\u7565\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u9884\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.05901", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05901", "abs": "https://arxiv.org/abs/2510.05901", "authors": ["Martin Benfeghoul", "Teresa Delgado", "Adnan Oomerjee", "Haitham Bou Ammar", "Jun Wang", "Zafeirios Fountas"], "title": "Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods", "comment": null, "summary": "Transformers' quadratic computational complexity limits their scalability\ndespite remarkable performance. While linear attention reduces this to linear\ncomplexity, pre-training such models from scratch remains, in most cases,\nprohibitively expensive. Recent post-training linearisation methods convert\npre-trained Transformers to linear models efficiently, often using hybrid\napproaches that combine linear attention with sliding-window softmax. We\nidentify a critical flaw: existing hybrid methods inadvertently bypass the\nlinear component, relying almost entirely on SWA. Component-level diagnostics\nreveal this previously undetected behaviour stems from overlooked evaluation\npractices on common-sense benchmarks. We propose three solutions to ensure\nbalanced component usage: (i) inference-time hybridisation of linear-only\nconversions with sliding-window softmax; (ii) HedgeCATs, combining\nattention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled\nSliding-window Dropout (SSD), which stochastically suppresses the softmax\nbranch during training to prevent component collapse. Our methods maintain\ncomputational efficiency while recovering most base model performance and\nensuring genuine linear attention adoption, restoring the validity of\nperformance attributions in hybrid conversions.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u73b0\u6709\u6df7\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff1a\u7ebf\u6027\u7ec4\u4ef6\u88ab\u65e0\u610f\u7ed5\u8fc7\uff0c\u4e3b\u8981\u4f9d\u8d56\u6ed1\u52a8\u7a97\u53e3softmax\u3002\u63d0\u51fa\u4e86\u4e09\u79cd\u89e3\u51b3\u65b9\u6848\u6765\u786e\u4fdd\u7ec4\u4ef6\u5e73\u8861\u4f7f\u7528\uff0c\u6062\u590d\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u771f\u6b63\u91c7\u7528\u3002", "motivation": "Transformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\uff0c\u800c\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u6df7\u5408\u65b9\u6cd5\u5728\u5b9e\u9645\u8bc4\u4f30\u4e2d\u5b58\u5728\u7ebf\u6027\u7ec4\u4ef6\u88ab\u7ed5\u8fc7\u7684\u7f3a\u9677\uff0c\u5bfc\u81f4\u6027\u80fd\u5f52\u56e0\u5931\u771f\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\uff1a(1) \u63a8\u7406\u65f6\u6df7\u5408\u7ebf\u6027\u8f6c\u6362\u4e0e\u6ed1\u52a8\u7a97\u53e3softmax\uff1b(2) HedgeCATs\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u6743\u91cd\u8f6c\u79fb\u548c\u9488\u5bf9\u6027LoRA\u5fae\u8c03\uff1b(3) \u8ba1\u5212\u6ed1\u52a8\u7a97\u53e3dropout(SSD)\uff0c\u5728\u8bad\u7ec3\u4e2d\u968f\u673a\u6291\u5236softmax\u5206\u652f\u4ee5\u9632\u6b62\u7ec4\u4ef6\u5d29\u6e83\u3002", "result": "\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6062\u590d\u4e86\u5927\u90e8\u5206\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u786e\u4fdd\u771f\u6b63\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u91c7\u7528\uff0c\u6062\u590d\u4e86\u6df7\u5408\u8f6c\u6362\u4e2d\u6027\u80fd\u5f52\u56e0\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u4e09\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6df7\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u4e2d\u7ebf\u6027\u7ec4\u4ef6\u88ab\u7ed5\u8fc7\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u4e86\u7ec4\u4ef6\u5e73\u8861\u4f7f\u7528\u548c\u6027\u80fd\u5f52\u56e0\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06128", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06128", "abs": "https://arxiv.org/abs/2510.06128", "authors": ["Muhammad Dehan Al Kautsar", "Fajri Koto"], "title": "Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer", "comment": "18 pages, 25 tables, 7 figures", "summary": "Tokenization defines the foundation of multilingual language models by\ndetermining how words are represented and shared across languages. However,\nexisting methods often fail to support effective cross-lingual transfer because\nsemantically equivalent words are assigned distinct embeddings. For example, \"I\neat rice\" in English and \"Ina cin shinkafa\" in Hausa are typically mapped to\ndifferent vocabulary indices, preventing shared representations and limiting\ncross-lingual generalization. We introduce parallel tokenizers. This new\nframework trains tokenizers monolingually and then aligns their vocabularies\nexhaustively using bilingual dictionaries or word-to-word translation, ensuring\nconsistent indices for semantically equivalent words. This alignment enforces a\nshared semantic space across languages while naturally improving fertility\nbalance. To assess their effectiveness, we pretrain a transformer encoder from\nscratch on thirteen low-resource languages and evaluate it on sentiment\nanalysis, hate speech detection, emotion classification, and sentence embedding\nsimilarity. Across all tasks, models trained with parallel tokenizers\noutperform conventional multilingual baselines, confirming that rethinking\ntokenization is essential for advancing multilingual representation\nlearning--especially in low-resource settings.", "AI": {"tldr": "\u63d0\u51fa\u5e73\u884c\u5206\u8bcd\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u6c47\u5bf9\u9f50\u5b9e\u73b0\u8de8\u8bed\u8a00\u8bed\u4e49\u7b49\u4ef7\u8bcd\u7684\u7edf\u4e00\u8868\u793a\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u6027\u80fd", "motivation": "\u73b0\u6709\u5206\u8bcd\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u652f\u6301\u8de8\u8bed\u8a00\u8fc1\u79fb\uff0c\u56e0\u4e3a\u8bed\u4e49\u7b49\u4ef7\u7684\u8bcd\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u88ab\u5206\u914d\u4e0d\u540c\u7684\u5d4c\u5165\u8868\u793a\uff0c\u9650\u5236\u4e86\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b", "method": "\u8bad\u7ec3\u5355\u8bed\u8a00\u5206\u8bcd\u5668\uff0c\u7136\u540e\u4f7f\u7528\u53cc\u8bed\u8bcd\u5178\u6216\u8bcd\u5bf9\u8bcd\u7ffb\u8bd1\u5bf9\u8bcd\u6c47\u8868\u8fdb\u884c\u5f7b\u5e95\u5bf9\u9f50\uff0c\u786e\u4fdd\u8bed\u4e49\u7b49\u4ef7\u8bcd\u5177\u6709\u4e00\u81f4\u7684\u7d22\u5f15", "result": "\u572813\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u60c5\u611f\u5206\u6790\u3001\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3001\u60c5\u611f\u5206\u7c7b\u548c\u53e5\u5b50\u5d4c\u5165\u76f8\u4f3c\u5ea6\u7b49\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u591a\u8bed\u8a00\u57fa\u7ebf", "conclusion": "\u91cd\u65b0\u601d\u8003\u5206\u8bcd\u65b9\u6cd5\u5bf9\u4e8e\u63a8\u8fdb\u591a\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b"}}
{"id": "2510.06029", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06029", "abs": "https://arxiv.org/abs/2510.06029", "authors": ["Guillaume Godin"], "title": "Fast Leave-One-Out Approximation from Fragment-Target Prevalence Vectors (molFTP) : From Dummy Masking to Key-LOO for Leakage-Free Feature Construction", "comment": "28 pages, 21 figures, 3 tables", "summary": "We introduce molFTP (molecular fragment-target prevalence), a compact\nrepresentation that delivers strong predictive performance. To prevent feature\nleakage across cross-validation folds, we implement a dummy-masking procedure\nthat removes information about fragments present in the held-out molecules. We\nfurther show that key leave-one-out (key-loo) closely approximates true\nmolecule-level leave-one-out (LOO), with deviation below 8% on our datasets.\nThis enables near full data training while preserving unbiased cross-validation\nestimates of model performance. Overall, molFTP provides a fast,\nleakage-resistant fragment-target prevalence vectorization with practical\nsafeguards (dummy masking or key-LOO) that approximate LOO at a fraction of its\ncost.", "AI": {"tldr": "molFTP\u662f\u4e00\u79cd\u7d27\u51d1\u7684\u5206\u5b50\u7247\u6bb5-\u9776\u70b9\u6d41\u884c\u5ea6\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u865a\u62df\u63a9\u7801\u9632\u6b62\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u7684\u7279\u5f81\u6cc4\u6f0f\uff0c\u5e76\u4f7f\u7528\u5173\u952e\u7559\u4e00\u6cd5\u8fd1\u4f3c\u771f\u5b9e\u5206\u5b50\u7ea7\u7559\u4e00\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u65e0\u504f\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u4ea4\u53c9\u9a8c\u8bc1\u65f6\u7684\u7279\u5f81\u6cc4\u6f0f\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u7559\u4e00\u6cd5\u9a8c\u8bc1\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51famolFTP\u8868\u793a\u65b9\u6cd5\uff0c\u91c7\u7528\u865a\u62df\u63a9\u7801\u6280\u672f\u9632\u6b62\u7279\u5f81\u6cc4\u6f0f\uff0c\u5e76\u4f7f\u7528\u5173\u952e\u7559\u4e00\u6cd5\u8fd1\u4f3c\u771f\u5b9e\u5206\u5b50\u7ea7\u7559\u4e00\u6cd5\u9a8c\u8bc1\u3002", "result": "\u5173\u952e\u7559\u4e00\u6cd5\u4e0e\u771f\u5b9e\u5206\u5b50\u7ea7\u7559\u4e00\u6cd5\u7684\u504f\u5dee\u4f4e\u4e8e8%\uff0c\u80fd\u591f\u4ee5\u8f83\u4f4e\u6210\u672c\u5b9e\u73b0\u8fd1\u4f3c\u5168\u6570\u636e\u8bad\u7ec3\u548c\u65e0\u504f\u4ea4\u53c9\u9a8c\u8bc1\u6027\u80fd\u8bc4\u4f30\u3002", "conclusion": "molFTP\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u6297\u6cc4\u6f0f\u7684\u7247\u6bb5-\u9776\u70b9\u6d41\u884c\u5ea6\u5411\u91cf\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u7528\u7684\u4fdd\u62a4\u673a\u5236\uff0c\u80fd\u4ee5\u8f83\u4f4e\u6210\u672c\u8fd1\u4f3c\u7559\u4e00\u6cd5\u9a8c\u8bc1\u3002"}}
{"id": "2510.06050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06050", "abs": "https://arxiv.org/abs/2510.06050", "authors": ["David L\u00fcdke", "Marten Lienen", "Marcel Kollovieh", "Stephan G\u00fcnnemann"], "title": "Edit-Based Flow Matching for Temporal Point Processes", "comment": null, "summary": "Temporal point processes (TPPs) are a fundamental tool for modeling event\nsequences in continuous time, but most existing approaches rely on\nautoregressive parameterizations that are limited by their sequential sampling.\nRecent non-autoregressive, diffusion-style models mitigate these issues by\njointly interpolating between noise and data through event insertions and\ndeletions in a discrete Markov chain. In this work, we generalize this\nperspective and introduce an Edit Flow process for TPPs that transports noise\nto data via insert, delete, and substitute edit operations. By learning the\ninstantaneous edit rates within a continuous-time Markov chain framework, we\nattain a flexible and efficient model that effectively reduces the total number\nof necessary edit operations during generation. Empirical results demonstrate\nthe generative flexibility of our unconditionally trained model in a wide range\nof unconditional and conditional generation tasks on benchmark TPPs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u7684Edit Flow\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d2\u5165\u3001\u5220\u9664\u548c\u66ff\u6362\u7f16\u8f91\u64cd\u4f5c\u5c06\u566a\u58f0\u8f6c\u5316\u4e3a\u6570\u636e\uff0c\u5728\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u6846\u67b6\u4e2d\u5b66\u4e60\u77ac\u65f6\u7f16\u8f91\u7387\uff0c\u4ece\u800c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u51cf\u5c11\u5fc5\u8981\u7684\u7f16\u8f91\u64cd\u4f5c\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u81ea\u56de\u5f52\u53c2\u6570\u5316\uff0c\u53d7\u5230\u987a\u5e8f\u91c7\u6837\u7684\u9650\u5236\u3002\u6700\u8fd1\u7684\u975e\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u901a\u8fc7\u79bb\u6563\u9a6c\u5c14\u53ef\u592b\u94fe\u4e2d\u7684\u4e8b\u4ef6\u63d2\u5165\u548c\u5220\u9664\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u5f15\u5165Edit Flow\u8fc7\u7a0b\uff0c\u901a\u8fc7\u63d2\u5165\u3001\u5220\u9664\u548c\u66ff\u6362\u7f16\u8f91\u64cd\u4f5c\u5728\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u6846\u67b6\u4e2d\u5b66\u4e60\u77ac\u65f6\u7f16\u8f91\u7387\uff0c\u5c06\u566a\u58f0\u4f20\u8f93\u5230\u6570\u636e\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u6761\u4ef6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u57fa\u51c6\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u7684\u5404\u79cd\u65e0\u6761\u4ef6\u548c\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u751f\u6210\u7075\u6d3b\u6027\u3002", "conclusion": "Edit Flow\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u9ad8\u6548\u7684\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u6a21\u578b\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u5fc5\u8981\u7684\u7f16\u8f91\u64cd\u4f5c\u6570\u91cf\u3002"}}
