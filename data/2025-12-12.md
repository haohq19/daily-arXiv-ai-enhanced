<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 本文提出了一种基于脉冲神经网络的神经形态眼动追踪模型，通过将高性能事件驱动眼动追踪模型转换为SNN架构，在保持精度的同时大幅提升了能效。


<details>
  <summary>Details</summary>
Motivation: 可穿戴系统的眼动追踪需要低延迟和毫瓦级功耗，但传统基于帧的管道存在运动模糊、计算成本高和时间分辨率有限的问题。神经形态传感器和脉冲神经网络提供了有前景的替代方案，但现有SNN方法要么过于专门化，要么性能不及现代ANN架构。

Method: 开发了高性能事件驱动眼动追踪模型的神经形态版本，用轻量级LIF层替换其循环和注意力模块，并利用深度可分离卷积来降低模型复杂度。

Result: 模型获得3.7-4.1像素的平均误差，接近专用神经形态系统Retina（3.24像素）的精度，同时相比最接近的ANN变体，模型大小减少20倍，理论计算量减少850倍。这些高效变体预计在1kHz下以3.9-4.9毫瓦功耗和3毫秒延迟运行。

Conclusion: 高性能事件驱动眼动追踪架构可以重新设计为SNN，在保持适合实时可穿戴部署的精度的同时，获得显著的效率提升。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

</details>


### [2] [Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering](https://arxiv.org/abs/2512.10327)
*Cai Xu,Jinlong Liu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: ISMVC提出了一种基于信息量的选择性插补方法，用于处理不平衡缺失的多视图聚类问题，通过评估缺失位置的信息量选择性插补，结合变分自编码器和混合高斯先验学习聚类友好的潜在表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不完整多视图数据时存在局限：基于插补的方法会引入噪声和偏差，而免插补方法在严重不完整性下缺乏跨视图互补性。需要一种更稳健的方法来处理不平衡缺失场景。

Method: 1) 基于视图内相似性和跨视图一致性评估每个缺失位置的信息量，只在有足够支持时选择性插补；2) 结合变分自编码器和混合高斯先验学习聚类友好的潜在表示；3) 在分布层面进行插补，稳定后验分布聚合并显式建模插补不确定性。

Result: 在多个基准数据集上的实验表明，该方法在不平衡缺失场景下优于现有的基于插补和免插补方法。该方法轻量级、数据驱动、模型无关，可作为插件模块集成到现有IMC模型中。

Conclusion: ISMVC通过选择性插补和分布级建模，有效解决了不完整多视图聚类中的噪声和偏差问题，在严重不平衡缺失场景下表现出优越性能，为多视图聚类提供了更稳健的解决方案。

Abstract: Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.

</details>


### [3] [Topology-Agnostic Animal Motion Generation from Text Prompt](https://arxiv.org/abs/2512.10352)
*Keyi Chen,Mingze Sun,Zhenyu Liu,Zhangquan Chen,Ruqi Huang*

Main category: cs.CV

TL;DR: OmniZoo：首个大规模异质动物运动数据集与通用生成框架，支持任意骨骼拓扑的文本驱动运动生成


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法大多依赖固定骨骼模板，无法泛化到不同或扰动拓扑的骨骼结构。缺乏大规模异质动物运动数据和统一生成框架是核心限制。

Method: 1) 引入OmniZoo数据集：涵盖140个物种、32,979个序列，带多模态标注；2) 提出通用自回归运动生成框架：包含拓扑感知骨骼嵌入模块，将任意骨骼的几何和结构属性编码到共享标记空间，与文本语义融合；3) 支持文本提示+目标骨骼输入，生成时序连贯、物理合理、语义对齐的运动，并实现跨物种运动风格迁移。

Result: 构建了首个大规模异质动物运动数据集OmniZoo，并提出通用框架能够为任意骨骼拓扑生成文本驱动的运动，实现跨物种运动风格迁移。

Conclusion: OmniZoo解决了当前运动生成方法在数据规模和拓扑泛化方面的核心限制，为任意骨骼结构的文本驱动运动生成提供了统一解决方案，具有广泛的应用前景。

Abstract: Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.

</details>


### [4] [Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos](https://arxiv.org/abs/2512.10363)
*Mingyu Jeon,Jisoo Yang,Sungjin Han,Jinkwon Hwang,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: P2S是首个无需训练的零样本长视频时刻检索框架，通过自适应跨度生成和查询分解技术，在小时级视频中超越有监督SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决长视频时刻检索中的计算效率问题。现有方法面临搜索阶段候选爆炸和精炼阶段需要高成本VLM验证的双重挑战，导致计算开销大且泛化能力差。

Method: 提出P2S框架：1）自适应跨度生成器防止搜索阶段候选爆炸；2）查询分解技术无需高成本VLM验证即可精炼候选片段。

Result: 在MAD数据集上R5@0.1指标提升3.7%，是首个能在小时级视频中进行时序定位的零样本框架，显著超越有监督SOTA方法。

Conclusion: P2S通过创新的无训练方法有效解决了长视频时刻检索的计算效率问题，为视频理解领域提供了高效且可扩展的解决方案。

Abstract: Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: HB-SGE是一种结合重球动量和预测梯度外推的鲁棒一阶优化方法，在病态和非凸问题上比NAG和标准动量方法更稳定，但在良好条件下可能不如NAG快。


<details>
  <summary>Details</summary>
Motivation: NAG等加速梯度方法在良好条件下收敛快，但在病态或非凸问题上容易因动量积累而发散。需要一种既能自适应加速又能保持稳定性的方法。

Method: HB-SGE结合重球动量和预测梯度外推，使用局部泰勒近似估计未来梯度方向，而不是累积历史梯度。该方法具有O(d)内存开销，超参数与标准动量方法相同。

Result: 在病态二次问题(条件数κ=50)上，HB-SGE在119次迭代收敛，而SGD和NAG发散。在非凸Rosenbrock函数上，HB-SGE在2718次迭代收敛，而经典动量方法在10步内发散。HB-SGE在强凸函数上有收敛保证。

Conclusion: HB-SGE提供了一种鲁棒的优化方法，在病态和非凸问题上比NAG和标准动量方法更稳定，虽然良好条件下可能不如NAG快，但在多样化问题上比SGD有加速效果。

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [6] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: CHyLL提出了一种在潜在空间中学习混合系统连续神经表示的方法，无需轨迹分割、事件函数或模式切换，通过将状态空间重构为分段光滑商流形来实现连续流学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法学习每个离散模式的动态，但面临模式切换和流不连续性的组合挑战，需要一种能处理混合系统连续和离散动态的新方法。

Method: 将重置映射在守卫表面粘合状态空间，将状态空间重构为分段光滑商流形，使流在空间上连续；基于微分拓扑的嵌入定理，在高维空间中同时学习无奇点的神经嵌入和连续流。

Result: CHyLL能准确预测混合系统的流，具有优越的准确性，并能识别混合系统的拓扑不变量，成功应用于随机最优控制问题。

Conclusion: CHyLL通过将混合系统表示为连续流，避免了传统方法中的模式切换和不连续性问题，为混合系统学习提供了新的有效框架。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [7] [Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography](https://arxiv.org/abs/2512.10179)
*Abolfazl Shahrooei,Luke Arthur,Om Patel,Derek Kamper*

Main category: cs.LG

TL;DR: 比较SNN和TCN在从高密度表面肌电信号解码指尖力方面的性能，TCN表现更优但SNN作为神经形态架构有改进潜力


<details>
  <summary>Details</summary>
Motivation: 高密度表面肌电信号为非侵入式神经接口提供了可能，但将神经活动映射到用户运动意图仍然具有挑战性，需要探索更有效的解码方法

Method: 使用单个参与者的数据，通过FastICA分解获取运动单元放电信息，采用重叠窗口训练，比较SNN和TCN两种架构在指尖力解码任务上的表现

Result: TCN在测试集上达到4.44% MVC RMSE（Pearson r=0.974），SNN达到8.25% MVC（r=0.922），TCN更准确但SNN作为神经形态架构有改进空间

Conclusion: 虽然TCN在解码精度上优于SNN，但SNN作为神经形态架构具有潜力，通过适度的架构和超参数优化可以缩小性能差距

Abstract: High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.

</details>


### [8] [A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field](https://arxiv.org/abs/2512.10287)
*Apurba Sarker,Reza T. Batley,Darshan Sarojini,Sourav Saha*

Main category: cs.LG

TL;DR: KHRONOS是一种基于变分原理和插值理论的核神经网络代理模型，在资源受限条件下（特别是高保真数据稀缺时）比传统密集神经网络参数更少、训练和推理更快，同时保持相当的预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统气动模拟计算成本高昂，代理模型可以提供快速替代方案。然而，现有神经网络模型通常需要大量高保真数据，在资源受限条件下表现不佳。本研究旨在开发一种能够在高保真数据稀缺时仍能有效工作的代理模型。

Method: 提出KHRONOS模型，基于变分原理、插值理论和张量分解构建核神经网络。使用AirfRANS数据集作为高保真基准，NeuralFoil生成低保真数据。将KHRONOS与多层感知机、图神经网络和物理信息神经网络进行比较，考虑不同高保真数据可用性（0%、10%、30%）和几何参数化复杂度，预测翼型表面压力系数分布。

Result: 所有模型最终都能达到相当的预测精度，但KHRONOS在资源受限条件下表现优异：需要数量级更少的可训练参数，训练和推理速度远快于传统密集神经网络，同时在精度上保持可比性。

Conclusion: KHRONOS及其类似架构在多保真气动场预测中能够有效平衡精度和效率，特别适用于高保真数据稀缺的资源受限场景，为气动设计和优化提供了更高效的代理模型方案。

Abstract: Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.

</details>


### [9] [Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks](https://arxiv.org/abs/2512.10355)
*Hyunsung Kim,Sangwoo Seo,Hoyoung Choi,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: DEFCON是一个基于图注意力网络的足球防守贡献评估框架，通过量化球员在每次进攻情境中的防守贡献，填补了现有方法主要关注有球动作的不足。


<details>
  <summary>Details</summary>
Motivation: 现有足球防守评估方法主要关注拦截、抢断等可见的有球动作，但有效的防守往往体现在阻止危险机会发生之前。这导致防守球员的真实影响大部分未被测量，需要更全面的评估框架。

Method: 使用图注意力网络（GAT）构建DEFCON框架，估计每个进攻选项的成功概率和期望价值，以及每个防守球员阻止该选项的责任。通过计算每次动作前后进攻方的期望控球价值变化，根据防守球员是否减少或增加对手的EPV来分配正负信用分。

Result: 在2023-24赛季训练、2024-25赛季荷甲联赛事件和追踪数据上评估，DEFCON聚合的球员信用分与市场估值呈现强正相关。展示了多种实际应用：比赛中的防守贡献时间线、场地区域的空间分析、攻防球员互动的配对总结。

Conclusion: DEFCON提供了一个全面的防守贡献评估框架，能够量化球员在每次进攻情境中的影响，填补了现有方法的空白，为足球防守分析提供了新的工具和见解。

Abstract: Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.

</details>


### [10] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: 提出DCFO方法，为LOF异常检测算法生成反事实解释，通过数据空间分区实现梯度优化，在50个数据集上验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异常检测需要解释来理解异常原因、验证显著性并识别潜在偏差。现有反事实解释方法大多未针对经典异常检测算法如LOF，而LOF作为最流行的无监督异常检测方法之一缺乏可解释性。

Method: 提出DCFO方法，专门为LOF生成反事实解释。通过将数据空间划分为LOF行为平滑的区域，实现高效的基于梯度的优化。

Result: 在50个OpenML数据集上的广泛实验验证表明，DCFO在生成反事实的邻近性和有效性方面持续优于基准竞争对手。

Conclusion: DCFO成功解决了LOF异常检测算法的可解释性问题，为这一广泛使用的经典算法提供了有效的反事实解释框架。

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [11] [UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting](https://arxiv.org/abs/2508.01426)
*Hang Ni,Weijia Zhang,Hao Liu*

Main category: cs.LG

TL;DR: UniExtreme是一个通用的极端天气预测基础模型，通过自适应频率调制和事件先验增强模块，解决现有模型对多样化极端天气事件预测能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测基础模型主要关注一般天气条件或特定类型的极端事件，忽视了真实世界中多样化极端天气事件的复杂大气模式。极端事件具有与正常天气不同的频谱特征，以及层次化的驱动因素和地理混合特性。

Method: 提出UniExtreme模型，包含两个核心模块：(1) 自适应频率调制(AFM)模块，通过可学习的Beta分布滤波器和多粒度频谱聚合，捕捉正常与极端天气的区域性频谱差异；(2) 事件先验增强(EPA)模块，通过双级记忆融合网络，整合区域特定的极端事件先验知识，解决极端事件的层次多样性和复合极端模式。

Result: 大量实验表明，UniExtreme在极端天气和一般天气预测任务上都优于现有最先进的基线模型，展现出在多样化极端场景下的卓越适应性。

Conclusion: UniExtreme通过整合频谱差异分析和事件先验知识，成功构建了一个通用的极端天气预测基础模型，能够有效处理真实世界中复杂多样的极端天气事件。

Abstract: Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出一个全面的数据偏见检测与缓解管道，包含四个组件来处理表示偏见和刻板印象两种偏见类型，通过人类验证和模型微调评估效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据存在多方面偏见，包括有害语言和倾斜的人口分布。欧盟AI法案等法规要求识别和缓解对受保护群体的偏见，但缺乏实践指导和操作化方法。

Method: 提出四组件管道：1) 基于质量标准的LLM生成词表检测群体标签；2) 使用人口表示分数量化表示偏见；3) 社会语言学过滤检测缓解刻板印象；4) 语法和上下文感知的反事实数据增强补偿表示偏见。

Result: 成功减少文本数据集中的表示偏见和显性刻板印象。但使用去偏见数据微调的LLM在偏见基准测试中并未一致表现改善，暴露当前评估方法的差距。

Conclusion: 数据去偏见能有效减少数据偏见，但对模型偏见的缓解效果有限，需要更有针对性的数据操作来解决模型偏见，同时需要改进评估方法。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [13] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT是一个三层调度员支持架构，旨在解决加勒比口音英语在紧急语音识别系统中的性能下降问题，通过结合口音调优的ASR、本地实体提取和生物声学压力检测，为调度员提供结构化输入以应用标准分诊协议。


<details>
  <summary>Details</summary>
Motivation: 紧急语音识别系统在非标准英语变体（特别是加勒比口音）上存在系统性性能下降，导致加勒比人群在紧急服务中面临关键差距。需要确保这些人群能够平等获得国家分诊协议服务。

Method: 三层架构：1) 加勒比口音调优的自动语音识别；2) 通过大语言模型进行本地实体提取；3) 生物声学压力检测。系统提供三个互补信号：转录置信度、结构化临床实体和声音压力指标。关键洞察：低ASR置信度可作为队列优先级信号，特别是与升高的声音压力指标结合时。

Result: 建立了口音弹性紧急AI框架，确保加勒比声音能够平等获得已建立的国家分诊协议（ESI常规操作和START大规模伤亡事件）。系统设计考虑了灾难场景下的离线操作。加勒比紧急呼叫的实证验证是未来工作。

Conclusion: TRIDENT为口音弹性紧急AI提供了一个框架，通过将低ASR置信度重新定义为有价值的优先级信号，并结合语义分析和副语言特征，确保加勒比人群在紧急情况下获得公平服务。系统基于压力诱导语码转换的心理语言学研究，并考虑了灾难场景的实际部署需求。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>
