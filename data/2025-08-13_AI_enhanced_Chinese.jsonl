{"id": "2508.08266", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.08266", "abs": "https://arxiv.org/abs/2508.08266", "authors": ["Ryan Mioduski"], "title": "Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants", "comment": null, "summary": "Virginia's seventeenth- and eighteenth-century land patents survive primarily\nas narrative metes-and-bounds descriptions, limiting spatial analysis. This\nstudy systematically evaluates current-generation large language models (LLMs)\nin converting these prose abstracts into geographically accurate\nlatitude/longitude coordinates within a focused evaluation context. A digitized\ncorpus of 5,471 Virginia patent abstracts (1695-1732) is released, with 43\nrigorously verified test cases serving as an initial, geographically focused\nbenchmark. Six OpenAI models across three architectures (o-series, GPT-4-class,\nand GPT-3.5) were tested under two paradigms: direct-to-coordinate and\ntool-augmented chain-of-thought invoking external geocoding APIs. Results were\ncompared with a GIS-analyst baseline, the Stanford NER geoparser, Mordecai-3,\nand a county-centroid heuristic.\n  The top single-call model, o3-2025-04-16, achieved a mean error of 23 km\n(median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest\nLLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70%\n(Stanford NER). A five-call ensemble further reduced errors to 19 km (median 12\nkm) at minimal additional cost (approx. USD 0.20 per grant), outperforming the\nmedian LLM by 48.6%. A patentee-name-redaction ablation increased error by\nabout 9%, indicating reliance on textual landmark and adjacency descriptions\nrather than memorization. The cost-efficient gpt-4o-2024-08-06 model maintained\na 28 km mean error at USD 1.09 per 1,000 grants, establishing a strong\ncost-accuracy benchmark; external geocoding tools offered no measurable benefit\nin this evaluation.\n  These findings demonstrate the potential of LLMs for scalable, accurate, and\ncost-effective historical georeferencing.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c06\u5f17\u5409\u5c3c\u4e9a17-18\u4e16\u7eaa\u571f\u5730\u4e13\u5229\u63cf\u8ff0\u8f6c\u6362\u4e3a\u5730\u7406\u5750\u6807\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u67d0\u4e9b\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f17\u5409\u5c3c\u4e9a17-18\u4e16\u7eaa\u7684\u571f\u5730\u4e13\u5229\u63cf\u8ff0\u591a\u4e3a\u53d9\u8ff0\u6027\u6587\u672c\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u5c06\u5176\u8f6c\u6362\u4e3a\u5730\u7406\u5750\u6807\u3002", "method": "\u7814\u7a76\u6d4b\u8bd5\u4e86\u516d\u79cdOpenAI\u6a21\u578b\uff08\u4e09\u79cd\u67b6\u6784\uff09\u5728\u4e24\u79cd\u8303\u5f0f\u4e0b\u7684\u8868\u73b0\uff1a\u76f4\u63a5\u751f\u6210\u5750\u6807\u548c\u5de5\u5177\u589e\u5f3a\u7684\u94fe\u5f0f\u601d\u8003\uff08\u8c03\u7528\u5916\u90e8\u5730\u7406\u7f16\u7801API\uff09\uff0c\u5e76\u4e0eGIS\u5206\u6790\u5e08\u7b49\u57fa\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6700\u4f73\u6a21\u578b\uff08o3-2025-04-16\uff09\u5e73\u5747\u8bef\u5dee\u4e3a23\u516c\u91cc\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u548c\u57fa\u51c6\u3002\u4e94\u6a21\u578b\u96c6\u6210\u8fdb\u4e00\u6b65\u964d\u4f4e\u8bef\u5dee\u81f319\u516c\u91cc\uff0c\u6210\u672c\u6548\u76ca\u663e\u8457\u3002", "conclusion": "LLMs\u5728\u5386\u53f2\u5730\u7406\u5750\u6807\u8f6c\u6362\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08317", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08317", "abs": "https://arxiv.org/abs/2508.08317", "authors": ["Saptarshi Banerjee", "Tausif Mallick", "Amlan Chakroborty", "Himadri Nath Saha", "Nityananda T. Takur"], "title": "Evaluation of State-of-the-Art Deep Learning Techniques for Plant Disease and Pest Detection", "comment": "AI/ML, Computer Vision", "summary": "Addressing plant diseases and pests is critical for enhancing crop production\nand preventing economic losses. Recent advances in artificial intelligence\n(AI), machine learning (ML), and deep learning (DL) have significantly improved\nthe precision and efficiency of detection methods, surpassing the limitations\nof manual identification. This study reviews modern computer-based techniques\nfor detecting plant diseases and pests from images, including recent AI\ndevelopments. The methodologies are organized into five categories:\nhyperspectral imaging, non-visualization techniques, visualization approaches,\nmodified deep learning architectures, and transformer models. This structured\ntaxonomy provides researchers with detailed, actionable insights for selecting\nadvanced state-of-the-art detection methods. A comprehensive survey of recent\nwork and comparative studies demonstrates the consistent superiority of modern\nAI-based approaches, which often outperform older image analysis methods in\nspeed and accuracy. In particular, vision transformers such as the Hierarchical\nVision Transformer (HvT) have shown accuracy exceeding 99.3% in plant disease\ndetection, outperforming architectures like MobileNetV3. The study concludes by\ndiscussing system design challenges, proposing solutions, and outlining\npromising directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eAI\u3001ML\u548cDL\u7684\u690d\u7269\u75c5\u866b\u5bb3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5206\u7c7b\u4e3a\u4e94\u79cd\u6280\u672f\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u4ee3AI\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u53d8\u6362\u5668\uff08\u5982HvT\uff09\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u63d0\u9ad8\u4f5c\u7269\u4ea7\u91cf\u548c\u51cf\u5c11\u7ecf\u6d4e\u635f\u5931\u9700\u8981\u66f4\u7cbe\u786e\u548c\u9ad8\u6548\u7684\u690d\u7269\u75c5\u866b\u5bb3\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u5c06\u65b9\u6cd5\u5206\u4e3a\u4e94\u7c7b\uff1a\u9ad8\u5149\u8c31\u6210\u50cf\u3001\u975e\u53ef\u89c6\u5316\u6280\u672f\u3001\u53ef\u89c6\u5316\u65b9\u6cd5\u3001\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u53d8\u6362\u5668\u6a21\u578b\u3002", "result": "\u73b0\u4ee3AI\u65b9\u6cd5\uff08\u5982HvT\uff09\u5728\u68c0\u6d4b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cHvT\u7684\u51c6\u786e\u7387\u8d85\u8fc799.3%\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u7cfb\u7edf\u8bbe\u8ba1\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.08272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08272", "abs": "https://arxiv.org/abs/2508.08272", "authors": ["Tadej \u0160kvorc", "Nikola Iva\u010di\u010d", "Sebastjan Hribar", "Marko Robnik-\u0160ikonja"], "title": "Real-time News Story Identification", "comment": null, "summary": "To improve the reading experience, many news sites organize news into topical\ncollections, called stories. In this work, we present an approach for\nimplementing real-time story identification for a news monitoring system that\nautomatically collects news articles as they appear online and processes them\nin various ways. Story identification aims to assign each news article to a\nspecific story that the article is covering. The process is similar to text\nclustering and topic modeling, but requires that articles be grouped based on\nparticular events, places, and people, rather than general text similarity (as\nin clustering) or general (predefined) topics (as in topic modeling). We\npresent an approach to story identification that is capable of functioning in\nreal time, assigning articles to stories as they are published online. In the\nproposed approach, we combine text representation techniques, clustering\nalgorithms, and online topic modeling methods. We combine various text\nrepresentation methods to extract specific events and named entities necessary\nfor story identification, showing that a mixture of online topic-modeling\napproaches such as BERTopic, DBStream, and TextClust can be adapted for story\ndiscovery. We evaluate our approach on a news dataset from Slovene media\ncovering a period of 1 month. We show that our real-time approach produces\nsensible results as judged by human evaluators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u65b0\u95fb\u6545\u4e8b\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u8868\u793a\u3001\u805a\u7c7b\u548c\u5728\u7ebf\u4e3b\u9898\u5efa\u6a21\u6280\u672f\uff0c\u7528\u4e8e\u5c06\u65b0\u95fb\u6587\u7ae0\u5206\u914d\u5230\u7279\u5b9a\u6545\u4e8b\u4e2d\u3002", "motivation": "\u63d0\u5347\u65b0\u95fb\u9605\u8bfb\u4f53\u9a8c\uff0c\u901a\u8fc7\u81ea\u52a8\u5c06\u65b0\u95fb\u6587\u7ae0\u5f52\u7c7b\u5230\u7279\u5b9a\u6545\u4e8b\u4e2d\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u5730\u83b7\u53d6\u4fe1\u606f\u3002", "method": "\u7ed3\u5408\u6587\u672c\u8868\u793a\u6280\u672f\u3001\u805a\u7c7b\u7b97\u6cd5\u548c\u5728\u7ebf\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff08\u5982BERTopic\u3001DBStream\u3001TextClust\uff09\uff0c\u63d0\u53d6\u7279\u5b9a\u4e8b\u4ef6\u548c\u547d\u540d\u5b9e\u4f53\u4ee5\u5b9e\u73b0\u6545\u4e8b\u8bc6\u522b\u3002", "result": "\u5728\u65af\u6d1b\u6587\u5c3c\u4e9a\u5a92\u4f53\u7684\u4e00\u4e2a\u6708\u65b0\u95fb\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u5408\u7406\u7684\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b9e\u65f6\u6545\u4e8b\u8bc6\u522b\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u591f\u6ee1\u8db3\u65b0\u95fb\u76d1\u63a7\u7cfb\u7edf\u7684\u9700\u6c42\u3002"}}
{"id": "2508.08501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08501", "abs": "https://arxiv.org/abs/2508.08501", "authors": ["Yuchen Li", "Cong Lin", "Muhammad Umair Nasir", "Philip Bontrager", "Jialin Liu", "Julian Togelius"], "title": "GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games", "comment": null, "summary": "We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning\nand problem-solving capabilities of large language models (LLMs). Built on the\nGeneral Video Game AI framework, it features a diverse collection of\narcade-style games designed to test a model's ability to handle tasks that\ndiffer from most existing LLM benchmarks. The benchmark leverages a game\ndescription language that enables rapid creation of new games and levels,\nhelping to prevent overfitting over time. Each game scene is represented by a\ncompact set of ASCII characters, allowing for efficient processing by language\nmodels. GVGAI-LLM defines interpretable metrics, including the meaningful step\nratio, step efficiency, and overall score, to assess model behavior. Through\nzero-shot evaluations across a broad set of games and levels with diverse\nchallenges and skill depth, we reveal persistent limitations of LLMs in spatial\nreasoning and basic planning. Current models consistently exhibit spatial and\nlogical errors, motivating structured prompting and spatial grounding\ntechniques. While these interventions lead to partial improvements, the\nbenchmark remains very far from solved. GVGAI-LLM provides a reproducible\ntestbed for advancing research on language model capabilities, with a\nparticular emphasis on agentic behavior and contextual reasoning.", "AI": {"tldr": "GVGAI-LLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u6e38\u620f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u57fa\u7840\u89c4\u5212\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u672a\u80fd\u6db5\u76d6\u591a\u6837\u5316\u7684\u4efb\u52a1\u9700\u6c42\uff0cGVGAI-LLM\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u6e38\u620f\u573a\u666f\u6d4b\u8bd5\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "method": "\u5229\u7528\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u5feb\u901f\u751f\u6210\u65b0\u6e38\u620f\u548c\u5173\u5361\uff0c\u907f\u514d\u8fc7\u62df\u5408\uff1b\u901a\u8fc7ASCII\u5b57\u7b26\u8868\u793a\u6e38\u620f\u573a\u666f\uff0c\u4fbf\u4e8e\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff1b\u5b9a\u4e49\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793aLLMs\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u57fa\u7840\u89c4\u5212\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u63a5\u5730\u6280\u672f\u4ec5\u5e26\u6765\u90e8\u5206\u6539\u8fdb\u3002", "conclusion": "GVGAI-LLM\u4e3a\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c24\u5176\u5173\u6ce8\u4ee3\u7406\u884c\u4e3a\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u3002"}}
{"id": "2508.08605", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08605", "abs": "https://arxiv.org/abs/2508.08605", "authors": ["Honglei Xu", "Zhilu Zhang", "Junjie Fan", "Xiaohe Wu", "Wangmeng Zuo"], "title": "SelfHVD: Self-Supervised Handheld Video Deblurring for Mobile Phones", "comment": null, "summary": "Shooting video with a handheld mobile phone, the most common photographic\ndevice, often results in blurry frames due to shaking hands and other\ninstability factors. Although previous video deblurring methods have achieved\nimpressive progress, they still struggle to perform satisfactorily on\nreal-world handheld video due to the blur domain gap between training and\ntesting data. To address the issue, we propose a self-supervised method for\nhandheld video deblurring, which is driven by sharp clues in the video. First,\nto train the deblurring model, we extract the sharp clues from the video and\ntake them as misalignment labels of neighboring blurry frames. Second, to\nimprove the model's ability, we propose a novel Self-Enhanced Video Deblurring\n(SEVD) method to create higher-quality paired video data. Third, we propose a\nSelf-Constrained Spatial Consistency Maintenance (SCSCM) method to regularize\nthe model, preventing position shifts between the output and input frames.\nMoreover, we construct a synthetic and a real-world handheld video dataset for\nhandheld video deblurring. Extensive experiments on these two and other common\nreal-world datasets demonstrate that our method significantly outperforms\nexisting self-supervised ones. The code and datasets are publicly available at\nhttps://github.com/cshonglei/SelfHVD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u624b\u6301\u89c6\u9891\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u6e05\u6670\u7ebf\u7d22\u4f5c\u4e3a\u8bad\u7ec3\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u81ea\u589e\u5f3a\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7ef4\u62a4\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u6a21\u7cca\u6548\u679c\u3002", "motivation": "\u624b\u6301\u79fb\u52a8\u8bbe\u5907\u62cd\u6444\u7684\u89c6\u9891\u5e38\u56e0\u6296\u52a8\u5bfc\u81f4\u6a21\u7cca\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u7684\u6a21\u7cca\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "method": "1. \u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u6e05\u6670\u7ebf\u7d22\u4f5c\u4e3a\u6a21\u7cca\u5e27\u7684\u9519\u4f4d\u6807\u7b7e\uff1b2. \u63d0\u51fa\u81ea\u589e\u5f3a\u89c6\u9891\u53bb\u6a21\u7cca\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u914d\u5bf9\u6570\u636e\uff1b3. \u5f15\u5165\u81ea\u7ea6\u675f\u7a7a\u95f4\u4e00\u81f4\u6027\u7ef4\u62a4\u6280\u672f\u9632\u6b62\u8f93\u51fa\u5e27\u4f4d\u7f6e\u504f\u79fb\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u589e\u5f3a\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u624b\u6301\u89c6\u9891\u53bb\u6a21\u7cca\u95ee\u9898\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.08785", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08785", "abs": "https://arxiv.org/abs/2508.08785", "authors": ["Yunfeng Ning", "Mayi Xu", "Jintao Wen", "Qiankun Pi", "Yuanyuan Zhu", "Ming Zhong", "Jiawei Jiang", "Tieyun Qian"], "title": "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering", "comment": null, "summary": "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684RAG\u6846\u67b6ARoG\uff0c\u901a\u8fc7\u5173\u7cfb\u4e2d\u5fc3\u62bd\u8c61\u548c\u7ed3\u6784\u5bfc\u5411\u62bd\u8c61\u7b56\u7565\u89e3\u51b3\u533f\u540d\u5b9e\u4f53\u68c0\u7d22\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u548c\u9690\u79c1\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5728RAG\u7cfb\u7edf\u4e2d\u56e0\u533f\u540d\u5b9e\u4f53\u5bfc\u81f4\u7684\u8bed\u4e49\u7f3a\u5931\u548c\u9690\u79c1\u98ce\u9669\u95ee\u9898\u3002", "method": "\u63d0\u51faARoG\u6846\u67b6\uff0c\u5305\u542b\u5173\u7cfb\u4e2d\u5fc3\u62bd\u8c61\u548c\u7ed3\u6784\u5bfc\u5411\u62bd\u8c61\u7b56\u7565\uff0c\u5c06\u533f\u540d\u5b9e\u4f53\u8f6c\u5316\u4e3a\u53ef\u68c0\u7d22\u4fe1\u606f\u5e76\u4f18\u5316\u68c0\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86ARoG\u7684\u9ad8\u6548\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002", "conclusion": "ARoG\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2508.08919", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08919", "abs": "https://arxiv.org/abs/2508.08919", "authors": ["Hao Liu", "Chun Yang", "Zhang xiaoxing", "Rui Ma", "Xiaobin Zhu"], "title": "Stationarity Exploration for Multivariate Time Series Forecasting", "comment": null, "summary": "Deep learning-based time series forecasting has found widespread\napplications. Recently, converting time series data into the frequency domain\nfor forecasting has become popular for accurately exploring periodic patterns.\nHowever, existing methods often cannot effectively explore stationary\ninformation from complex intertwined frequency components. In this paper, we\npropose a simple yet effective Amplitude-Phase Reconstruct Network (APRNet)\nthat models the inter-relationships of amplitude and phase, which prevents the\namplitude and phase from being constrained by different physical quantities,\nthereby decoupling the distinct characteristics of signals for capturing\nstationary information. Specifically, we represent the multivariate time series\ninput across sequence and channel dimensions, highlighting the correlation\nbetween amplitude and phase at multiple interaction frequencies. We propose a\nnovel Kolmogorov-Arnold-Network-based Local Correlation (KLC) module to\nadaptively fit local functions using univariate functions, enabling more\nflexible characterization of stationary features across different amplitudes\nand phases. This significantly enhances the model's capability to capture\ntime-varying patterns. Extensive experiments demonstrate the superiority of our\nAPRNet against the state-of-the-arts (SOTAs).", "AI": {"tldr": "APRNet\u901a\u8fc7\u89e3\u8026\u632f\u5e45\u548c\u76f8\u4f4d\uff0c\u6709\u6548\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5e73\u7a33\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u590d\u6742\u7684\u9891\u7387\u6210\u5206\u4e2d\u63d0\u53d6\u5e73\u7a33\u4fe1\u606f\uff0cAPRNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faAPRNet\uff0c\u5229\u7528\u632f\u5e45\u548c\u76f8\u4f4d\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u7ed3\u5408KLC\u6a21\u5757\u81ea\u9002\u5e94\u62df\u5408\u5c40\u90e8\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAPRNet\u5728\u6355\u6349\u65f6\u53d8\u6a21\u5f0f\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "APRNet\u901a\u8fc7\u89e3\u8026\u4fe1\u53f7\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09012", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09012", "abs": "https://arxiv.org/abs/2508.09012", "authors": ["Adri\u00e1n Gude", "Roi Santos-R\u00edos", "Francisco Prado-Vali\u00f1o", "Ana Ezquerro", "Jes\u00fas Vilares"], "title": "LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA", "comment": "Accepted to SemEval 2025. Camera-ready version", "summary": "This paper describes our participation in SemEval 2025 Task 8, focused on\nTabular Question Answering. We developed a zero-shot pipeline that leverages an\nLarge Language Model to generate functional code capable of extracting the\nrelevant information from tabular data based on an input question. Our approach\nconsists of a modular pipeline where the main code generator module is\nsupported by additional components that identify the most relevant columns and\nanalyze their data types to improve extraction accuracy. In the event that the\ngenerated code fails, an iterative refinement process is triggered,\nincorporating the error feedback into a new generation prompt to enhance\nrobustness. Our results show that zero-shot code generation is a valid approach\nfor Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of\ntask-specific fine-tuning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728SemEval 2025\u4efb\u52a18\u4e2d\u7684\u53c2\u4e0e\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u96f6\u6837\u672c\u4ee3\u7801\u751f\u6210\u5728\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5305\u62ec\u4ee3\u7801\u751f\u6210\u6a21\u5757\u3001\u5217\u9009\u62e9\u6a21\u5757\u548c\u6570\u636e\u7c7b\u578b\u5206\u6790\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6d4b\u8bd5\u9636\u6bb5\u6392\u540d33/53\uff0c\u8bc1\u660e\u4e86\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u96f6\u6837\u672c\u4ee3\u7801\u751f\u6210\u662f\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u7684\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.09056", "categories": ["cs.LG", "cs.CR", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09056", "abs": "https://arxiv.org/abs/2508.09056", "authors": ["Shreya Ghosh", "Abu Shafin Mohammad Mahdee Jameel", "Aly El Gamal"], "title": "FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm", "comment": null, "summary": "Intrusion Detection Systems (IDS) have an increasingly important role in\npreventing exploitation of network vulnerabilities by malicious actors. Recent\ndeep learning based developments have resulted in significant improvements in\nthe performance of IDS systems. In this paper, we present FetFIDS, where we\nexplore the employment of feature embedding instead of positional embedding to\nimprove intrusion detection performance of a transformer based deep learning\nsystem. Our model is developed with the aim of deployments in edge learning\nscenarios, where federated learning over multiple communication rounds can\nensure both privacy and localized performance improvements. FetFIDS outperforms\nmultiple state-of-the-art intrusion detection systems in a federated\nenvironment and demonstrates a high degree of suitability to federated\nlearning. The code for this work can be found at\nhttps://github.com/ghosh64/fetfids.", "AI": {"tldr": "FetFIDS\u662f\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5d4c\u5165\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8eTransformer\u7684\u5165\u4fb5\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u8054\u90a6\u5b66\u4e60\u73af\u5883\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u7684\u6027\u80fd\uff0c\u4f46\u5982\u4f55\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u5e76\u4fdd\u62a4\u9690\u79c1\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "FetFIDS\u91c7\u7528\u7279\u5f81\u5d4c\u5165\u800c\u975e\u4f4d\u7f6e\u5d4c\u5165\uff0c\u7ed3\u5408Transformer\u67b6\u6784\uff0c\u4e13\u4e3a\u8054\u90a6\u5b66\u4e60\u73af\u5883\u8bbe\u8ba1\uff0c\u652f\u6301\u591a\u8f6e\u901a\u4fe1\u4ee5\u4f18\u5316\u672c\u5730\u6027\u80fd\u3002", "result": "FetFIDS\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u663e\u793a\u51fa\u5bf9\u8054\u90a6\u5b66\u4e60\u7684\u9ad8\u5ea6\u9002\u5e94\u6027\u3002", "conclusion": "FetFIDS\u901a\u8fc7\u7279\u5f81\u5d4c\u5165\u548c\u8054\u90a6\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5165\u4fb5\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u62a4\u4e86\u9690\u79c1\u3002"}}
{"id": "2508.09059", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09059", "abs": "https://arxiv.org/abs/2508.09059", "authors": ["Jonas Valbj\u00f8rn Andersena", "Anders Peder H\u00f8jer Karlsen", "Markus Harboe Olsen", "Nikolaj Krebs Pedersen"], "title": "Causal Machine Learning for Patient-Level Intraoperative Opioid Dose Prediction from Electronic Health Records", "comment": null, "summary": "This paper introduces the OPIAID algorithm, a novel approach for predicting\nand recommending personalized opioid dosages for individual patients. The\nalgorithm optimizes pain management while minimizing opioid related adverse\nevents (ORADE) by employing machine learning models trained on observational\nelectronic health records (EHR) data. It leverages a causal machine learning\napproach to understand the relationship between opioid dose, case specific\npatient and intraoperative characteristics, and pain versus ORADE outcomes. The\nOPIAID algorithm considers patient-specific characteristics and the influence\nof different opiates, enabling personalized dose recommendations. This paper\noutlines the algorithm's methodology and architecture, and discusses key\nassumptions, and approaches to evaluating its performance.", "AI": {"tldr": "OPIAID\u7b97\u6cd5\u662f\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u963f\u7247\u7c7b\u836f\u7269\u5242\u91cf\u9884\u6d4b\u4e0e\u63a8\u8350\u65b9\u6cd5\uff0c\u65e8\u5728\u4f18\u5316\u75bc\u75db\u7ba1\u7406\u5e76\u51cf\u5c11\u4e0d\u826f\u53cd\u5e94\u3002", "motivation": "\u89e3\u51b3\u963f\u7247\u7c7b\u836f\u7269\u5242\u91cf\u4e2a\u6027\u5316\u63a8\u8350\u95ee\u9898\uff0c\u4ee5\u5e73\u8861\u75bc\u75db\u7ba1\u7406\u548c\u4e0d\u826f\u53cd\u5e94\u7684\u6700\u5c0f\u5316\u3002", "method": "\u5229\u7528\u89c2\u5bdf\u6027\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u91c7\u7528\u56e0\u679c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u5242\u91cf\u3001\u60a3\u8005\u7279\u5f81\u4e0e\u7ed3\u679c\u7684\u5173\u7cfb\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u6839\u636e\u60a3\u8005\u7279\u5f81\u548c\u963f\u7247\u7c7b\u836f\u7269\u7c7b\u578b\u63d0\u4f9b\u4e2a\u6027\u5316\u5242\u91cf\u63a8\u8350\u3002", "conclusion": "OPIAID\u7b97\u6cd5\u4e3a\u4e2a\u6027\u5316\u963f\u7247\u7c7b\u836f\u7269\u5242\u91cf\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6027\u80fd\u3002"}}
{"id": "2508.09096", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.09096", "abs": "https://arxiv.org/abs/2508.09096", "authors": ["Anastasia Zhukova", "Thomas Walton", "Christian E. Matt", "Bela Gipp"], "title": "Link Prediction for Event Logs in the Process Industry", "comment": null, "summary": "Knowledge management (KM) is vital in the process industry for optimizing\noperations, ensuring safety, and enabling continuous improvement through\neffective use of operational data and past insights. A key challenge in this\ndomain is the fragmented nature of event logs in shift books, where related\nrecords, e.g., entries documenting issues related to equipment or processes and\nthe corresponding solutions, may remain disconnected. This fragmentation\nhinders the recommendation of previous solutions to the users. To address this\nproblem, we investigate record linking (RL) as link prediction, commonly\nstudied in graph-based machine learning, by framing it as a cross-document\ncoreference resolution (CDCR) task enhanced with natural language inference\n(NLI) and semantic text similarity (STS) by shifting it into the causal\ninference (CI). We adapt CDCR, traditionally applied in the news domain, into\nan RL model to operate at the passage level, similar to NLI and STS, while\naccommodating the process industry's specific text formats, which contain\nunstructured text and structured record attributes. Our RL model outperformed\nthe best versions of NLI- and STS-driven baselines by 28% (11.43 points) and\n27% (11.21 points), respectively. Our work demonstrates how domain adaptation\nof the state-of-the-art CDCR models, enhanced with reasoning capabilities, can\nbe effectively tailored to the process industry, improving data quality and\nconnectivity in shift logs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5f55\u94fe\u63a5\uff08RL\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u8de8\u6587\u6863\u5171\u6307\u6d88\u89e3\uff08CDCR\uff09\u4efb\u52a1\uff0c\u5e76\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u5ea6\uff08STS\uff09\uff0c\u4ee5\u89e3\u51b3\u6d41\u7a0b\u5de5\u4e1a\u4e2d\u4e8b\u4ef6\u65e5\u5fd7\u788e\u7247\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u6d41\u7a0b\u5de5\u4e1a\u4e2d\u7684\u77e5\u8bc6\u7ba1\u7406\uff08KM\uff09\u56e0\u4e8b\u4ef6\u65e5\u5fd7\u7684\u788e\u7247\u5316\u800c\u53d7\u9650\uff0c\u5bfc\u81f4\u76f8\u5173\u8bb0\u5f55\uff08\u5982\u8bbe\u5907\u95ee\u9898\u53ca\u89e3\u51b3\u65b9\u6848\uff09\u65e0\u6cd5\u6709\u6548\u5173\u8054\uff0c\u963b\u788d\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u63a8\u8350\u3002", "method": "\u5c06\u8bb0\u5f55\u94fe\u63a5\uff08RL\uff09\u95ee\u9898\u5efa\u6a21\u4e3a\u8de8\u6587\u6863\u5171\u6307\u6d88\u89e3\uff08CDCR\uff09\u4efb\u52a1\uff0c\u7ed3\u5408NLI\u548cSTS\uff0c\u5e76\u5f15\u5165\u56e0\u679c\u63a8\u7406\uff08CI\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u7684RL\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8eNLI\u548cSTS\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u5347\u4e8628%\uff0811.43\u5206\uff09\u548c27%\uff0811.21\u5206\uff09\u3002", "conclusion": "\u901a\u8fc7\u9886\u57df\u9002\u914d\u548c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0cCDCR\u6a21\u578b\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u6d41\u7a0b\u5de5\u4e1a\uff0c\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u65e5\u5fd7\u8bb0\u5f55\u7684\u5173\u8054\u6027\u3002"}}
{"id": "2508.09022", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09022", "abs": "https://arxiv.org/abs/2508.09022", "authors": ["Zhiqiang Yang", "Renshuai Tao", "Xiaolong Zheng", "Guodong Yang", "Chunjie Zhang"], "title": "When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges", "comment": "10pages,5figures", "summary": "Existing deepfake detection methods heavily depend on labeled training data.\nHowever, as AI-generated content becomes increasingly realistic, even\n\\textbf{human annotators struggle to distinguish} between deepfakes and\nauthentic images. This makes the labeling process both time-consuming and less\nreliable. Specifically, there is a growing demand for approaches that can\neffectively utilize large-scale unlabeled data from online social networks.\nUnlike typical unsupervised learning tasks, where categories are distinct,\nAI-generated faces closely mimic real image distributions and share strong\nsimilarities, causing performance drop in conventional strategies. In this\npaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key\nchallenges: (1) bridging the domain gap between faces from different generation\nmodels, and (2) utilizing unlabeled image samples. The method features two core\nmodules: text-guided cross-domain alignment, which uses learnable prompts to\nunify visual and textual embeddings into a domain-invariant feature space, and\ncurriculum-driven pseudo label generation, which dynamically exploit more\ninformative unlabeled samples. To prevent catastrophic forgetting, we also\nfacilitate bridging between domains via cross-domain knowledge distillation.\nExtensive experiments on \\textbf{11 popular datasets}, show that DPGNet\noutperforms SoTA approaches by \\textbf{6.3\\%}, highlighting its effectiveness\nin leveraging unlabeled data to address the annotation challenges posed by the\nincreasing realism of deepfakes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDPGNet\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u5f15\u5bfc\u7f51\u7edc\u89e3\u51b3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5229\u7528\u6587\u672c\u5f15\u5bfc\u8de8\u57df\u5bf9\u9f50\u548c\u8bfe\u7a0b\u9a71\u52a8\u4f2a\u6807\u7b7e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u4eba\u5de5\u6807\u6ce8\u53d8\u5f97\u8017\u65f6\u4e14\u4e0d\u53ef\u9760\uff0c\u4e9f\u9700\u80fd\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "DPGNet\u5305\u542b\u6587\u672c\u5f15\u5bfc\u8de8\u57df\u5bf9\u9f50\u548c\u8bfe\u7a0b\u9a71\u52a8\u4f2a\u6807\u7b7e\u751f\u6210\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u7ed3\u5408\u8de8\u57df\u77e5\u8bc6\u84b8\u998f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDPGNet\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u53476.3%\u3002", "conclusion": "DPGNet\u901a\u8fc7\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6807\u6ce8\u6311\u6218\u3002"}}
