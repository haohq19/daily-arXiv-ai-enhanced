{"id": "2601.20969", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20969", "abs": "https://arxiv.org/abs/2601.20969", "authors": ["Alessandro Burigana", "Francesco Fabiano"], "title": "The Epistemic Planning Domain Definition Language: Official Guideline", "comment": null, "summary": "Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.", "AI": {"tldr": "\u63d0\u51faEPDDL\u8bed\u8a00\uff0c\u4e3a\u57fa\u4e8e\u52a8\u6001\u8ba4\u77e5\u903b\u8f91\u7684\u8ba4\u77e5\u89c4\u5212\u63d0\u4f9b\u7edf\u4e00\u7684PDDL\u5f0f\u8868\u793a\uff0c\u89e3\u51b3\u73b0\u6709\u89c4\u5212\u5668\u8bed\u8a00\u788e\u7247\u5316\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u52a8\u6001\u8ba4\u77e5\u903b\u8f91\u7684\u8ba4\u77e5\u89c4\u5212\u5668\u4f7f\u7528\u4e0d\u540c\u7684\u7247\u6bb5\u548c\u4e34\u65f6\u8bed\u8a00\uff0c\u7f3a\u4e4f\u7edf\u4e00\u8868\u793a\uff0c\u963b\u788d\u4e86\u6bd4\u8f83\u3001\u91cd\u7528\u548c\u7cfb\u7edf\u5316\u57fa\u51c6\u5f00\u53d1", "method": "1. \u5f00\u53d1\u62bd\u8c61\u4e8b\u4ef6\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u52a8\u4f5c\u7684\u65b0\u8868\u793a\uff1b2. \u57fa\u4e8e\u62bd\u8c61\u4e8b\u4ef6\u6a21\u578b\u548c\u52a8\u6001\u8ba4\u77e5\u903b\u8f91\u5f62\u5f0f\u5316\u5b9a\u4e49EPDDL\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\uff1b3. \u8bc6\u522b\u9002\u5408\u73b0\u6709\u89c4\u5212\u5668\u7684\u6709\u7528\u7247\u6bb5\u5e76\u5c55\u793a\u5982\u4f55\u5728EPDDL\u4e2d\u8868\u793a", "result": "EPDDL\u80fd\u591f\u6355\u83b7\u5b8c\u6574\u7684\u52a8\u6001\u8ba4\u77e5\u903b\u8f91\u8bed\u4e49\uff0c\u901a\u8fc7\u4ee3\u8868\u6027\u57fa\u51c6\u793a\u4f8b\u5c55\u793a\u4e86\u5176\u4fc3\u8fdb\u4e92\u64cd\u4f5c\u6027\u3001\u53ef\u91cd\u590d\u8bc4\u4f30\u548c\u672a\u6765\u53d1\u5c55\u7684\u80fd\u529b", "conclusion": "EPDDL\u4e3a\u8ba4\u77e5\u89c4\u5212\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u9886\u57df\u5b9a\u4e49\u8bed\u8a00\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u788e\u7247\u5316\u95ee\u9898\uff0c\u652f\u6301\u4e92\u64cd\u4f5c\u6027\u548c\u7cfb\u7edf\u5316\u57fa\u51c6\u5f00\u53d1"}}
{"id": "2601.21173", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21173", "abs": "https://arxiv.org/abs/2601.21173", "authors": ["Zeyi Liu", "Shuang Liu", "Jihai Min", "Zhaoheng Zhang", "Jun Cen", "Pengyu Han", "Songqiao Hu", "Zihan Meng", "Xiao He", "Donghua Zhou"], "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios", "comment": "15 pages, 7 figures", "summary": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.", "AI": {"tldr": "InspecSafe-V1\u662f\u9996\u4e2a\u7528\u4e8e\u5de5\u4e1a\u5de1\u68c0\u5b89\u5168\u8bc4\u4f30\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u4e0b\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u3001\u50cf\u7d20\u7ea7\u5206\u5272\u6807\u6ce8\u548c\u5b89\u5168\u7b49\u7ea7\u6807\u7b7e\uff0c\u8986\u76d65\u79cd\u5178\u578b\u5de5\u4e1a\u573a\u666f\u3002", "motivation": "\u5de5\u4e1a\u667a\u80fd\u5316\u548c\u65e0\u4eba\u5de1\u68c0\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u6a21\u62df\u6570\u636e\u3001\u5355\u6a21\u6001\u611f\u77e5\u6216\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5de5\u4e1a\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u573a\u666f\u7406\u89e3\u548c\u591a\u6a21\u6001\u5b89\u5168\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4ece\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d41\u53f0\u8f6e\u5f0f\u548c\u8f68\u9053\u5f0f\u5de1\u68c0\u673a\u5668\u4eba\u7684\u65e5\u5e38\u64cd\u4f5c\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u8986\u76d6239\u4e2a\u6709\u6548\u5de1\u68c0\u7ad9\u70b9\uff0c\u6784\u5efa\u5305\u542b5013\u4e2a\u5de1\u68c0\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\u3002\u63d0\u4f9b\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u5206\u5272\u6807\u6ce8\u3001\u8bed\u4e49\u573a\u666f\u63cf\u8ff0\u548c\u5b89\u5168\u7b49\u7ea7\u6807\u7b7e\uff0c\u5e76\u5305\u542b7\u79cd\u540c\u6b65\u611f\u77e5\u6a21\u6001\u3002", "result": "\u53d1\u5e03\u4e86InspecSafe-V1\u6570\u636e\u96c6\uff0c\u8986\u76d6\u96a7\u9053\u3001\u7535\u529b\u8bbe\u65bd\u3001\u70e7\u7ed3\u8bbe\u5907\u3001\u77f3\u6cb9\u5316\u5de5\u548c\u7164\u70ad\u8f93\u9001\u6808\u68655\u79cd\u5de5\u4e1a\u573a\u666f\uff0c\u5305\u542b\u7ea2\u5916\u89c6\u9891\u3001\u97f3\u9891\u3001\u6df1\u5ea6\u70b9\u4e91\u3001\u96f7\u8fbe\u70b9\u4e91\u3001\u6c14\u4f53\u6d4b\u91cf\u3001\u6e29\u6e7f\u5ea6\u7b497\u79cd\u540c\u6b65\u6a21\u6001\uff0c\u652f\u6301\u591a\u6a21\u6001\u5f02\u5e38\u8bc6\u522b\u548c\u8de8\u6a21\u6001\u878d\u5408\u3002", "conclusion": "InspecSafe-V1\u586b\u8865\u4e86\u5de5\u4e1a\u5de1\u68c0\u5b89\u5168\u8bc4\u4f30\u9886\u57df\u771f\u5b9e\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u5de5\u4e1a\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u5f02\u5e38\u8bc6\u522b\u3001\u8de8\u6a21\u6001\u878d\u5408\u548c\u7efc\u5408\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u8d44\u6e90\u3002"}}
{"id": "2601.21049", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21049", "abs": "https://arxiv.org/abs/2601.21049", "authors": ["Rita Qiuran Lyu", "Michelle Manqiao Wang", "Lei Shi"], "title": "QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation", "comment": "11 pages, 5 figures, 6 tables", "summary": "User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.", "AI": {"tldr": "QUARK\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u67e5\u8be2\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u6062\u590d\u5047\u8bbe\u548c\u67e5\u8be2\u951a\u5b9a\u805a\u5408\uff0c\u5728\u975e\u5fe0\u5b9e\u67e5\u8be2\u4e0b\u5b9e\u73b0\u9c81\u68d2\u68c0\u7d22\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u68c0\u7d22\u67e5\u8be2\u5f80\u5f80\u662f\u975e\u5fe0\u5b9e\u7684\uff08\u6709\u566a\u58f0\u3001\u4e0d\u5b8c\u6574\u6216\u626d\u66f2\uff09\uff0c\u5bfc\u81f4\u68c0\u7d22\u5668\u5728\u5173\u952e\u8bed\u4e49\u7f3a\u5931\u65f6\u5931\u8d25\u3002\u8fd9\u88ab\u5f62\u5f0f\u5316\u4e3a\u53ec\u56de\u566a\u58f0\u4e0b\u7684\u68c0\u7d22\u95ee\u9898\u3002", "method": "\u63d0\u51faQUARK\u6846\u67b6\uff1a1) \u901a\u8fc7\u6062\u590d\u5047\u8bbe\u663e\u5f0f\u5efa\u6a21\u67e5\u8be2\u4e0d\u786e\u5b9a\u6027\uff1b2) \u5f15\u5165\u67e5\u8be2\u951a\u5b9a\u805a\u5408\u6765\u9c81\u68d2\u5730\u7ed3\u5408\u8fd9\u4e9b\u4fe1\u53f7\uff1b3) \u539f\u59cb\u67e5\u8be2\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\uff0c\u6062\u590d\u5047\u8bbe\u63d0\u4f9b\u53d7\u63a7\u7684\u8f85\u52a9\u8bc1\u636e\u3002", "result": "\u5728\u53d7\u63a7\u6a21\u62df\u548cBEIR\u57fa\u51c6\u6d4b\u8bd5\uff08FIQA\u3001SciFact\u3001NFCorpus\uff09\u4e0a\uff0cQUARK\u5728\u7a00\u758f\u548c\u7a20\u5bc6\u68c0\u7d22\u5668\u4e0a\u5747\u63d0\u9ad8\u4e86Recall\u3001MRR\u548cnDCG\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793aQUARK\u5bf9\u6062\u590d\u5047\u8bbe\u6570\u91cf\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u951a\u5b9a\u805a\u5408\u4f18\u4e8e\u672a\u951a\u5b9a\u7684\u6700\u5927/\u5e73\u5747/\u4e2d\u503c\u6c60\u5316\u3002", "conclusion": "\u901a\u8fc7\u6062\u590d\u5047\u8bbe\u5efa\u6a21\u67e5\u8be2\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u7ed3\u5408\u539f\u5219\u6027\u7684\u951a\u5b9a\u805a\u5408\uff0c\u5bf9\u4e8e\u975e\u5fe0\u5b9e\u67e5\u8be2\u4e0b\u7684\u9c81\u68d2\u68c0\u7d22\u81f3\u5173\u91cd\u8981\u3002QUARK\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u514d\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2601.20906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20906", "abs": "https://arxiv.org/abs/2601.20906", "authors": ["Nikita Makarov", "Maria Bordukova", "Lena Voith von Voithenberg", "Estrella Pivel-Villanueva", "Sabrina Mielke", "Jonathan Wickes", "Hanchen Wang", "Mingyu Derek Ma", "Keunwoo Choi", "Kyunghyun Cho", "Stephen Ra", "Raul Rodriguez-Esteban", "Fabian Schmich", "Michael Menden"], "title": "TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins", "comment": null, "summary": "Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.", "AI": {"tldr": "TwinWeaver\u6846\u67b6\u5c06\u60a3\u8005\u7eb5\u5411\u5386\u53f2\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u7edf\u4e00\u8fdb\u884c\u4e8b\u4ef6\u9884\u6d4b\u548c\u9884\u540e\uff0c\u5728\u764c\u75c7\u60a3\u8005\u6570\u636e\u4e0a\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u98ce\u9669\u5206\u5c42\u80fd\u529b\u3002", "motivation": "\u7cbe\u51c6\u80bf\u7624\u5b66\u9700\u8981\u9884\u6d4b\u4e34\u5e8a\u4e8b\u4ef6\u548c\u75c5\u7a0b\u8f68\u8ff9\uff0c\u4f46\u5efa\u6a21\u7a00\u758f\u3001\u591a\u6a21\u6001\u7684\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u590d\u6742\u6570\u636e\u3002", "method": "\u63d0\u51faTwinWeaver\u5f00\u6e90\u6846\u67b6\uff0c\u5c06\u60a3\u8005\u7eb5\u5411\u5386\u53f2\u5e8f\u5217\u5316\u4e3a\u6587\u672c\u683c\u5f0f\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u7edf\u4e00\u8fdb\u884c\u4e8b\u4ef6\u9884\u6d4b\u548c\u9884\u540e\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efaGenie Digital Twin (GDT)\u6a21\u578b\uff0c\u4f7f\u752893,054\u540d20\u79cd\u764c\u75c7\u7c7b\u578b\u60a3\u8005\u7684\u6570\u636e\u3002", "result": "GDT\u663e\u8457\u964d\u4f4e\u9884\u6d4b\u8bef\u5dee\uff0c\u4e2d\u4f4dMASE\u4e3a0.87\uff08\u57fa\u7ebf0.97\uff0cp<0.001\uff09\u3002\u5728\u751f\u5b58\u3001\u8fdb\u5c55\u548c\u6cbb\u7597\u5207\u6362\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747C-index\u8fbe0.703\uff08\u6700\u4f73\u57fa\u7ebf0.662\uff09\u3002\u5728\u5206\u5e03\u5916\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u4e0a\uff0c\u96f6\u6837\u672c\u5339\u914d\u57fa\u7ebf\uff0c\u5fae\u8c03\u540e\u8868\u73b0\u66f4\u4f18\uff0c\u4e2d\u4f4dMASE 0.75-0.88\uff0c\u4e8b\u4ef6\u9884\u6d4b\u5e73\u5747C-index 0.672 vs 0.648\u3002", "conclusion": "TwinWeaver\u4e3a\u7eb5\u5411\u4e34\u5e8a\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u57fa\u7840\uff0c\u5176\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u63a8\u7406\u6269\u5c55\u589e\u5f3a\u4e86\u6a21\u578b\u5b9e\u7528\u6027\uff0c\u4e3a\u7cbe\u51c6\u80bf\u7624\u5b66\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2601.21346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21346", "abs": "https://arxiv.org/abs/2601.21346", "authors": ["Wei Zuo", "Chengyang Li", "Yikun Wang", "Bingyang Cheng", "Zeyi Ren", "Shuai Wang", "Derrick Wing Kwan Ng", "Yik-Chung Wu"], "title": "HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control", "comment": "Accepted by IEEE ICASSP 2026", "summary": "Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u4e3b\u52a8\u8c03\u4f18\u6846\u67b6HPTune\uff0c\u901a\u8fc7\u8bc4\u4f30\u5df2\u6267\u884c\u548c\u672a\u6267\u884c\u52a8\u4f5c\u6765\u9ad8\u6548\u8c03\u4f18MPC\u8fd0\u52a8\u89c4\u5212\u5668\u53c2\u6570\uff0c\u7ed3\u5408\u5feb\u901f\u7ea7\u548c\u6162\u901f\u7ea7\u8c03\u4f18\uff0c\u5e76\u6574\u5408\u591a\u666e\u52d2\u6fc0\u5149\u96f7\u8fbe\u589e\u5f3a\u8fd0\u52a8\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709MPC\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u901a\u5e38\u53ea\u8bc4\u4f30\u5df2\u6267\u884c\u52a8\u4f5c\uff0c\u5bfc\u81f4\u53c2\u6570\u66f4\u65b0\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u5931\u8d25\u4e8b\u4ef6\uff08\u5982\u969c\u788d\u7269\u63a5\u8fd1\u6216\u78b0\u649e\uff09\u7a00\u758f\u3002\u9700\u8981\u6269\u5c55\u8bc4\u4f30\u8303\u56f4\u4ee5\u6539\u5584\u8c03\u4f18\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u4e3b\u52a8\u8c03\u4f18\u6846\u67b6HPTune\uff1a1\uff09\u5feb\u901f\u7ea7\u8c03\u4f18\u91c7\u7528\u9884\u6d4b\u63a5\u8fd1\u901f\u5ea6\u548c\u9884\u6d4b\u63a5\u8fd1\u8ddd\u79bb\u7684\u98ce\u9669\u6307\u6807\uff1b2\uff09\u6162\u901f\u7ea7\u8c03\u4f18\u5229\u7528\u6269\u5c55\u8bc4\u4f30\u635f\u5931\u8fdb\u884c\u95ed\u73af\u53cd\u5411\u4f20\u64ad\uff1b3\uff09\u6574\u5408\u591a\u666e\u52d2\u6fc0\u5149\u96f7\u8fbe\u63d0\u4f9b\u969c\u788d\u7269\u901f\u5ea6\u548c\u4f4d\u7f6e\u4fe1\u606f\u4ee5\u589e\u5f3a\u8fd0\u52a8\u9884\u6d4b\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHPTune\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684MPC\u8c03\u4f18\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6848\u3002HPTune\u80fd\u591f\u901a\u8fc7\u5236\u5b9a\u5b89\u5168\u3001\u654f\u6377\u7684\u907f\u78b0\u7b56\u7565\u5b9e\u73b0\u60c5\u5883\u5b9a\u5236\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "conclusion": "HPTune\u6846\u67b6\u901a\u8fc7\u6269\u5c55\u8bc4\u4f30\u8303\u56f4\u5230\u672a\u6267\u884c\u52a8\u4f5c\uff0c\u7ed3\u5408\u5206\u5c42\u8c03\u4f18\u548c\u591a\u666e\u52d2\u6fc0\u5149\u96f7\u8fbe\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86MPC\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u53c2\u6570\u8c03\u4f18\u6548\u7387\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u654f\u6377\u7684\u907f\u78b0\u7b56\u7565\u3002"}}
{"id": "2601.20987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20987", "abs": "https://arxiv.org/abs/2601.20987", "authors": ["Md Muhtasim Munif Fahim", "Md Rezaul Karim"], "title": "Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings", "comment": null, "summary": "A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u5168\u7403\u513f\u7ae5\u53d1\u5c55\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u4f7f\u7528UNICEF 35.7\u4e07\u513f\u7ae5\u6570\u636e\uff0c\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u663e\u8457\u63d0\u5347\u53d1\u5c55\u8fdf\u7f13\u76d1\u6d4b\u6027\u80fd", "motivation": "\u6bcf\u5e74\u5927\u91cf\u513f\u7ae5\u7ecf\u5386\u53ef\u9884\u9632\u7684\u53d1\u5c55\u8fdf\u7f13\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u5728\u65b0\u56fd\u5bb6\u90e8\u7f72\u9762\u4e34\u6570\u636e\u74f6\u9888\uff1a\u53ef\u9760\u6a21\u578b\u9700\u8981\u6570\u5343\u6837\u672c\uff0c\u800c\u65b0\u9879\u76ee\u901a\u5e38\u53ea\u6709\u4e0d\u5230100\u4e2a\u6837\u672c", "method": "\u5f15\u5165\u9996\u4e2a\u5168\u7403\u513f\u7ae5\u53d1\u5c55\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u4f7f\u7528UNICEF\u8c03\u67e5\u6570\u636e\u572844\u4e2a\u56fd\u5bb6357,709\u540d\u513f\u7ae5\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6", "result": "\u4ec550\u4e2a\u8bad\u7ec3\u6837\u672c\u65f6\u5e73\u5747AUC\u8fbe0.65\uff0c\u4f18\u4e8e\u51b7\u542f\u52a8\u68af\u5ea6\u63d0\u5347\u76840.61\uff1b500\u6837\u672c\u65f6AUC\u8fbe0.73\uff1b\u96f6\u6837\u672c\u90e8\u7f72\u5230\u672a\u89c1\u56fd\u5bb6AUC\u6700\u9ad8\u8fbe0.84", "conclusion": "\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u53ef\u4ee5\u6539\u53d8\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0bSDG 4.2.1\u76d1\u6d4b\u7684\u673a\u5668\u5b66\u4e60\u53ef\u884c\u6027\uff0c\u9884\u8bad\u7ec3\u591a\u6837\u6027\u4f7f\u5c11\u6837\u672c\u6cdb\u5316\u6210\u4e3a\u53ef\u80fd"}}
{"id": "2601.20911", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20911", "abs": "https://arxiv.org/abs/2601.20911", "authors": ["Haochen Zhang", "Animesh Sinha", "Felix Juefei-Xu", "Haoyu Ma", "Kunpeng Li", "Zhipeng Fan", "Meng Dong", "Xiaoliang Dai", "Tingbo Hou", "Peizhao Zhang", "Zecheng He"], "title": "Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs", "comment": "19 pages, 19 figures, plan for TIP", "summary": "Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u9a6c\u5c14\u53ef\u592b\u591a\u8f6e\u5bf9\u8bdd\u56fe\u50cf\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5386\u53f2\u6761\u4ef6\u8bad\u7ec3\u6846\u67b6\u3001\u56de\u6eda\u5f0f\u7f16\u8f91\u548c\u57fa\u4e8e\u540d\u79f0\u7684\u4e2a\u6027\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5927\u591a\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\uff08\u4ec5\u4f9d\u8d56\u6700\u8fd1\u56fe\u50cf\uff09\uff0c\u65e0\u6cd5\u5904\u7406\u7528\u6237\u5f15\u7528\u5386\u53f2\u72b6\u6001\u3001\u64a4\u9500\u66f4\u6539\u6216\u8de8\u591a\u8f6e\u5f15\u7528\u5b9e\u4f53\u7b49\u590d\u6742\u573a\u666f\u3002", "method": "1) \u6784\u5efa\u975e\u9a6c\u5c14\u53ef\u592b\u591a\u8f6e\u6570\u636e\uff1a\u5305\u62ec\u56de\u6eda\u5f0f\u7f16\u8f91\uff08\u5f3a\u5236\u68c0\u7d22\u65e9\u671f\u89c6\u89c9\u72b6\u6001\uff09\u548c\u57fa\u4e8e\u540d\u79f0\u7684\u591a\u8f6e\u4e2a\u6027\u5316\uff08\u8de8\u8f6e\u6b21\u7ed1\u5b9a\u540d\u79f0\u4e0e\u5916\u89c2\uff09\uff1b2) \u5386\u53f2\u6761\u4ef6\u8bad\u7ec3\u4e0e\u63a8\u7406\u6846\u67b6\uff1a\u91c7\u7528\u4ee4\u724c\u7ea7\u7f13\u5b58\u9632\u6b62\u591a\u8f6e\u8eab\u4efd\u6f02\u79fb\uff1b3) \u6539\u8fdb\u9ad8\u4fdd\u771f\u56fe\u50cf\u91cd\u5efa\u548c\u53ef\u7f16\u8f91\u4e2a\u6027\u5316\uff1a\u5305\u62ec\u57fa\u4e8e\u91cd\u5efa\u7684DiT\u89e3\u4ee4\u724c\u5668\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u8bfe\u7a0b\u3002", "result": "\u663e\u5f0f\u8bad\u7ec3\u975e\u9a6c\u5c14\u53ef\u592b\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u5355\u8f6e\u7f16\u8f91\u548c\u4e2a\u6027\u5316\u6027\u80fd\u3002", "conclusion": "\u9488\u5bf9\u975e\u9a6c\u5c14\u53ef\u592b\u8bbe\u7f6e\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\u662f\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u7684\u5173\u952e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5355\u8f6e\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5386\u53f2\u5f15\u7528\u548c\u8de8\u8f6e\u4e00\u81f4\u6027\u7b49\u6311\u6218\u3002"}}
{"id": "2601.21548", "categories": ["cs.RO", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21548", "abs": "https://arxiv.org/abs/2601.21548", "authors": ["Irene Ambrosini", "Ingo Blakowski", "Dmitrii Zendrikov", "Cristiano Capone", "Luna Gava", "Giacomo Indiveri", "Chiara De Luca", "Chiara Bartolozzi"], "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning", "comment": null, "summary": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.", "AI": {"tldr": "\u4f7f\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u7a7a\u6c14\u66f2\u68cd\u7403\u5b9e\u65f6\u5b66\u4e60\u63a7\u5236\uff0c\u901a\u8fc7\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u5feb\u901f\u5f3a\u5316\u5b66\u4e60", "motivation": "\u7a7a\u6c14\u66f2\u68cd\u7403\u9700\u8981\u5728\u9ad8\u901f\u5ea6\u4e0b\u505a\u51fa\u77ac\u95f4\u51b3\u7b56\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002\u7814\u7a76\u65e8\u5728\u5c06\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u786c\u4ef6\u4e0e\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u5c55\u793a\u8111\u542f\u53d1\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u5feb\u901f\u4ea4\u4e92\u4efb\u52a1\u5e76\u652f\u6301\u667a\u80fd\u673a\u5668\u7684\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4fe1\u53f7\u6a21\u62df/\u6570\u5b57\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u4e0a\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u786c\u4ef6\u4e0e\u5b66\u4e60\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u3002\u5229\u7528\u56fa\u5b9a\u968f\u673a\u8fde\u63a5\u6355\u6349\u4efb\u52a1\u65f6\u95f4\u7ed3\u6784\uff0c\u5728\u8bfb\u51fa\u5c42\u91c7\u7528\u5c40\u90e8e-prop\u5b66\u4e60\u89c4\u5219\uff0c\u5229\u7528\u4e8b\u4ef6\u9a71\u52a8\u6d3b\u52a8\u5b9e\u73b0\u5feb\u901f\u9ad8\u6548\u5b66\u4e60\u3002\u901a\u8fc7\u8ba1\u7b97\u673a\u548c\u795e\u7ecf\u5f62\u6001\u82af\u7247\u5728\u73af\u8bbe\u7f6e\u8fdb\u884c\u5b9e\u65f6\u5b66\u4e60\u3002", "result": "\u7cfb\u7edf\u5728\u6781\u5c11\u6570\u8bd5\u9a8c\u4e2d\u6210\u529f\u5b9e\u73b0\u51b0\u7403\u4ea4\u4e92\uff0c\u5c55\u793a\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u8bad\u7ec3\u80fd\u529b\u3002\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5b66\u4e60\uff0c\u5c06\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u786c\u4ef6\u4e0e\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u63a7\u5236\u76f8\u8fde\u63a5\u3002", "conclusion": "\u8111\u542f\u53d1\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u5feb\u901f\u4ea4\u4e92\u4efb\u52a1\uff0c\u652f\u6301\u667a\u80fd\u673a\u5668\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u4e3a\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5b9e\u9645\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.21031", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21031", "abs": "https://arxiv.org/abs/2601.21031", "authors": ["Zongheng Guo", "Tao Chen", "Yang Jiao", "Yi Pan", "Xiao Hu", "Manuela Ferrario"], "title": "SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model", "comment": "31 pages, 9 figures, 14 tables", "summary": "Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.", "AI": {"tldr": "SIGMA-PPG\u662f\u4e00\u4e2a\u57fa\u4e8e\u7edf\u8ba1\u5148\u9a8c\u7684\u751f\u6210\u5f0fPPG\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6297\u63a9\u7801\u673a\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u572812\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dPPG\u4fe1\u53f7\u57fa\u7840\u6a21\u578b\u9762\u4e34\u4fe1\u53f7\u5185\u5728\u5197\u4f59\u548c\u566a\u58f0\u7684\u6311\u6218\u3002\u6807\u51c6\u63a9\u7801\u5efa\u6a21\u5bb9\u6613\u4ea7\u751f\u5e73\u51e1\u89e3\uff0c\u800c\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u6001\u5b66\u7cbe\u5ea6\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u7edf\u8ba1\u5148\u9a8c\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u63a9\u7801\u67b6\u6784(SIGMA-PPG)\uff0c\u5305\u542b\u5148\u9a8c\u5f15\u5bfc\u7684\u5bf9\u6297\u63a9\u7801\u673a\u5236\uff08\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u6559\u5e08\u7f51\u7edc\u5229\u7528\u7edf\u8ba1\u5148\u9a8c\u521b\u5efa\u5177\u6709\u6311\u6218\u6027\u7684\u5b66\u4e60\u8def\u5f84\uff09\u548c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u5b9e\u73b0\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5728\u8d85\u8fc7120,000\u5c0f\u65f6\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cSIGMA-PPG\u572812\u4e2a\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u4e2d\u76f8\u6bd45\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u5e73\u5747\u6027\u80fd\u3002", "conclusion": "SIGMA-PPG\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6297\u63a9\u7801\u673a\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86PPG\u4fe1\u53f7\u5efa\u6a21\u4e2d\u7684\u566a\u58f0\u548c\u5197\u4f59\u95ee\u9898\uff0c\u4e3aPPG\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21667", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21667", "abs": "https://arxiv.org/abs/2601.21667", "authors": ["Hao Ju", "Shaofei Huang", "Hongyu Li", "Zihan Ding", "Si Liu", "Meng Wang", "Zhedong Zheng"], "title": "From Instruction to Event: Sound-Triggered Mobile Manipulation", "comment": null, "summary": "Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.", "AI": {"tldr": "\u63d0\u51fa\u58f0\u97f3\u89e6\u53d1\u7684\u79fb\u52a8\u64cd\u4f5c\u65b0\u8303\u5f0f\uff0c\u8ba9\u667a\u80fd\u4f53\u4e3b\u52a8\u611f\u77e5\u58f0\u97f3\u6e90\u5e76\u6267\u884c\u64cd\u4f5c\uff0c\u65e0\u9700\u663e\u5f0f\u6307\u4ee4\uff0c\u901a\u8fc7Habitat-Echo\u5e73\u53f0\u548c\u5206\u5c42\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b0", "motivation": "\u5f53\u524d\u79fb\u52a8\u64cd\u4f5c\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u9884\u5b9a\u4e49\u6587\u672c\u6307\u4ee4\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u81ea\u4e3b\u6027\u548c\u5bf9\u52a8\u6001\u73af\u5883\u4e8b\u4ef6\u7684\u54cd\u5e94\u80fd\u529b\uff0c\u9700\u8981\u66f4\u4e3b\u52a8\u7684\u611f\u77e5\u548c\u4ea4\u4e92\u65b9\u5f0f", "method": "\u5f15\u5165\u58f0\u97f3\u89e6\u53d1\u7684\u79fb\u52a8\u64cd\u4f5c\u8303\u5f0f\uff0c\u5f00\u53d1Habitat-Echo\u6570\u636e\u5e73\u53f0\uff08\u96c6\u6210\u58f0\u5b66\u6e32\u67d3\u4e0e\u7269\u7406\u4ea4\u4e92\uff09\uff0c\u63d0\u51fa\u5305\u542b\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u5668\u548c\u4f4e\u5c42\u7b56\u7565\u6a21\u578b\u7684\u5206\u5c42\u57fa\u7ebf\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u8868\u660e\u57fa\u7ebf\u65b9\u6cd5\u4f7f\u667a\u80fd\u4f53\u80fd\u4e3b\u52a8\u68c0\u6d4b\u548c\u54cd\u5e94\u542c\u89c9\u4e8b\u4ef6\uff0c\u65e0\u9700\u9010\u6848\u6307\u4ee4\uff1b\u5728\u6311\u6218\u6027\u7684\u53cc\u58f0\u6e90\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u6210\u529f\u4ece\u91cd\u53e0\u58f0\u5b66\u5e72\u6270\u4e2d\u5206\u79bb\u4e3b\u58f0\u6e90\u6267\u884c\u9996\u6b21\u4ea4\u4e92\uff0c\u968f\u540e\u64cd\u4f5c\u6b21\u58f0\u6e90\u5bf9\u8c61", "conclusion": "\u58f0\u97f3\u89e6\u53d1\u7684\u79fb\u52a8\u64cd\u4f5c\u8303\u5f0f\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u81ea\u4e3b\u6027\uff0cHabitat-Echo\u5e73\u53f0\u548c\u5206\u5c42\u57fa\u7ebf\u65b9\u6cd5\u6709\u6548\u652f\u6301\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e3b\u52a8\u611f\u77e5\u548c\u4ea4\u4e92\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027"}}
{"id": "2601.21343", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21343", "abs": "https://arxiv.org/abs/2601.21343", "authors": ["Ellen Xiaoqing Tan", "Shehzaad Dhuliawala", "Jing Xu", "Ping Yu", "Sainbayar Sukhbaatar", "Jason Weston", "Olga Golovneva"], "title": "Self-Improving Pretraining: using post-trained models to pretrain better models", "comment": null, "summary": "Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u76f4\u63a5\u63d0\u5347\u6a21\u578b\u751f\u6210\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\uff0c\u907f\u514d\u540e\u8bad\u7ec3\u9636\u6bb5\u96be\u4ee5\u7ea0\u6b63\u9884\u8bad\u7ec3\u4e60\u5f97\u7684\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u4e3b\u8981\u901a\u8fc7\u6536\u96c6\u6602\u8d35\u6570\u636e\u96c6\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u5bf9\u9f50\u6765\u89e3\u51b3LLM\u7684\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u7ea0\u6b63\u9884\u8bad\u7ec3\u9636\u6bb5\u4e60\u5f97\u7684\u6a21\u5f0f\u3002\u9700\u8981\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5c31\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u5851\u9020\u4e86\u6a21\u578b\u7684\u6838\u5fc3\u884c\u4e3a\uff0c\u9632\u6b62\u4e0d\u5b89\u5168\u6216\u5e7b\u89c9\u8f93\u51fa\u88ab\u6df1\u5ea6\u5d4c\u5165\u3002", "method": "\u5f15\u5165\u65b0\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff1a\u6d41\u5f0f\u5904\u7406\u6587\u6863\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u6bcf\u4e00\u6b65\u6539\u8fdb\u63a5\u4e0b\u6765K\u4e2a\u751f\u6210\u7684token\u3002\u4f7f\u7528\u4e00\u4e2a\u7ecf\u8fc7\u540e\u8bad\u7ec3\u7684\u5f3a\u6a21\u578b\u6765\u8bc4\u4f30\u5019\u9009\u751f\u6210\uff08\u5305\u62ec\u6a21\u578brollout\u3001\u539f\u59cb\u540e\u7f00\u548c\u91cd\u5199\u540e\u7f00\uff09\u7684\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\u3002\u8bad\u7ec3\u65e9\u671f\u4f9d\u8d56\u539f\u59cb\u548c\u91cd\u5199\u540e\u7f00\uff0c\u968f\u7740\u6a21\u578b\u6539\u8fdb\uff0cRL\u5956\u52b1\u9ad8\u8d28\u91cf\u7684rollout\u3002", "result": "\u76f8\u6bd4\u6807\u51c6\u9884\u8bad\u7ec3\uff0c\u5728\u4e8b\u5b9e\u6027\u65b9\u9762\u83b7\u5f9736.2%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5b89\u5168\u6027\u65b9\u9762\u83b7\u5f9718.5%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5728\u6574\u4f53\u751f\u6210\u8d28\u91cf\u65b9\u9762\u83b7\u5f97\u9ad8\u8fbe86.3%\u7684\u80dc\u7387\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ece\u57fa\u7840\u5c42\u9762\u6784\u5efa\u4e86\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5b89\u5168\u3001\u66f4\u4e8b\u5b9e\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u76f4\u63a5\u96c6\u6210\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\u76ee\u6807\uff0c\u907f\u514d\u4e86\u540e\u8bad\u7ec3\u9636\u6bb5\u96be\u4ee5\u7ea0\u6b63\u9884\u8bad\u7ec3\u4e60\u5f97\u6a21\u5f0f\u7684\u95ee\u9898\u3002"}}
{"id": "2601.21220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21220", "abs": "https://arxiv.org/abs/2601.21220", "authors": ["Alvi Md Ishmam", "Najibul Haque Sarker", "Zaber Ibn Abdul Hakim", "Chris Thomas"], "title": "LAMP: Learning Universal Adversarial Perturbations for Multi-Image Tasks via Pre-trained Models", "comment": "Accepted in main technical track AAAI 2026", "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance across vision-language tasks. Recent advancements allow these models to process multiple images as inputs. However, the vulnerabilities of multi-image MLLMs remain unexplored. Existing adversarial attacks focus on single-image settings and often assume a white-box threat model, which is impractical in many real-world scenarios. This paper introduces LAMP, a black-box method for learning Universal Adversarial Perturbations (UAPs) targeting multi-image MLLMs. LAMP applies an attention-based constraint that prevents the model from effectively aggregating information across images. LAMP also introduces a novel cross-image contagious constraint that forces perturbed tokens to influence clean tokens, spreading adversarial effects without requiring all inputs to be modified. Additionally, an index-attention suppression loss enables a robust position-invariant attack. Experimental results show that LAMP outperforms SOTA baselines and achieves the highest attack success rates across multiple vision-language tasks and models.", "AI": {"tldr": "LAMP\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u56fe\u50cfMLLMs\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u7ea6\u675f\u548c\u8de8\u56fe\u50cf\u4f20\u67d3\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u5bf9\u6297\u653b\u51fb", "motivation": "\u591a\u56fe\u50cfMLLMs\u7684\u8106\u5f31\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u56fe\u50cf\u8bbe\u7f6e\u4e14\u5047\u8bbe\u767d\u76d2\u5a01\u80c1\u6a21\u578b\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5b9e\u7528", "method": "\u63d0\u51faLAMP\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7ea6\u675f\u9632\u6b62\u6a21\u578b\u6709\u6548\u805a\u5408\u8de8\u56fe\u50cf\u4fe1\u606f\uff1b2) \u8de8\u56fe\u50cf\u4f20\u67d3\u7ea6\u675f\u4f7f\u6270\u52a8\u4ee4\u724c\u5f71\u54cd\u5e72\u51c0\u4ee4\u724c\uff1b3) \u7d22\u5f15\u6ce8\u610f\u529b\u6291\u5236\u635f\u5931\u5b9e\u73b0\u4f4d\u7f6e\u4e0d\u53d8\u653b\u51fb", "result": "LAMP\u5728\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387", "conclusion": "LAMP\u4e3a\u591a\u56fe\u50cfMLLMs\u7684\u9ed1\u76d2\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u8fd9\u7c7b\u6a21\u578b\u7684\u5b89\u5168\u8106\u5f31\u6027"}}
{"id": "2601.21255", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21255", "abs": "https://arxiv.org/abs/2601.21255", "authors": ["Esteban Rodr\u00edguez-Betancourt", "Edgar Casasola-Murillo"], "title": "Hypersolid: Emergent Vision Representations via Short-Range Repulsion", "comment": "17 pages, 16 figures", "summary": "A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.", "AI": {"tldr": "Hypersolid\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u8868\u793a\u5b66\u4e60\u91cd\u65b0\u89e3\u91ca\u4e3a\u79bb\u6563\u6253\u5305\u95ee\u9898\uff0c\u4f7f\u7528\u77ed\u7a0b\u786c\u7403\u6392\u65a5\u9632\u6b62\u5c40\u90e8\u78b0\u649e\uff0c\u4ece\u800c\u907f\u514d\u8868\u793a\u5d29\u6e83", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u9632\u6b62\u8868\u793a\u5d29\u6e83\u662f\u4e00\u4e2a\u6301\u7eed\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5168\u5c40\u6b63\u5219\u5316\uff08\u5982\u6700\u5927\u5316\u8ddd\u79bb\u3001\u53bb\u76f8\u5173\u7ef4\u5ea6\u6216\u5f3a\u5236\u7279\u5b9a\u5206\u5e03\uff09\uff0c\u4f5c\u8005\u5e0c\u671b\u627e\u5230\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u5c06\u8868\u793a\u5b66\u4e60\u91cd\u65b0\u89e3\u91ca\u4e3a\u79bb\u6563\u6253\u5305\u95ee\u9898\uff0c\u5c06\u4fdd\u6301\u4fe1\u606f\u7b80\u5316\u4e3a\u4fdd\u6301\u5355\u5c04\u6027\uff0c\u4f7f\u7528Hypersolid\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ed\u7a0b\u786c\u7403\u6392\u65a5\u9632\u6b62\u5c40\u90e8\u78b0\u649e", "result": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u9ad8\u5206\u79bb\u51e0\u4f55\u673a\u5236\uff0c\u80fd\u591f\u4fdd\u6301\u589e\u5f3a\u591a\u6837\u6027\uff0c\u5728\u7ec6\u7c92\u5ea6\u548c\u4f4e\u5206\u8fa8\u7387\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "\u901a\u8fc7\u5c06\u8868\u793a\u5b66\u4e60\u89c6\u4e3a\u79bb\u6563\u6253\u5305\u95ee\u9898\u5e76\u4f7f\u7528\u5c40\u90e8\u786c\u7403\u6392\u65a5\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u9632\u6b62\u8868\u793a\u5d29\u6e83\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u5168\u5c40\u6b63\u5219\u5316\u65b9\u6cd5"}}
{"id": "2601.22074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22074", "abs": "https://arxiv.org/abs/2601.22074", "authors": ["Kevin Zakka", "Qiayuan Liao", "Brent Yi", "Louis Le Lay", "Koushil Sreenath", "Pieter Abbeel"], "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning", "comment": "Code is available at https://github.com/mujocolab/mjlab", "summary": "We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.", "AI": {"tldr": "mjlab\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408GPU\u52a0\u901f\u4eff\u771f\u3001\u6a21\u5757\u5316\u73af\u5883\u548c\u6700\u5c0f\u5316\u5b89\u88c5\u914d\u7f6e\uff0c\u63d0\u4f9b\u5355\u547d\u4ee4\u5b89\u88c5\u548c\u539f\u751fMuJoCo\u6570\u636e\u7ed3\u6784\u8bbf\u95ee", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\u5b58\u5728\u5b89\u88c5\u590d\u6742\u3001\u4f9d\u8d56\u591a\u3001\u914d\u7f6e\u7e41\u7410\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6613\u5b89\u88c5\u3001GPU\u52a0\u901f\u7684\u89e3\u51b3\u65b9\u6848\u6765\u964d\u4f4e\u673a\u5668\u4eba\u5b66\u4e60\u7684\u7814\u7a76\u95e8\u69db", "method": "\u91c7\u7528Isaac Lab\u5f15\u5165\u7684manager-based API\uff0c\u7ed3\u5408MuJoCo Warp\u8fdb\u884cGPU\u52a0\u901f\u7269\u7406\u4eff\u771f\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u6784\u5efa\u5757\uff08\u89c2\u6d4b\u3001\u5956\u52b1\u3001\u4e8b\u4ef6\uff09\uff0c\u5b9e\u73b0\u5355\u547d\u4ee4\u5b89\u88c5\u548c\u6700\u5c0f\u4f9d\u8d56", "result": "\u5f00\u53d1\u51famjlab\u6846\u67b6\uff0c\u5305\u542b\u901f\u5ea6\u8ddf\u8e2a\u3001\u8fd0\u52a8\u6a21\u4eff\u548c\u64cd\u4f5c\u4efb\u52a1\u7684\u53c2\u8003\u5b9e\u73b0\uff0c\u63d0\u4f9b\u76f4\u63a5\u8bbf\u95ee\u539f\u751fMuJoCo\u6570\u636e\u7ed3\u6784\u7684\u80fd\u529b", "conclusion": "mjlab\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u6613\u7528\u7684\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7GPU\u52a0\u901f\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u7814\u7a76\u95e8\u69db\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177"}}
{"id": "2601.21147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21147", "abs": "https://arxiv.org/abs/2601.21147", "authors": ["Kevin Han", "Haolin Cong", "Bowen Deng", "Amir Barati Farimani"], "title": "Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials", "comment": null, "summary": "Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u622a\u65ad\u534a\u5f84\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u56fa\u5b9a\u622a\u65ad\u534a\u5f84\uff0c\u663e\u8457\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\u80fd\uff08MLIPs\uff09\u7684\u5185\u5b58\u6d88\u8017\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u62df\u7a33\u5b9a\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\u80fd\uff08MLIPs\uff09\u5728\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u622a\u65ad\u534a\u5f84\u9650\u5236\u4e86MLIPs\u8fbe\u5230\u5b9e\u9645\u6a21\u62df\u89c4\u6a21\u7684\u80fd\u529b\u3002", "method": "\u9996\u6b21\u5f15\u5165\u52a8\u6001\u622a\u65ad\u534a\u5f84\u516c\u5f0f\uff0c\u901a\u8fc7\u9488\u5bf9\u6bcf\u4e2a\u539f\u5b50\u8bbe\u5b9a\u7279\u5b9a\u7684\u90bb\u5c45\u6570\u91cf\u6765\u8bf1\u5bfc\u5e95\u5c42\u539f\u5b50\u56fe\u7684\u7a00\u758f\u6027\uff0c\u4ece\u800c\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u548c\u63a8\u7406\u65f6\u95f4\u3002\u8be5\u65b9\u6cd5\u5728MACE\u3001Nequip\u3001Orbv3\u548cTensorNet\u56db\u79cd\u5148\u8fdbMLIPs\u4e0a\u5b9e\u73b0\u3002", "result": "\u52a8\u6001\u622a\u65ad\u65b9\u6cd5\u4f7f\u5185\u5b58\u6d88\u8017\u51cf\u5c112.26\u500d\uff0c\u63a8\u7406\u65f6\u95f4\u52a0\u5feb2.04\u500d\uff08\u5177\u4f53\u6570\u503c\u56e0\u6a21\u578b\u548c\u539f\u5b50\u7cfb\u7edf\u800c\u5f02\uff09\u3002\u5728\u6750\u6599\u548c\u5206\u5b50\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u56fa\u5b9a\u622a\u65ad\u6a21\u578b\u76f8\u6bd4\uff0c\u7cbe\u5ea6\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "\u52a8\u6001\u622a\u65ad\u534a\u5f84\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MLIPs\u7684\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6a21\u62df\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6240\u6709\u5b9e\u73b0\u548c\u8bad\u7ec3\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2601.21340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21340", "abs": "https://arxiv.org/abs/2601.21340", "authors": ["Lang Cao", "Qingyu Chen", "Yue Guo"], "title": "EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation", "comment": null, "summary": "Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.", "AI": {"tldr": "EHR-RAG\uff1a\u9488\u5bf9\u957f\u65f6\u7a0b\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8bbe\u8ba1\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u548c\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u3001\u81ea\u9002\u5e94\u8fed\u4ee3\u68c0\u7d22\u548c\u53cc\u8def\u5f84\u8bc1\u636e\u68c0\u7d22\u63a8\u7406\uff0c\u63d0\u5347\u4e34\u5e8a\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5305\u542b\u4e30\u5bcc\u7684\u7eb5\u5411\u4e34\u5e8a\u8bc1\u636e\uff0c\u4f46\u957f\u65f6\u7a0bEHR\u5e38\u8d85\u51faLLM\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u622a\u65ad\u6216\u7b80\u5355\u68c0\u7d22\u4f1a\u4e22\u5931\u4e34\u5e8a\u76f8\u5173\u4e8b\u4ef6\u548c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u7ed3\u6784\u5316EHR\u6570\u636e\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\u3002", "method": "\u63d0\u51faEHR-RAG\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u4e8b\u4ef6\u548c\u65f6\u95f4\u611f\u77e5\u6df7\u5408EHR\u68c0\u7d22\uff0c\u4fdd\u7559\u4e34\u5e8a\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\uff1b2\uff09\u81ea\u9002\u5e94\u8fed\u4ee3\u68c0\u7d22\uff0c\u9010\u6b65\u4f18\u5316\u67e5\u8be2\u4ee5\u6269\u5927\u8bc1\u636e\u8986\u76d6\uff1b3\uff09\u53cc\u8def\u5f84\u8bc1\u636e\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u540c\u65f6\u68c0\u7d22\u548c\u63a8\u7406\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u8bc1\u636e\u3002", "result": "\u5728\u56db\u4e2a\u957f\u65f6\u7a0bEHR\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEHR-RAG\u59cb\u7ec8\u4f18\u4e8e\u6700\u5f3a\u7684LLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747Macro-F1\u63d0\u9ad8\u4e8610.76%\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u7684LLMs\u5728\u7ed3\u6784\u5316EHR\u6570\u636e\u7684\u4e34\u5e8a\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0cEHR-RAG\u6846\u67b6\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u68c0\u7d22\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0bEHR\u5904\u7406\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2601.21170", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21170", "abs": "https://arxiv.org/abs/2601.21170", "authors": ["Augusto Santos", "Teresa Santos", "Catarina Rodrigues", "Jos\u00e9 M. F. Moura"], "title": "The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset", "comment": null, "summary": "Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u534f\u65b9\u5dee\u77e9\u9635\u5e42\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u6d8c\u73b0\u73b0\u8c61\uff08\u5982\u766b\u75eb\u53d1\u4f5c\u3001\u5ba2\u6237\u6d41\u5931\uff09\uff0c\u901a\u8fc7\u63ed\u793a\u6f5c\u5728\u56e0\u679c\u7ed3\u6784\u5b9e\u73b0\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u590d\u6742\u7cfb\u7edf\u4e2d\u6d8c\u73b0\u73b0\u8c61\uff08\u766b\u75eb\u53d1\u4f5c\u3001\u5ba2\u6237\u6d41\u5931\u3001\u75ab\u60c5\u7206\u53d1\uff09\u5f80\u5f80\u6e90\u4e8e\u9690\u85cf\u7684\u56e0\u679c\u4ea4\u4e92\u4f5c\u7528\uff0c\u4f46\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u672a\u77e5\u4e14\u90e8\u5206\u53ef\u89c2\u6d4b\uff0c\u96be\u4ee5\u65e9\u671f\u68c0\u6d4b\u3002", "method": "\u4ece\u5355\u53c2\u6570\u4f30\u8ba1\u5668\u65cf\uff08\u7ecf\u9a8c\u534f\u65b9\u5dee\u6216\u7cbe\u5ea6\u77e9\u9635\u7684\u5e42\uff09\u4e2d\u5b66\u4e60\u6700\u4f18\u7279\u5f81\u8868\u793a\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u6a21\u5757\u5bf9\u5b66\u4e60\u5230\u7684\u8868\u793a\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u63ed\u793a\u9a71\u52a8\u5173\u952e\u4e8b\u4ef6\u7684\u5e95\u5c42\u7ed3\u6784\u3002", "result": "\u5728\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u548c\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u4e0a\u83b7\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u8bc1\u660e\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u4e14\u6700\u4f18\u534f\u65b9\u5dee\u5e42\u663e\u793a\u51fa\u826f\u597d\u7684\u53ef\u8bc6\u522b\u6027\u5e76\u6355\u83b7\u7ed3\u6784\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u9884\u6d4b\u6d8c\u73b0\u73b0\u8c61\uff0c\u8fd8\u80fd\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u7edf\u8ba1\u7ed3\u6784\u5b9e\u73b0\u9884\u6d4b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u7edf\u4e00\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.21345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21345", "abs": "https://arxiv.org/abs/2601.21345", "authors": ["Ruiqi Liu", "Boyu Diao", "Zijia An", "Runjie Shao", "Zhulin An", "Fei Wang", "Yongjun Xu"], "title": "Semantic-Guided Dynamic Sparsification for Pre-Trained Model-based Class-Incremental Learning", "comment": null, "summary": "Class-Incremental Learning (CIL) requires a model to continually learn new classes without forgetting old ones. A common and efficient solution freezes a pre-trained model and employs lightweight adapters, whose parameters are often forced to be orthogonal to prevent inter-task interference. However, we argue that this parameter-constraining method is detrimental to plasticity. To this end, we propose Semantic-Guided Dynamic Sparsification (SGDS), a novel method that proactively guides the activation space by governing the orientation and rank of its subspaces through targeted sparsification. Specifically, SGDS promotes knowledge transfer by encouraging similar classes to share a compact activation subspace, while simultaneously preventing interference by assigning non-overlapping activation subspaces to dissimilar classes. By sculpting class-specific sparse subspaces in the activation space, SGDS effectively mitigates interference without imposing rigid constraints on the parameter space. Extensive experiments on various benchmark datasets demonstrate the state-of-the-art performance of SGDS.", "AI": {"tldr": "SGDS\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u52a8\u6001\u7a00\u758f\u5316\uff0c\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5851\u9020\u7c7b\u7279\u5b9a\u7a00\u758f\u5b50\u7a7a\u95f4\uff0c\u89e3\u51b3CIL\u4e2d\u6b63\u4ea4\u53c2\u6570\u7ea6\u675f\u635f\u5bb3\u53ef\u5851\u6027\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u4e0e\u5e72\u6270\u9884\u9632\u7684\u5e73\u8861\u3002", "motivation": "\u4f20\u7edfCIL\u65b9\u6cd5\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u4f7f\u7528\u8f7b\u91cf\u9002\u914d\u5668\uff0c\u901a\u5e38\u5f3a\u5236\u53c2\u6570\u6b63\u4ea4\u4ee5\u9632\u6b62\u4efb\u52a1\u95f4\u5e72\u6270\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u53c2\u6570\u7ea6\u675f\u65b9\u6cd5\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u53ef\u5851\u6027\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u5f15\u5bfc\u7684\u52a8\u6001\u7a00\u758f\u5316(SGDS)\uff0c\u901a\u8fc7\u5b9a\u5411\u7a00\u758f\u5316\u63a7\u5236\u6fc0\u6d3b\u7a7a\u95f4\u5b50\u7a7a\u95f4\u7684\u65b9\u5411\u548c\u79e9\uff0c\u4fc3\u8fdb\u76f8\u4f3c\u7c7b\u5171\u4eab\u7d27\u51d1\u6fc0\u6d3b\u5b50\u7a7a\u95f4\u4ee5\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\uff0c\u540c\u65f6\u4e3a\u4e0d\u76f8\u4f3c\u7c7b\u5206\u914d\u4e0d\u91cd\u53e0\u7684\u6fc0\u6d3b\u5b50\u7a7a\u95f4\u4ee5\u9632\u6b62\u5e72\u6270\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSGDS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SGDS\u901a\u8fc7\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5851\u9020\u7c7b\u7279\u5b9a\u7a00\u758f\u5b50\u7a7a\u95f4\uff0c\u6709\u6548\u7f13\u89e3\u5e72\u6270\u800c\u4e0d\u5bf9\u53c2\u6570\u7a7a\u95f4\u65bd\u52a0\u521a\u6027\u7ea6\u675f\uff0c\u4e3aCIL\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21968", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21968", "abs": "https://arxiv.org/abs/2601.21968", "authors": ["Jing Xiong", "Hui Shen", "Shansan Gong", "Yuxin Cheng", "Jianghan Shen", "Chaofan Tao", "Haochen Tan", "Haoli Bai", "Lifeng Shang", "Ngai Wong"], "title": "OVD: On-policy Verbal Distillation", "comment": "Technical Report", "summary": "Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io", "AI": {"tldr": "\u63d0\u51faOVD\u6846\u67b6\uff0c\u7528\u8f68\u8ff9\u5339\u914d\u66ff\u4ee3token\u7ea7\u6982\u7387\u5339\u914d\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u79bb\u6563\u8a00\u8bed\u8bc4\u5206\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709token\u7ea7on-policy\u84b8\u998f\u65b9\u6cd5\u9700\u8981token\u7ea7\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4ea4\u4e92\u73af\u5883\u53cd\u9988\uff0c\u4e14\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u4e25\u91cd\u5185\u5b58\u74f6\u9888", "method": "\u5f15\u5165On-policy Verbal Distillation (OVD)\u6846\u67b6\uff0c\u7528\u79bb\u6563\u8a00\u8bed\u8bc4\u5206\uff080-9\uff09\u8fdb\u884c\u8f68\u8ff9\u5339\u914d\uff0c\u66ff\u4ee3token\u7ea7\u6982\u7387\u5339\u914d\uff0c\u907f\u514dtoken\u7ea7\u5bf9\u9f50\uff0c\u5141\u8bb8\u5b66\u751f\u6a21\u578b\u81ea\u7531\u63a2\u7d22\u8f93\u51fa\u7a7a\u95f4", "result": "\u5728Web\u95ee\u7b54\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cWeb Q&A\u4efb\u52a1\u5e73\u5747EM\u63d0\u5347+12.9%\uff0c\u6570\u5b66\u57fa\u51c6\u63d0\u5347+25.7%\uff08\u4ec5\u4f7f\u7528\u4e00\u4e2a\u968f\u673a\u6837\u672c\u8bad\u7ec3\uff09\uff0c\u540c\u65f6\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8", "conclusion": "OVD\u6846\u67b6\u901a\u8fc7\u8a00\u8bed\u8bc4\u5206\u8f68\u8ff9\u5339\u914d\u89e3\u51b3\u4e86\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u7684\u5185\u5b58\u74f6\u9888\u548c\u63a2\u7d22\u9650\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b"}}
{"id": "2601.21293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21293", "abs": "https://arxiv.org/abs/2601.21293", "authors": ["Changyu Li", "Dingcheng Huang", "Kexuan Yao", "Xiaoya Ni", "Lijuan Shen", "Fei Luo"], "title": "Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning", "comment": "Submitted to IEEE Transactions on Reliability", "summary": "Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.", "AI": {"tldr": "PG-TMT\uff1a\u4e00\u79cd\u7528\u4e8e\u65cb\u8f6c\u673a\u68b0\u6545\u969c\u9884\u8b66\u7684\u7269\u7406\u5f15\u5bfc\u5fae\u578bMamba Transformer\uff0c\u901a\u8fc7\u4e09\u5206\u652f\u7f16\u7801\u5668\u3001\u7269\u7406\u5bf9\u9f50\u673a\u5236\u548c\u6781\u503c\u7406\u8bba\u6821\u51c6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u5728\u7ebf\u72b6\u6001\u76d1\u6d4b\u3002", "motivation": "\u65cb\u8f6c\u673a\u68b0\u7684\u53ef\u9760\u6027\u4e2d\u5fc3\u9884\u6d4b\u9700\u8981\u5728\u975e\u5e73\u7a33\u5de5\u51b5\u3001\u57df\u504f\u79fb\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0b\u4fdd\u6301\u51c6\u786e\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u4e14\u53ef\u9884\u6d4b\u7684\u8bef\u62a5\u7387\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u7269\u7406\u5f15\u5bfc\u5fae\u578bMamba Transformer (PG-TMT)\uff1a1) \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u4e3b\u5e72\u6355\u6349\u5fae\u77ac\u53d8\uff1b2) Tiny-Mamba\u72b6\u6001\u7a7a\u95f4\u5206\u652f\u5efa\u6a21\u8fd1\u7ebf\u6027\u957f\u7a0b\u52a8\u6001\uff1b3) \u8f7b\u91cf\u7ea7\u5c40\u90e8Transformer\u7f16\u7801\u8de8\u901a\u9053\u5171\u632f\u3002\u901a\u8fc7\u89e3\u6790\u65f6\u9891\u6620\u5c04\u5c06\u6ce8\u610f\u529b\u8c31\u4e0e\u7ecf\u5178\u8f74\u627f\u6545\u969c\u9636\u6b21\u5e26\u5bf9\u9f50\uff0c\u5e76\u4f7f\u7528\u6781\u503c\u7406\u8bba\u5efa\u6a21\u5065\u5eb7\u5206\u6570\u8d85\u9650\uff0c\u5b9e\u73b0\u76ee\u6807\u8bef\u62a5\u5f3a\u5ea6\u7684\u9608\u503c\u6821\u51c6\u3002", "result": "\u5728CWRU\u3001Paderborn\u3001XJTU-SY\u548c\u5de5\u4e1a\u8bd5\u70b9\u6570\u636e\u96c6\u4e0a\uff0cPG-TMT\u5728\u65e0\u6cc4\u6f0f\u6d41\u5f0f\u534f\u8bae\u4e0b\u83b7\u5f97\u66f4\u9ad8\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387AUC\uff08\u4e3b\u8981\u9488\u5bf9\u4e0d\u5e73\u8861\u95ee\u9898\uff09\uff0c\u7ade\u4e89\u6027\u6216\u66f4\u597d\u7684ROC AUC\uff0c\u5728\u5339\u914d\u8bef\u62a5\u5f3a\u5ea6\u4e0b\u66f4\u77ed\u7684\u5e73\u5747\u68c0\u6d4b\u65f6\u95f4\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u8de8\u57df\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "PG-TMT\u901a\u8fc7\u7269\u7406\u5bf9\u9f50\u8868\u793a\u4e0eEVT\u6821\u51c6\u51b3\u7b56\u89c4\u5219\u7684\u7ed3\u5408\uff0c\u4e3a\u53ef\u9760\u6027\u4e2d\u5fc3\u7684\u9884\u6d4b\u548c\u5065\u5eb7\u7ba1\u7406\u63d0\u4f9b\u4e86\u6821\u51c6\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u76f4\u63a5\u90e8\u7f72\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2601.21312", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21312", "abs": "https://arxiv.org/abs/2601.21312", "authors": ["Xiaozhuang Li", "Xindi Tang", "Fang He"], "title": "Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach", "comment": null, "summary": "With the rapid expansion of electric vehicles (EVs) and charging infrastructure, the effective management of Autonomous Electric Taxi (AET) fleets faces a critical challenge in environments with dynamic and uncertain charging availability. While most existing research assumes a static charging network, this simplification creates a significant gap between theoretical models and real-world operations. To bridge this gap, we propose GAT-PEARL, a novel meta-reinforcement learning framework that learns an adaptive operational policy. Our approach integrates a graph attention network (GAT) to effectively extract robust spatial representations under infrastructure layouts and model the complex spatiotemporal relationships of the urban environment, and employs probabilistic embeddings for actor-critic reinforcement learning (PEARL) to enable rapid, inference-based adaptation to changes in charging network layouts without retraining. Through extensive simulations on real-world data in Chengdu, China, we demonstrate that GAT-PEARL significantly outperforms conventional reinforcement learning baselines, showing superior generalization to unseen infrastructure layouts and achieving higher overall operational efficiency in dynamic settings.", "AI": {"tldr": "\u63d0\u51faGAT-PEARL\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u6982\u7387\u5d4c\u5165\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u52a8\u6001\u5145\u7535\u7f51\u7edc\u4e0b\u81ea\u52a8\u9a7e\u9a76\u7535\u52a8\u51fa\u79df\u8f66\u8f66\u961f\u7ba1\u7406\u95ee\u9898", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u548c\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u81ea\u52a8\u9a7e\u9a76\u7535\u52a8\u51fa\u79df\u8f66\u8f66\u961f\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u7684\u5145\u7535\u53ef\u7528\u6027\u73af\u5883\u4e2d\u9762\u4e34\u7ba1\u7406\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u5927\u591a\u5047\u8bbe\u9759\u6001\u5145\u7535\u7f51\u7edc\uff0c\u8fd9\u4e0e\u73b0\u5b9e\u8fd0\u8425\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "method": "\u63d0\u51faGAT-PEARL\u5143\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a1) \u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u63d0\u53d6\u9c81\u68d2\u7684\u7a7a\u95f4\u8868\u793a\u5e76\u5efa\u6a21\u590d\u6742\u7684\u65f6\u7a7a\u5173\u7cfb\uff1b2) \u91c7\u7528\u6982\u7387\u5d4c\u5165\u7684actor-critic\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u5feb\u901f\u63a8\u7406\u9002\u5e94", "result": "\u57fa\u4e8e\u4e2d\u56fd\u6210\u90fd\u771f\u5b9e\u6570\u636e\u7684\u6a21\u62df\u663e\u793a\uff0cGAT-PEARL\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u5145\u7535\u7f51\u7edc\u5e03\u5c40\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u6574\u4f53\u8fd0\u8425\u6548\u7387", "conclusion": "GAT-PEARL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u5145\u7535\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u7535\u52a8\u51fa\u79df\u8f66\u8f66\u961f\u7ba1\u7406\u95ee\u9898\uff0c\u586b\u8865\u4e86\u7406\u8bba\u6a21\u578b\u4e0e\u73b0\u5b9e\u8fd0\u8425\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.21608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21608", "abs": "https://arxiv.org/abs/2601.21608", "authors": ["Saisubramaniam Gopalakrishnan", "Harikrishnan P M", "Dagnachew Birru"], "title": "Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget", "comment": null, "summary": "Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \\emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u4f01\u4e1a\u7ea7\u667a\u80fd\u6587\u6863\u5904\u7406\u7cfb\u7edf\u7684\u9a8c\u8bc1\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u57fa\u4e8e\u641c\u7d22\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u95ee\u9898\uff0c\u901a\u8fc7\u6bd4\u8f83\u591a\u79cd\u641c\u7d22\u7b56\u7565\u6765\u6700\u5927\u5316\u5728\u6709\u9650\u9884\u7b97\u5185\u53d1\u73b0\u7684\u4e0d\u540c\u6545\u969c\u7c7b\u578b\u6570\u91cf\u3002", "motivation": "\u4f01\u4e1a\u7ea7\u667a\u80fd\u6587\u6863\u5904\u7406\u7cfb\u7edf\u5728\u91d1\u878d\u3001\u4fdd\u9669\u548c\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u65e9\u671f\u7cfb\u7edf\u9a8c\u8bc1\u9700\u8981\u5728\u6709\u9650\u9884\u7b97\u4e0b\u53d1\u73b0\u591a\u6837\u5316\u7684\u6545\u969c\u673a\u5236\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bc6\u522b\u5355\u4e2a\u6700\u574f\u60c5\u51b5\u6587\u6863\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u57fa\u4e8e\u641c\u7d22\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u95ee\u9898\uff0c\u5728\u6587\u6863\u914d\u7f6e\u7684\u7ec4\u5408\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u5b9e\u4f8b\u5316\u7ed3\u6784\u98ce\u9669\u7279\u5f81\u4ee5\u8bf1\u5bfc\u73b0\u5b9e\u6545\u969c\u6761\u4ef6\u3002\u6bd4\u8f83\u4e86\u8fdb\u5316\u7b97\u6cd5\u3001\u7fa4\u4f53\u667a\u80fd\u3001\u8d28\u91cf\u591a\u6837\u6027\u3001\u5b66\u4e60\u578b\u548c\u91cf\u5b50\u7b97\u6cd5\u7b49\u591a\u79cd\u641c\u7d22\u7b56\u7565\u3002", "result": "\u4e0d\u540c\u6c42\u89e3\u5668\u5728\u76f8\u540c\u9884\u7b97\u4e0b\u80fd\u53d1\u73b0\u5176\u4ed6\u65b9\u6cd5\u672a\u53d1\u73b0\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u6ca1\u6709\u5355\u4e00\u7b56\u7565\u5177\u6709\u7edd\u5bf9\u4f18\u52bf\u3002\u6240\u6709\u6c42\u89e3\u5668\u7684\u5e76\u96c6\u6700\u7ec8\u80fd\u8986\u76d6\u89c2\u5bdf\u5230\u7684\u6545\u969c\u7a7a\u95f4\uff0c\u4f46\u4f9d\u8d56\u4efb\u4f55\u5355\u4e00\u65b9\u6cd5\u90fd\u4f1a\u7cfb\u7edf\u6027\u5730\u5ef6\u8fdf\u91cd\u8981\u98ce\u9669\u7684\u53d1\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u6c42\u89e3\u5668\u4e4b\u95f4\u5b58\u5728\u5185\u5728\u4e92\u8865\u6027\uff0c\u9700\u8981\u57fa\u4e8e\u7ec4\u5408\u7b56\u7565\u7684SBST\u65b9\u6cd5\u6765\u8fdb\u884c\u7a33\u5065\u7684\u5de5\u4e1aIDP\u9a8c\u8bc1\u3002"}}
{"id": "2601.21359", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21359", "abs": "https://arxiv.org/abs/2601.21359", "authors": ["Luan Pham"], "title": "Graph-Free Root Cause Analysis", "comment": null, "summary": "Failures in complex systems demand rapid Root Cause Analysis (RCA) to prevent cascading damage. Existing RCA methods that operate without dependency graph typically assume that the root cause having the highest anomaly score. This assumption fails when faults propagate, as a small delay at the root cause can accumulate into a much larger anomaly downstream. In this paper, we propose PRISM, a simple and efficient framework for RCA when the dependency graph is absent. We formulate a class of component-based systems under which PRISM performs RCA with theoretical guarantees. On 735 failures across 9 real-world datasets, PRISM achieves 68% Top-1 accuracy, a 258% improvement over the best baseline, while requiring only 8ms per diagnosis.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u65e0\u9700\u4f9d\u8d56\u56fe\u5373\u53ef\u8fdb\u884c\u6839\u56e0\u5206\u6790\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u5728735\u4e2a\u771f\u5b9e\u6545\u969c\u6848\u4f8b\u4e2d\u8fbe\u523068%\u7684Top-1\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u5347258%\uff0c\u6bcf\u6b21\u8bca\u65ad\u4ec5\u97008ms\u3002", "motivation": "\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u9700\u8981\u5feb\u901f\u6839\u56e0\u5206\u6790\u4ee5\u9632\u6b62\u7ea7\u8054\u635f\u5bb3\u3002\u73b0\u6709\u65e0\u9700\u4f9d\u8d56\u56fe\u7684RCA\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6839\u56e0\u5177\u6709\u6700\u9ad8\u7684\u5f02\u5e38\u5206\u6570\uff0c\u4f46\u5f53\u6545\u969c\u4f20\u64ad\u65f6\uff0c\u6839\u56e0\u5904\u7684\u5fae\u5c0f\u5ef6\u8fdf\u53ef\u80fd\u5728\u4e0b\u6e38\u7d2f\u79ef\u6210\u66f4\u5927\u7684\u5f02\u5e38\uff0c\u5bfc\u81f4\u8fd9\u4e00\u5047\u8bbe\u5931\u6548\u3002", "method": "\u63d0\u51faPRISM\u6846\u67b6\uff0c\u4e3a\u7f3a\u4e4f\u4f9d\u8d56\u56fe\u7684\u7cfb\u7edf\u8bbe\u8ba1RCA\u65b9\u6cd5\u3002\u5b9a\u4e49\u4e86\u7ec4\u4ef6\u5316\u7cfb\u7edf\u7684\u4e00\u7c7b\u6a21\u578b\uff0c\u5728\u8be5\u6a21\u578b\u4e0bPRISM\u80fd\u591f\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u6839\u56e0\u5206\u6790\u3002", "result": "\u57289\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684735\u4e2a\u6545\u969c\u6848\u4f8b\u4e0a\uff0cPRISM\u8fbe\u523068%\u7684Top-1\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u5347258%\uff0c\u6bcf\u6b21\u8bca\u65ad\u4ec5\u97008\u6beb\u79d2\u3002", "conclusion": "PRISM\u662f\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684RCA\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u56fe\u5373\u53ef\u5728\u6545\u969c\u4f20\u64ad\u573a\u666f\u4e2d\u51c6\u786e\u8bc6\u522b\u6839\u56e0\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.21760", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21760", "abs": "https://arxiv.org/abs/2601.21760", "authors": ["Ruian Tie", "Wenbo Xiong", "Zhengyu Shi", "Xinyu Su", "Chenyu jiang", "Libo Wu", "Hao Li"], "title": "Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling", "comment": null, "summary": "Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.", "AI": {"tldr": "\u63d0\u51faZSSD\u96f6\u6837\u672c\u7edf\u8ba1\u964d\u5c3a\u5ea6\u6846\u67b6\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u7269\u7406\u4e00\u81f4\u6027\u6c14\u5019\u5148\u9a8c\u548c\u7edf\u4e00\u5750\u6807\u5f15\u5bfc\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7269\u7406\u4e0d\u4e00\u81f4\u548c\u68af\u5ea6\u6d88\u5931\u95ee\u9898", "motivation": "\u4f20\u7edf\u76d1\u7763\u5f0f\u6c14\u5019\u964d\u5c3a\u5ea6\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u4e14\u4e0e\u518d\u5206\u6790\u6570\u636e\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u5168\u7403\u6c14\u5019\u6a21\u578b\uff1b\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u5b58\u5728\u7269\u7406\u4e0d\u4e00\u81f4\u6027\u548c\u5927\u5c3a\u5ea6\u56e0\u5b50\u4e0b\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898", "method": "\u63d0\u51faZSSD\u96f6\u6837\u672c\u7edf\u8ba1\u964d\u5c3a\u5ea6\u6846\u67b6\uff1a1) \u4ece\u518d\u5206\u6790\u6570\u636e\u5b66\u4e60\u7269\u7406\u4e00\u81f4\u6027\u6c14\u5019\u5148\u9a8c\uff0c\u4ee5\u5730\u7403\u7269\u7406\u8fb9\u754c\u548c\u65f6\u5e8f\u4fe1\u606f\u4e3a\u6761\u4ef6\u786e\u4fdd\u7269\u7406\u6709\u6548\u6027\uff1b2) \u5f15\u5165\u7edf\u4e00\u5750\u6807\u5f15\u5bfc\u7b56\u7565\u89e3\u51b3DPS\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u786e\u4fdd\u4e0e\u5927\u5c3a\u5ea6\u573a\u7684\u4e00\u81f4\u6027", "result": "ZSSD\u572899\u767e\u5206\u4f4d\u8bef\u5dee\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6210\u529f\u91cd\u5efa\u590d\u6742\u5929\u6c14\u4e8b\u4ef6\uff08\u5982\u70ed\u5e26\u6c14\u65cb\uff09\uff0c\u5e76\u5728\u5f02\u8d28GCMs\u4e2d\u8868\u73b0\u826f\u597d", "conclusion": "ZSSD\u4e3a\u96f6\u6837\u672c\u6c14\u5019\u964d\u5c3a\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7269\u7406\u4e00\u81f4\u6027\u5148\u9a8c\u548c\u7edf\u4e00\u5750\u6807\u5f15\u5bfc\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u79cdGCMs\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2601.21909", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21909", "abs": "https://arxiv.org/abs/2601.21909", "authors": ["Shaojie Wang", "Liang Zhang"], "title": "From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning", "comment": null, "summary": "Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\\% and 4.63\\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.", "AI": {"tldr": "CoMT\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8ba4\u77e5\u5bf9\u9f50\u65b9\u6cd5\u6539\u8fdbLLM\u540e\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e13\u6ce8\u4e8e\u62bd\u8c61\u63a8\u7406\u6a21\u5f0f\u5b66\u4e60\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4efb\u52a1\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5f53\u524dLLM\u540e\u8bad\u7ec3\u65b9\u6cd5\u5c06\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u4f5c\u4e3a\u57fa\u672c\u5355\u5143\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u89e3\u51b3\u95ee\u9898\u7684\u4e24\u9636\u6bb5\u8ba4\u77e5\u8fc7\u7a0b\uff08\u5148\u83b7\u53d6\u62bd\u8c61\u7b56\u7565\uff0c\u518d\u9002\u5e94\u5177\u4f53\u95ee\u9898\uff09\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u62bd\u8c61\u7b56\u7565\u4e0e\u95ee\u9898\u7279\u5b9a\u6267\u884c\u7ea0\u7f20\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) Chain-of-Meta-Thought (CoMT)\uff1a\u4e13\u6ce8\u4e8e\u62bd\u8c61\u63a8\u7406\u6a21\u5f0f\u7684\u6709\u76d1\u7763\u5b66\u4e60\uff0c\u4e0d\u6d89\u53ca\u5177\u4f53\u6267\u884c\uff0c\u83b7\u53d6\u53ef\u6cdb\u5316\u7684\u7b56\u7565\uff1b2) Confidence-Calibrated Reinforcement Learning (CCRL)\uff1a\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u5956\u52b1\u4f18\u5316\u4e2d\u95f4\u6b65\u9aa4\u7684\u4efb\u52a1\u9002\u5e94\uff0c\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u57284\u4e2a\u6a21\u578b\u548c8\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u65b9\u6cd5\uff0c\u5206\u5e03\u5185\u6027\u80fd\u63d0\u53472.19%\uff0c\u5206\u5e03\u5916\u6027\u80fd\u63d0\u53474.63%\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1165-70%\uff0ctoken\u6d88\u8017\u51cf\u5c1150%\u3002", "conclusion": "\u5c06\u540e\u8bad\u7ec3\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u539f\u5219\u5bf9\u9f50\u4e0d\u4ec5\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u8ba4\u77e5\u542f\u53d1\u65b9\u6cd5\u5728LLM\u4f18\u5316\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2601.21452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21452", "abs": "https://arxiv.org/abs/2601.21452", "authors": ["Yu Xie", "Xing Kai Ren", "Ying Qi", "Hu Yao"], "title": "SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation", "comment": "arXiv admin note: text overlap with arXiv:2506.19235", "summary": "While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a \"Symmetric Conservatism\" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the \"Reward Collapse\" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a \"Boost Factor\" to high-potential cold start items to achieve super-linear updates and employs an \"Entropy Aware Penalty\" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSAGE\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2dLLM\u6a21\u578b\u4f9d\u8d56\u72ec\u7acb\u8bcd\u6c47\u8868\u7684\u95ee\u9898\uff0c\u5e76\u6539\u8fdbOneRec\u7684GBPO\u65b9\u6cd5\u5b58\u5728\u7684\u5bf9\u79f0\u4fdd\u5b88\u4e3b\u4e49\u95ee\u9898\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u4fe1\u53f7\u89e3\u8026\u548c\u975e\u5bf9\u79f0\u81ea\u9002\u5e94\u52a8\u6001\u673a\u5236\u63d0\u5347\u51b7\u542f\u52a8\u6027\u80fd\u548c\u63a8\u8350\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u5982OneRec\u9700\u8981\u6784\u5efa\u72ec\u7acb\u7684\u8bcd\u6c47\u8868\uff0c\u5bfc\u81f4\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u5dee\u3002\u540c\u65f6\uff0c\u5176GBPO\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\"\u5bf9\u79f0\u4fdd\u5b88\u4e3b\u4e49\"\u95ee\u9898\uff0c\u6291\u5236\u51b7\u542f\u52a8\u7269\u54c1\u7684\u66f4\u65b0\u52a8\u91cf\uff0c\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\u65e0\u6cd5\u9632\u6b62\u591a\u6837\u6027\u5d29\u6e83\u3002", "method": "\u63d0\u51faSAGE\u4f18\u5316\u6846\u67b6\uff1a1) \u5e8f\u5217\u7ea7\u4fe1\u53f7\u89e3\u8026\uff1a\u7ed3\u5408\u51e0\u4f55\u5e73\u5747\u91cd\u8981\u6027\u6bd4\u7387\u548c\u89e3\u8026\u7684\u591a\u76ee\u6807\u4f18\u52bf\uff0c\u6d88\u9664token\u7ea7\u65b9\u5dee\uff0c\u89e3\u51b3\"\u5956\u52b1\u5d29\u6e83\"\u95ee\u9898\uff1b2) \u975e\u5bf9\u79f0\u81ea\u9002\u5e94\u52a8\u6001\uff1a\u6784\u5efa\u52a8\u6001\u68af\u5ea6\u6d41\u5f62\uff0c\u5bf9\u9ad8\u6f5c\u529b\u51b7\u542f\u52a8\u7269\u54c1\u5e94\u7528\"\u63d0\u5347\u56e0\u5b50\"\u5b9e\u73b0\u8d85\u7ebf\u6027\u66f4\u65b0\uff0c\u5e76\u4f7f\u7528\"\u71b5\u611f\u77e5\u60e9\u7f5a\"\u6253\u7834\u4fe1\u606f\u8327\u623f\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cSAGE\u6709\u6548\u91ca\u653e\u51b7\u542f\u52a8\u6d41\u91cf\u5e76\u7ef4\u6301\u63a8\u8350\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86GBPO\u7684\u6570\u503c\u7a33\u5b9a\u6027\u3002", "conclusion": "SAGE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u590d\u7528\u5f00\u6e90LLM\u67b6\u6784\u800c\u65e0\u9700\u6784\u5efa\u72ec\u7acb\u8bcd\u6c47\u8868\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5bf9\u79f0\u4fdd\u5b88\u4e3b\u4e49\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u7684\u540c\u65f6\u63d0\u5347\u4e86\u51b7\u542f\u52a8\u6027\u80fd\u548c\u63a8\u8350\u591a\u6837\u6027\u3002"}}
{"id": "2601.21937", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21937", "abs": "https://arxiv.org/abs/2601.21937", "authors": ["Shuangshuang Ying", "Zheyu Wang", "Yunjian Peng", "Jin Chen", "Yuhao Wu", "Hongbin Lin", "Dingyu He", "Siyi Liu", "Gengchen Yu", "YinZhu Piao", "Yuchen Wu", "Xin Gui", "Zhongyuan Peng", "Xin Li", "Xeron Du", "Libo Qin", "YiXin Cao", "Ge Zhang"], "title": "Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities", "comment": null, "summary": "Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.", "AI": {"tldr": "DeR2\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u53d7\u63a7\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u56db\u79cd\u8bc1\u636e\u8bbf\u95ee\u673a\u5236\u5206\u79bb\u68c0\u7d22\u635f\u5931\u4e0e\u63a8\u7406\u635f\u5931\uff0c\u63ed\u793a\u6a21\u578b\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u771f\u5b9e\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u65b0\u9896\u79d1\u5b66\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u7aef\u5230\u7aefRAG\u8bc4\u4f30\u4e2d\u63a8\u7406\u4e0e\u68c0\u7d22\u3001\u5de5\u5177\u94fe\u9009\u62e9\u6df7\u6dc6\uff0c\u4e14\u53d7\u5230\u53c2\u6570\u8bb0\u5fc6\u548c\u7f51\u7edc\u6570\u636e\u6ce2\u52a8\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1DeR2\u6df1\u5ea6\u7814\u7a76\u6c99\u7bb1\uff0c\u901a\u8fc7\u56db\u79cd\u8bc1\u636e\u8bbf\u95ee\u673a\u5236\uff08\u4ec5\u6307\u4ee4\u3001\u4ec5\u6982\u5ff5\u3001\u4ec5\u76f8\u5173\u6587\u6863\u3001\u5b8c\u6574\u6587\u6863\u96c6\uff09\u5206\u79bb\u8bc1\u636e\u83b7\u53d6\u4e0e\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u9a8c\u8bc1\u9632\u6b62\u53c2\u6570\u6cc4\u6f0f\uff0c\u6784\u5efa\u57fa\u4e8e2023-2025\u5e74\u7406\u8bba\u8bba\u6587\u7684\u51bb\u7ed3\u6587\u6863\u5e93\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1a\u90e8\u5206\u6a21\u578b\u5b58\u5728\u6a21\u5f0f\u5207\u6362\u8106\u5f31\u6027\uff08\u5b8c\u6574\u6587\u6863\u96c6\u8868\u73b0\u6bd4\u4ec5\u6307\u4ee4\u66f4\u5dee\uff09\uff0c\u53e6\u4e00\u4e9b\u6a21\u578b\u5b58\u5728\u7ed3\u6784\u6027\u6982\u5ff5\u8bef\u7528\uff08\u80fd\u6b63\u786e\u547d\u540d\u6982\u5ff5\u4f46\u65e0\u6cd5\u5c06\u5176\u4f5c\u4e3a\u7a0b\u5e8f\u6267\u884c\uff09\u3002", "conclusion": "DeR2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53d7\u63a7\u73af\u5883\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u5b9e\u8d28\u6027\u5c40\u9650\u548c\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u9519\u8bef\u5f52\u56e0\u6846\u67b6\u3002"}}
{"id": "2601.21981", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.21981", "abs": "https://arxiv.org/abs/2601.21981", "authors": ["Geonhee Jo", "Mingu Kang", "Kangmin Lee", "Minho Lee", "Pascal Bauer", "Sang-Ki Ko"], "title": "VERSA: Verified Event Data Format for Reliable Soccer Analytics", "comment": "13 pages, 5 figures, 3 tables", "summary": "Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.", "AI": {"tldr": "VERSA\u662f\u4e00\u4e2a\u9488\u5bf9\u8db3\u7403\u4e8b\u4ef6\u6d41\u6570\u636e\u7684\u7cfb\u7edf\u5316\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u68c0\u6d4b\u548c\u7ea0\u6b63\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u4e0b\u6e38\u5206\u6790\u53ef\u9760\u6027\u3002", "motivation": "\u4e8b\u4ef6\u6d41\u6570\u636e\u5728\u4f53\u80b2\u5206\u6790\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u5b58\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff08\u5982\u4e8b\u4ef6\u987a\u5e8f\u9519\u8bef\u3001\u4e8b\u4ef6\u7f3a\u5931\uff09\uff0c\u9650\u5236\u4e86\u5206\u6790\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faVERSA\u6846\u67b6\uff0c\u57fa\u4e8e\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u5b9a\u4e49\u6709\u6548\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u6d41\u6570\u636e\u4e2d\u5f02\u5e38\u6a21\u5f0f\u7684\u81ea\u52a8\u68c0\u6d4b\u548c\u7ea0\u6b63\u3002", "result": "\u5728K League 1\uff082024\u8d5b\u5b63\uff09\u6570\u636e\u4e2d\u53d1\u73b018.81%\u7684\u4e8b\u4ef6\u5b58\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\uff1bVERSA\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6570\u636e\u63d0\u4f9b\u5546\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u6539\u5584\u4e86VAEP\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u9a8c\u8bc1\u8fc7\u7a0b\u80fd\u6709\u6548\u63d0\u9ad8\u6570\u636e\u9a71\u52a8\u5206\u6790\u7684\u53ef\u9760\u6027\uff0cVERSA\u4e3a\u8db3\u7403\u5206\u6790\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7edf\u4e00\u7684\u6570\u636e\u8868\u793a\u57fa\u7840\u3002"}}
{"id": "2601.21993", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21993", "abs": "https://arxiv.org/abs/2601.21993", "authors": ["Dhiogo de S\u00e1", "Carlos Schmiedel", "Carlos Pereira Lopes"], "title": "Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems", "comment": "28 pages, 2 figures", "summary": "Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems", "AI": {"tldr": "\u63d0\u51faLiquid Interfaces\uff08\u6db2\u6001\u63a5\u53e3\uff09\u534f\u8c03\u8303\u5f0f\uff0c\u5c06\u63a5\u53e3\u4ece\u9759\u6001\u6280\u672f\u6784\u4ef6\u8f6c\u53d8\u4e3a\u8fd0\u884c\u65f6\u901a\u8fc7\u610f\u56fe\u8868\u8fbe\u548c\u8bed\u4e49\u534f\u5546\u4ea7\u751f\u7684\u77ed\u6682\u5173\u7cfb\u4e8b\u4ef6\uff0c\u4ee5\u652f\u6301\u81ea\u9002\u5e94\u3001\u6982\u7387\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u67b6\u6784\u96be\u4ee5\u652f\u6301\u5177\u6709\u81ea\u9002\u5e94\u3001\u6982\u7387\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u63a8\u7406\u80fd\u529b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u800c\u7cfb\u7edf\u96c6\u6210\u4ecd\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u63a5\u53e3\u548c\u786e\u5b9a\u6027\u5951\u7ea6\uff0c\u8fd9\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u80fd\u529b\u3002", "method": "\u63d0\u51faLiquid Interfaces\u534f\u8c03\u8303\u5f0f\uff0c\u5c06\u63a5\u53e3\u89c6\u4e3a\u77ed\u6682\u7684\u5173\u7cfb\u4e8b\u4ef6\u800c\u975e\u6301\u4e45\u6280\u672f\u6784\u4ef6\uff1b\u5f62\u5f0f\u5316\u8be5\u6a21\u578b\u5e76\u8bbe\u8ba1Liquid Interface Protocol\uff08LIP\uff09\uff0c\u7ba1\u7406\u610f\u56fe\u9a71\u52a8\u4ea4\u4e92\u3001\u534f\u5546\u6267\u884c\u548c\u5728\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u77ed\u6682\u6027\u6267\u884c\uff1b\u8ba8\u8bba\u6cbb\u7406\u5f71\u54cd\u5e76\u63d0\u4f9b\u53c2\u8003\u67b6\u6784\u3002", "result": "\u5efa\u7acb\u4e86Liquid Interfaces\u7684\u7406\u8bba\u6846\u67b6\u548c\u534f\u8bae\u89c4\u8303\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u4e3a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u81ea\u9002\u5e94\u534f\u8c03\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002", "conclusion": "Liquid Interfaces\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u534f\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u5c06\u63a5\u53e3\u52a8\u6001\u5316\u548c\u4e8b\u4ef6\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u63a5\u53e3\u5728\u652f\u6301\u81ea\u9002\u5e94\u3001\u6982\u7387\u6027\u667a\u80fd\u4f53\u534f\u8c03\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.22020", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22020", "abs": "https://arxiv.org/abs/2601.22020", "authors": ["Chengyi Cai", "Zesheng Ye", "Peike Li", "Bo Han", "Jianzhong Qi", "Feng Liu"], "title": "Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning", "comment": null, "summary": "Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.", "AI": {"tldr": "\u63d0\u51faViKeR\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u7684\u5173\u952e\u4ee4\u724c\u6b63\u5219\u5316\u6765\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9057\u5fd8\u5b66\u4e60\uff0c\u91cd\u70b9\u5173\u6ce8\u7b54\u6848\u4e2d\u4e0d\u540c\u4ee4\u724c\u7684\u91cd\u8981\u6027\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709MLLM\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u6cbf\u7528LLM\u7684\u65b9\u6cd5\uff0c\u5c06\u6240\u6709\u7b54\u6848\u4ee4\u724c\u540c\u7b49\u5bf9\u5f85\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u5728\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u91cd\u8981\u6027\u4e0d\u540c\u7684\u95ee\u9898\uff0c\u4e14\u53ea\u5173\u6ce8\u8bed\u8a00\u6a21\u6001\u800c\u5ffd\u7565\u4e86\u89c6\u89c9\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u5f15\u5bfc\u7684\u5173\u952e\u4ee4\u724c\u6b63\u5219\u5316(ViKeR)\uff1a1)\u5229\u7528\u65e0\u5173\u89c6\u89c9\u8f93\u5165\u9884\u6d4b\u7406\u60f3\u7684\u9057\u5fd8\u540e\u4ee4\u724c\u7ea7\u5206\u5e03\uff1b2)\u7528\u8fd9\u4e9b\u5206\u5e03\u6b63\u5219\u5316\u9057\u5fd8\u8fc7\u7a0b\uff0c\u4f18\u5148\u5904\u7406\u5173\u952e\u4ee4\u724c\uff1b3)\u901a\u8fc7\u4fe1\u606f\u71b5\u5b9a\u4e49\u5173\u952e\u4ee4\u724c\uff1b4)\u901a\u8fc7\u4ee4\u724c\u7ea7\u68af\u5ea6\u91cd\u52a0\u6743\u653e\u5927\u5173\u952e\u4ee4\u724c\u7684\u66f4\u65b0\u3002", "result": "\u5728MLLMU\u548cCLEAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6267\u884c\u9057\u5fd8\u5b66\u4e60\uff0c\u540c\u65f6\u51cf\u8f7b\u9057\u5fd8\u6548\u5e94\u5e76\u4fdd\u6301\u56de\u7b54\u7684\u8fde\u8d2f\u6027\u3002", "conclusion": "ViKeR\u65b9\u6cd5\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u548c\u5173\u952e\u4ee4\u724c\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MLLM\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u4e2d\u4ee4\u724c\u91cd\u8981\u6027\u5dee\u5f02\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9057\u5fd8\u5b66\u4e60\u7684\u6548\u679c\u3002"}}
{"id": "2601.21669", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21669", "abs": "https://arxiv.org/abs/2601.21669", "authors": ["Abhijeet Sinha", "Sundari Elango", "Dianbo Liu"], "title": "Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling", "comment": null, "summary": "Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f3a\u5316\u5b66\u4e60\u4e2d\u671f\u671b\u56de\u62a5\u6700\u5927\u5316\u76ee\u6807\u672c\u8eab\u4f1a\u5bfc\u81f4\u7ed3\u679c\u5c42\u9762\u7684\u6a21\u5f0f\u5d29\u6e83\uff0c\u5e76\u63d0\u51fa\u9006\u6982\u7387\u7f29\u653e\u4fee\u6b63\u65b9\u6cd5IPS-GRPO\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u8bb8\u591a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u5b58\u5728\u591a\u4e2a\u8d28\u91cf\u76f8\u5f53\u7684\u6700\u4f18\u89e3\uff0c\u4f46\u6807\u51c6\u671f\u671b\u56de\u62a5\u6700\u5927\u5316\u8bad\u7ec3\u7684\u7b56\u7565\u901a\u5e38\u4f1a\u5d29\u6e83\u5230\u5c11\u6570\u7ed3\u679c\u4e0a\u3002\u4f20\u7edf\u89e3\u91ca\u5f52\u56e0\u4e8e\u63a2\u7d22\u4e0d\u8db3\u6216\u6b63\u5219\u5316\u4e0d\u591f\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u671f\u671b\u56de\u62a5\u76ee\u6807\u672c\u8eab\u7684\u7ed3\u6784\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9006\u6982\u7387\u7f29\u653e(IPS)\u4fee\u6b63\u65b9\u6cd5\uff0c\u79fb\u9664\u5b66\u4e60\u4fe1\u53f7\u4e2d\u7684\u6982\u7387\u653e\u5927\u6548\u5e94\u3002\u5c06\u5176\u5b9e\u4f8b\u5316\u4e3aIPS-GRPO\uff08Group Relative Policy Optimization\u7684\u6539\u8fdb\u7248\uff09\uff0c\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u6216\u67b6\u6784\u6539\u53d8\u3002", "result": "\u5728\u63a8\u7406\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\uff0cIPS-GRPO\u6301\u7eed\u51cf\u5c11\u7ed3\u679c\u5c42\u9762\u7684\u6a21\u5f0f\u5d29\u6e83\uff0c\u540c\u65f6\u5339\u914d\u6216\u8d85\u8fc7\u57fa\u7ebf\u6027\u80fd\uff0c\u8868\u660e\u4fee\u6b63\u76ee\u6807\u51fd\u6570\u6bd4\u6dfb\u52a0\u63a2\u7d22\u542f\u53d1\u5f0f\u65b9\u6cd5\u66f4\u5173\u952e\u3002", "conclusion": "\u671f\u671b\u56de\u62a5\u76ee\u6807\u672c\u8eab\u4f1a\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\uff0c\u9006\u6982\u7387\u7f29\u653e\u4fee\u6b63\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u5b66\u4e60\u52a8\u6001\uff0c\u4ea7\u751f\u4e0e\u5956\u52b1\u6210\u6bd4\u4f8b\u7684\u7ec8\u7aef\u5206\u5e03\uff0c\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u9632\u6b62\u5d29\u6e83\uff0c\u662f\u53ef\u9760\u591a\u6a21\u6001\u7b56\u7565\u4f18\u5316\u7684\u5173\u952e\u3002"}}
{"id": "2601.21794", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21794", "abs": "https://arxiv.org/abs/2601.21794", "authors": ["Yejin Kim", "Dongjun Hwang", "Sungmin Cha", "Junsuk Choe"], "title": "Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.", "AI": {"tldr": "KVW\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u524a\u5f31\u77e5\u8bc6\u5411\u91cf\u6765\u5b9e\u73b0\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u6570\u636e\u9057\u5fd8\uff0c\u907f\u514d\u68af\u5ea6\u8ba1\u7b97\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u548c\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u7b49\u4e25\u91cd\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u9057\u5fd8\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "KVW\uff08\u77e5\u8bc6\u5411\u91cf\u524a\u5f31\uff09\u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u76f4\u63a5\u5e72\u9884\u5b8c\u6574\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u68af\u5ea6\u8ba1\u7b97\u3002\u5b83\u8bc6\u522b\u5728\u9057\u5fd8\u96c6\u4e0a\u6fc0\u6d3b\u7684\u77e5\u8bc6\u5411\u91cf\uff0c\u5e76\u9010\u6b65\u524a\u5f31\u5b83\u4eec\u7684\u8d21\u732e\uff0c\u9632\u6b62\u6a21\u578b\u5229\u7528\u4e0d\u826f\u77e5\u8bc6\u3002", "result": "\u5728MLLMU\u548cCLEAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKVW\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u9057\u5fd8-\u4fdd\u7559\u6743\u8861\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u548c\u57fa\u4e8eLoRA\u7684\u9057\u5fd8\u65b9\u6cd5\u3002", "conclusion": "KVW\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u79fb\u9664\u7279\u5b9a\u6570\u636e\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.21945", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21945", "abs": "https://arxiv.org/abs/2601.21945", "authors": ["Qingshan Wang", "Clara C. Wanjura", "Florian Marquardt"], "title": "Dependence of Equilibrium Propagation Training Success on Network Architecture", "comment": "9 pages, 5 figures", "summary": "The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08\u5e73\u8861\u4f20\u64ad\uff09\u5728\u5c40\u90e8\u8fde\u63a5\u6676\u683c\u67b6\u6784\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u7a00\u758f\u7f51\u7edc\u80fd\u8fbe\u5230\u4e0e\u5bc6\u96c6\u7f51\u7edc\u76f8\u5f53\u7684\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u80fd\u6e90\u6d88\u8017\u4e0d\u53ef\u6301\u7eed\u589e\u957f\uff0c\u8fd9\u63a8\u52a8\u4e86\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u548c\u57fa\u4e8e\u7269\u7406\u7684\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u7406\u8bba\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5168\u8fde\u63a5\u6216\u5bc6\u96c6\u8fde\u63a5\u7f51\u7edc\uff0c\u8fd9\u4e9b\u67b6\u6784\u5728\u5b9e\u9a8c\u5b9e\u73b0\u4e0a\u5b58\u5728\u8fde\u63a5\u6027\u7ea6\u675f\u7b49\u6311\u6218", "method": "\u91c7\u7528\u57fa\u4e8e\u7269\u7406\u7684\u8bad\u7ec3\u65b9\u6cd5\u2014\u2014\u5e73\u8861\u4f20\u64ad\uff0c\u5728XY\u6a21\u578b\u4e0a\u8bad\u7ec3\u5c40\u90e8\u8fde\u63a5\u7684\u6676\u683c\u67b6\u6784\uff0c\u63a2\u7d22\u4e0d\u540c\u67b6\u6784\u5728\u5404\u79cd\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u8ffd\u8e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7a7a\u95f4\u5206\u5e03\u5f0f\u54cd\u5e94\u548c\u8026\u5408\u7684\u6f14\u5316", "result": "\u7a00\u758f\u7f51\u7edc\uff08\u4ec5\u542b\u5c40\u90e8\u8fde\u63a5\uff09\u80fd\u591f\u8fbe\u5230\u4e0e\u5bc6\u96c6\u7f51\u7edc\u76f8\u5f53\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u57fa\u4e8e\u5e73\u8861\u4f20\u64ad\u7684\u67b6\u6784\u6269\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc", "conclusion": "\u5c40\u90e8\u8fde\u63a5\u7684\u7a00\u758f\u7f51\u7edc\u67b6\u6784\u5728\u57fa\u4e8e\u7269\u7406\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u548c\u8282\u80fdAI\u7cfb\u7edf\u7684\u5b9e\u9645\u5b9e\u73b0\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003"}}
{"id": "2601.21978", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21978", "abs": "https://arxiv.org/abs/2601.21978", "authors": ["Shiqi Fan", "Quanming Yao", "Hongyi Nie", "Wentao Ma", "Zhen Wang", "Wen Hua"], "title": "Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.", "AI": {"tldr": "IGETR\u662f\u4e00\u4e2a\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548cLLM\u7684\u6df7\u5408\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u4e0a\u4e0b\u6587\u5173\u7cfb\u800c\u5ffd\u89c6\u7ed3\u6784\u5173\u7cfb\uff0c\u96be\u4ee5\u4ece\u52a8\u6001\u56fe\u4e2d\u63d0\u53d6\u76f8\u5173\u5b50\u56fe\uff0c\u5bfc\u81f4\u63a8\u7406\u7f3a\u4e4f\u7ed3\u6784\u4fe1\u606f\u3001\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u7279\u522b\u662f\u5728\u65f6\u5e8f\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b", "method": "\u63d0\u51faIGETR\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528\u65f6\u5e8fGNN\u8bc6\u522b\u7ed3\u6784\u548c\u65f6\u5e8f\u4e00\u81f4\u7684\u5019\u9009\u8def\u5f84\uff1b2) LLM\u5f15\u5bfc\u7684\u8def\u5f84\u7f16\u8f91\uff0c\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u4fee\u6b63\u903b\u8f91\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\uff1b3) \u6574\u5408\u7cbe\u70bc\u540e\u7684\u63a8\u7406\u8def\u5f84\u751f\u6210\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b", "result": "\u5728\u6807\u51c6TKG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728ICEWS\u6570\u636e\u96c6\u4e0aHits@1\u76f8\u5bf9\u63d0\u53475.6%\uff0cHits@3\u76f8\u5bf9\u63d0\u53478.1%\uff0c\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u5404\u7ec4\u4ef6\u6709\u6548\u6027", "conclusion": "IGETR\u901a\u8fc7\u7ed3\u5408GNN\u7684\u7ed3\u6784\u5316\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\u548cLLM\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u7ed3\u6784\u4fe1\u606f\u7f3a\u5931\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u53ef\u9760\u7684\u9884\u6d4b"}}
