<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053)
*Zibin Liu,Banglei Guan,Yang Shang,Shunkun Liang,Zhenbao Yu,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于事件相机的光流引导6DoF物体姿态跟踪方法，通过2D-3D混合特征提取和光流关联实现精确跟踪


<details>
  <summary>Details</summary>
Motivation: 传统相机在物体姿态跟踪中面临运动模糊、传感器噪声、部分遮挡和光照变化等挑战，而事件相机具有高动态范围和低延迟的优势，有望解决这些问题

Method: 1) 采用2D-3D混合特征提取策略从事件和物体模型中检测角点和边缘；2) 通过最大化时空窗口内事件关联概率搜索角点的光流；3) 在光流引导下建立角点与边缘的关联；4) 通过最小化角点与边缘距离迭代优化6DoF物体姿态

Result: 在模拟和真实事件数据上的实验结果表明，该方法在准确性和鲁棒性方面优于现有的基于事件相机的最先进方法

Conclusion: 提出的光流引导6DoF物体姿态跟踪方法有效利用了事件相机的优势，在复杂环境下实现了精确和鲁棒的姿态跟踪

Abstract: Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

</details>


### [2] [Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval](https://arxiv.org/abs/2512.21221)
*Dao Sy Duy Minh,Huynh Trung Kiet,Nguyen Lam Phu Quy,Phu-Hoa Pham,Tran Chi Nguyen*

Main category: cs.CV

TL;DR: 提出基于事件实体提取的轻量级两阶段图像检索方法，结合BM25实体过滤和BEiT-3多模态重排序，在OpenEvents v1基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像文本检索面临模糊查询、语境依赖、语言变异性和可扩展性等挑战，需要更有效的解决方案来处理复杂真实场景。

Method: 采用两阶段检索流程：第一阶段使用BM25基于事件中心实体提取进行高效候选过滤；第二阶段应用BEiT-3模型捕获深度多模态语义并进行结果重排序。

Result: 在OpenEvents v1基准测试中达到0.559的平均精度均值，显著优于先前基线方法。

Conclusion: 结合事件引导过滤和长文本视觉语言建模的方法在复杂真实场景中实现了准确高效的图像检索，为实际应用提供了有效解决方案。

Abstract: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval

</details>


### [3] [Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential](https://arxiv.org/abs/2512.21284)
*Shihao Zou,Jingjing Li,Wei Ji,Jincai Huang,Kai Wang,Guo Dan,Weixin Si,Yi Pan*

Main category: cs.CV

TL;DR: SpikeSurgSeg：首个基于脉冲神经网络的实时手术场景分割框架，在非GPU平台上实现8-20倍加速，性能媲美ANN模型


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型（特别是大型基础模型）虽然分割精度高，但计算需求大、功耗高，难以在资源受限的手术环境中实时部署。脉冲神经网络（SNN）作为高效计算范式具有潜力，但受限于标记数据稀缺和手术视频表示的稀疏性。

Method: 提出SpikeSurgSeg框架：1）采用手术场景掩码自编码预训练策略，通过分层管状掩码实现鲁棒的时空表示学习；2）基于预训练骨干网络，采用轻量级脉冲驱动分割头，在保持SNN低延迟特性的同时产生时间一致的预测。

Result: 在EndoVis18和内部SurgBleed数据集上，SpikeSurgSeg达到与SOTA ANN模型相当的mIoU，同时推理延迟降低至少8倍，相对于大多数基础模型基线实现20倍以上加速。

Conclusion: SpikeSurgSeg首次展示了SNN在手术场景分割中的潜力，为资源受限环境下的实时手术智能提供了高效解决方案，具有重要的临床应用前景。

Abstract: Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.

</details>


### [4] [Streaming Video Instruction Tuning](https://arxiv.org/abs/2512.21334)
*Jiaer Xia,Peixian Chen,Mengdan Zhang,Xing Sun,Kaiyang Zhou*

Main category: cs.CV

TL;DR: Streamo是一个实时流式视频LLM，作为通用交互助手，能够执行多种流式视频任务，包括实时叙述、动作理解、事件描述、时间事件定位和时间敏感问答。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频模型主要专注于问答或描述等狭窄任务，缺乏能够处理多种流式视频任务的通用交互助手。需要弥合离线视频感知模型与实时多模态助手之间的差距。

Method: 构建了Streamo-Instruct-465K大规模指令跟随数据集，涵盖多样化时间上下文和多任务监督。通过简化流水线在指令跟随数据集上进行端到端训练。

Result: Streamo在多种流式基准测试中表现出强大的时间推理能力、响应式交互和广泛泛化能力，在流式视频任务上取得优异性能。

Conclusion: Streamo弥合了离线视频感知模型与实时多模态助手之间的差距，向连续视频流中的统一智能视频理解迈出了一步。

Abstract: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams](https://arxiv.org/abs/2512.20631)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.LG

TL;DR: 该研究提出了一种无需训练的零训练时间漂移分析方法，用于评估基于Transformer的情感模型在真实社交媒体数据上的稳定性，发现事件驱动期间模型准确率下降高达23.4%，并开发了优于基线的新漂移指标。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在动态社交媒体内容上的稳定性问题尚未得到充分研究，特别是在真实世界事件期间。现有方法通常需要重新训练或复杂计算，缺乏适合生产部署的实时监控方案。

Method: 采用零训练方法，系统评估三种Transformer架构，在12,279个真实社交媒体帖子上进行统计验证。引入四种新的漂移指标，这些指标优于基于嵌入的基线方法，同时保持计算效率。

Result: 模型在事件驱动期间表现出显著不稳定性，准确率下降达23.4%。最大置信度下降13.0%（Bootstrap 95% CI: [9.1%, 16.5%]），与实际性能下降强相关。新指标在多个事件上表现出稳健的检测能力，超过行业监控阈值。

Conclusion: 该零训练方法可直接部署于实时情感监控系统，为Transformer模型在动态内容期间的行为提供了新见解，具有实际生产部署价值。

Abstract: We present a comprehensive zero-training temporal drift analysis of transformer-based sentiment models validated on authentic social media data from major real-world events. Through systematic evaluation across three transformer architectures and rigorous statistical validation on 12,279 authentic social media posts, we demonstrate significant model instability with accuracy drops reaching 23.4% during event-driven periods. Our analysis reveals maximum confidence drops of 13.0% (Bootstrap 95% CI: [9.1%, 16.5%]) with strong correlation to actual performance degradation. We introduce four novel drift metrics that outperform embedding-based baselines while maintaining computational efficiency suitable for production deployment. Statistical validation across multiple events confirms robust detection capabilities with practical significance exceeding industry monitoring thresholds. This zero-training methodology enables immediate deployment for real-time sentiment monitoring systems and provides new insights into transformer model behavior during dynamic content periods.

</details>


### [6] [TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform](https://arxiv.org/abs/2512.20761)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Henrik Albers,Oliver Müller*

Main category: cs.LG

TL;DR: TS-Arena平台解决时间序列基础模型评估中的信息泄露危机，通过在实时数据流上实施预注册机制，确保评估目标在推理时物理上不存在，从而建立严格的全局时间分割。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型存在评估危机，主要源于训练集和测试集重叠导致的信息泄露，以及全局模式向测试数据的非法转移。虽然学习共享时间动态是这些模型的主要优势，但在历史档案上的评估往往允许利用观测到的全局冲击，这违反了有效基准测试所需的独立性要求。

Method: 引入TS-Arena平台，将真正未知的未来作为确定性测试环境。通过实施实时数据流上的预注册机制，确保评估目标在推理时物理上不存在，从而强制执行严格的全局时间分割。该方法建立了一个移动的时间边界，防止历史污染。

Result: TS-Arena为在真实世界约束下比较基础模型提供了可持续的基础设施。该平台原型已在能源领域初步应用，可通过Hugging Face访问。

Conclusion: TS-Arena通过将真正未知的未来作为测试环境，恢复了预测的操作完整性，防止了历史污染，为时间序列基础模型提供了真实的泛化能力评估框架。

Abstract: While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.

</details>


### [7] [STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting](https://arxiv.org/abs/2512.21118)
*Shi Quan Foo,Chi-Ho Wong,Zhihan Gao,Dit-Yan Yeung,Ka-Hing Wong,Wai-Kin Wong*

Main category: cs.LG

TL;DR: STLDM是一种基于扩散模型的降水临近预报方法，通过变分自编码器和条件网络学习端到端的潜在表示，将任务分解为确定性预报和增强两个阶段，在多个雷达数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报对预防极端天气灾害至关重要，但现有方法面临挑战：确定性模型预测模糊，生成模型精度不佳。需要一种既能保持准确性又能生成清晰预测的方法。

Method: STLDM采用基于扩散的模型架构，结合变分自编码器和条件网络学习端到端的潜在表示。将任务分解为两个阶段：1）条件网络处理确定性预报阶段；2）潜在扩散模型执行增强阶段。

Result: 在多个雷达数据集上的实验表明，STLDM相比现有最先进方法取得了更优的性能，同时提高了推理效率。

Conclusion: STLDM通过将降水临近预报分解为确定性预报和增强两个阶段，有效解决了现有方法在准确性和清晰度方面的局限性，为复杂时空预测任务提供了简单而有效的解决方案。

Abstract: Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.

</details>


### [8] [BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft](https://arxiv.org/abs/2512.21165)
*Qizhi Wang*

Main category: cs.LG

TL;DR: BALLAST使用上下文赌博机替代Raft中静态随机超时机制，通过安全探索在长尾延迟和网络分区等挑战下显著提升可用性


<details>
  <summary>Details</summary>
Motivation: Raft中的随机选举超时机制在长尾延迟、抖动和分区恢复等场景下变得脆弱，重复的分裂投票会显著增加不可用时间

Method: BALLAST采用轻量级在线自适应机制，使用线性上下文赌博机（LinUCB变体）从离散的超时选项中选择，并通过安全探索在不稳定期间限制风险

Result: 在具有长尾延迟、丢包、相关突发、节点异构性和分区/恢复扰动的可重现离散事件模拟中，BALLAST在挑战性WAN环境下显著减少了恢复时间和不可写时间，同时在稳定LAN/WAN设置下保持竞争力

Conclusion: BALLAST通过上下文赌博机有效解决了Raft在复杂网络环境下的选举超时问题，相比传统随机超时和常见启发式方法，在保持轻量级的同时显著提升了系统可用性

Abstract: Randomized election timeouts are a simple and effective liveness heuristic for Raft, but they become brittle under long-tail latency, jitter, and partition recovery, where repeated split votes can inflate unavailability. This paper presents BALLAST, a lightweight online adaptation mechanism that replaces static timeout heuristics with contextual bandits. BALLAST selects from a discrete set of timeout "arms" using efficient linear contextual bandits (LinUCB variants), and augments learning with safe exploration to cap risk during unstable periods. We evaluate BALLAST on a reproducible discrete-event simulation with long-tail delay, loss, correlated bursts, node heterogeneity, and partition/recovery turbulence. Across challenging WAN regimes, BALLAST substantially reduces recovery time and unwritable time compared to standard randomized timeouts and common heuristics, while remaining competitive on stable LAN/WAN settings.

</details>


### [9] [Learning to Solve PDEs on Neural Shape Representations](https://arxiv.org/abs/2512.21311)
*Lilian Welschinger,Yilin Liu,Zican Wang,Niloy Mitra*

Main category: cs.LG

TL;DR: 提出一种无需网格、基于神经表示的表面PDE求解方法，可直接在神经域中求解表面偏微分方程，支持端到端工作流。


<details>
  <summary>Details</summary>
Motivation: 现代3D资产越来越多地采用神经表示，但现有的PDE求解器主要针对多边形/三角形网格，导致在神经域中无法直接求解表面PDE，需要显式提取网格或进行逐实例训练，阻碍了端到端工作流。

Method: 提出一种无网格公式，学习一个基于神经（局部）形状属性的局部更新算子，使表面PDE能够直接在神经数据所在位置求解。该算子与主流神经表面表示自然集成，只需在单个代表性形状上训练一次，即可泛化到不同形状和拓扑变化。

Result: 在解析基准测试（球体上的热方程和泊松求解）和不同表示的真实神经资产上，该方法略微优于CPM，同时与FEM保持合理接近，提供了首个在神经和经典表面表示上求解表面PDE的端到端管道。

Conclusion: 该方法实现了在神经域中直接求解表面PDE，无需显式网格化或逐实例优化，同时保持可微性，为神经形状分析和工程任务提供了有效的PDE求解解决方案。

Abstract: Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 论文提出结合LangChain多智能体系统和许可区块链的架构，用于确保自主AI系统的监控、策略执行和不可篡改审计


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在医疗、智慧城市等领域应用增长，但存在信任、监督和信息完整性等问题，需要确保其决策过程的可审计性和安全性

Method: 采用LangChain多智能体系统与Hyperledger Fabric许可区块链结合，将感知-概念化-行动周期与区块链治理层关联，验证输入、评估行动建议并记录执行结果

Result: 区块链安全验证能有效防止未授权操作，提供完整决策过程追溯，并在智能库存管理、交通信号控制等实验中保持合理操作延迟

Conclusion: 该框架为高影响力自主AI应用提供了通用系统，实现了既自主又负责任的AI部署

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: 论文挑战了ARC基准测试中性能差距主要源于推理缺陷的传统观点，提出视觉感知限制才是主要瓶颈，并通过两阶段实验验证了这一假设。


<details>
  <summary>Details</summary>
Motivation: 传统上认为ARC等推理基准测试中的性能差距反映了机器推理能力的不足，但作者质疑这一解释，认为视觉感知限制可能是主要原因。

Method: 设计了两阶段实验管道：第一阶段将图像独立转换为自然语言描述（感知阶段），第二阶段使用这些描述进行规则归纳和应用（推理阶段），以此分离感知和推理过程。

Result: 在三个ARC风格数据集上的实验表明，感知能力是性能差距的主导因素；手动检查发现约80%的模型失败源于感知错误而非推理错误。

Conclusion: ARC基准测试混淆了感知和推理挑战，现有性能差距可能高估了机器推理的缺陷，需要开发能够分离感知和推理的评估协议。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [12] [Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction](https://arxiv.org/abs/2512.21043)
*Cheng-Yu Kuo,Hirofumi Shin,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 提出一种基于物理能量抽象的方法，让机器人手通过触觉快速学习抓握力控制，无需外部传感或物体先验知识


<details>
  <summary>Details</summary>
Motivation: 在动态物体交互中调节抓握力以防止滑动是机器人操作的基本挑战，特别是在多滚动接触、物体属性未知、外部传感不可靠的情况下。人类仅凭触觉就能快速调节抓握力，受此启发，希望让机器人手也能通过触觉学习抓握力控制

Method: 提出物理信息能量抽象，将物体建模为虚拟能量容器。通过手指施加功率与物体保留能量之间的不一致性，提供物理基础信号来推断滑动感知的稳定性。基于此抽象，采用基于模型的学习和规划，从触觉传感高效建模能量动力学，并进行实时抓握力优化

Result: 在仿真和硬件实验中，该方法能够在几分钟内从零开始学习抓握力控制，有效减少滑动，延长不同运动-物体组合的抓握持续时间，且不依赖外部传感或物体先验知识

Conclusion: 通过物理能量抽象和模型学习，实现了机器人仅凭触觉快速学习抓握力控制的能力，为解决动态物体交互中的滑动问题提供了有效方案

Abstract: Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.

</details>
