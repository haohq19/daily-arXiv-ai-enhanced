<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning](https://arxiv.org/abs/2512.04219)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur\\*

Main category: cs.CV

TL;DR: PARSE是一个无监督学习框架，通过多尺度递归预测器从流式视频中学习层次化事件结构，预测误差峰值自然形成事件边界，在多个基准测试中达到最先进的流式方法性能。


<details>
  <summary>Details</summary>
Motivation: 人类自然地将连续经验感知为时间嵌套的事件层次结构，但计算机视觉需要能够预测性和层次性地分割视频的模型，而不仅仅是回顾性地分割。

Method: PARSE采用统一框架，通过多尺度递归预测器学习视频的层次化事件结构：低层建模短期动态，高层通过基于注意力的反馈整合长期上下文，预测误差的瞬时峰值自然形成事件边界。

Result: 在Breakfast Actions、50 Salads和Assembly 101三个基准测试中，PARSE在流式方法中达到最先进性能，在时间对齐（H-GEBD）和结构一致性（TED, hF1）方面与离线基线相当。

Conclusion: 预测学习在不确定性下为人类类似的时间抽象和组合事件理解提供了可扩展的路径，能够产生与人类事件感知相似的层次化、嵌套的事件结构。

Abstract: Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.

</details>


### [2] [DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision](https://arxiv.org/abs/2512.04314)
*Jiashu Liao,Pietro Liò,Marc de Kamps,Duygu Sarikaya*

Main category: cs.CV

TL;DR: DisentangleFormer提出了一种解耦空间和通道维度的视觉Transformer架构，通过并行处理空间token和通道token流，在保持竞争力的同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 标准自注意力机制同时处理空间和通道维度，导致表示纠缠，无法独立建模结构和语义依赖关系。这在多通道视觉任务（如高光谱成像）中尤为突出，因为不同通道捕获不同的生物物理或生化信息。

Method: 基于信息论的去相关表示学习原则，采用并行解耦设计：1) 并行解耦：独立处理空间token流和通道token流；2) 压缩token增强器：动态融合空间和通道流的自适应校准模块；3) 多尺度FFN：补充全局注意力，捕获细粒度结构和语义依赖。

Result: 在高光谱基准测试（Indian Pine、Pavia University、Houston、BigEarthNet和红外病理数据集）上达到最先进性能，同时在ImageNet上保持竞争力，计算成本降低17.8% FLOPs。

Conclusion: DisentangleFormer通过空间-通道解耦实现了鲁棒的多通道视觉表示，在多个任务上表现出色，同时降低了计算复杂度，为多通道视觉任务提供了有效的解决方案。

Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.

</details>


### [3] [StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios](https://arxiv.org/abs/2512.04451)
*Yifei Wang,Zhenkai Li,Tianwen Qian,Huanran Zheng,Zheng Wang,Yuqian Fu,Xiaoling Wang*

Main category: cs.CV

TL;DR: StreamEQA：首个面向具身场景的流式视频问答基准，评估MLLM在具身和流式两个维度上的能力，包含感知、交互、规划三个层次和向后、实时、向前三种推理模式。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能向真实世界部署发展，需要持续感知和理解流式视觉输入的能力。现有基准无法充分评估模型在具身场景下对流式视频的理解和推理能力。

Method: 构建StreamEQA基准，基于156个独立长视频，通过自动化生成和人工精炼的混合流程，定义了42个任务并生成了约21K个带精确时间戳的问答对。评估维度包括具身（感知、交互、规划）和流式（向后、实时、向前推理）。

Result: 评估了13个最先进的视频-LLM，发现尽管在传统基准上表现良好，但这些模型在具身场景下的流式视频理解方面仍然存在困难。

Conclusion: StreamEQA基准将促进具身应用中的流式视频理解研究，揭示了当前模型在该领域的局限性，为未来研究提供了方向。

Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.

</details>


### [4] [SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding](https://arxiv.org/abs/2512.04643)
*Chang-Hsun Wu,Kai-Po Chang,Yu-Yang Sheng,Hung-Kai Chung,Kuei-Chun Wang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: SEASON是一种无需训练的方法，通过自适应对比解码来增强VideoLLMs的时空忠实度，减少视频理解中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在利用视频中的丰富时间信息方面存在困难，经常产生时间不一致或因果不合理的描述，导致严重的幻觉问题。现有研究主要关注空间幻觉（如物体不匹配），而时间推理在视频理解中相对未被充分探索。

Method: 提出Self-Diagnostic Contrastive Decoding (SEASON)，这是一种无需训练的方法，通过动态诊断每个输出token的幻觉倾向，并对其对应的时空负样本应用自适应对比解码，来增强每个输出token的时空忠实度。

Result: 在三个幻觉检测基准测试中，SEASON优于所有现有的无需训练的幻觉缓解方法，同时在四个通用视频理解基准测试中进一步提升了VideoLLMs的性能。

Conclusion: SEASON通过自适应对比解码有效解决了VideoLLMs中的时空幻觉问题，无需额外训练即可显著提升模型的时空忠实度和视频理解能力。

Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.

</details>


### [5] [Controllable Long-term Motion Generation with Extended Joint Targets](https://arxiv.org/abs/2512.04487)
*Eunjong Lee,Eunhee Kim,Sanghoon Hong,Eunho Jung,Jihoon Kim*

Main category: cs.CV

TL;DR: COMET：实时角色动画生成框架，通过Transformer条件VAE实现精细关节控制，引入参考引导反馈机制确保长期稳定性，支持实时风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有实时角色动画生成方法存在两个主要问题：1）无法提供细粒度控制；2）长序列中运动质量会逐渐退化。这限制了它们在交互式应用中的使用。

Method: 提出COMET自回归框架：1）使用高效的Transformer条件VAE实现精确的关节级控制；2）引入参考引导反馈机制防止误差累积，确保长期稳定性；3）该机制还可作为即插即用模块实现实时风格迁移。

Result: COMET能够实时生成高质量运动，在复杂运动控制任务中显著优于现有方法，证实了其在要求苛刻的交互应用中的实用性。

Conclusion: COMET解决了实时角色动画生成的细粒度控制和长期稳定性问题，为交互式应用提供了可靠的解决方案，其参考引导反馈机制还扩展了风格迁移功能。

Abstract: Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.

</details>


### [6] [Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation](https://arxiv.org/abs/2512.04678)
*Yunhong Lu,Yanhong Zeng,Haobo Li,Hao Ouyang,Qiuyu Wang,Ka Leong Cheng,Jiapeng Zhu,Hengyuan Cao,Zhipeng Zhang,Xing Zhu,Yujun Shen,Min Zhang*

Main category: cs.CV

TL;DR: 提出Reward Forcing框架，通过EMA-Sink和Re-DMD两个创新设计解决视频生成中初始帧复制和运动动态不足的问题，实现高效流式视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频生成方法使用滑动窗口注意力，将初始帧作为sink tokens来维持注意力性能和减少误差累积，但这导致视频帧过度依赖静态tokens，造成初始帧复制和运动动态减弱的问题。

Method: 提出Reward Forcing框架：1) EMA-Sink：维护固定大小的tokens，从初始帧初始化并通过指数移动平均融合被淘汰的tokens，在不增加计算成本的情况下捕获长期上下文和近期动态；2) Re-DMD（Rewarded Distribution Matching Distillation）：通过视觉语言模型评估动态性，优先处理高奖励样本，将模型输出分布偏向高动态区域。

Result: 在标准基准测试中达到最先进性能，在单个H100 GPU上实现23.1 FPS的高质量流式视频生成，显著提升运动质量同时保持数据保真度。

Conclusion: Reward Forcing框架有效解决了现有流式视频生成方法的局限性，通过创新的EMA-Sink和Re-DMD设计实现了高质量、高效率的视频生成，为交互式和动态世界模拟提供了实用解决方案。

Abstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [SIMA 2: A Generalist Embodied Agent for Virtual Worlds](https://arxiv.org/abs/2512.04797)
*SIMA team,Adrian Bolton,Alexander Lerchner,Alexandra Cordell,Alexandre Moufarek,Andrew Bolt,Andrew Lampinen,Anna Mitenkova,Arne Olav Hallingstad,Bojan Vujatovic,Bonnie Li,Cong Lu,Daan Wierstra,Daniel P. Sawyer,Daniel Slater,David Reichert,Davide Vercelli,Demis Hassabis,Drew A. Hudson,Duncan Williams,Ed Hirst,Fabio Pardo,Felix Hill,Frederic Besse,Hannah Openshaw,Harris Chan,Hubert Soyer,Jane X. Wang,Jeff Clune,John Agapiou,John Reid,Joseph Marino,Junkyung Kim,Karol Gregor,Kaustubh Sridhar,Kay McKinney,Laura Kampis,Lei M. Zhang,Loic Matthey,Luyu Wang,Maria Abi Raad,Maria Loks-Thompson,Martin Engelcke,Matija Kecman,Matthew Jackson,Maxime Gazeau,Ollie Purkiss,Oscar Knagg,Peter Stys,Piermaria Mendolicchio,Raia Hadsell,Rosemary Ke,Ryan Faulkner,Sarah Chakera,Satinder Singh Baveja,Shane Legg,Sheleem Kashem,Tayfun Terzi,Thomas Keck,Tim Harley,Tim Scholtes,Tyson Roberts,Volodymyr Mnih,Yulan Liu,Zhengdong Wang,Zoubin Ghahramani*

Main category: cs.AI

TL;DR: SIMA 2是基于Gemini基础模型构建的通用具身智能体，能够在多种3D虚拟世界中理解和行动，支持高级目标推理、用户对话以及语言和图像的复杂指令处理，显著缩小与人类性能的差距并展示强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 先前的工作（如SIMA 1）仅限于简单的语言指令，无法实现主动、目标导向的交互。需要开发能够理解高级目标、与用户对话、处理复杂指令的通用具身智能体，为虚拟和物理世界创造多功能且持续学习的智能体。

Method: 基于Gemini基础模型构建，支持语言和图像的多模态指令输入。通过在不同游戏环境中训练，实现高级目标推理和用户对话能力。利用Gemini生成任务和提供奖励，实现开放式的自我改进，能够在新环境中从零开始自主学习新技能。

Result: 在多样化的游戏组合中，SIMA 2显著缩小了与人类性能的差距，并在未见过的环境中展示了强大的泛化能力，同时保持了基础模型的核心推理能力。能够通过自我改进机制在新环境中从零开始学习新技能。

Conclusion: SIMA 2代表了在具身环境中实现主动、目标导向交互的重要进展，验证了创建多功能且持续学习的智能体的可行路径，为未来虚拟和物理世界的应用奠定了基础。

Abstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

</details>


### [8] [Sequential Enumeration in Large Language Models](https://arxiv.org/abs/2512.04727)
*Kuinan Hou,Marco Zorzi,Alberto Testolin*

Main category: cs.AI

TL;DR: LLMs在序列计数任务中表现有限：虽然能在明确提示下执行计数程序，但不会自发进行计数，显示神经网络与符号系统在组合泛化上的差距。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够像基于规则的符号系统那样可靠地执行序列计数任务，探索神经网络在获取系统性计数能力方面的局限性。

Method: 测试5个最先进的LLM（包括专有、开源和推理模型）在字母和单词列表的序列命名和生产任务中的表现，采用多种提示策略探索思维链的作用，分析不同规模开源模型的计数能力，并研究嵌入动态。

Result: 一些LLM在明确提示下能够执行计数程序，但没有模型在简单要求枚举序列项数时会自发进行计数。模型规模增加并未带来计数能力的系统性提升。

Conclusion: 尽管LLM展现出令人印象深刻的涌现能力，但它们仍无法稳健且系统地执行计数程序，突显了神经网络与符号方法在组合泛化方面的持续差距。

Abstract: Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.

</details>


### [9] [The AI Consumer Index (ACE)](https://arxiv.org/abs/2512.04921)
*Julien Benchek,Rohit Shetty,Benjamin Hunsberger,Ajay Arun,Zach Richards,Brendan Foody,Osvald Nitski,Bertie Vidgen*

Main category: cs.AI

TL;DR: AI Consumer Index (ACE) 是首个评估前沿AI模型执行高价值消费者任务能力的基准，包含400个测试用例，涵盖购物、饮食、游戏和DIY四个领域。GPT-5在排行榜上表现最佳，但整体模型性能与消费者需求仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估AI模型在真实消费者任务中表现的标准基准，需要衡量前沿模型是否能够满足消费者的实际需求，特别是在购物、饮食、游戏和DIY等高价值领域。

Method: 创建包含400个隐藏测试用例的ACE基准，涵盖四个消费者活动领域。采用新颖的评分方法，动态检查模型响应是否基于检索到的网页来源。在排行榜上评估了10个前沿模型（开启网络搜索功能）。

Result: GPT-5（Thinking = High）以56.1%的得分排名第一，其次是o3 Pro（55.2%）和GPT-5.1（55.1%）。不同领域表现差异明显，购物领域最佳模型得分低于50%。模型在提供正确价格或有效链接等任务上容易出现幻觉。

Conclusion: ACE基准揭示了即使是最佳AI模型的表现与消费者实际需求之间仍存在显著差距，特别是在具体细节准确性和避免幻觉方面需要进一步改进。

Abstract: We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.

</details>


### [10] [Detecting Perspective Shifts in Multi-agent Systems](https://arxiv.org/abs/2512.05013)
*Eric Bridgeford,Hayden Helm*

Main category: cs.AI

TL;DR: 提出TDKPS框架，用于监测黑盒多智能体系统中的行为动态变化，通过时间联合嵌入和假设检验检测智能体及群体层面的行为变化。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体部署规模扩大，动态多智能体系统自然涌现，需要监控其行为动态变化。现有研究主要基于单时间点的查询响应进行低维表示，缺乏时间维度的分析框架。

Method: 提出Temporal Data Kernel Perspective Space (TDKPS)框架，将智能体在时间维度上联合嵌入，并设计多种新颖的假设检验方法，用于检测智能体层面和群体层面的行为变化。

Result: 通过模拟实验验证了所提检验方法的经验特性，包括对关键超参数的敏感性。通过自然实验证明，该方法能够检测到与真实外部事件敏感、特异且显著相关的行为变化。

Conclusion: TDKPS是首个用于监测黑盒多智能体系统中行为动态变化的原理性框架，为生成式智能体规模化部署提供了关键能力。

Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral](https://arxiv.org/abs/2512.04220)
*Wenlong Deng,Yushu Li,Boying Gong,Yi Ren,Christos Thrampoulidis,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 论文发现GRPO方法在工具集成强化学习中存在训练崩溃问题，核心原因是"懒惰似然位移"(LLD)，并提出了一种轻量级的正则化方法LLDS来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 工具集成强化学习(TI-RL)让大语言模型能够通过外部工具进行多步推理，但GRPO方法虽然收敛快且无需价值函数，却经常出现训练崩溃问题，这阻碍了其实际应用。

Method: 首先识别出导致GRPO训练崩溃的核心机制——懒惰似然位移(LLD)，然后提出LLDS正则化方法，该方法只在轨迹似然下降时激活，并且只对相关token进行正则化，以最小化对优化的干扰。

Result: 在7个开放域和多跳QA基准测试中，LLDS方法稳定了训练，防止了梯度爆炸，带来了显著的性能提升，如在Qwen2.5-3B上提升37.8%，在Qwen2.5-7B上提升32.0%。

Conclusion: LLD是GRPO基TI-RL的根本瓶颈，提出的LLDS方法为稳定、可扩展的工具集成LLM训练提供了实用路径。

Abstract: Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.

</details>


### [12] [MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection](https://arxiv.org/abs/2512.04492)
*Yuanshuo Zhang,Aohua Li,Bo Chen,Jingbo Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: MSME：一个多阶段、多专家的零样本立场检测框架，通过知识准备、专家推理和决策聚合三阶段解决复杂现实场景中的立场理解问题


<details>
  <summary>Details</summary>
Motivation: 现有的LLM方法在零样本立场检测中表现不错，但在复杂现实场景中仍面临挑战：需要动态背景知识、目标定义涉及复合实体/事件需要与立场标签明确关联、以及反讽等修辞手法会掩盖作者真实意图

Method: 提出MSME框架，包含三个阶段：1) 知识准备阶段：检索相关背景知识并澄清立场标签；2) 专家推理阶段：包含三个专家模块（知识专家从知识角度提炼关键事实和推理，标签专家细化立场标签并进行相应推理，语用专家检测反讽等修辞线索从语用角度推断意图）；3) 决策聚合阶段：元法官整合所有专家分析产生最终立场预测

Result: 在三个公开数据集上的实验表明，MSME在所有数据集上都达到了最先进的性能

Conclusion: MSME框架通过多阶段、多专家的方法有效解决了复杂现实场景中的零样本立场检测问题，显著提升了性能

Abstract: LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.

</details>


### [13] [UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction](https://arxiv.org/abs/2512.04518)
*Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 该论文介绍了ChemoTimelines共享任务中从临床记录提取化疗时间线的子任务2方法，通过多种LLM策略提升时间线提取效果，最佳模型达到0.678分。


<details>
  <summary>Details</summary>
Motivation: 从癌症患者的电子健康记录中构建系统性抗癌治疗时间线对临床决策和研究至关重要，但自动化提取化疗时间线面临挑战，需要开发有效的方法来处理原始临床记录。

Method: 采用两步工作流程：1）使用LLM从单个临床记录中提取化疗事件；2）通过算法将事件标准化并聚合成患者级别的时间线。评估了多种策略：思维链推理、监督微调、直接偏好优化和基于字典的查找方法，不同方法主要在LLM的使用和训练方式上有所区别。

Result: 多种方法在测试集排行榜上表现出竞争力，其中微调的Qwen3-14B模型获得了最佳官方分数0.678。分析结果为未来类似任务提供了有价值的见解。

Conclusion: 该研究展示了从临床记录提取化疗时间线的有效方法，证明了LLM在该任务中的潜力，为未来类似医疗信息提取任务的设计和实施提供了参考。

Abstract: The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions](https://arxiv.org/abs/2512.04773)
*Giorgos Polychronis,Foivos Pournaropoulos,Christos D. Antonopoulos,Spyros Lalis*

Main category: cs.RO

TL;DR: 无人机在数据驱动任务中需要实时处理数据以决定是否采取后续行动，本文提出基于分支预测和强化学习的机器学习方法，显著优化任务时间。


<details>
  <summary>Details</summary>
Motivation: 无人机在数据采集任务中面临决策困境：如果原地等待处理结果可能浪费时间，如果提前移动而需要返回则增加飞行时间。现有基于回归的方法无法有效应对事件概率随时间变化的复杂场景。

Method: 提出多种机器学习方法，包括分支预测和强化学习技术，用于无人机在数据采集点决定是否等待处理结果或直接移动到下一个点。

Result: 提出的方法在事件概率随时间变化的各种场景中，相比现有回归方法显著提升性能，最坏情况任务时间最多改善4.1倍，中位任务时间仅比完美知识方法高2.7%。

Conclusion: 基于分支预测和强化学习的方法能有效优化无人机在动态环境中的决策效率，显著减少任务时间，接近理想性能。

Abstract: Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.

</details>
