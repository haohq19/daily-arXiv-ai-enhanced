{"id": "2511.11639", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11639", "abs": "https://arxiv.org/abs/2511.11639", "authors": ["Jie Fan", "Francesco Visentin", "Barbara Mazzolai", "Emanuela Del Dottore"], "title": "Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature", "comment": "This manuscript is a preprint version of the article currently under peer review at International Journal of Computer Vision (IJCV)", "summary": "Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u76843D\u51e0\u4f55\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u85e4\u8513\u89e6\u987b\u5728\u673a\u68b0\u523a\u6fc0\u4e0b\u7684\u5f62\u72b6\u53d8\u5316\uff0c\u53d1\u73b0\u9876\u7aef\u533a\u57df\u54cd\u5e94\u6027\u66f4\u9ad8\uff0c\u4e3a\u690d\u7269\u751f\u7269\u529b\u5b66\u7814\u7a76\u548c\u4eff\u751f\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u85e4\u8513\u89e6\u987b\u5f62\u72b6\u53d8\u5316\u4e0e\u673a\u68b0\u523a\u6fc0\u89e6\u53d1\u4e8b\u4ef6\u53ca\u63a5\u89e6\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u76ee\u524d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u5efa\u7acb\u8fd9\u79cd\u5173\u7cfb\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e3D\u5206\u6bb5Clothoid\u6a21\u578b\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u91cd\u5efa\u85e4\u8513\u89e6\u987b\u5728\u673a\u68b0\u6469\u64e6\u540e\u7684\u5f62\u72b6\u914d\u7f6e\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u7684\u5f62\u72b6\u53d8\u5316\u3002", "result": "\u91cd\u5efa\u65b9\u6cd5\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff08R2 > 0.99\uff09\uff0c\u76f8\u6bd4\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u6570\u636e\u9700\u6c42\u5c11\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u548c\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u4f18\u52bf\uff0c\u53d1\u73b0\u89e6\u987b\u9876\u7aef\u533a\u57df\u54cd\u5e94\u6027\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u690d\u7269\u751f\u7269\u529b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u53d7\u6500\u63f4\u690d\u7269\u542f\u53d1\u7684\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u8bbe\u8ba1\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11600", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11600", "abs": "https://arxiv.org/abs/2511.11600", "authors": ["Piyushkumar Patel"], "title": "CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models", "comment": null, "summary": "While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This \"hallucination\" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.\n  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.\n  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\\% of the time while missing only 8.3\\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.", "AI": {"tldr": "CausalGuard\u662f\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u548c\u7b26\u53f7\u903b\u8f91\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u548c\u9632\u6b62\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523089.3%\u7684\u5e7b\u89c9\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u5c06\u865a\u5047\u58f0\u660e\u51cf\u5c11\u8fd180%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4f1a\u81ea\u4fe1\u5730\u9648\u8ff0\u542c\u8d77\u6765\u5408\u7406\u4f46\u5b9e\u9645\u9519\u8bef\u7684\u4fe1\u606f\uff0c\u8fd9\u6210\u4e3a\u5728\u51c6\u786e\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u4e3b\u8981\u969c\u788d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u8981\u4e48\u589e\u52a0\u663e\u8457\u8ba1\u7b97\u6210\u672c\uff0c\u6216\u8005\u672a\u80fd\u89e3\u51b3\u5e7b\u89c9\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "CausalGuard\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u4e0e\u7b26\u53f7\u903b\u8f91\u76f8\u7ed3\u5408\uff0c\u5728\u5e7b\u89c9\u53d1\u751f\u65f6\u8fdb\u884c\u68c0\u6d4b\u548c\u9884\u9632\u3002\u7cfb\u7edf\u7406\u89e3\u5bfc\u81f4\u9519\u8bef\u9648\u8ff0\u7684\u56e0\u679c\u94fe\uff0c\u5e76\u5728\u8fc7\u7a0b\u4e2d\u65e9\u671f\u5e72\u9884\u3002\u901a\u8fc7\u4e24\u6761\u4e92\u8865\u8def\u5f84\u5de5\u4f5c\uff1a\u4e00\u6761\u8ffd\u8e2a\u6a21\u578b\u77e5\u8bc6\u4e0e\u5176\u751f\u6210\u5185\u5bb9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53e6\u4e00\u6761\u4f7f\u7528\u81ea\u52a8\u63a8\u7406\u68c0\u67e5\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u572812\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCausalGuard\u6b63\u786e\u8bc6\u522b\u5e7b\u89c9\u7684\u65f6\u95f4\u8fbe\u523089.3%\uff0c\u4ec5\u9057\u6f0f8.3%\u7684\u5b9e\u9645\u5e7b\u89c9\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u5c06\u865a\u5047\u58f0\u660e\u51cf\u5c11\u4e86\u8fd180%\uff0c\u540c\u65f6\u4fdd\u6301\u56de\u7b54\u7684\u81ea\u7136\u6027\u548c\u5e2e\u52a9\u6027\u3002\u5728\u9700\u8981\u591a\u6b65\u903b\u8f91\u63a8\u7406\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5c24\u5176\u51fa\u8272\u3002", "conclusion": "CausalGuard\u901a\u8fc7\u5c55\u793a\u5176\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u533b\u7597\u8bca\u65ad\u6216\u91d1\u878d\u5206\u6790\u7b49\u654f\u611f\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u9886\u57df\u7406\u89e3\u51b3\u7b56\u539f\u56e0\u4e0e\u51b3\u7b56\u672c\u8eab\u540c\u6837\u91cd\u8981\u3002\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11924", "abs": "https://arxiv.org/abs/2511.11924", "authors": ["Yongkang Huo", "Fulvio Forni", "Rodolphe Sepulchre"], "title": "A Neuromorphic Architecture for Scalable Event-Based Control", "comment": null, "summary": "This paper introduces the ``rebound Winner-Take-All (RWTA)\" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\"\u53cd\u5f39\u8d62\u5bb6\u901a\u5403(RWTA)\"\u57fa\u5143\u4f5c\u4e3a\u53ef\u6269\u5c55\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\u7684\u57fa\u672c\u5143\u7d20\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u79bb\u6563\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u8fde\u7eed\u8c03\u8282\u7684\u53ef\u8c03\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7edf\u4e00\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u8fde\u7eed\u8282\u5f8b\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\uff0c\u7ed3\u5408\u79bb\u6563\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u8fde\u7eed\u8c03\u8282\u7684\u53ef\u8c03\u6027\u3002", "method": "\u4f7f\u7528RWTA\u57fa\u5143\u6784\u5efa\u4ece\u7ec6\u80de\u7ea7\u5230\u7cfb\u7edf\u7ea7\u7684\u67b6\u6784\uff0c\u7ee7\u627f\u8d62\u5bb6\u901a\u5403\u72b6\u6001\u673a\u7684\u79bb\u6563\u8ba1\u7b97\u80fd\u529b\u548c\u53ef\u5174\u594b\u751f\u7269\u7269\u7406\u7535\u8def\u7684\u8fde\u7eed\u8c03\u8282\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u86c7\u5f62\u673a\u5668\u4eba\u795e\u7ecf\u7cfb\u7edf\u8bbe\u8ba1\u5c55\u793a\u4e86\u8be5\u67b6\u6784\u7684\u901a\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u6a21\u5757\u5316\u7279\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6846\u67b6\u5728\u7edf\u4e00\u7684\u7269\u7406\u5efa\u6a21\u8bed\u8a00\u4e2d\u89e3\u51b3\u4e86\u8fde\u7eed\u8282\u5f8b\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\u95ee\u9898\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12237", "categories": ["cs.RO", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12237", "abs": "https://arxiv.org/abs/2511.12237", "authors": ["Alysson Ribeiro da Silva", "Luiz Chaimowicz"], "title": "Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration", "comment": "9 pages, 9 figures, International Conference on Advanced Robotics", "summary": "Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u591a\u673a\u5668\u4eba\u63a2\u7d22\u4e2d\u901a\u4fe1\u7ea6\u675f\u548c\u95f4\u6b47\u8fde\u63a5\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5305\u62ecMILP\u89c4\u5212\u751f\u6210\u5668\u548cRTUS\u8ddf\u8e2a\u673a\u5236\uff0c\u5728Gazebo\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u673a\u5668\u4eba\u63a2\u7d22\u7cfb\u7edf\u5728\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8981\u4e48\u9700\u8981\u9884\u5148\u77e5\u9053\u73af\u5883\u4fe1\u606f\uff0c\u8981\u4e48\u65e0\u6cd5\u751f\u6210\u6700\u4f18\u8ba1\u5212\u6216\u8003\u8651\u5b9e\u9645\u8f68\u8ff9\uff0c\u4e0d\u9002\u5408\u73b0\u5b9e\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86MRE-CCIC\u95ee\u9898\uff0c\u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u751f\u6210\u4f1a\u5408\u8ba1\u5212\uff0c\u5e76\u901a\u8fc7RTUS\u673a\u5236\u8ba9\u673a\u5668\u4eba\u8ddf\u8e2a\u8ba1\u5212\uff0c\u8003\u8651\u672a\u77e5\u73af\u5883\u6761\u4ef6\u3002", "result": "\u5728Gazebo\u5927\u89c4\u6a21\u73af\u5883\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u591f\u53ca\u65f6\u8ddf\u8e2a\u8ba1\u5212\u5e76\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u63a2\u7d22\u4e2d\u7684\u901a\u4fe1\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2511.11945", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11945", "abs": "https://arxiv.org/abs/2511.11945", "authors": ["Mohammed Temraz", "Mark T Keane"], "title": "Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes", "comment": "31 pages, 8 figures", "summary": "In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of \"climate outlier events\". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.", "AI": {"tldr": "\u63d0\u51faCFA-SMOTE\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u548cSMOTE\u8fc7\u91c7\u6837\u6280\u672f\uff0c\u89e3\u51b3\u6c14\u5019\u53d8\u5316\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u5bf9\u6c14\u5019\u5f02\u5e38\u4e8b\u4ef6\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u589e\u591a\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5206\u5e03\u5916\u6570\u636e\u3002\u5386\u53f2\u6570\u636e\u96c6\u7f3a\u4e4f\u8db3\u591f\u7684'\u6c14\u5019\u5f02\u5e38\u4e8b\u4ef6'\u6837\u672c\uff0c\u9020\u6210\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "CFA-SMOTE\u65b9\u6cd5\u5c06\u53ef\u89e3\u91caAI\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u4e0eSMOTE\u8fc7\u91c7\u6837\u6280\u672f\u7ed3\u5408\uff0c\u751f\u6210\u4ee3\u8868\u6c14\u5019\u5f02\u5e38\u4e8b\u4ef6\u7684\u5408\u6210\u6570\u636e\u70b9\u6765\u589e\u5f3a\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e0d\u540c\u7c7b\u522b\u4e0d\u5e73\u8861\u6bd4\u4f8b\u4e0b\uff0cCFA-SMOTE\u65b9\u6cd5\u76f8\u6bd4\u57fa\u51c6\u53cd\u4e8b\u5b9e\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "CFA-SMOTE\u80fd\u6709\u6548\u89e3\u51b3\u6c14\u5019\u53d8\u5316\u9884\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u6c14\u5019\u5f02\u5e38\u4e8b\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2511.12361", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12361", "abs": "https://arxiv.org/abs/2511.12361", "authors": ["Leroy D'Souza", "Akash Karthikeyan", "Yash Vardhan Pant", "Sebastian Fischmeister"], "title": "SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty", "comment": null, "summary": "Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.\n  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.", "AI": {"tldr": "SAC-MoE\u662f\u4e00\u79cd\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u5177\u6709\u4e0d\u53ef\u89c2\u6d4b\u6a21\u5f0f\u548c\u5207\u6362\u4e8b\u4ef6\u7684\u590d\u6742\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u4e2d\u4e0d\u53ef\u89c2\u6d4b\u53c2\u6570\u548c\u6a21\u5f0f\u5207\u6362\u5e26\u6765\u7684\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u6807\u51c6\u65e0\u6a21\u578bRL\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7a81\u7136\u7684\u6a21\u5f0f\u5207\u6362\u3002", "method": "\u5c06SAC\u6846\u67b6\u4e2d\u7684\u884c\u52a8\u8005\u5efa\u6a21\u4e3a\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u5305\u542b\u5b66\u4e60\u5230\u7684\u8def\u7531\u5668\u81ea\u9002\u5e94\u9009\u62e9\u4e13\u5bb6\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u8bfe\u7a0b\u7684\u8bad\u7ec3\u7b97\u6cd5\u4f18\u5148\u6536\u96c6\u6311\u6218\u6027\u6570\u636e\u3002", "result": "\u5728\u6df7\u5408\u81ea\u4e3b\u8d5b\u8f66\u548c\u817f\u5f0f\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0cSAC-MoE\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u73af\u5883\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u6700\u9ad86\u500d\uff09\uff0c\u8bfe\u7a0b\u7b56\u7565\u5728\u6240\u6709\u8bc4\u4f30\u7b56\u7565\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SAC-MoE\u901a\u8fc7\u53ef\u89e3\u91ca\u7684MoE\u8def\u7531\u5668\u4e3a\u4e0d\u540c\u6f5c\u5728\u6a21\u5f0f\u6fc0\u6d3b\u4e0d\u540c\u4e13\u5bb6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u4e2d\u7684\u6cdb\u5316\u6311\u6218\u3002"}}
{"id": "2511.12116", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12116", "abs": "https://arxiv.org/abs/2511.12116", "authors": ["Piotr P\u0119zik", "Konrad Kaczy\u0144ski", "Maria Szyma\u0144ska", "Filip \u017barnecki", "Zuzanna Deckert", "Jakub Kwiatkowski", "Wojciech Janowski"], "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.", "AI": {"tldr": "LLMLagBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u8bad\u7ec3\u6570\u636e\u65f6\u95f4\u8fb9\u754c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u6a21\u578b\u5bf9\u8fd1\u671f\u4e8b\u4ef6\u7684\u4e86\u89e3\u6765\u8bc6\u522b\u5176\u77e5\u8bc6\u7684\u65f6\u95f4\u9650\u5236\u3002", "motivation": "LLM\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u524d\u7684\u6587\u672c\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u5f62\u6210\u4e86\u4e25\u683c\u7684\u77e5\u8bc6\u8fb9\u754c\u3002\u5f53\u8fd9\u4e00\u9650\u5236\u672a\u77e5\u6216\u88ab\u5ffd\u89c6\u65f6\uff0c\u6a21\u578b\u53ef\u80fd\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u65e0\u610f\u95f4\u6df7\u5408\u8fc7\u65f6\u7684\u65f6\u6548\u6027\u4fe1\u606f\u4e0e\u901a\u7528\u77e5\u8bc6\uff0c\u4ece\u800c\u5f71\u54cd\u56de\u7b54\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165LLMLagBench\u4f5c\u4e3a\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30LLM\u5bf9\u8fd1\u671f\u4e8b\u4ef6\u7684\u77e5\u8bc6\u6765\u8bc6\u522b\u5176\u8bad\u7ec3\u6570\u636e\u7684\u6700\u65e9\u53ef\u80fd\u65f6\u95f4\u8fb9\u754c\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5927\u91cfLLM\uff0c\u5305\u62ec\u660e\u786e\u58f0\u660e\u548c\u672a\u58f0\u660e\u8bad\u7ec3\u622a\u6b62\u65f6\u95f4\u7684\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u548c\u4e0e\u516c\u5f00\u7684LLM\u9884\u8bad\u7ec3\u4fe1\u606f\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u8be5\u57fa\u51c6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "LLMLagBench\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u8bc6\u522bLLM\u7684\u77e5\u8bc6\u65f6\u95f4\u8fb9\u754c\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u7684\u77e5\u8bc6\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12008", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12008", "abs": "https://arxiv.org/abs/2511.12008", "authors": ["Yunqi Hong", "Johnson Kao", "Liam Edwards", "Nein-Tzu Liu", "Chung-Yen Huang", "Alex Oliveira-Kowaleski", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models", "comment": null, "summary": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.", "AI": {"tldr": "RECAP-PATH\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u75c5\u7406AI\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u5b66\u4e60\u8303\u5f0f\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u88ab\u52a8\u6a21\u5f0f\u8bc6\u522b\u8f6c\u53d8\u4e3a\u8bc1\u636e\u5173\u8054\u7684\u8bca\u65ad\u63a8\u7406\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u6a21\u578b\u6743\u91cd\u66f4\u65b0\u5373\u53ef\u751f\u6210\u764c\u75c7\u8bca\u65ad\u3002", "motivation": "\u5f53\u524d\u75c5\u7406AI\u5de5\u5177\u867d\u7136\u63d0\u9ad8\u4e86\u7b5b\u67e5\u6548\u7387\u548c\u6807\u51c6\u5316\u91cf\u5316\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u96be\u4ee5\u5ba1\u8ba1\u51b3\u7b56\u548c\u9632\u6b62\u9519\u8bef\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u5b66\u4e60\u8fc7\u7a0b\uff1a\u591a\u6837\u5316\u9636\u6bb5\u6269\u5c55\u75c5\u7406\u5b66\u98ce\u683c\u7684\u89e3\u91ca\uff0c\u4f18\u5316\u9636\u6bb5\u7cbe\u70bc\u8fd9\u4e9b\u89e3\u91ca\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u8be5\u6846\u67b6\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u65e0\u9700\u767d\u76d2\u8bbf\u95ee\u6216\u6743\u91cd\u66f4\u65b0\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u524d\u5217\u817a\u764c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRECAP-PATH\u751f\u6210\u7684\u63a8\u7406\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\uff0c\u8bca\u65ad\u51c6\u786e\u6027\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RECAP-PATH\u901a\u8fc7\u5c06\u89c6\u89c9\u7406\u89e3\u4e0e\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e34\u5e8a\u53ef\u4fe1\u8d56\u7684AI\uff0c\u5c55\u793a\u4e86\u5b9e\u73b0\u8bc1\u636e\u5173\u8054\u89e3\u91ca\u7684\u901a\u7528\u8def\u5f84\u3002"}}
{"id": "2511.11882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11882", "abs": "https://arxiv.org/abs/2511.11882", "authors": ["Simon Durand", "Samuel Foucher", "Alexandre Delplanque", "Jo\u00eblle Taillon", "J\u00e9r\u00f4me Th\u00e9au"], "title": "Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)", "comment": "34 pages, 10 figures, submitted to Remote Sensing in Ecology and Conservation", "summary": "Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u8865\u5145\u6709\u9650\u8bad\u7ec3\u6570\u636e\u6765\u63d0\u9ad8\u9e9d\u725b\u68c0\u6d4b\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u5408\u6210\u56fe\u50cf\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u91ce\u751f\u52a8\u7269\u8c03\u67e5\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u53d7\u9650\u4e8e\u540e\u52e4\u6311\u6218\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u6548\u679c\u6709\u9650\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u50cf\u9e9d\u725b\u8fd9\u6837\u7a00\u758f\u5206\u5e03\u7684\u7269\u79cd\u3002", "method": "\u6bd4\u8f83\u4e86\u4ec5\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u7684\u57fa\u7ebf\u6a21\u578b\u4e0e5\u4e2a\u96f6\u6837\u672c\u548c5\u4e2a\u5c11\u6837\u672c\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u4e2d\u9010\u6b65\u52a0\u5165\u66f4\u591a\u5408\u6210\u56fe\u50cf\u3002", "result": "\u96f6\u6837\u672c\u6a21\u578b\u4e2d\uff0c\u6dfb\u52a0\u5408\u6210\u56fe\u50cf\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u4f46\u8d85\u8fc7\u57fa\u7ebf\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6100%\u540e\u51fa\u73b0\u6536\u76ca\u9012\u51cf\u3002\u5c11\u6837\u672c\u6a21\u578b\u4e2d\uff0c\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u7ed3\u5408\u4f7f\u7528\u53ef\u83b7\u5f97\u66f4\u597d\u7684\u53ec\u56de\u7387\u548c\u7565\u9ad8\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u80fd\u591f\u8bad\u7ec3\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u4e3a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c6\u89d2\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7a00\u6709\u6216\u96be\u4ee5\u63a5\u8fd1\u7684\u7269\u79cd\u3002"}}
{"id": "2511.11944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11944", "abs": "https://arxiv.org/abs/2511.11944", "authors": ["Ling Wang", "Yunfan Lu", "Wenzong Ma", "Huizai Yao", "Pengteng Li", "Hui Xiong"], "title": "From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing", "comment": "11 pages, 8 figures. Completed in April 2025", "summary": "Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \\textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \\textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.", "AI": {"tldr": "\u9996\u6b21\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u56fe\u50cf\u53bb\u96fe\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u5c06\u4e8b\u4ef6\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u4fe1\u606f\u4f20\u8f93\u5230RGB\u56fe\u50cf\u4e2d\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u96fe\u973e\u573a\u666f\u4e0b\u52a8\u6001\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u53bb\u96fe\u65b9\u6cd5\u53d7\u9650\u4e8e\u6709\u9650\u7684\u52a8\u6001\u8303\u56f4\uff0c\u5bb9\u6613\u4e22\u5931\u7ed3\u6784\u548c\u5149\u7167\u7ec6\u8282\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u66f4\u9ad8\u7684HDR\uff08120dB vs 60dB\uff09\u548c\u5fae\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u66f4\u9002\u5408\u96fe\u973e\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e8b\u4ef6\u5f15\u5bfc\u6a21\u5757\u5c06\u7a00\u758f\u7684HDR\u4e8b\u4ef6\u7279\u5f81\uff08\u5982\u8fb9\u7f18\u3001\u89d2\u70b9\uff09\u6620\u5c04\u5230\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\uff0c\u4e3a\u751f\u6210\u8fc7\u7a0b\u63d0\u4f9b\u7cbe\u786e\u7684\u7ed3\u6784\u6307\u5bfc\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u81ea\u5efa\u7684\u91cd\u96fe\u973e\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff08AQI=341\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53bb\u96fe\u6548\u679c\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u7ed3\u5408\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u96fe\u973e\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u53bb\u96fe\u95ee\u9898\uff0c\u901a\u8fc7HDR\u4fe1\u606f\u4f20\u8f93\u663e\u8457\u63d0\u5347\u53bb\u96fe\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u771f\u5b9e\u611f\u3002"}}
{"id": "2511.12020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12020", "abs": "https://arxiv.org/abs/2511.12020", "authors": ["Xianglong Shi", "Silin Cheng", "Sirui Zhao", "Yunhan Jiang", "Enhong Chen", "Yang Liu", "Sebastien Ourselin"], "title": "LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension", "comment": null, "summary": "Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\\%. The code is available at https://anonymous.4open.science/r/LIHE.", "AI": {"tldr": "\u63d0\u51fa\u4e86LIHE\u6846\u67b6\u89e3\u51b3\u5f31\u76d1\u7763\u5e7f\u4e49\u6307\u79f0\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5904\u7406\u53ef\u53d8\u6570\u91cf\u76ee\u6807\u7684\u8868\u8fbe\uff0c\u7ed3\u5408\u53cc\u66f2\u548c\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u89e3\u51b3\u8bed\u4e49\u8868\u793a\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f31\u76d1\u7763\u6307\u79f0\u8868\u8fbe\u7406\u89e3\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e00\u5bf9\u4e00\u6620\u5c04\u5047\u8bbe\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u573a\u666f\u4e2d\u5bf9\u5e94\u96f6\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u7684\u8868\u8fbe\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u5e7f\u4e49\u6307\u79f0\u8868\u8fbe\u7406\u89e3\u8303\u5f0f\u3002", "method": "LIHE\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u6307\u79f0\u89e3\u8026\u9636\u6bb5\u9884\u6d4b\u76ee\u6807\u6570\u91cf\u5e76\u5206\u89e3\u590d\u6742\u8868\u8fbe\u4e3a\u5b50\u8868\u8fbe\uff1b\u6307\u79f0\u5b9a\u4f4d\u9636\u6bb5\u4f7f\u7528HEMix\u6df7\u5408\u76f8\u4f3c\u5ea6\u6a21\u5757\uff0c\u7ed3\u5408\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u7684\u7cbe\u786e\u5bf9\u9f50\u80fd\u529b\u548c\u53cc\u66f2\u51e0\u4f55\u7684\u5c42\u6b21\u5efa\u6a21\u4f18\u52bf\u3002", "result": "\u5728gRefCOCO\u548cRef-ZOM\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u9996\u4e2a\u6709\u6548\u7684\u5f31\u76d1\u7763WGREC\u57fa\u51c6\uff0cHEMix\u5728\u6807\u51c6REC\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e00\u81f4\u6539\u8fdb\uff0cIoU@0.5\u63d0\u5347\u9ad8\u8fbe2.5%\u3002", "conclusion": "LIHE\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u5e7f\u4e49\u6307\u79f0\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\uff0c\u901a\u8fc7\u6df7\u5408\u51e0\u4f55\u65b9\u6cd5\u6709\u6548\u9632\u6b62\u8bed\u4e49\u5d29\u6e83\uff0c\u4e3a\u5904\u7406\u53ef\u53d8\u6570\u91cf\u76ee\u6807\u7684\u6307\u79f0\u8868\u8fbe\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12027", "abs": "https://arxiv.org/abs/2511.12027", "authors": ["Jeong Hun Yeo", "Sangyun Chung", "Sungjune Park", "Dae Hoe Kim", "Jinyoung Moon", "Yong Man Ro"], "title": "GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory", "comment": null, "summary": "Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\\% accuracy on the Long split and the highest overall average (71.9\\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.", "AI": {"tldr": "GCAgent\u662f\u4e00\u4e2a\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u793a\u610f\u56fe\u548c\u53d9\u4e8b\u60c5\u666f\u8bb0\u5fc6\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u95ee\u9898\uff0c\u5728Video-MME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684token\u9650\u5236\u548c\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6355\u83b7\u56f0\u96be\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u590d\u6742\u4e8b\u4ef6\u5173\u7cfb\u3002", "method": "\u63d0\u51faGCAgent\u6846\u67b6\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u793a\u610f\u56fe\u548c\u53d9\u4e8b\u60c5\u666f\u8bb0\u5fc6\uff0c\u5c06\u4e8b\u4ef6\u53ca\u5176\u56e0\u679c\u548c\u65f6\u95f4\u5173\u7cfb\u7ed3\u6784\u5316\u5efa\u6a21\u4e3a\u6709\u7ec4\u7ec7\u7684\u4e0a\u4e0b\u6587\uff0c\u91c7\u7528\u611f\u77e5-\u884c\u52a8-\u53cd\u601d\u7684\u591a\u9636\u6bb5\u5faa\u73af\uff0c\u4f7f\u7528\u8bb0\u5fc6\u7ba1\u7406\u5668\u68c0\u7d22\u76f8\u5173\u60c5\u666f\u4e0a\u4e0b\u6587\u3002", "result": "\u5728Video-MME Long split\u4e0a\u76f8\u6bd4\u5f3a\u57fa\u7ebfMLLM\u5b9e\u73b0\u4e8623.5%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u57287B\u89c4\u6a21MLLM\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cLong split\u51c6\u786e\u738773.4%\uff0c\u6574\u4f53\u5e73\u574771.9%\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u63a8\u7406\u8303\u5f0f\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\u4e3a\u8ba4\u77e5\u542f\u53d1\u7684\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.12563", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12563", "abs": "https://arxiv.org/abs/2511.12563", "authors": ["Eljas Linna", "Kestutis Baltakys", "Alexandros Iosifidis", "Juho Kanniainen"], "title": "LOBERT: Generative AI Foundation Model for Limit Order Book Messages", "comment": "Submission for NeurIPS 2025 GenAI in Finance Workshop", "summary": "Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.", "AI": {"tldr": "LOBERT\u662f\u4e00\u4e2a\u9488\u5bf9\u9650\u4ef7\u8ba2\u5355\u7c3f\u6570\u636e\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6807\u8bb0\u5316\u65b9\u6848\u5c06\u591a\u7ef4\u6d88\u606f\u4f5c\u4e3a\u5355\u4e2a\u6807\u8bb0\u5904\u7406\uff0c\u5728\u9884\u6d4b\u4e2d\u95f4\u4ef7\u683c\u53d8\u52a8\u548c\u4e0b\u4e00\u6d88\u606f\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u9886\u5148\u3002", "motivation": "\u73b0\u6709\u7684\u9650\u4ef7\u8ba2\u5355\u7c3f\u6a21\u578b\u9700\u8981\u7e41\u7410\u7684\u6570\u636e\u8868\u793a\uff0c\u4e14\u7f3a\u4e4f\u539f\u59cb\u4efb\u52a1\u4e4b\u5916\u7684\u9002\u5e94\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u95f4\u3001\u5feb\u901f\u673a\u5236\u8f6c\u6362\u548c\u9ad8\u9891\u4ea4\u6613\u8005\u5bf9\u53ef\u89c1\u8ba2\u5355\u6d41\u7684\u53cd\u5e94\u7b49\u6311\u6218\u3002", "method": "LOBERT\u57fa\u4e8eBERT\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6807\u8bb0\u5316\u65b9\u6848\uff0c\u5c06\u5b8c\u6574\u7684\u591a\u7ef4\u6d88\u606f\u4f5c\u4e3a\u5355\u4e2a\u6807\u8bb0\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u4ef7\u683c\u3001\u6570\u91cf\u548c\u65f6\u95f4\u7b49\u8fde\u7eed\u7279\u5f81\u7684\u8868\u793a\u3002", "result": "LOBERT\u5728\u9884\u6d4b\u4e2d\u95f4\u4ef7\u683c\u53d8\u52a8\u548c\u4e0b\u4e00\u6d88\u606f\u7b49\u4efb\u52a1\u4e2d\u53d6\u5f97\u9886\u5148\u6027\u80fd\uff0c\u540c\u65f6\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u51cf\u5c11\u4e86\u6240\u9700\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "conclusion": "LOBERT\u4e3a\u9650\u4ef7\u8ba2\u5355\u7c3f\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u57fa\u7840\u6a21\u578b\uff0c\u9002\u5408\u4e0b\u6e38\u5fae\u8c03\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.13100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13100", "abs": "https://arxiv.org/abs/2511.13100", "authors": ["Xuecheng Chen", "Jingao Xu", "Wenhua Ding", "Haoyang Wang", "Xinyu Luo", "Ruiyang Duan", "Jialong Chen", "Xueqian Wang", "Yunhao Liu", "Xinlei Chen"], "title": "Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing", "comment": null, "summary": "As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \\sysname. \\sysname features two components: \\textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \\textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \\sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\\%. Additionally, \\sysname infers drone flight commands with 96.5\\% precision and improves drone tracking accuracy by over 22\\% when combined with other sensing modalities. \\textit{ Demo: {\\color{blue}https://eventpro25.github.io/EventPro/.} }", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u65e0\u4eba\u673a\u87ba\u65cb\u6868\u8f6c\u901f\u611f\u77e5\u7cfb\u7edfEventPro\uff0c\u901a\u8fc7\u7cbe\u786e\u6d4b\u91cf\u87ba\u65cb\u6868\u8f6c\u901f\u6765\u63d0\u5347\u65e0\u4eba\u673a\u611f\u77e5\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u914d\u9001\u573a\u666f\u4e2d\u5b9e\u73b0\u4e863ms\u7684\u4f4e\u5ef6\u8fdf\u548c0.23%\u7684\u8f6c\u901f\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u7684\u666e\u53ca\uff0c\u4ece\u5730\u9762\u8fdb\u884c\u975e\u63a5\u89e6\u5f0f\u65e0\u4eba\u673a\u611f\u77e5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u611f\u77e5\u6027\u80fd\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65e0\u4eba\u673a\u72b6\u6001\u76d1\u6d4b\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aCount Every Rotation\u901a\u8fc7\u51cf\u8f7b\u4e8b\u4ef6\u76f8\u673a\u5bf9\u73af\u5883\u566a\u58f0\u7684\u8d85\u9ad8\u654f\u611f\u6027\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u87ba\u65cb\u6868\u8f6c\u901f\u4f30\u8ba1\uff1bEvery Rotation Counts\u5229\u7528\u8fd9\u4e9b\u8f6c\u901f\u63a8\u65ad\u65e0\u4eba\u673a\u7684\u5185\u5916\u52a8\u6001\u7279\u6027\u3002", "result": "\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u914d\u9001\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff1a\u611f\u77e5\u5ef6\u8fdf\u4ec5\u4e3a3ms\uff0c\u87ba\u65cb\u6868\u8f6c\u901f\u4f30\u8ba1\u8bef\u5dee\u4ec5\u4e3a0.23%\uff0c\u65e0\u4eba\u673a\u98de\u884c\u6307\u4ee4\u63a8\u65ad\u7cbe\u5ea6\u8fbe96.5%\uff0c\u4e0e\u5176\u4ed6\u611f\u77e5\u6a21\u6001\u7ed3\u5408\u65f6\u65e0\u4eba\u673a\u8ddf\u8e2a\u7cbe\u5ea6\u63d0\u5347\u8d85\u8fc722%\u3002", "conclusion": "EventPro\u901a\u8fc7\u4e13\u6ce8\u4e8e\u87ba\u65cb\u6868\u8f6c\u901f\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u611f\u77e5\u6027\u80fd\uff0c\u4e3a\u65e0\u4eba\u673a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11675", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11675", "abs": "https://arxiv.org/abs/2511.11675", "authors": ["Junchen Liu", "Yi Sheng"], "title": "Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff", "comment": null, "summary": "As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5411\u526a\u679d-\u518d\u751f\u7b56\u7565\uff0c\u4ece\u6ee1\u8db3\u786c\u4ef6\u7ea6\u675f\u7684\u6781\u5ea6\u538b\u7f29\u7f51\u7edc\u5f00\u59cb\uff0c\u9009\u62e9\u6027\u518d\u751f\u5173\u952e\u8fde\u63a5\u6765\u6062\u590d\u6027\u80fd\uff0c\u89e3\u51b3\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u7a00\u758f\u5ea6\u8d85\u8fc7\u7279\u5b9a\u9608\u503c\u65f6\uff0c\u8fed\u4ee3\u548c\u4e00\u6b21\u6027\u526a\u679d\u65b9\u6cd5\u90fd\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u53ef\u5b9e\u73b0\u7684\u538b\u7f29\u6bd4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u67d0\u4e9b\u786c\u4ef6\u5e73\u53f0\u7684\u4e25\u683c\u5c3a\u5bf8\u7ea6\u675f\u3002", "method": "\u53cc\u5411\u526a\u679d-\u518d\u751f\u7b56\u7565\uff1a\u4ece\u6781\u5ea6\u538b\u7f29\u7684\u7f51\u7edc\u5f00\u59cb\uff0c\u9009\u62e9\u6027\u518d\u751f\u5173\u952e\u8fde\u63a5\u6765\u6062\u590d\u4e22\u5931\u7684\u6027\u80fd\u3002", "result": "\u6709\u6548\u7f13\u89e3\u4e86\u9ad8\u7a00\u758f\u5ea6\u6761\u4ef6\u4e0b\u5e38\u89c1\u7684\u7cbe\u5ea6\u6025\u5267\u4e0b\u964d\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u7684\u6027\u80fd\u9650\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6ee1\u8db3\u786c\u4ef6\u5e73\u53f0\u7684\u5c3a\u5bf8\u7ea6\u675f\u8981\u6c42\u3002"}}
{"id": "2511.11676", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11676", "abs": "https://arxiv.org/abs/2511.11676", "authors": ["Hanchen David Wang", "Siwoo Bae", "Zirong Chen", "Meiyi Ma"], "title": "Learning with Preserving for Continual Multitask Learning", "comment": "25 pages, 16 figures, accepted at AAAI-2026", "summary": "Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86Learning with Preserving (LwP)\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u6301\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u89e3\u51b3\u6301\u7eed\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u653e\u7f13\u51b2\u533a\u3002", "motivation": "\u5173\u952e\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u800c\u4e0d\u9057\u5fd8\u5148\u524d\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u5b66\u4e60\u788e\u7247\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u800c\u5931\u8d25\u3002", "method": "\u5f15\u5165\u52a8\u6001\u52a0\u6743\u8ddd\u79bb\u4fdd\u6301(DWDP)\u635f\u5931\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u6570\u636e\u8868\u793a\u4e4b\u95f4\u7684\u6210\u5bf9\u8ddd\u79bb\u6765\u9632\u6b62\u8868\u793a\u6f02\u79fb\uff0c\u4fdd\u6301\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLwP\u4e0d\u4ec5\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u4e14\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u662f\u552f\u4e00\u8d85\u8d8a\u5f3a\u5355\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\u7684\u65b9\u6cd5\u3002", "conclusion": "LwP\u6846\u67b6\u901a\u8fc7\u4fdd\u6301\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u6709\u6548\u652f\u6301\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6301\u7eed\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5177\u6709\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12769", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12769", "abs": "https://arxiv.org/abs/2511.12769", "authors": ["Luyao Niu", "Zepu Wang", "Shuyi Guan", "Yang Liu", "Peng Sun"], "title": "Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting", "comment": null, "summary": "While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.", "AI": {"tldr": "Event-CausNet\uff1a\u5229\u7528LLM\u91cf\u5316\u975e\u7ed3\u6784\u5316\u4e8b\u4ef6\u62a5\u544a\uff0c\u6784\u5efa\u56e0\u679c\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u5c06\u56e0\u679c\u77e5\u8bc6\u6ce8\u5165\u53cc\u6d41GNN-LSTM\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u975e\u91cd\u590d\u4e8b\u4ef6\u671f\u95f4\u7684\u4ea4\u901a\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u91cd\u590d\u6027\u4ea4\u901a\u6a21\u5f0f\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e8b\u6545\u7b49\u975e\u91cd\u590d\u6027\u4e8b\u4ef6\u671f\u95f4\u53ef\u9760\u6027\u6025\u5267\u4e0b\u964d\uff0c\u56e0\u4e3aGNN\u672c\u8d28\u4e0a\u662f\u76f8\u5173\u6027\u6a21\u578b\uff0c\u65e0\u6cd5\u5904\u7406\u7a81\u53d1\u4e8b\u4ef6\u5f15\u5165\u7684\u65b0\u56e0\u679c\u56e0\u7d20\u3002", "method": "1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u975e\u7ed3\u6784\u5316\u4e8b\u4ef6\u62a5\u544a\uff1b2\uff09\u901a\u8fc7\u4f30\u8ba1\u5e73\u5747\u5904\u7406\u6548\u5e94\u6784\u5efa\u56e0\u679c\u77e5\u8bc6\u5e93\uff1b3\uff09\u4f7f\u7528\u65b0\u9896\u7684\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u5c06\u56e0\u679c\u77e5\u8bc6\u6ce8\u5165\u53cc\u6d41GNN-LSTM\u7f51\u7edc\u6765\u8c03\u6574\u548c\u589e\u5f3a\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvent-CausNet\u5c06\u9884\u6d4b\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u4e86\u9ad8\u8fbe35.87%\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5f25\u5408\u4e86\u76f8\u5173\u6027\u6a21\u578b\u4e0e\u56e0\u679c\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u53ef\u8fc1\u79fb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u63d0\u4f9b\u5173\u952e\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5173\u952e\u4e2d\u65ad\u671f\u95f4\u7684\u5b9e\u9645\u4ea4\u901a\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2511.12782", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12782", "abs": "https://arxiv.org/abs/2511.12782", "authors": ["Thomas Rivasseau"], "title": "LLM Reinforcement in Context", "comment": "4 pages", "summary": "Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u4e2d\u65ad\u673a\u5236\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u7528\u6237\u8f93\u5165\u4e2d\u5b9a\u671f\u63d2\u5165\u63a7\u5236\u8bed\u53e5\uff0c\u4ee5\u9632\u6b62\u6a21\u578b\u88ab\u8d8a\u72f1\u548c\u6076\u610f\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u8bad\u7ec3\u548c\u63d0\u793a\u6765\u63d0\u9ad8\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u968f\u7740\u7528\u6237\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u800c\u6709\u6548\u589e\u5f3a\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u8868\u660e\uff0cLLM\u88ab\u8d8a\u72f1\u7684\u6982\u7387\u4f1a\u968f\u7740\u7528\u6237\u8f93\u5165\u6216\u5bf9\u8bdd\u957f\u5ea6\u7684\u589e\u52a0\u800c\u4e0a\u5347\u3002", "method": "\u63d0\u51fa\u4e2d\u65ad\u673a\u5236\uff0c\u5373\u5728\u7528\u6237\u8f93\u5165\u4e2d\u6bcf\u9694x\u4e2atoken\u63d2\u5165\u63a7\u5236\u8bed\u53e5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u601d\u7ef4\u94fe\u8fc7\u7a0b\u4e2d\u4ee5\u9632\u6b62\u6076\u610f\u8ba1\u5212\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u6570\u636e\u3002", "conclusion": "\u4e2d\u65ad\u673a\u5236\u662f\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6027\u7684\u4e00\u4e2a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u968f\u7740\u7528\u6237\u8f93\u5165\u957f\u5ea6\u7684\u589e\u52a0\u800c\u6709\u6548\u9632\u6b62\u6a21\u578b\u88ab\u8d8a\u72f1\u3002"}}
{"id": "2511.12913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12913", "abs": "https://arxiv.org/abs/2511.12913", "authors": ["Yiming Zhao", "Jiwei Tang", "Shimin Di", "Libin Zheng", "Jianxing Yu", "Jian Yin"], "title": "CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling", "comment": null, "summary": "Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.", "AI": {"tldr": "\u63d0\u51fa\u4e86Chain-of-Scheduling (CoS)\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u5f0f\u8c03\u5ea6\u8fc7\u7a0b\u6fc0\u6d3b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u4ef6\u8c03\u5ea6\u80fd\u529b\uff0c\u5728\u4e8b\u4ef6\u793e\u4ea4\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65e5\u7a0b\u63a8\u8350\u3002", "motivation": "\u4e8b\u4ef6\u793e\u4ea4\u7f51\u7edc\u4e2d\u63a8\u8350\u65e5\u7a0b\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u3001\u6548\u679c\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\uff0c\u56e0\u4e3a\u8be5\u95ee\u9898\u5177\u6709NP\u96be\u7279\u6027\u3002", "method": "\u5c06\u65e5\u7a0b\u4efb\u52a1\u5206\u89e3\u4e3a\u63a2\u7d22\u3001\u9a8c\u8bc1\u548c\u96c6\u6210\u4e09\u4e2a\u539f\u5b50\u9636\u6bb5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u751f\u6210CoS\u8c03\u5ea6\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u6548\u679c\uff0c\u5177\u6709\u9ad8\u6548\u7387\u4e14\u53ef\u89e3\u91ca\uff0c\u5e76\u5728\u57df\u5916\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "CoS\u6846\u67b6\u6210\u529f\u6fc0\u6d3b\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u4ef6\u8c03\u5ea6\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u3001\u6548\u679c\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2511.12095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12095", "abs": "https://arxiv.org/abs/2511.12095", "authors": ["Shuhan Ye", "Yi Yu", "Qixin Zhang", "Chenqi Kong", "Qiangqiang Wu", "Kun Wang", "Xudong Jiang"], "title": "Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio", "comment": null, "summary": "Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \\textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \\textbf{ST-DSM} and \\textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \\(84.4\\%\\) accuracy, about \\(85\\%\\) of the full training set performance, while reducing training time by more than \\(50\\times\\) and storage cost by \\(6000\\times\\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.", "AI": {"tldr": "PACE\u662f\u9996\u4e2a\u9488\u5bf9SNN\u548c\u4e8b\u4ef6\u89c6\u89c9\u7684\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7ST-DSM\u548cPEQ-N\u6a21\u5757\u5c06\u5927\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u538b\u7f29\u6210\u7d27\u51d1\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u5feb\u901fSNN\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4e0eSNN\u7684\u751f\u7269\u5b66\u7279\u6027\u76f8\u5951\u5408\uff0c\u4f46SNN\u7531\u4e8e\u65f6\u95f4\u7f16\u7801\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u4e3a\u964d\u4f4eSNN\u8bad\u7ec3\u6210\u672c\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "PACE\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aST-DSM\u4f7f\u7528\u6b8b\u5dee\u819c\u7535\u4f4d\u6765\u7a20\u5316\u57fa\u4e8e\u5c16\u5cf0\u7684\u7279\u5f81\uff0c\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u5339\u914d\uff1bPEQ-N\u63d0\u4f9b\u5373\u63d2\u5373\u7528\u7684\u6982\u7387\u6574\u6570\u91cf\u5316\u5668\uff0c\u517c\u5bb9\u6807\u51c6\u4e8b\u4ef6\u5e27\u6d41\u6c34\u7ebf\u3002", "result": "\u5728DVS-Gesture\u3001CIFAR10-DVS\u548cN-MNIST\u6570\u636e\u96c6\u4e0a\uff0cPACE\u4f18\u4e8e\u73b0\u6709\u7684\u6838\u5fc3\u96c6\u9009\u62e9\u548c\u6570\u636e\u96c6\u84b8\u998f\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u4e8b\u4ef6\u6d41\u548c\u4f4e/\u4e2d\u7b49IPC\u4e0b\u8868\u73b0\u7a81\u51fa\u3002\u5728N-MNIST\u4e0a\u8fbe\u523084.4%\u51c6\u786e\u7387\uff0c\u7ea6\u4e3a\u5b8c\u6574\u8bad\u7ec3\u96c6\u6027\u80fd\u768485%\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f450\u500d\u4ee5\u4e0a\uff0c\u5b58\u50a8\u6210\u672c6000\u500d\u3002", "conclusion": "PACE\u80fd\u591f\u751f\u6210\u7d27\u51d1\u7684\u66ff\u4ee3\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u5206\u949f\u7ea7\u7684SNN\u8bad\u7ec3\u548c\u9ad8\u6548\u7684\u8fb9\u7f18\u90e8\u7f72\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u548cSNN\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12097", "abs": "https://arxiv.org/abs/2511.12097", "authors": ["Shuhan Ye", "Yi Yu", "Qixin Zhang", "Chenqi Kong", "Qiangqiang Wu", "Xudong Jiang", "Dacheng Tao"], "title": "Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks", "comment": null, "summary": "Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \\emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \\emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \\textbf{SpikeNM}, the first SNN-oriented \\emph{semi-structured} \\(N{:}M\\) pruning framework that learns sparse SNNs \\emph{from scratch}, enforcing \\emph{at most \\(N\\)} non-zeros per \\(M\\)-weight block. To avoid the combinatorial space complexity \\(\\sum_{k=1}^{N}\\binom{M}{k}\\) growing exponentially with \\(M\\), SpikeNM adopts an \\(M\\)-way basis-logit parameterization with a differentiable top-\\(k\\) sampler, \\emph{linearizing} per-block complexity to \\(\\mathcal O(M)\\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \\emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \\(2{:}4\\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.", "AI": {"tldr": "SpikeNM\u662f\u9996\u4e2a\u9762\u5411SNN\u7684\u534a\u7ed3\u6784\u5316N:M\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7M\u8def\u57fa\u5bf9\u6570\u53c2\u6570\u5316\u548c\u53ef\u5fae\u5206top-k\u91c7\u6837\u5668\uff0c\u7ebf\u6027\u5316\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u8d44\u683c\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u786c\u4ef6\u53cb\u597d\u7684\u7a00\u758f\u6a21\u5f0f\u3002", "motivation": "\u89e3\u51b3SNN\u6df1\u5ea6\u67b6\u6784\u53c2\u6570\u81a8\u80c0\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u8981\u4e48\u96be\u4ee5\u5728\u901a\u7528\u786c\u4ef6\u4e0a\u52a0\u901f\uff08\u975e\u7ed3\u6784\u5316\uff09\uff0c\u8981\u4e48\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u7cbe\u5ea6\u4e0b\u964d\uff08\u7ed3\u6784\u5316\uff09\u3002", "method": "\u63d0\u51fa\u534a\u7ed3\u6784\u5316N:M\u526a\u679d\u6846\u67b6\uff0c\u4f7f\u7528M\u8def\u57fa\u5bf9\u6570\u53c2\u6570\u5316\u548c\u53ef\u5fae\u5206top-k\u91c7\u6837\u5668\u7ebf\u6027\u5316\u590d\u6742\u5ea6\uff0c\u7ed3\u5408\u8d44\u683c\u84b8\u998f\u65b9\u6cd5\u5c06\u65f6\u95f4\u7d2f\u79ef\u4fe1\u7528\u8f6c\u6362\u4e3a\u5757\u7ea7\u8f6f\u76ee\u6807\u3002", "result": "\u57282:4\u7a00\u758f\u5ea6\u4e0b\uff0cSpikeNM\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u7cbe\u5ea6\uff0c\u540c\u65f6\u4ea7\u751f\u786c\u4ef6\u53cb\u597d\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u4e0e\u56fa\u6709\u8109\u51b2\u7a00\u758f\u6027\u4e92\u8865\u3002", "conclusion": "SpikeNM\u6210\u529f\u5e73\u8861\u4e86SNN\u7684\u7a00\u758f\u6027\u3001\u7cbe\u5ea6\u548c\u786c\u4ef6\u53cb\u597d\u6027\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13118", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13118", "abs": "https://arxiv.org/abs/2511.13118", "authors": ["Quanjiang Guo", "Sijie Wang", "Jinchuan Zhang", "Ben Zhang", "Zhao Kang", "Ling Tian", "Ke Yan"], "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction", "comment": "11 pages, 5 figures, accepted by AAAI 2026 (Oral)", "summary": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.", "AI": {"tldr": "Agent-Event-Coder (AEC) \u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u96f6\u6837\u672c\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u7c7b\u4f3c\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3LLM\u5728\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u7684\u7ed3\u6784\u8f93\u51fa\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u4e0d\u5b8c\u6574\u8f93\u51fa\u3001\u9519\u8bef\u5206\u7c7b\u89e6\u53d1\u8bcd\u3001\u7f3a\u5931\u53c2\u6570\u548c\u6a21\u5f0f\u8fdd\u89c4\u7b49\u95ee\u9898\u3002", "method": "\u5c06\u4e8b\u4ef6\u62bd\u53d6\u5206\u89e3\u4e3a\u68c0\u7d22\u3001\u89c4\u5212\u3001\u7f16\u7801\u548c\u9a8c\u8bc1\u56db\u4e2a\u4e13\u95e8\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7531\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u5904\u7406\uff0c\u4e8b\u4ef6\u6a21\u5f0f\u8868\u793a\u4e3a\u53ef\u6267\u884c\u7684\u7c7b\u5b9a\u4e49\uff0c\u901a\u8fc7\u9a8c\u8bc1\u667a\u80fd\u4f53\u8fdb\u884c\u786e\u5b9a\u6027\u9a8c\u8bc1\u548c\u7cbe\u786e\u53cd\u9988\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u548c\u516d\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAEC\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4ee3\u7801\u751f\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7f16\u7a0b\u542f\u53d1\u7684\u65b9\u6cd5\u548c\u534f\u4f5c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0cAEC\u80fd\u591f\u4f7fLLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4ea7\u751f\u7cbe\u786e\u3001\u5b8c\u6574\u4e14\u6a21\u5f0f\u4e00\u81f4\u7684\u4e8b\u4ef6\u62bd\u53d6\u7ed3\u679c\u3002"}}
{"id": "2511.12136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12136", "abs": "https://arxiv.org/abs/2511.12136", "authors": ["Karol C. Jurzec", "Tomasz Szydlo", "Maciej Wielgosz"], "title": "Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware", "comment": "6 pages, 6 figures, 1 table; code available at https://github.com/karol-jurzec/snn-generator/", "summary": "Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7C\u8bed\u8a00\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u6267\u884c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5e03\u5c40\u548c\u5229\u7528\u7a00\u758f\u6027\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u548c\u5185\u5b58\u51cf\u5c11\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNNs)\u5177\u6709\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u548c\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u8fd0\u884c\u65f6\u6765\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002", "method": "\u5c06SNNTorch\u8bad\u7ec3\u7684\u6a21\u578b\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684C\u8868\u793a\uff0c\u91c7\u7528\u9759\u6001\u7f13\u5b58\u53cb\u597d\u7684\u6570\u636e\u5e03\u5c40\u548c\u9884\u5206\u914d\u7b56\u7565\uff0c\u5e76\u5229\u7528\u7a00\u758f\u8109\u51b2\u6d3b\u52a8\u4fee\u526a\u4e0d\u6d3b\u8dc3\u7684\u795e\u7ecf\u5143\u548c\u7a81\u89e6\u3002", "result": "\u5728N-MNIST\u548cST-MNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0ePython\u57fa\u7ebf\u529f\u80fd\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u5728\u684c\u9762CPU\u4e0a\u83b7\u5f97\u7ea610\u500d\u52a0\u901f\uff0c\u901a\u8fc7\u4fee\u526a\u83b7\u5f97\u989d\u5916\u589e\u76ca\uff0c\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u53ef\u5728Arduino Portenta H7\u7b49\u5fae\u63a7\u5236\u5668\u4e0a\u90e8\u7f72\u3002", "conclusion": "\u5f53\u914d\u5907\u4f18\u5316\u8fd0\u884c\u65f6\u548c\u8109\u51b2\u9a71\u52a8\u6a21\u578b\u538b\u7f29\u65f6\uff0c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5728\u4f20\u7edf\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u9ad8\u6548\u6267\u884c\u3002"}}
{"id": "2511.12150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12150", "abs": "https://arxiv.org/abs/2511.12150", "authors": ["Yuqi Xie", "Shuhan Ye", "Yi Yu", "Chong Wang", "Qixin Zhang", "Jiazhen Xu", "Le Shen", "Yuanbin Qian", "Jiangbo Qian", "Guoqi Li"], "title": "Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain", "comment": null, "summary": "The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.", "AI": {"tldr": "TMKT\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u6b65\u6df7\u5408\u7b56\u7565\u5c06RGB\u548cDVS\u8f93\u5165\u5728\u591a\u4e2a\u65f6\u95f4\u6b65\u8fdb\u884c\u63d2\u503c\uff0c\u7ed3\u5408\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u6001\u611f\u77e5\u76ee\u6807\uff0c\u5b9e\u73b0\u4eceRGB\u5230\u4e8b\u4ef6\u76f8\u673a\u7684\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\u5177\u6709\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u4e8b\u4ef6\u6570\u636e\u7a00\u7f3a\u4e14DVS\u8f93\u51fa\u7a00\u758f\uff0c\u4f20\u7edfRGB\u5230DVS\u7684\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\u56e0\u6a21\u6001\u5206\u5e03\u5dee\u5f02\u5927\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u6b65\u6df7\u5408\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u5305\u542b\u6982\u7387\u65f6\u95f4\u6b65\u6df7\u5408\u7b56\u7565\u548c\u4e24\u4e2a\u6a21\u6001\u611f\u77e5\u76ee\u6807\uff1a\u6a21\u6001\u611f\u77e5\u6307\u5bfc\u548c\u6df7\u5408\u6bd4\u611f\u77e5\uff0c\u901a\u8fc7\u63d2\u503cRGB\u548cDVS\u8f93\u5165\u5e76\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u7279\u5f81\u4e0e\u6df7\u5408\u8ba1\u5212\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548cSNN\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u7f13\u89e3\u4e86\u6a21\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u8109\u51b2\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "TMKT\u6846\u67b6\u901a\u8fc7\u65f6\u95f4\u6b65\u6df7\u5408\u548c\u6a21\u6001\u611f\u77e5\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86RGB\u5230\u4e8b\u4ef6\u76f8\u673a\u7684\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff0c\u4e3a\u8de8\u6a21\u6001\u89c6\u89c9\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13481", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13481", "abs": "https://arxiv.org/abs/2511.13481", "authors": ["Attapol T. Rutherford", "Sirisak Chueykamhang", "Thachaparn Bunditlurdruk", "Nanthicha Angsuwichitkul"], "title": "Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns", "comment": null, "summary": "Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u9762\u60c5\u611f\u5206\u6790(ABSA)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u7801\u6cf0\u8bed\u8d22\u52a1\u5e74\u62a5\u4e2d\u7684\u6a21\u7cca\u60c5\u611f\uff0c\u5e76\u901a\u8fc7\u4e8b\u4ef6\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u80a1\u4ef7\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "motivation": "\u8d22\u52a1\u6587\u4ef6\u4e2d\u7684\u60c5\u611f\u7406\u89e3\u5bf9\u6d1e\u5bdf\u5e02\u573a\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e9b\u62a5\u544a\u5e38\u4f7f\u7528\u6a21\u7cca\u8bed\u8a00\u6765\u5448\u73b0\u79ef\u6781\u6216\u4e2d\u6027\u524d\u666f\uff0c\u5373\u4f7f\u5b9e\u9645\u6761\u4ef6\u53ef\u80fd\u4e0d\u5229\u3002", "method": "\u5f00\u53d1\u4e86\u6807\u6ce8\u6a21\u7cca\u60c5\u611f\u7684\u7279\u5b9a\u6307\u5357\uff0c\u6807\u6ce8\u4e86100\u591a\u4efd\u8d22\u52a1\u62a5\u544a\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u57fa\u51c6\u6d4b\u8bd5\u4e86\u591a\u79cd\u6587\u672c\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u5728\u60c5\u611f\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e8b\u4ef6\u7814\u7a76\u8868\u660e\u5e02\u573a\u53cd\u5e94\u53d7\u5230\u62a5\u544a\u4e2d\u7279\u5b9a\u65b9\u9762\u7684\u9009\u62e9\u6027\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8d22\u52a1\u6587\u672c\u60c5\u611f\u5206\u6790\u7684\u590d\u6742\u6027\uff0c\u5e76\u51f8\u663e\u4e86\u89e3\u51b3\u6a21\u7cca\u8bed\u8a00\u4ee5\u51c6\u786e\u8bc4\u4f30\u5e02\u573a\u60c5\u7eea\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.13565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13565", "abs": "https://arxiv.org/abs/2511.13565", "authors": ["Jingyi Zhao", "Daqian Shi", "Zhengda Wang", "Xiongfeng Tang", "Yanguo Qin"], "title": "Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction", "comment": "5 pages, l figure, l table. Accepted at AI4RWC@WI-IAT 2025", "summary": "Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.", "AI": {"tldr": "\u63d0\u51fa\u4eba\u7c7b\u5171\u751f\u5065\u5eb7\u667a\u80fd\uff08HSHI\uff09\u6846\u67b6\uff0c\u6574\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u7f51\u7edc\u3001\u8fb9\u7f18\u4e91\u534f\u540c\u8ba1\u7b97\u548c\u6df7\u5408\u6570\u636e\u77e5\u8bc6\u5efa\u6a21\uff0c\u5b9e\u73b0\u4ece\u88ab\u52a8\u76d1\u6d4b\u5230\u4e3b\u52a8\u534f\u4f5c\u7684\u5065\u5eb7\u7ba1\u7406\u8f6c\u53d8\u3002", "motivation": "\u4f20\u7edf\u53ef\u7a7f\u6234\u8bbe\u5907\u4f9d\u8d56\u7ecf\u9a8c\u6027\u6750\u6599\u8bbe\u8ba1\u548c\u57fa\u7840\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u5065\u5eb7\u7ba1\u7406\u3002", "method": "\u91c7\u7528AI\u9a71\u52a8\u7684\u6750\u6599\u548c\u5fae\u7ed3\u6784\u4f18\u5316\u3001\u591a\u6a21\u6001\u4fe1\u53f7\u9c81\u68d2\u89e3\u91ca\u3001\u7fa4\u4f53\u6d1e\u5bdf\u4e0e\u4e2a\u6027\u5316\u9002\u5e94\u7684\u53cc\u673a\u5236\uff0c\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u7684\u95ed\u73af\u4f18\u5316\u3002", "result": "HSHI\u6846\u67b6\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e2a\u4f53\u95f4\u548c\u4e2a\u4f53\u5185\u53d8\u5f02\u6027\uff0c\u4fc3\u8fdb\u9884\u9632\u6027\u3001\u9002\u5e94\u6027\u7684\u5065\u5eb7\u7ba1\u7406\u6a21\u5f0f\u3002", "conclusion": "HSHI\u4ee3\u8868\u4e86\u533b\u7597\u4fdd\u5065\u5411\u9884\u9632\u3001\u9002\u5e94\u6027\u548c\u6280\u672f\u4e0e\u5065\u5eb7\u7ba1\u7406\u548c\u8c10\u5173\u7cfb\u6a21\u5f0f\u7684\u91cd\u5927\u8f6c\u53d8\u3002"}}
{"id": "2511.13593", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13593", "abs": "https://arxiv.org/abs/2511.13593", "authors": ["Piaohong Wang", "Motong Tian", "Jiaxian Li", "Yuan Liang", "Yuqing Wang", "Qianben Chen", "Tiannan Wang", "Zhicong Lu", "Jiawei Ma", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents", "comment": null, "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86O-Mem\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u7528\u6237\u753b\u50cf\u52a8\u6001\u63d0\u53d6\u548c\u66f4\u65b0\u7528\u6237\u7279\u5f81\uff0c\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7ef4\u6301\u957f\u671f\u4ea4\u4e92\u65f6\u9762\u4e34\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u52a8\u6001\u4e2a\u6027\u5316\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u8bed\u4e49\u5206\u7ec4\u7684\u8bb0\u5fc6\u7cfb\u7edf\u4f1a\u5ffd\u7565\u8bed\u4e49\u65e0\u5173\u4f46\u5173\u952e\u7684\u7528\u6237\u4fe1\u606f\u5e76\u5f15\u5165\u68c0\u7d22\u566a\u58f0\u3002", "method": "\u57fa\u4e8e\u4e3b\u52a8\u7528\u6237\u753b\u50cf\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u52a8\u6001\u63d0\u53d6\u548c\u66f4\u65b0\u7528\u6237\u7279\u5f81\u4e0e\u4e8b\u4ef6\u8bb0\u5f55\uff0c\u652f\u6301\u4eba\u7269\u5c5e\u6027\u548c\u4e3b\u9898\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u5206\u5c42\u68c0\u7d22\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u4e0a\u8fbe\u523051.76%\uff08\u6bd4LangMem\u63d0\u5347\u8fd13%\uff09\uff0c\u5728PERSONAMEM\u4e0a\u8fbe\u523062.99%\uff08\u6bd4A-Mem\u63d0\u53473.5%\uff09\uff0c\u540c\u65f6\u63d0\u5347\u4e86token\u548c\u4ea4\u4e92\u54cd\u5e94\u65f6\u95f4\u6548\u7387\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u9ad8\u6548\u4e14\u7c7b\u4eba\u7684\u4e2a\u6027\u5316AI\u52a9\u624b\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2511.12233", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12233", "abs": "https://arxiv.org/abs/2511.12233", "authors": ["Dongdong Zhao", "Qiben Xu", "Ranxin Fang", "Baogang Song"], "title": "Model Inversion Attack Against Deep Hashing", "comment": null, "summary": "Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86DHMI\uff0c\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6\u54c8\u5e0c\u7684\u6269\u6563\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u4ece\u54c8\u5e0c\u7801\u91cd\u6784\u9ad8\u8d28\u91cf\u539f\u59cb\u56fe\u50cf\uff0c\u63ed\u793a\u4e86\u6df1\u5ea6\u54c8\u5e0c\u7cfb\u7edf\u7684\u4e25\u91cd\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u6df1\u5ea6\u54c8\u5e0c\u7cfb\u7edf\u867d\u7136\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u7387\uff0c\u4f46\u5b58\u5728\u88ab\u5ffd\u89c6\u7684\u4e25\u91cd\u9690\u79c1\u98ce\u9669\u2014\u2014\u653b\u51fb\u8005\u53ef\u80fd\u4ece\u54c8\u5e0c\u7801\u91cd\u6784\u539f\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u751f\u7269\u7279\u5f81\u4f2a\u9020\u548c\u9690\u79c1\u6cc4\u9732\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u6df1\u5ea6\u54c8\u5e0c\u7684\u79bb\u6563\u6c49\u660e\u7a7a\u95f4\u7279\u6027\u3002", "method": "DHMI\u9996\u5148\u5bf9\u8f85\u52a9\u6570\u636e\u96c6\u805a\u7c7b\u5f97\u5230\u8bed\u4e49\u54c8\u5e0c\u4e2d\u5fc3\u4f5c\u4e3a\u4ee3\u7406\u951a\u70b9\uff0c\u7136\u540e\u63d0\u51fa\u4ee3\u7406\u5f15\u5bfc\u7684\u53bb\u566a\u4f18\u5316\u65b9\u6cd5\uff0c\u878d\u5408\u5206\u7c7b\u4e00\u81f4\u6027\u548c\u54c8\u5e0c\u90bb\u8fd1\u5ea6\u4f5c\u4e3a\u653b\u51fb\u6307\u6807\u52a8\u6001\u9009\u62e9\u5019\u9009\u6837\u672c\uff0c\u901a\u8fc7\u4ee3\u7406\u6a21\u578b\u7c07\u6307\u5bfc\u5019\u9009\u6837\u672c\u7ec6\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDHMI\u5373\u4f7f\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u6210\u529f\u91cd\u6784\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "DHMI\u8bc1\u5b9e\u4e86\u6df1\u5ea6\u54c8\u5e0c\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u4ece\u54c8\u5e0c\u7801\u91cd\u6784\u539f\u59cb\u6570\u636e\uff0c\u4e3a\u6df1\u5ea6\u54c8\u5e0c\u7cfb\u7edf\u7684\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.11880", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11880", "abs": "https://arxiv.org/abs/2511.11880", "authors": ["David Montero", "Miguel D. Mahecha", "Francesco Martinuzzi", "C\u00e9sar Aybar", "Anne Klosterhalfen", "Alexander Knohl", "Jes\u00fas Anaya", "Clemens Mosig", "Sebastian Wieneke"], "title": "Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production", "comment": null, "summary": "Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.", "AI": {"tldr": "\u6bd4\u8f83GPT-2\u548cLSTM\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u68ee\u6797CO\u2082\u5438\u6536\u91cf\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0LSTM\u6574\u4f53\u8868\u73b0\u66f4\u597d\uff0c\u4f46GPT-2\u5728\u6781\u7aef\u4e8b\u4ef6\u4e2d\u66f4\u4f18\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u67b6\u6784\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u591a\u6a21\u6001\u8f93\u5165\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5171\u540c\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u68ee\u6797CO\u2082\u5438\u6536\u91cf\u65f6\u7a7a\u52a8\u6001\u76d1\u6d4b\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u6da1\u5ea6\u534f\u65b9\u5dee\u5854\u7a7a\u95f4\u8986\u76d6\u6709\u9650\uff0c\u9065\u611f\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5355\u4e00\u4f20\u611f\u5668\u548c\u7edf\u8ba1\u6a21\u578b\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684GPP\u65f6\u95f4\u52a8\u6001\u3002", "method": "\u4f7f\u7528GPT-2\uff08Transformer\u67b6\u6784\uff09\u548cLSTM\uff08\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\u4e24\u79cd\u4ee3\u8868\u6027\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u53d8\u91cf\u8f93\u5165\u8fdb\u884cGPP\u9884\u6d4b\uff0c\u5206\u6790\u4e0d\u540c\u65f6\u95f4\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u8fbe\u5230\u76f8\u4f3c\u7cbe\u5ea6\uff0cLSTM\u6574\u4f53\u8868\u73b0\u66f4\u597d\uff0c\u4f46GPT-2\u5728\u6781\u7aef\u4e8b\u4ef6\u4e2d\u8868\u73b0\u66f4\u4f18\u3002LSTM\u4f7f\u7528\u66f4\u77ed\u7684\u8f93\u5165\u7a97\u53e3\u5c31\u80fd\u8fbe\u5230\u7c7b\u4f3c\u7cbe\u5ea6\uff0c\u8f90\u5c04\u662f\u6700\u91cd\u8981\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u591a\u6a21\u6001\u8f93\u5165\u5171\u540c\u51b3\u5b9aGPP\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u76d1\u6d4b\u9646\u5730\u78b3\u52a8\u6001\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.12291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12291", "abs": "https://arxiv.org/abs/2511.12291", "authors": ["Andrea Bertogalli", "Giacomo Boracchi", "Luca Magri"], "title": "One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving", "comment": null, "summary": "We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5916\u53c2\u6807\u5b9a\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u4f30\u8ba1\u4e8b\u4ef6\u76f8\u673a\u3001LiDAR\u548cRGB\u76f8\u673a\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u59ff\uff0c\u7279\u522b\u5173\u6ce8\u5177\u6709\u6311\u6218\u6027\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u590d\u6742\u89c6\u89c9\u7cfb\u7edf\u4e2d\uff0c\u7cbe\u786e\u7684\u591a\u4f20\u611f\u5668\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u72ec\u7684\u53cc\u5bf9\u6807\u5b9a\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4e00\u7ad9\u5f0f\u8054\u5408\u6807\u5b9a\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u5e76\u6784\u5efa\u4e86\u65b0\u578b3D\u6807\u5b9a\u9776\u6807\uff0c\u5305\u542b\u5e73\u9762\u7279\u5f81\u3001ChArUco\u56fe\u6848\u548c\u4e3b\u52a8LED\u6a21\u5f0f\uff0c\u5206\u522b\u9488\u5bf9LiDAR\u3001RGB\u76f8\u673a\u548c\u4e8b\u4ef6\u76f8\u673a\u7684\u7279\u6027\u3002\u5b9e\u73b0\u4e00\u6b21\u6027\u8054\u5408\u5916\u53c2\u6807\u5b9a\u8fc7\u7a0b\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8be5\u6570\u636e\u96c6\u4f7f\u7528\u5148\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\u8bbe\u7f6e\u8bb0\u5f55\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u51c6\u786e\u3001\u9c81\u68d2\u7684\u591a\u4f20\u611f\u5668\u8054\u5408\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.11935", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11935", "abs": "https://arxiv.org/abs/2511.11935", "authors": ["Munib Mesinovic", "Tingting Zhu"], "title": "SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis", "comment": null, "summary": "Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the \"preprocessing gap\" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.", "AI": {"tldr": "SurvBench\u662f\u4e00\u4e2a\u5f00\u6e90\u9884\u5904\u7406\u7ba1\u9053\uff0c\u5c06\u539f\u59cbPhysioNet\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u7684\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u5f20\u91cf\uff0c\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\u95ee\u9898\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e3a\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u5206\u6790\u63d0\u4f9b\u4e86\u5de8\u5927\u673a\u4f1a\uff0c\u4f46\u7531\u4e8e\u9884\u5904\u7406\u65b9\u6cd5\u4e0d\u4e00\u81f4\uff0c\u53ef\u590d\u73b0\u6027\u53d7\u5230\u4e25\u91cd\u9650\u5236\u3002", "method": "\u63d0\u4f9b\u4e09\u4e2a\u91cd\u75c7\u76d1\u62a4\u6570\u636e\u5e93\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u5b9e\u73b0\u4e25\u683c\u7684\u6570\u636e\u8d28\u91cf\u63a7\u5236\u3001\u60a3\u8005\u7ea7\u5206\u5272\u3001\u663e\u5f0f\u7f3a\u5931\u503c\u8ddf\u8e2a\u548c\u6807\u51c6\u5316\u65f6\u95f4\u805a\u5408\uff0c\u652f\u6301\u5355\u98ce\u9669\u548c\u7ade\u4e89\u98ce\u9669\u573a\u666f\u3002", "result": "\u8f93\u51fa\u4e0epycox\u5e93\u517c\u5bb9\uff0c\u652f\u6301\u6807\u51c6\u7edf\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u963b\u788d\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u6a21\u578b\u516c\u5e73\u6bd4\u8f83\u7684\"\u9884\u5904\u7406\u5dee\u8ddd\"\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u3001\u914d\u7f6e\u9a71\u52a8\u7684\u9884\u5904\u7406\u548c\u5168\u9762\u6587\u6863\uff0cSurvBench\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u65b9\u6cd5\u521b\u65b0\u800c\u975e\u6570\u636e\u5de5\u7a0b\u3002"}}
{"id": "2511.12365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12365", "abs": "https://arxiv.org/abs/2511.12365", "authors": ["Yiqing Shen", "Mathias Unberath"], "title": "Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning", "comment": null, "summary": "Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.", "AI": {"tldr": "DT-R1\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u591a\u6a21\u6001\u89c6\u89c9\u8f93\u5165\u7684\u6570\u5b57\u5316\u53cc\u80de\u80ce\u8868\u793a\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u9ad8\u5c42\u8868\u793a\u8fdb\u884c\u7edf\u4e00\u89c6\u89c9\u63a8\u7406\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5fae\u8c03\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u9650\u5236\u4e86\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u6570\u5b57\u5316\u53cc\u80de\u80ce\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\u9a8c\u8bc1\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8f93\u51fa\u51c6\u786e\u6027\u3002", "result": "\u5728\u6db5\u76d6\u4e24\u79cd\u6a21\u6001\u548c\u56db\u79cd\u4efb\u52a1\u7c7b\u578b\u7684\u516d\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDT-R1\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u3002", "conclusion": "DT-R1\u5f00\u521b\u4e86\u901a\u8fc7\u6570\u5b57\u5316\u53cc\u80de\u80ce\u8868\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u89c6\u89c9\u63a8\u7406\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.12389", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12389", "abs": "https://arxiv.org/abs/2511.12389", "authors": ["Divake Kumar", "Patrick Poggi", "Sina Tayebati", "Devashri Naik", "Nilesh Ahuja", "Amit Ranjan Trivedi"], "title": "Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation", "comment": null, "summary": "Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\u89e3\u8026\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u91c7\u6837\u3001\u96c6\u6210\u6216\u989d\u5916\u524d\u5411\u4f20\u64ad\uff0c\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u548c\u66f4\u7d27\u5bc6\u7684\u9884\u6d4b\u533a\u95f4\u3002", "motivation": "\u4f20\u7edf\u4f30\u8ba1\u5668\u5c06\u6240\u6709\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u538b\u7f29\u4e3a\u5355\u4e00\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u65e0\u6cd5\u53ef\u9760\u6307\u5bfc\u4f55\u65f6\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u6216\u8c03\u6574\u63a8\u7406\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u5316\u5168\u5c40\u5bc6\u5ea6\u6a21\u578b\u4f30\u8ba1\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7531\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\u6784\u6210\uff1a\u5c40\u90e8\u652f\u6301\u4e0d\u8db3\u3001\u6d41\u5f62\u8c31\u584c\u9677\u548c\u8de8\u5c42\u7279\u5f81\u4e0d\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u7ecf\u9a8c\u6b63\u4ea4\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u3002", "result": "\u5728MOT17\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u7ea660%\u8ba1\u7b97\u91cf\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u6bd4\u603b\u4e0d\u786e\u5b9a\u6027\u57fa\u7ebf\u63d0\u9ad813.6\u4e2a\u767e\u5206\u70b9\u7684\u8ba1\u7b97\u8282\u7701\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u81ea\u8c03\u8282\u89c6\u89c9\u63a8\u7406\uff0c\u901a\u8fc7\u6b63\u4ea4\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.12154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12154", "abs": "https://arxiv.org/abs/2511.12154", "authors": ["Gustavo Polleti", "Marlesson Santana", "Eduardo Fontes"], "title": "Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions", "comment": null, "summary": "We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u91d1\u878d\u4ea4\u6613\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u7ed3\u6784\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5f00\u653e\u94f6\u884c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u662f\u5317\u7f8e\u9996\u4e2a\u5927\u89c4\u6a21\u8de8\u91d1\u878d\u673a\u6784\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u548c\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u65b9\u6cd5\u5728\u91d1\u878d\u4ea4\u6613\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5f00\u653e\u94f6\u884c\u573a\u666f\u4e0b\uff0c\u63a2\u7d22\u591a\u6a21\u6001\u8868\u793a\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u9002\u5e94\u4ea4\u6613\u5e8f\u5217\uff0c\u6574\u5408\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002", "result": "\u6a21\u578b\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u8868\u73b0\u5c24\u5176\u7a81\u51fa\uff0c\u8bc1\u660e\u591a\u6a21\u6001\u8868\u793a\u80fd\u591f\u8de8\u5730\u7406\u533a\u57df\u548c\u91d1\u878d\u673a\u6784\u6cdb\u5316\u3002", "conclusion": "\u81ea\u76d1\u7763\u6a21\u578b\u5728\u91d1\u878d\u5e94\u7528\uff08\u5982\u6b3a\u8bc8\u9884\u9632\u3001\u4fe1\u7528\u98ce\u9669\u548c\u5ba2\u6237\u6d1e\u5bdf\uff09\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u591a\u6a21\u6001\u8868\u793a\u662f\u63a8\u8fdb\u91d1\u878d\u6280\u672f\u53d1\u5c55\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.12460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12460", "abs": "https://arxiv.org/abs/2511.12460", "authors": ["Changzeng Fu", "Shiwen Zhao", "Yunze Zhang", "Zhongquan Jian", "Shiqi Zhao", "Chaoran Liu"], "title": "Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection", "comment": "AAAI 2026 accepted", "summary": "Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.", "AI": {"tldr": "\u63d0\u51fa\u4e86P\u00b3HF\u6a21\u578b\uff0c\u901a\u8fc7\u4e2a\u6027\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\u3001\u8d85\u56fe-Transformer\u67b6\u6784\u548c\u4e8b\u4ef6\u7ea7\u9886\u57df\u89e3\u8026\uff0c\u5728\u6291\u90c1\u75c7\u68c0\u6d4b\u4efb\u52a1\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u548c\u52a0\u6743F1\u503c\u63d0\u5347\u7ea610%\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u6216\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\u5728\u5efa\u6a21\u4e2a\u4f53\u5dee\u5f02\u548c\u8de8\u6a21\u6001\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u8fdb\u884c\u4e2a\u6027\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\uff0c\u5c06\u79bb\u6563\u4e2a\u4f53\u7279\u5f81\u8f6c\u5316\u4e3a\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff1b\u91c7\u7528\u8d85\u56fe-Former\u67b6\u6784\u5efa\u6a21\u9ad8\u9636\u8de8\u6a21\u6001\u65f6\u95f4\u5173\u7cfb\uff1b\u901a\u8fc7\u4e8b\u4ef6\u7ea7\u9886\u57df\u89e3\u8026\u548c\u5bf9\u6bd4\u5b66\u4e60\u63d0\u9ad8\u8de8\u884c\u4e3a\u4e0a\u4e0b\u6587\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728MPDD-Young\u6570\u636e\u96c6\u4e0a\uff0cP\u00b3HF\u5728\u4e8c\u5143\u548c\u4e09\u5143\u6291\u90c1\u75c7\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u548c\u52a0\u6743F1\u503c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u7ea610%\u3002", "conclusion": "\u4e2a\u6027\u5f15\u5bfc\u8868\u793a\u5b66\u4e60\u548c\u9ad8\u9636\u8d85\u56fe\u63a8\u7406\u5bf9\u4e8e\u751f\u6210\u9c81\u68d2\u7684\u3001\u4e2a\u4f53\u611f\u77e5\u7684\u6291\u90c1\u75c7\u76f8\u5173\u8868\u793a\u90fd\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002"}}
{"id": "2511.12868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12868", "abs": "https://arxiv.org/abs/2511.12868", "authors": ["Ruiqi Yang", "Tian Yun", "Zihan Wang", "Ellie Pavlick"], "title": "Video Finetuning Improves Reasoning Between Frames", "comment": "Accepted at CogInterp @ NeurIPS 2025", "summary": "Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u601d\u7ef4\u94fe(vCoT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5e27\u95f4\u8fc7\u6e21\u4e8b\u4ef6\u63cf\u8ff0\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u5df2\u9690\u542b\u638c\u63e1\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u56fe\u50cf\u6269\u5c55\u5230\u89c6\u9891\u65f6\uff0c\u901a\u5e38\u53ea\u662f\u7b80\u5355\u62fc\u63a5\u5e27token\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u9891\u65f6\u5e8f\u5173\u7cfb\u7684\u6709\u6548\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u601d\u7ef4\u94fe(vCoT)\uff0c\u5728\u8fde\u7eed\u5e27\u4e4b\u95f4\u751f\u6210\u8fc7\u6e21\u4e8b\u4ef6\u63cf\u8ff0\u4f5c\u4e3a\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u56fe\u50cf\u6a21\u578b\u4e0e\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u5728\u6709\u65e0\u8fc7\u6e21\u7ebf\u7d22\u4e0b\u7684\u8868\u73b0\u3002", "result": "vCoT\u663e\u8457\u63d0\u5347\u56fe\u50cf\u6a21\u578b\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u6027\u80fd\uff0c\u4f46\u5bf9\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u589e\u76ca\u6709\u9650\uff1b\u89c6\u9891\u6a21\u578b\u80fd\u5c06\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u9759\u6001\u573a\u666f\uff0c\u5728\u5173\u7cfb\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u56fe\u50cf\u6a21\u578b\u3002", "conclusion": "\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u5df2\u9690\u542b\u638c\u63e1\u5e27\u95f4\u8fc7\u6e21\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u79cd\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u53ef\u8fc1\u79fb\u5230\u9759\u6001\u89c6\u89c9\u4efb\u52a1\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.12709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12709", "abs": "https://arxiv.org/abs/2511.12709", "authors": ["Sangwoo Seo", "Hyunsung Kim", "Jiwan Kim", "Chanyoung Park"], "title": "Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations", "comment": "Preprint", "summary": "Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdaMeshNet\u6846\u67b6\uff0c\u5728\u57fa\u4e8e\u7f51\u683c\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u8fde\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8ba1\u7b97\u91cd\u8fde\u5ef6\u8fdf\u5206\u6570\u6765\u52a8\u6001\u9009\u62e9\u6d88\u606f\u4f20\u9012\u5c42\u8fdb\u884c\u91cd\u8fde\uff0c\u6709\u6548\u7f13\u89e3\u8fc7\u538b\u7f29\u95ee\u9898\u5e76\u63d0\u9ad8\u6d41\u4f53\u6a21\u62df\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7f51\u683c\u91cd\u8fde\u65b9\u6cd5\u5728\u5e94\u7528GNN\u524d\u5b8c\u6210\u6240\u6709\u91cd\u8fde\u64cd\u4f5c\uff0c\u5047\u8bbe\u8fdc\u8ddd\u79bb\u8282\u70b9\u95f4\u77ac\u65f6\u76f8\u4e92\u4f5c\u7528\u4e14\u5ffd\u7565\u7c92\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f\uff0c\u8fd9\u5728\u7269\u7406\u4e0a\u4e0d\u73b0\u5b9e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6a21\u62df\u7269\u7406\u76f8\u4e92\u4f5c\u7528\u9010\u6b65\u4f20\u64ad\u7684\u65b9\u6cd5\u3002", "method": "AdaMeshNet\u6846\u67b6\u5728\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u8fde\uff0c\u4e3a\u7f51\u683c\u56fe\u4e2d\u7684\u74f6\u9888\u8282\u70b9\u8ba1\u7b97\u57fa\u4e8e\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u548c\u901f\u5ea6\u5dee\u7684\u91cd\u8fde\u5ef6\u8fdf\u5206\u6570\uff0c\u52a8\u6001\u9009\u62e9\u6d88\u606f\u4f20\u9012\u5c42\u8fdb\u884c\u91cd\u8fde\u3002", "result": "\u5728\u57fa\u4e8e\u7f51\u683c\u7684\u6d41\u4f53\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cAdaMeshNet\u4f18\u4e8e\u4f20\u7edf\u91cd\u8fde\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u6a21\u62df\u7269\u7406\u76f8\u4e92\u4f5c\u7528\u7684\u987a\u5e8f\u7279\u6027\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002", "conclusion": "\u81ea\u9002\u5e94\u91cd\u8fde\u8fc7\u7a0b\u80fd\u66f4\u597d\u5730\u5efa\u6a21\u7269\u7406\u76f8\u4e92\u4f5c\u7528\u7684\u9010\u6b65\u4f20\u64ad\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u7269\u7406\u73b0\u5b9e\u6027\u548c\u8ddd\u79bb\u4fe1\u606f\u5904\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u7f51\u683cGNN\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12760", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12760", "abs": "https://arxiv.org/abs/2511.12760", "authors": ["Ben Gao", "Jordan Patracone", "St\u00e9phane Chr\u00e9tien", "Olivier Alata"], "title": "Conformal Online Learning of Deep Koopman Linear Embeddings", "comment": null, "summary": "We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.", "AI": {"tldr": "COLoKe\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u66f4\u65b0\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edfKoopman\u5d4c\u5165\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u63d0\u5347\u7a7a\u95f4\u4e2d\u7684\u591a\u6b65\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u8bc4\u4f30\u673a\u5236\u9009\u62e9\u6027\u66f4\u65b0\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u5efa\u6a21\u4e2dKoopman\u8868\u793a\u7684\u81ea\u9002\u5e94\u66f4\u65b0\u95ee\u9898\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u5e76\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6a21\u578b\u66f4\u65b0\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u591a\u6b65\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u5728\u63d0\u5347\u7684\u7ebf\u6027\u7a7a\u95f4\u4e2d\u5efa\u6a21\u52a8\u529b\u5b66\uff0c\u91c7\u7528\u4e00\u81f4\u6027\u8bc4\u4f30\u673a\u5236\u52a8\u6001\u6821\u51c6\u9608\u503c\u6765\u89e6\u53d1\u9009\u62e9\u6027\u66f4\u65b0\u3002", "result": "\u5728\u57fa\u51c6\u52a8\u529b\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCOLoKe\u80fd\u4fdd\u6301\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u66f4\u65b0\u5e76\u907f\u514d\u8fc7\u62df\u5408\u3002", "conclusion": "COLoKe\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86Koopman\u5d4c\u5165\u7684\u81ea\u9002\u5e94\u5728\u7ebf\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u4f18\u5316\u4e86\u66f4\u65b0\u6548\u7387\u3002"}}
{"id": "2511.12962", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12962", "abs": "https://arxiv.org/abs/2511.12962", "authors": ["Daniel Cavadia"], "title": "EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics", "comment": null, "summary": "Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.", "AI": {"tldr": "EndoSight AI\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u548c\u5206\u5272\u80c3\u80a0\u9053\u606f\u8089\uff0c\u5728GPU\u4e0a\u8fbe\u523035+ FPS\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u68c0\u6d4bmAP\u4e3a88.3%\uff0c\u5206\u5272Dice\u7cfb\u6570\u8fbe69%\u3002", "motivation": "\u5728\u5185\u7aa5\u955c\u68c0\u67e5\u8fc7\u7a0b\u4e2d\u7cbe\u786e\u5b9e\u65f6\u68c0\u6d4b\u80c3\u80a0\u9053\u606f\u8089\u5bf9\u4e8e\u7ed3\u76f4\u80a0\u764c\u7684\u65e9\u671f\u8bca\u65ad\u548c\u9884\u9632\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u516c\u5f00\u7684Hyper-Kvasir\u6570\u636e\u96c6\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u91c7\u7528\u4e34\u5e8a\u76f8\u5173\u6027\u80fd\u6307\u6807\u548c\u70ed\u611f\u77e5\u8bad\u7ec3\u7a0b\u5e8f\u786e\u4fdd\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b088.3%\u7684\u5e73\u5747\u7cbe\u5ea6\u7528\u4e8e\u606f\u8089\u68c0\u6d4b\uff0c\u5206\u5272Dice\u7cfb\u6570\u8fbe69%\uff0cGPU\u63a8\u7406\u901f\u5ea6\u8d85\u8fc735\u5e27/\u79d2\u3002", "conclusion": "\u8be5\u96c6\u6210AI\u89e3\u51b3\u65b9\u6848\u65e8\u5728\u65e0\u7f1d\u90e8\u7f72\u5230\u5185\u7aa5\u955c\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u6709\u671b\u63d0\u9ad8\u80c3\u80a0\u9053\u533b\u7597\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2511.12903", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12903", "abs": "https://arxiv.org/abs/2511.12903", "authors": ["Bo Hu", "Jose C. Principe"], "title": "Contrastive Entropy Bounds for Density and Conditional Density Decomposition", "comment": null, "summary": "This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.\n  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.\n  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u8d1d\u53f6\u65af\u9ad8\u65af\u89c6\u89d2\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u548c\u7b97\u5b50\u5206\u89e3\u5206\u6790\u81ea\u7f16\u7801\u5668\u548c\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff0c\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u7b97\u5b50\u8ff9\u548c\u6838\u8303\u6570\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4f7f\u7528\u5185\u79ef\u548c\u8303\u6570\u5b9a\u4e49\u8fb9\u754c\u4ee5\u589e\u52a0\u6837\u672c\u591a\u6837\u6027\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u6982\u7387\u89d2\u5ea6\u7406\u89e3\u4f18\u5316\u8fc7\u7a0b\uff0c\u63a2\u7d22\u81ea\u7f16\u7801\u5668\u548c\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u7684\u8bad\u7ec3\u76ee\u6807\u4e0e\u6982\u7387\u8fb9\u754c\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u548c\u7b97\u5b50\u5206\u89e3\u65b9\u6cd5\uff0c\u5206\u6790\u591a\u8f93\u51fa\u7f51\u7edc\u4ea7\u751f\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff1b\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u7b97\u5b50\u8ff9\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\uff0c\u57fa\u4e8e\u6838\u8303\u6570\u8bad\u7ec3MDNs\uff1b\u4f7f\u7528\u5185\u79ef\u548c\u8303\u6570\u5b9a\u4e49\u8fb9\u754c\u4ee5\u9632\u6b62\u5e73\u51e1\u89e3\u3002", "result": "\u53d1\u73b0\u81ea\u7f16\u7801\u5668\u76ee\u6807\u7b49\u4ef7\u4e8e\u6700\u5927\u5316\u9ad8\u65af\u7b97\u5b50\u8ff9\uff1b\u63d0\u51fa\u6838\u8303\u6570\u4f5c\u4e3aMDNs\u7684\u6563\u5ea6\u5ea6\u91cf\uff1b\u8bbe\u8ba1\u4e86\u7f16\u7801\u5668-\u6df7\u5408-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u89e3\u7801\u5668\u4e3a\u591a\u8f93\u51fa\u4ee5\u4ea7\u751f\u591a\u4e2a\u4e2d\u5fc3\u3002", "conclusion": "\u4ece\u8d1d\u53f6\u65af\u9ad8\u65af\u89c6\u89d2\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7b97\u5b50\u8ff9\u548c\u6838\u8303\u6570\u65b9\u6cd5\u6539\u8fdb\u4e86\u81ea\u7f16\u7801\u5668\u548c\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u7684\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2511.13026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13026", "abs": "https://arxiv.org/abs/2511.13026", "authors": ["Jiaze Li", "Hao Yin", "Wenhui Tan", "Jingyang Chen", "Boshen Xu", "Yuxun Qu", "Yijing Chen", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan"], "title": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding", "comment": null, "summary": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86REVISOR\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u53cd\u601d\u673a\u5236\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u5fae\u8c03\u6216\u5916\u90e8\u6a21\u578b\u3002", "motivation": "\u7eaf\u6587\u672c\u53cd\u601d\u673a\u5236\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a1) \u4ec5\u53cd\u601d\u6587\u672c\u4fe1\u606f\u4e0d\u8db3\u4ee5\u5904\u7406\u4e30\u5bcc\u7684\u52a8\u6001\u89c6\u89c9\u8f93\u5165\uff1b2) \u7f3a\u4e4f\u8de8\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u53cd\u601d\u4e2d\u5145\u5206\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u3002", "method": "REVISOR\u6846\u67b6\u652f\u6301\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u534f\u4f5c\u53cd\u601d\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u4e86\u53cc\u5f52\u56e0\u89e3\u8026\u5956\u52b1\u673a\u5236(DADR)\u4e0eGRPO\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408\uff0c\u786e\u4fdd\u6a21\u578b\u63a8\u7406\u4e0e\u6240\u9009\u89c6\u9891\u8bc1\u636e\u7684\u56e0\u679c\u5bf9\u9f50\u3002", "result": "\u5728VideoMME\u3001LongVideoBench\u3001MLVU\u548cLVBench\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "REVISOR\u901a\u8fc7\u8de8\u6a21\u6001\u53cd\u601d\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7eaf\u6587\u672c\u53cd\u601d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12951", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12951", "abs": "https://arxiv.org/abs/2511.12951", "authors": ["Ziling Fan", "Ruijia Liang", "Yiwen Hu"], "title": "A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series", "comment": null, "summary": "Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eFEDformer\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u98ce\u9669\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728RMSE\u548cF1\u5206\u6570\u4e0a\u6709\u663e\u8457\u63d0\u5347", "motivation": "\u91d1\u878d\u5e02\u573a\u5177\u6709\u9ad8\u5ea6\u6ce2\u52a8\u6027\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6355\u6349\u957f\u671f\u4f9d\u8d56\u548c\u590d\u6742\u5468\u671f\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5", "method": "\u96c6\u6210\u9891\u7387\u589e\u5f3a\u5206\u89e3\u53d8\u6362\u5668(FEDformer)\u4e0e\u57fa\u4e8e\u6b8b\u5dee\u7684\u5f02\u5e38\u68c0\u6d4b\u5668\u548c\u98ce\u9669\u9884\u6d4b\u5934\uff0c\u5728\u65f6\u57df\u548c\u9891\u57df\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u5206\u89e3\u4fe1\u53f7\u4e3a\u8d8b\u52bf\u548c\u5b63\u8282\u5206\u91cf", "result": "\u5728S&P 500\u3001NASDAQ\u548c\u5e03\u4f26\u7279\u539f\u6cb9\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5RMSE\u964d\u4f4e15.7%\uff0c\u5f02\u5e38\u68c0\u6d4bF1\u5206\u6570\u63d0\u534711.5%", "conclusion": "\u8be5\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u91d1\u878d\u6ce2\u52a8\u6027\uff0c\u4e3a\u5e02\u573a\u5d29\u76d8\u9884\u6d4b\u548c\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf"}}
{"id": "2511.12955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12955", "abs": "https://arxiv.org/abs/2511.12955", "authors": ["Onur Vural", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series", "comment": "This work has been accepted at the 2025 IEEE International Conference on Big Data (IEEE BigData 2025) on October 23, 2025", "summary": "Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684GCTAF\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8de8\u6ce8\u610f\u529b\u5168\u5c40\u4ee4\u724c\u6765\u589e\u5f3a\u957f\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\uff0c\u7528\u4e8e\u89e3\u51b3\u592a\u9633\u8000\u6591\u9884\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u592a\u9633\u8000\u6591\u9884\u6d4b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u2014\u2014\u5f3a\u70c8\u8000\u6591\u4e8b\u4ef6\u76f8\u6bd4\u8f7b\u5fae\u8000\u6591\u4e8b\u4ef6\u975e\u5e38\u7f55\u89c1\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u7684\u5b66\u4e60\u3002", "method": "GCTAF\u67b6\u6784\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8de8\u6ce8\u610f\u529b\u5168\u5c40\u4ee4\u724c\uff0c\u8fd9\u4e9b\u4ee4\u724c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u4e0e\u8f93\u5165\u5e8f\u5217\u4ea4\u4e92\u5e76\u878d\u5408\u56de\u65f6\u95f4\u8868\u793a\u4e2d\uff0c\u80fd\u591f\u8bc6\u522b\u5bf9\u8000\u6591\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u7684\u5168\u5c40\u663e\u8457\u975e\u8fde\u7eed\u65f6\u95f4\u70b9\u3002", "result": "\u5728\u57fa\u51c6\u592a\u9633\u8000\u6591\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cGCTAF\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5f3a\u70c8\u8000\u6591\u5e76\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u6539\u8fdb\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u4e3a\u592a\u9633\u8000\u6591\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6f5c\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.13127", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.13127", "abs": "https://arxiv.org/abs/2511.13127", "authors": ["Zonghao Ying", "Moyang Chen", "Nizhang Li", "Zhiqiang Wang", "Wenxin Zhang", "Quanchen Zou", "Zonglei Jing", "Aishan Liu", "Xianglong Liu"], "title": "VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language", "comment": null, "summary": "Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.", "AI": {"tldr": "VEIL\u662f\u4e00\u4e2a\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u5305\u542b\u4e2d\u6027\u573a\u666f\u951a\u70b9\u3001\u6f5c\u5728\u542c\u89c9\u89e6\u53d1\u5668\u548c\u98ce\u683c\u8c03\u5236\u5668\u7684\u6a21\u5757\u5316\u63d0\u793a\u8bbe\u8ba1\uff0c\u5229\u7528\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5f0f\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u8fdd\u53cd\u5b89\u5168\u7b56\u7565\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u8d8a\u72f1\u653b\u51fb\u901a\u5e38\u901a\u8fc7\u6dfb\u52a0\u660e\u663e\u4e0d\u5b89\u5168\u7684\u5bf9\u6297\u6027\u6270\u52a8\u6765\u5b9e\u73b0\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\u548c\u9632\u5fa1\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u770b\u4f3c\u826f\u6027\u7684\u63d0\u793a\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u8bed\u4e49\u4e0a\u4e0d\u5b89\u5168\u7684\u89c6\u9891\u3002", "method": "\u63d0\u51faVEIL\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u63d0\u793a\u8bbe\u8ba1\uff1a\u4e2d\u6027\u573a\u666f\u951a\u70b9\u63d0\u4f9b\u8868\u9762\u573a\u666f\u63cf\u8ff0\u4fdd\u6301\u5408\u7406\u6027\uff1b\u6f5c\u5728\u542c\u89c9\u89e6\u53d1\u5668\u5229\u7528\u97f3\u9891-\u89c6\u89c9\u5171\u73b0\u5148\u9a8c\uff1b\u98ce\u683c\u8c03\u5236\u5668\u901a\u8fc7\u7535\u5f71\u6307\u4ee4\u589e\u5f3a\u89e6\u53d1\u6548\u679c\u3002\u5c06\u653b\u51fb\u751f\u6210\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u5f15\u5bfc\u641c\u7d22\u7b97\u6cd5\u5e73\u8861\u9690\u853d\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u57287\u4e2a\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u653b\u51fb\u65b9\u6cd5\u6709\u6548\uff0c\u5728\u5546\u4e1a\u6a21\u578b\u4e2d\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u4e8623%\u3002", "conclusion": "VEIL\u6846\u67b6\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u63d0\u793a\uff0c\u53ef\u4ee5\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5f0f\u5b9e\u73b0\u9690\u853d\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5b89\u5168\u9632\u62a4\u7684\u76f2\u70b9\u3002"}}
{"id": "2511.13178", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13178", "abs": "https://arxiv.org/abs/2511.13178", "authors": ["Mingxuan Tian", "Haochen Mu", "Donghong Ding", "Mengjiao Li", "Yuhan Ding", "Jianping Zhao"], "title": "Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach", "comment": null, "summary": "With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7b97\u5b50(PINO)\u65b9\u6cd5\uff0c\u7528\u4e8e\u91d1\u5c5e\u589e\u6750\u5236\u9020\u4e2d\u5b9e\u65f6\u9884\u6d4b\u672a\u676515\u79d2\u7684z\u548cy\u65b9\u5411\u53d8\u5f62\u573a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u503c\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u7684\u95ee\u9898\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u548c\u667a\u80fd\u5236\u9020\u7cfb\u7edf\u7684\u53d1\u5c55\u8feb\u5207\u9700\u8981\u5b9e\u65f6\u53d8\u5f62\u573a\u9884\u6d4b\u6765\u63a7\u5236\u91d1\u5c5e\u589e\u6750\u5236\u9020\u7f3a\u9677\uff0c\u4f46\u4f20\u7edf\u6570\u503c\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8fd0\u884c\u65f6\u95f4\u957f\uff0c\u800c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u63d0\u53d6\u957f\u65f6\u57df\u9884\u6d4b\u7684\u65f6\u7a7a\u7279\u5f81\u4e14\u65e0\u6cd5\u89e3\u8026\u70ed\u529b\u573a\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc-\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(PIDeepONet-RNN)\uff0c\u4f7f\u7528\u5206\u652f\u7f51\u7edc\u5904\u7406\u6e29\u5ea6\u5386\u53f2\uff0c\u4e3b\u5e72\u7f51\u7edc\u7f16\u7801\u53d8\u5f62\u573a\uff0c\u5b9e\u73b0\u70ed\u529b\u54cd\u5e94\u7684\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u70ed\u4f20\u5bfc\u65b9\u7a0b\u4f5c\u4e3a\u8f6f\u7ea6\u675f\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u6a21\u578b\u5728\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6709\u9650\u5143\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6d4b\u8bd5\uff0c\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8bef\u5dee\u7d2f\u79ef\u548c\u65f6\u95f4\u6548\u7387\u3002z\u548cy\u65b9\u5411\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a0.9733mm\u548c0.2049mm\uff0c\u7194\u6c60\u533a\u57df\u8bef\u5dee\u8f83\u9ad8\u4f46\u6c89\u79ef\u533a\u548c\u5173\u952e\u533a\u57df\u68af\u5ea6\u8303\u6570\u8f83\u4f4e\u3002", "conclusion": "PINO\u4ee3\u7406\u6a21\u578b\u5728\u5b9e\u65f6\u957f\u65f6\u57df\u7269\u7406\u573a\u9884\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u7f3a\u9677\u63a7\u5236\uff0c\u5176\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u7684\u57fa\u51fd\u6570\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u3002"}}
{"id": "2511.13223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13223", "abs": "https://arxiv.org/abs/2511.13223", "authors": ["Yuxiang Zhang", "Zhengxu Yu", "Weihang Pan", "Zhongming Jin", "Qiang Fu", "Deng Cai", "Binbin Lin", "Jieping Ye"], "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs", "comment": "Accepted to NeurIPS 2025", "summary": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.", "AI": {"tldr": "TokenSqueeze\u662f\u4e00\u79cd\u65b0\u9896\u7684\u957f\u63a8\u7406\u8def\u5f84\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406LLM\u7684token\u4f7f\u7528\u91cf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6df1\u5ea6\u548c\u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\u6765\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406LLM\u751f\u6210\u7684\u957f\u94fe\u5f0f\u601d\u7ef4\u8f68\u8ff9\u5bfc\u81f4token\u4f7f\u7528\u91cf\u589e\u52a0\uff0c\u5e26\u6765\u66f4\u9ad8\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\u3002\u73b0\u6709\u957f\u8f6c\u77ed\u65b9\u6cd5\u5f80\u5f80\u727a\u7272\u51c6\u786e\u6027\uff0c\u9700\u8981\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4etoken\u6210\u672c\u3002", "method": "1) \u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6df1\u5ea6\uff1a\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u9009\u62e9\u81ea\u751f\u6210\u6837\u672c\uff1b2) \u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\uff1a\u4f18\u5316\u8bed\u8a00\u8868\u8fbe\u800c\u4e0d\u6539\u53d8\u63a8\u7406\u8def\u5f84\uff0c\u4fdd\u6301\u903b\u8f91\u5b8c\u6574\u6027\u540c\u65f6\u63d0\u9ad8\u6e05\u6670\u5ea6\u548c\u7b80\u6d01\u6027\u3002", "result": "DeepSeek-R1-Distill-Qwen-7B\u4f7f\u7528\u8be5\u65b9\u6cd5\u5728MATH500\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8650%\u7684\u5e73\u5747token\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6a21\u578b\u81ea\u751f\u6210\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u77ed\u7b54\u6848\u6570\u636e\u96c6\u3002", "conclusion": "TokenSqueeze\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406LLM\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13204", "abs": "https://arxiv.org/abs/2511.13204", "authors": ["Junhee Lee", "ChaeBeen Bang", "MyoungChul Kim", "MyeongAh Cho"], "title": "RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection", "comment": "Accepted to AAAI 2026", "summary": "Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both \"how\" motion evolves and \"what\" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.", "AI": {"tldr": "RefineVAD\u662f\u4e00\u4e2a\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u5f02\u5e38\u7684\u53cc\u8fc7\u7a0b\u63a8\u7406\uff0c\u8054\u5408\u5229\u7528\u65f6\u95f4\u52a8\u6001\u548c\u8bed\u4e49\u7ed3\u6784\u6765\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u7684\u5f02\u5e38\u4e8b\u4ef6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5f02\u5e38\u4e8b\u4ef6\u89c6\u4e3a\u5355\u4e00\u7c7b\u522b\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u5f02\u5e38\u4e2d\u591a\u6837\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u7279\u6027\u3002\u53d7\u4eba\u7c7b\u611f\u77e5\u5f02\u5e38\u65b9\u5f0f\u7684\u542f\u53d1\uff0c\u9700\u8981\u540c\u65f6\u89e3\u91ca\u65f6\u95f4\u8fd0\u52a8\u6a21\u5f0f\u548c\u4e0d\u540c\u5f02\u5e38\u7c7b\u578b\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u63d0\u51faRefineVAD\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aMoTAR\uff08\u8fd0\u52a8\u611f\u77e5\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u91cd\u6821\u51c6\uff09\u4f30\u8ba1\u8fd0\u52a8\u663e\u8457\u6027\u5e76\u901a\u8fc7\u79fb\u4f4d\u6ce8\u610f\u529b\u548cTransformer\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u7126\u70b9\uff1bCORE\uff08\u7c7b\u522b\u5bfc\u5411\u7ec6\u5316\uff09\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u5c06\u7247\u6bb5\u7ea7\u7279\u5f81\u4e0e\u53ef\u5b66\u4e60\u7c7b\u522b\u539f\u578b\u5bf9\u9f50\uff0c\u6ce8\u5165\u8f6f\u5f02\u5e38\u7c7b\u522b\u5148\u9a8c\u3002", "result": "\u5728WVAD\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RefineVAD\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6574\u5408\u8bed\u4e49\u4e0a\u4e0b\u6587\u4ee5\u5f15\u5bfc\u7279\u5f81\u7ec6\u5316\u671d\u5411\u5f02\u5e38\u76f8\u5173\u6a21\u5f0f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u65f6\u95f4\u52a8\u6001\u548c\u8bed\u4e49\u7ed3\u6784\uff0cRefineVAD\u80fd\u591f\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u5982\u4f55\u6f14\u5316\u4ee5\u53ca\u5b83\u7c7b\u4f3c\u4e8e\u4ec0\u4e48\u8bed\u4e49\u7c7b\u522b\uff0c\u4e3a\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13351", "abs": "https://arxiv.org/abs/2511.13351", "authors": ["Xinlan Wu", "Bin Zhu", "Feng Han", "Pengkun Jiao", "Jingjing Chen"], "title": "Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning", "comment": null, "summary": "Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u98df\u7269\u5b66\u4e60\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53ccLoRA\u67b6\u6784\u548c\u8d28\u91cf\u589e\u5f3a\u4f2a\u91cd\u653e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u98df\u7269\u5206\u6790\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u98df\u7269\u5206\u6790\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9700\u8981\u6602\u8d35\u7684\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u53ccLoRA\u67b6\u6784\uff1a\u4e13\u7528LoRA\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u5e76\u4fdd\u6301\u4e0e\u5148\u524d\u4efb\u52a1\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u6027\uff0c\u534f\u4f5cLoRA\u901a\u8fc7\u4f2a\u91cd\u653e\u6574\u5408\u8de8\u4efb\u52a1\u5171\u4eab\u77e5\u8bc6\u3002\u8d28\u91cf\u589e\u5f3a\u4f2a\u91cd\u653e\u7b56\u7565\u5229\u7528\u81ea\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u51cf\u5c11\u751f\u6210\u6837\u672c\u4e2d\u7684\u5e7b\u89c9\u3002", "result": "\u5728\u7efc\u5408Uni-Food\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u8f7b\u9057\u5fd8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u590d\u6742\u98df\u7269\u4efb\u52a1\u7684\u6709\u6548\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2511.13373", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13373", "abs": "https://arxiv.org/abs/2511.13373", "authors": ["Prakrit Timilsina", "Anuj Nepal", "Rajan Kadel", "Robin Doss"], "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs", "comment": null, "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u53c2\u6570\u7a7a\u95f4\u5408\u5e76\u6280\u672f\uff0c\u53d1\u73b0\u5bf9\u4e8e\u67b6\u6784\u517c\u5bb9\u7684\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7b80\u5355\u7684\u53c2\u6570\u5e73\u5747\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u590d\u6742\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u4e3a\u5206\u5e03\u5f0f\u533b\u7597AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u533b\u7597\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u7684\u6311\u6218\uff1a\u6574\u5408\u8de8\u673a\u6784\u7684\u4e13\u4e1a\u77e5\u8bc6\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3001\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3001\u9632\u6b62\u6a21\u578b\u66f4\u65b0\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u65b9\u6cd5\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u6ce8\u610f\u529b\u5c42\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u52a0\u6743\u63d2\u503c\uff0c\u8bc4\u4f30\u516d\u79cd\u53c2\u6570\u5408\u5e76\u6280\u672f\uff08\u4efb\u52a1\u7b97\u672f\u3001\u7ebf\u6027\u5e73\u5747\u3001DARE-TIES\u3001DELLA\u3001Breadcrumbs\u548c\u5206\u5c42\u65b9\u6cd5\uff09\u3002", "result": "\u67b6\u6784\u517c\u5bb9\u6a21\u578b\u4ece\u7b80\u5355\u5e73\u5747\u65b9\u6cd5\u4e2d\u83b7\u76ca\u663e\u8457\uff0c\u4efb\u52a1\u7b97\u672f\u5728MedQA\u4e0a\u8fbe\u523045.80%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u590d\u6742\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "\u5bf9\u4e8e\u67b6\u6784\u517c\u5bb9\u6a21\u578b\uff0c\u7b80\u5355\u5e73\u5747\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u77e5\u8bc6\u6574\u5408\u57fa\u7ebf\uff0c\u4e3a\u53ef\u6269\u5c55\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.13276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13276", "abs": "https://arxiv.org/abs/2511.13276", "authors": ["Noam Tsfaty", "Avishai Weizman", "Liav Cohen", "Moshe Tshuva", "Yehudit Aperstein"], "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models", "comment": "1 figure, 1 table", "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u89c6\u9891\u7ea7\u76d1\u7763\u68c0\u6d4b\u76d1\u63a7\u89c6\u9891\u4e2d\u7f55\u89c1\u591a\u6837\u5f02\u5e38\u7684\u53cc\u4e3b\u5e72\u6846\u67b6\uff0c\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u8fbe\u523090.7% AUC", "motivation": "\u89e3\u51b3\u5728\u4ec5\u6709\u89c6\u9891\u7ea7\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u76d1\u63a7\u89c6\u9891\u4e2d\u7f55\u89c1\u4e14\u591a\u6837\u5f02\u5e38\u7684\u6311\u6218", "method": "\u53cc\u4e3b\u5e72\u6846\u67b6\u7ed3\u5408\u5377\u79ef\u548ctransformer\u8868\u793a\uff0c\u901a\u8fc7top-k\u6c60\u5316\u6574\u5408\u7279\u5f81", "result": "\u5728UCF-Crime\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8690.7%\u7684AUC\u6027\u80fd", "conclusion": "\u8be5\u53cc\u4e3b\u5e72\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u7f55\u89c1\u5f02\u5e38\uff0c\u4ec5\u9700\u89c6\u9891\u7ea7\u76d1\u7763\u5373\u53ef\u8fbe\u5230\u826f\u597d\u6027\u80fd"}}
{"id": "2511.13419", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.13419", "abs": "https://arxiv.org/abs/2511.13419", "authors": ["Shaheen Mohammed Saleh Ahmed", "Hakan Hakan Guneyli"], "title": "MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction", "comment": null, "summary": "Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data", "AI": {"tldr": "\u63d0\u51fa\u4e86MMWSTM-ADRAN+\u53cc\u6d41\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u5929\u6c14\u72b6\u6001\u8f6c\u6362\u6a21\u578b\u548c\u5f02\u5e38\u9a71\u52a8\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u6781\u7aef\u6c14\u6e29\u4e8b\u4ef6\u7684\u77ed\u671f\u9884\u6d4b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u6781\u7aef\u6c14\u6e29\u4e8b\u4ef6\u5728\u6c14\u5019\u98ce\u9669\u7ba1\u7406\u4e2d\u4ecd\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u5bf9\u7f55\u89c1\u6781\u7aef\u4e8b\u4ef6\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u53cc\u6d41\u67b6\u6784\uff1aMMWSTM\u6d41\u7ed3\u5408\u53cc\u5411LSTM\u548c\u53ef\u5b66\u4e60\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u6355\u6349\u5929\u6c14\u72b6\u6001\u53d8\u5316\uff1bADRAN\u6d41\u96c6\u6210\u53cc\u5411GRU\u3001\u591a\u5934\u81ea\u6ce8\u610f\u529b\u548c\u5f02\u5e38\u653e\u5927\u5c42\u589e\u5f3a\u5bf9\u4f4e\u6982\u7387\u4fe1\u53f7\u7684\u654f\u611f\u6027\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u878d\u5408\u95e8\u81ea\u9002\u5e94\u786e\u5b9a\u5404\u6d41\u8d21\u732e\u3002", "result": "\u6a21\u578b\u901a\u8fc7\u81ea\u5b9a\u4e49\u7684ExtremeWeatherLoss\u51fd\u6570\uff08\u5bf9\u6e29\u5ea6\u5206\u5e03\u4e0a\u4e0b5%\u7684\u8bef\u5dee\u52a0\u6743\uff09\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u589e\u5f3a\u5957\u4ef6\uff08\u6296\u52a8\u3001\u7f29\u653e\u3001\u65f6\u95f4/\u5e45\u5ea6\u626d\u66f2\uff09\u6709\u6548\u4f18\u5316\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u67b6\u6784\u901a\u8fc7\u8026\u5408\u5929\u6c14\u72b6\u6001\u52a8\u6001\u5efa\u6a21\u548c\u5f02\u5e38\u4fe1\u53f7\u589e\u5f3a\u673a\u5236\uff0c\u63d0\u5347\u4e86\u6781\u7aef\u6c14\u6e29\u4e8b\u4ef6\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2511.13399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13399", "abs": "https://arxiv.org/abs/2511.13399", "authors": ["Yuchen Bao", "Yiting Wang", "Wenjian Huang", "Haowei Wang", "Shen Chen", "Taiping Yao", "Shouhong Ding", "Jianguo Zhang"], "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing", "comment": "Accepted by AAAI2026", "summary": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS", "AI": {"tldr": "TripleFDS\u662f\u4e00\u4e2a\u7528\u4e8e\u573a\u666f\u6587\u672c\u7f16\u8f91\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6587\u672c\u6837\u5f0f\u3001\u6587\u672c\u5185\u5bb9\u548c\u80cc\u666f\u4e09\u4e2a\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u4e00\u81f4\u7684\u6587\u672c\u7f16\u8f91\u3002\u8be5\u65b9\u6cd5\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u573a\u666f\u6587\u672c\u7f16\u8f91\u65b9\u6cd5\u5728\u53ef\u7f16\u8f91\u5c5e\u6027\u7684\u89e3\u8026\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u901a\u5e38\u53ea\u80fd\u5904\u7406\u6587\u672c\u5185\u5bb9\u7f16\u8f91\u7b49\u5355\u4e00\u65b9\u9762\uff0c\u9650\u5236\u4e86\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faTripleFDS\u6846\u67b6\u548cSCB Synthesis\u6570\u636e\u96c6\uff0c\u4f7f\u7528SCB Group\u4f5c\u4e3a\u57fa\u672c\u8bad\u7ec3\u5355\u5143\uff0c\u901a\u8fc7\u7ec4\u95f4\u5bf9\u6bd4\u6b63\u5219\u5316\u548c\u7ec4\u5185\u591a\u7279\u5f81\u6b63\u4ea4\u6027\u5b9e\u73b0\u4e09\u7279\u5f81\u89e3\u8026\uff0c\u5728\u5408\u6210\u9636\u6bb5\u8fdb\u884c\u7279\u5f81\u91cd\u6620\u5c04\u4ee5\u9632\u6b62\u91cd\u5efa\u4e2d\u7684\"\u6377\u5f84\"\u73b0\u8c61\u3002", "result": "\u5728125,000\u4e2aSCB Groups\u4e0a\u8bad\u7ec3\u540e\uff0cTripleFDS\u5728\u4e3b\u6d41STE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8644.54\u7684SSIM\u548c93.58%\u7684ACC\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u51c6\u786e\u6027\u3002", "conclusion": "TripleFDS\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u652f\u6301\u66f4\u7075\u6d3b\u7684\u7f16\u8f91\u64cd\u4f5c\uff0c\u5982\u6837\u5f0f\u66ff\u6362\u548c\u80cc\u666f\u8f6c\u79fb\uff0c\u4e3a\u573a\u666f\u6587\u672c\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13701", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13701", "abs": "https://arxiv.org/abs/2511.13701", "authors": ["Gianluigi Pillonetto", "Alberto Giaretta", "Mauro Bisiacco"], "title": "Learning stochasticity: a nonparametric framework for intrinsic noise estimation", "comment": null, "summary": "Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.", "AI": {"tldr": "Trine\u662f\u4e00\u4e2a\u975e\u53c2\u6570\u3001\u57fa\u4e8e\u6838\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63a8\u65ad\u72b6\u6001\u4f9d\u8d56\u7684\u5185\u5728\u566a\u58f0\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7b97\u6cd5\u7ed3\u5408\u89e3\u6790\u53ef\u89e3\u5b50\u95ee\u9898\u548c\u7ed3\u6784\u5316\u6838\u67b6\u6784\u6765\u6355\u83b7\u566a\u58f0\u9a71\u52a8\u7684\u7a81\u53d8\u6ce2\u52a8\u548c\u72b6\u6001\u4f9d\u8d56\u7684\u65b9\u5dee\u5e73\u6ed1\u53d8\u5316\u3002", "motivation": "\u5728\u751f\u7269\u5b66\u548c\u751f\u6001\u5b66\u7b49\u79d1\u5b66\u9886\u57df\u4e2d\uff0c\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u548c\u968f\u673a\u6548\u5e94\u7684\u4e0d\u5b8c\u5168\u77e5\u8bc6\u4f7f\u5f97\u81ea\u4e0b\u800c\u4e0a\u7684\u5efa\u6a21\u65b9\u6cd5\u5f80\u5f80\u65e0\u6548\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u63a7\u5236\u65b9\u7a0b\u7684\u65b9\u6cd5\u3002\u53c2\u6570\u6a21\u578b\u5728\u6ca1\u6709\u5f3a\u5148\u9a8c\u77e5\u8bc6\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u4f30\u8ba1\u5185\u5728\u566a\u58f0\u65f6\uff0c\u800c\u7eb3\u5165\u968f\u673a\u6548\u5e94\u5bf9\u4e8e\u7406\u89e3\u590d\u6742\u7cfb\u7edf\uff08\u5982\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u548c\u4fe1\u53f7\u901a\u8def\uff09\u7684\u52a8\u6001\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002", "method": "Trine\u91c7\u7528\u4e09\u9636\u6bb5\u7b97\u6cd5\uff0c\u7ed3\u5408\u89e3\u6790\u53ef\u89e3\u5b50\u95ee\u9898\u548c\u7ed3\u6784\u5316\u6838\u67b6\u6784\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6355\u83b7\u566a\u58f0\u9a71\u52a8\u7684\u7a81\u53d8\u6ce2\u52a8\u548c\u72b6\u6001\u4f9d\u8d56\u7684\u65b9\u5dee\u5e73\u6ed1\u53d8\u5316\uff0c\u662f\u4e00\u4e2a\u975e\u53c2\u6570\u3001\u57fa\u4e8e\u6838\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u751f\u7269\u548c\u751f\u6001\u7cfb\u7edf\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cTrine\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u53c2\u6570\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u63ed\u793a\u9690\u85cf\u7684\u52a8\u6001\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cTrine\u5b9e\u73b0\u4e86\u4e0eoracle\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8fd9\u4e2aoracle\u53ef\u4ee5\u88ab\u89c6\u4e3a\u80fd\u591f\u76f4\u63a5\u8ddf\u8e2a\u7ec6\u80de\u5185\u5206\u5b50\u6d53\u5ea6\u6216\u53cd\u5e94\u4e8b\u4ef6\u968f\u673a\u6ce2\u52a8\u7684\u7406\u60f3\u5316\u89c2\u5bdf\u8005\u3002", "conclusion": "Trine\u6846\u67b6\u4e3a\u7406\u89e3\u5185\u5728\u566a\u58f0\u5982\u4f55\u5f71\u54cd\u590d\u6742\u7cfb\u7edf\u884c\u4e3a\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u63a7\u5236\u65b9\u7a0b\u5e76\u4f30\u8ba1\u72b6\u6001\u4f9d\u8d56\u7684\u5185\u5728\u566a\u58f0\u3002"}}
{"id": "2511.13587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13587", "abs": "https://arxiv.org/abs/2511.13587", "authors": ["Haotian Dong", "Ye Li", "Rongwei Lu", "Chen Tang", "Shu-Tao Xia", "Zhi Wang"], "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping", "comment": null, "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.", "AI": {"tldr": "\u63d0\u51faVVS\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u9a8c\u8bc1\u8df3\u8fc7\u6765\u52a0\u901f\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\uff0c\u51cf\u5c11\u76ee\u6807\u6a21\u578b\u524d\u5411\u4f20\u9012\u6b21\u65702.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u7684'\u4e00\u6b65\u8349\u7a3f\u3001\u4e00\u6b65\u9a8c\u8bc1'\u8303\u5f0f\u65e0\u6cd5\u76f4\u63a5\u51cf\u5c11\u524d\u5411\u4f20\u9012\u6b21\u6570\uff0c\u9650\u5236\u4e86\u52a0\u901f\u6f5c\u529b\u3002", "method": "\u57fa\u4e8e\u9a8c\u8bc1\u5197\u4f59\u548c\u7279\u5f81\u53ef\u91cd\u7528\u6027\u7684\u89c2\u5bdf\uff0c\u63d0\u51faVVS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u52a8\u6001\u622a\u65ad\u7684\u9a8c\u8bc1\u65e0\u5173\u4ee4\u724c\u9009\u62e9\u5668\u3001\u4ee4\u724c\u7ea7\u7279\u5f81\u7f13\u5b58\u4e0e\u91cd\u7528\u3001\u7ec6\u7c92\u5ea6\u8df3\u8fc7\u6b65\u8c03\u5ea6\u3002", "result": "\u76f8\u6bd4\u539f\u59cb\u81ea\u56de\u5f52\u89e3\u7801\uff0cVVS\u5c06\u76ee\u6807\u6a21\u578b\u524d\u5411\u4f20\u9012\u6b21\u6570\u51cf\u5c112.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "VVS\u63d0\u4f9b\u4e86\u4f18\u4e8e\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\u7684\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\uff0c\u5c55\u793a\u4e86\u91cd\u5851\u63a8\u6d4b\u89e3\u7801\u8303\u5f0f\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.13712", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13712", "abs": "https://arxiv.org/abs/2511.13712", "authors": ["Kiana Vu", "\u0130smet Sel\u00e7uk \u00d6zer", "Phung Lai", "Zheng Wu", "Thilanka Munasinghe", "Jennifer Wei"], "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness", "comment": null, "summary": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53ef\u89e3\u91caAI\u5728\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u91ce\u706b\u9884\u6d4b\u4e3a\u4f8b\uff0c\u4f7f\u7528SHAP\u65b9\u6cd5\u63ed\u793a\u6a21\u578b\u51b3\u7b56\u8def\u5f84\u548c\u6f5c\u5728\u504f\u89c1\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u6781\u7aef\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u4e25\u91cd\u6027\uff0c\u9700\u8981\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\u3002\u867d\u7136AI\u6a21\u578b\u5728\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u9650\u5236\u4e86\u5728\u5b9e\u9645\u51b3\u7b56\u4e2d\u7684\u91c7\u7528\uff0c\u5f71\u54cd\u4e86\u4fe1\u4efb\u5ea6\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "method": "\u4f7f\u7528\u91ce\u706b\u9884\u6d4b\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30\u591a\u79cdAI\u6a21\u578b\uff0c\u5e76\u91c7\u7528SHAP\u65b9\u6cd5\u6765\u63ed\u793a\u5173\u952e\u7279\u5f81\u3001\u51b3\u7b56\u8def\u5f84\u548c\u6a21\u578b\u884c\u4e3a\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\u3002\u63d0\u4f9b\u652f\u6301\u53ef\u89c6\u5316\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5206\u6790\u8868\u660eXAI\u4e0d\u4ec5\u80fd\u6f84\u6e05\u6a21\u578b\u63a8\u7406\uff0c\u8fd8\u80fd\u652f\u6301\u9886\u57df\u4e13\u5bb6\u548c\u54cd\u5e94\u56e2\u961f\u7684\u5173\u952e\u51b3\u7b56\u3002\u53ef\u89c6\u5316\u589e\u5f3a\u4e86\u7279\u5f81\u91cd\u8981\u6027\u548c\u65f6\u7a7a\u6a21\u5f0f\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03AI\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\uff0c\u8fd8\u9700\u8981\u53ef\u89e3\u91ca\u3001\u53ef\u8bbf\u95ee\u4e14\u503c\u5f97\u4fe1\u8d56\uff0c\u8fd9\u5bf9\u4e8e\u707e\u5bb3\u51c6\u5907\u3001\u98ce\u9669\u7f13\u89e3\u548c\u6c14\u5019\u97e7\u6027\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002"}}
