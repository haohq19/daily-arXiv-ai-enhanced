<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Factorized Learning for Temporally Grounded Video-Language Models](https://arxiv.org/abs/2512.24097)
*Wenzheng Zeng,Difei Gao,Mike Zheng Shou,Hwee Tou Ng*

Main category: cs.CV

TL;DR: D²VLM：通过解耦学习和因子化偏好优化，提升视频语言模型在时间定位和文本响应方面的性能


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在事件级感知的时间定位方面存在困难，时间定位和文本响应两个任务通常耦合处理，缺乏清晰的逻辑层次结构，导致次优目标

Method: 提出D²VLM框架，解耦时间定位和文本响应学习，采用"先定位后基于证据回答"范式，引入证据令牌进行证据定位；提出因子化偏好优化(FPO)算法，将概率时间定位建模纳入优化目标；构建合成数据集支持因子化偏好学习

Result: 在多个任务上的实验表明该方法具有明显优势

Conclusion: 通过解耦学习和因子化偏好优化，D²VLM能够更好地处理视频理解中的时间定位和文本响应任务，提升事件级感知能力

Abstract: Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.

</details>


### [2] [Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design](https://arxiv.org/abs/2512.24120)
*Chandini Vysyaraju,Raghuvir Duvvuri,Avi Goyal,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出FSAP（Few-Shot Architecture Prompting）方法，系统研究LLM在计算机视觉架构生成中的示例数量优化，并引入轻量级去重验证技术，在7个视觉基准上生成1900个独特架构。


<details>
  <summary>Details</summary>
Motivation: 自动化神经网络架构设计面临任务多样性和计算约束的挑战。LLM为替代计算密集的NAS提供了有前景的途径，但在计算机视觉领域的架构生成应用尚未得到系统研究，特别是在提示工程和验证策略方面。

Method: 1. 提出Few-Shot Architecture Prompting (FSAP)：系统研究支持示例数量（n=1-6）对LLM生成视觉架构的影响；2. 引入Whitespace-Normalized Hash Validation：轻量级去重方法（<1ms），比AST解析快100倍；3. 基于任务无关的NNGPT/LEMUR框架；4. 在7个计算机视觉基准上进行大规模实验。

Result: 发现n=3个示例在视觉任务中最佳平衡架构多样性和上下文聚焦；生成1900个独特架构；提出数据集平衡评估方法以解决异构视觉任务间的架构比较挑战；验证方法显著提高效率。

Conclusion: 为计算机视觉中基于LLM的架构搜索提供了可操作指南，建立了严格的评估实践，使计算资源有限的研究者更容易进行自动化设计。

Abstract: Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.

</details>


### [3] [Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning](https://arxiv.org/abs/2512.24146)
*Chubin Chen,Sujie Hu,Jiashu Zhu,Meiqi Wu,Jintao Chen,Yanxun Li,Nisha Huang,Chengyu Fang,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: 提出D²-Align框架解决文本到图像扩散模型在人类反馈强化学习中出现的偏好模式崩溃问题，通过方向性解耦奖励信号来保持生成多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈强化学习的文本到图像扩散模型虽然能在自动化奖励指标上获得高分，但会导致偏好模式崩溃——模型收敛到狭窄的高分输出（如单一风格或普遍过曝），严重降低生成多样性。

Method: 提出方向性解耦对齐框架：1）在奖励模型的嵌入空间中学习方向性校正（保持模型冻结）；2）在优化过程中将校正应用于奖励信号，防止模型崩溃到特定模式。

Result: D²-Align在质量和多样性指标上均表现出色，实现了与人类偏好的更好对齐，同时避免了偏好模式崩溃问题。

Conclusion: 通过方向性校正奖励信号，D²-Align有效缓解了偏好模式崩溃问题，在保持生成多样性的同时实现了与人类偏好的更好对齐。

Abstract: Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.

</details>


### [4] [MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation](https://arxiv.org/abs/2512.24243)
*Fuqiang Gu,Yuanke Li,Xianlei Long,Kangping Ji,Chao Chen,Qingyi Gu,Zhenliang Ni*

Main category: cs.CV

TL;DR: MambaSeg：一种新颖的双分支语义分割框架，使用并行Mamba编码器处理RGB图像和事件流，通过双维交互模块（DDIM）在空间和时间维度进行细粒度融合，在DDD17和DSEC数据集上实现SOTA性能并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RGB方法在快速运动、低光照或高动态范围条件下性能下降，事件相机具有高时间分辨率和低延迟但缺乏颜色和纹理。现有多模态融合方法计算昂贵且主要关注空间融合，忽略了事件流的时间动态特性。

Method: 提出MambaSeg双分支语义分割框架，使用并行Mamba编码器分别处理RGB图像和事件流。引入双维交互模块（DDIM），包含跨空间交互模块（CSIM）和跨时间交互模块（CTIM），在空间和时间维度进行细粒度融合，改善跨模态对齐并减少歧义。

Result: 在DDD17和DSEC数据集上的广泛实验表明，MambaSeg实现了最先进的分割性能，同时显著降低了计算成本，展示了其高效、可扩展和鲁棒的多模态感知潜力。

Conclusion: MambaSeg通过并行Mamba编码器和双维交互模块，有效融合RGB和事件数据，在保持高性能的同时降低计算复杂度，为高效、可扩展的鲁棒多模态感知提供了有前景的解决方案。

Abstract: Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.

</details>


### [5] [DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model](https://arxiv.org/abs/2512.24408)
*Bohong Chen,Haiyang Liu*

Main category: cs.CV

TL;DR: DyStream：基于流匹配的自回归模型，实时生成说话人-听者双向对话视频，延迟低于100ms，实现高质量唇部同步


<details>
  <summary>Details</summary>
Motivation: 现有基于分块的方法需要完整的非因果上下文窗口，导致显著延迟，无法满足实时对话中听者即时非语言反馈的需求

Method: 采用流友好的自回归框架，结合流匹配头部进行概率建模；提出因果编码器增强方法，通过前瞻模块引入短未来上下文（60ms）以提升质量同时保持低延迟

Result: 每帧生成时间34ms，系统总延迟低于100ms；在HDTF数据集上获得离线8.13和在线7.61的LipSync Confidence分数，达到最先进的唇部同步质量

Conclusion: DyStream通过流匹配自回归框架和因果编码器增强，实现了实时、高质量的说话人-听者对话视频生成，显著优于其他因果策略

Abstract: Generating realistic, dyadic talking head video requires ultra-low latency. Existing chunk-based methods require full non-causal context windows, introducing significant delays. This high latency critically prevents the immediate, non-verbal feedback required for a realistic listener. To address this, we present DyStream, a flow matching-based autoregressive model that could generate video in real-time from both speaker and listener audio. Our method contains two key designs: (1) we adopt a stream-friendly autoregressive framework with flow-matching heads for probabilistic modeling, and (2) We propose a causal encoder enhanced by a lookahead module to incorporate short future context (e.g., 60 ms) to improve quality while maintaining low latency. Our analysis shows this simple-and-effective method significantly surpass alternative causal strategies, including distillation and generative encoder. Extensive experiments show that DyStream could generate video within 34 ms per frame, guaranteeing the entire system latency remains under 100 ms. Besides, it achieves state-of-the-art lip-sync quality, with offline and online LipSync Confidence scores of 8.13 and 7.61 on HDTF, respectively. The model, weights and codes are available.

</details>


### [6] [From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation](https://arxiv.org/abs/2512.24639)
*Siyang Wang,Hanting Li,Wei Li,Jie Hu,Xinghao Chen,Feng Zhao*

Main category: cs.CV

TL;DR: RadAR提出了一种基于径向拓扑的并行自回归视觉生成框架，通过环状并行预测和嵌套注意力机制，在保持生成质量的同时大幅提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型在视觉生成中采用顺序解码机制，导致推理效率低下。视觉token具有强烈的局部依赖性和空间相关性，但标准光栅扫描解码顺序未能充分利用这一特性。

Method: 采用径向拓扑组织生成过程：选择初始token作为起点，将所有其他token按空间距离分组到多个同心环中。从内到外进行环状生成，实现同一环内所有token的并行预测。引入嵌套注意力机制动态修正不一致的预测输出。

Result: RadAR框架显著提高了生成效率，同时保持了自回归模型的表示能力。通过并行预测和动态输出修正，有效缓解了错误累积和模型崩溃问题。

Conclusion: RadAR通过创新的径向并行预测架构，成功解决了传统自回归视觉生成的低效问题，为高效视觉生成提供了新的解决方案。

Abstract: Inspired by the remarkable success of autoregressive models in language modeling, this paradigm has been widely adopted in visual generation. However, the sequential token-by-token decoding mechanism inherent in traditional autoregressive models leads to low inference efficiency.In this paper, we propose RadAR, an efficient and parallelizable framework designed to accelerate autoregressive visual generation while preserving its representational capacity. Our approach is motivated by the observation that visual tokens exhibit strong local dependencies and spatial correlations with their neighbors--a property not fully exploited in standard raster-scan decoding orders. Specifically, we organize the generation process around a radial topology: an initial token is selected as the starting point, and all other tokens are systematically grouped into multiple concentric rings according to their spatial distances from this center. Generation then proceeds in a ring-wise manner, from inner to outer regions, enabling the parallel prediction of all tokens within the same ring. This design not only preserves the structural locality and spatial coherence of visual scenes but also substantially increases parallelization. Furthermore, to address the risk of inconsistent predictions arising from simultaneous token generation with limited context, we introduce a nested attention mechanism. This mechanism dynamically refines implausible outputs during the forward pass, thereby mitigating error accumulation and preventing model collapse. By integrating radial parallel prediction with dynamic output correction, RadAR significantly improves generation efficiency.

</details>


### [7] [EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation](https://arxiv.org/abs/2512.24731)
*Bingxuan Li,Yiming Cui,Yicheng He,Yiwei Wang,Shu Zhang,Longyin Wen,Yulei Niu*

Main category: cs.CV

TL;DR: EchoFoley提出视频声音生成新任务，通过符号化事件表示实现细粒度控制，构建大规模数据集，并开发基于慢快思考策略的生成框架，显著提升可控性和音质。


<details>
  <summary>Details</summary>
Motivation: 当前视频-文本到音频（VT2A）方法存在三个关键局限：1）视觉与文本条件不平衡导致视觉主导；2）缺乏细粒度可控生成的具体定义；3）指令理解能力弱，现有数据集依赖简短分类标签。

Method: 提出EchoFoley任务，使用符号化声音事件表示（指定何时、什么、如何产生声音），构建EchoFoley-6k大规模专家标注数据集，开发EchoVidia框架采用慢快思考策略进行以声音事件为中心的智能生成。

Result: EchoVidia在可控性上超越现有VT2A模型40.7%，在感知质量上提升12.5%。

Conclusion: EchoFoley任务和EchoVidia框架有效解决了现有VT2A方法的局限，通过细粒度符号化控制和智能生成策略显著提升了视频声音生成的质量和可控性。

Abstract: Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.

</details>


### [8] [Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection](https://arxiv.org/abs/2512.24922)
*Bartłomiej Olber,Jakub Winter,Paweł Wawrzyński,Andrii Gamalii,Daniel Górniak,Marcin Łojek,Robert Nowak,Krystian Radlak*

Main category: cs.CV

TL;DR: 提出基于神经元激活模式的LiDAR域自适应方法，仅需标注目标域中少量代表性样本即可达到SOTA性能，结合持续学习技术防止权重漂移。


<details>
  <summary>Details</summary>
Motivation: 3D目标检测器在自动驾驶感知系统中至关重要，但在不同地理区域（如美国、亚洲、欧洲）之间存在显著的域泛化问题，导致模型性能下降。

Method: 基于神经元激活模式选择目标域中少量代表性且多样化的样本进行标注，结合受持续学习启发的后训练技术防止原始模型权重漂移。

Result: 该方法在域自适应任务中超越了线性探测和现有SOTA域自适应技术，且仅需极小的标注成本。

Conclusion: 通过神经元激活模式选择关键样本并配合持续学习技术，能够以最小标注成本实现高效的LiDAR域自适应，提升3D目标检测器在不同地理区域的泛化能力。

Abstract: 3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [HINTS: Extraction of Human Insights from Time-Series Without External Sources](https://arxiv.org/abs/2512.23755)
*Sheo Yon Jhin,Noseong Park*

Main category: cs.LG

TL;DR: HINTS是一个自监督学习框架，通过从时间序列残差中内生提取人类因素（社会影响、记忆和偏见），无需外部数据，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 人类决策、情感和集体心理是影响金融经济系统时间动态的复杂因素。现有方法依赖外部数据（新闻、社交媒体）来捕捉这些因素，但存在数据依赖成本高（财务、计算、实践）的问题。

Method: 提出HINTS自监督学习框架，利用Friedkin-Johnsen意见动力学模型作为结构归纳偏置，从时间序列残差中内生提取潜在人类因素（社会影响、记忆、偏见模式），并将提取的因素作为注意力图集成到最先进的骨干模型中。

Result: 在9个真实世界和基准数据集上的实验表明，HINTS能持续提高预测准确性。案例研究和消融研究验证了HINTS的可解释性，显示提取的因素与现实世界事件有强语义对齐。

Conclusion: HINTS无需外部数据就能有效提取人类因素，不仅提高了预测性能，还具有可解释性和实际应用价值。

Abstract: Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.

</details>


### [10] [FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading](https://arxiv.org/abs/2512.23773)
*Molei Qin,Xinyu Cai,Yewen Li,Haochong Xia,Chuqiao Zong,Shuo Sun,Xinrun Wang,Bo An*

Main category: cs.LG

TL;DR: FineFT提出了一种三阶段集成强化学习框架，专门针对高杠杆期货交易，通过选择性更新、盈利能力筛选和VAE引导的风险管理，在保持高收益的同时显著降低风险。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要针对现货市场，无法直接应用于高杠杆期货交易，面临两大挑战：1）高杠杆放大奖励波动，导致训练不稳定难以收敛；2）缺乏对能力边界的自我认知，遇到新市场状态（如黑天鹅事件）时面临重大损失风险。

Method: 提出三阶段集成RL框架：阶段I通过集成TD误差选择性更新Q学习器以提高收敛性；阶段II基于盈利能力筛选Q学习器，并训练VAE识别学习器的能力边界；阶段III根据训练好的VAE指导，从筛选后的集成策略和保守策略中选择，以维持盈利能力并降低新市场状态风险。

Result: 在高频交易环境下对加密货币期货进行5倍杠杆实验，FineFT在6个金融指标上优于12个SOTA基线，风险降低超过40%的同时获得优于第二名的盈利能力。可视化显示不同智能体专注于不同市场动态，消融研究证实VAE路由有效降低最大回撤，选择性更新改善收敛和性能。

Conclusion: FineFT成功解决了高杠杆期货交易中的训练不稳定和风险管理问题，通过集成学习、选择性更新和VAE引导的能力边界识别，实现了在保持高收益的同时显著降低风险的平衡。

Abstract: Futures are contracts obligating the exchange of an asset at a predetermined date and price, notable for their high leverage and liquidity and, therefore, thrive in the Crypto market. RL has been widely applied in various quantitative tasks. However, most methods focus on the spot and could not be directly applied to the futures market with high leverage because of 2 challenges. First, high leverage amplifies reward fluctuations, making training stochastic and difficult to converge. Second, prior works lacked self-awareness of capability boundaries, exposing them to the risk of significant loss when encountering new market state (e.g.,a black swan event like COVID-19). To tackle these challenges, we propose the Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading (FineFT), a novel three-stage ensemble RL framework with stable training and proper risk management. In stage I, ensemble Q learners are selectively updated by ensemble TD errors to improve convergence. In stage II, we filter the Q-learners based on their profitabilities and train VAEs on market states to identify the capability boundaries of the learners. In stage III, we choose from the filtered ensemble and a conservative policy, guided by trained VAEs, to maintain profitability and mitigate risk with new market states. Through extensive experiments on crypto futures in a high-frequency trading environment with high fidelity and 5x leverage, we demonstrate that FineFT outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by more than 40% while achieving superior profitability compared to the runner-up. Visualization of the selective update mechanism shows that different agents specialize in distinct market dynamics, and ablation studies certify routing with VAEs reduces maximum drawdown effectively, and selective update improves convergence and performance.

</details>


### [11] [GARDO: Reinforcing Diffusion Models without Reward Hacking](https://arxiv.org/abs/2512.24138)
*Haoran He,Yuxiao Ye,Jie Liu,Jiajun Liang,Zhiyong Wang,Ziyang Yuan,Xintao Wang,Hangyu Mao,Pengfei Wan,Ling Pan*

Main category: cs.LG

TL;DR: GARDO框架通过选择性正则化、自适应参考模型更新和多样性奖励增强，解决扩散模型RL微调中的奖励黑客、探索不足和模式崩溃问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型通过在线强化学习微调可提升文本-图像对齐，但代理奖励与真实目标不匹配会导致奖励黑客问题（代理分数上升但真实图像质量下降、多样性崩溃）。现有正则化方法使用次优参考策略，会损害样本效率和探索能力。

Method: 提出GARDO框架：1）选择性正则化：仅对高不确定性样本施加惩罚；2）自适应正则化：定期更新参考模型以匹配在线策略能力；3）多样性感知优化：对高质量且高多样性的样本增强奖励，鼓励模式覆盖。

Result: 在多种代理奖励和未见指标上的实验表明，GARDO能有效缓解奖励黑客、增强生成多样性，同时不牺牲样本效率或探索能力，展现出有效性和鲁棒性。

Conclusion: GARDO通过选择性正则化、自适应参考模型更新和多样性奖励增强，解决了扩散模型RL微调中奖励黑客、探索不足和模式崩溃的竞争需求，为视觉任务强化学习提供了通用框架。

Abstract: Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.

</details>


### [12] [Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning](https://arxiv.org/abs/2512.24404)
*Soham Pahari,M. Srinivas*

Main category: cs.LG

TL;DR: ViReLoc框架通过视觉推理进行空间规划和定位，无需GPS数据，在导航和定位任务中表现优于基于文本的推理方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理系统主要依赖文本信息进行推理，限制了其在空间任务（如视觉导航和地理定位）中的有效性。文本推理难以理解空间依赖和几何关系，需要一种基于视觉的推理范式。

Method: 提出Geo-Consistent Visual Planning范式，开发ViReLoc框架，通过视觉表示进行规划和定位。方法包括：学习空间依赖和几何关系，在视觉域进行逐步推理，使用强化学习目标优化，结合对比学习和自适应特征交互来对齐跨视角并减少视角差异。

Result: 在多种导航和定位场景中的实验显示，ViReLoc在空间推理准确性和跨视角检索性能方面持续改进，证明了视觉推理在导航和定位任务中的有效性。

Conclusion: 视觉推理是导航和定位的强大补充方法，可以在无需实时GPS数据的情况下执行任务，提供更安全的导航解决方案，为空间智能开辟了新方向。

Abstract: Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.

</details>


### [13] [Scaling Open-Ended Reasoning to Predict the Future](https://arxiv.org/abs/2512.25070)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.LG

TL;DR: 训练语言模型进行开放式预测，使用自动化方法从新闻生成预测问题，开发OpenForecaster 8B模型，在准确性、校准和一致性方面表现优异，匹配更大规模的专有模型。


<details>
  <summary>Details</summary>
Motivation: 高风险决策需要在不确定性下对未来进行推理，需要开发能够进行开放式预测的语言模型系统。

Method: 使用自动化方法从日常新闻中合成预测问题，创建OpenForesight数据集，训练Qwen3思维模型，采用离线新闻语料防止信息泄露，结合检索和改进的强化学习奖励函数。

Result: OpenForecaster 8B模型在2025年5-8月的测试中，在准确性、校准和一致性方面表现优异，匹配更大规模的专有模型，且校准改进能泛化到其他基准测试。

Conclusion: 通过自动化数据生成和专门训练，可以开发出高性能的预测模型，开源所有模型、代码和数据以促进语言模型预测研究。

Abstract: High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自我进化代理框架，通过将LLM集成到"计划-执行-总结"认知范式中，显著提高了进化效率，在算法发现和机器学习管道优化中优于现有基线60%。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法在从静态LLM向自我改进代理过渡时存在结构化推理不足的问题，现有方法在高维代码空间中容易过早收敛和探索效率低下。

Method: 提出LoongFlow框架，将LLM集成到"计划-执行-总结"认知范式中，采用混合进化记忆系统结合多岛模型、MAP-Elites和自适应玻尔兹曼选择，平衡探索与利用。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow比OpenEvolve、ShinkaEvolve等领先基线在进化效率上提升高达60%，同时发现更优解决方案。

Conclusion: LoongFlow在自主科学发现方面迈出重要一步，能够以更低的计算成本生成专家级解决方案，标志着从静态LLM向自我进化代理的重要进展。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning](https://arxiv.org/abs/2512.23765)
*Tiancheng Su,Meicong Zhang,Guoxiu He*

Main category: cs.CL

TL;DR: EASD提出了一种无需训练的推测解码增强方法，通过动态熵惩罚机制，在模型不确定性高时拒绝候选token，让目标模型重新采样，从而可能超越目标模型自身性能。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码中，草稿模型与目标模型过度对齐，限制了性能只能达到目标模型的水平。需要一种方法能够突破这一限制，让推测解码有可能超越目标模型本身的性能。

Method: 在标准推测解码基础上，引入动态熵惩罚机制：在每个解码步骤，利用采样分布的熵量化模型不确定性。当两个模型都表现出高熵且它们的top-N预测有显著重叠时，拒绝对应token，让目标LLM重新采样，防止低置信度错误传播。

Result: 在多个推理基准测试中，EASD始终优于现有推测解码方法，并且在大多数情况下超越了目标LLM本身的性能。同时证明了EASD的效率与标准推测解码相当。

Conclusion: EASD通过熵感知的动态惩罚机制，不仅保持了推测解码的效率优势，还突破了传统推测解码的性能上限，实现了可能超越目标模型性能的推理加速。

Abstract: Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.

</details>


### [16] [Modeling Language as a Sequence of Thoughts](https://arxiv.org/abs/2512.25026)
*Nasim Borazjanizadeh,James McClelland*

Main category: cs.CL

TL;DR: 提出Thought Gestalt模型，一种双层次抽象（token和句子级"思想"状态）的循环Transformer，通过记忆机制提升语言建模的全局一致性和数据效率


<details>
  <summary>Details</summary>
Motivation: 传统Transformer语言模型主要依赖表层共现统计，缺乏对实体和事件的全局一致潜在表示，导致关系方向脆弱（如逆转诅咒）、上下文错误和数据效率低下。而人类认知科学表明，人类理解语言时会将其转换为紧凑的事件式表示并持久记忆

Method: 提出Thought Gestalt模型：1）双层次抽象建模（token和句子级"思想"状态）；2）循环Transformer结构，逐句生成token时交叉关注先前句子表示的记忆；3）使用相同参数集生成token和句子表示，通过单一目标（下一个token交叉熵）训练；4）保留写入记忆的句子表示计算图，使未来token损失的梯度通过交叉注意力反向传播优化早期句子向量生成参数

Result: 1）在扩展实验中，TG相比匹配的GPT-2和其他基线模型持续提升效率，缩放拟合表明GPT-2需要多5-8%的数据和33-42%的参数才能达到TG的损失水平；2）在父子关系逆转诅咒探测任务上，TG减少了关系方向泛化错误

Conclusion: Thought Gestalt模型通过引入类似人类认知的句子级"思想"状态记忆机制，有效提升了语言模型的全局一致性、数据效率和关系方向泛化能力，为构建更稳健的语言模型提供了新思路

Abstract: Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level "thought" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.

</details>
