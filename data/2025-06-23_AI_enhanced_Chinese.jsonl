{"id": "2506.15734", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15734", "abs": "https://arxiv.org/abs/2506.15734", "authors": ["Peiyuan Tang", "Haojie Xin", "Xiaodong Zhang", "Jun Sun", "Qin Xia", "Zijiang Yang"], "title": "The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models", "comment": "23 pages, 10 figures", "summary": "As Vision-Language Models (VLMs) demonstrate increasing capabilities across\nreal-world applications such as code generation and chatbot assistance,\nensuring their safety has become paramount. Unlike traditional Large Language\nModels (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,\nallowing adversaries to modify visual or textual inputs to bypass safety\nguardrails and trigger the generation of harmful content. Through systematic\nanalysis of VLM behavior under attack, we identify a novel phenomenon termed\n``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs\nmay initially be compromised to produce harmful content, but eventually\nrecognize the associated risks and attempt to self-correct. This pattern\nsuggests that VLMs retain their underlying safety awareness but experience a\ntemporal delay in their activation. Building on this insight, we hypothesize\nthat VLMs' safety awareness can be proactively reactivated through carefully\ndesigned prompts. To this end, we introduce ``The Safety Reminder'', a soft\nprompt tuning approach that optimizes learnable prompt tokens, which are\nperiodically injected during the text generation process to enhance safety\nawareness, effectively preventing harmful content generation. Additionally, our\nsafety reminder only activates when harmful content is detected, leaving normal\nconversations unaffected and preserving the model's performance on benign\ntasks. Through comprehensive evaluation across three established safety\nbenchmarks and one adversarial attacks, we demonstrate that our approach\nsignificantly reduces attack success rates while maintaining model utility,\noffering a practical solution for deploying safer VLMs in real-world\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5b89\u5168\u63d0\u9192\u201d\u7684\u8f6f\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u6807\u8bb0\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5b89\u5168\u6027\uff0c\u9632\u6b62\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u80fd\u529b\u589e\u5f3a\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002VLM\u56e0\u5176\u591a\u6a21\u6001\u7279\u6027\u9762\u4e34\u72ec\u7279\u7684\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u4fee\u6539\u89c6\u89c9\u6216\u6587\u672c\u8f93\u5165\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\uff0c\u89e6\u53d1\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790VLM\u5728\u653b\u51fb\u4e0b\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5ef6\u8fdf\u5b89\u5168\u610f\u8bc6\u201d\u7684\u73b0\u8c61\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5b89\u5168\u63d0\u9192\u201d\u7684\u8f6f\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u6807\u8bb0\uff0c\u5728\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9a\u671f\u6ce8\u5165\u4ee5\u589e\u5f3a\u5b89\u5168\u610f\u8bc6\u3002", "result": "\u5728\u4e09\u4e2a\u5b89\u5168\u57fa\u51c6\u548c\u4e00\u4e2a\u5bf9\u6297\u653b\u51fb\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u5728\u826f\u6027\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u201c\u5b89\u5168\u63d0\u9192\u201d\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\u66f4\u5b89\u5168\u7684VLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15852", "abs": "https://arxiv.org/abs/2506.15852", "authors": ["Dominic Akt", "Marco Peer", "Florian Kleber"], "title": "Assessing the impact of Binarization for Writer Identification in Greek Papyrus", "comment": "Accepted for publication for AIROV 2025", "summary": "This paper tackles the task of writer identification for Greek papyri. A\ncommon preprocessing step in writer identification pipelines is image\nbinarization, which prevents the model from learning background features. This\nis challenging in historical documents, in our case Greek papyri, as background\nis often non-uniform, fragmented, and discolored with visible fiber structures.\nWe compare traditional binarization methods to state-of-the-art Deep Learning\n(DL) models, evaluating the impact of binarization quality on subsequent writer\nidentification performance. DL models are trained with and without a custom\ndata augmentation technique, as well as different model selection criteria are\napplied. The performance of these binarization methods, is then systematically\nevaluated on the DIBCO 2019 dataset. The impact of binarization on writer\nidentification is subsequently evaluated using a state-of-the-art approach for\nwriter identification. The results of this analysis highlight the influence of\ndata augmentation for DL methods. Furthermore, findings indicate a strong\ncorrelation between binarization effectiveness on papyri documents of DIBCO\n2019 and downstream writer identification performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e0c\u814a\u7eb8\u838e\u8349\u6587\u732e\u7684\u4f5c\u8005\u8bc6\u522b\u4efb\u52a1\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u56fe\u50cf\u4e8c\u503c\u5316\u9884\u5904\u7406\u5bf9\u8bc6\u522b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u5386\u53f2\u6587\u732e\uff08\u5982\u5e0c\u814a\u7eb8\u838e\u8349\uff09\u80cc\u666f\u590d\u6742\uff08\u4e0d\u5747\u5300\u3001\u788e\u7247\u5316\u3001\u892a\u8272\u4e14\u6709\u7ea4\u7ef4\u7ed3\u6784\uff09\uff0c\u4f20\u7edf\u4e8c\u503c\u5316\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u63a2\u7d22\u66f4\u4f18\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u4f5c\u8005\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u6bd4\u8f83\u4f20\u7edf\u4e8c\u503c\u5316\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a\u6280\u672f\u548c\u4e0d\u540c\u6a21\u578b\u9009\u62e9\u6807\u51c6\uff0c\u5e76\u5728DIBCO 2019\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6570\u636e\u589e\u5f3a\u5bf9\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u6709\u6548\uff0c\u4e14\u4e8c\u503c\u5316\u8d28\u91cf\u4e0e\u540e\u7eed\u4f5c\u8005\u8bc6\u522b\u6027\u80fd\u5f3a\u76f8\u5173\u3002", "conclusion": "\u4e8c\u503c\u5316\u8d28\u91cf\u5bf9\u4f5c\u8005\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2506.15911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15911", "abs": "https://arxiv.org/abs/2506.15911", "authors": ["Mohammad Amaan Sayeed", "Mohammed Talha Alam", "Raza Imam", "Shahab Saquib Sohail", "Amir Hussain"], "title": "From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents", "comment": "Under-review at the 4th Muslims in Machine Learning (MusIML) Workshop\n  (ICML-25)", "summary": "Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the\nProphetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and\nholistic therapies, yet remain inaccessible to many and underutilized in modern\nAI systems. Existing language-model benchmarks focus narrowly on factual recall\nor user preference, leaving a gap in validating culturally grounded medical\nguidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that\naligns 30 carefully curated Prophetic-medicine questions with human-verified\nremedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three\nconfigurations: direct generation, retrieval-augmented generation, and a\nscientific self-critique filter. Each answer is then assessed by a secondary\nLLM serving as an agentic judge, yielding a single 3C3H quality score.\nRetrieval improves factual accuracy by 13%, while the agentic prompt adds\nanother 10% improvement through deeper mechanistic insight and safety\nconsiderations. Our results demonstrate that blending classical Islamic texts\nwith retrieval and self-evaluation enables reliable, culturally sensitive\nmedical question-answering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u7ba1\u9053Tibbe-AG\uff0c\u7528\u4e8e\u9a8c\u8bc1\u57fa\u4e8e\u4f0a\u65af\u5170\u533b\u5b66\u6587\u672c\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u81ea\u6211\u8bc4\u4f30\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u3002", "motivation": "\u4f0a\u65af\u5170\u533b\u5b66\u6587\u672c\u5982\u300a\u533b\u5178\u300b\u548c\u300a\u5148\u77e5\u533b\u5b66\u300b\u8574\u542b\u4e30\u5bcc\u7684\u9884\u9632\u548c\u6574\u4f53\u7597\u6cd5\uff0c\u4f46\u672a\u88ab\u73b0\u4ee3AI\u7cfb\u7edf\u5145\u5206\u5229\u7528\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u9a8c\u8bc1\u5176\u6587\u5316\u80cc\u666f\u4e0b\u7684\u533b\u5b66\u6307\u5bfc\u3002", "method": "\u63d0\u51faTibbe-AG\u8bc4\u4f30\u7ba1\u9053\uff0c\u7ed3\u540830\u4e2a\u7cbe\u9009\u95ee\u9898\u3001\u4eba\u7c7b\u9a8c\u8bc1\u7597\u6cd5\uff0c\u6d4b\u8bd5\u4e09\u79cdLLM\uff08LLaMA-3\u3001Mistral-7B\u3001Qwen2-7B\uff09\u5728\u76f4\u63a5\u751f\u6210\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u79d1\u5b66\u81ea\u8bc4\u8fc7\u6ee4\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u68c0\u7d22\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u602713%\uff0c\u81ea\u8bc4\u63d0\u793a\u901a\u8fc7\u673a\u5236\u6d1e\u5bdf\u548c\u5b89\u5168\u6027\u8003\u8651\u518d\u63d0\u534710%\u3002", "conclusion": "\u7ed3\u5408\u53e4\u5178\u4f0a\u65af\u5170\u6587\u672c\u3001\u68c0\u7d22\u548c\u81ea\u6211\u8bc4\u4f30\uff0c\u53ef\u5b9e\u73b0\u53ef\u9760\u4e14\u6587\u5316\u654f\u611f\u7684\u533b\u5b66\u95ee\u7b54\u3002"}}
{"id": "2506.15937", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.15937", "abs": "https://arxiv.org/abs/2506.15937", "authors": ["Yosub Shin", "Igor Molybog"], "title": "Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization", "comment": null, "summary": "Video synchronization-aligning multiple video streams capturing the same\nevent from different angles-is crucial for applications such as reality TV show\nproduction, sports analysis, surveillance, and autonomous systems. Prior work\nhas heavily relied on audio cues or specific visual events, limiting\napplicability in diverse settings where such signals may be unreliable or\nabsent. Additionally, existing benchmarks for video synchronization lack\ngenerality and reproducibility, restricting progress in the field. In this\nwork, we introduce VideoSync, a video synchronization framework that operates\nindependently of specific feature extraction methods, such as human pose\nestimation, enabling broader applicability across different content types. We\nevaluate our system on newly composed datasets covering single-human,\nmulti-human, and non-human scenarios, providing both the methodology and code\nfor dataset creation to establish reproducible benchmarks. Our analysis reveals\nbiases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,\nleading to inflated performance claims. We correct these biases and propose a\nmore rigorous evaluation framework, demonstrating that VideoSync outperforms\nexisting approaches, including SeSyn-Net, under fair experimental conditions.\nAdditionally, we explore various synchronization offset prediction methods,\nidentifying a convolutional neural network (CNN)-based model as the most\neffective. Our findings advance video synchronization beyond domain-specific\nconstraints, making it more generalizable and robust for real-world\napplications.", "AI": {"tldr": "VideoSync\u662f\u4e00\u4e2a\u72ec\u7acb\u4e8e\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u7684\u89c6\u9891\u540c\u6b65\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u540c\u6b65\u65b9\u6cd5\u4f9d\u8d56\u97f3\u9891\u6216\u7279\u5b9a\u89c6\u89c9\u4e8b\u4ef6\uff0c\u9002\u7528\u6027\u53d7\u9650\uff0c\u4e14\u7f3a\u4e4f\u901a\u7528\u548c\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faVideoSync\u6846\u67b6\uff0c\u4e0d\u4f9d\u8d56\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7ea0\u6b63\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u504f\u89c1\u3002", "result": "VideoSync\u5728\u516c\u5e73\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982SeSyn-Net\uff09\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8eCNN\u7684\u6a21\u578b\u6700\u6709\u6548\u3002", "conclusion": "VideoSync\u63d0\u5347\u4e86\u89c6\u9891\u540c\u6b65\u7684\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u5e94\u7528\u3002"}}
{"id": "2506.15890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15890", "abs": "https://arxiv.org/abs/2506.15890", "authors": ["Thomas Manzini", "Priyankari Perali", "Robin R. Murphy", "David Merrick"], "title": "Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene", "comment": "6 pages, 5 Figures, 1 Table", "summary": "This paper details four principal challenges encountered with machine\nlearning (ML) damage assessment using small uncrewed aerial systems (sUAS) at\nHurricanes Debby and Helene that prevented, degraded, or delayed the delivery\nof data products during operations and suggests three research directions for\nfuture real-world deployments. The presence of these challenges is not\nsurprising given that a review of the literature considering both datasets and\nproposed ML models suggests this is the first sUAS-based ML system for disaster\ndamage assessment actually deployed as a part of real-world operations. The\nsUAS-based ML system was applied by the State of Florida to Hurricanes Helene\n(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne\nsUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a\nWintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial\nimagery of inland flood damage resulting from post-tropical remnants of\nHurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing\nfurther insights into the advantages and limitations of sUAS for disaster\nresponse. The four challenges (variationin spatial resolution of input imagery,\nspatial misalignment between imagery and geospatial data, wireless\nconnectivity, and data product format) lead to three recommendations that\nspecify research needed to improve ML model capabilities to accommodate the\nwide variation of potential spatial resolutions used in practice, handle\nspatial misalignment, and minimize the dependency on wireless connectivity.\nThese recommendations are expected to improve the effective operational use of\nsUAS and sUAS-based ML damage assessment systems for disaster response.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u5728\u98d3\u98ceDebby\u548cHelene\u4e2d\u4f7f\u7528\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\uff08sUAS\uff09\u8fdb\u884c\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u635f\u5bb3\u8bc4\u4f30\u65f6\u9047\u5230\u7684\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u4e2dsUAS\u548cML\u7cfb\u7edf\u5728\u707e\u5bb3\u635f\u5bb3\u8bc4\u4f30\u4e2d\u7684\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u98d3\u98ceDebby\u548cHelene\u4e2d\u6536\u96c6\u7684sUAS\u5f71\u50cf\u6570\u636e\uff0c\u4ee5\u53ca\u5bbe\u5915\u6cd5\u5c3c\u4e9a\u5dde\u98d3\u98ceDebby\u540e\u7684\u6709\u4eba\u673a\u5f71\u50cf\u6570\u636e\uff0c\u8bc4\u4f30ML\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8f93\u5165\u5f71\u50cf\u7a7a\u95f4\u5206\u8fa8\u7387\u53d8\u5316\u3001\u5f71\u50cf\u4e0e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u4e0d\u5bf9\u9f50\u3001\u65e0\u7ebf\u8fde\u63a5\u95ee\u9898\u548c\u6570\u636e\u4ea7\u54c1\u683c\u5f0f\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u6539\u8fdbML\u6a21\u578b\u7684\u5b9e\u9645\u64cd\u4f5c\u80fd\u529b\uff0c\u589e\u5f3asUAS\u5728\u707e\u5bb3\u54cd\u5e94\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.16082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16082", "abs": "https://arxiv.org/abs/2506.16082", "authors": ["Yizhe Li", "Sanping Zhou", "Zheng Qin", "Le Wang"], "title": "PR-DETR: Injecting Position and Relation Prior for Dense Video Captioning", "comment": null, "summary": "Dense video captioning is a challenging task that aims to localize and\ncaption multiple events in an untrimmed video. Recent studies mainly follow the\ntransformer-based architecture to jointly perform the two sub-tasks, i.e.,\nevent localization and caption generation, in an end-to-end manner. Based on\nthe general philosophy of detection transformer, these methods implicitly learn\nthe event locations and event semantics, which requires a large amount of\ntraining data and limits the model's performance in practice. In this paper, we\npropose a novel dense video captioning framework, named PR-DETR, which injects\nthe explicit position and relation prior into the detection transformer to\nimprove the localization accuracy and caption quality, simultaneously. On the\none hand, we first generate a set of position-anchored queries to provide the\nscene-specific position and semantic information about potential events as\nposition prior, which serves as the initial event search regions to eliminate\nthe implausible event proposals. On the other hand, we further design an event\nrelation encoder to explicitly calculate the relationship between event\nboundaries as relation prior to guide the event interaction to improve the\nsemantic coherence of the captions. Extensive ablation studies are conducted to\nverify the effectiveness of the position and relation prior. Experimental\nresults also show the competitive performance of our method on ActivityNet\nCaptions and YouCook2 datasets.", "AI": {"tldr": "PR-DETR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u4f4d\u7f6e\u548c\u5173\u7cfb\u5148\u9a8c\u6539\u8fdb\u68c0\u6d4b\u53d8\u6362\u5668\uff0c\u63d0\u5347\u4e8b\u4ef6\u5b9a\u4f4d\u548c\u5b57\u5e55\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u65b9\u6cd5\u9690\u5f0f\u5b66\u4e60\u4e8b\u4ef6\u4f4d\u7f6e\u548c\u8bed\u4e49\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u6027\u80fd\u53d7\u9650\u3002", "method": "PR-DETR\u5f15\u5165\u4f4d\u7f6e\u951a\u5b9a\u67e5\u8be2\u548c\u4e8b\u4ef6\u5173\u7cfb\u7f16\u7801\u5668\uff0c\u5206\u522b\u63d0\u4f9b\u4f4d\u7f6e\u5148\u9a8c\u548c\u5173\u7cfb\u5148\u9a8c\u3002", "result": "\u5728ActivityNet Captions\u548cYouCook2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u663e\u5f0f\u5148\u9a8c\u6ce8\u5165\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.15706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15706", "abs": "https://arxiv.org/abs/2506.15706", "authors": ["Yunze Lin"], "title": "MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning", "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) as it requires ensuring the correctness of each reasoning step.\nResearchers have been strengthening the mathematical reasoning abilities of\nLLMs through supervised fine-tuning, but due to the inability to suppress\nincorrect outputs, illusions can easily arise. Recently, Direct Preference\nOptimization (DPO) has been widely adopted for aligning human intent by using\npreference data to prevent LLMs from generating incorrect outputs. However, it\nhas shown limited benefits in long-chain mathematical reasoning, mainly because\nDPO struggles to effectively capture the differences between accepted and\nrejected answers from preferences in long-chain data. The inconsistency between\nDPO training and LLMs' generation metrics also affects the effectiveness of\nsuppressing incorrect outputs. We propose the Multi-Granularity Direct\nPreference Optimization (MDPO) method, optimizing the mathematical reasoning of\nLLMs at three granularities: Solution2Solution, Inference2Inference, and\nStep2Step. Solution2Solution focuses on the correctness of entire long-chain\nreasoning; Inference2Inference concentrates on logical reasoning between steps;\nStep2Step corrects computational errors in steps, enhancing the computational\ncapabilities of LLMs. Additionally, we unify the training objectives of the\nthree granularities to align with the generation metrics. We conducted\nexperiments on the open-source models Qwen2 and Llama3, achieving improvements\nof 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,\noutperforming DPO and other DPO variant methods. Furthermore, we also provide a\npipeline for constructing MDPO training data that is simple and does not\nrequire manual annotation costs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u7c92\u5ea6\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08MDPO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7c92\u5ea6\u4f18\u5316LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86DPO\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eDPO\u53ca\u5176\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709DPO\u65b9\u6cd5\u5728\u957f\u94fe\u6570\u5b66\u63a8\u7406\u4e2d\u6548\u679c\u6709\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u504f\u597d\u6570\u636e\u4e2d\u7684\u5dee\u5f02\uff0c\u4e14\u8bad\u7ec3\u76ee\u6807\u4e0e\u751f\u6210\u6307\u6807\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faMDPO\u65b9\u6cd5\uff0c\u5728Solution2Solution\u3001Inference2Inference\u548cStep2Step\u4e09\u4e2a\u7c92\u5ea6\u4e0a\u4f18\u5316LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7edf\u4e00\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5728Qwen2\u548cLlama3\u6a21\u578b\u4e0a\uff0cGSM8K\u6570\u636e\u96c6\u63d0\u53471.7%\u548c0.9%\uff0cMATH\u6570\u636e\u96c6\u63d0\u53472.3%\u548c1.2%\uff0c\u4f18\u4e8eDPO\u53ca\u5176\u53d8\u4f53\u3002", "conclusion": "MDPO\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u6d41\u7a0b\u3002"}}
{"id": "2506.16429", "categories": ["cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16429", "abs": "https://arxiv.org/abs/2506.16429", "authors": ["Sami Abboud", "Eleanor Hanna", "Olivier Jeunen", "Vineesha Raheja", "Schaun Wheeler"], "title": "Agentic Personalisation of Cross-Channel Marketing Experiences", "comment": null, "summary": "Consumer applications provide ample opportunities to surface and communicate\nvarious forms of content to users. From promotional campaigns for new features\nor subscriptions, to evergreen nudges for engagement, or personalised\nrecommendations; across e-mails, push notifications, and in-app surfaces. The\nconventional approach to orchestration for communication relies heavily on\nlabour-intensive manual marketer work, and inhibits effective personalisation\nof content, timing, frequency, and copy-writing. We formulate this task under a\nsequential decision-making framework, where we aim to optimise a modular\ndecision-making policy that maximises incremental engagement for any funnel\nevent. Our approach leverages a Difference-in-Differences design for Individual\nTreatment Effect estimation, and Thompson sampling to balance the\nexplore-exploit trade-off. We present results from a multi-service application,\nwhere our methodology has resulted in significant increases to a variety of\ngoal events across several product features, and is currently deployed across\n150 million users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e8f\u5217\u51b3\u7b56\u6846\u67b6\u7684\u81ea\u52a8\u5316\u901a\u4fe1\u7f16\u6392\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u624b\u52a8\u8425\u9500\u5de5\u4f5c\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u4f18\u5316\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u901a\u4fe1\u7f16\u6392\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u4f18\u5316\u5185\u5bb9\u3001\u65f6\u673a\u3001\u9891\u7387\u548c\u6587\u6848\uff0c\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u51b3\u7b56\u6846\u67b6\uff0c\u7ed3\u5408\u5dee\u5206\u8bbe\u8ba1\uff08Difference-in-Differences\uff09\u4f30\u8ba1\u4e2a\u4f53\u5904\u7406\u6548\u5e94\uff0c\u5e76\u4f7f\u7528Thompson\u91c7\u6837\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u591a\u670d\u52a1\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u76ee\u6807\u4e8b\u4ef6\u7684\u53c2\u4e0e\u5ea6\uff0c\u5e76\u5df2\u90e8\u7f72\u81f31.5\u4ebf\u7528\u6237\u3002", "conclusion": "\u81ea\u52a8\u5316\u901a\u4fe1\u7f16\u6392\u65b9\u6cd5\u6709\u6548\u66ff\u4ee3\u4f20\u7edf\u624b\u52a8\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.16696", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16696", "abs": "https://arxiv.org/abs/2506.16696", "authors": ["Kenjiro Ide", "Taiga Someya", "Kohei Kawaguchi", "Keisuke Fujii"], "title": "Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics", "comment": "5 pages, 3 figures, presented in iCSports 2024 Abstract Track", "summary": "Understanding football tactics is crucial for managers and analysts. Previous\nresearch has proposed models based on spatial and kinematic equations, but\nthese are computationally expensive. Also, Reinforcement learning approaches\nuse player positions and velocities but lack interpretability and require large\ndatasets. Rule-based models align with expert knowledge but have not fully\nconsidered all players' states. This study explores whether low-dimensional,\nrule-based models using spatiotemporal data can effectively capture football\ntactics. Our approach defines interpretable state variables for both the\nball-holder and potential pass receivers, based on criteria that explore\noptions like passing. Through discussions with a manager, we identified key\nvariables representing the game state. We then used StatsBomb event data and\nSkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost\nmodel to predict pass success. The analysis revealed that the distance between\nthe player and the ball, as well as the player's space score, were key factors\nin determining successful passes. Our interpretable low-dimensional modeling\nfacilitates tactical analysis through the use of intuitive variables and\nprovides practical value as a tool to support decision-making in football.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f4e\u7ef4\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u662f\u5426\u80fd\u6709\u6548\u6355\u6349\u8db3\u7403\u6218\u672f\uff0c\u901a\u8fc7\u5b9a\u4e49\u53ef\u89e3\u91ca\u7684\u72b6\u6001\u53d8\u91cf\u5e76\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\uff0c\u8bad\u7ec3XGBoost\u6a21\u578b\u9884\u6d4b\u4f20\u7403\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u672a\u5145\u5206\u8003\u8651\u6240\u6709\u7403\u5458\u72b6\u6001\uff0c\u56e0\u6b64\u7814\u7a76\u4f4e\u7ef4\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u662f\u5426\u53ef\u884c\u3002", "method": "\u5b9a\u4e49\u7403\u6301\u6709\u8005\u548c\u6f5c\u5728\u63a5\u7403\u8005\u7684\u72b6\u6001\u53d8\u91cf\uff0c\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\uff0c\u4f7f\u7528StatsBomb\u548cSkillCorner\u6570\u636e\u8bad\u7ec3XGBoost\u6a21\u578b\u9884\u6d4b\u4f20\u7403\u6210\u529f\u7387\u3002", "result": "\u7403\u5458\u4e0e\u7403\u7684\u8ddd\u79bb\u53ca\u5176\u7a7a\u95f4\u5f97\u5206\u662f\u5f71\u54cd\u4f20\u7403\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u4f4e\u7ef4\u6a21\u578b\u901a\u8fc7\u76f4\u89c2\u53d8\u91cf\u652f\u6301\u6218\u672f\u5206\u6790\uff0c\u4e3a\u8db3\u7403\u51b3\u7b56\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.16427", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16427", "abs": "https://arxiv.org/abs/2506.16427", "authors": ["Mohamad Hachem", "Cl\u00e9ment Roos", "Thierry Miquel", "Murat Bronz"], "title": "Full-Pose Tracking via Robust Control for Over-Actuated Multirotors", "comment": null, "summary": "This paper presents a robust cascaded control architecture for over-actuated\nmultirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)\ncontrol combined with structured H_inf control, initially proposed for\nunder-actuated multirotors, to a broader range of multirotor configurations. To\nachieve precise and robust attitude and position tracking, we employ a weighted\nleast-squares geometric guidance control allocation method, formulated as a\nquadratic optimization problem, enabling full-pose tracking. The proposed\napproach effectively addresses key challenges, such as preventing infeasible\npose references and enhancing robustness against disturbances, as well as\nconsidering multirotor's actual physical limitations. Numerical simulations\nwith an over-actuated hexacopter validate the method's effectiveness,\ndemonstrating its adaptability to diverse mission scenarios and its potential\nfor real-world aerial applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fc7\u9a71\u52a8\u591a\u65cb\u7ffc\u7684\u9c81\u68d2\u7ea7\u8054\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408INDI\u548cH_inf\u63a7\u5236\uff0c\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u63a7\u5236\u5206\u914d\u5b9e\u73b0\u7cbe\u786e\u59ff\u6001\u548c\u4f4d\u7f6e\u8ddf\u8e2a\u3002", "motivation": "\u6269\u5c55INDI\u548cH_inf\u63a7\u5236\u65b9\u6cd5\u81f3\u8fc7\u9a71\u52a8\u591a\u65cb\u7ffc\uff0c\u89e3\u51b3\u59ff\u6001\u548c\u4f4d\u7f6e\u8ddf\u8e2a\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u51e0\u4f55\u5f15\u5bfc\u63a7\u5236\u5206\u914d\u65b9\u6cd5\uff0c\u5f62\u5f0f\u5316\u4e3a\u4e8c\u6b21\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u5168\u59ff\u6001\u8ddf\u8e2a\u3002", "result": "\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u8fc7\u9a71\u52a8\u516d\u65cb\u7ffc\u4e0a\u7684\u6709\u6548\u6027\uff0c\u9002\u5e94\u6027\u5f3a\u4e14\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u4e0d\u53ef\u884c\u59ff\u6001\u53c2\u8003\u548c\u5e72\u6270\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.16370", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16370", "abs": "https://arxiv.org/abs/2506.16370", "authors": ["Iwan Williams"], "title": "Can structural correspondences ground real world representational content in Large Language Models?", "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4 produce compelling responses to a\nwide range of prompts. But their representational capacities are uncertain.\nMany LLMs have no direct contact with extra-linguistic reality: their inputs,\noutputs and training data consist solely of text, raising the questions (1) can\nLLMs represent anything and (2) if so, what? In this paper, I explore what it\nwould take to answer these questions according to a structural-correspondence\nbased account of representation, and make an initial survey of this evidence. I\nargue that the mere existence of structural correspondences between LLMs and\nworldly entities is insufficient to ground representation of those entities.\nHowever, if these structural correspondences play an appropriate role - they\nare exploited in a way that explains successful task performance - then they\ncould ground real world contents. This requires overcoming a challenge: the\ntext-boundedness of LLMs appears, on the face of it, to prevent them engaging\nin the right sorts of tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u80fd\u8868\u793a\u73b0\u5b9e\u4e16\u754c\u5185\u5bb9\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u7ed3\u6784\u5bf9\u5e94\u6027\u7684\u89e3\u91ca\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3LLMs\u662f\u5426\u80fd\u591f\u8868\u793a\u975e\u8bed\u8a00\u73b0\u5b9e\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5f53\u5b83\u4eec\u7684\u8f93\u5165\u3001\u8f93\u51fa\u548c\u8bad\u7ec3\u6570\u636e\u4ec5\u5305\u542b\u6587\u672c\u65f6\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5bf9\u5e94\u6027\u7406\u8bba\u5206\u6790LLMs\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u4efb\u52a1\u8868\u73b0\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5355\u7eaf\u7684\u7ed3\u6784\u5bf9\u5e94\u6027\u4e0d\u8db3\u4ee5\u652f\u6301LLMs\u8868\u793a\u73b0\u5b9e\u5185\u5bb9\uff0c\u4f46\u5982\u679c\u8fd9\u4e9b\u5bf9\u5e94\u6027\u5728\u4efb\u52a1\u8868\u73b0\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u5219\u53ef\u80fd\u5b9e\u73b0\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cLLMs\u7684\u6587\u672c\u5c40\u9650\u6027\u53ef\u80fd\u963b\u788d\u5176\u53c2\u4e0e\u5fc5\u8981\u7684\u4efb\u52a1\uff0c\u4f46\u901a\u8fc7\u9002\u5f53\u5229\u7528\u7ed3\u6784\u5bf9\u5e94\u6027\uff0c\u4ecd\u53ef\u80fd\u5b9e\u73b0\u73b0\u5b9e\u5185\u5bb9\u7684\u8868\u793a\u3002"}}
{"id": "2506.17085", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17085", "abs": "https://arxiv.org/abs/2506.17085", "authors": ["Fabian Neuhaus"], "title": "Dispositions and Roles of Generically Dependent Entities", "comment": null, "summary": "BFO 2020 does not support functions, dispositions, and roles of generically\ndependent continuants (like software or datasets). In this paper, we argue that\nthis is a severe limitation, which prevents, for example, the adequate\nrepresentation of the functions of computer models or the various roles of\ndatasets during the execution of these models. We discuss the aspects of BFO\n2020 that prevent the representation of realizable entities of generically\ndependent continuants. Two approaches to address the issue are presented: (a)\nthe use of defined classes and (b) a proposal of changes that allow BFO to\nsupport functions, dispositions, and roles of generically dependent\ncontinuants.", "AI": {"tldr": "BFO 2020\u65e0\u6cd5\u652f\u6301\u901a\u7528\u4f9d\u8d56\u6301\u7eed\u4f53\u7684\u529f\u80fd\u3001\u503e\u5411\u548c\u89d2\u8272\uff08\u5982\u8f6f\u4ef6\u6216\u6570\u636e\u96c6\uff09\u3002\u672c\u6587\u8ba8\u8bba\u4e86\u8fd9\u4e00\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a\u5b9a\u4e49\u7c7b\u548c\u4fee\u6539BFO\u4ee5\u652f\u6301\u8fd9\u4e9b\u5b9e\u4f53\u7684\u8868\u793a\u3002", "motivation": "BFO 2020\u7684\u5c40\u9650\u6027\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u8868\u793a\u901a\u7528\u4f9d\u8d56\u6301\u7eed\u4f53\uff08\u5982\u8ba1\u7b97\u673a\u6a21\u578b\u6216\u6570\u636e\u96c6\uff09\u7684\u529f\u80fd\u548c\u89d2\u8272\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6267\u884c\u548c\u63cf\u8ff0\u3002", "method": "\u8ba8\u8bba\u4e86BFO 2020\u7684\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u4f7f\u7528\u5b9a\u4e49\u7c7b\u548c\u4fee\u6539BFO\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u652f\u6301\u901a\u7528\u4f9d\u8d56\u6301\u7eed\u4f53\u7684\u529f\u80fd\u3001\u503e\u5411\u548c\u89d2\u8272\u7684\u8868\u793a\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u4e49\u7c7b\u6216\u4fee\u6539BFO\uff0c\u53ef\u4ee5\u89e3\u51b3BFO 2020\u5728\u8868\u793a\u901a\u7528\u4f9d\u8d56\u6301\u7eed\u4f53\u529f\u80fd\u4e0e\u89d2\u8272\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2506.15721", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15721", "abs": "https://arxiv.org/abs/2506.15721", "authors": ["Junqi Gao", "Zhichang Guo", "Dazhi Zhang", "Dong Li", "Runze Liu", "Pengfei Li", "Kai Tian", "Biqing Qi"], "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration", "comment": null, "summary": "Heterogeneous Large Language Model (LLM) fusion integrates the strengths of\nmultiple source LLMs with different architectures into a target LLM with low\ncomputational overhead. While promising, existing methods suffer from two major\nlimitations: 1) reliance on real data from limited domain for knowledge fusion,\npreventing the target LLM from fully acquiring knowledge across diverse\ndomains, and 2) fixed data allocation proportions across domains, failing to\ndynamically adjust according to the target LLM's varying capabilities across\ndomains, leading to a capability imbalance. To overcome these limitations, we\npropose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.\nThrough the organization of knowledge domains into a hierarchical tree\nstructure, Bohdi enables automatic domain exploration and multi-domain data\ngeneration through multi-model collaboration, thereby comprehensively\nextracting knowledge from source LLMs. By formalizing domain expansion and data\nsampling proportion allocation on the knowledge tree as a Hierarchical\nMulti-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism\nto adaptively adjust sampling proportions based on the target LLM's performance\nfeedback across domains. Integrated with our proposed Introspection-Rebirth\n(IR) mechanism, DynaBranches dynamically tracks capability shifts during target\nLLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),\nfurther enhancing its online adaptation capability. Comparative experimental\nresults on a comprehensive suite of benchmarks demonstrate that Bohdi\nsignificantly outperforms existing baselines on multiple target LLMs, exhibits\nhigher data efficiency, and virtually eliminates the imbalance in the target\nLLM's capabilities. Our code is available at\nhttps://github.com/gjq100/Bohdi.git.", "AI": {"tldr": "Bohdi\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u5f02\u6784\u5927\u8bed\u8a00\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6811\u7ed3\u6784\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5f02\u6784\u5927\u8bed\u8a00\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u9886\u57df\u7684\u771f\u5b9e\u6570\u636e\u4e14\u56fa\u5b9a\u6570\u636e\u5206\u914d\u6bd4\u4f8b\uff0c\u5bfc\u81f4\u77e5\u8bc6\u83b7\u53d6\u4e0d\u5168\u9762\u548c\u80fd\u529b\u4e0d\u5e73\u8861\u3002", "method": "Bohdi\u901a\u8fc7\u5206\u5c42\u6811\u7ed3\u6784\u7ec4\u7ec7\u77e5\u8bc6\u57df\uff0c\u5229\u7528\u591a\u6a21\u578b\u534f\u4f5c\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u901a\u8fc7DynaBranches\u673a\u5236\u52a8\u6001\u8c03\u6574\u6570\u636e\u91c7\u6837\u6bd4\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBohdi\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u6570\u636e\u6548\u7387\u66f4\u9ad8\u4e14\u80fd\u529b\u66f4\u5e73\u8861\u3002", "conclusion": "Bohdi\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u5927\u8bed\u8a00\u6a21\u578b\u878d\u5408\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2506.16445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16445", "abs": "https://arxiv.org/abs/2506.16445", "authors": ["Haotian Xia", "Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "StoryWriter: A Multi-Agent Framework for Long Story Generation", "comment": null, "summary": "Long story generation remains a challenge for existing large language models\n(LLMs), primarily due to two main factors: (1) discourse coherence, which\nrequires plot consistency, logical coherence, and completeness in the long-form\ngeneration, and (2) narrative complexity, which requires an interwoven and\nengaging narrative. To address these challenges, we propose StoryWriter, a\nmulti-agent story generation framework, which consists of three main modules:\n(1) outline agent, which generates event-based outlines containing rich event\nplots, character, and event-event relationships. (2) planning agent, which\nfurther details events and plans which events should be written in each chapter\nto maintain an interwoven and engaging story. (3) writing agent, which\ndynamically compresses the story history based on the current event to generate\nand reflect new plots, ensuring the coherence of the generated story. We\nconduct both human and automated evaluation, and StoryWriter significantly\noutperforms existing story generation baselines in both story quality and\nlength. Furthermore, we use StoryWriter to generate a dataset, which contains\nabout $6,000$ high-quality long stories, with an average length of $8,000$\nwords. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning\non LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which\ndemonstrates advanced performance in long story generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStoryWriter\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u957f\u6545\u4e8b\u751f\u6210\u7684\u8fde\u8d2f\u6027\u548c\u590d\u6742\u6027\u6311\u6218\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u957f\u6545\u4e8b\u751f\u6210\u5bf9\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u662f\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u8fde\u8d2f\u6027\u548c\u53d9\u4e8b\u590d\u6742\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e09\u6a21\u5757\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u63d0\u7eb2\u3001\u89c4\u5212\u548c\u5199\u4f5c\u667a\u80fd\u4f53\uff0c\u5206\u522b\u8d1f\u8d23\u751f\u6210\u4e8b\u4ef6\u63d0\u7eb2\u3001\u8be6\u7ec6\u89c4\u5212\u548c\u52a8\u6001\u5199\u4f5c\u3002", "result": "StoryWriter\u5728\u6545\u4e8b\u8d28\u91cf\u548c\u957f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u751f\u6210\u4e86\u5305\u542b6000\u7bc7\u9ad8\u8d28\u91cf\u957f\u6545\u4e8b\u7684\u6570\u636e\u96c6\u3002", "conclusion": "StoryWriter\u6846\u67b6\u5728\u957f\u6545\u4e8b\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bad\u7ec3\u6a21\u578b\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.15809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15809", "abs": "https://arxiv.org/abs/2506.15809", "authors": ["Deyi Li", "Zijun Yao", "Muxuan Liang", "Mei Liu"], "title": "DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling", "comment": null, "summary": "In recent years, graph learning has gained significant interest for modeling\ncomplex interactions among medical events in structured Electronic Health\nRecord (EHR) data. However, existing graph-based approaches often work in a\nstatic manner, either restricting interactions within individual encounters or\ncollapsing all historical encounters into a single snapshot. As a result, when\nit is necessary to identify meaningful groups of medical events spanning\nlongitudinal encounters, existing methods are inadequate in modeling\ninteractions cross encounters while accounting for temporal dependencies. To\naddress this limitation, we introduce Deep Patient Journey (DeepJ), a novel\ngraph convolutional transformer model with differentiable graph pooling to\neffectively capture intra-encounter and inter-encounter medical event\ninteractions. DeepJ can identify groups of temporally and functionally related\nmedical events, offering valuable insights into key event clusters pertinent to\npatient outcome prediction. DeepJ significantly outperformed five\nstate-of-the-art baseline models while enhancing interpretability,\ndemonstrating its potential for improved patient risk stratification.", "AI": {"tldr": "DeepJ\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u5377\u79ef\u53d8\u6362\u5668\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u4e2d\u7684\u8de8\u65f6\u95f4\u533b\u7597\u4e8b\u4ef6\u4ea4\u4e92\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u5efa\u6a21\u8de8\u65f6\u95f4\u533b\u7597\u4e8b\u4ef6\u4ea4\u4e92\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u7eb5\u5411\u4e8b\u4ef6\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faDeepJ\u6a21\u578b\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u548c\u53d8\u6362\u5668\u6280\u672f\uff0c\u91c7\u7528\u53ef\u5fae\u5206\u56fe\u6c60\u5316\u65b9\u6cd5\uff0c\u5efa\u6a21\u533b\u7597\u4e8b\u4ef6\u7684\u8de8\u65f6\u95f4\u4ea4\u4e92\u3002", "result": "DeepJ\u5728\u9884\u6d4b\u60a3\u8005\u98ce\u9669\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4e94\u79cd\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "DeepJ\u4e3a\u60a3\u8005\u98ce\u9669\u5206\u5c42\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.16371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16371", "abs": "https://arxiv.org/abs/2506.16371", "authors": ["Yunhao Hou", "Bochao Zou", "Min Zhang", "Ran Chen", "Shangdong Yang", "Yanmei Zhang", "Junbao Zhuo", "Siheng Chen", "Jiansheng Chen", "Huimin Ma"], "title": "AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios", "comment": null, "summary": "By sharing information across multiple agents, collaborative perception helps\nautonomous vehicles mitigate occlusions and improve overall perception\naccuracy. While most previous work focus on vehicle-to-vehicle and\nvehicle-to-infrastructure collaboration, with limited attention to aerial\nperspectives provided by UAVs, which uniquely offer dynamic, top-down views to\nalleviate occlusions and monitor large-scale interactive environments. A major\nreason for this is the lack of high-quality datasets for aerial-ground\ncollaborative scenarios. To bridge this gap, we present AGC-Drive, the first\nlarge-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The\ndata collection platform consists of two vehicles, each equipped with five\ncameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and\na LiDAR sensor, enabling comprehensive multi-view and multi-agent perception.\nConsisting of approximately 120K LiDAR frames and 440K images, the dataset\ncovers 14 diverse real-world driving scenarios, including urban roundabouts,\nhighway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic\ninteraction events, including vehicle cut-ins, cut-outs, and frequent lane\nchanges. AGC-Drive contains 400 scenes, each with approximately 100 frames and\nfully annotated 3D bounding boxes covering 13 object categories. We provide\nbenchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative\nperception and vehicle-to-UAV collaborative perception. Additionally, we\nrelease an open-source toolkit, including spatiotemporal alignment verification\ntools, multi-agent visualization systems, and collaborative annotation\nutilities. The dataset and code are available at\nhttps://github.com/PercepX/AGC-Drive.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6AGC-Drive\uff0c\u7528\u4e8e\u7a7a\u4e2d-\u5730\u9762\u534f\u540c3D\u611f\u77e5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8f66\u8f86\u95f4\u6216\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u7684\u534f\u540c\u611f\u77e5\uff0c\u5ffd\u89c6\u4e86\u65e0\u4eba\u673a\u63d0\u4f9b\u7684\u7a7a\u4e2d\u89c6\u89d2\u3002\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u662f\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u4e24\u8f86\u88c5\u5907\u591a\u6444\u50cf\u5934\u548cLiDAR\u7684\u8f66\u8f86\u53ca\u4e00\u67b6\u65e0\u4eba\u673a\u6536\u96c6\u6570\u636e\uff0c\u6db5\u76d6\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u548c\u52a8\u6001\u4ea4\u4e92\u4e8b\u4ef6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b120K LiDAR\u5e27\u548c440K\u56fe\u50cf\uff0c\u8986\u76d614\u79cd\u573a\u666f\uff0c\u63d0\u4f9b13\u7c7b\u7269\u4f53\u76843D\u6807\u6ce8\uff0c\u5e76\u53d1\u5e03\u5f00\u6e90\u5de5\u5177\u5305\u3002", "conclusion": "AGC-Drive\u586b\u8865\u4e86\u7a7a\u4e2d-\u5730\u9762\u534f\u540c\u611f\u77e5\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2506.16720", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16720", "abs": "https://arxiv.org/abs/2506.16720", "authors": ["Weitao Zhou", "Bo Zhang", "Zhong Cao", "Xiang Li", "Qian Cheng", "Chunyang Liu", "Yaqin Zhang", "Diange Yang"], "title": "DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy", "comment": null, "summary": "With the increasing presence of automated vehicles on open roads under driver\nsupervision, disengagement cases are becoming more prevalent. While some\ndata-driven planning systems attempt to directly utilize these disengagement\ncases for policy improvement, the inherent scarcity of disengagement data\n(often occurring as a single instances) restricts training effectiveness.\nFurthermore, some disengagement data should be excluded since the disengagement\nmay not always come from the failure of driving policies, e.g. the driver may\ncasually intervene for a while. To this end, this work proposes\ndisengagement-reason-augmented reinforcement learning (DRARL), which enhances\ndriving policy improvement process according to the reason of disengagement\ncases. Specifically, the reason of disengagement is identified by a\nout-of-distribution (OOD) state estimation model. When the reason doesn't\nexist, the case will be identified as a casual disengagement case, which\ndoesn't require additional policy adjustment. Otherwise, the policy can be\nupdated under a reason-augmented imagination environment, improving the policy\nperformance of disengagement cases with similar reasons. The method is\nevaluated using real-world disengagement cases collected by autonomous driving\nrobotaxi. Experimental results demonstrate that the method accurately\nidentifies policy-related disengagement reasons, allowing the agent to handle\nboth original and semantically similar cases through reason-augmented training.\nFurthermore, the approach prevents the agent from becoming overly conservative\nafter policy adjustments. Overall, this work provides an efficient way to\nimprove driving policy performance with disengagement cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8131\u7ba1\u539f\u56e0\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08DRARL\uff09\uff0c\u901a\u8fc7\u8bc6\u522b\u8131\u7ba1\u539f\u56e0\u4f18\u5316\u9a7e\u9a76\u7b56\u7565\uff0c\u907f\u514d\u65e0\u6548\u6570\u636e\u5e72\u6270\uff0c\u63d0\u5347\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5f00\u653e\u9053\u8def\u4e0a\u7684\u589e\u591a\uff0c\u8131\u7ba1\u6848\u4f8b\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u8131\u7ba1\u6570\u636e\u7a00\u7f3a\u4e14\u90e8\u5206\u6570\u636e\u65e0\u6548\uff0c\u9650\u5236\u4e86\u7b56\u7565\u6539\u8fdb\u6548\u679c\u3002", "method": "\u63d0\u51faDRARL\u65b9\u6cd5\uff0c\u5229\u7528OOD\u72b6\u6001\u4f30\u8ba1\u6a21\u578b\u8bc6\u522b\u8131\u7ba1\u539f\u56e0\uff0c\u533a\u5206\u65e0\u6548\u8131\u7ba1\u6848\u4f8b\uff0c\u5e76\u5728\u539f\u56e0\u589e\u5f3a\u7684\u865a\u62df\u73af\u5883\u4e2d\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u8bc6\u522b\u7b56\u7565\u76f8\u5173\u8131\u7ba1\u539f\u56e0\uff0c\u63d0\u5347\u7b56\u7565\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u7b56\u7565\u8fc7\u4e8e\u4fdd\u5b88\u3002", "conclusion": "DRARL\u4e3a\u5229\u7528\u8131\u7ba1\u6848\u4f8b\u4f18\u5316\u9a7e\u9a76\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u3002"}}
{"id": "2506.16574", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16574", "abs": "https://arxiv.org/abs/2506.16574", "authors": ["Enes Yavuz Ugan", "Ngoc-Quan Pham", "Alexander Waibel"], "title": "Weight Factorization and Centralization for Continual Learning in Speech Recognition", "comment": "Accepted to INTERSPEECH 2025", "summary": "Modern neural network based speech recognition models are required to\ncontinually absorb new data without re-training the whole system, especially in\ndownstream applications using foundation models, having no access to the\noriginal training data. Continually training the models in a rehearsal-free,\nmultilingual, and language agnostic condition, likely leads to catastrophic\nforgetting, when a seemingly insignificant disruption to the weights can\ndestructively harm the quality of the models. Inspired by the ability of human\nbrains to learn and consolidate knowledge through the waking-sleeping cycle, we\npropose a continual learning approach with two distinct phases: factorization\nand centralization, learning and merging knowledge accordingly. Our experiments\non a sequence of varied code-switching datasets showed that the centralization\nstage can effectively prevent catastrophic forgetting by accumulating the\nknowledge in multiple scattering low-rank adapters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u5316\u548c\u4e2d\u5fc3\u5316\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u9632\u6b62\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u9700\u8981\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u7cfb\u7edf\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u5438\u6536\u65b0\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002\u6301\u7eed\u5b66\u4e60\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u53d7\u4eba\u7c7b\u5927\u8111\u901a\u8fc7\u9192\u7761\u5468\u671f\u5b66\u4e60\u548c\u5de9\u56fa\u77e5\u8bc6\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u56e0\u5b50\u5316\u548c\u4e2d\u5fc3\u5316\u4e24\u4e2a\u9636\u6bb5\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e2d\u5fc3\u5316\u9636\u6bb5\u901a\u8fc7\u79ef\u7d2f\u591a\u4e2a\u4f4e\u79e9\u9002\u914d\u5668\u7684\u77e5\u8bc6\uff0c\u6709\u6548\u9632\u6b62\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u4ee3\u7801\u5207\u6362\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17212", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17212", "abs": "https://arxiv.org/abs/2506.17212", "authors": ["Tianjiao Yu", "Vedant Shah", "Muntasir Wahed", "Ying Shen", "Kiet A. Nguyen", "Ismini Lourentzou"], "title": "Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting", "comment": null, "summary": "Articulated objects are common in the real world, yet modeling their\nstructure and motion remains a challenging task for 3D reconstruction methods.\nIn this work, we introduce Part$^{2}$GS, a novel framework for modeling\narticulated digital twins of multi-part objects with high-fidelity geometry and\nphysically consistent articulation. Part$^{2}$GS leverages a part-aware 3D\nGaussian representation that encodes articulated components with learnable\nattributes, enabling structured, disentangled transformations that preserve\nhigh-fidelity geometry. To ensure physically consistent motion, we propose a\nmotion-aware canonical representation guided by physics-based constraints,\nincluding contact enforcement, velocity consistency, and vector-field\nalignment. Furthermore, we introduce a field of repel points to prevent part\ncollisions and maintain stable articulation paths, significantly improving\nmotion coherence over baselines. Extensive evaluations on both synthetic and\nreal-world datasets show that Part$^{2}$GS consistently outperforms\nstate-of-the-art methods by up to 10$\\times$ in Chamfer Distance for movable\nparts.", "AI": {"tldr": "Part$^{2}$GS\u662f\u4e00\u79cd\u7528\u4e8e\u5efa\u6a21\u591a\u90e8\u5206\u7269\u4f53\u9ad8\u4fdd\u771f\u51e0\u4f55\u548c\u7269\u7406\u4e00\u81f4\u8fd0\u52a8\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u611f\u77e5\u76843D\u9ad8\u65af\u8868\u793a\u548c\u7269\u7406\u7ea6\u675f\u5b9e\u73b0\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u8282\u7269\u4f53\u5efa\u6a21\u7ed3\u6784\u548c\u8fd0\u52a8\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u51e0\u4f55\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u90e8\u5206\u611f\u77e5\u76843D\u9ad8\u65af\u8868\u793a\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\uff08\u5982\u63a5\u89e6\u5f3a\u5236\u3001\u901f\u5ea6\u4e00\u81f4\u6027\u548c\u77e2\u91cf\u573a\u5bf9\u9f50\uff09\u548c\u6392\u65a5\u70b9\u573a\u9632\u6b62\u78b0\u649e\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cPart$^{2}$GS\u5728\u53ef\u79fb\u52a8\u90e8\u5206\u7684Chamfer Distance\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe10\u500d\u3002", "conclusion": "Part$^{2}$GS\u901a\u8fc7\u7ed3\u6784\u5316\u89e3\u8026\u53d8\u6362\u548c\u7269\u7406\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u8282\u7269\u4f53\u5efa\u6a21\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.16056", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16056", "abs": "https://arxiv.org/abs/2506.16056", "authors": ["Puchun Liu", "C. L. Philip Chen", "Yubin He", "Tong Zhang"], "title": "CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations", "comment": null, "summary": "The difficulty of extracting deep features from EEG data and effectively\nintegrating information from multiple views presents significant challenges for\ndeveloping a generalizable pretraining framework for EEG representation\nlearning. However, most existing pre-training methods rely solely on the\ncontextual semantics of a single view, failing to capture the complex and\nsynergistic interactions among different perspectives, limiting the\nexpressiveness and generalization of learned representations. To address these\nissues, this paper proposes CRIA, an adaptive framework that utilizes\nvariable-length and variable-channel coding to achieve a unified representation\nof EEG data across different datasets. In this work, we define cross-view\ninformation as the integrated representation that emerges from the interaction\namong temporal, spectral, and spatial views of EEG signals. The model employs a\ncross-attention mechanism to fuse temporal, spectral, and spatial features\neffectively, and combines an attention matrix masking strategy based on the\ninformation bottleneck principle with a novel viewpoint masking pre-training\nscheme. Experimental results on the Temple University EEG corpus and the\nCHB-MIT dataset show that CRIA outperforms existing methods with the same\npre-training conditions, achieving a balanced accuracy of 57.02% for\nmulti-class event classification and 80.03% for anomaly detection, highlighting\nits strong generalization ability.", "AI": {"tldr": "CRIA\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u89c6\u56fe\u4fe1\u606f\u6574\u5408\u548c\u591a\u89c6\u56fe\u7279\u5f81\u878d\u5408\uff0c\u89e3\u51b3\u4e86EEG\u6570\u636e\u9884\u8bad\u7ec3\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u548c\u4fe1\u606f\u6574\u5408\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5355\u4e00\u89c6\u56fe\u7684\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u89c6\u56fe\u95f4\u7684\u590d\u6742\u534f\u540c\u4f5c\u7528\uff0c\u9650\u5236\u4e86EEG\u8868\u793a\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002", "method": "CRIA\u91c7\u7528\u53d8\u957f\u548c\u53d8\u901a\u9053\u7f16\u7801\u5b9e\u73b0EEG\u6570\u636e\u7684\u7edf\u4e00\u8868\u793a\uff0c\u5229\u7528\u8de8\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u65f6\u95f4\u3001\u9891\u8c31\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684\u6ce8\u610f\u529b\u77e9\u9635\u63a9\u7801\u7b56\u7565\u3002", "result": "\u5728Temple University EEG corpus\u548cCHB-MIT\u6570\u636e\u96c6\u4e0a\uff0cCRIA\u5728\u76f8\u540c\u9884\u8bad\u7ec3\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u591a\u7c7b\u4e8b\u4ef6\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u7684\u5e73\u8861\u51c6\u786e\u7387\u5206\u522b\u4e3a57.02%\u548c80.03%\u3002", "conclusion": "CRIA\u901a\u8fc7\u591a\u89c6\u56fe\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u8868\u793a\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2506.16253", "categories": ["cs.LG", "cs.GT", "cs.IT", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16253", "abs": "https://arxiv.org/abs/2506.16253", "authors": ["Hadar Tal", "Oron Sabag"], "title": "Optimal Online Bookmaking for Any Number of Outcomes", "comment": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2025", "summary": "We study the Online Bookmaking problem, where a bookmaker dynamically updates\nbetting odds on the possible outcomes of an event. In each betting round, the\nbookmaker can adjust the odds based on the cumulative betting behavior of\ngamblers, aiming to maximize profit while mitigating potential loss. We show\nthat for any event and any number of betting rounds, in a worst-case setting\nover all possible gamblers and outcome realizations, the bookmaker's optimal\nloss is the largest root of a simple polynomial. Our solution shows that\nbookmakers can be as fair as desired while avoiding financial risk, and the\nexplicit characterization reveals an intriguing relation between the\nbookmaker's regret and Hermite polynomials. We develop an efficient algorithm\nthat computes the optimal bookmaking strategy: when facing an optimal gambler,\nthe algorithm achieves the optimal loss, and in rounds where the gambler is\nsuboptimal, it reduces the achieved loss to the optimal opportunistic loss, a\nnotion that is related to subgame perfect Nash equilibrium. The key technical\ncontribution to achieve these results is an explicit characterization of the\nBellman-Pareto frontier, which unifies the dynamic programming updates for\nBellman's value function with the multi-criteria optimization framework of the\nPareto frontier in the context of vector repeated games.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u7ebf\u535a\u5f69\u95ee\u9898\uff0c\u5e84\u5bb6\u52a8\u6001\u8c03\u6574\u8d54\u7387\u4ee5\u6700\u5927\u5316\u5229\u6da6\u5e76\u51cf\u5c11\u6f5c\u5728\u635f\u5931\uff0c\u8bc1\u660e\u4e86\u6700\u4f18\u635f\u5931\u662f\u591a\u9879\u5f0f\u6700\u5927\u6839\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u5e84\u5bb6\u5982\u4f55\u5728\u52a8\u6001\u8c03\u6574\u8d54\u7387\u65f6\u5e73\u8861\u516c\u5e73\u6027\u548c\u8d22\u52a1\u98ce\u9669\uff0c\u540c\u65f6\u5e94\u5bf9\u8d4c\u5f92\u7684\u6700\u4f18\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u591a\u9879\u5f0f\u6839\u548c\u8d1d\u5c14\u66fc-\u5e15\u7d2f\u6258\u524d\u6cbf\u7684\u663e\u5f0f\u8868\u5f81\uff0c\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u535a\u5f69\u7b56\u7565\u3002", "result": "\u5e84\u5bb6\u53ef\u5b9e\u73b0\u4efb\u610f\u516c\u5e73\u6027\u4e14\u907f\u514d\u8d22\u52a1\u98ce\u9669\uff0c\u6700\u4f18\u635f\u5931\u4e0e\u57c3\u5c14\u7c73\u7279\u591a\u9879\u5f0f\u76f8\u5173\uff0c\u7b97\u6cd5\u5728\u6700\u4f18\u8d4c\u5f92\u4e0b\u8fbe\u5230\u6700\u4f18\u635f\u5931\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u535a\u5f69\u7b56\u7565\u4e0e\u6570\u5b66\u7ed3\u6784\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u4e3a\u52a8\u6001\u535a\u5f08\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2506.16962", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.16962", "abs": "https://arxiv.org/abs/2506.16962", "authors": ["Haoran Sun", "Yankai Jiang", "Wenjie Lou", "Yujie Zhang", "Wenjie Li", "Lilong Wang", "Mianxin Liu", "Lei Liu", "Xiaosong Wang"], "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs", "comment": null, "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMICS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u533b\u5b66\u63a8\u7406\u8def\u5f84\u6570\u636e\uff0c\u5e76\u6784\u5efa\u4e86\u591a\u4efb\u52a1\u533b\u5b66\u63a8\u7406\u6570\u636e\u96c6MMRP\u548c\u533b\u5b66MLLM\u6a21\u578bChiron-o1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u9886\u57df\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u6846\u67b6\u6765\u641c\u7d22\u548c\u8bc4\u4f30\u6709\u6548\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51faMICS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bfc\u5e08\u6a21\u578b\u521d\u59cb\u5316\u63a8\u7406\u8def\u5f84\uff0c\u5b9e\u4e60\u6a21\u578b\u7ee7\u7eed\u63a8\u7406\uff0c\u5e76\u6839\u636eMICS-Score\u9009\u62e9\u6700\u4f18\u8def\u5f84\uff0c\u6784\u5efaMMRP\u6570\u636e\u96c6\u548cChiron-o1\u6a21\u578b\u3002", "result": "Chiron-o1\u5728\u591a\u4e2a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MICS\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66MLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0cChiron-o1\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u89c6\u89c9\u95ee\u7b54\u548c\u6cdb\u5316\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.16314", "categories": ["cs.LG", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2506.16314", "abs": "https://arxiv.org/abs/2506.16314", "authors": ["Emmanuel Gangler", "Emille E. O. Ishida", "Matwey V. Kornilov", "Vladimir Korolev", "Anastasia Lavrukhina", "Konstantin Malanchev", "Maria V. Pruzhinskaya", "Etienne Russeil", "Timofey Semenikhin", "Sreevarsha Sreejith", "Alina A. Volnova"], "title": "Signatures to help interpretability of anomalies", "comment": "7 pages, 3 figure, proceedings of the International Conference on\n  Machine Learning for Astrophysics (ML4ASTRO2)", "summary": "Machine learning is often viewed as a black box when it comes to\nunderstanding its output, be it a decision or a score. Automatic anomaly\ndetection is no exception to this rule, and quite often the astronomer is left\nto independently analyze the data in order to understand why a given event is\ntagged as an anomaly. We introduce here idea of anomaly signature, whose aim is\nto help the interpretability of anomalies by highlighting which features\ncontributed to the decision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5f02\u5e38\u7b7e\u540d\u201d\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u7a81\u51fa\u663e\u793a\u54ea\u4e9b\u7279\u5f81\u5f71\u54cd\u4e86\u51b3\u7b56\uff0c\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5e38\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u8f93\u51fa\uff08\u5982\u51b3\u7b56\u6216\u8bc4\u5206\uff09\uff0c\u5c24\u5176\u662f\u5728\u81ea\u52a8\u5f02\u5e38\u68c0\u6d4b\u4e2d\uff0c\u5929\u6587\u5b66\u5bb6\u901a\u5e38\u9700\u8981\u72ec\u7acb\u5206\u6790\u6570\u636e\u4ee5\u7406\u89e3\u5f02\u5e38\u6807\u8bb0\u7684\u539f\u56e0\u3002", "method": "\u5f15\u5165\u4e86\u201c\u5f02\u5e38\u7b7e\u540d\u201d\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u7a81\u51fa\u663e\u793a\u5bf9\u51b3\u7b56\u6709\u8d21\u732e\u7684\u7279\u5f81\u6765\u5e2e\u52a9\u89e3\u91ca\u5f02\u5e38\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u7814\u7a76\u8005\u66f4\u5bb9\u6613\u7406\u89e3\u5f02\u5e38\u6807\u8bb0\u7684\u539f\u56e0\u3002", "conclusion": "\u201c\u5f02\u5e38\u7b7e\u540d\u201d\u662f\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\uff0c\u80fd\u591f\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.16352", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16352", "abs": "https://arxiv.org/abs/2506.16352", "authors": ["Theo Zangato", "Aomar Osmani", "Pegah Alizadeh"], "title": "Data-Driven Policy Mapping for Safe RL-based Energy Management Systems", "comment": null, "summary": "Increasing global energy demand and renewable integration complexity have\nplaced buildings at the center of sustainable energy management. We present a\nthree-step reinforcement learning(RL)-based Building Energy Management System\n(BEMS) that combines clustering, forecasting, and constrained policy learning\nto address scalability, adaptability, and safety challenges. First, we cluster\nnon-shiftable load profiles to identify common consumption patterns, enabling\npolicy generalization and transfer without retraining for each new building.\nNext, we integrate an LSTM based forecasting module to anticipate future\nstates, improving the RL agents' responsiveness to dynamic conditions. Lastly,\ndomain-informed action masking ensures safe exploration and operation,\npreventing harmful decisions. Evaluated on real-world data, our approach\nreduces operating costs by up to 15% for certain building types, maintains\nstable environmental performance, and quickly classifies and optimizes new\nbuildings with limited data. It also adapts to stochastic tariff changes\nwithout retraining. Overall, this framework delivers scalable, robust, and\ncost-effective building energy management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u6b65\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff08BEMS\uff09\uff0c\u7ed3\u5408\u805a\u7c7b\u3001\u9884\u6d4b\u548c\u7ea6\u675f\u7b56\u7565\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u5168\u7403\u80fd\u6e90\u9700\u6c42\u589e\u957f\u548c\u53ef\u518d\u751f\u80fd\u6e90\u96c6\u6210\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f7f\u5f97\u5efa\u7b51\u6210\u4e3a\u53ef\u6301\u7eed\u80fd\u6e90\u7ba1\u7406\u7684\u6838\u5fc3\u3002", "method": "1. \u805a\u7c7b\u975e\u53ef\u8f6c\u79fb\u8d1f\u8377\u6a21\u5f0f\u4ee5\u8bc6\u522b\u5e38\u89c1\u6d88\u8d39\u6a21\u5f0f\uff1b2. \u96c6\u6210LSTM\u9884\u6d4b\u6a21\u5757\u4ee5\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff1b3. \u4f7f\u7528\u9886\u57df\u77e5\u8bc6\u7ea6\u675f\u7b56\u7565\u5b66\u4e60\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u964d\u4f4e\u4e86\u67d0\u4e9b\u5efa\u7b51\u7c7b\u578b\u7684\u8fd0\u8425\u6210\u672c\u8fbe15%\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u73af\u5883\u6027\u80fd\uff0c\u5e76\u80fd\u5feb\u901f\u5206\u7c7b\u548c\u4f18\u5316\u65b0\u5efa\u7b51\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u7a33\u5065\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u3002"}}
{"id": "2506.16436", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16436", "abs": "https://arxiv.org/abs/2506.16436", "authors": ["Antonio Giulio Coretti", "Mattia Varile", "Mario Edoardo Bertaina"], "title": "An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras", "comment": "18th International Conference on Space Operations - Safety and\n  sustainability of Space Operations (SSU)", "summary": "Space debris poses a significant threat, driving research into active and\npassive mitigation strategies. This work presents an innovative collision\navoidance system utilizing event-based cameras - a novel imaging technology\nwell-suited for Space Situational Awareness (SSA) and Space Traffic Management\n(STM). The system, employing a Stack-CNN algorithm (previously used for meteor\ndetection), analyzes real-time event-based camera data to detect faint moving\nobjects. Testing on terrestrial data demonstrates the algorithm's ability to\nenhance signal-to-noise ratio, offering a promising approach for on-board space\nimaging and improving STM/SSA operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u521b\u65b0\u78b0\u649e\u907f\u514d\u7cfb\u7edf\uff0c\u7528\u4e8e\u7a7a\u95f4\u788e\u7247\u76d1\u6d4b\u3002", "motivation": "\u7a7a\u95f4\u788e\u7247\u5bf9\u822a\u5929\u5668\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u6d4b\u548c\u907f\u514d\u7b56\u7565\u3002", "method": "\u91c7\u7528Stack-CNN\u7b97\u6cd5\u5206\u6790\u4e8b\u4ef6\u76f8\u673a\u7684\u5b9e\u65f6\u6570\u636e\uff0c\u68c0\u6d4b\u5fae\u5f31\u79fb\u52a8\u76ee\u6807\u3002", "result": "\u5730\u9762\u6d4b\u8bd5\u663e\u793a\u7b97\u6cd5\u80fd\u63d0\u9ad8\u4fe1\u566a\u6bd4\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u6210\u50cf\u548cSSA/STM\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u7a7a\u95f4\u4ea4\u901a\u7ba1\u7406\u548c\u6001\u52bf\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17144", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17144", "abs": "https://arxiv.org/abs/2506.17144", "authors": ["Ritabrata Chakraborty", "Rajatsubhra Chakraborty", "Avijit Dasgupta", "Sandeep Chaurasia"], "title": "Do We Need Large VLMs for Spotting Soccer Actions?", "comment": "5 pages, 2 figures", "summary": "Traditional video-based tasks like soccer action spotting rely heavily on\nvisual inputs, often requiring complex and computationally expensive models to\nprocess dense video data. In this work, we propose a shift from this\nvideo-centric approach to a text-based task, making it lightweight and scalable\nby utilizing Large Language Models (LLMs) instead of Vision-Language Models\n(VLMs). We posit that expert commentary, which provides rich, fine-grained\ndescriptions and contextual cues such as excitement and tactical insights,\ncontains enough information to reliably spot key actions in a match. To\ndemonstrate this, we use the SoccerNet Echoes dataset, which provides\ntimestamped commentary, and employ a system of three LLMs acting as judges\nspecializing in outcome, excitement, and tactics. Each LLM evaluates sliding\nwindows of commentary to identify actions like goals, cards, and substitutions,\ngenerating accurate timestamps for these events. Our experiments show that this\nlanguage-centric approach performs effectively in detecting critical match\nevents, providing a lightweight and training-free alternative to traditional\nvideo-based methods for action spotting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66ff\u4ee3\u4f20\u7edf\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6765\u8bc6\u522b\u8db3\u7403\u6bd4\u8d5b\u4e2d\u7684\u5173\u952e\u52a8\u4f5c\uff0c\u5982\u8fdb\u7403\u3001\u9ec4\u724c\u548c\u6362\u4eba\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5206\u6790\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u6602\u8d35\uff0c\u800c\u4e13\u5bb6\u8bc4\u8bba\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u7ec6\u7c92\u5ea6\u63cf\u8ff0\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8db3\u4ee5\u53ef\u9760\u5730\u8bc6\u522b\u5173\u952e\u52a8\u4f5c\u3002", "method": "\u4f7f\u7528SoccerNet Echoes\u6570\u636e\u96c6\u4e2d\u7684\u65f6\u95f4\u6233\u8bc4\u8bba\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u95e8\u8bc4\u4f30\u7ed3\u679c\u3001\u5174\u594b\u5ea6\u548c\u6218\u672f\u7684LLM\u4f5c\u4e3a\u8bc4\u59d4\uff0c\u6ed1\u52a8\u7a97\u53e3\u5206\u6790\u8bc4\u8bba\u4ee5\u8bc6\u522b\u52a8\u4f5c\u5e76\u751f\u6210\u51c6\u786e\u65f6\u95f4\u6233\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u57fa\u4e8e\u8bed\u8a00\u7684\u65b9\u6cd5\u5728\u68c0\u6d4b\u5173\u952e\u6bd4\u8d5b\u4e8b\u4ef6\u4e0a\u8868\u73b0\u6709\u6548\uff0c\u4e3a\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8bed\u8a00\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4e3a\u8db3\u7403\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2506.16698", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16698", "abs": "https://arxiv.org/abs/2506.16698", "authors": ["Dinesh Ramasamy", "Shakti Kumar", "Chris Cadonic", "Jiaxin Yang", "Sohini Roychowdhury", "Esam Abdel Rhman", "Srihari Reddy"], "title": "SIDE: Semantic ID Embedding for effective learning from sequences", "comment": "7 pages, 4 images, 6 tables", "summary": "Sequence-based recommendations models are driving the state-of-the-art for\nindustrial ad-recommendation systems. Such systems typically deal with user\nhistories or sequence lengths ranging in the order of O(10^3) to O(10^4)\nevents. While adding embeddings at this scale is manageable in pre-trained\nmodels, incorporating them into real-time prediction models is challenging due\nto both storage and inference costs. To address this scaling challenge, we\npropose a novel approach that leverages vector quantization (VQ) to inject a\ncompact Semantic ID (SID) as input to the recommendation models instead of a\ncollection of embeddings. Our method builds on recent works of SIDs by\nintroducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ\nfusion that fuses multiple content embeddings and categorical predictions into\na single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding\nconversion technique, called SIDE, that is validated with two content embedding\ncollections, thereby eliminating the need for a large parameterized lookup\ntable; and (iii) a novel quantization method called Discrete-PCA (DPCA) which\ngeneralizes and enhances residual quantization techniques. The proposed\nenhancements when applied to a large-scale industrial ads-recommendation system\nachieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in\ndata footprint compared to traditional SID methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7d27\u51d1\u7684\u8bed\u4e49ID\uff08SID\uff09\u66ff\u4ee3\u4f20\u7edf\u5d4c\u5165\u96c6\u5408\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5b9e\u65f6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5b58\u50a8\u548c\u63a8\u7406\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u5e7f\u544a\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u5904\u7406\u7528\u6237\u5386\u53f2\u6216\u4e8b\u4ef6\u5e8f\u5217\u957f\u5ea6\u5728O(10^3)\u5230O(10^4)\u8303\u56f4\u5185\u7684\u6570\u636e\uff0c\u4f20\u7edf\u5d4c\u5165\u65b9\u6cd5\u5728\u5b9e\u65f6\u9884\u6d4b\u6a21\u578b\u4e2d\u96be\u4ee5\u9ad8\u6548\u5b9e\u73b0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u591a\u4efb\u52a1VQ-VAE\u6846\u67b6\uff08VQ\u878d\u5408\uff09\uff0c\u878d\u5408\u591a\u5185\u5bb9\u5d4c\u5165\u548c\u5206\u7c7b\u9884\u6d4b\u4e3a\u5355\u4e00\u8bed\u4e49ID\uff1b2\uff09\u53c2\u6570\u81ea\u7531\u7684SID\u5230\u5d4c\u5165\u8f6c\u6362\u6280\u672f\uff08SIDE\uff09\uff1b3\uff09\u65b0\u578b\u91cf\u5316\u65b9\u6cd5\u79bb\u6563PCA\uff08DPCA\uff09\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u5e7f\u544a\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e862.4\u500d\u7684\u5f52\u4e00\u5316\u71b5\u589e\u76ca\u63d0\u5347\u548c3\u500d\u7684\u6570\u636e\u5360\u7528\u51cf\u5c11\u3002", "conclusion": "\u901a\u8fc7\u7d27\u51d1\u8bed\u4e49ID\u548c\u65b0\u578b\u91cf\u5316\u6280\u672f\uff0c\u8bba\u6587\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.16840", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16840", "abs": "https://arxiv.org/abs/2506.16840", "authors": ["Zeyneddin Oz", "Shreyas Korde", "Marius Bock", "Kristof Van Laerhoven"], "title": "FedFitTech: A Baseline in Federated Learning for Fitness Tracking", "comment": "This submission includes a total of 7 pages and 6 figures", "summary": "Rapid evolution of sensors and resource-efficient machine learning models\nhave spurred the widespread adoption of wearable fitness tracking devices.\nEquipped with inertial sensors, such devices can continuously capture physical\nmovements for fitness technology (FitTech), enabling applications from sports\noptimization to preventive healthcare. Traditional centralized learning\napproaches to detect fitness activities struggle with privacy concerns,\nregulatory constraints, and communication inefficiencies. In contrast,\nFederated Learning (FL) enables a decentralized model training by communicating\nmodel updates rather than private wearable sensor data. Applying FL to FitTech\npresents unique challenges, such as data imbalance, lack of labelled data,\nheterogeneous user activity patterns, and trade-offs between personalization\nand generalization. To simplify research on FitTech in FL, we present the\nFedFitTech baseline, under the Flower framework, which is publicly available\nand widely used by both industry and academic researchers. Additionally, to\nillustrate its usage, this paper presents a case study that implements a system\nbased on the FedFitTech baseline, incorporating a client-side early stopping\nstrategy and comparing the results. For instance, this system allows wearable\ndevices to optimize the trade-off between capturing common fitness activity\npatterns and preserving individuals' nuances, thereby enhancing both the\nscalability and efficiency of privacy-aware fitness tracking applications.\nResults show that this reduces overall redundant communications by 13 percent,\nwhile maintaining the overall recognition performance at a negligible\nrecognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation\nfor a wide range of new research and development opportunities in FitTech, and\nit is available as open-source at:\nhttps://github.com/adap/flower/tree/main/baselines/fedfittech", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FedFitTech\u57fa\u7ebf\uff0c\u7528\u4e8e\u89e3\u51b3\u53ef\u7a7f\u6234\u8bbe\u5907\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5982\u6570\u636e\u4e0d\u5e73\u8861\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u5728\u5065\u8eab\u6280\u672f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9762\u4e34\u9690\u79c1\u3001\u76d1\u7ba1\u548c\u901a\u4fe1\u6548\u7387\u95ee\u9898\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u8fdb\u884c\u5206\u6563\u5f0f\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u51faFedFitTech\u57fa\u7ebf\uff0c\u5e76\u7ed3\u5408\u5ba2\u6237\u7aef\u65e9\u671f\u505c\u6b62\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u51cf\u5c11\u4e8613%\u7684\u5197\u4f59\u901a\u4fe1\uff0c\u8bc6\u522b\u6027\u80fd\u4ec5\u4e0b\u964d1%\uff0c\u5e73\u8861\u4e86\u901a\u7528\u6027\u548c\u4e2a\u6027\u5316\u3002", "conclusion": "FedFitTech\u4e3a\u5065\u8eab\u6280\u672f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\uff0c\u5e76\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2506.16846", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16846", "abs": "https://arxiv.org/abs/2506.16846", "authors": ["Antonio Consoloa", "Edoardo Amaldi", "Emilio Carrizosa"], "title": "Soft decision trees for survival analysis", "comment": null, "summary": "Decision trees are popular in survival analysis for their interpretability\nand ability to model complex relationships. Survival trees, which predict the\ntiming of singular events using censored historical data, are typically built\nthrough heuristic approaches. Recently, there has been growing interest in\nglobally optimized trees, where the overall tree is trained by minimizing the\nerror function over all its parameters. We propose a new soft survival tree\nmodel (SST), with a soft splitting rule at each branch node, trained via a\nnonlinear optimization formulation amenable to decomposition. Since SSTs\nprovide for every input vector a specific survival function associated to a\nsingle leaf node, they satisfy the conditional computation property and inherit\nthe related benefits. SST and the training formulation combine flexibility with\ninterpretability: any smooth survival function (parametric, semiparametric, or\nnonparametric) estimated through maximum likelihood can be used, and each leaf\nnode of an SST yields a cluster of distinct survival functions which are\nassociated to the data points routed to it. Numerical experiments on 15\nwell-known datasets show that SSTs, with parametric and spline-based\nsemiparametric survival functions, trained using an adaptation of the\nnode-based decomposition algorithm proposed by Consolo et al. (2024) for soft\nregression trees, outperform three benchmark survival trees in terms of four\nwidely-used discrimination and calibration measures. SSTs can also be extended\nto consider group fairness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f6f\u751f\u5b58\u6811\u6a21\u578b\uff08SST\uff09\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u8bad\u7ec3\uff0c\u7ed3\u5408\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u51b3\u7b56\u6811\u5728\u751f\u5b58\u5206\u6790\u4e2d\u56e0\u5176\u53ef\u89e3\u91ca\u6027\u548c\u5efa\u6a21\u590d\u6742\u5173\u7cfb\u7684\u80fd\u529b\u800c\u53d7\u6b22\u8fce\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8f6f\u751f\u5b58\u6811\u6a21\u578b\uff08SST\uff09\uff0c\u91c7\u7528\u8f6f\u5206\u5272\u89c4\u5219\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u8bad\u7ec3\uff0c\u652f\u6301\u591a\u79cd\u751f\u5b58\u51fd\u6570\uff08\u53c2\u6570\u3001\u534a\u53c2\u6570\u3001\u975e\u53c2\u6570\uff09\u3002", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSST\u5728\u5224\u522b\u548c\u6821\u51c6\u6307\u6807\u4e0a\u4f18\u4e8e\u4e09\u79cd\u57fa\u51c6\u751f\u5b58\u6811\u6a21\u578b\u3002", "conclusion": "SST\u7ed3\u5408\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u7fa4\u4f53\u516c\u5e73\u6027\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2506.16855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16855", "abs": "https://arxiv.org/abs/2506.16855", "authors": ["Shaoyu Dou", "Kai Yang", "Yang Jiao", "Chengbo Qiu", "Kui Ren"], "title": "Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning", "comment": "16 pages, 14 figures. Published in IEEE Transactions on Dependable\n  and Secure Computing. arXiv admin note: substantial text overlap with\n  arXiv:2207.08159", "summary": "Time series analysis has achieved great success in cyber security such as\nintrusion detection and device identification. Learning similarities among\nmultiple time series is a crucial problem since it serves as the foundation for\ndownstream analysis. Due to the complex temporal dynamics of the\nevent-triggered time series, it often remains unclear which similarity metric\nis appropriate for security-related tasks, such as anomaly detection and\nclustering. The overarching goal of this paper is to develop an unsupervised\nlearning framework that is capable of learning similarities among a set of\nevent-triggered time series. From the machine learning vantage point, the\nproposed framework harnesses the power of both hierarchical multi-resolution\nsequential autoencoders and the Gaussian Mixture Model (GMM) to effectively\nlearn the low-dimensional representations from the time series. Finally, the\nobtained similarity measure can be easily visualized for the explanation. The\nproposed framework aspires to offer a stepping stone that gives rise to a\nsystematic approach to model and learn similarities among a multitude of\nevent-triggered time series. Through extensive qualitative and quantitative\nexperiments, it is revealed that the proposed method outperforms\nstate-of-the-art methods considerably.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u4e8b\u4ef6\u89e6\u53d1\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u7ed3\u5408\u4e86\u5206\u5c42\u591a\u5206\u8fa8\u7387\u5e8f\u5217\u81ea\u7f16\u7801\u5668\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4e8b\u4ef6\u89e6\u53d1\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u52a8\u6001\u6027\uff0c\u73b0\u6709\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u5728\u5b89\u5168\u76f8\u5173\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u5206\u8fa8\u7387\u5e8f\u5217\u81ea\u7f16\u7801\u5668\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684\u4f4e\u7ef4\u8868\u793a\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5efa\u6a21\u548c\u5b66\u4e60\u4e8b\u4ef6\u89e6\u53d1\u65f6\u95f4\u5e8f\u5217\u76f8\u4f3c\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
