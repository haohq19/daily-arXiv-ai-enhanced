{"id": "2506.21784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21784", "abs": "https://arxiv.org/abs/2506.21784", "authors": ["Yifan Liu", "Xishun Liao", "Haoxuan Ma", "Jonathan Liu", "Rohan Jadhav", "Jiaqi Ma"], "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models", "comment": null, "summary": "Understanding and modeling human mobility patterns is crucial for effective\ntransportation planning and urban development. Despite significant advances in\nmobility research, there remains a critical gap in simulation platforms that\nallow for algorithm development, policy implementation, and comprehensive\nevaluation at scale. Traditional activity-based models require extensive data\ncollection and manual calibration, machine learning approaches struggle with\nadaptation to dynamic conditions, and treding agent-based Large Language Models\n(LLMs) implementations face computational constraints with large-scale\nsimulations. To address these challenges, we propose MobiVerse, a hybrid\nframework leverages the efficiency of lightweight domain-specific generator for\ngenerating base activity chains with the adaptability of LLMs for context-aware\nmodifications. A case study was conducted in Westwood, Los Angeles, where we\nefficiently generated and dynamically adjusted schedules for the whole\npopulation of approximately 53,000 agents on a standard PC. Our experiments\ndemonstrate that MobiVerse successfully enables agents to respond to\nenvironmental feedback, including road closures, large gathering events like\nfootball games, and congestion, through our hybrid framework. Its modular\ndesign facilitates testing various mobility algorithms at both transportation\nsystem and agent levels. Results show our approach maintains computational\nefficiency while enhancing behavioral realism. MobiVerse bridges the gap in\nmobility simulation by providing a customizable platform for mobility systems\nplanning and operations with benchmark algorithms. Code and videos are\navailable at https://github.com/ucla-mobility/MobiVerse.", "AI": {"tldr": "MobiVerse\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9886\u57df\u7279\u5b9a\u751f\u6210\u5668\u548cLLMs\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u548c\u52a8\u6001\u8c03\u6574\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\uff0c\u586b\u8865\u4e86\u5927\u89c4\u6a21\u6a21\u62df\u5e73\u53f0\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u6a21\u62df\u5e73\u53f0\u5728\u7b97\u6cd5\u5f00\u53d1\u3001\u653f\u7b56\u5b9e\u65bd\u548c\u5927\u89c4\u6a21\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f20\u7edf\u65b9\u6cd5\u6570\u636e\u9700\u6c42\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\uff0cLLMs\u5219\u9762\u4e34\u8ba1\u7b97\u9650\u5236\u3002", "method": "\u63d0\u51faMobiVerse\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u751f\u6210\u5668\u751f\u6210\u57fa\u7840\u6d3b\u52a8\u94fe\u548cLLMs\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u6574\uff0c\u652f\u6301\u52a8\u6001\u73af\u5883\u53cd\u9988\u3002", "result": "\u5728\u6d1b\u6749\u77f6Westwood\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6210\u529f\u4e3a53,000\u4e2a\u4ee3\u7406\u751f\u6210\u5e76\u52a8\u6001\u8c03\u6574\u65e5\u7a0b\uff0c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u5e76\u63d0\u5347\u884c\u4e3a\u771f\u5b9e\u6027\u3002", "conclusion": "MobiVerse\u4e3a\u79fb\u52a8\u7cfb\u7edf\u89c4\u5212\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u5b9a\u5236\u5e73\u53f0\uff0c\u586b\u8865\u4e86\u6a21\u62df\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.21655", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21655", "abs": "https://arxiv.org/abs/2506.21655", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e0d\u5bf9\u79f0\u7b56\u7565\u4f18\u5316\uff08APO\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574KL\u6563\u5ea6\u6743\u91cd\u548c\u60e9\u7f5a\u8fc7\u957f\u54cd\u5e94\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u867d\u7136\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8eMLLMs\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51faAPO\u65b9\u6cd5\uff0c\u5c06\u6837\u672c\u5206\u4e3a\u6b63\u8d1f\u4e24\u7ec4\uff1a\u5bf9\u6b63\u6837\u672c\u4f7f\u7528\u96be\u5ea6\u81ea\u9002\u5e94\u6563\u5ea6\u5851\u5f62\uff08DADS\uff09\u52a8\u6001\u8c03\u6574KL\u6563\u5ea6\u6743\u91cd\uff1b\u5bf9\u8d1f\u6837\u672c\u4f7f\u7528\u6b21\u4f18\u8f68\u8ff9\u590d\u6742\u5ea6\u6b63\u5219\u5316\uff08STCR\uff09\u60e9\u7f5a\u8fc7\u957f\u54cd\u5e94\u3002", "result": "\u5728Qwen2.5-VL-3B\u6a21\u578b\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\u540e\uff0c\u63a8\u7406\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u9ad87%\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u66f4\u5927\u7684MLLMs\uff087-11B\uff09\u3002", "conclusion": "APO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2506.21558", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21558", "abs": "https://arxiv.org/abs/2506.21558", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter M\u00fchlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "comment": null, "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BTF\u57fa\u51c6\uff0c\u901a\u8fc7\u5df2\u77e5\u7ed3\u679c\u7684\u201c\u8fc7\u53bb\u9884\u6d4b\u201d\u95ee\u9898\u8bc4\u4f30LLM\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u57fa\u51c6\u7f3a\u4e4f\u771f\u5b9e\u3001\u5c01\u95ed\u4e14\u53ef\u91cd\u590d\u7684\u73af\u5883\uff0c\u96be\u4ee5\u8bc4\u4f30LLM\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5df2\u77e5\u7ed3\u679c\u7684\u201c\u8fc7\u53bb\u9884\u6d4b\u201d\u95ee\u9898\u548c\u76f8\u5173\u7f51\u9875\u8bed\u6599\u5e93\uff0c\u6a21\u62df\u771f\u5b9e\u9884\u6d4b\u573a\u666f\uff0c\u6d4b\u8bd5LLM\u7684\u8868\u73b0\u3002", "result": "BTF\u73af\u5883\u80fd\u591f\u4ea7\u751f\u4e0e\u672a\u89e3\u51b3\u95ee\u9898\u9884\u6d4b\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u5e76\u80fd\u8ffd\u8e2a\u9884\u6d4b\u80fd\u529b\u7684\u8fdb\u6b65\u3002", "conclusion": "BTF\u662f\u4e00\u4e2a\u52a8\u6001\u57fa\u51c6\uff0c\u5c06\u6301\u7eed\u66f4\u65b0\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2506.21724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21724", "abs": "https://arxiv.org/abs/2506.21724", "authors": ["Remco F. Leijenaar", "Hamidreza Kasaei"], "title": "Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning", "comment": "for associated source code, see\n  https://github.com/RFLeijenaar/AsymDSD", "summary": "Learning semantically meaningful representations from unstructured 3D point\nclouds remains a central challenge in computer vision, especially in the\nabsence of large-scale labeled datasets. While masked point modeling (MPM) is\nwidely used in self-supervised 3D learning, its reconstruction-based objective\ncan limit its ability to capture high-level semantics. We propose AsymDSD, an\nAsymmetric Dual Self-Distillation framework that unifies masked modeling and\ninvariance learning through prediction in the latent space rather than the\ninput space. AsymDSD builds on a joint embedding architecture and introduces\nseveral key design choices: an efficient asymmetric setup, disabling attention\nbetween masked queries to prevent shape leakage, multi-mask sampling, and a\npoint cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results\non ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k\nshapes, surpassing prior methods.", "AI": {"tldr": "AsymDSD\u662f\u4e00\u79cd\u81ea\u76d1\u77633D\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u7ed3\u5408\u63a9\u7801\u5efa\u6a21\u548c\u4e0d\u53d8\u6027\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u6807\u7b7e3D\u70b9\u4e91\u6570\u636e\u4e2d\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u63a9\u7801\u70b9\u5efa\u6a21\uff08MPM\uff09\u5728\u9ad8\u5c42\u6b21\u8bed\u4e49\u6355\u6349\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faAsymDSD\u6846\u67b6\uff0c\u91c7\u7528\u975e\u5bf9\u79f0\u53cc\u81ea\u84b8\u998f\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u3001\u591a\u63a9\u7801\u91c7\u6837\u548c\u591a\u88c1\u526a\u70b9\u4e91\u9002\u914d\u3002", "result": "\u5728ScanObjectNN\u4e0a\u8fbe\u523090.53%\u7684\u51c6\u786e\u7387\uff0c\u9884\u8bad\u7ec3\u540e\u63d0\u5347\u81f393.72%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AsymDSD\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u7684\u8bed\u4e49\u8868\u793a\u80fd\u529b\uff0c\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.21742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21742", "abs": "https://arxiv.org/abs/2506.21742", "authors": ["Sirnam Swetha", "Rohit Gupta", "Parth Parag Kulkarni", "David G Shatwell", "Jeffrey A Chan Santiago", "Nyle Siddiqui", "Joseph Fioresi", "Mubarak Shah"], "title": "ImplicitQA: Going beyond frames towards Implicit Video Reasoning", "comment": null, "summary": "Video QA has made significant strides by leveraging multimodal learning to\nalign visual and textual modalities. However, current benchmarks overwhelmingly\nfocus on questions answerable through explicit visual content - actions,\nobjects & events directly observable within individual frames or short clips.\nIn contrast, creative and cinematic videos - such as movies, TV shows, and\nnarrative-driven content - employ storytelling techniques that deliberately\nomit certain depictions, requiring viewers to infer motives, causality, and\nrelationships across discontinuous frames. Humans naturally excel at such\nimplicit reasoning, seamlessly integrating information across time and context\nto construct coherent narratives. Current VideoQA systems and benchmarks fail\nto capture this essential dimension of human-like understanding. To bridge this\ngap, we present ImplicitQA, a novel benchmark specifically designed to test\nmodels on implicit reasoning. It comprises 1K meticulously annotated QA pairs\nderived from 320+ high-quality creative video clips, systematically categorized\ninto key reasoning dimensions: lateral and vertical spatial reasoning, depth\nand proximity, viewpoint and visibility, motion and trajectory, causal and\nmotivational reasoning, social interactions, physical context, and inferred\ncounting. These annotations are deliberately challenging, crafted by authors\nensuring high-quality. Our extensive evaluations on leading VideoQA models\nreveals performance degradation, underscoring their reliance on surface-level\nvisual cues and highlighting the difficulty of implicit reasoning. Performance\nvariations across models further illustrate the complexity and diversity of the\nchallenges presented by ImplicitQA. By releasing both the dataset and our data\ncollection framework, we aim to stimulate further research and development in\nthe community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ImplicitQA\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6d4b\u8bd5\u89c6\u9891\u95ee\u7b54\u4e2d\u9690\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u7406\u89e3\u590d\u6742\u53d9\u4e8b\u5185\u5bb9\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u53ef\u76f4\u63a5\u89c2\u5bdf\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u800c\u5ffd\u7565\u4e86\u9700\u8981\u9690\u5f0f\u63a8\u7406\u7684\u590d\u6742\u53d9\u4e8b\u5185\u5bb9\uff08\u5982\u7535\u5f71\u3001\u7535\u89c6\u5267\uff09\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1K\u9ad8\u8d28\u91cfQA\u5bf9\u7684ImplicitQA\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u9690\u5f0f\u63a8\u7406\u7ef4\u5ea6\uff0c\u5e76\u5bf9\u4e3b\u6d41\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u9690\u5f0f\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f9d\u8d56\u8868\u5c42\u89c6\u89c9\u7ebf\u7d22\u3002", "conclusion": "ImplicitQA\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63a8\u52a8\u89c6\u9891\u95ee\u7b54\u7cfb\u7edf\u5728\u9690\u5f0f\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\u3002"}}
{"id": "2412.15194", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2412.15194", "abs": "https://arxiv.org/abs/2412.15194", "authors": ["Qihao Zhao", "Yangyu Huang", "Tengchao Lv", "Lei Cui", "Qinzheng Sun", "Shaoguang Mao", "Xin Zhang", "Ying Xin", "Qiufeng Yin", "Scarlett Li", "Furu Wei"], "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark", "comment": null, "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMMLU-CF\u7684\u65e0\u6c61\u67d3\u591a\u9009\u95ee\u7b54\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u56e0\u6570\u636e\u6cc4\u9732\u5bfc\u81f4\u7684\u8bc4\u4f30\u4e0d\u53ef\u9760\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MCQ\u6570\u636e\u96c6\uff08\u5982MMLU\uff09\u56e0\u5f00\u6e90\u6027\u548cLLM\u8bad\u7ec3\u6570\u636e\u7684\u5e7f\u6cdb\u6765\u6e90\u5bfc\u81f4\u57fa\u51c6\u6c61\u67d3\uff0c\u8bc4\u4f30\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u4ece\u66f4\u5e7f\u6cdb\u9886\u57df\u83b7\u53d6\u6570\u636e\u5e76\u8bbe\u8ba1\u4e09\u6761\u53bb\u6c61\u67d3\u89c4\u5219\u907f\u514d\u65e0\u610f\u6570\u636e\u6cc4\u9732\uff1b\u5c06\u57fa\u51c6\u5206\u4e3a\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4ee5\u9632\u6b62\u6076\u610f\u6570\u636e\u6cc4\u9732\u3002", "result": "\u4e3b\u6d41LLM\uff08\u5982GPT-4\uff09\u5728\u6d4b\u8bd5\u96c6\u4e0a\u76845-shot\u548c0-shot\u5f97\u5206\u5206\u522b\u4e3a73.4%\u548c71.9%\uff0c\u9a8c\u8bc1\u4e86MMLU-CF\u7684\u4e25\u8c28\u6027\u3002", "conclusion": "MMLU-CF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u4e25\u683c\u4e14\u65e0\u6c61\u67d3\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u51c6\u6c61\u67d3\u95ee\u9898\u3002"}}
{"id": "2506.21952", "categories": ["cs.LG", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.21952", "abs": "https://arxiv.org/abs/2506.21952", "authors": ["Yangyang Wan", "Haotian Wang", "Xuhui Yu", "Jiageng Chen", "Xinyu Fan", "Zuyuan He"], "title": "Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications", "comment": null, "summary": "Distributed acoustic sensing (DAS) has attracted considerable attention\nacross various fields and artificial intelligence (AI) technology plays an\nimportant role in DAS applications to realize event recognition and denoising.\nExisting AI models require real-world data (RWD), whether labeled or not, for\ntraining, which is contradictory to the fact of limited available event data in\nreal-world scenarios. Here, a physics-informed DAS neural network paradigm is\nproposed, which does not need real-world events data for training. By\nphysically modeling target events and the constraints of real world and DAS\nsystem, physical functions are derived to train a generative network for\ngeneration of DAS events data. DAS debackground net is trained by using the\ngenerated DAS events data to eliminate background noise in DAS data. The\neffectiveness of the proposed paradigm is verified in event identification\napplication based on a public dataset of DAS spatiotemporal data and in belt\nconveyor fault monitoring application based on DAS time-frequency data, and\nachieved comparable or better performance than data-driven networks trained\nwith RWD. Owing to the introduction of physical information and capability of\nbackground noise removal, the paradigm demonstrates generalization in same\napplication on different sites. A fault diagnosis accuracy of 91.8% is achieved\nin belt conveyor field with networks which transferred from simulation test\nsite without any fault events data of test site and field for training. The\nproposed paradigm is a prospective solution to address significant obstacles of\ndata acquisition and intense noise in practical DAS applications and explore\nmore potential fields for DAS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684DAS\u795e\u7ecf\u7f51\u7edc\u8303\u5f0f\uff0c\u65e0\u9700\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u751f\u6210\u6570\u636e\uff0c\u5e76\u5728\u53bb\u566a\u548c\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3DAS\u5e94\u7528\u4e2d\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u53bb\u566a\u548c\u4e8b\u4ef6\u8bc6\u522b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u751f\u6210DAS\u4e8b\u4ef6\u6570\u636e\uff0c\u8bad\u7ec3\u751f\u6210\u7f51\u7edc\u548c\u53bb\u80cc\u666f\u566a\u58f0\u7f51\u7edc\u3002", "result": "\u5728\u4e8b\u4ef6\u8bc6\u522b\u548c\u76ae\u5e26\u8f93\u9001\u673a\u6545\u969c\u76d1\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6545\u969c\u8bca\u65ad\u51c6\u786e\u7387\u8fbe91.8%\u3002", "conclusion": "\u8be5\u8303\u5f0f\u4e3a\u89e3\u51b3DAS\u6570\u636e\u83b7\u53d6\u548c\u566a\u58f0\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20893", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20893", "abs": "https://arxiv.org/abs/2506.20893", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "comment": null, "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8f93\u51fa\u91cd\u52a0\u6743\u9057\u5fd8\u65b9\u6cd5RWFT\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u4ece\u5206\u7c7b\u5668\u4e2d\u5220\u9664\u7279\u5b9a\u7c7b\u522b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5728\u9884\u6d4b\u672a\u5b66\u4e60\u7c7b\u522b\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f3a\u5236\u6267\u884c\u7528\u6237\u5220\u9664\u6743\u548c\u51cf\u5c11\u6709\u5bb3\u6216\u504f\u89c1\u9884\u6d4b\u7684\u9700\u6c42\uff0c\u540c\u65f6\u907f\u514d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u9884\u6d4b\u6982\u7387\u8d28\u91cf\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5bf9MIA-NN\u653b\u51fb\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u603b\u53d8\u5dee\u8ddd\u79bb\u7684\u65b0\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRWFT\u5728\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u548c\u65b0\u63d0\u51fa\u7684TV\u8ddd\u79bb\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u5347\u4e862.79%\u548c111.45%\u3002", "conclusion": "RWFT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u76f8\u5f53\u7684\u6548\u679c\u3002"}}
{"id": "2506.21574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21574", "abs": "https://arxiv.org/abs/2506.21574", "authors": ["Yicheng Mao", "Yang Zhao"], "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", "comment": null, "summary": "With globalization and increasing immigrant populations, immigration\ndepartments face significant work-loads and the challenge of ensuring fairness\nin decision-making processes. Integrating artificial intelligence offers a\npromising solution to these challenges. This study investigates the potential\nof large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting\nimmigration decision-making. Utilizing a mixed-methods approach,this paper\nconducted discrete choice experiments and in-depth interviews to study LLM\ndecision-making strategies and whether they are fair. Our findings demonstrate\nthat LLMs can align their decision-making with human strategies, emphasizing\nutility maximization and procedural fairness. Meanwhile, this paper also\nreveals that while ChatGPT has safeguards to prevent unintentional\ndiscrimination, it still exhibits stereotypes and biases concerning nationality\nand shows preferences toward privileged group. This dual analysis highlights\nboth the potential and limitations of LLMs in automating and enhancing\nimmigration decisions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-3.5\u548cGPT-4\uff09\u5728\u79fb\u6c11\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u80fd\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u7b56\u7565\uff0c\u4f46\u4e5f\u5b58\u5728\u504f\u89c1\u3002", "motivation": "\u5168\u7403\u5316\u4e0e\u79fb\u6c11\u589e\u957f\u4f7f\u79fb\u6c11\u90e8\u95e8\u9762\u4e34\u5de8\u5927\u5de5\u4f5c\u91cf\u548c\u516c\u5e73\u51b3\u7b56\u7684\u6311\u6218\uff0c\u4eba\u5de5\u667a\u80fd\u53ef\u80fd\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u79bb\u6563\u9009\u62e9\u5b9e\u9a8c\u548c\u6df1\u5ea6\u8bbf\u8c08\uff0c\u5206\u6790LLM\u7684\u51b3\u7b56\u7b56\u7565\u53ca\u5176\u516c\u5e73\u6027\u3002", "result": "LLM\u80fd\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u7b56\u7565\uff0c\u6ce8\u91cd\u6548\u7528\u6700\u5927\u5316\u548c\u7a0b\u5e8f\u516c\u5e73\uff0c\u4f46\u4ecd\u5b58\u5728\u56fd\u7c4d\u504f\u89c1\u548c\u7279\u6743\u7fa4\u4f53\u504f\u597d\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u5316\u79fb\u6c11\u51b3\u7b56\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e5f\u9700\u6ce8\u610f\u5176\u5c40\u9650\u6027\uff0c\u5982\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2506.21588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21588", "abs": "https://arxiv.org/abs/2506.21588", "authors": ["Ilya Lasy", "Peter Knees", "Stefan Woltran"], "title": "Understanding Verbatim Memorization in LLMs Through Circuit Discovery", "comment": "The First Workshop on Large Language Model Memorization @ ACL 2025,\n  Vienna, August 1st, 2025", "summary": "Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of\ntraining data -- remain poorly understood. What exact part of the network\ndecides to retrieve a token that we would consider as start of memorization\nsequence? How exactly is the models' behaviour different when producing\nmemorized sentence vs non-memorized? In this work we approach these questions\nfrom mechanistic interpretability standpoint by utilizing transformer circuits\n-- the minimal computational subgraphs that perform specific functions within\nthe model. Through carefully constructed contrastive datasets, we identify\npoints where model generation diverges from memorized content and isolate the\nspecific circuits responsible for two distinct aspects of memorization. We find\nthat circuits that initiate memorization can also maintain it once started,\nwhile circuits that only maintain memorization cannot trigger its initiation.\nIntriguingly, memorization prevention mechanisms transfer robustly across\ndifferent text domains, while memorization induction appears more\ncontext-dependent.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8bb0\u5fc6\u5316\u7684\u673a\u5236\uff0c\u901a\u8fc7\u53d8\u538b\u5668\u7535\u8def\u8bc6\u522b\u4e86\u89e6\u53d1\u548c\u7ef4\u6301\u8bb0\u5fc6\u5316\u7684\u4e0d\u540c\u5b50\u7f51\u7edc\uff0c\u5e76\u53d1\u73b0\u8bb0\u5fc6\u5316\u9884\u9632\u673a\u5236\u5177\u6709\u8de8\u6587\u672c\u57df\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7406\u89e3LLMs\u4e2d\u8bb0\u5fc6\u5316\u7684\u5177\u4f53\u673a\u5236\uff0c\u5305\u62ec\u54ea\u4e9b\u7f51\u7edc\u90e8\u5206\u51b3\u5b9a\u8bb0\u5fc6\u5316\u5e8f\u5217\u7684\u5f00\u59cb\uff0c\u4ee5\u53ca\u6a21\u578b\u5728\u751f\u6210\u8bb0\u5fc6\u5316\u548c\u975e\u8bb0\u5fc6\u5316\u5185\u5bb9\u65f6\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "method": "\u4ece\u673a\u5236\u53ef\u89e3\u91ca\u6027\u89d2\u5ea6\u51fa\u53d1\uff0c\u5229\u7528\u53d8\u538b\u5668\u7535\u8def\uff08\u7279\u5b9a\u529f\u80fd\u7684\u6700\u5c0f\u8ba1\u7b97\u5b50\u56fe\uff09\u548c\u5bf9\u6bd4\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u8bb0\u5fc6\u5316\u4e0e\u975e\u8bb0\u5fc6\u5316\u5185\u5bb9\u7684\u5206\u6b67\u70b9\uff0c\u5e76\u5206\u79bb\u51fa\u8d1f\u8d23\u8bb0\u5fc6\u5316\u7684\u7279\u5b9a\u7535\u8def\u3002", "result": "\u53d1\u73b0\u89e6\u53d1\u8bb0\u5fc6\u5316\u7684\u7535\u8def\u4e5f\u80fd\u7ef4\u6301\u8bb0\u5fc6\u5316\uff0c\u800c\u4ec5\u7ef4\u6301\u8bb0\u5fc6\u5316\u7684\u7535\u8def\u65e0\u6cd5\u89e6\u53d1\u5176\u5f00\u59cb\uff1b\u8bb0\u5fc6\u5316\u9884\u9632\u673a\u5236\u5177\u6709\u8de8\u57df\u9c81\u68d2\u6027\uff0c\u800c\u8bb0\u5fc6\u5316\u8bf1\u5bfc\u5219\u66f4\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8bb0\u5fc6\u5316\u5728LLMs\u4e2d\u7684\u5177\u4f53\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u548c\u63a7\u5236\u6a21\u578b\u8bb0\u5fc6\u5316\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.21589", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21589", "abs": "https://arxiv.org/abs/2506.21589", "authors": ["Minjia Mao", "Dongjun Wei", "Xiao Fang", "Michael Chau"], "title": "A General Method for Detecting Information Generated by Large Language Models", "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly\ntransformed the digital information landscape, making it increasingly\nchallenging to distinguish between human-written and LLM-generated content.\nDetecting LLM-generated information is essential for preserving trust on\ndigital platforms (e.g., social media and e-commerce sites) and preventing the\nspread of misinformation, a topic that has garnered significant attention in IS\nresearch. However, current detection methods, which primarily focus on\nidentifying content generated by specific LLMs in known domains, face\nchallenges in generalizing to new (i.e., unseen) LLMs and domains. This\nlimitation reduces their effectiveness in real-world applications, where the\nnumber of LLMs is rapidly multiplying and content spans a vast array of\ndomains. In response, we introduce a general LLM detector (GLD) that combines a\ntwin memory networks design and a theory-guided detection generalization module\nto detect LLM-generated information across unseen LLMs and domains. Using\nreal-world datasets, we conduct extensive empirical evaluations and case\nstudies to demonstrate the superiority of GLD over state-of-the-art detection\nmethods. The study has important academic and practical implications for\ndigital platforms and LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528LLM\u68c0\u6d4b\u5668\uff08GLD\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u672a\u89c1\u8fc7\u7684LLM\u548c\u9886\u57df\u751f\u6210\u7684\u5185\u5bb9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u7684\u666e\u53ca\uff0c\u533a\u5206\u4eba\u7c7b\u64b0\u5199\u548cLLM\u751f\u6210\u7684\u5185\u5bb9\u53d8\u5f97\u56f0\u96be\uff0c\u8fd9\u5bf9\u6570\u5b57\u5e73\u53f0\u7684\u4fe1\u4efb\u548c\u9632\u6b62\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u81f3\u5173\u91cd\u8981\u3002", "method": "GLD\u7ed3\u5408\u4e86\u53cc\u8bb0\u5fc6\u7f51\u7edc\u8bbe\u8ba1\u548c\u7406\u8bba\u6307\u5bfc\u7684\u68c0\u6d4b\u6cdb\u5316\u6a21\u5757\uff0c\u4ee5\u63d0\u5347\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\uff0cGLD\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GLD\u4e3a\u6570\u5b57\u5e73\u53f0\u548cLLM\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b66\u672f\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2506.21783", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21783", "abs": "https://arxiv.org/abs/2506.21783", "authors": ["Alexandru Dumitru", "V Venktesh", "Adam Jatowt", "Avishek Anand"], "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models", "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages", "summary": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTLQA\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5f15\u7528\u5217\u8868\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5b8c\u6574\u7b54\u6848\u751f\u6210\u548c\u65f6\u95f4\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6d89\u53ca\u591a\u5b9e\u4f53\u548c\u65f6\u95f4\u7406\u89e3\u7684\u95ee\u7b54\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u548c\u9519\u8bef\uff0c\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u8bc4\u4f30\u6b64\u7c7b\u80fd\u529b\u3002", "method": "\u63d0\u51faTLQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6a21\u578b\u751f\u6210\u4e0e\u65f6\u95f4\u5468\u671f\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u5217\u8868\u7b54\u6848\uff0c\u5e76\u5728\u95ed\u5377\u548c\u5f00\u653e\u57df\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u95ed\u5377\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u63d0\u4f9b\u5b8c\u6574\u7b54\u6848\u548c\u65f6\u95f4\u5bf9\u9f50\uff0c\u5f00\u653e\u57df\u8bbe\u7f6e\u4e2d\u68c0\u7d22\u80fd\u529b\u6709\u5f85\u63d0\u5347\u3002", "conclusion": "TLQA\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\uff0c\u4ee3\u7801\u548c\u57fa\u51c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.21840", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21840", "abs": "https://arxiv.org/abs/2506.21840", "authors": ["Kourosh Shahnazari", "Mohammadali Keshtparvar", "Seyed Moein Ayyoubzadeh"], "title": "PARSI: Persian Authorship Recognition via Stylometric Integration", "comment": null, "summary": "The intricate linguistic, stylistic, and metrical aspects of Persian\nclassical poetry pose a challenge for computational authorship attribution. In\nthis work, we present a versatile framework to determine authorship among 67\nprominent poets. We employ a multi-input neural framework consisting of a\ntransformer-based language encoder complemented by features addressing the\nsemantic, stylometric, and metrical dimensions of Persian poetry. Our feature\nset encompasses 100-dimensional Word2Vec embeddings, seven stylometric\nmeasures, and categorical encodings of poetic form and meter. We compiled a\nvast corpus of 647,653 verses of the Ganjoor digital collection, validating the\ndata through strict preprocessing and author verification while preserving\npoem-level splitting to prevent overlap. This work employs verse-level\nclassification and majority and weighted voting schemes in evaluation,\nrevealing that weighted voting yields 71% accuracy. We further investigate\nthreshold-based decision filtering, allowing the model to generate highly\nconfident predictions, achieving 97% accuracy at a 0.9 threshold, though at\nlower coverage. Our work focuses on the integration of deep representational\nforms with domain-specific features for improved authorship attribution. The\nresults illustrate the potential of our approach for automated classification\nand the contribution to stylistic analysis, authorship disputes, and general\ncomputational literature research. This research will facilitate further\nresearch on multilingual author attribution, style shift, and generative\nmodeling of Persian poetry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u3001\u98ce\u683c\u548c\u97f5\u5f8b\u7279\u5f81\uff0c\u7528\u4e8e\u6ce2\u65af\u53e4\u5178\u8bd7\u6b4c\u7684\u4f5c\u8005\u5f52\u5c5e\uff0c\u53d6\u5f97\u4e8671%\u7684\u51c6\u786e\u7387\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u65f6\u53ef\u8fbe97%\u3002", "motivation": "\u6ce2\u65af\u53e4\u5178\u8bd7\u6b4c\u7684\u8bed\u8a00\u3001\u98ce\u683c\u548c\u97f5\u5f8b\u590d\u6742\u6027\u5bf9\u8ba1\u7b97\u4f5c\u8005\u5f52\u5c5e\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\u6765\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u591a\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408Transformer\u8bed\u8a00\u7f16\u7801\u5668\u3001Word2Vec\u5d4c\u5165\u3001\u98ce\u683c\u7279\u5f81\u548c\u97f5\u5f8b\u7f16\u7801\uff0c\u57fa\u4e8e647,653\u8282\u8bd7\u53e5\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u52a0\u6743\u6295\u7968\u65b9\u6848\u8fbe\u523071%\u51c6\u786e\u7387\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u9608\u503c\uff080.9\uff09\u4e0b\u51c6\u786e\u7387\u63d0\u5347\u81f397%\uff0c\u4f46\u8986\u76d6\u7387\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6ce2\u65af\u8bd7\u6b4c\u7684\u4f5c\u8005\u5f52\u5c5e\u548c\u98ce\u683c\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u652f\u6301\u591a\u8bed\u8a00\u4f5c\u8005\u5f52\u5c5e\u548c\u751f\u6210\u5efa\u6a21\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.22338", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22338", "abs": "https://arxiv.org/abs/2506.22338", "authors": ["Luigi Russo", "Deodato Tapete", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake", "comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted\n  to IEEE Journal of Selected Topics in Applied Earth Observations and Remote\n  Sensing", "summary": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u65e5\u671f\u9ad8\u5206\u8fa8\u7387SAR\u56fe\u50cf\u548c\u591a\u6e90\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u68c0\u6d4b\u5efa\u7b51\u7269\u635f\u574f\uff0c\u65e0\u9700\u4f9d\u8d56\u707e\u524d\u6570\u636e\u3002", "motivation": "\u707e\u5bb3\u540e\u5feb\u901f\u8bc6\u522b\u5efa\u7b51\u7269\u635f\u574f\u5bf9\u5e94\u6025\u54cd\u5e94\u548c\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5149\u5b66\u536b\u661f\u56fe\u50cf\u5e38\u53d7\u4e91\u5c42\u6216\u7f3a\u4e4f\u707e\u524d\u6570\u636e\u7684\u9650\u5236\u3002", "method": "\u7ed3\u5408SAR\u56fe\u50cf\u3001OSM\u5efa\u7b51\u8f6e\u5ed3\u3001DSM\u6570\u636e\u548cGEM\u7684\u7ed3\u6784\u4e0e\u66b4\u9732\u5c5e\u6027\uff0c\u6784\u5efa\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u707e\u540e\u6570\u636e\u3002", "result": "\u5728\u571f\u8033\u51762023\u5e74\u5730\u9707\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u7ed3\u5408\u5730\u7406\u7a7a\u95f4\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u707e\u524d\u6570\u636e\uff0c\u80fd\u5feb\u901f\u53ef\u9760\u5730\u8bc4\u4f30\u5efa\u7b51\u7269\u635f\u574f\uff0c\u652f\u6301\u707e\u5bb3\u7ba1\u7406\u548c\u6062\u590d\u5de5\u4f5c\u3002"}}
{"id": "2506.22360", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22360", "abs": "https://arxiv.org/abs/2506.22360", "authors": ["Nouf Almesafri", "Hector Figueiredo", "Miguel Arana-Catania"], "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION\n  Forum 2025", "summary": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86CNN\uff08ResNet34\uff09\u548cViT\uff08ViT B16\uff09\u5728\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0ResNet34\u5728\u5206\u7c7b\u7cbe\u5ea6\u4e0a\u7565\u4f18\uff0c\u4f46ViT B16\u5728\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\uff08\u5982\u65e0\u4eba\u673a\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff09\uff0c\u4f46\u76f8\u5173\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u6027\u80fd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528GEN1\u4e8b\u4ef6\u6570\u636e\u96c6\u5bf9ResNet34\u548cViT B16\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u6807\u51c6\u6761\u4ef6\u548c\u6a21\u62df\u566a\u58f0\u4e0b\u8bc4\u4f30\u3002", "result": "ResNet34\u548cViT B16\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a88%\u548c86%\uff0cViT B16\u5728\u566a\u58f0\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65e0\u4eba\u673a\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4e8b\u4ef6\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u3002"}}
