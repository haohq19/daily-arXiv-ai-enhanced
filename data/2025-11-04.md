<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 11]
- [cs.LG](#cs.LG) [Total: 18]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 提出一种高效的迁移学习方法，用于卫星互联网地面终端组件的细粒度天气条件检测，能够检测雪、潮湿等天气相关状况，性能优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 天气事件对低轨卫星互联网性能和可靠性有显著影响，需要细粒度的地面终端组件天气条件检测能力来协助故障诊断和缓解，但目前缺乏有效的解决方案。

Method: 采用高效的迁移学习方法，使地面组件能够本地检测代表性的天气相关条件，包括雪、潮湿等由恶劣和典型天气事件导致的状况。

Result: 该方法在检测雪、潮湿等天气条件方面表现出色，性能优于YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法，并具有良好的泛化能力。

Conclusion: 所提出的迁移学习方法能够有效检测卫星互联网地面终端组件的天气相关条件，具有优越性能和泛化能力，适用于实际部署场景。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [2] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出了一种基于图像-文本对齐度的假图像检测方法ITEM，利用生成图像与描述文本在CLIP空间中的不对齐性作为判别线索，通过全局和局部语义不对齐分析实现鲁棒的假图像检测。


<details>
  <summary>Details</summary>
Motivation: 现有假图像检测方法仅关注视觉线索，容易过拟合特定图像模式且无法泛化到未见过的生成模型。研究发现生成图像与对应描述文本在视觉-语言空间中的对齐度不如真实图像。

Method: 在预训练CLIP空间中测量图像与描述文本的不对齐度，然后训练MLP头进行分类。提出分层不对齐方案：先关注整张图像，再关注描述中的每个语义对象，探索全局和细粒度局部语义不对齐。

Result: 在多个最新生成模型上的实验表明，该方法优于其他最先进方法，具有出色的泛化能力和鲁棒性。

Conclusion: 利用图像-文本不对齐作为判别线索是检测生成假图像的有效方法，该方法简单但效果显著，能够解决现有方法的泛化问题。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [3] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出基于Grad-CAM的弱监督深度学习框架，用于胸部X光肺炎分类和定位，仅需图像级标签即可生成临床有意义的肺炎区域热力图。


<details>
  <summary>Details</summary>
Motivation: 解决传统肺炎诊断方法需要昂贵像素级标注的问题，开发更透明、可信的AI辅助医学影像诊断系统。

Method: 使用七种ImageNet预训练架构在相同训练条件下评估，采用焦点损失和患者级数据分割防止数据泄露，通过Grad-CAM生成解释性热力图。

Result: ResNet-18和EfficientNet-B0达到98%测试准确率，ROC-AUC=0.997，F1=0.987；MobileNet-V2在准确率和计算成本间取得最佳平衡。

Conclusion: 弱监督可解释模型能有效提升肺炎筛查透明度，增强临床对AI辅助医学影像的信任度。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [4] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了T-VAD框架，基于大型视觉语言模型，通过异常热图解码器和区域感知异常编码器实现细粒度视频异常检测，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法输出有限（仅正常或异常），且多为半自动化需要人工评估，需要提升检测的细粒度和交互性。

Method: 构建基于LVLM的T-VAD框架，包含异常热图解码器进行像素级视觉-文本特征对齐生成细粒度热图，以及区域感知异常编码器将热图转换为可学习文本嵌入来指导LVLM。

Result: 在UBnormal数据集上达到94.8% AUC，异常热图精度67.8%/76.7%；在ShanghaiTech和UBnormal数据集上文本描述质量显著提升，BLEU-4和Yes/No准确率均表现优异。

Conclusion: T-VAD显著提升了视频异常检测的细粒度和交互性，实现了更精确的异常定位和描述，在多个指标上达到最先进水平。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [5] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 提出了一种基于记忆增强的上下文感知零样本异常检测方法，通过跨注意力机制融合时序特征和视觉嵌入，实现实时零样本异常分类。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测器缺乏对上下文信息的感知能力，无法识别在不同上下文中正常/异常行为的差异，限制了在真实场景中的泛化能力。

Method: 采用记忆增强管道，通过跨注意力机制关联时序信号与视觉嵌入，并使用上下文相似性评分进行实时零样本异常分类。

Result: 在UCF-Crime数据集上达到90.4% AUC，在XD-Violence数据集上达到83.67% AP，创下零样本模型的新SOTA，并实现实时推理。

Conclusion: 通过融合跨注意力时序融合和上下文记忆，实现了高保真度的异常检测，为零样本模型在真实世界监控和基础设施监测中的应用迈出了重要一步。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [6] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench是首个专注于上下文感知视频异常理解的基准测试，通过统一评估框架和事件中心层次分类法，系统评估现有模型在复杂真实世界异常理解上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常理解方法对真实世界异常的理解较为肤浅，缺乏对复杂原理和微妙上下文的理解能力，需要更全面的评估基准。

Method: 提出CueBench基准，建立事件中心层次分类法，包含14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性。同时开发Cue-R1模型，基于R1风格强化微调，使用可验证、任务对齐和层次细化的奖励。

Result: 实验结果显示现有视觉语言模型在真实世界异常理解上表现不佳，而Cue-R1模型在CueBench上平均超越现有最优方法超过24%。

Conclusion: 现有模型距离真实世界视频异常理解仍有很大差距，CueBench为系统评估提供了重要基准，Cue-R1展示了显著改进潜力。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [7] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出了一种用于生成水下环境中AUV搭载事件相机合成数据的管道，用于训练视觉模型，并在岩石检测任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光照条件差、高动态范围场景等挑战，传统视觉技术难以适应。事件相机通过逐帧追踪变化可缓解这些问题，但缺乏真实数据。

Method: 开发了一个生成事件相机合成数据的管道，模拟AUV在水下环境中的场景，特别针对能见度差和悬浮颗粒物的情况。

Result: 在岩石检测任务中验证了管道的有效性，该方法可推广到其他水下任务。

Conclusion: 该合成数据生成管道为水下事件相机视觉模型训练提供了有效解决方案。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [8] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一个统一的推理框架，通过链式测试时推理过程连接时间检测、空间定位和文本解释，实现零样本视频异常分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常研究大多停留在帧级检测，缺乏空间和语义上下文，无法解释异常原因。现有方法虽然提高了可解释性，但仍依赖数据和特定任务。

Method: 基于链式测试时推理过程，通过任务内推理细化时间检测，任务间链接实现空间和语义理解，无需额外训练。

Result: 在多个视频异常检测、定位和解释基准测试中实现了最先进的零样本性能，无需额外数据或梯度更新。

Conclusion: 精心设计的提示与任务链式连接能够释放基础模型的推理能力，实现实用、可解释的零样本视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [9] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream实现了亚秒级延迟的视频生成，通过将双向教师模型蒸馏为因果学生模型，结合滑动窗口因果注意力和注意力汇，支持实时无限长视频流生成。


<details>
  <summary>Details</summary>
Motivation: 现有运动条件视频生成方法存在延迟高（分钟级）和非因果处理的问题，无法实现实时交互。

Method: 通过自强制分布匹配蒸馏将双向教师模型蒸馏为因果学生模型，引入滑动窗口因果注意力和注意力汇，结合自展开和KV缓存滚动训练。

Result: 在单个GPU上实现最高29 FPS的流式生成，运动跟随和视频质量达到最先进水平，速度快两个数量级。

Conclusion: MotionStream首次实现了无限长度实时视频流生成，用户可通过绘制轨迹、控制相机或传输运动获得真正的交互体验。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [10] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出了APDM框架，通过将保护目标从图像转移到扩散模型本身来防止特定主体的个性化，包含DPO损失函数和L2P双路径优化策略，有效阻止未经授权的个性化生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高质量特定主体合成能力带来了隐私风险，现有基于对抗样本的方法在少量干净图像或简单图像变换下失效，需要更鲁棒的保护方法。

Method: 提出了APDM框架，包含Direct Protective Optimization (DPO)损失函数和Learning to Protect (L2P)双路径优化策略，通过交替进行个性化和保护路径来模拟未来个性化轨迹并自适应强化保护。

Result: 实验结果表明该框架优于现有方法，在防止未经授权个性化方面达到了最先进的性能。

Conclusion: APDM框架通过模型层面的保护而非图像层面的扰动，有效解决了扩散模型个性化带来的隐私风险，提供了更鲁棒的保护方案。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [11] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: UniSOT是一个统一的目标跟踪器，能够处理三种参考模态（边界框、自然语言或两者）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件）的不同组合，使用统一参数实现跨模态跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常针对单一或少数几种视频模态和参考模态设计，导致模型分离且限制实际应用。需要一个统一的跟踪器来处理各种需求，但目前尚无能够同时处理上述所有参考模态和视频模态的跟踪器。

Method: 提出了UniSOT统一跟踪器，通过统一参数设计，支持三种参考模态（边界框、自然语言或两者）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件）的不同组合。

Result: 在18个视觉跟踪、视觉语言跟踪和RGB+X跟踪基准测试中，UniSOT相比模态特定对应方法表现出优越性能。在TNL2K上所有三种参考模态上比先前方法提升超过3.0% AUC，在Un-Track上所有三种RGB+X视频模态上主要指标提升超过2.0%。

Conclusion: UniSOT证明了统一跟踪器在处理多种参考模态和视频模态组合方面的有效性和优越性，为实际应用提供了更灵活的解决方案。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了DynBERG模型，将Graph-BERT与GRU结合用于动态金融欺诈检测，支持有向边和时间演化分析，在比特币交易数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-BERT模型主要针对静态无向图设计，而金融交易网络是动态有向的，需要能够捕捉时间演化和资金流向的模型。

Method: 结合Graph-BERT与GRU层，支持有向边，捕捉多时间步长的时序演化，专门针对动态金融交易网络设计。

Result: 在Elliptic比特币数据集上评估，在Dark Market Shutdown事件前后均优于EvolveGCN和GCN，消融实验证明GRU组件对时序建模至关重要。

Conclusion: DynBERG能有效适应重大市场变化，为动态金融欺诈检测提供了更强大的解决方案，时序深度学习组件对性能提升有显著贡献。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [13] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了GraphKeeper方法来解决图域增量学习中的灾难性遗忘问题，通过知识解耦和保持来应对嵌入偏移和决策边界偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的图增量学习方法主要关注单域内的任务增量和类别增量场景，而图域增量学习（Domain-IL）在多个图域间更新模型的需求随着图基础模型的发展变得至关重要，但该领域在文献中尚未被探索。

Method: GraphKeeper方法包括：1）域特定参数高效微调结合域内和域间解耦目标来防止嵌入偏移；2）无偏差知识保持来维持稳定决策边界；3）对于不可观测域的图，执行域感知分布判别以获得精确嵌入。

Result: 大量实验表明，GraphKeeper取得了最先进的结果，相比第二名有6.5%~16.6%的提升，且遗忘效应可忽略不计。该方法能与各种代表性图基础模型无缝集成。

Conclusion: GraphKeeper通过知识解耦和保持有效解决了图域增量学习中的灾难性遗忘问题，展现了广泛的应用潜力。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [14] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 开发了一个机器学习模型，使用XGBoost分类器预测足球中的防线突破事件，模型基于球员位置、速度和空间配置等特征，在J1联赛数据上表现出高准确度。


<details>
  <summary>Details</summary>
Motivation: 足球中突破对手防线是创造得分机会的关键战术行为，但以往研究主要关注射门或进球机会，缺乏对防线突破机制的系统分析。

Method: 使用2023年J1联赛的事件和追踪数据，构建包含189个特征的XGBoost分类器模型，特征包括球员位置、速度和空间配置等。

Result: 模型预测准确度很高，AUC达到0.982，Brier分数为0.015。SHAP分析显示进攻球员速度、防线空隙和进攻球员空间分布是重要影响因素。

Conclusion: 防线突破与得分机会创造密切相关，该研究为理解足球战术动态提供了量化框架。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [15] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 使用统计物理中的交叉涨落分析扩散模型的采样动态，发现样本经历尖锐的离散转变，这些转变可被检测并用于提升采样效率、加速条件生成和改善零样本任务。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型中采样动态的演变过程，特别是样本从初始分布到目标分布的转变机制，以理解生成过程中的结构形成。

Method: 利用交叉涨落这一中心矩统计量分析采样动态，推导方差保持SDE的交叉涨落闭式解，检测样本转变过程。

Result: 发现样本经历离散转变，这些转变可被检测并用于提升采样效率20-30%，加速条件生成，改善图像分类和风格迁移等零样本任务。

Conclusion: 该框架统一了离散马尔可夫链理论与连续动力学，将经典耦合和混合概念扩展到随机SDE和非马尔可夫采样器，为生成建模提供了新视角。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [16] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 本文旨在设计和评估多种机器学习模型来预测PM2.5浓度水平，包括线性回归、集成方法、深度学习模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: PM2.5空气污染对健康造成严重影响，预测未来PM2.5水平可以提供早期预警并帮助预防疾病。

Method: 设计、微调、测试和评估多种机器学习模型，包括线性回归算法、集成方法、深度学习模型（如循环神经网络和变换器）以及大型语言模型。

Result: 论文评估和比较了这些模型在PM2.5预测任务上的性能表现。

Conclusion: 通过系统比较多种机器学习方法，为PM2.5浓度预测提供了有效的模型评估框架。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [17] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出了一种基于单智能体强化学习的区域自适应交通信号控制框架，使用DreamerV3世界模型和邻接矩阵统一编码路网拓扑、实时排队状态和信号配时参数，在SUMO仿真中显著降低了排队长度并表现出抗波动能力。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵严重影响城市生活质量和经济效率，传统交通信号控制模型难以捕捉真实交通复杂性，多智能体系统存在协调复杂性问题。

Method: 采用单智能体强化学习框架，通过邻接矩阵统一编码路网拓扑、实时排队状态和信号配时参数，利用DreamerV3世界模型学习控制策略，动作序列选择交叉口并调整信号相位配时。

Result: 在SUMO仿真实验中，面对10%-30%的起讫点需求波动，该框架表现出强大的抗波动能力，并显著降低了排队长度。

Conclusion: 该研究建立了与探测车辆技术兼容的智能交通控制新范式，未来将关注在训练中纳入随机需求波动和探索区域应急优化机制。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [18] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出了一个统一的数据驱动框架，通过五个阶段量化并提升冰球比赛中的进攻势头和得分可能性（预期进球xG），结果显示采用优化的事件序列和阵型能显著提高15%的得分潜力。


<details>
  <summary>Details</summary>
Motivation: 开发一个端到端的分析框架，为冰球教练和分析师提供实时、可操作的战术优化见解，推动冰球分析向基于因果关系的战术优化发展。

Method: 五阶段流程：1) 逻辑回归进行可解释的势头加权；2) 梯度提升决策树进行非线性xG估计；3) LSTM网络进行时序建模；4) PCA和K-Means聚类发现空间阵型；5) X-Learner因果推断估计器量化最优事件序列和阵型的平均处理效应。

Result: 观察到平均处理效应为0.12（95%置信区间：0.05-0.17，p < 1e-50），对应得分潜力相对提升15%，证明策略性结构化的序列和紧凑阵型能因果性地提升进攻表现。

Conclusion: 该框架为教练和分析师提供了实时、可操作的见解，将冰球分析推向基于原则和因果基础的战术优化。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [19] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 该论文提出了一种融合卫星和雷达数据的多模态临近预报模型，用于预测5、15和30分钟内的降水，特别针对强降雨事件。实验表明该方法显著优于仅使用雷达的方法，能提供更准确详细的预报。


<details>
  <summary>Details</summary>
Motivation: 传统地面传感器对城市强降雨事件的监测能力有限（德国2001-2018年间仅17.3%的强降雨被记录），雷达数据单独预报强降雨存在挑战，需要更有效的预报方法来应对日益频繁的强降雨事件。

Method: 开发多模态临近预报模型，结合雷达和卫星图像数据，预测5、15和30分钟内的降水情况。

Result: 多模态方法显著优于仅使用雷达的方法，在5分钟预报时间内，强降雨和暴雨的临界成功指数分别提高4%和3%，且在更长时间尺度上保持更高的预测能力。

Conclusion: 融合卫星和雷达数据的多模态模型能提供更准确详细的强降雨预报，有助于及时可靠的预警，具有重要的生命安全保障价值。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [20] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 该论文分析了预训练Transformer在微调时出现的输出饱和和梯度抑制问题，提出了诊断指标来识别拐点层，并通过在拐点层选择性注入LoRA适配器来恢复梯度信号，实验验证了不同初始化条件下该策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在微调时经常表现出对源域模式的过度自信，难以形成新的目标域模式，这源于输出饱和导致的梯度抑制问题。

Method: 通过交叉熵和softmax分析形式化输出饱和机制，提出层间诊断指标（注意力熵、激活梯度范数、参数梯度范数、Delta-CKA），并在拐点层选择性注入LoRA适配器。

Result: 实验表明，在过训练初始化条件下，拐点层LoRA注入能提升性能；而在欠训练初始化条件下，则需要全路径解阻塞才能实现低层重构。

Conclusion: 该研究揭示了Transformer微调中的梯度抑制机制，提出了诊断优先、轻量注入的微调策略，为不同初始化条件下的适配提供了指导。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [21] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 开发了一个用于油气管道段塞流检测的交互式应用，集成了数据探索、标注、模型训练、可视化和实时推理功能，通过用户友好界面实现端到端的数据驱动检测。


<details>
  <summary>Details</summary>
Motivation: 现有段塞流检测方法通常需要离线操作、依赖领域专家且缺乏实时可解释性，难以满足工业实时监测需求。

Method: 构建交互式应用，包含数据探索标注、可配置模型训练评估、分类结果可视化（时间序列叠加显示）和实时推理模块（基于持久性警报），支持从CSV上传到实时推理的无缝工作流。

Result: 开发出轻量级、便携且易于部署的工具，结合领域相关分析和创新UI/UX功能（快照持久化、可视化标注、实时警报），既可作为研究原型又具有实际工业应用价值。

Conclusion: 交互式人机协同机器学习系统能够弥合数据科学方法与关键过程工业中实际决策之间的差距，在油气领域之外的时序故障诊断任务中具有更广泛适用性。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [22] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种新颖的安全强化学习算法，结合了基于Kronecker分解近似曲率的二阶策略优化和安全感知梯度操作，在保持安全约束的同时实现了更高的平均回报。


<details>
  <summary>Details</summary>
Motivation: 为了解决奖励最大化与约束满足之间的权衡问题，需要开发既能高效优化策略又能确保安全约束的算法，避免传统方法中固定硬阈值导致的性能损失。

Method: 使用K-FAC近似Fisher信息矩阵进行高效稳定的自然梯度更新；引入边缘感知梯度操作机制，根据代理与安全边界的接近程度自适应调整奖励和成本梯度的影响；采用小批量KL回滚策略确保信任区域合规性。

Result: 在Safety Gymnasium环境中的实验表明，KFCPO相比最佳基线方法实现了10.3%到50.2%的平均回报提升，同时保持了安全约束。

Conclusion: KFCPO通过结合二阶优化和自适应梯度操作，在安全强化学习中实现了安全性和性能的优越平衡，为复杂环境中的安全策略优化提供了有效解决方案。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [23] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 开发基于SARIMAX的短期停电预测系统，通过两阶段特征工程和稳健优化策略，在极端天气事件中实现比基线方法提升8.4%的预测性能


<details>
  <summary>Details</summary>
Motivation: 解决极端天气事件期间短期电力中断预测的挑战，提高预测准确性和可靠性

Method: 采用两阶段特征工程（数据清理+相关性过滤），结合时间嵌入、多尺度滞后特征和天气变量作为外生输入，使用SARIMAX模型并实施分层拟合策略和稳健优化方法

Result: 在24小时和48小时预测范围内，模型达到RMSE 177.2，相比基线方法（RMSE 193.4）提升8.4%

Conclusion: 所提出的特征工程和稳健优化策略对极端天气相关停电预测有效，证明了方法的实用价值

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [24] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习和基于代理模拟的新框架，用于优化电动汽车充电站布局，通过混合奖励函数和双Q网络来应对现实世界的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在充电站布局优化中面临确定性奖励系统的限制，无法充分捕捉现实世界的动态和不确定性，导致评估成本高且不反映真实场景。

Method: 集成深度强化学习与基于代理的模拟，使用具有双Q网络的混合RL代理来选择最优位置和配置充电端口，采用结合确定性因素和模拟反馈的混合奖励函数。

Result: 在越南河内的案例研究中，该方法相比初始状态将平均等待时间减少了53.28%，优于静态基线方法。

Conclusion: 该可扩展的自适应解决方案增强了电动汽车基础设施规划，有效应对现实世界的复杂性并改善用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [25] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: STAN是一种对抗性时空注意力网络，用于从多变量EEG信号预测癫痫发作，通过级联注意力块联合建模空间大脑连接性和时间神经动态，实现高灵敏度和低误报率。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测在医疗时间序列预测中面临关键挑战，需要高灵敏度、低误报率和受试者特异性适应性。现有方法假设固定的发作前持续时间或分别处理空间和时间特征，无法有效捕捉时空模式的双向依赖关系。

Method: 提出STAN网络，通过级联注意力块交替使用空间和时间模块，联合建模空间大脑连接性和时间神经动态。采用带梯度惩罚的对抗训练，从明确定义的15分钟发作前窗口学习发作间期和发作前状态的稳健区分。

Result: 在两个基准EEG数据集上实现最先进性能：CHB-MIT头皮数据集96.6%灵敏度，0.011次/小时误报；MSSM颅内数据集94.2%灵敏度，0.063次/小时误报。计算效率高（230万参数，45毫秒延迟，180MB内存），适合实时边缘部署。

Conclusion: STAN框架不仅为癫痫发作预测提供了有效解决方案，还为医疗和其他时间序列领域的时空预测提供了一般范式，其中个体异质性和可解释性至关重要。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [26] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 该论文探索使用数据驱动的Koopman方法来解决无人机自组网(FANETs)中的动态环境挑战，通过集中式和分布式两种方法预测无人机轨迹动态和通信质量，从而提高网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法主要针对静态无线环境，在高度动态的FANETs环境中应用受限。需要开发能够适应网络拓扑不断变化的新方法。

Method: 利用Koopman算子理论，提出集中式和分布式两种数据驱动方法，建模无人机轨迹动态，预测信号干扰噪声比(SINR)来确保无人机间可靠通信。

Result: 结果显示这些方法能够准确预测导致通信中断的连接和隔离事件，为无人机基于预测调度传输提供可能。

Conclusion: 数据驱动的Koopman方法能够有效解决FANETs动态环境中的通信挑战，提高网络性能和可靠性。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [27] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 提出MiniFool算法，一种基于物理启发的对抗攻击方法，用于测试粒子物理和天体粒子物理中的神经网络分类任务，并在MNIST和CMS实验数据上验证其通用性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够考虑实验不确定性的对抗攻击方法，以测试神经网络在物理数据分析中的鲁棒性，特别是在IceCube中微子观测站的天体物理tau中微子搜索等应用中。

Method: 基于最小化成本函数的方法，结合χ²检验统计量和目标分数偏差，通过扰动数据并量化基于实验不确定性的扰动概率。

Result: 发现正确分类和错误分类事件的分类翻转概率不同，通过调节攻击参数（实验不确定性缩放因子）可以量化网络决策的鲁棒性。

Conclusion: MiniFool算法能够有效测试神经网络分类器的鲁棒性，特别适用于未标记实验数据的分类稳健性评估，具有跨领域应用的通用性。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [28] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: CLP-SNN是一种基于脉冲神经网络的在线持续学习架构，在Intel Loihi 2芯片上实现，通过事件驱动稀疏学习、自归一化学习规则和神经发生机制，在OpenLORIS数据集上实现与回放方法相当的精度，同时获得70倍速度提升和5600倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备在开放世界中面临的数据分布漂移和新类出现的问题，克服传统离线训练的局限性，实现在功率受限环境下的在线持续学习。

Method: 提出三种创新：事件驱动时空稀疏局部学习、自归一化三因子学习规则保持权重归一化、集成神经发生和元可塑性实现容量扩展和遗忘缓解。

Result: 在OpenLORIS少样本学习实验中，CLP-SNN达到与回放方法竞争的精度，同时实现70倍速度提升（0.33ms vs 23.2ms）和5600倍能效提升（0.05mJ vs 281mJ）。

Conclusion: 脑启发算法与神经形态硬件的协同设计能够打破传统精度-效率权衡，为未来边缘AI系统提供突破性解决方案。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [29] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 论文提出了时间特征分析(Temporal Feature Analysis)方法，通过引入时间归纳偏置来分解语言模型表示，解决了现有稀疏自编码器(SAEs)在处理语言时序动态结构时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法假设概念是独立方向，但语言模型表示具有丰富的时间动态特性(如概念维度增长、上下文相关相关性、非平稳性)，这与SAEs的独立性先验相冲突。

Method: 引入时间特征分析，将给定时间的表示分解为两部分：可预测部分(可从上下文推断)和残差部分(捕获上下文无法解释的新信息)。

Result: 时间特征分析器能正确解析花园路径句子、识别事件边界，并更广泛地区分抽象慢变信息和新颖快变信息，而现有SAEs在上述任务中均存在显著缺陷。

Conclusion: 结果表明设计稳健的可解释性工具需要与数据匹配的归纳偏置。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本研究首次将RLVR LLM训练应用于公共交通运营中的实时预测挑战，通过引入基于容差的奖励函数来适应连续预测任务，在NYC MTA服务警报数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调方法难以处理领域稀疏性和噪声连续标签的问题，而RLVR虽然在数学推理等二元正确性任务中表现出色，但在噪声连续预测中的应用仍是一个开放性问题。

Method: 通过引入基于容差的奖励函数，在连续误差范围内给予部分信用，而不是要求单一正确答案，将RLVR适应于该任务，并在NYC MTA服务警报数据集上进行系统评估。

Result: 通用指令调优LLM显著优于专门的数学推理模型，后者在处理模糊的现实世界文本时表现不佳。RLVR方法在最具挑战性的指标上表现优异，相比最强基线在5分钟准确率上实现了35%的相对改进。

Conclusion: RLVR可以成功适应现实世界中的噪声预测任务，但需要设计反映问题连续性质的验证器，二元奖励不稳定且会降低性能，而设计的奖励函数至关重要。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [31] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘反复发作，LGBM模型表现最佳，相比现有决策规则有显著改进


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复发作是常见但可预防的问题，通过电子病历数据开发机器学习算法可以准确识别高风险儿童并转诊进行预防性综合护理

Method: 使用回顾性电子病历数据训练多种机器学习模型（包括LGBM、XGB和三种大型语言模型），结合环境污染物暴露和社区边缘化信息，在COVID19前后数据集上进行验证

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型的AUC为0.712，F1分数为0.51，显著优于现有决策规则（F1=0.334）

Conclusion: 机器学习模型能够有效预测儿童哮喘反复发作风险，为高风险儿童提供预防性护理转诊提供了可行工具

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [32] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题，在任务成功率、分解效率和协作平衡等方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单智能体在复杂任务执行中任务分解和协作能力的局限性，提升多智能体系统在复杂环境下的任务执行效率。

Method: 使用大语言模型将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，通过动态调度和路由机制实现智能体间的合理分工和实时协作，并设计约束解析和全局一致性机制确保子任务连贯性和负载均衡。

Result: 实验验证表明，该方法在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度均优于现有方法，在整体性能和鲁棒性方面表现更佳，实现了任务复杂度和通信开销之间的更好平衡。

Conclusion: 研究证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境下的任务执行提供了系统性解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [33] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM是一种新型语言模型架构，通过外部可读知识库解决大语言模型的知识陈旧性和不可解释性问题，实现了知识的直接检查和更新。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在知识陈旧和缺乏可解释性的问题，因为知识隐式存储在纠缠的网络参数中，无法进行针对性更新和推理透明化。

Method: 设计了包含百万级外部知识库的架构，使用可读的token序列存储知识；采用两阶段可微分检索机制，通过产品键分解进行粗粒度过滤，Gumbel-Softmax进行细粒度匹配；基于双系统认知理论，将知识分为冻结的显性事实(20%)和可学习的隐性模式(80%)。

Result: 在知识密集型任务上比标准Transformer提升43.67%，在低数据场景(1万样本)下获得3.62倍增益；正确预测的命中率比错误预测高49%。

Conclusion: 联合优化的架构证明可解释、可更新的模型在保持竞争力的同时，能够提供前所未有的知识透明度，优于使用固定检索的RAG系统。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [34] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [35] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: 提出了一种将折纸结构与针织面料相结合的新制造方法，通过编程针迹和材料图案来创建具有可控折叠方向的针织折纸结构，用于可穿戴机器人应用。


<details>
  <summary>Details</summary>
Motivation: 软机器人使用柔性材料为可穿戴设备提供了舒适性和安全性，但实现结构完整性和舒适性仍然是一个挑战。需要结合折纸结构的优势与针织面料的材料可编程性和可穿戴性。

Method: 开发了一种通用设计方法，通过编程针迹图案和选择性加入热熔纱线来将折纸图案转化为针织设计。热熔纱线在褶皱周围形成刚性面板，同时保持柔性褶皱。

Result: 实验量化了折叠力矩，证明针迹图案增强了折叠方向性，热熔纱线减少了边缘卷曲并防止平面外变形。成功复制了复杂的折纸镶嵌图案，并展示了可穿戴针织万花筒循环机器人的运动能力。

Conclusion: 针织折纸结合了结构可重构性、材料可编程性和制造可扩展性，为下一代可穿戴机器人提供了一个有前景的平台。

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [36] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM是一个将预训练视觉语言模型适配到空中机械臂控制的框架，通过分离高层推理和底层控制来解决直接部署VLM策略的安全性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 直接部署VLM驱动策略到空中机械臂存在不安全、不可靠的问题，因为生成的动作往往不一致、容易产生幻觉，且对飞行来说动态不可行。

Method: 将自然语言指令、任务上下文和安全约束编码为结构化提示，引导模型生成逐步推理轨迹，然后从预定义的离散、飞行安全技能库中选择执行。

Result: 在仿真和硬件上验证了多种多步骤拾放任务，展示了对未见指令、物体和环境的强泛化能力。

Conclusion: 通过解耦符号推理和物理动作，AERMANI-VLM减轻了幻觉命令并防止不安全行为，实现了鲁棒的任务完成。

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [37] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 提出Phy-Tac方法，通过物理条件触觉实现力最优稳定抓取，结合姿态选择、触觉预测和力调节，使机器人能够像人类一样用最小必要力稳定抓取物体。


<details>
  <summary>Details</summary>
Motivation: 人类自然抓取时使用最小必要力保持稳定，而机器人通常依赖刚性、过度挤压的控制。为了缩小这一差距，需要开发更智能的力控制方法。

Method: 1) 基于物理的姿态选择器识别最优力分布的可行接触区域；2) 物理条件潜在扩散模型预测FOSG目标下的触觉印记；3) 潜在空间LQR控制器以最小驱动将夹爪导向该触觉印记。

Result: 在多样化物体和接触条件下的物理条件触觉数据集上训练，Phy-LDM实现了优越的触觉预测精度，Phy-Tac在抓取稳定性和力效率方面优于固定力和基于GraspNet的基线方法。

Conclusion: 该方法在经典机器人平台上展示了力高效和自适应操作，缩小了机器人与人类抓取之间的差距。

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>
