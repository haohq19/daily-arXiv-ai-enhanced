{"id": "2512.03055", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03055", "abs": "https://arxiv.org/abs/2512.03055", "authors": ["Xiaowu Sun", "Thabo Mahendiran", "Ortal Senouf", "Denise Auberson", "Bernard De Bruyne", "Stephane Fournier", "Olivier Muller", "Pascal Frossard", "Emmanuel Abbe", "Dorina Thanou"], "title": "Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins", "comment": "19 pages", "summary": "Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.", "AI": {"tldr": "PINS-CAD\uff1a\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u51a0\u72b6\u52a8\u8109\u538b\u529b\u548c\u8840\u6d41\uff0c\u65e0\u9700CFD\u6216\u6807\u6ce8\u6570\u636e\uff0c\u5728\u4e34\u5e8a\u6570\u636e\u4e0a\u5fae\u8c03\u540e\u80fd\u6709\u6548\u9884\u6d4b\u5fc3\u8840\u7ba1\u4e8b\u4ef6", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u51a0\u72b6\u52a8\u8109\u75be\u75c5\uff08CAD\uff09\u662f\u6700\u5e38\u89c1\u5f62\u5f0f\uff0c\u9700\u8981\u65e9\u671f\u98ce\u9669\u9884\u6d4b\u3002\u4f20\u7edf3D\u51a0\u72b6\u52a8\u8109\u6570\u5b57\u5b6a\u751f\u5206\u6790\u4f9d\u8d56\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\uff0c\u8ba1\u7b97\u91cf\u5927\u96be\u4ee5\u6269\u5c55\uff1b\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u7f3a\u4e4f\u751f\u7406\u5b66\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u63d0\u51faPINS-CAD\u6846\u67b6\uff1a1\uff09\u572820\u4e07\u4e2a\u5408\u6210\u7684\u51a0\u72b6\u52a8\u8109\u6570\u5b57\u5b6a\u751f\u4e0a\u9884\u8bad\u7ec3\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u57fa\u4e8e1D Navier-Stokes\u65b9\u7a0b\u548c\u538b\u964d\u5b9a\u5f8b\u9884\u6d4b\u538b\u529b\u548c\u8840\u6d41\uff0c\u65e0\u9700CFD\u6216\u6807\u6ce8\u6570\u636e\uff1b2\uff09\u5728FAME2\u591a\u4e2d\u5fc3\u7814\u7a76\u7684635\u540d\u60a3\u8005\u4e34\u5e8a\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "1\uff09\u9884\u6d4b\u672a\u6765\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u7684AUC\u8fbe\u52300.73\uff0c\u4f18\u4e8e\u4e34\u5e8a\u98ce\u9669\u8bc4\u5206\u548c\u6570\u636e\u9a71\u52a8\u57fa\u7ebf\u65b9\u6cd5\uff1b2\uff09\u7269\u7406\u4fe1\u606f\u9884\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u4ea7\u751f\u751f\u7406\u5b66\u6709\u610f\u4e49\u7684\u8868\u793a\uff1b3\uff09\u80fd\u751f\u6210\u7a7a\u95f4\u5206\u8fa8\u7684\u538b\u529b\u548c\u8840\u6d41\u50a8\u5907\u5206\u6570\u66f2\u7ebf\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7269\u7406\u5148\u9a8c\u5d4c\u5165\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\uff0cPINS-CAD\u5c06\u5e38\u89c4\u8840\u7ba1\u9020\u5f71\u8f6c\u5316\u4e3a\u65e0\u9700\u6a21\u62df\u3001\u5177\u6709\u751f\u7406\u611f\u77e5\u7684\u6846\u67b6\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u9884\u9632\u6027\u5fc3\u810f\u75c5\u5b66\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.03214", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.03214", "abs": "https://arxiv.org/abs/2512.03214", "authors": ["Paulina Garcia-Corral"], "title": "Identifying attributions of causality in political text", "comment": null, "summary": "Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e00\u4e2a\u68c0\u6d4b\u548c\u5206\u6790\u653f\u6cbb\u6587\u672c\u4e2d\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u56e0\u679c\u4e3b\u5f20\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5c55\u793a\u8be5\u65b9\u6cd5\u5728\u89c4\u6a21\u7814\u7a76\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u89e3\u91ca\u662f\u7406\u89e3\u653f\u6cbb\u4e16\u754c\u7684\u57fa\u672c\u8981\u7d20\uff0c\u516c\u6c11\u7ecf\u5e38\u8be2\u95ee\u548c\u56de\u7b54\u5173\u4e8e\u4e8b\u4ef6\u539f\u56e0\u3001\u8d23\u4efb\u5f52\u5c5e\u548c\u4e0d\u540c\u884c\u52a8\u65b9\u6848\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u89e3\u91ca\u5f88\u91cd\u8981\uff0c\u4f46\u5728\u653f\u6cbb\u79d1\u5b66\u4e2d\uff0c\u5b83\u4eec\u4ecd\u7136\u662f\u7cfb\u7edf\u5206\u6790\u4e2d\u53d1\u5c55\u4e0d\u8db3\u7684\u5bf9\u8c61\uff0c\u73b0\u6709\u65b9\u6cd5\u5206\u6563\u4e14\u901a\u5e38\u662f\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u7684\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e00\u4e2a\u68c0\u6d4b\u548c\u89e3\u6790\u653f\u6cbb\u6587\u672c\u4e2d\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u56e0\u679c\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8fd4\u56de\u56e0\u679c\u4e3b\u5f20\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u4ee5\u539f\u56e0-\u7ed3\u679c\u5bf9\u7684\u5f62\u5f0f\u4f9b\u4e0b\u6e38\u5206\u6790\u4f7f\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5982\u4f55\u5728\u89c4\u6a21\u4e0a\u7814\u7a76\u56e0\u679c\u89e3\u91ca\uff0c\u5e76\u663e\u793a\u4e86\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u4eba\u5de5\u7f16\u7801\u7684\u9002\u5ea6\u6807\u6ce8\u8981\u6c42\u3001\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5206\u6790\u653f\u6cbb\u6587\u672c\u4e2d\u89e3\u91ca\u7684\u53ef\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u56e0\u679c\u89e3\u91ca\u7684\u68c0\u6d4b\u548c\u5206\u6790\uff0c\u586b\u8865\u4e86\u653f\u6cbb\u79d1\u5b66\u4e2d\u89e3\u91ca\u7cfb\u7edf\u5206\u6790\u7684\u7a7a\u767d\u3002"}}
{"id": "2512.03684", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03684", "abs": "https://arxiv.org/abs/2512.03684", "authors": ["Shahid Ansari", "Mahendra Kumar Gohil", "Yusuke Maeda", "Bishakh Bhattacharya"], "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection", "comment": null, "summary": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u8f6f\u4f53\u624b\u6307\u4e0e\u521a\u6027\u5916\u9aa8\u9abc\u7684\u6df7\u5408\u5939\u722a\uff0c\u914d\u5408\u89c6\u89c9\u611f\u77e5\u4e0e\u529b\u63a7\uff0c\u5b9e\u73b0\u756a\u8304\u81ea\u4e3b\u91c7\u6458\uff0c\u5e73\u5747\u5468\u671f24.34\u79d2\uff0c\u6210\u529f\u7387\u7ea680%", "motivation": "\u73b0\u6709\u756a\u8304\u91c7\u6458\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u9700\u8981\u8f7b\u67d4\u6293\u53d6\u4ee5\u907f\u514d\u635f\u4f24\uff0c\u540c\u65f6\u5904\u7406\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u91c7\u6458\u5faa\u73af", "method": "1) \u6df7\u5408\u5939\u722a\u8bbe\u8ba1\uff1a6\u4e2a\u8f6f\u4f53\u8d1f\u6cca\u677e\u6bd4\u624b\u6307+\u521a\u6027\u5916\u9aa8\u9abc+\u4e73\u80f6\u7bee\uff0c\u5b9e\u73b0\u7b3c\u5f0f\u6293\u53d6\uff1b2) \u89c6\u89c9\u7cfb\u7edf\uff1aRGB-D\u76f8\u673a+Detectron2\u5206\u5272\u6210\u719f/\u672a\u719f\u756a\u8304\uff0c\u5b9a\u4f4d\u679c\u6897\u548c\u679c\u5b9e\u4e2d\u5fc3\uff1b3) \u529b\u63a7\uff1a\u57fa\u4e8e\u865a\u62df\u529f\u539f\u7406\u5efa\u7acb\u4f3a\u670d\u626d\u77e9-\u6293\u53d6\u529b\u6a21\u578b\uff0c\u4f7f\u7528PID\u63a7\u5236\u5668+\u529b\u654f\u7535\u963b\u53cd\u9988\u8c03\u8282\u6293\u53d6\u529b\uff1b4) \u8fd0\u52a8\u89c4\u5212\uff1aPSO\u4f18\u53165\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u8f68\u8ff9", "result": "\u5b8c\u6574\u91c7\u6458\u5faa\u73af\uff08\u63a5\u8fd1\u3001\u5206\u79bb\u3001\u5207\u5272\u3001\u6293\u53d6\u3001\u8fd0\u8f93\u3001\u91ca\u653e\uff09\u5e73\u5747\u65f6\u95f424.34\u79d2\uff0c\u603b\u4f53\u6210\u529f\u7387\u7ea680%\uff0c\u6293\u53d6\u529b\u4fdd\u6301\u57280.20-0.50N\u7684\u4f4e\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u5939\u722a\u8bbe\u8ba1\u4e0e\u96c6\u6210\u89c6\u89c9-\u63a7\u5236\u7ba1\u9053\u80fd\u6709\u6548\u5b9e\u73b0\u756a\u8304\u7684\u53ef\u9760\u91c7\u6458\uff0c\u5728\u4fdd\u6301\u4f4e\u6293\u53d6\u529b\u7684\u540c\u65f6\u8fbe\u5230\u8f83\u9ad8\u6210\u529f\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u519c\u4e1a\u73af\u5883"}}
{"id": "2512.03078", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03078", "abs": "https://arxiv.org/abs/2512.03078", "authors": ["Vahid R. Ramezani", "Benjamin Englard"], "title": "Risk-Entropic Flow Matching", "comment": "29 pages, 5 figures", "summary": "Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.", "AI": {"tldr": "\u8bba\u6587\u5c06\u503e\u659c\u98ce\u9669\uff08tilted risk\uff09\u5e94\u7528\u4e8e\u6d41\u5339\u914d\uff08Flow Matching\uff09\uff0c\u901a\u8fc7log-exponential\u53d8\u6362\u6539\u8fdb\u6807\u51c6\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u6d41\u5f62\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u5c11\u6570\u5206\u652f\u3002", "motivation": "\u6807\u51c6\u6d41\u5339\u914d\u4e2d\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u5c06\u6240\u6709\u5230\u8fbe\u540c\u4e00\u65f6\u7a7a\u70b9\u7684\u901f\u5ea6\u76ee\u6807\u538b\u7f29\u4e3a\u5355\u4e00\u6761\u4ef6\u5747\u503c\uff0c\u5ffd\u7565\u4e86\u9ad8\u9636\u6761\u4ef6\u4fe1\u606f\uff08\u65b9\u5dee\u3001\u504f\u5ea6\u3001\u591a\u6a21\u6001\uff09\uff0c\u8fd9\u4e9b\u4fe1\u606f\u7f16\u7801\u4e86\u6570\u636e\u6d41\u5f62\u7684\u7cbe\u7ec6\u51e0\u4f55\u7ed3\u6784\u548c\u5c11\u6570\u5206\u652f\u3002", "method": "\u5c06\u6807\u51c6\u98ce\u9669\u654f\u611f\uff08log-exponential\uff09\u53d8\u6362\u5e94\u7528\u4e8e\u6761\u4ef6\u6d41\u5339\u914d\u635f\u5931\uff0c\u5f97\u5230\u503e\u659c\u98ce\u9669\u635f\u5931\uff0c\u8be5\u635f\u5931\u662f\u6709\u610f\u4e49\u7684\u6761\u4ef6\u71b5\u6d41\u5339\u914d\u76ee\u6807\u7684\u4e0a\u754c\u3002\u901a\u8fc7\u68af\u5ea6\u7684\u5c0f\u9636\u5c55\u5f00\u5f97\u5230\u4e24\u4e2a\u53ef\u89e3\u91ca\u7684\u4e00\u9636\u4fee\u6b63\uff1a\u6d41\u5339\u914d\u6b8b\u5dee\u7684\u534f\u65b9\u5dee\u9884\u5904\u7406\u548c\u504f\u597d\u975e\u5bf9\u79f0\u6216\u7a00\u6709\u5206\u652f\u7684\u504f\u5c3e\u9879\u3002", "result": "\u5728\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u63a2\u6d4b\u6a21\u7cca\u6027\u548c\u5c3e\u90e8\u7684\u5408\u6210\u6570\u636e\u4e0a\uff0c\u98ce\u9669\u654f\u611f\u635f\u5931\u6bd4\u6807\u51c6\u6574\u6d41\u6d41\u5339\u914d\u6539\u5584\u4e86\u7edf\u8ba1\u6307\u6807\uff0c\u5e76\u66f4\u5fe0\u5b9e\u5730\u6062\u590d\u4e86\u51e0\u4f55\u7ed3\u6784\u3002", "conclusion": "\u503e\u659c\u98ce\u9669\u635f\u5931\u4e3a\u6d41\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u6570\u636e\u6d41\u5f62\u7684\u51e0\u4f55\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7a00\u6709\u4e8b\u4ef6\u548c\u975e\u5bf9\u79f0\u5206\u652f\u3002"}}
{"id": "2512.03405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03405", "abs": "https://arxiv.org/abs/2512.03405", "authors": ["Jiangtao Wu", "Shihao Li", "Zhaozhou Bian", "Yuanxing Zhang", "Jialu Chen", "Runzhe Wen", "An Ping", "Yiwen He", "Jiakai Wang", "Jiaheng Liu"], "title": "ViDiC: Video Difference Captioning", "comment": null, "summary": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u89c6\u9891\u5dee\u5f02\u63cf\u8ff0\u4efb\u52a1ViDiC\u53ca\u5176\u6570\u636e\u96c6ViDiC-1K\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u89c6\u9891\u5bf9\u4e4b\u95f4\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5dee\u5f02\u63cf\u8ff0\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u573a\u666f\u4e2d\u7684\u8fd0\u52a8\u8fde\u7eed\u6027\u3001\u4e8b\u4ef6\u6f14\u53d8\u6216\u7f16\u8f91\u4e00\u81f4\u6027\uff0c\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u5728\u6bd4\u8f83\u52a8\u6001\u573a\u666f\u7684\u7ec4\u6210\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faViDiC\u4efb\u52a1\u548cViDiC-1K\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u89c6\u9891\u5bf9\uff0c\u6807\u6ce8\u4e864000\u591a\u4e2a\u6bd4\u8f83\u68c0\u67e5\u9879\uff0c\u6db5\u76d67\u4e2a\u7c7b\u522b\u3002\u91c7\u7528\u57fa\u4e8eLLM-as-a-Judge\u534f\u8bae\u7684\u53cc\u68c0\u67e5\u8868\u6846\u67b6\u5206\u522b\u8bc4\u4f30\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u7684\u51c6\u786e\u6027\u3002", "result": "\u572819\u4e2a\u4ee3\u8868\u6027\u591a\u6a21\u6001\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5b83\u4eec\u5728\u6bd4\u8f83\u63cf\u8ff0\u548c\u5dee\u5f02\u611f\u77e5\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "ViDiC-1K\u53ef\u4f5c\u4e3a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u667a\u80fd\u4e2d\u7684\u89c6\u9891\u7406\u89e3\u3001\u7f16\u8f91\u611f\u77e5\u548c\u6bd4\u8f83\u63a8\u7406\u5960\u5b9a\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2512.03582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03582", "abs": "https://arxiv.org/abs/2512.03582", "authors": ["Zeba Afroz", "Harsh Vardhan", "Pawan Bhakuni", "Aanchal Punia", "Rajdeep Kumar", "Md. Shad Akhtar"], "title": "Fine-grained Narrative Classification in Biased News Articles", "comment": null, "summary": "Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86INDI-PROP\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5370\u5ea6\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u5ba3\u4f20\u5206\u6790\uff0c\u5305\u542b\u4e09\u5c42\u6ce8\u91ca\uff1a\u610f\u8bc6\u5f62\u6001\u504f\u89c1\u3001\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u6846\u67b6\u548c\u8bf4\u670d\u6280\u5de7\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eGPT-4o-mini\u7684FANTA\u548cTPTC\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u5206\u7c7b\u3002", "motivation": "\u53d9\u4e8b\u662f\u5ba3\u4f20\u7684\u8ba4\u77e5\u548c\u60c5\u611f\u652f\u67b6\uff0c\u5c06\u5b64\u7acb\u7684\u8bf4\u670d\u6280\u5de7\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684\u6545\u4e8b\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9\u5370\u5ea6\u65b0\u95fb\u5a92\u4f53\u7684\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u5206\u7c7b\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5ba3\u4f20\u4e2d\u7684\u591a\u5c42\u6b21\u7ed3\u6784\u3002", "method": "1) \u521b\u5efaINDI-PROP\u6570\u636e\u96c6\uff1a\u5305\u542b1,266\u7bc7\u5173\u4e8eCAA\u548c\u519c\u6c11\u6297\u8bae\u7684\u6587\u7ae0\uff0c\u8fdb\u884c\u4e09\u5c42\u6ce8\u91ca\uff1a\u610f\u8bc6\u5f62\u6001\u504f\u89c1\u3001\u4e8b\u4ef6\u7279\u5b9a\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u6846\u67b6\u3001\u8bf4\u670d\u6280\u5de7\uff1b2) \u5f00\u53d1FANTA\u548cTPTC\u6846\u67b6\uff1a\u57fa\u4e8eGPT-4o-mini\u7684\u591a\u8df3\u63d0\u793a\u63a8\u7406\u6846\u67b6\uff0cFANTA\u6574\u5408\u4fe1\u606f\u63d0\u53d6\u548c\u4e0a\u4e0b\u6587\u6846\u67b6\u8fdb\u884c\u5206\u5c42\u63a8\u7406\uff0cTPTC\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\u7cfb\u7edf\u5206\u89e3\u8bf4\u670d\u7ebf\u7d22\u3002", "result": "\u63d0\u51fa\u7684FANTA\u548cTPTC\u6846\u67b6\u5728\u504f\u89c1\u3001\u53d9\u4e8b\u548c\u8bf4\u670d\u6280\u5de7\u5206\u7c7b\u4efb\u52a1\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u591a\u5c42\u6b21\u6ce8\u91ca\u548c\u63a8\u7406\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u7684\u5370\u5ea6\u5ba3\u4f20\u5206\u6790\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u591a\u8df3\u63a8\u7406\u6846\u67b6\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u5206\u7c7b\u548c\u5ba3\u4f20\u5206\u6790\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5ba3\u4f20\u4e2d\u7684\u591a\u5c42\u6b21\u7ed3\u6784\u3002"}}
{"id": "2512.03125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03125", "abs": "https://arxiv.org/abs/2512.03125", "authors": ["Xiwen Wei", "Mustafa Munir", "Radu Marculescu"], "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models", "comment": "NeurIPS 2025", "summary": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git", "AI": {"tldr": "MoDE\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u53ef\u6269\u5c55\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u7279\u5b9a\u66f4\u65b0\u6765\u7f13\u89e3\u68af\u5ea6\u51b2\u7a81\uff0c\u89e3\u51b3\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u95f4\u9057\u5fd8\u95ee\u9898", "motivation": "\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5305\u62ec\u6a21\u6001\u5185\u9057\u5fd8\u548c\u6a21\u6001\u95f4\u9057\u5fd8\u3002\u867d\u7136\u6a21\u6001\u5185\u9057\u5fd8\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u6a21\u6001\u95f4\u9057\u5fd8\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u8bc6\u522b\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u4ece\u6a21\u6001\u95f4\u68af\u5ea6\u51b2\u7a81\u7684\u89d2\u5ea6\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u63d0\u51faMoDE\uff08Modality-Decoupled Experts\uff09\u67b6\u6784\uff1a1\uff09\u901a\u8fc7\u9694\u79bb\u6a21\u6001\u7279\u5b9a\u66f4\u65b0\u6765\u7f13\u89e3\u68af\u5ea6\u51b2\u7a81\uff1b2\uff09\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u5e76\u4fdd\u7559\u9884\u8bad\u7ec3\u80fd\u529b\uff1b3\uff09\u4e0e\u4e4b\u524d\u4fdd\u6301\u6a21\u6001\u8026\u5408\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cMoDE\u663e\u5f0f\u89e3\u8026\u6a21\u6001\u4ee5\u9632\u6b62\u5e72\u6270\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoDE\u663e\u8457\u7f13\u89e3\u4e86\u6a21\u6001\u95f4\u548c\u6a21\u6001\u5185\u9057\u5fd8\uff0c\u5728\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5148\u524d\u7684\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MoDE\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u7279\u5b9a\u66f4\u65b0\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u95f4\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03704", "abs": "https://arxiv.org/abs/2512.03704", "authors": ["Yijun Liao"], "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue", "comment": "22 pages, 2 figures, 13 tables. Code available at https://github.com/lyj20071013/DZ-TDPO", "summary": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO", "AI": {"tldr": "DZ-TDPO\uff1a\u4e00\u4e2a\u975e\u7834\u574f\u6027\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u7684\u52a8\u6001KL\u7ea6\u675f\u548c\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u6ce8\u610f\u529b\u504f\u7f6e\u89e3\u51b3\u957f\u5bf9\u8bdd\u4e2d\u7684\u72b6\u6001\u60ef\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0SOTA\u80dc\u7387\u3002", "motivation": "\u957f\u5bf9\u8bdd\u7cfb\u7edf\u5b58\u5728\"\u72b6\u6001\u60ef\u6027\"\u95ee\u9898\uff0c\u9759\u6001\u7ea6\u675f\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u7528\u6237\u610f\u56fe\u6f14\u53d8\u4e0e\u5386\u53f2\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u9650\u5236\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u52a8\u6001\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faDZ-TDPO\u6846\u67b6\uff0c\u7ed3\u5408\u51b2\u7a81\u611f\u77e5\u7684\u52a8\u6001KL\u7ea6\u675f\u548c\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u6ce8\u610f\u529b\u504f\u7f6e\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u6ce8\u610f\u529b\u8c03\u8282\u800c\u975e\u7834\u574f\u6027\u6743\u91cd\u66f4\u65b0\u6765\u7f13\u89e3\u72b6\u6001\u60ef\u6027\u3002", "result": "\u5728MSC\u6570\u636e\u96c6\u4e0a\u8fbe\u523086.2%\u80dc\u7387\uff08Phi-3.5\uff09\uff0cQwen2.5-7B\u6a21\u578b\u5b9e\u73b099.4%\u80dc\u7387\u4e14\u56f0\u60d1\u5ea6\u5f00\u9500\u53ef\u5ffd\u7565\uff1b\u53d1\u73b0\"\u5bb9\u91cf-\u7a33\u5b9a\u6027\u6743\u8861\"\u73b0\u8c61\uff1a\u5c0f\u6a21\u578b\u9700\u4ed8\u51fa\"\u5bf9\u9f50\u7a0e\"\u514b\u670d\u5386\u53f2\u60ef\u6027\uff0c\u5927\u6a21\u578b\u53ef\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5bf9\u9f50\u3002", "conclusion": "DZ-TDPO\u901a\u8fc7\u7cbe\u786e\u7684\u6ce8\u610f\u529b\u8c03\u8282\u6709\u6548\u7f13\u89e3\u72b6\u6001\u60ef\u6027\uff0c\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\uff08MMLU\uff09\uff0c\u4e3a\u975e\u7834\u574f\u6027\u5bf9\u8bdd\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03911", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03911", "abs": "https://arxiv.org/abs/2512.03911", "authors": ["Kenneth Stewart", "Roxana Leontie", "Samantha Chapin", "Joe Hays", "Sumit Bam Shrestha", "Carl Glen Henshaw"], "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware", "comment": "Submitted for review at NICE 2026 (Neuro-Inspired Computational Elements) conference", "summary": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.", "AI": {"tldr": "\u5c06\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684ANN\u8f6c\u6362\u4e3a\u8109\u51b2Sigma-Delta\u795e\u7ecf\u7f51\u7edc\uff0c\u90e8\u7f72\u5728Loihi 2\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\u7684\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u63a8\u7406", "motivation": "\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u5e94\u7528\u5f00\u53d1\u80fd\u91cf\u9ad8\u6548\u3001\u5b9e\u65f6\u54cd\u5e94\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u65b9\u6848\uff0c\u7279\u522b\u662f\u9762\u5411\u592a\u7a7a\u548c\u5730\u9762\u673a\u5668\u4eba\u5e94\u7528", "method": "1) \u5728\u4eff\u771f\u4e2d\u8bad\u7ec3ANN\u7b56\u7565\uff1b2) \u5c06\u4f7f\u7528ReLU\u6fc0\u6d3b\u7684ANN\u8f6c\u6362\u4e3a\u8109\u51b2Sigma-Delta\u795e\u7ecf\u7f51\u7edc\uff1b3) \u90e8\u7f72\u5230Intel Loihi 2\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\uff1b4) \u5728NVIDIA Omniverse Isaac Lab\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u95ed\u73af\u63a7\u5236\u8bc4\u4f30", "result": "\u6210\u529f\u5c06ANN\u7b56\u7565\u8f6c\u6362\u4e3aSDNN\u5e76\u90e8\u7f72\u5728Loihi 2\u4e0a\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u80fd\u91cf\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u6bd4\u8f83\u4e86GPU\u548cLoihi 2\u7684\u6267\u884c\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u5f62\u6001\u5e73\u53f0\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u7684\u53ef\u884c\u6027", "conclusion": "\u5efa\u7acb\u4e86\u5c06\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u90e8\u7f72\u5230\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u4e3a\u672a\u6765\u592a\u7a7a\u548c\u5730\u9762\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u80fd\u91cf\u9ad8\u6548\u3001\u5b9e\u65f6\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5f00\u8f9f\u4e86\u9014\u5f84"}}
{"id": "2512.03244", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03244", "abs": "https://arxiv.org/abs/2512.03244", "authors": ["Salman Rahman", "Sruthi Gorantla", "Arpit Gupta", "Swastik Roy", "Nanyun Peng", "Yang Liu"], "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning", "comment": null, "summary": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.", "AI": {"tldr": "SPARK\u6846\u67b6\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1)\u751f\u6210\u5668\u4ea7\u751f\u591a\u6837\u89e3\uff0c\u9a8c\u8bc1\u5668\u5e76\u884c\u548c\u5e8f\u5217\u8bc4\u4f30\uff1b2)\u7528\u9a8c\u8bc1\u8f93\u51fa\u8bad\u7ec3\u751f\u6210\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff1b3)\u5c06PRM\u4f5c\u4e3aRL\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u4e8eground-truth\u7684\u65b9\u6cd5\u3002", "motivation": "\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u6b65\u9aa4\u7ea7\u6807\u6ce8\u6216ground truth\u53c2\u8003\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u65e0\u9700\u53c2\u8003\u7684RL\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7b54\u6848\u6216ground truth\u7684\u9886\u57df\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u751f\u6210\u5668\u4ea7\u751f\u591a\u6837\u89e3\uff0c\u9a8c\u8bc1\u5668\u901a\u8fc7\u5e76\u884c\u7f29\u653e\uff08\u81ea\u4e00\u81f4\u6027\uff09\u548c\u5e8f\u5217\u7f29\u653e\uff08\u5143\u6279\u5224\uff09\u8bc4\u4f30\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u7528\u9a8c\u8bc1\u8f93\u51fa\u4f5c\u4e3a\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5fae\u8c03\u751f\u6210\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff1b\u7b2c\u4e09\u9636\u6bb5\uff0c\u5c06PRM\u4f5c\u4e3aRL\u5956\u52b1\u4fe1\u53f7\uff0c\u52a0\u5165\u683c\u5f0f\u7ea6\u675f\u9632\u6b62\u5956\u52b1\u653b\u51fb\u3002", "result": "\u5728ProcessBench\u4e0a\u8fbe\u523067.5 F1\uff0c\u4f18\u4e8e\u53c2\u8003\u6307\u5bfc\u8bad\u7ec3\u768466.4\u548cGPT-4o\u768461.9\uff1b\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u738747.4%\uff0c\u4f18\u4e8e\u57fa\u4e8eground-truth\u7684RLVR\u768443.9%\u3002", "conclusion": "SPARK\u5b9e\u73b0\u4e86\u65e0\u9700\u53c2\u8003\u7684RL\u8bad\u7ec3\uff0c\u6027\u80fd\u8d85\u8d8a\u57fa\u4e8eground-truth\u7684\u65b9\u6cd5\uff0c\u4e3a\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7b54\u6848\u6216ground truth\u7684\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2512.03290", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2512.03290", "abs": "https://arxiv.org/abs/2512.03290", "authors": ["Julian Evan Chrisnanto", "Nurfauzi Fadillah", "Yulison Herry Chrisnanto"], "title": "ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics", "comment": "15 pages, 7 figures", "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.", "AI": {"tldr": "ASPEN\u662f\u4e00\u79cd\u65b0\u578bPINN\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c31\u5c42\u548c\u53ef\u5b66\u4e60\u5085\u91cc\u53f6\u7279\u5f81\u89e3\u51b3\u4f20\u7edfPINN\u5728\u521a\u6027\u3001\u591a\u5c3a\u5ea6\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u9891\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u6210\u529f\u6c42\u89e3\u4e86\u590d\u6742\u7684Ginzburg-Landau\u65b9\u7a0b\u3002", "motivation": "\u4f20\u7edfPINN\u5728\u5904\u7406\u521a\u6027\u3001\u591a\u5c3a\u5ea6\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6807\u51c6\u591a\u5c42\u611f\u77e5\u673a\u67b6\u6784\u7684\u9891\u8c31\u504f\u5dee\u65e0\u6cd5\u5145\u5206\u8868\u793a\u9ad8\u9891\u5206\u91cf\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u4e2d\u5931\u6548\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u8c31\u7269\u7406\u7f51\u7edc\uff08ASPEN\uff09\uff0c\u5728\u7f51\u7edc\u8f93\u5165\u9636\u6bb5\u96c6\u6210\u81ea\u9002\u5e94\u8c31\u5c42\u548c\u53ef\u5b66\u4e60\u5085\u91cc\u53f6\u7279\u5f81\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u81ea\u8eab\u7684\u8c31\u57fa\uff0c\u9ad8\u6548\u5b66\u4e60\u89e3\u6240\u9700\u7684\u7cbe\u786e\u9891\u7387\u5185\u5bb9\u3002", "result": "\u5728\u590d\u6742\u7684Ginzburg-Landau\u65b9\u7a0b\u4e0a\uff0c\u6807\u51c6PINN\u5b8c\u5168\u5931\u6548\u5e76\u4ea7\u751f\u975e\u7269\u7406\u632f\u8361\uff0c\u800cASPEN\u6210\u529f\u6c42\u89e3\uff0c\u9884\u6d4b\u89e3\u4e0e\u9ad8\u5206\u8fa8\u7387\u771f\u5b9e\u89e3\u89c6\u89c9\u4e0a\u65e0\u6cd5\u533a\u5206\uff0c\u4e2d\u4f4d\u7269\u7406\u6b8b\u5dee\u4ec5\u4e3a5.10\u00d710^-3\uff0c\u4e14\u80fd\u6b63\u786e\u6355\u6349\u81ea\u7531\u80fd\u5feb\u901f\u5f1b\u8c6b\u548c\u7574\u58c1\u524d\u6cbf\u957f\u671f\u7a33\u5b9a\u6027\u7b49\u7269\u7406\u7279\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u8c31\u57fa\uff0cASPEN\u4e3a\u4f20\u7edfPINN\u5931\u6548\u7684\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u7269\u7406\u4e00\u81f4\u7684\u6c42\u89e3\u5668\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u5728\u6311\u6218\u6027\u7269\u7406\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.03520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03520", "abs": "https://arxiv.org/abs/2512.03520", "authors": ["Yiyi Cai", "Yuhan Wu", "Kunhang Li", "You Zhou", "Bo Zheng", "Haiyang Liu"], "title": "FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation", "comment": "15 pages, 7 figures", "summary": "We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/", "AI": {"tldr": "FloodDiffusion\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u9a71\u52a8\u6d41\u5f0f\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6269\u6563\u5f3a\u5236\u65b9\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u5ef6\u8fdf\u4e0b\u7684\u6587\u672c\u5bf9\u9f50\u65e0\u7f1d\u8fd0\u52a8\u5e8f\u5217\u751f\u6210", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5206\u5757\u5904\u7406\u6216\u5e26\u6269\u6563\u5934\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u65f6\u95f4\u53d8\u5316\u7684\u6587\u672c\u63d0\u793a\u4e0b\u7684\u6d41\u5f0f\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u8bc1\u8f93\u51fa\u5206\u5e03\u5efa\u6a21\u7684\u65b0\u6846\u67b6", "method": "\u91c7\u7528\u6269\u6563\u5f3a\u5236\u6846\u67b6\uff0c\u4f46\u6539\u8fdb\u4e86\u4e09\u4e2a\u5173\u952e\u65b9\u9762\uff1a1) \u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\u800c\u975e\u56e0\u679c\u6ce8\u610f\u529b\uff1b2) \u5b9e\u73b0\u4e0b\u4e09\u89d2\u65f6\u95f4\u8c03\u5ea6\u5668\u800c\u975e\u968f\u673a\u8c03\u5ea6\uff1b3) \u91c7\u7528\u8fde\u7eed\u65f6\u95f4\u53d8\u5316\u7684\u65b9\u5f0f\u5f15\u5165\u6587\u672c\u6761\u4ef6", "result": "\u5728HumanML3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.057\u7684FID\u5206\u6570\uff0c\u9996\u6b21\u8bc1\u660e\u6269\u6563\u5f3a\u5236\u6846\u67b6\u5728\u6d41\u5f0f\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7684\u6269\u6563\u5f3a\u5236\u6846\u67b6\uff0cFloodDiffusion\u6210\u529f\u5b9e\u73b0\u4e86\u6587\u672c\u5bf9\u9f50\u7684\u5b9e\u65f6\u6d41\u5f0f\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.03394", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.03394", "abs": "https://arxiv.org/abs/2512.03394", "authors": ["Hamed Poursiami", "Shay Snyder", "Guojing Cong", "Thomas Potok", "Maryam Parsa"], "title": "VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing", "comment": null, "summary": "Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.", "AI": {"tldr": "VS-Graph\uff1a\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u7684\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c16\u5cf0\u6269\u6563\u673a\u5236\u548c\u5173\u8054\u6d88\u606f\u4f20\u9012\uff0c\u5728\u4fdd\u6301HDC\u9ad8\u6548\u6027\u7684\u540c\u65f6\u8fbe\u5230\u63a5\u8fd1GNN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe450\u500d\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u5728\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u8d85\u7ef4\u8ba1\u7b97(HDC)\u867d\u7136\u8f7b\u91cf\u4f46\u6027\u80fd\u901a\u5e38\u4e0d\u5982GNN\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301HDC\u9ad8\u6548\u6027\u53c8\u80fd\u63a5\u8fd1GNN\u8868\u8fbe\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVS-Graph\u6846\u67b6\uff1a1) \u5c16\u5cf0\u6269\u6563\u673a\u5236(Spike Diffusion)\u7528\u4e8e\u62d3\u6251\u9a71\u52a8\u7684\u8282\u70b9\u8bc6\u522b\uff1b2) \u5173\u8054\u6d88\u606f\u4f20\u9012(Associative Message Passing)\u5728\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4\u5185\u5b9e\u73b0\u591a\u8df3\u90bb\u57df\u805a\u5408\u3002\u65e0\u9700\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u6216\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728MUTAG\u548cDD\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u4e4b\u524d\u7684HDC\u57fa\u7ebf\u63d0\u53474-5%\u51c6\u786e\u7387\uff0c\u4e0eGNN\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe450\u500d\u3002\u5373\u4f7f\u5728\u8d85\u5411\u91cf\u7ef4\u5ea6\u964d\u81f3D=128\u65f6\u4ecd\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "VS-Graph\u6210\u529f\u7f29\u5c0f\u4e86HDC\u6548\u7387\u4e0e\u6d88\u606f\u4f20\u9012\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8fb9\u7f18\u548c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u7684\u8d85\u9ad8\u6548\u6267\u884c\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u56fe\u5b66\u4e60\u6846\u67b6\u7684\u7a81\u7834\u3002"}}
{"id": "2512.03467", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.03467", "abs": "https://arxiv.org/abs/2512.03467", "authors": ["Hongtao Hao", "Joseph L. Austerweil"], "title": "Bayesian Event-Based Model for Disease Subtype and Stage Inference", "comment": "32 pages; machine learning for health symposium (2025); Proceedings of the 5th Machine Learning for Health Symposium in PMLR", "summary": "Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u4e8b\u4ef6\u6a21\u578bBEBMS\uff0c\u5728\u75be\u75c5\u4e9a\u578b\u63a8\u65ad\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5SuStaIn\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6570\u636e\u4e0a\u5f97\u5230\u66f4\u7b26\u5408\u79d1\u5b66\u5171\u8bc6\u7684\u7ed3\u679c\u3002", "motivation": "\u6162\u6027\u75be\u75c5\u5728\u4e0d\u540c\u60a3\u8005\u4e2d\u8fdb\u5c55\u65b9\u5f0f\u5b58\u5728\u5dee\u5f02\uff0c\u901a\u5e38\u5b58\u5728\u5c11\u91cf\u4e9a\u578b\u3002\u73b0\u6709\u65b9\u6cd5SuStaIn\u88ab\u5e7f\u6cdb\u7528\u4e8e\u75be\u75c5\u4e9a\u578b\u63a8\u65ad\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u8d1d\u53f6\u65af\u7684\u4e8b\u4ef6\u6a21\u578bBEBMS\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u5b9e\u9a8c\uff08\u5305\u542b\u4e0d\u540c\u7a0b\u5ea6\u7684\u6a21\u578b\u8bef\u8bbe\uff09\u4e0eSuStaIn\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u5728\u771f\u5b9e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "BEBMS\u5728\u6392\u5e8f\u3001\u5206\u671f\u548c\u4e9a\u578b\u5206\u914d\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eSuStaIn\u3002\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6570\u636e\u4e0a\uff0cBEBMS\u7684\u7ed3\u679c\u66f4\u7b26\u5408\u8be5\u75be\u75c5\u8fdb\u5c55\u7684\u79d1\u5b66\u5171\u8bc6\u3002", "conclusion": "BEBMS\u4f5c\u4e3a\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u5728\u75be\u75c5\u4e9a\u578b\u63a8\u65ad\u4e2d\u6bd4SuStaIn\u66f4\u7a33\u5065\u53ef\u9760\uff0c\u4e3a\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u63d0\u4f9b\u4e86\u6539\u8fdb\u5de5\u5177\u3002"}}
{"id": "2512.03475", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.03475", "abs": "https://arxiv.org/abs/2512.03475", "authors": ["Hongtao Hao", "Joseph L. Austerweil"], "title": "Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression", "comment": "49 pages; Machine Learning for Health (ML4H) Symposium 2025", "summary": "Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.", "AI": {"tldr": "JPM\u662f\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6a2a\u65ad\u9762\u6570\u636e\u63a8\u65ad\u6df7\u5408\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u8054\u5408\u8fdb\u5c55\uff0c\u901a\u8fc7\u5c06\u5355\u75c5\u8f68\u8ff9\u89c6\u4e3a\u90e8\u5206\u6392\u5e8f\u5e76\u6784\u5efa\u8054\u5408\u8fdb\u5c55\u5148\u9a8c\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u75c5EBM\u63d0\u9ad8\u4e86\u6392\u5e8f\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u4e8b\u4ef6\u6a21\u578b\u5047\u8bbe\u4e2a\u4f53\u53ea\u6709\u5355\u4e00\u75be\u75c5\uff0c\u4f46\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u4e2d\u6df7\u5408\u75c5\u7406\u5f88\u5e38\u89c1\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u591a\u79cd\u75be\u75c5\u540c\u65f6\u8fdb\u5c55\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u8054\u5408\u8fdb\u5c55\u6a21\u578b\uff0c\u5c06\u5355\u75c5\u8f68\u8ff9\u89c6\u4e3a\u90e8\u5206\u6392\u5e8f\uff0c\u6784\u5efa\u8054\u5408\u8fdb\u5c55\u5148\u9a8c\uff0c\u7814\u7a76\u4e86\u56db\u79cd\u53d8\u4f53\uff1aPairwise\u3001Bradley-Terry\u3001Plackett-Luce\u548cMallows\uff0c\u5206\u6790\u4e86\u6821\u51c6\u6027\u3001\u5206\u79bb\u6027\u548c\u9510\u5ea6\u4e09\u4e2a\u5c5e\u6027\u3002", "result": "\u6240\u6709\u53d8\u4f53\u90fd\u5177\u5907\u6821\u51c6\u6027\uff0c\u5206\u79bb\u6027\u63a5\u8fd1\u5b8c\u7f8e\uff1b\u9510\u5ea6\u56e0\u53d8\u4f53\u800c\u5f02\uff0c\u53ef\u901a\u8fc7\u8f93\u5165\u90e8\u5206\u6392\u5e8f\u7684\u7b80\u5355\u7279\u5f81\u9884\u6d4b\uff1b\u5728\u5408\u6210\u5b9e\u9a8c\u4e2d\uff0cJPM\u6bd4\u4f20\u7edfSA-EBM\u57fa\u7ebf\u63d0\u9ad8\u4e86\u7ea621%\u7684\u6392\u5e8f\u51c6\u786e\u6027\uff1b\u5728NACC\u6570\u636e\u4e2d\uff0cMallows\u53d8\u4f53\u548c\u57fa\u7ebf\u6a21\u578b\u7684\u7ed3\u679c\u4e0eAD\u548cVaD\u6df7\u5408\u75c5\u7406\u8fdb\u5c55\u7684\u5148\u524d\u6587\u732e\u66f4\u4e00\u81f4\u3002", "conclusion": "JPM\u4e3a\u6df7\u5408\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u8fdb\u5c55\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u75c5\u7406\u540c\u65f6\u8fdb\u5c55\u7684\u60c5\u51b5\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u75c5\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002"}}
{"id": "2512.03564", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.03564", "abs": "https://arxiv.org/abs/2512.03564", "authors": ["Xun Yuan", "Zilong Zhao", "Jiayu Li", "Aryan Pasikhani", "Prosanta Gope", "Biplab Sikdar"], "title": "Towards Irreversible Machine Unlearning for Diffusion Models", "comment": null, "summary": "Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u5fae\u8c03\u5f0f\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u7684\u9006\u5411\u653b\u51fbDiMRA\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8bb0\u5fc6\u7684\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5DiMUM\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5408\u6210\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u3001\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\uff0c\u9700\u8981\u9057\u5fd8\u5b66\u4e60\u6280\u672f\u6765\u8ba9\u6a21\u578b\u5fd8\u8bb0\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u3002\u7136\u800c\u73b0\u6709\u7684\u5fae\u8c03\u5f0f\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u88ab\u9006\u5411\u653b\u51fb\u7684\u98ce\u9669\u3002", "method": "1. DiMRA\u653b\u51fb\uff1a\u5728\u4e0d\u77e5\u9053\u9057\u5fd8\u5143\u7d20\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5728\u8f85\u52a9\u6570\u636e\u96c6\u4e0a\u4f18\u5316\u5df2\u9057\u5fd8\u7684\u6269\u6563\u6a21\u578b\u6765\u9006\u8f6c\u9057\u5fd8\u8fc7\u7a0b\uff1b2. DiMUM\u9632\u5fa1\uff1a\u901a\u8fc7\u8bb0\u5fc6\u66ff\u4ee3\u6570\u636e/\u7279\u5f81\u6765\u66ff\u6362\u76ee\u6807\u9057\u5fd8\u6570\u636e/\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u9057\u5fd8\u3002", "result": "DiMRA\u6210\u529f\u9006\u8f6c\u4e86\u6700\u5148\u8fdb\u7684\u5fae\u8c03\u5f0f\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u8fd9\u7c7b\u6280\u672f\u7684\u8106\u5f31\u6027\u3002DiMUM\u5728\u4fdd\u6301\u6269\u6563\u6a21\u578b\u751f\u6210\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5bf9DiMRA\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u73b0\u6709\u7684\u5fae\u8c03\u5f0f\u6269\u6563\u6a21\u578b\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u88ab\u9006\u5411\u653b\u51fb\u7684\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002\u63d0\u51fa\u7684DiMUM\u65b9\u6cd5\u901a\u8fc7\u8bb0\u5fc6\u66ff\u4ee3\u7b56\u7565\u6709\u6548\u589e\u5f3a\u4e86\u9057\u5fd8\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.03610", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03610", "abs": "https://arxiv.org/abs/2512.03610", "authors": ["Julius Lenz"], "title": "CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion", "comment": "15 pages, 4 figures, 8 equations", "summary": "Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.", "AI": {"tldr": "CoGraM\u662f\u4e00\u79cd\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u5b66\u4e60\u7684\u795e\u7ecf\u7f51\u7edc\u5408\u5e76\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u5c42\u3001\u795e\u7ecf\u5143\u548c\u6743\u91cd\u7ea7\u522b\u5bf9\u9f50\u51b3\u7b56\uff0c\u907f\u514d\u6709\u5bb3\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u5408\u5e76\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5408\u5e76\u795e\u7ecf\u7f51\u7edc\u662f\u5173\u952e\u9700\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6743\u91cd\u5e73\u5747\u6216Fisher\u5408\u5e76\u7ecf\u5e38\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\u4e14\u5728\u4e0d\u540c\u79cd\u5b50\u4e0b\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5408\u5e76\u65b9\u6cd5\u3002", "method": "CoGraM\u662f\u4e00\u79cd\u591a\u9636\u6bb5\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u3001\u57fa\u4e8e\u635f\u5931\u7684\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u5c42\u3001\u795e\u7ecf\u5143\u548c\u6743\u91cd\u7ea7\u522b\u8fdb\u884c\u64cd\u4f5c\u3002\u5b83\u901a\u8fc7\u635f\u5931\u5dee\u5f02\u548c\u9608\u503c\u5bf9\u9f50\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u56de\u6eda\u673a\u5236\u9632\u6b62\u6709\u5bb3\u66f4\u65b0\u3002", "result": "CoGraM\u80fd\u591f\u663e\u8457\u6539\u5584\u5408\u5e76\u540e\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u89e3\u51b3\u4e86Fisher\u7b49\u65b9\u6cd5\u7684\u5f31\u70b9\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u51c6\u786e\u7684\u795e\u7ecf\u7f51\u7edc\u5408\u5e76\u65b9\u6848\u3002", "conclusion": "CoGraM\u4e3a\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u5408\u5e76\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7cbe\u7ec6\u7684\u4f18\u5316\u7b56\u7565\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a21\u578b\u5408\u5e76\u6548\u679c\u3002"}}
{"id": "2512.03730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03730", "abs": "https://arxiv.org/abs/2512.03730", "authors": ["Melane Navaratnarajah", "David A. Kelly", "Hana Chockler"], "title": "Out-of-the-box: Black-box Causal Attacks on Object Detectors", "comment": null, "summary": "Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.", "AI": {"tldr": "BlackCAtt\u662f\u4e00\u79cd\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u56e0\u679c\u50cf\u7d20\u96c6\u6765\u751f\u6210\u53ef\u89e3\u91ca\u3001\u4e0d\u53ef\u611f\u77e5\u3001\u53ef\u590d\u73b0\u7684\u5bf9\u6297\u6837\u672c\uff0c\u653b\u51fb\u76ee\u6807\u68c0\u6d4b\u5668\u4f7f\u5176\u4e22\u5931\u3001\u4fee\u6539\u6216\u4ea7\u751f\u865a\u5047\u68c0\u6d4b\u6846\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u591a\u4e3a\u767d\u76d2\u4e14\u67b6\u6784\u7279\u5b9a\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u6210\u529f\u673a\u5236\u3002\u9700\u8981\u4e00\u79cd\u9ed1\u76d2\u3001\u67b6\u6784\u65e0\u5173\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u50cf\u7d20\u5206\u6790\u6765\u63ed\u793a\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u6f0f\u6d1e\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u548c\u9632\u5fa1\u653b\u51fb\u3002", "method": "\u63d0\u51faBlackCAtt\u7b97\u6cd5\uff0c\u5c06\u76ee\u6807\u68c0\u6d4b\u5668\u89c6\u4e3a\u9ed1\u76d2\uff0c\u8bc6\u522b\u6700\u5c0f\u56e0\u679c\u50cf\u7d20\u96c6\uff0c\u7ed3\u5408\u68c0\u6d4b\u5668\u8f93\u51fa\u7684\u8fb9\u754c\u6846\u6784\u9020\u5bf9\u6297\u653b\u51fb\u3002\u901a\u8fc7\u56e0\u679c\u50cf\u7d20\u5206\u6790\u5b9e\u73b0\u7cbe\u786e\u3001\u4e0d\u53ef\u611f\u77e5\u7684\u653b\u51fb\uff0c\u652f\u6301\u4e09\u79cd\u653b\u51fb\u7c7b\u578b\uff1a\u68c0\u6d4b\u6846\u4e22\u5931\u3001\u4fee\u6539\u548c\u65b0\u589e\u865a\u5047\u68c0\u6d4b\u3002", "result": "\u5728COCO\u6d4b\u8bd5\u96c6\u4e0a\uff0cBlackCAtt\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff1a\u79fb\u9664\u68c0\u6d4b\u6548\u679c\u63d0\u53472.7\u500d\uff0c\u4fee\u6539\u68c0\u6d4b\u6548\u679c\u63d0\u53473.86\u500d\uff0c\u89e6\u53d1\u65b0\u865a\u5047\u68c0\u6d4b\u6548\u679c\u63d0\u53475.75\u500d\u3002\u653b\u51fb\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u539f\u59cb\u56fe\u50cf\u5dee\u5f02\u6781\u5c0f\uff0c\u51e0\u4e4e\u4e0d\u53ef\u611f\u77e5\u3002", "conclusion": "\u56e0\u679c\u50cf\u7d20\u8bc6\u522b\u662f\u5b9e\u73b0\u7cbe\u786e\u3001\u4e0d\u53ef\u611f\u77e5\u5bf9\u6297\u653b\u51fb\u7684\u5173\u952e\u3002BlackCAtt\u4f5c\u4e3a\u9ed1\u76d2\u3001\u67b6\u6784\u65e0\u5173\u7684\u653b\u51fb\u5de5\u5177\uff0c\u4e0d\u4ec5\u80fd\u6709\u6548\u653b\u51fb\u591a\u79cd\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u8fd8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u653b\u51fb\u673a\u5236\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u5b89\u5168\u5206\u6790\u548c\u9632\u5fa1\u3002"}}
{"id": "2512.03745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03745", "abs": "https://arxiv.org/abs/2512.03745", "authors": ["Jiaze Li", "Yan Lu", "Bin Liu", "Guojun Yin", "Mang Ye"], "title": "Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification", "comment": null, "summary": "Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.", "AI": {"tldr": "\u63d0\u51faDMDL\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5c42\u548c\u4f18\u5316\u5c42\u7684\u53cc\u91cd\u53bb\u504f\u5b66\u4e60\u89e3\u51b3\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u5b66\u4e60\u7ba1\u9053\u5728\u5355\u6a21\u6001\u5b66\u4e60\u9636\u6bb5\u4f1a\u5f15\u5165\u6a21\u6001\u7279\u5b9a\u7ebf\u7d22\uff0c\u8fd9\u4e9b\u504f\u5dee\u4f1a\u4f20\u64ad\u5230\u8de8\u6a21\u6001\u5b66\u4e60\u9636\u6bb5\uff0c\u635f\u5bb3\u8eab\u4efd\u8bc6\u522b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u6a21\u6001\u53bb\u504f\u5b66\u4e60\u6846\u67b6\uff1a1) \u6a21\u578b\u5c42\u4f7f\u7528\u56e0\u679c\u542f\u53d1\u7684\u8c03\u6574\u5e72\u9884\u6a21\u5757\u66ff\u4ee3\u57fa\u4e8e\u4f3c\u7136\u7684\u5efa\u6a21\uff1b2) \u4f18\u5316\u5c42\u91c7\u7528\u534f\u4f5c\u65e0\u504f\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u589e\u5f3a\u3001\u6807\u7b7e\u7ec6\u5316\u548c\u7279\u5f81\u5bf9\u9f50\u4e2d\u65ad\u504f\u5dee\u4f20\u64ad\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDMDL\u80fd\u591f\u5b9e\u73b0\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u548c\u66f4\u901a\u7528\u7684\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u91cd\u6a21\u6001\u53bb\u504f\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u3002"}}
{"id": "2512.03744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03744", "abs": "https://arxiv.org/abs/2512.03744", "authors": ["Xuhui Lin", "Qiuchen Lu"], "title": "Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm", "comment": null, "summary": "Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and \"false recovery,\" where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining \"structural irreversibility detection\" and \"energy landscape reconstruction\" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u54c8\u5bc6\u987f\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7ed3\u6784\u4e0d\u53ef\u9006\u6027\u548c\u91cd\u5efa\u80fd\u91cf\u666f\u89c2\u6765\u533a\u5206\u4ea4\u901a\u7cfb\u7edf\u7684\u771f\u5b9e\u6062\u590d\u4e0e\u865a\u5047\u6062\u590d\uff0c\u53d1\u73b0\u4f20\u7edf\u6307\u6807\u4f1a\u9057\u6f0f64.8%\u7684\u7ed3\u6784\u635f\u4f24\u3002", "motivation": "\u5f53\u524d\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u97e7\u6027\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u8868\u9762\u6062\u590d\u6307\u6807\uff0c\u65e0\u6cd5\u68c0\u6d4b\u9690\u85cf\u7684\u7ed3\u6784\u635f\u4f24\uff0c\u65e0\u6cd5\u533a\u5206\u771f\u5b9e\u6062\u590d\u4e0e\"\u865a\u5047\u6062\u590d\"\uff08\u4ea4\u901a\u6307\u6807\u6b63\u5e38\u5316\u4f46\u7cfb\u7edf\u52a8\u529b\u5b66\u6c38\u4e45\u9000\u5316\uff09\u3002", "method": "\u5f00\u53d1\u7269\u7406\u7ea6\u675f\u54c8\u5bc6\u987f\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\"\u7ed3\u6784\u4e0d\u53ef\u9006\u6027\u68c0\u6d4b\"\u548c\"\u80fd\u91cf\u666f\u89c2\u91cd\u5efa\"\uff0c\u63d0\u53d6\u4f4e\u7ef4\u72b6\u6001\u8868\u793a\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u4f18\u5316\u8bc6\u522b\u51c6\u54c8\u5bc6\u987f\u7ed3\u6784\uff0c\u901a\u8fc7\u80fd\u91cf\u666f\u89c2\u6bd4\u8f83\u91cf\u5316\u7ed3\u6784\u53d8\u5316\u3002", "result": "\u5bf9\u4f26\u65662021\u5e74\u6781\u7aef\u964d\u96e8\u7684\u5206\u6790\u663e\u793a\uff0c\u867d\u7136\u8868\u9762\u6307\u6807\u5b8c\u5168\u6062\u590d\uff0c\u4f46\u7b97\u6cd5\u68c0\u6d4b\u5230\u4f20\u7edf\u76d1\u6d4b\u9057\u6f0f\u768464.8%\u7ed3\u6784\u635f\u4f24\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e3b\u52a8\u7ed3\u6784\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u5de5\u5177\uff0c\u4f7f\u57fa\u7840\u8bbe\u65bd\u6295\u8d44\u80fd\u591f\u57fa\u4e8e\u771f\u5b9e\u7cfb\u7edf\u5065\u5eb7\u72b6\u51b5\u800c\u975e\u8bef\u5bfc\u6027\u7684\u8868\u9762\u6307\u6807\u3002"}}
{"id": "2512.03883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03883", "abs": "https://arxiv.org/abs/2512.03883", "authors": ["Jorge Tapias Gomez", "Despoina Kanata", "Aneesh Rangnekar", "Christina Lee", "Julio Garcia-Aguilar", "Joshua Jesse Smith", "Harini Veeraraghavan"], "title": "Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy", "comment": "6 pages, 5 figures, 1 table, submitted to ISBI conference", "summary": "Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\\% $\\pm$ 0.04), sensitivity (90.07\\% $\\pm$ 0.08), and specificity (72.86\\% $\\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\\pm$ 0.19) with SSDCA, confirming discriminative representation learning.", "AI": {"tldr": "\u63d0\u51faSSDCA\u6a21\u578b\uff0c\u7ed3\u5408\u4e24\u6b21\u5185\u7aa5\u955c\u56fe\u50cf\uff0c\u7528\u4e8e\u76f4\u80a0\u764c\u60a3\u8005\u89c2\u5bdf\u7b49\u5f85\u671f\u95f4\u65e9\u671f\u68c0\u6d4b\u5c40\u90e8\u590d\u53d1", "motivation": "\u76f4\u80a0\u764c\u60a3\u8005\u7ecf\u65b0\u8f85\u52a9\u6cbb\u7597\u540e\u4e34\u5e8a\u5b8c\u5168\u7f13\u89e3\uff0c\u89c2\u5bdf\u7b49\u5f85\u7b56\u7565\u9700\u8981\u51c6\u786e\u65b9\u6cd5\u65e9\u671f\u68c0\u6d4b\u5c40\u90e8\u590d\u53d1\uff0c\u4ee5\u9632\u6b62\u8fdc\u5904\u8f6c\u79fb", "method": "\u5f00\u53d1Siamese Swin Transformer with Dual Cross-Attention (SSDCA)\u6a21\u578b\uff0c\u7ed3\u5408\u65b0\u8f85\u52a9\u6cbb\u7597\u540e\u91cd\u65b0\u5206\u671f\u548c\u968f\u8bbf\u671f\u95f4\u7684\u5185\u7aa5\u955c\u56fe\u50cf\uff0c\u65e0\u9700\u7a7a\u95f4\u5bf9\u9f50\u5373\u53ef\u9884\u6d4b\u6cbb\u7597\u53cd\u5e94", "result": "SSDCA\u572862\u540d\u60a3\u8005\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff1a\u5e73\u8861\u51c6\u786e\u738781.76%\uff0c\u654f\u611f\u602790.07%\uff0c\u7279\u5f02\u602772.86%\uff0c\u5bf9\u8840\u6db2\u3001\u7caa\u4fbf\u7b49\u4f2a\u5f71\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "SSDCA\u80fd\u591f\u6709\u6548\u533a\u5206\u4e34\u5e8a\u5b8c\u5168\u7f13\u89e3\u548c\u5c40\u90e8\u590d\u53d1\uff0c\u4e3a\u76f4\u80a0\u764c\u89c2\u5bdf\u7b49\u5f85\u7b56\u7565\u63d0\u4f9b\u53ef\u9760\u7684\u65e9\u671f\u68c0\u6d4b\u5de5\u5177"}}
{"id": "2512.03932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03932", "abs": "https://arxiv.org/abs/2512.03932", "authors": ["Donghun Ryou", "Inju Ha", "Sanghyeok Chu", "Bohyung Han"], "title": "Beyond the Ground Truth: Enhanced Supervision for Image Restoration", "comment": null, "summary": "Deep learning-based image restoration has achieved significant success. However, when addressing real-world degradations, model performance is limited by the quality of ground-truth images in datasets due to practical constraints in data acquisition. To address this limitation, we propose a novel framework that enhances existing ground truth images to provide higher-quality supervision for real-world restoration. Our framework generates perceptually enhanced ground truth images using super-resolution by incorporating adaptive frequency masks, which are learned by a conditional frequency mask generator. These masks guide the optimal fusion of frequency components from the original ground truth and its super-resolved variants, yielding enhanced ground truth images. This frequency-domain mixup preserves the semantic consistency of the original content while selectively enriching perceptual details, preventing hallucinated artifacts that could compromise fidelity. The enhanced ground truth images are used to train a lightweight output refinement network that can be seamlessly integrated with existing restoration models. Extensive experiments demonstrate that our approach consistently improves the quality of restored images. We further validate the effectiveness of both supervision enhancement and output refinement through user studies. Code is available at https://github.com/dhryougit/Beyond-the-Ground-Truth.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u8fc7\u9891\u7387\u57df\u6df7\u5408\u589e\u5f3a\u73b0\u6709ground truth\u56fe\u50cf\u7684\u65b0\u6846\u67b6\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u76d1\u7763\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8f93\u51fa\u7ec6\u5316\u7f51\u7edc\u63d0\u5347\u6062\u590d\u8d28\u91cf\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6062\u590d\u4e2d\uff0c\u7531\u4e8e\u6570\u636e\u91c7\u96c6\u7684\u5b9e\u9645\u9650\u5236\uff0cground truth\u56fe\u50cf\u8d28\u91cf\u6709\u9650\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002\u9700\u8981\u63d0\u5347\u73b0\u6709ground truth\u56fe\u50cf\u8d28\u91cf\u4ee5\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u76d1\u7763\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u6846\u67b6\uff1a1) \u4f7f\u7528\u6761\u4ef6\u9891\u7387\u63a9\u7801\u751f\u6210\u5668\u5b66\u4e60\u81ea\u9002\u5e94\u9891\u7387\u63a9\u7801\uff1b2) \u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u751f\u6210\u611f\u77e5\u589e\u5f3a\u7684ground truth\u56fe\u50cf\uff1b3) \u9891\u7387\u57df\u6df7\u5408\u878d\u5408\u539f\u59cbground truth\u548c\u8d85\u5206\u8fa8\u7387\u53d8\u4f53\u7684\u9891\u7387\u6210\u5206\uff1b4) \u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8f93\u51fa\u7ec6\u5316\u7f51\u7edc\uff0c\u53ef\u4e0e\u73b0\u6709\u6062\u590d\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u6062\u590d\u56fe\u50cf\u8d28\u91cf\u3002\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u76d1\u7763\u589e\u5f3a\u548c\u8f93\u51fa\u7ec6\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u73b0\u6709ground truth\u56fe\u50cf\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u76d1\u7763\uff0c\u540c\u65f6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f93\u51fa\u7ec6\u5316\u7f51\u7edc\u8fdb\u4e00\u6b65\u63d0\u5347\u6062\u590d\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.04082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04082", "abs": "https://arxiv.org/abs/2512.04082", "authors": ["Jiazhe Wei", "Ken Li", "Tianyu Lao", "Haofan Wang", "Liang Wang", "Caifeng Shan", "Chenyang Si"], "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "comment": "Project page: https://postercopilot.github.io/", "summary": "Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.", "AI": {"tldr": "PosterCopilot\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u56fe\u5f62\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u5e03\u5c40\u51e0\u4f55\u51c6\u786e\u6027\u548c\u5ba1\u7f8e\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u5206\u5c42\u53ef\u63a7\u7684\u8fed\u4ee3\u7f16\u8f91\uff0c\u5b9e\u73b0\u4e13\u4e1a\u7ea7\u6d77\u62a5\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u56fe\u5f62\u8bbe\u8ba1\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u5e03\u5c40\u4e0d\u51c6\u786e\u3001\u7f3a\u4e4f\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u6240\u9700\u7684\u8fed\u4ee3\u5206\u5c42\u7f16\u8f91\u80fd\u529b\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e13\u4e1a\u8bbe\u8ba1\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1) \u6270\u52a8\u76d1\u7763\u5fae\u8c03\uff1b2) \u89c6\u89c9-\u73b0\u5b9e\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\uff1b3) \u5ba1\u7f8e\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u3002\u7ed3\u5408\u751f\u6210\u6a21\u578b\u6784\u5efa\u5b8c\u6574\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u5206\u5c42\u53ef\u63a7\u7684\u8fed\u4ee3\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePosterCopilot\u80fd\u591f\u751f\u6210\u51e0\u4f55\u51c6\u786e\u3001\u5ba1\u7f8e\u4f18\u8d8a\u7684\u5e03\u5c40\uff0c\u4e3a\u4e13\u4e1a\u8fed\u4ee3\u8bbe\u8ba1\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u53ef\u63a7\u6027\u3002", "conclusion": "PosterCopilot\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u52a8\u5316\u56fe\u5f62\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e13\u4e1a\u7ea7\u7684\u5e03\u5c40\u63a8\u7406\u548c\u53ef\u63a7\u7f16\u8f91\u80fd\u529b\u3002"}}
