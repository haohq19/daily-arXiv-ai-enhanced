<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出了一种无监督、无需专家标注的医学图像异常检测框架，通过增量扩展正常样本集来提升异常检测性能，无需异常标签或生成模型。


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临标注数据稀缺和专家监督成本高的问题，需要开发无需异常标签的无监督方法。

Method: 使用冻结的预训练视觉主干网络，添加小型卷积适配器进行快速领域适应。通过k-NN异常评分和双概率门控机制（距离z-score阈值和SWAG不确定性边界）安全地增量扩展正常样本集。

Result: 在多个医学图像数据集上显著提升性能：COVID-CXR的ROC-AUC从0.9489提升到0.9982，Pneumonia CXR从0.6834到0.8968，Brain MRI ND-5从0.6041到0.7269。

Conclusion: 该框架在标签稀缺的医学成像应用中具有高效性和有效性，能够通过增量学习不断优化正常性概念。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [2] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 提出一种基于体素稀疏化和子流形稀疏卷积网络的两阶段方法，用于医学CT图像中肿瘤的自动分割，在保持高分辨率输入的同时显著降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 放射图像中肿瘤的精确分割是专业且耗时的任务，传统方法受限于3D扫描的大计算量而需要降采样或分块处理，这限制了定量分析在临床中的常规应用。

Method: 采用两阶段方法：体素稀疏化和子流形稀疏卷积网络，允许使用高分辨率输入和原生3D模型架构进行分割。

Result: 在KiTS23挑战赛的肾癌CT图像上，获得了与优胜者相当的结果：肾脏+肿块Dice系数95.8%，肿瘤+囊肿85.7%，单独肿瘤80.3%。计算效率显著提升，推理时间减少60%，VRAM使用减少75%。

Conclusion: 该方法在保持最先进分割精度的同时，显著降低了计算资源需求，为临床常规应用提供了可行的解决方案。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [3] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 提出了空间超感知的概念，认为真正的多模态智能需要从任务驱动系统转向能够感知、记忆、推理和预测世界的模型。开发了VSI-SUPER基准测试，证明仅靠数据扩展不足以实现空间超感知，而预测性感知方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前多模态系统主要停留在反应式任务驱动阶段，缺乏对空间认知和世界建模的深度理解。需要从单纯的语言理解转向更全面的空间超感知能力。

Method: 提出了空间超感知的四个阶段：语义感知、流式事件认知、隐式3D空间认知和预测性世界建模。开发了VSI-SUPER基准测试（包含VSR和VSC任务），并训练了Cambrian-S模型。还提出了基于预测误差的预测性感知方法。

Result: 数据扩展（VSI-590K）使VSI-Bench性能提升30%，但在VSI-SUPER上表现仍然有限。预测性感知方法显著优于现有基线模型，证明预测能力对空间超感知至关重要。

Conclusion: 仅靠数据扩展无法实现真正的空间超感知。预测性感知（通过预测误差驱动记忆和事件分割）是更有效的路径，需要模型不仅能看，还要能预测、选择和组织经验。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 本文展示了连续血糖监测和可穿戴技术如何实现从静态血糖阈值向动态代谢表型分析的范式转变，利用机器学习模型预测胰岛素抵抗和β细胞功能，为精准糖尿病预防开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 传统糖尿病和前驱糖尿病的分类基于静态血糖阈值，掩盖了由胰岛素抵抗、β细胞功能障碍和肠促胰岛素缺乏驱动的病理生理异质性，需要更精准的代谢表型分析方法。

Method: 使用连续血糖监测和可穿戴技术收集高分辨率血糖数据，结合机器学习模型预测肌肉胰岛素抵抗和β细胞功能，分析个体对标准化餐食的餐后血糖反应，并整合饮食、睡眠和体力活动模式数据。

Result: 机器学习模型能够准确预测金标准胰岛素抵抗和β细胞功能测量值；个体餐后血糖反应可作为代谢亚型生物标志物；生活习惯模式与特定代谢功能障碍相关；饮食缓解剂效果具有表型依赖性。

Conclusion: 连续血糖监测可将早期血糖异常解构为可操作的亚表型，超越简单血糖控制，为针对个体核心代谢缺陷的精准营养、行为和药物策略铺平道路，开启精准糖尿病预防新时代。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [5] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 提出了首个针对边缘设备LLM部署的自回归感知分割计算框架，通过混合精度量化、中间压缩和联合优化，在保持精度的同时显著减少通信开销和内存占用。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在资源受限的物联网设备上部署的挑战，特别是自回归推理的迭代token生成过程和不断扩展的KV缓存需求。

Method: 1. 单点分割压缩(OPSC)：混合精度量化方案；2. 两阶段中间压缩管道：阈值分割(TS)和token自适应位量化(TAB-Q)；3. 统一优化框架：联合选择最优分割点、量化设置和序列长度。

Result: 相比SmoothQuant、OmniQuant和Atom等最先进量化方法，实现了1.49倍推理加速和显著通信开销减少，同时保持或提高了模型精度。

Conclusion: 该框架为边缘设备上的LLM部署提供了实用解决方案，有效平衡了性能、内存和通信需求。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [6] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出交换策略优化(EPO)框架，解决半无限安全强化学习问题，通过迭代调整约束集实现最优策略性能和有界安全保证。


<details>
  <summary>Details</summary>
Motivation: 传统安全强化学习处理有限约束，但实际应用中常面临无限约束问题，如需要在连续参数空间上确保安全条件。

Method: EPO通过迭代求解有限约束集的安全RL子问题，自适应调整活跃约束集：添加违反约束，删除零拉格朗日乘子约束。

Result: 理论分析表明，在温和假设下，EPO训练的策略性能接近最优解，且全局约束违反严格保持在预定界限内。

Conclusion: EPO为半无限安全RL问题提供了有效的算法框架，能同时保证策略性能和确定性有界安全。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [7] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出基于Transformer的纵向建模方法，解决临床风险分类中电子健康记录数据的异构性、不规则时间模式和复杂语义结构问题，在多项指标上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录数据中的异构性挑战，包括不规则时间模式、大模态差异和复杂语义结构，以提升临床风险分类的准确性。

Method: 使用特征嵌入层统一表示结构化和非结构化数据，引入可学习的时间编码机制处理不均匀采样间隔，采用多头自注意力结构进行全局依赖建模，设计语义加权池化模块自适应分配关键医疗事件的重要性。

Result: 在准确性、召回率、精确率和F1分数等指标上优于传统机器学习和时序深度学习模型，在多源异构EHR环境中实现稳定精确的风险识别。

Conclusion: 为临床智能决策提供了一个高效可靠的框架，能够有效处理多源异构电子健康记录数据，提升风险分类性能。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [8] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 提出了Relational Graph Perceiver (RGP)模型，通过时间子图采样器和跨注意力潜在瓶颈来整合时空依赖关系，支持多任务预测，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注空间结构，将时间信息仅作为过滤约束而非建模信号，且通常只支持单任务预测，无法满足医疗、金融等领域的复杂时空交互和多任务需求。

Method: 开发了时间子图采样器来捕获时间相关关系，并提出RGP图变换器架构，使用跨注意力潜在瓶颈整合结构和时间上下文信息，通过灵活的解码器支持多任务联合学习。

Result: 在RelBench、SALT和CTU数据集上的实验表明，RGP实现了最先进的性能，为关系深度学习提供了通用且可扩展的解决方案。

Conclusion: RGP通过有效整合时空上下文和跨任务学习，为复杂关系数据建模提供了强大的框架，在多个领域展现出优越性能。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [9] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 本文提出在无线时间触发联邦学习系统中引入自适应模型剪枝，通过联合优化剪枝率和带宽分配来最小化训练损失并确保低延迟。


<details>
  <summary>Details</summary>
Motivation: 联邦学习网络中的用户设备数量不断增加且无线带宽有限，导致训练延迟和通信开销问题日益严重。

Method: 基于模型剪枝对TT-Fed模型的梯度l2范数进行收敛分析，利用KKT条件推导剪枝率和带宽分配的闭式解。

Result: 模拟结果显示，模型剪枝可将通信成本降低40%，同时保持模型性能不变。

Conclusion: 自适应模型剪枝能有效解决无线联邦学习中的通信开销和延迟问题。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [10] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一个灰盒三维临近预报框架，结合物理约束神经网络和数据驱动学习，直接处理三维雷达反射率数据，实现更准确的极端降水预报。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：数值天气预报及其深度学习模拟速度慢、分辨率低；外推法和纯数据驱动模型存在误差累积和过度平滑问题；二维雷达方法丢弃了关键的垂直信息。

Method: 使用三维雷达反射率数据，结合物理约束神经网络学习垂直变化的3D平流场，参数化空间变化的扩散，引入布朗运动启发的随机项表示未解析运动，通过残差分支捕捉小尺度对流启动和微物理变化。

Result: 在长达三小时的预报时间内实现了更准确的降水预报，在160名气象学家的盲评中57%的情况下排名第一。

Conclusion: 通过恢复完整的三维动力学并保持物理一致性，为极端降水的准确可靠临近预报提供了可扩展且稳健的途径。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: SSPO提出句子级别的重要性比率，平衡了GRPO和GSPO的优缺点，避免训练崩溃和低数据利用率问题，在五个数据集上平均得分46.57，优于GRPO(43.01)和GSPO(44.42)。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO存在不稳定策略更新问题，GSPO存在采样数据利用率低的问题，需要一种平衡两者优点的新方法。

Method: SSPO采用句子级别的重要性比率，并在PPO-CLIP中应用句子熵来动态调整裁剪边界，鼓励高熵token探索，限制低熵token的裁剪范围。

Result: SSPO在五个数据集上平均得分46.57，超越GRPO(43.01)和GSPO(44.42)，在三个数据集上达到最先进性能。

Conclusion: SSPO通过采用GSPO的优点但避免其缺点，有效利用了生成数据，在强化学习验证奖励训练中表现出色。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [12] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 最新开源LLM代码助手在现实使用场景中仍存在早期报告的漏洞，安全-功能权衡阻碍了有效修复。作者提出Prompt Exposure和Model Exposure指标来评估LLM生成漏洞的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代码助手在软件开发中作用日益重要，其生成的漏洞对网络安全构成严重威胁。现有安全基准和改进方法对主流LLM的实际影响尚不明确。

Method: 引入Prompt Exposure(PE)指标，综合考虑漏洞严重性、生成概率和诱导漏洞的提示表述；基于PE定义Model Exposure(ME)评分，衡量模型生成漏洞的严重性和普遍性。

Result: 研究发现即使是最新开源模型在现实使用场景中仍易受早期报告的漏洞攻击，表明安全-功能权衡阻碍了漏洞的有效修复。

Conclusion: 需要新的严重性度量标准来优先缓解最严重和普遍的漏洞，PE和ME指标有助于推动LLM代码助手的安全性改进。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>
