<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053)
*Zibin Liu,Banglei Guan,Yang Shang,Shunkun Liang,Zhenbao Yu,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于事件相机的光流引导6DoF物体姿态跟踪方法，通过2D-3D混合特征提取和光流关联实现精确跟踪


<details>
  <summary>Details</summary>
Motivation: 传统相机在物体姿态跟踪中面临运动模糊、传感器噪声、部分遮挡和光照变化等挑战，而事件相机具有高动态范围和低延迟的优势，有望解决这些问题

Method: 采用2D-3D混合特征提取策略检测事件和物体模型的角点和边缘；通过最大化时空窗口内事件关联概率搜索角点光流；建立光流引导的角点-边缘关联；通过最小化角点与边缘距离迭代优化6DoF姿态

Result: 在模拟和真实事件数据上的实验结果表明，该方法在准确性和鲁棒性方面优于现有的基于事件相机的最先进方法

Conclusion: 提出的光流引导6DoF物体姿态跟踪方法有效利用了事件相机的优势，在复杂环境下实现了精确且鲁棒的姿态跟踪

Abstract: Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

</details>


### [2] [Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval](https://arxiv.org/abs/2512.21221)
*Dao Sy Duy Minh,Huynh Trung Kiet,Nguyen Lam Phu Quy,Phu-Hoa Pham,Tran Chi Nguyen*

Main category: cs.CV

TL;DR: 提出轻量级两阶段检索框架，结合事件实体提取与多模态语义建模，在OpenEvents v1基准上取得0.559 mAP，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像-文本检索面临模糊查询、语境依赖、语言变异性和可扩展性等挑战，需要更有效的解决方案来处理复杂真实场景。

Method: 采用两阶段检索流程：第一阶段基于事件中心实体提取，使用BM25进行高效候选过滤；第二阶段应用BEiT-3模型捕捉深度多模态语义并进行重排序。

Result: 在OpenEvents v1基准测试中达到0.559的平均精度均值，大幅超越先前基线方法，证明了事件引导过滤与长文本视觉语言建模结合的有效性。

Conclusion: 事件引导过滤与深度多模态语义建模的结合为复杂真实场景下的图像检索提供了准确高效的解决方案，展示了事件中心方法在跨模态检索中的潜力。

Abstract: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval

</details>


### [3] [Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential](https://arxiv.org/abs/2512.21284)
*Shihao Zou,Jingjing Li,Wei Ji,Jincai Huang,Kai Wang,Guo Dan,Weixin Si,Yi Pan*

Main category: cs.CV

TL;DR: 提出SpikeSurgSeg，首个基于脉冲神经网络的视频Transformer框架，用于手术场景分割，在非GPU平台上实现实时性能，相比现有方法显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前手术系统依赖智能场景理解，但现有深度学习模型（特别是大模型）计算需求大、功耗高，难以在资源受限的手术环境中实时部署。脉冲神经网络（SNN）作为高效计算范式具有潜力，但面临手术标注数据稀缺和手术视频表示稀疏的挑战。

Method: 提出SpikeSurgSeg框架：1）引入手术场景掩码自编码预训练策略，通过分层管状掩码实现鲁棒的时空表示学习；2）采用轻量级脉冲驱动分割头，在保持SNN低延迟特性的同时产生时间一致的预测。

Result: 在EndoVis18和内部SurgBleed数据集上的实验表明，SpikeSurgSeg达到与SOTA ANN模型相当的mIoU，同时推理延迟降低至少8倍，相比大多数基础模型基线加速超过20倍。

Conclusion: SpikeSurgSeg展示了脉冲神经网络在时间关键型手术场景分割中的潜力，能够在非GPU平台上实现实时性能，为资源受限的手术环境提供了高效解决方案。

Abstract: Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.

</details>


### [4] [Streaming Video Instruction Tuning](https://arxiv.org/abs/2512.21334)
*Jiaer Xia,Peixian Chen,Mengdan Zhang,Xing Sun,Kaiyang Zhou*

Main category: cs.CV

TL;DR: Streamo是一个实时流视频LLM，作为通用交互助手，能够执行多种流视频任务，包括实时解说、动作理解、事件描述、时间事件定位和时间敏感问答。


<details>
  <summary>Details</summary>
Motivation: 现有的在线视频模型主要专注于问答或字幕生成等狭窄任务，缺乏能够处理多种流视频任务的通用交互助手。需要填补离线视频感知模型和实时多模态助手之间的空白。

Method: 构建了Streamo-Instruct-465K大规模指令跟随数据集，涵盖多样时间上下文和多任务监督，通过端到端训练流程统一训练异构流视频任务。

Result: Streamo展现出强大的时间推理能力、响应式交互能力以及在多种流视频基准测试中的广泛泛化能力，在多个流视频任务上表现优异。

Conclusion: Streamo填补了离线视频感知模型和实时多模态助手之间的差距，向统一、智能的连续视频流理解迈出了一步。

Abstract: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams](https://arxiv.org/abs/2512.20631)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.LG

TL;DR: 无需训练的时序漂移分析显示，基于Transformer的情感模型在真实社交媒体事件中表现出显著不稳定性，准确率最大下降23.4%，同时提出四种优于基线的新漂移指标。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析Transformer情感模型在真实世界事件中的时序稳定性问题，特别是在社交媒体内容动态变化时期，现有监控方法可能无法有效检测模型性能退化。

Method: 采用零训练方法，对三种Transformer架构进行系统性评估，使用12,279个真实社交媒体帖子进行统计验证，提出四种新的漂移指标，并与基于嵌入的基线方法比较。

Result: 结果显示模型在事件驱动期间显著不稳定，准确率最大下降23.4%，置信度最大下降13.0%，新漂移指标优于基线且计算效率适合生产部署，统计验证证实检测能力超过行业监控阈值。

Conclusion: 零训练方法可直接部署于实时情感监控系统，为Transformer模型在动态内容期间的行为提供新见解，具有实际应用价值。

Abstract: We present a comprehensive zero-training temporal drift analysis of transformer-based sentiment models validated on authentic social media data from major real-world events. Through systematic evaluation across three transformer architectures and rigorous statistical validation on 12,279 authentic social media posts, we demonstrate significant model instability with accuracy drops reaching 23.4% during event-driven periods. Our analysis reveals maximum confidence drops of 13.0% (Bootstrap 95% CI: [9.1%, 16.5%]) with strong correlation to actual performance degradation. We introduce four novel drift metrics that outperform embedding-based baselines while maintaining computational efficiency suitable for production deployment. Statistical validation across multiple events confirms robust detection capabilities with practical significance exceeding industry monitoring thresholds. This zero-training methodology enables immediate deployment for real-time sentiment monitoring systems and provides new insights into transformer model behavior during dynamic content periods.

</details>


### [6] [TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform](https://arxiv.org/abs/2512.20761)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Henrik Albers,Oliver Müller*

Main category: cs.LG

TL;DR: TS-Arena平台解决时间序列基础模型评估危机，通过实时数据流预注册机制确保测试数据在推理时不存在，防止历史数据污染


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型存在评估危机，主要源于训练集和测试集重叠导致的信息泄露，以及全局模式向测试数据的非法转移，违反了有效基准测试所需的独立性要求

Method: 引入TS-Arena平台，采用实时数据流预注册机制，将真正未知的未来作为测试环境，实施严格的全局时间分割，防止历史数据污染

Result: 在能源领域初步应用，建立了可持续的基础模型比较基础设施，确保评估目标在推理时物理上不存在，提供真实的模型泛化能力评估

Conclusion: TS-Arena通过移动时间边界机制恢复了预测的操作完整性，为时间序列基础模型提供了真实世界的有效评估框架

Abstract: While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.

</details>


### [7] [STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting](https://arxiv.org/abs/2512.21118)
*Shi Quan Foo,Chi-Ho Wong,Zhihan Gao,Dit-Yan Yeung,Ka-Hing Wong,Wai-Kin Wong*

Main category: cs.LG

TL;DR: STLDM是一种基于扩散模型的降水临近预报方法，通过两阶段架构结合确定性预测和生成增强，在精度和效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报对预防极端天气灾害至关重要，但现有方法面临挑战：确定性模型预测模糊，生成模型精度不足。需要一种既能保持准确性又能生成清晰预测的方法。

Method: 提出STLDM模型，采用端到端的扩散模型架构，结合变分自编码器学习潜在表示。将任务分解为两个阶段：1) 确定性预报阶段（由条件网络处理）；2) 增强阶段（由潜在扩散模型执行）。

Result: 在多个雷达数据集上的实验表明，STLDM在性能上超越了现有最先进方法，同时提高了推理效率。

Conclusion: STLDM通过两阶段分解策略有效解决了降水临近预报中确定性模型模糊和生成模型精度不足的问题，实现了精度和效率的双重提升。

Abstract: Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.

</details>


### [8] [BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft](https://arxiv.org/abs/2512.21165)
*Qizhi Wang*

Main category: cs.LG

TL;DR: BALLAST使用上下文多臂老虎机替代Raft中静态的随机选举超时机制，通过安全探索在线适应网络延迟变化，显著提升了在长尾延迟、抖动和分区恢复等挑战性网络环境下的可用性。


<details>
  <summary>Details</summary>
Motivation: Raft协议中的随机选举超时机制在长尾延迟、网络抖动和分区恢复等情况下变得脆弱，重复的分票会导致可用性下降，需要更智能的自适应机制来应对动态网络环境。

Method: BALLAST采用线性上下文多臂老虎机（LinUCB变体），从离散的超时选项中选择最优策略，并引入安全探索机制以在不稳定期间限制风险，实现轻量级的在线自适应。

Result: 在包含长尾延迟、丢包、相关突发、节点异构性和分区/恢复动荡的可重复离散事件模拟中，BALLAST相比标准随机超时和常见启发式方法，在挑战性WAN环境下显著减少了恢复时间和不可写时间。

Conclusion: BALLAST通过上下文多臂老虎机实现了Raft选举超时的智能自适应，在保持稳定LAN/WAN环境竞争力的同时，有效提升了在挑战性网络条件下的可用性和恢复性能。

Abstract: Randomized election timeouts are a simple and effective liveness heuristic for Raft, but they become brittle under long-tail latency, jitter, and partition recovery, where repeated split votes can inflate unavailability. This paper presents BALLAST, a lightweight online adaptation mechanism that replaces static timeout heuristics with contextual bandits. BALLAST selects from a discrete set of timeout "arms" using efficient linear contextual bandits (LinUCB variants), and augments learning with safe exploration to cap risk during unstable periods. We evaluate BALLAST on a reproducible discrete-event simulation with long-tail delay, loss, correlated bursts, node heterogeneity, and partition/recovery turbulence. Across challenging WAN regimes, BALLAST substantially reduces recovery time and unwritable time compared to standard randomized timeouts and common heuristics, while remaining competitive on stable LAN/WAN settings.

</details>


### [9] [Learning to Solve PDEs on Neural Shape Representations](https://arxiv.org/abs/2512.21311)
*Lilian Welschinger,Yilin Liu,Zican Wang,Niloy Mitra*

Main category: cs.LG

TL;DR: 提出一种无需网格的神经表面PDE求解方法，可直接在神经表示上求解偏微分方程，无需显式网格提取或逐实例优化。


<details>
  <summary>Details</summary>
Motivation: 现代3D资产越来越多地采用神经表示，但现有PDE求解器主要针对多边形/三角形网格，导致在神经域中无法直接求解表面PDE，需要显式网格提取或逐实例残差训练，阻碍端到端工作流程。

Method: 提出一种基于神经局部形状属性的局部更新算子学习方法，该算子与主流神经表面表示自然集成，只需在单个代表性形状上训练一次，即可泛化到不同形状和拓扑变化。

Result: 在解析基准测试（球体上的热方程和泊松方程求解）和不同表示的真实神经资产上，方法略优于CPM，与FEM保持合理接近，并首次实现了在神经和经典表面表示上求解表面PDE的端到端流程。

Conclusion: 该方法实现了在神经表示上直接求解表面PDE，无需显式网格化或逐实例优化，同时保持可微性，为神经形状分析提供了端到端的解决方案。

Abstract: Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 提出一个结合LangChain多智能体系统和许可区块链的架构，用于保障自主AI系统的监控、策略执行和不可篡改审计


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在医疗、智慧城市、数字取证和供应链管理等领域的应用日益增长，但这些系统存在信任、监督和信息完整性等问题，需要确保其决策过程的可审计性和安全性

Method: 设计单一架构模型，将LangChain多智能体系统与许可区块链结合，将感知-概念化-行动周期与区块链治理层关联，验证输入、评估建议行动并记录执行结果。具体实现包括Hyperledger Fabric系统、MCP集成行动执行器和LangChain智能体

Result: 在智能库存管理、交通信号控制和医疗监控等实验中，区块链安全验证能有效防止未经授权的操作，提供全决策过程的可追溯性，并将操作延迟保持在合理范围内

Conclusion: 该框架为实施高影响力的自主AI应用提供了一个通用系统，既能保持自主性又能确保责任性

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: ARC类推理基准测试的性能差距主要源于视觉感知缺陷而非推理能力不足，通过分离感知与推理的两阶段实验验证了这一点。


<details>
  <summary>Details</summary>
Motivation: 挑战当前对ARC类基准测试性能差距的主流解释（归因于机器推理缺陷），提出假设：性能差距主要源于视觉感知限制而非归纳推理缺陷。

Method: 引入两阶段实验流程：1) 感知阶段：将每个图像独立转换为自然语言描述；2) 推理阶段：模型使用这些描述进行归纳和应用规则。在Mini-ARC、ACRE和Bongard-LOGO三个数据集上比较两阶段流程与标准端到端评估。

Result: 在三个ARC风格数据集上，感知能力是观察到的性能差距的主导因素；对VLM输出的推理轨迹手动检查显示约80%的模型失败源于感知错误。

Conclusion: ARC风格基准测试混淆了感知和推理挑战，观察到的性能差距可能夸大了机器推理的缺陷。评估机器智能进展时需要分离感知和推理的评估协议。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [12] [Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction](https://arxiv.org/abs/2512.21043)
*Cheng-Yu Kuo,Hirofumi Shin,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 提出一种基于物理能量抽象的方法，让机器人手通过触觉快速学习抓握力控制，无需外部传感或物体先验知识，在几分钟内就能学会减少滑动


<details>
  <summary>Details</summary>
Motivation: 在动态物体交互中调节抓握力以减少滑动是机器人操作的基本挑战，特别是当物体被多个滚动接触操纵、具有未知属性（如质量或表面条件）且外部传感不可靠时。而人类即使没有视觉线索也能通过触觉快速调节抓握力，受此启发，希望让机器人手也能快速探索物体并学习触觉驱动的抓握力控制

Method: 提出物理信息能量抽象，将物体建模为虚拟能量容器。手指施加的功率与物体保留能量之间的不一致性提供了推断滑动感知稳定性的物理基础信号。基于此抽象，采用基于模型的学习和规划，从触觉传感高效建模能量动力学，并执行实时抓握力优化

Result: 在仿真和硬件实验中，该方法能在几分钟内从零开始学习抓握力控制，有效减少滑动，并在各种运动-物体对中延长抓握持续时间，且不依赖外部传感或物体先验知识

Conclusion: 通过物理信息能量抽象和基于模型的学习，实现了机器人手快速学习触觉驱动的抓握力控制，为解决动态物体交互中的滑动问题提供了有效方法

Abstract: Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.

</details>
