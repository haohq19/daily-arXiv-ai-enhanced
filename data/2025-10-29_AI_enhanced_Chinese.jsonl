{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboOmni\u6846\u67b6\uff0c\u57fa\u4e8e\u5168\u6a21\u6001LLM\uff0c\u901a\u8fc7\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u6765\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u5b9e\u73b0\u4e3b\u52a8\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u534f\u4f5c\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\uff0c\u800c\u771f\u5b9e\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u5f88\u5c11\u76f4\u63a5\u53d1\u51fa\u6307\u4ee4\u3002\u9700\u8981\u673a\u5668\u4eba\u80fd\u591f\u4e3b\u52a8\u63a8\u65ad\u7528\u6237\u610f\u56fe\u4ee5\u5b9e\u73b0\u6709\u6548\u534f\u4f5c\u3002", "method": "\u63d0\u51faPerceiver-Thinker-Talker-Executor\u6846\u67b6\uff0c\u57fa\u4e8e\u7aef\u5230\u7aef\u5168\u6a21\u6001LLM\uff0c\u7edf\u4e00\u610f\u56fe\u8bc6\u522b\u3001\u4ea4\u4e92\u786e\u8ba4\u548c\u52a8\u4f5c\u6267\u884c\u3002\u901a\u8fc7\u65f6\u7a7a\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u9c81\u68d2\u7684\u610f\u56fe\u8bc6\u522b\uff0c\u652f\u6301\u76f4\u63a5\u8bed\u97f3\u4ea4\u4e92\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cRoboOmni\u5728\u6210\u529f\u7387\u3001\u63a8\u7406\u901f\u5ea6\u3001\u610f\u56fe\u8bc6\u522b\u548c\u4e3b\u52a8\u534f\u52a9\u65b9\u9762\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6587\u672c\u548cASR\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RoboOmni\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u6307\u4ee4\u5b9e\u73b0\u4e86\u4e3b\u52a8\u7684\u673a\u5668\u4eba\u610f\u56fe\u8bc6\u522b\u548c\u534f\u4f5c\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23630", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23630", "abs": "https://arxiv.org/abs/2510.23630", "authors": ["Ninghui Feng", "Yiyan Qi"], "title": "NUM2EVENT: Interpretable Event Reasoning from Numerical time-series", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated impressive multimodal\nreasoning capabilities, yet their understanding of purely numerical time-series\nsignals remains limited. Existing approaches mainly focus on forecasting or\ntrend description, without uncovering the latent events that drive numerical\nchanges or explaining the reasoning process behind them. In this work, we\nintroduce the task of number-to-event reasoning and decoding, which aims to\ninfer interpretable structured events from numerical inputs, even when current\ntext is unavailable. To address the data scarcity and semantic alignment\nchallenges, we propose a reasoning-aware framework that integrates an\nagent-guided event extractor (AGE), a marked multivariate Hawkes-based\nsynthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a\ntime-series encoder with a structured decoder. Our model explicitly reasons\nover numerical changes, generates intermediate explanations, and outputs\nstructured event hypotheses. Experiments on multi-domain datasets show that our\nmethod substantially outperforms strong LLM baselines in event-level precision\nand recall. These results suggest a new direction for bridging quantitative\nreasoning and semantic understanding, enabling LLMs to explain and predict\nevents directly from numerical dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6570\u5b57\u5230\u4e8b\u4ef6\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5fae\u8c03\u6846\u67b6\u4ece\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u4e2d\u63a8\u65ad\u7ed3\u6784\u5316\u4e8b\u4ef6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709LLM\u5728\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u7406\u89e3\u65b9\u9762\u6709\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u9884\u6d4b\u6216\u8d8b\u52bf\u63cf\u8ff0\uff0c\u672a\u80fd\u63ed\u793a\u9a71\u52a8\u6570\u503c\u53d8\u5316\u7684\u6f5c\u5728\u4e8b\u4ef6\u6216\u89e3\u91ca\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u611f\u77e5\u6846\u67b6\uff0c\u5305\u542b\u4ee3\u7406\u5f15\u5bfc\u4e8b\u4ef6\u63d0\u53d6\u5668\u3001\u57fa\u4e8e\u6807\u8bb0\u591a\u5143Hawkes\u7684\u5408\u6210\u751f\u6210\u5668\uff0c\u4ee5\u53ca\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u548c\u7ed3\u6784\u5316\u89e3\u7801\u5668\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e8b\u4ef6\u7ea7\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3aLLM\u57fa\u7ebf\u3002", "conclusion": "\u4e3a\u8fde\u63a5\u5b9a\u91cf\u63a8\u7406\u548c\u8bed\u4e49\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4f7fLLM\u80fd\u591f\u76f4\u63a5\u4ece\u6570\u503c\u52a8\u6001\u4e2d\u89e3\u91ca\u548c\u9884\u6d4b\u4e8b\u4ef6\u3002"}}
{"id": "2510.23896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23896", "abs": "https://arxiv.org/abs/2510.23896", "authors": ["Kosei Uemura", "Miaoran Zhang", "David Ifeoluwa Adelani"], "title": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages", "comment": null, "summary": "Text embeddings are an essential building component of several NLP tasks such\nas retrieval-augmented generation which is crucial for preventing\nhallucinations in LLMs. Despite the recent release of massively multilingual\nMTEB (MMTEB), African languages remain underrepresented, with existing tasks\noften repurposed from translation benchmarks such as FLORES clustering or\nSIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB\ncovering 59 languages, 14 tasks, and 38 datasets, including six newly added\ndatasets. Unlike many MMTEB datasets that include fewer than five languages,\nthe new additions span 14 to 56 African languages and introduce entirely new\ntasks, such as hate speech detection, intent detection, and emotion\nclassification, which were not previously covered. Complementing this, we\npresent AfriE5, an adaptation of the instruction-tuned mE5 model to African\nlanguages through cross-lingual contrastive distillation. Our evaluation shows\nthat AfriE5 achieves state-of-the-art performance, outperforming strong\nbaselines such as Gemini-Embeddings and mE5.", "AI": {"tldr": "AfriMTEB\u662f\u4e00\u4e2a\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b59\u79cd\u8bed\u8a00\u300114\u4e2a\u4efb\u52a1\u548c38\u4e2a\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u975e\u6d32\u8bed\u8a00\u5728\u6587\u672c\u5d4c\u5165\u8bc4\u4f30\u4e2d\u7684\u7a7a\u767d\u3002\u540c\u65f6\u63d0\u51fa\u4e86AfriE5\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\u65b9\u6cd5\u5728\u975e\u6d32\u8bed\u8a00\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMTEB\uff09\u4e2d\u975e\u6d32\u8bed\u8a00\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u4efb\u52a1\u5927\u591a\u662f\u4ece\u7ffb\u8bd1\u57fa\u51c6\uff08\u5982FLORES\u805a\u7c7b\u6216SIB-200\uff09\u91cd\u65b0\u8c03\u6574\u7528\u9014\u800c\u6765\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1. \u6784\u5efaAfriMTEB\u57fa\u51c6\uff1a\u8986\u76d659\u79cd\u975e\u6d32\u8bed\u8a00\u300114\u4e2a\u4efb\u52a1\u548c38\u4e2a\u6570\u636e\u96c6\uff0c\u5305\u62ec6\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u6db5\u76d614-56\u79cd\u8bed\u8a00\uff0c\u5f15\u5165\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3001\u610f\u56fe\u68c0\u6d4b\u548c\u60c5\u611f\u5206\u7c7b\u7b49\u65b0\u4efb\u52a1\u30022. \u5f00\u53d1AfriE5\u6a21\u578b\uff1a\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u6307\u4ee4\u8c03\u4f18\u7684mE5\u6a21\u578b\u9002\u914d\u5230\u975e\u6d32\u8bed\u8a00\u3002", "result": "AfriE5\u6a21\u578b\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86Gemini-Embeddings\u548cmE5\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "AfriMTEB\u4e3a\u975e\u6d32\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0cAfriE5\u6a21\u578b\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\u5728\u975e\u6d32\u8bed\u8a00\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\uff0c\u63a8\u52a8\u4e86\u975e\u6d32\u8bed\u8a00NLP\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo\u662f\u4e00\u4e2a\u957f\u671f\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u65e5\u5e38\u573a\u666f\u4e2d\u8bc4\u4f30\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684AI\u52a9\u624b\uff0c\u5305\u542b\u8d85\u8fc714\u5c0f\u65f6/\u53c2\u4e0e\u8005\u7684\u540c\u6b65\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u6570\u636e\uff0c\u6db5\u76d612\u4e2a\u8bca\u65ad\u5b50\u4efb\u52a1\u548c3,291\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684QA\u9879\u76ee\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5b64\u7acb\u8bc4\u4f30AI\u52a9\u624b\u7684\u80fd\u529b\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7684\u6d41\u5f0f\u573a\u666f\u6216\u4ec5\u652f\u6301\u77ed\u671f\u4efb\u52a1\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u4e2d\u5bf9\u591a\u6a21\u6001\u8f93\u5165\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u957f\u671f\u8bb0\u5fc6\u4fdd\u6301\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u5305\u542b\u5de5\u4f5c\u4e0e\u5b66\u4e60\u3001\u751f\u6d3b\u65b9\u5f0f\u4e0e\u65e5\u5e38\u3001\u793e\u4ea4\u6d3b\u52a8\u3001\u5916\u51fa\u4e0e\u6587\u5316\u56db\u4e2a\u9886\u57df\u7684\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6240\u6709\u6570\u636e\u5728\u7edf\u4e00\u5168\u5c40\u65f6\u95f4\u7ebf\u4e0a\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u7cbe\u70bc\u83b7\u5f97\u9ad8\u8d28\u91cf\u89c6\u89c9\u53d9\u8ff0\u548c\u8bed\u97f3\u8f6c\u5f55\u3002\u5b9a\u4e49\u4e8612\u4e2a\u8bca\u65ad\u5b50\u4efb\u52a1\uff0c\u6db5\u76d6\u8bb0\u5fc6\u3001\u7406\u89e3\u548c\u8de8\u8bb0\u5fc6\u63a8\u7406\u4e09\u5927\u6838\u5fc3\u80fd\u529b\u3002", "result": "TeleEgo\u63d0\u4f9b\u4e863,291\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684QA\u9879\u76ee\uff0c\u6db5\u76d6\u5355\u9009\u3001\u4e8c\u5143\u3001\u591a\u9009\u548c\u5f00\u653e\u5f0f\u591a\u79cd\u95ee\u9898\u683c\u5f0f\uff0c\u5e76\u5728\u4e25\u683c\u7684\u6d41\u5f0f\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002\u63d0\u51fa\u4e86\u5b9e\u65f6\u51c6\u786e\u7387\u548c\u8bb0\u5fc6\u6301\u4e45\u65f6\u95f4\u4e24\u4e2a\u5173\u952e\u6307\u6807\u3002", "conclusion": "TeleEgo\u4e3a\u5b9e\u7528AI\u52a9\u624b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u73b0\u5b9e\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u8bc4\u4f30\u6b63\u786e\u6027\u3001\u65f6\u95f4\u54cd\u5e94\u6027\u548c\u957f\u671f\u8bb0\u5fc6\u4fdd\u6301\u80fd\u529b\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u6765\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u3002", "motivation": "\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u524d\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4f53\u5f02\u8d28\u6027\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u7684\u6307\u6807\uff0c\u5e38\u7528\u7279\u5f81\u5728\u89c4\u6a21\u4e0a\u53d7\u9650\uff0c\u4e2a\u4f53\u79fb\u52a8\u4e0e\u7a7a\u95f4\u73af\u5883\u7684\u590d\u6742\u4ea4\u4e92\u672a\u88ab\u5145\u5206\u6355\u6349\uff0c\u4ee5\u53ca\u4e2a\u4f53\u7ea7\u79fb\u52a8\u6570\u636e\u7a00\u758f\u4e14\u4e0d\u9002\u5408\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u7eb3\u5165\u6a21\u578b\uff0c\u5229\u7528\u5927\u89c4\u6a21\u7a00\u758f\u4e2a\u4f53\u7ea7\u6570\u636e\u6355\u6349\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u5c40\u90e8\u7a7a\u95f4\u73af\u5883\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e2a\u4f53\u7684SIR\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u6761\u4ef6\u6a21\u578b\u80fd\u591f\u6355\u6349\u5230\u5177\u6709\u76f8\u4f3c\u4e8b\u4ef6\u524d\u79fb\u52a8\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u5728\u79fb\u52a8\u6a21\u5f0f\u4e0a\u7684\u5dee\u5f02\u53d8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u7eb3\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u4e8e\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u53d8\u5316\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u7406\u89e3\u4e2a\u4f53\u5728\u5371\u673a\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "SNELLA\u662f\u4e00\u79cd\u5355\u9636\u6bb5\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6838\u51fd\u6570\u6269\u5c55\u4f4e\u79e9\u5206\u89e3\u548c\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u5728\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u8c03\u4f18\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u4e24\u9636\u6bb5\u8303\u5f0f\u5ffd\u7565\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u53c2\u6570\u8c03\u6574\uff0c\u9650\u5236\u4e86\u6027\u80fd\uff1b2\uff09\u7531\u4e8e\u9700\u8981\u5b58\u50a8\u6240\u6709\u6743\u91cd\u77e9\u9635\uff0c\u5185\u5b58\u4f7f\u7528\u7387\u9ad8\u3002", "method": "\u63d0\u51faSNELLA\u5355\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u4e24\u4e2a\u4f4e\u79e9\u53ef\u5b66\u4e60\u77e9\u9635\u7684\u5408\u5e76\u6765\u9009\u62e9\u6027\u66f4\u65b0\u6743\u91cd\u77e9\u9635\uff0c\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u9632\u6b62\u6743\u91cd\u66f4\u65b0\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\uff1b2\uff09\u63d0\u51fa\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u57fa\u4e8e\u91cd\u8981\u6027\u5206\u6570\u7aef\u5230\u7aef\u5730\u4fc3\u8fdb\u6743\u91cd\u5728\u5c42\u95f4\u548c\u5c42\u5185\u7ade\u4e89\u3002", "result": "\u5728\u5206\u7c7b\u3001\u5206\u5272\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSNELLA\u5728\u4f4e\u5185\u5b58\u4f7f\u7528\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff1a\u5728FGVC\u57fa\u51c6\u4e0a\u6bd4SPT-LoRA\u63d0\u9ad81.8%\u7684Top-1\u51c6\u786e\u7387\uff1b\u572886M\u5230632M\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u5b9e\u73b031.1%-39.9%\u7684\u5185\u5b58\u51cf\u5c11\u3002", "conclusion": "SNELLA\u901a\u8fc7\u5355\u9636\u6bb5\u8bbe\u8ba1\u548c\u521b\u65b0\u7684\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer\u662f\u4e00\u4e2a\u57fa\u4e8e\u805a\u7c7b\u7684\u65e5\u5fd7\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7b80\u5316\u65e5\u5fd7\u5206\u6790\u6d41\u7a0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709LLM\u804a\u5929\u673a\u5668\u4eba\u6027\u80fd\u63d0\u534739%-68%\u3002", "motivation": "\u7cfb\u7edf\u65e5\u5fd7\u662f\u7f51\u7edc\u5b89\u5168\u7684\u6838\u5fc3\uff0c\u4f46\u5206\u6790\u5927\u91cf\u591a\u6837\u5316\u65e5\u5fd7\u6570\u636e\u9762\u4e34\u9ad8\u6210\u672c\u3001\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\u9650\u5236\u7b49\u6311\u6218\uff0c\u4f7f\u5f97\u8bb8\u591a\u7ec4\u7ec7\u96be\u4ee5\u8fdb\u884c\u57fa\u672c\u5206\u6790\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u62ec\u8def\u7531\u5668\u3001\u65e5\u5fd7\u8bc6\u522b\u5668\u3001\u65e5\u5fd7\u89e3\u6790\u5668\u548c\u641c\u7d22\u5de5\u5177\uff0c\u901a\u8fc7\u805a\u7c7b\u65b9\u6cd5\u89e3\u51b3LLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u7ed3\u6784\u5316\u6587\u672c\u5904\u7406\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u65e5\u5fd7\u548c\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4ChatGPT\u3001ChatPDF\u548cNotebookLM\u7b49\u6700\u5148\u8fdb\u7684LLM\u804a\u5929\u673a\u5668\u4eba\uff0c\u6027\u80fd\u663e\u8457\u63d0\u534739%-68%\uff0c\u9c81\u68d2\u6027\u589e\u5f3a\uff0c\u4f7f\u7528ROUGE-1\u5206\u6570\u65f6\u56db\u5206\u4f4d\u8ddd\u51cf\u5c1193%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u5f3aLLM\u5728\u7ed3\u6784\u5316\u6587\u672c\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u4e13\u5bb6\u548c\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u6bd4\u8f83\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u7269\u7406\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u666e\u53ca\uff0c\u9700\u8981\u7406\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u7ecf\u5178\u6a21\u578b\uff08IDM\u3001OVM\u3001OVRV\u3001CACC\uff09\u548c\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8fdb\u884c\u6821\u51c6\u548c\u9884\u6d4b\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cRMSE\u5206\u522b\u4e3a0.0046\uff08\u4e2d\u7b49\u95f4\u8ddd\uff09\u30010.0016\uff08\u957f\u95f4\u8ddd\uff09\u548c0.0025\uff08\u8d85\u957f\u95f4\u8ddd\uff09\uff1b\u7ecf\u5178\u6a21\u578b\u4e2dCACC\u8868\u73b0\u6700\u597d\uff0c\u957f\u95f4\u8dddRMSE\u4e3a2.67\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u65b9\u9762\u4f18\u4e8e\u7269\u7406\u6a21\u578b\uff0c\u5bf9\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u5206\u6790\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86UniPlanner\uff0c\u9996\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u591a\u6570\u636e\u96c6\u96c6\u6210\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u6280\u672f\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u89c4\u5212\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u7684\u8f66\u8f86\u8f68\u8ff9\u5206\u5e03\u548c\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\u5177\u6709\u663e\u8457\u4e00\u81f4\u6027\uff0c\u8fd9\u4e3a\u591a\u6570\u636e\u96c6\u96c6\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "method": "1. \u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5b57\u5178\u7f51\u7edc(HFTDN)\uff1a\u805a\u5408\u591a\u6570\u636e\u96c6\u7684\u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5bf9\uff0c\u57fa\u4e8e\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u672a\u6765\u8f68\u8ff9\u751f\u6210\u8de8\u6570\u636e\u96c6\u89c4\u5212\u6307\u5bfc\n2. \u65e0\u68af\u5ea6\u8f68\u8ff9\u6620\u5c04\u5668(GFTM)\uff1a\u4ece\u591a\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\uff0c\u5c06\u5386\u53f2\u8f68\u8ff9\u8f6c\u6362\u4e3a\u901a\u7528\u89c4\u5212\u5148\u9a8c\n3. \u7a00\u758f\u5230\u5bc6\u96c6(S2D)\u8303\u5f0f\uff1a\u8bad\u7ec3\u65f6\u81ea\u9002\u5e94dropout\u9009\u62e9\u6027\u6291\u5236\u89c4\u5212\u5148\u9a8c\uff0c\u63a8\u7406\u65f6\u5145\u5206\u5229\u7528\u5148\u9a8c\u6700\u5927\u5316\u89c4\u5212\u6027\u80fd", "result": "UniPlanner\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u9632\u6b62\u4e86\u6377\u5f84\u5b66\u4e60\uff0c\u786e\u4fdd\u89c4\u5212\u77e5\u8bc6\u7684\u5b89\u5168\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6570\u636e\u96c6\u96c6\u6210\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0cUniPlanner\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.23667", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23667", "abs": "https://arxiv.org/abs/2510.23667", "authors": ["Amin Heyrani Nobari", "Lyle Regenwetter", "Cyril Picard", "Ligong Han", "Faez Ahmed"], "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization", "comment": null, "summary": "Structural topology optimization (TO) is central to engineering design but\nremains computationally intensive due to complex physics and hard constraints.\nExisting deep-learning methods are limited to fixed square grids, a few\nhand-coded boundary conditions, and post-hoc optimization, preventing general\ndeployment. We introduce Optimize Any Topology (OAT), a foundation-model\nframework that directly predicts minimum-compliance layouts for arbitrary\naspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines\na resolution- and shape-agnostic autoencoder with an implicit neural-field\ndecoder and a conditional latent-diffusion model trained on OpenTO, a new\ncorpus of 2.2 million optimized structures covering 2 million unique\nboundary-condition configurations. On four public benchmarks and two\nchallenging unseen tests, OAT lowers mean compliance up to 90% relative to the\nbest prior models and delivers sub-1 second inference on a single GPU across\nresolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These\nresults establish OAT as a general, fast, and resolution-free framework for\nphysics-aware topology optimization and provide a large-scale dataset to spur\nfurther research in generative modeling for inverse design. Code & data can be\nfound at https://github.com/ahnobari/OptimizeAnyTopology.", "AI": {"tldr": "OAT\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u9884\u6d4b\u4efb\u610f\u957f\u5bbd\u6bd4\u3001\u5206\u8fa8\u7387\u3001\u4f53\u79ef\u5206\u6570\u3001\u8f7d\u8377\u548c\u56fa\u5b9a\u6761\u4ef6\u4e0b\u7684\u6700\u5c0f\u67d4\u5ea6\u5e03\u5c40\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u67d4\u5ea6\uff0c\u5e76\u5728\u5355GPU\u4e0a\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fa\u5b9a\u65b9\u5f62\u7f51\u683c\u3001\u5c11\u91cf\u624b\u52a8\u7f16\u7801\u7684\u8fb9\u754c\u6761\u4ef6\u548c\u540e\u5904\u7406\u4f18\u5316\uff0c\u65e0\u6cd5\u5b9e\u73b0\u901a\u7528\u90e8\u7f72\u3002", "method": "OAT\u7ed3\u5408\u4e86\u5206\u8fa8\u7387\u65e0\u5173\u548c\u5f62\u72b6\u65e0\u5173\u7684\u81ea\u7f16\u7801\u5668\u3001\u9690\u5f0f\u795e\u7ecf\u573a\u89e3\u7801\u5668\u4ee5\u53ca\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5728\u5305\u542b220\u4e07\u4e2a\u4f18\u5316\u7ed3\u6784\u7684OpenTO\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u548c\u4e24\u4e2a\u672a\u89c1\u6d4b\u8bd5\u4e2d\uff0cOAT\u76f8\u6bd4\u6700\u4f73\u73b0\u6709\u6a21\u578b\u5c06\u5e73\u5747\u67d4\u5ea6\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u572864\u00d764\u5230256\u00d7256\u5206\u8fa8\u7387\u548c\u9ad8\u8fbe10:1\u7684\u957f\u5bbd\u6bd4\u4e0b\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u63a8\u7406\u3002", "conclusion": "OAT\u4e3a\u7269\u7406\u611f\u77e5\u62d3\u6251\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u5feb\u901f\u4e14\u5206\u8fa8\u7387\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u9006\u5411\u8bbe\u8ba1\u4e2d\u7684\u751f\u6210\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5224\u65ad\u6027\u9884\u6d4b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u540c\u667a\u80fd\u4f53\u5bf9\u58f0\u660e\u771f\u5b9e\u6027\u8fdb\u884c\u8fa9\u8bba\u5e76\u751f\u6210\u8bc1\u636e\uff0c\u4f7f\u7528\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6(QBAFs)\u8868\u793a\uff0c\u7ed3\u5408\u591a\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u591a\u667a\u80fd\u4f53\u7ec4\u5408\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "motivation": "\u5c06\u5224\u65ad\u6027\u9884\u6d4b\u89c6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u672a\u6765\u4e8b\u4ef6\u7684\u53ef\u80fd\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bc1\u636e\u6536\u96c6\u548c\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u5168\u9762\u548c\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u6765\u6574\u5408\u4e0d\u540c\u6765\u6e90\u7684\u8bc1\u636e\u548c\u89c2\u70b9\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u58f0\u660e\u9a8c\u8bc1\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\uff1a(1)ArgLLM\u667a\u80fd\u4f53\u751f\u6210\u548c\u8bc4\u4f30QBAFs\uff1b(2)RbAM\u667a\u80fd\u4f53\u4ece\u5916\u90e8\u6e90\u8fdb\u884c\u5173\u7cfb\u578b\u8bba\u8bc1\u6316\u6398\uff1b(3)RAG-ArgLLM\u667a\u80fd\u4f53\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u3002\u4f7f\u75282-3\u4e2a\u667a\u80fd\u4f53\u7ec4\u5408\uff0c\u57fa\u4e8e6\u79cd\u4e0d\u540c\u57fa\u7840LLM\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u6807\u51c6\u5224\u65ad\u6027\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u667a\u80fd\u4f53\u7ec4\u5408\uff08\u7279\u522b\u662f\u4e09\u4e2a\u667a\u80fd\u4f53\uff09\u80fd\u591f\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u7528\u4e8e\u58f0\u660e\u9a8c\u8bc1\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u6765\u6e90\u7684\u8bc1\u636e\u548c\u89c2\u70b9\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5224\u65ad\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u58f0\u660e\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bba\u8bc1\u8fc7\u7a0b\u3002"}}
{"id": "2510.23682", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.SE", "I.2.11; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.23682", "abs": "https://arxiv.org/abs/2510.23682", "authors": ["Gokturk Aytug Akarlar"], "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents", "comment": "35 pages, 15 figures, 2 tables. Keywords: Large Language Models,\n  Autonomous Agents, Neuro-Symbolic AI, Causal Inference, Formal Verification,\n  Multi-Objective Optimization. Open-source code and interactive demo available", "summary": "Large language models show promise as autonomous decision-making agents, yet\ntheir deployment in high-stakes domains remains fraught with risk. Without\narchitectural safeguards, LLM agents exhibit catastrophic brittleness:\nidentical capabilities produce wildly different outcomes depending solely on\nprompt framing. We present Chimera, a neuro-symbolic-causal architecture that\nintegrates three complementary components - an LLM strategist, a formally\nverified symbolic constraint engine, and a causal inference module for\ncounterfactual reasoning. We benchmark Chimera against baseline architectures\n(LLM-only, LLM with symbolic constraints) across 52-week simulations in a\nrealistic e-commerce environment featuring price elasticity, trust dynamics,\nand seasonal demand. Under organizational biases toward either volume or margin\noptimization, LLM-only agents fail catastrophically (total loss of \\$99K in\nvolume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding\nsymbolic constraints prevents disasters but achieves only 43-87% of Chimera's\nprofit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M\nrespectively, some cases +\\$2.2M) while improving brand trust (+1.8% and\n+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+\nformal verification proves zero constraint violations across all scenarios.\nThese results establish that architectural design not prompt engineering\ndetermines the reliability of autonomous agents in production environments. We\nprovide open-source implementations and interactive demonstrations for\nreproducibility.", "AI": {"tldr": "Chimera\u662f\u4e00\u4e2a\u795e\u7ecf-\u7b26\u53f7-\u56e0\u679c\u67b6\u6784\uff0c\u96c6\u6210\u4e86LLM\u7b56\u7565\u5e08\u3001\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u7b26\u53f7\u7ea6\u675f\u5f15\u64ce\u548c\u56e0\u679c\u63a8\u7406\u6a21\u5757\uff0c\u5728\u7535\u5b50\u5546\u52a1\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u81ea\u51b3\u7b56\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528LLM\u6216LLM\u52a0\u7b26\u53f7\u7ea6\u675f\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u4ee3\u7406\u5b58\u5728\u707e\u96be\u6027\u8106\u5f31\u6027\uff0c\u76f8\u540c\u80fd\u529b\u4ec5\u56e0\u63d0\u793a\u6846\u67b6\u4e0d\u540c\u5c31\u4f1a\u4ea7\u751f\u622a\u7136\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u65f6\u5b58\u5728\u4e25\u91cd\u98ce\u9669\u3002", "method": "\u63d0\u51faChimera\u67b6\u6784\uff0c\u6574\u5408\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1aLLM\u7b56\u7565\u5e08\u7528\u4e8e\u51b3\u7b56\u5236\u5b9a\uff0c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u7b26\u53f7\u7ea6\u675f\u5f15\u64ce\u786e\u4fdd\u5b89\u5168\u7ea6\u675f\uff0c\u56e0\u679c\u63a8\u7406\u6a21\u5757\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002\u5728\u5305\u542b\u4ef7\u683c\u5f39\u6027\u3001\u4fe1\u4efb\u52a8\u6001\u548c\u5b63\u8282\u6027\u9700\u6c42\u7684\u7535\u5b50\u5546\u52a1\u73af\u5883\u4e2d\u8fdb\u884c52\u5468\u6a21\u62df\u6d4b\u8bd5\u3002", "result": "\u5728\u504f\u5411\u9500\u91cf\u6216\u5229\u6da6\u4f18\u5316\u7684\u7ec4\u7ec7\u504f\u89c1\u4e0b\uff0c\u4ec5\u4f7f\u7528LLM\u7684\u4ee3\u7406\u4f1a\u707e\u96be\u6027\u5931\u8d25\uff08\u9500\u91cf\u573a\u666f\u635f\u593199K\u7f8e\u5143\uff09\u6216\u7834\u574f\u54c1\u724c\u4fe1\u4efb\uff08\u5229\u6da6\u573a\u666f\u4e0b\u964d48.6%\uff09\u3002\u6dfb\u52a0\u7b26\u53f7\u7ea6\u675f\u53ef\u9632\u6b62\u707e\u96be\u4f46\u4ec5\u8fbe\u5230Chimera\u5229\u6da6\u768443-87%\u3002Chimera\u59cb\u7ec8\u63d0\u4f9b\u6700\u9ad8\u56de\u62a5\uff08152\u4e07\u7f8e\u5143\u548c196\u4e07\u7f8e\u5143\uff0c\u67d0\u4e9b\u60c5\u51b5\u8fbe220\u4e07\u7f8e\u5143\uff09\uff0c\u540c\u65f6\u63d0\u5347\u54c1\u724c\u4fe1\u4efb\uff08+1.8%\u548c+10.8%\uff0c\u67d0\u4e9b\u60c5\u51b5+20.86%\uff09\u3002", "conclusion": "\u67b6\u6784\u8bbe\u8ba1\u800c\u975e\u63d0\u793a\u5de5\u7a0b\u51b3\u5b9a\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002TLA+\u5f62\u5f0f\u5316\u9a8c\u8bc1\u8bc1\u660e\u6240\u6709\u573a\u666f\u4e0b\u96f6\u7ea6\u675f\u8fdd\u89c4\u3002"}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5fae\u773c\u52a8\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6210\u529f\u5206\u7c7b\u4e0d\u540c\u89d2\u4f4d\u79fb\u7684\u5fae\u773c\u52a8\uff0c\u51c6\u786e\u7387\u7ea690%\u3002", "motivation": "\u4f20\u7edf\u5fae\u773c\u52a8\u7814\u7a76\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u4e8b\u4ef6\u76f8\u673a\u63d0\u4f9b\u4e86\u9ad8\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9700\u8981\u5efa\u7acb\u76f8\u5173\u6570\u636e\u96c6\u652f\u6301\u8ba4\u77e5\u8ba1\u7b97\u7814\u7a76\u3002", "method": "\u4f7f\u7528Blender\u6e32\u67d3\u9ad8\u4fdd\u771f\u773c\u52a8\u573a\u666f\uff0c\u6a21\u62df0.5-2.0\u5ea6\u89d2\u4f4d\u79fb\u7684\u5fae\u773c\u52a8\uff0c\u901a\u8fc7v2e\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u6d41\uff0c\u4f7f\u7528Spiking-VGG\u7cfb\u5217\u7f51\u7edc\u548c\u5149\u5b66\u6d41\u589e\u5f3a\u7684Spiking-VGG16Flow\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u5230\u7ea690%\uff0c\u80fd\u591f\u72ec\u7acb\u4e8e\u4e8b\u4ef6\u6570\u91cf\u6216\u6301\u7eed\u65f6\u95f4\u5bf9\u5fae\u773c\u52a8\u8fdb\u884c\u89d2\u4f4d\u79fb\u5206\u7c7b\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u7cbe\u7ec6\u8fd0\u52a8\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\u548c\u5f15\u5165\u7279\u6743\u5f15\u5bfc\u7b56\u7565\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u89e3\u51b3SNN\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u975e\u53ef\u5fae\u8109\u51b2\u795e\u7ecf\u5143\u9700\u8981\u66ff\u4ee3\u68af\u5ea6\u4f46\u4f18\u5316\u7279\u6027\u4e0d\u660e\u786e\uff0c\u4ee5\u53caSNN\u72b6\u6001\u52a8\u6001\u9700\u8981\u5e8f\u5217\u8bad\u7ec3\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u65e9\u671f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u6709\u9650\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\uff0c\u63d0\u51fa\u4f7f\u7528\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u6765\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u5229\u7528\u8109\u51b2\u7b56\u7565\u7684\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u8f83\u6d45\u659c\u7387\u6216\u8c03\u5ea6\u659c\u7387\u4f7f\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u63d0\u9ad82.1\u500d\u3002\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u56de\u62a5\u8fbe\u5230400\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u548cTD3BC\u7b49\u73b0\u6709\u6280\u672f\uff08\u6700\u591a-200\u5206\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u4e3a\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u7684\u5b9e\u9645\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u521b\u65b0\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2510.23794", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23794", "abs": "https://arxiv.org/abs/2510.23794", "authors": ["Jun Liu", "Tao Zhou", "Jiarui Li", "Xiaohui Zhong", "Peng Zhang", "Jie Feng", "Lei Chen", "Hao Li"], "title": "Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction", "comment": "30 pages, 21 figures, 1 table", "summary": "Tropical cyclones (TCs) are highly destructive and inherently uncertain\nweather systems. Ensemble forecasting helps quantify these uncertainties, yet\ntraditional systems are constrained by high computational costs and limited\ncapability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a\nlearnable perturbation scheme for ensemble generation, representing a novel\nAI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with\nECMWF-ENS using all 90 global TCs in 2018, examining their performance in\nTC-related physical variables, track and intensity forecasts, and the\nassociated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear\nadvantages in predicting TC-related physical variables, and achieves more\naccurate track forecasts with reduced ensemble spread, though it still\nunderestimates intensity relative to observations. Further dynamical and\nthermodynamical analyses reveal that FuXi-ENS better captures large-scale\ncirculation, with moisture turbulent energy more tightly concentrated around\nthe TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.\nThese findings highlight the potential of learnable perturbations to improve TC\nforecasting skill and provide valuable insights for advancing AI-based ensemble\nprediction of extreme weather events that have significant societal impacts.", "AI": {"tldr": "FuXi-ENS\u662f\u4e00\u79cd\u57fa\u4e8eAI\u7684\u53f0\u98ce\u96c6\u5408\u9884\u62a5\u7cfb\u7edf\uff0c\u76f8\u6bd4\u4f20\u7edfECMWF-ENS\u5728\u53f0\u98ce\u8def\u5f84\u9884\u62a5\u548c\u7269\u7406\u53d8\u91cf\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5728\u5f3a\u5ea6\u9884\u62a5\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u53f0\u98ce\u96c6\u5408\u9884\u62a5\u7cfb\u7edf\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5b8c\u5168\u8868\u5f81\u5927\u6c14\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684AI\u66ff\u4ee3\u65b9\u6848\u3002", "method": "FuXi-ENS\u91c7\u7528\u53ef\u5b66\u4e60\u6270\u52a8\u65b9\u6848\u751f\u6210\u96c6\u5408\u9884\u62a5\uff0c\u5e76\u4e0eECMWF-ENS\u57282018\u5e74\u5168\u740390\u4e2a\u53f0\u98ce\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "result": "FuXi-ENS\u5728\u53f0\u98ce\u76f8\u5173\u7269\u7406\u53d8\u91cf\u9884\u6d4b\u548c\u8def\u5f84\u9884\u62a5\u65b9\u9762\u4f18\u52bf\u660e\u663e\uff0c\u96c6\u5408\u79bb\u6563\u5ea6\u66f4\u5c0f\uff0c\u4f46\u5f3a\u5ea6\u9884\u62a5\u4ecd\u504f\u4f4e\uff1b\u80fd\u66f4\u597d\u5730\u6355\u6349\u5927\u5c3a\u5ea6\u73af\u6d41\u548c\u6696\u6838\u5468\u56f4\u7684\u6e7f\u5ea6\u6e4d\u6d41\u80fd\u91cf\u5206\u5e03\u3002", "conclusion": "\u53ef\u5b66\u4e60\u6270\u52a8\u65b9\u6848\u6709\u6f5c\u529b\u63d0\u5347\u53f0\u98ce\u9884\u62a5\u6280\u80fd\uff0c\u4e3a\u57fa\u4e8eAI\u7684\u6781\u7aef\u5929\u6c14\u96c6\u5408\u9884\u62a5\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff1aDOM\u8f6c\u6362\u6a21\u5f0f\u5904\u7406\u7f51\u9875\u590d\u6742\u6027\uff0c\u5c06\u5197\u957fDOM\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u8868\u793a\uff1b\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u4f7f\u4ee3\u7406\u80fd\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210\u672a\u77e5Web\u670d\u52a1\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u4ee3\u7406\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u53ef\u64cd\u4f5c\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u539f\u59cbHTML\u7684\u5197\u957f\u6027\u4f7f\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u5904\u7406\uff0c\u4ee5\u53ca\u786c\u7f16\u7801API\u96c6\u6210\u7684\u9759\u6001\u6027\u963b\u788d\u4ee3\u7406\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u670d\u52a1\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u6570\u636e\u4e16\u754c\u5efa\u6a21\u7684\u6a21\u5f0f\u8bed\u8a00\uff0c\u5305\u542bDOM\u8f6c\u6362\u6a21\u5f0f\u548c\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u3002DOM\u8f6c\u6362\u6a21\u5f0f\u5c06\u539f\u59cbDOM\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u4e16\u754c\u6a21\u578b\uff1b\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u901a\u8fc7\u89e3\u6790\u6807\u51c6\u5316\u8bed\u4e49\u63cf\u8ff0\u6765\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210Web\u670d\u52a1\u80fd\u529b\u3002", "result": "\u8fd9\u4e24\u79cd\u6a21\u5f0f\u5171\u540c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6846\u67b6\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u5f0f\u8bed\u8a00\u4e3a\u5de5\u7a0b\u5316\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4e92\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5728Web\u53ca\u5176\u6269\u5c55\u8d44\u6e90\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u4e16\u754c\u5efa\u6a21\u3002"}}
{"id": "2510.24366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24366", "abs": "https://arxiv.org/abs/2510.24366", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Vi Vu", "Bach X. Nguyen", "Jianhua Xing", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation", "comment": "The paper is under review at Pattern Recognition Journal", "summary": "Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5207\u6362\u5f0f\u53cc\u5b66\u751f\u67b6\u6784\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u53ef\u9760\u7684\u5b66\u751f\u6765\u589e\u5f3a\u534f\u4f5c\uff0c\u5e76\u5f15\u5165\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7b56\u7565\u6765\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u5e08\u751f\u6846\u67b6\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u53d7\u5230\u5e08\u751f\u7f51\u7edc\u5f3a\u76f8\u5173\u6027\u548c\u4e0d\u53ef\u9760\u77e5\u8bc6\u4f20\u9012\u7684\u9650\u5236\uff0c\u9700\u8981\u6539\u8fdb\u5b66\u4e60\u6548\u679c\u3002", "method": "\u91c7\u7528\u5207\u6362\u5f0f\u53cc\u5b66\u751f\u67b6\u6784\uff0c\u8fed\u4ee3\u9009\u62e9\u6700\u53ef\u9760\u5b66\u751f\uff1b\u5f15\u5165\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7b56\u7565\uff0c\u52a8\u6001\u786e\u4fdd\u6559\u5e08\u4ece\u5b66\u751f\u5438\u6536\u6709\u610f\u4e49\u4fe1\u606f\u3002", "result": "\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u76d1\u7763\u4e0b\u663e\u8457\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5373\u63d2\u5373\u7528\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u5e08\u751f\u534f\u4f5c\u548c\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u77e5\u8bc6\u4f20\u9012\u4e0d\u53ef\u9760\u95ee\u9898\u3002"}}
{"id": "2510.23868", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23868", "abs": "https://arxiv.org/abs/2510.23868", "authors": ["Zhichao Wang"], "title": "GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA", "comment": null, "summary": "I propose \\textbf{G}roup-relative \\textbf{I}mplicit \\textbf{F}ine\n\\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning\nLLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT\nminimizes the discrepancy between implicit and explicit reward models. It\ncombines three key ideas: (1) the online multi-response generation and\nnormalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the\nimplicit-explicit reward alignment principle of UNA. By jointly normalizing the\nimplicit and explicit rewards, GIFT eliminates an otherwise intractable term\nthat prevents effective use of implicit rewards. This normalization transforms\nthe complex reward maximization objective into a simple mean squared error\n(MSE) loss between the normalized reward functions, converting a non-convex\noptimization problem into a convex, stable, and analytically differentiable\nformulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy\nand thus retains exploration capability. Compared to GRPO, it requires fewer\nhyperparameters, converges faster, and generalizes better with significantly\nreduced training overfitting. Empirically, GIFT achieves superior reasoning and\nalignment performance on mathematical benchmarks while remaining\ncomputationally efficient.", "AI": {"tldr": "GIFT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9690\u5f0f\u548c\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5bf9\u9f50LLMs\uff0c\u5c06\u590d\u6742\u7684\u5956\u52b1\u6700\u5927\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u7b80\u5355\u7684MSE\u635f\u5931\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\u5982PPO\u3001DPO\u7b49\u5b58\u5728\u5404\u79cd\u9650\u5236\uff1aPPO\u76f4\u63a5\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u4f46\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0cDPO\u4f7f\u7528\u79bb\u7ebf\u6570\u636e\u7f3a\u4e4f\u63a2\u7d22\u80fd\u529b\uff0cGRPO\u9700\u8981\u8f83\u591a\u8d85\u53c2\u6570\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5728\u7ebf\u63a2\u7d22\u80fd\u529b\uff0c\u53c8\u5177\u6709\u7a33\u5b9a\u8bad\u7ec3\u548c\u826f\u597d\u6cdb\u5316\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e86GRPO\u7684\u591a\u54cd\u5e94\u751f\u6210\u548c\u5f52\u4e00\u5316\u3001DPO\u7684\u9690\u5f0f\u5956\u52b1\u516c\u5f0f\u5316\u3001UNA\u7684\u9690\u5f0f-\u663e\u5f0f\u5956\u52b1\u5bf9\u9f50\u539f\u5219\uff0c\u901a\u8fc7\u8054\u5408\u5f52\u4e00\u5316\u9690\u5f0f\u548c\u663e\u5f0f\u5956\u52b1\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u7684MSE\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u63a8\u7406\u548c\u5bf9\u9f50\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u8bad\u7ec3\u8fc7\u62df\u5408\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "GIFT\u901a\u8fc7\u521b\u65b0\u7684\u9690\u5f0f-\u663e\u5f0f\u5956\u52b1\u5bf9\u9f50\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u3001\u63a2\u7d22\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24591", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2510.24591", "abs": "https://arxiv.org/abs/2510.24591", "authors": ["Christine Ye", "Sihan Yuan", "Suchetha Cooray", "Steven Dillmann", "Ian L. V. Roque", "Dalya Baron", "Philipp Frank", "Sergio Martin-Alvarez", "Nolan Koblischke", "Frank J Qu", "Diyi Yang", "Risa Wechsler", "Ioana Ciuca"], "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?", "comment": null, "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.", "AI": {"tldr": "ReplicationBench\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u4ee3\u7406\u4f5c\u4e3a\u79d1\u7814\u52a9\u624b\u80fd\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9AI\u590d\u5236\u5929\u4f53\u7269\u7406\u5b66\u8bba\u6587\u6765\u6d4b\u8bd5\u5176\u5de5\u4f5c\u5fe0\u5b9e\u6027\u548c\u6b63\u786e\u6027\u3002", "motivation": "\u968f\u7740\u524d\u6cbfAI\u4ee3\u7406\u5728\u79d1\u7814\u52a9\u624b\u4e2d\u7684\u6f5c\u529b\u589e\u52a0\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u5f00\u653e\u7814\u7a76\u5de5\u4f5c\u4e2d\u7684\u57fa\u7840\u5fe0\u5b9e\u6027\u548c\u6b63\u786e\u6027\u3002", "method": "\u5c06\u5929\u4f53\u7269\u7406\u5b66\u8bba\u6587\u5206\u89e3\u4e3a\u4efb\u52a1\uff0c\u8981\u6c42AI\u4ee3\u7406\u590d\u5236\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\uff0c\u5305\u62ec\u5b9e\u9a8c\u8bbe\u7f6e\u3001\u63a8\u5bfc\u3001\u6570\u636e\u5206\u6790\u548c\u4ee3\u7801\u5e93\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u4e0e\u539f\u8bba\u6587\u4f5c\u8005\u5171\u540c\u5f00\u53d1\u3002", "result": "ReplicationBench\u5bf9\u5f53\u524d\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u6781\u5177\u6311\u6218\u6027\uff1a\u5373\u4f7f\u8868\u73b0\u6700\u597d\u7684\u8bed\u8a00\u6a21\u578b\u5f97\u5206\u4e5f\u4f4e\u4e8e20%\u3002", "conclusion": "ReplicationBench\u5efa\u7acb\u4e86\u9996\u4e2a\u8bba\u6587\u89c4\u6a21\u3001\u4e13\u5bb6\u9a8c\u8bc1\u7684\u5929\u4f53\u7269\u7406\u5b66\u7814\u7a76\u4efb\u52a1\u57fa\u51c6\uff0c\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u591a\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u4e3a\u8861\u91cfAI\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2510.23977", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23977", "abs": "https://arxiv.org/abs/2510.23977", "authors": ["Yohan Abeysinghe", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Ron Sarafian", "Fahad Shahbaz Khan", "Yinon Rudich", "Salman Khan"], "title": "Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling", "comment": null, "summary": "Air pollution remains a leading global health and environmental risk,\nparticularly in regions vulnerable to episodic air pollution spikes due to\nwildfires, urban haze and dust storms. Accurate forecasting of particulate\nmatter (PM) concentrations is essential to enable timely public health warnings\nand interventions, yet existing models often underestimate rare but hazardous\npollution events. Here, we present SynCast, a high-resolution neural\nforecasting model that integrates meteorological and air composition data to\nimprove predictions of both average and extreme pollution levels. Built on a\nregionally adapted transformer backbone and enhanced with a diffusion-based\nstochastic refinement module, SynCast captures the nonlinear dynamics driving\nPM spikes more accurately than existing approaches. Leveraging on harmonized\nERA5 and CAMS datasets, our model shows substantial gains in forecasting\nfidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),\nespecially under extreme conditions. We demonstrate that conventional loss\nfunctions underrepresent distributional tails (rare pollution events) and show\nthat SynCast, guided by domain-aware objectives and extreme value theory,\nsignificantly enhances performance in highly impacted regions without\ncompromising global accuracy. This approach provides a scalable foundation for\nnext-generation air quality early warning systems and supports climate-health\nrisk mitigation in vulnerable regions.", "AI": {"tldr": "SynCast\u662f\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u6c14\u8c61\u548c\u7a7a\u6c14\u6210\u5206\u6570\u636e\u6765\u6539\u8fdb\u9897\u7c92\u7269\u6d53\u5ea6\u7684\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u6c61\u67d3\u4e8b\u4ef6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7a7a\u6c14\u6c61\u67d3\u662f\u5168\u7403\u5065\u5eb7\u548c\u73af\u5883\u7684\u4e3b\u8981\u98ce\u9669\uff0c\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u4f4e\u4f30\u7f55\u89c1\u4f46\u5371\u9669\u7684\u6c61\u67d3\u4e8b\u4ef6\uff0c\u9700\u8981\u66f4\u51c6\u786e\u9884\u6d4b\u9897\u7c92\u7269\u6d53\u5ea6\u4ee5\u652f\u6301\u53ca\u65f6\u7684\u516c\u5171\u536b\u751f\u9884\u8b66\u548c\u5e72\u9884\u3002", "method": "\u57fa\u4e8e\u533a\u57df\u9002\u5e94\u7684transformer\u67b6\u6784\uff0c\u7ed3\u5408\u6269\u6563\u57fa\u968f\u673a\u7cbe\u70bc\u6a21\u5757\uff0c\u5229\u7528ERA5\u548cCAMS\u6570\u636e\u96c6\uff0c\u91c7\u7528\u9886\u57df\u611f\u77e5\u76ee\u6807\u548c\u6781\u503c\u7406\u8bba\u6307\u5bfc\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u9897\u7c92\u7269\u53d8\u91cf\uff08PM1\u3001PM2.5\u3001PM10\uff09\u9884\u6d4b\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\uff0c\u5728\u4e0d\u5f71\u54cd\u5168\u5c40\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u5f71\u54cd\u533a\u57df\u7684\u6027\u80fd\u3002", "conclusion": "SynCast\u4e3a\u4e0b\u4e00\u4ee3\u7a7a\u6c14\u8d28\u91cf\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u652f\u6301\u8106\u5f31\u533a\u57df\u7684\u6c14\u5019\u5065\u5eb7\u98ce\u9669\u7f13\u89e3\u3002"}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "PathoGaze1.0\u662f\u4e00\u4e2a\u5168\u9762\u7684\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e86\u75c5\u7406\u5b66\u5bb6\u5728\u764c\u75c7\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u5b8c\u6574\u7684\u89c6\u89c9\u641c\u7d22\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5305\u62ec\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\u3002", "motivation": "\u75c5\u7406\u5b66\u5bb6\u5bf9\u5343\u5146\u50cf\u7d20\u5168\u5207\u7247\u56fe\u50cf\u7684\u89e3\u91ca\u51c6\u786e\u7387\u5e73\u5747\u53ea\u670970%\uff0c\u4e14\u589e\u52a0\u7b2c\u4e8c\u4f4d\u75c5\u7406\u5b66\u5bb6\u5e76\u4e0d\u80fd\u663e\u8457\u6539\u5584\u51b3\u7b56\u4e00\u81f4\u6027\u3002\u8be5\u9886\u57df\u7f3a\u4e4f\u8db3\u591f\u7684\u884c\u4e3a\u6570\u636e\u6765\u89e3\u91ca\u8bca\u65ad\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528PTAH\u6d4b\u8bd5\u5e73\u53f0\u6536\u96c6\u4e8619\u4f4d\u75c5\u7406\u5b66\u5bb6\u89e3\u8bfb397\u5f20WSIs\u7684\u6570\u636e\uff0c\u5305\u62ec18.69\u5c0f\u65f6\u7684\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u3001\u523a\u6fc0\u8ffd\u8e2a\u3001\u89c6\u53e3\u5bfc\u822a\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\u3002", "result": "\u603b\u5171\u8bb0\u5f55\u4e86171,909\u6b21\u6ce8\u89c6\u3001263,320\u6b21\u626b\u89c6\u548c1,867,362\u6b21\u9f20\u6807\u4ea4\u4e92\u4e8b\u4ef6\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u884c\u4e3a\u6570\u636e\u96c6\u3002", "conclusion": "\u8fd9\u4e9b\u6570\u636e\u53ef\u7528\u4e8e\u6539\u5584\u75c5\u7406\u5b66\u5bb6\u548cAI\u7cfb\u7edf\u7684\u8bad\u7ec3\uff0c\u652f\u6301\u4eba\u7c7b\u4e13\u5bb6\u3002\u6240\u6709\u5b9e\u9a8c\u5df2\u9884\u6ce8\u518c\uff0c\u5b8c\u6574\u6570\u636e\u96c6\u548c\u5206\u6790\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.24592", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24592", "abs": "https://arxiv.org/abs/2510.24592", "authors": ["Guoxin Chen", "Jing Wu", "Xinjie Chen", "Wayne Xin Zhao", "Ruihua Song", "Chengxi Li", "Kai Fan", "Dayiheng Liu", "Minpeng Liao"], "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization", "comment": "Ongoing Work", "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.", "AI": {"tldr": "ReForm\u662f\u4e00\u79cd\u53cd\u5c04\u5f0f\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u6574\u5408\u5230\u5f62\u5f0f\u5316\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fed\u4ee3\u751f\u6210\u5f62\u5f0f\u5316\u8bed\u53e5\u3001\u8bc4\u4f30\u8bed\u4e49\u4fdd\u771f\u5ea6\u5e76\u901a\u8fc7\u6e10\u8fdb\u4f18\u5316\u81ea\u6211\u7ea0\u6b63\u9519\u8bef\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u5f62\u5f0f\u5316\u8bed\u53e5\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u539f\u59cb\u95ee\u9898\u7684\u8bed\u4e49\u610f\u56fe\uff0c\u8fd9\u6e90\u4e8e\u5c06\u81ea\u52a8\u5f62\u5f0f\u5316\u89c6\u4e3a\u7b80\u5355\u7ffb\u8bd1\u4efb\u52a1\u800c\u7f3a\u4e4f\u4eba\u7c7b\u4e13\u5bb6\u81ea\u7136\u4f7f\u7528\u7684\u81ea\u6211\u53cd\u601d\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u3002", "method": "\u63d0\u51faReForm\u53cd\u5c04\u5f0f\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u524d\u77bb\u6709\u754c\u5e8f\u5217\u4f18\u5316(PBSO)\u8bad\u7ec3\uff0c\u5728\u4e0d\u540c\u5e8f\u5217\u4f4d\u7f6e\u4f7f\u7528\u4e0d\u540c\u5956\u52b1\u6765\u786e\u4fdd\u6a21\u578b\u65e2\u53d1\u5c55\u51c6\u786e\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u80fd\u529b\uff0c\u53c8\u8fdb\u884c\u6b63\u786e\u7684\u8bed\u4e49\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u81ea\u52a8\u5f62\u5f0f\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReForm\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5e73\u5747\u63d0\u5347\u4e8617.2\u4e2a\u767e\u5206\u70b9\u3002\u8fd8\u5f15\u5165\u4e86\u5305\u542b859\u4e2a\u4e13\u5bb6\u6807\u6ce8\u9879\u7684ConsistencyCheck\u57fa\u51c6\uff0c\u9a8c\u8bc1\u4e86LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u81ea\u52a8\u5f62\u5f0f\u5316\u672c\u8d28\u4e0a\u5177\u6709\u6311\u6218\u6027\uff0c\u5373\u4f7f\u4eba\u7c7b\u4e13\u5bb6\u4e5f\u4f1a\u5728\u9ad8\u8fbe38.5%\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u8bed\u4e49\u9519\u8bef\uff0c\u800cReForm\u901a\u8fc7\u53cd\u5c04\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23980", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.23980", "abs": "https://arxiv.org/abs/2510.23980", "authors": ["Guojing Cong", "Tom Potok", "Hamed Poursiami", "Maryam Parsa"], "title": "HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing", "comment": null, "summary": "We present a novel algorithm, \\hdgc, that marries graph convolution with\nbinding and bundling operations in hyperdimensional computing for transductive\ngraph learning. For prediction accuracy \\hdgc outperforms major and popular\ngraph neural network implementations as well as state-of-the-art\nhyperdimensional computing implementations for a collection of homophilic\ngraphs and heterophilic graphs. Compared with the most accurate learning\nmethodologies we have tested, on the same target GPU platform, \\hdgc is on\naverage 9561.0 and 144.5 times faster than \\gcnii, a graph neural network\nimplementation and HDGL, a hyperdimensional computing implementation,\nrespectively. As the majority of the learning operates on binary vectors, we\nexpect outstanding energy performance of \\hdgc on neuromorphic and emerging\nprocess-in-memory devices.", "AI": {"tldr": "HDGC\u7b97\u6cd5\u5c06\u56fe\u5377\u79ef\u4e0e\u8d85\u7ef4\u8ba1\u7b97\u4e2d\u7684\u7ed1\u5b9a\u548c\u6346\u7ed1\u64cd\u4f5c\u76f8\u7ed3\u5408\uff0c\u5728\u8f6c\u5bfc\u56fe\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8d85\u7ef4\u8ba1\u7b97\u7684\u4f18\u52bf\uff0c\u5f00\u53d1\u4e00\u79cd\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u7684\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u80fd\u6548\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "method": "\u5c06\u56fe\u5377\u79ef\u4e0e\u8d85\u7ef4\u8ba1\u7b97\u4e2d\u7684\u7ed1\u5b9a\u548c\u6346\u7ed1\u64cd\u4f5c\u76f8\u7ed3\u5408\uff0c\u4e3b\u8981\u64cd\u4f5c\u5728\u4e8c\u503c\u5411\u91cf\u4e0a\u8fdb\u884c\u3002", "result": "\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8d85\u7ef4\u8ba1\u7b97\u65b9\u6cd5\uff1b\u5728\u76f8\u540cGPU\u5e73\u53f0\u4e0a\uff0c\u6bd4GCNII\u5feb9561\u500d\uff0c\u6bd4HDGL\u5feb144.5\u500d\u3002", "conclusion": "HDGC\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u5408\u5728\u795e\u7ecf\u5f62\u6001\u548c\u5185\u5b58\u8ba1\u7b97\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f18\u5f02\u7684\u80fd\u6548\u8868\u73b0\u3002"}}
{"id": "2510.24698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24698", "abs": "https://arxiv.org/abs/2510.24698", "authors": ["Baixuan Li", "Dingchu Zhang", "Jialong Wu", "Wenbiao Yin", "Zhengwei Tao", "Yida Zhao", "Liwen Zhang", "Haiyang Shen", "Runnan Fang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking", "comment": null, "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.", "AI": {"tldr": "ParallelMuse\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8303\u5f0f\uff0c\u901a\u8fc7\u529f\u80fd\u6027\u6307\u5b9a\u90e8\u5206\u5c55\u5f00\u548c\u538b\u7f29\u63a8\u7406\u805a\u5408\u6765\u89e3\u51b3\u5e76\u884c\u601d\u7ef4\u5728\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u957f\u65f6\u7a0b\u63a8\u7406\u6574\u5408\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5e76\u884c\u601d\u7ef4\u5728\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u4e2d\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4ece\u5934\u5f00\u59cb\u91cd\u590d\u5c55\u5f00\u7684\u4f4e\u6548\u7387\uff0c\u4ee5\u53ca\u7531\u4e8e\u4e0a\u4e0b\u6587\u5bb9\u91cf\u9650\u5236\u800c\u96be\u4ee5\u5728\u7b54\u6848\u751f\u6210\u8fc7\u7a0b\u4e2d\u6574\u5408\u957f\u65f6\u7a0b\u63a8\u7406\u8f68\u8ff9\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u662f\u529f\u80fd\u6027\u6307\u5b9a\u90e8\u5206\u5c55\u5f00\uff0c\u5c06\u751f\u6210\u7684\u5e8f\u5217\u5212\u5206\u4e3a\u529f\u80fd\u533a\u57df\uff0c\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u8def\u5f84\u91cd\u7528\u548c\u5206\u652f\u4ee5\u63d0\u5347\u63a2\u7d22\u6548\u7387\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u538b\u7f29\u63a8\u7406\u805a\u5408\uff0c\u5229\u7528\u63a8\u7406\u5197\u4f59\u65e0\u635f\u538b\u7f29\u4e0e\u7b54\u6848\u63a8\u5bfc\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u5e76\u5408\u6210\u8fde\u8d2f\u7684\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u4ee3\u7406\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe62%\uff0c\u540c\u65f6\u63a2\u7d22\u6027token\u6d88\u8017\u51cf\u5c1110-30%\u3002", "conclusion": "ParallelMuse\u901a\u8fc7\u9ad8\u6548\u7684\u63a2\u7d22\u7b56\u7565\u548c\u4fe1\u606f\u538b\u7f29\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2510.24240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24240", "abs": "https://arxiv.org/abs/2510.24240", "authors": ["Edward Markai", "Sina Molavipour"], "title": "Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction", "comment": null, "summary": "Temporal Knowledge Graphs have emerged as a powerful way of not only modeling\nstatic relationships between entities but also the dynamics of how relations\nevolve over time. As these informational structures can be used to store\ninformation from a real-world setting, such as a news flow, predicting future\ngraph components to a certain extent equates predicting real-world events. Most\nof the research in this field focuses on embedding-based methods, often\nleveraging convolutional neural net architectures. These solutions act as black\nboxes, limiting insight. In this paper, we explore an extension to an\nestablished rule-based framework, TLogic, that yields a high accuracy in\ncombination with explainable predictions. This offers transparency and allows\nthe end-user to critically evaluate the rules applied at the end of the\nprediction stage. The new rule format incorporates entity category as a key\ncomponent with the purpose of limiting rule application only to relevant\nentities. When categories are unknown for building the graph, we propose a\ndata-driven method to generate them with an LLM-based approach. Additionally,\nwe investigate the choice of aggregation method for scores of retrieved\nentities when performing category prediction.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u57fa\u4e8e\u89c4\u5219\u7684TLogic\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5b9e\u4f53\u7c7b\u522b\u6765\u9650\u5236\u89c4\u5219\u5e94\u7528\u8303\u56f4\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5f53\u7c7b\u522b\u672a\u77e5\u65f6\uff0c\u4f7f\u7528LLM\u65b9\u6cd5\u751f\u6210\u7c7b\u522b\uff0c\u5e76\u7814\u7a76\u4e86\u7c7b\u522b\u9884\u6d4b\u4e2d\u7684\u805a\u5408\u65b9\u6cd5\u9009\u62e9\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\u65b9\u6cd5\u591a\u4e3a\u57fa\u4e8e\u5d4c\u5165\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u9ad8\u51c6\u786e\u7387\u548c\u53ef\u89e3\u91ca\u9884\u6d4b\uff0c\u63d0\u4f9b\u900f\u660e\u6027\u8ba9\u7528\u6237\u80fd\u591f\u8bc4\u4f30\u9884\u6d4b\u9636\u6bb5\u5e94\u7528\u7684\u89c4\u5219\u3002", "method": "\u6269\u5c55TLogic\u89c4\u5219\u6846\u67b6\uff0c\u65b0\u89c4\u5219\u683c\u5f0f\u5c06\u5b9e\u4f53\u7c7b\u522b\u4f5c\u4e3a\u5173\u952e\u7ec4\u4ef6\uff0c\u9650\u5236\u89c4\u5219\u4ec5\u5e94\u7528\u4e8e\u76f8\u5173\u5b9e\u4f53\u3002\u5f53\u7c7b\u522b\u672a\u77e5\u65f6\uff0c\u91c7\u7528\u57fa\u4e8eLLM\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u751f\u6210\u7c7b\u522b\uff0c\u5e76\u7814\u7a76\u7c7b\u522b\u9884\u6d4b\u4e2d\u7684\u5b9e\u4f53\u5f97\u5206\u805a\u5408\u65b9\u6cd5\u9009\u62e9\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u901a\u8fc7\u5b9e\u4f53\u7c7b\u522b\u9650\u5236\u63d0\u9ad8\u4e86\u89c4\u5219\u5e94\u7528\u7684\u9488\u5bf9\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6269\u5c55TLogic\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u4f53\u7c7b\u522b\u7684\u5f15\u5165\u589e\u5f3a\u4e86\u89c4\u5219\u5e94\u7528\u7684\u7cbe\u786e\u6027\uff0cLLM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u522b\u672a\u77e5\u7684\u95ee\u9898\uff0c\u4e3a\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
