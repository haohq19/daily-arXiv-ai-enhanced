<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4是一个无需训练的四维时空检索增强推理框架，为视觉语言模型提供结构化终身记忆，通过在度量空间和时间中锚定物体级语义描述来构建持久世界模型，支持多智能体共享和协作推理。


<details>
  <summary>Details</summary>
Motivation: 人类通过构建持久、结构化的内部表征来感知和推理四维环境，这些多模态记忆支持回忆过去事件、推断未观察状态和上下文相关推理。受此启发，研究旨在为视觉语言模型提供类似的四维时空推理能力。

Method: R4框架通过将物体级语义描述锚定在度量空间和时间中，持续构建四维知识数据库。推理时将自然语言查询分解为语义、空间和时间键来检索相关观察，然后将检索结果整合到视觉语言模型的推理过程中。该方法无需训练，可直接在四维空间中进行检索。

Result: 在具身问答和导航基准测试中，R4在时空信息检索和推理方面显著优于基线方法，展示了在动态环境中进行四维推理的新范式。

Conclusion: R4框架成功地为视觉语言模型提供了结构化终身记忆能力，实现了无需训练的四维时空检索增强推理，为动态环境中的具身智能体推理开辟了新途径。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [2] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于准直器闪烁星点模式的事件相机标定方法，用于解决长距离高精度测量场景下的几何标定问题


<details>
  <summary>Details</summary>
Motivation: 事件相机作为新型仿生视觉传感器具有高动态范围和高时间分辨率优势，但在长距离测量场景下的几何标定（包括内参和外参）仍然是一个重要挑战，需要满足长距离和高精度的双重需求

Method: 使用带有闪烁星点模式的准直器进行标定，首先基于准直器的球体运动模型线性求解相机参数，然后通过非线性优化对这些参数进行高精度细化

Result: 通过不同条件下的综合真实世界实验验证，该方法在准确性和可靠性方面始终优于现有的事件相机标定方法

Conclusion: 提出的基于准直器闪烁星点模式的事件相机标定方法能够有效解决长距离高精度测量需求，为事件相机的几何标定提供了更准确可靠的解决方案

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [3] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 提出CattleAct方法，通过将牛群行为交互分解为个体动作组合，利用对比学习构建统一动作-交互潜在空间，实现从单张图像自动检测放牧牛群的行为交互


<details>
  <summary>Details</summary>
Motivation: 智能畜牧业需要自动检测牛群行为交互（如发情检测），但现有研究主要针对人类交互检测，牛群交互检测面临缺乏全面行为数据集的挑战，因为放牧牛群的交互是罕见事件

Method: 1) 从大规模牛群动作数据集学习动作潜在空间；2) 通过对比学习微调预训练潜在空间，将罕见交互嵌入其中，构建统一的动作-交互潜在空间；3) 开发集成视频和GPS输入的实用工作系统

Result: 在商业规模牧场上的实验表明，该方法相比基线实现了准确的行为交互检测

Conclusion: CattleAct通过数据高效的方法解决了牛群交互检测的数据稀缺问题，为智能畜牧业提供了实用的行为交互检测系统

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [4] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: C-DGPA提出了一种基于类中心的双对齐生成提示适应方法，通过双分支架构同时优化边缘分布对齐和条件分布对齐，以解决无监督域适应中视觉语言模型提示调优面临的领域差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有的提示调优策略主要对齐边缘分布，但忽视了条件分布差异，导致类原型错位和语义判别性下降等关键问题。需要一种能够同时处理两种分布差异的方法。

Method: C-DGPA采用双分支架构：1) 边缘分布对齐分支使用动态对抗训练框架桥接边缘分布差异；2) 条件分布对齐分支引入类映射机制(CMM)通过标准化语义提示理解和防止源域过依赖来对齐条件分布差异。

Result: 在OfficeHome、Office31和VisDA-2017数据集上的广泛实验验证了C-DGPA的优越性，在所有基准测试中都取得了新的最先进结果。

Conclusion: C-DGPA通过双对齐策略将领域知识有效整合到提示学习中，确保获得领域不变且语义可判别的表示，解决了现有方法在无监督域适应中的局限性。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [5] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 事件相机在端到端驾驶中能更好地应对昼夜光照差异的领域差距，相比传统相机保持更一致的性能


<details>
  <summary>Details</summary>
Motivation: 传统相机在端到端驾驶中面临领域差距问题，特别是昼夜光照差异会导致性能显著下降。需要寻找能跨光照条件保持性能的替代传感器

Method: 提出使用事件相机替代传统相机，评估其在昼夜光照差异下的跨领域性能表现

Result: 事件相机在不同光照条件下保持更一致的性能，其领域偏移惩罚通常与灰度帧相当或更小，在跨领域场景中提供更优的基线性能

Conclusion: 事件相机是应对端到端驾驶中昼夜光照领域差距的有效替代方案，无需额外调整即可跨光照条件保持性能

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [6] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出基于Grad-CAM的弱监督深度学习框架，仅使用图像级标签实现肺炎分类与定位，评估7种预训练模型，ResNet-18和EfficientNet-B0表现最佳。


<details>
  <summary>Details</summary>
Motivation: 胸部X光常用于肺炎诊断，但精确定位肺炎区域需要昂贵的像素级标注。为解决这一问题，本研究旨在开发仅需图像级标签的弱监督方法，降低标注成本。

Method: 使用Grad-CAM生成肺炎区域热力图，评估7种预训练模型（包括Vision Transformer），采用焦点损失和患者级数据分割防止数据泄漏。

Result: 所有模型分类准确率达96-98%，ResNet-18和EfficientNet-B0表现最佳，MobileNet-V3作为轻量级替代方案。Grad-CAM热力图显示模型聚焦于临床相关肺区域。

Conclusion: 弱监督可解释AI模型在肺炎筛查中具有潜力，能增强临床信任和透明度，为放射诊断提供实用解决方案。

Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.

</details>


### [7] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: TTP是一种轻量级测试时防御框架，通过空间填充前后的余弦相似度变化检测对抗样本，并使用可训练填充恢复注意力模式，在不损害干净准确率的情况下显著提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本识别方面表现优异，但对对抗扰动高度敏感，在安全关键场景中存在风险。现有训练时防御需要标注数据和昂贵重训练，而测试时策略无法可靠区分干净和对抗输入，无法同时达到最优对抗鲁棒性和干净准确率。

Method: 提出测试时填充（TTP）框架：1）通过计算空间填充前后CLIP特征嵌入的余弦相似度变化来检测对抗输入，使用通用阈值；2）对检测到的对抗样本使用可训练填充恢复被破坏的注意力模式，并结合相似度感知集成策略；3）对干净输入保持不变或可选集成现有测试时适应技术。

Result: 在多种CLIP骨干网络和细粒度基准测试上的实验表明，TTP持续超越最先进的测试时防御方法，在不损害干净准确率的情况下显著提升对抗鲁棒性。

Conclusion: TTP提供了一种轻量级、有效的测试时防御方案，解决了现有方法在对抗检测和适应方面的局限性，为视觉语言模型的对抗鲁棒性提供了实用解决方案。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [8] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个多模态框架，通过结合文本、轨迹和参考图像来生成用户可控的世界事件视频，支持多智能体交互、物体进出、参考引导外观等复杂场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法如纯文本生成或轨迹控制的图像转视频方法在生成复杂、可控的世界事件方面存在局限，缺乏对运动、时序、可见性和视觉一致性的综合控制。

Method: 采用多模态方法，结合轨迹（编码运动、时序和可见性）、自然语言（语义意图）和参考图像（物体视觉基础），生成连贯可控的事件视频。

Result: 生成的视频不仅具有时间连贯性，还表现出涌现一致性，能保持物体身份和场景的稳定性，即使物体暂时消失也能保持一致。

Conclusion: WorldCanvas将世界模型从被动预测器提升为交互式、用户可塑造的模拟器，支持表达性世界事件的生成。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: CSA-TTA：一种跨样本增强的测试时适应框架，通过整合其他患者的低血压事件来改善术中低血压的个性化预测，解决了低血压事件稀少导致的测试时训练不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 术中低血压（IOH）具有重要的手术风险，但由于患者特异性差异，准确预测仍然具有挑战性。虽然测试时适应（TTA）为个性化预测提供了有前景的方法，但低血压事件的稀少性常常导致测试时训练不可靠。

Method: 提出CSA-TTA框架：1）构建跨样本库，将历史数据分割为低血压和非低血压样本；2）采用粗到细的检索策略：先用K-Shape聚类识别代表性聚类中心，然后基于当前患者信号检索前K个语义相似样本；3）训练时整合自监督掩码重建和回顾性序列预测信号，增强模型对快速微妙术中动态的适应性。

Result: 在VitalDB数据集和真实医院数据集上评估，与TimesFM和UniTS等先进时间序列预测模型集成。在VitalDB上，微调场景下Recall和F1分数分别提升+1.33%和+1.13%；零样本场景下分别提升+7.46%和+5.07%，表现出强大的鲁棒性和泛化能力。

Conclusion: CSA-TTA通过跨样本增强有效解决了术中低血压预测中测试时训练不可靠的问题，显著提升了预测性能，特别是在低血压事件稀少的零样本场景下表现尤为突出，为个性化医疗预测提供了有效解决方案。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [10] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 该论文将LDA主题模型的主题数选择问题形式化为离散黑盒优化问题，比较了四种优化方法，发现摊销优化器（SABBO和PABBO）在样本和时间效率上显著优于传统进化算法。


<details>
  <summary>Details</summary>
Motivation: LDA主题模型中主题数T的选择是关键设计决策，直接影响模型的统计拟合度和可解释性。传统方法通常依赖启发式或网格搜索，缺乏系统化的优化框架。

Method: 将主题数选择问题形式化为离散黑盒优化问题，每个函数评估对应训练一个LDA模型并计算验证困惑度。在固定评估预算下，比较四种优化器：遗传算法（GA）、进化策略（ES）、优先摊销黑盒优化（PABBO）和锐度感知黑盒优化（SABBO）。

Result: 虽然四种方法最终都能达到相似的困惑度范围，但摊销优化器（SABBO和PABBO）在样本和时间效率上显著更优。SABBO通常只需一次评估就能找到接近最优的主题数，PABBO在几次评估内就能找到有竞争力的配置，而GA和ES需要几乎全部预算才能达到相同区域。

Conclusion: 摊销黑盒优化方法在LDA主题数选择问题上具有显著优势，能够以更少的评估次数获得高质量结果，为超参数优化提供了更高效的解决方案。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [11] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: AIMM是一个AI驱动的框架，通过融合Reddit活动、机器人指标和市场数据，为每个股票代码生成每日操纵风险评分，用于检测社交媒体驱动的市场操纵。


<details>
  <summary>Details</summary>
Motivation: 市场操纵现在经常源于协调的社交媒体活动而非孤立交易，零售投资者、监管机构和经纪商需要能够将在线叙事和协调模式与市场行为联系起来的工具。

Method: 使用parquet-native管道和Streamlit仪表板，融合Reddit活动、机器人和协调指标以及OHLCV市场特征，生成每日AIMM操纵风险评分。由于Reddit API限制，采用校准的合成社交特征匹配事件特征，市场数据使用真实历史数据。

Result: 构建了AIMM-GT数据集（33个标记的股票-日，涵盖8只股票），展示了初步的判别能力，AIMM在GME 2021年1月挤压峰值前22天发出预警。

Conclusion: AIMM框架能够检测社交媒体驱动的市场操纵，提供了早期预警能力，虽然当前标记数据集较小，但展示了研究潜力，并发布了代码、数据集架构和仪表板设计以支持相关研究。

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [12] [Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models](https://arxiv.org/abs/2512.16244)
*Xueqi Ma,Xingjun Ma,Sarah Monazam Erfani,Danilo Mandic,James Bailey*

Main category: cs.LG

TL;DR: 提出CFC框架，利用大语言模型进行图数据的开放集分类，实现从OOD检测到OOD分类的扩展，无需真实标签信息


<details>
  <summary>Details</summary>
Motivation: 现有开放集分类方法通常将所有OOD样本视为单一类别，但在欺诈检测和医疗诊断等高风险应用中，需要对OOD样本进行更深入分析，包括可能的标签预测。这引出了一个关键问题：能否在没有真实标签信息的情况下将OOD检测扩展到OOD分类？

Method: 提出粗到细开放集分类（CFC）框架，包含三个关键组件：1）粗分类器使用LLM提示进行OOD检测和异常标签生成；2）基于GNN的细分类器，使用粗分类器识别的OOD样本进行训练，增强OOD检测和ID分类；3）通过LLM提示和后处理的OOD标签实现精细化的OOD分类

Result: 实验结果表明，CFC在图和文本领域的OOD检测性能比最先进方法提高10%，在图数据集上的OOD分类准确率达到70%

Conclusion: CFC框架成功将OOD检测扩展到OOD分类，利用LLM生成语义OOD实例，提高了可解释性和实际应用价值，为高风险场景下的开放集分类提供了有效解决方案

Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.

</details>


### [13] [Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning](https://arxiv.org/abs/2512.16476)
*Pengfei Sun,Wenyu Jiang,Piew Yoong Chee,Paul Devos,Dick Botteldooren*

Main category: cs.LG

TL;DR: 提出一种无需批量归一化(BN)的全整数量化神经网络训练方法，通过渐进式逐层蒸馏实现整数推理，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有量化神经网络(QNNs)虽然通过低比特算术缩小模型并降低推理能耗，但大多仍依赖批量归一化(BN)层，这阻碍了真正的整数部署。先前尝试通过参数折叠或定制初始化移除BN，但很少能恢复BN的稳定性和准确性。

Method: 采用渐进式逐层蒸馏方案：从预训练的BN-enabled教师模型开始，使用逐层目标和渐进补偿训练学生模型，使其仅使用整数算术进行推理且不包含BN操作。

Result: 在ImageNet数据集上使用AlexNet架构，BN-free模型在激进量化下获得了具有竞争力的Top-1准确率。

Conclusion: 该方法可直接集成到标准量化工作流程中，为边缘和嵌入式设备等资源受限环境实现端到端的整数推理。

Abstract: Quantised neural networks (QNNs) shrink models and reduce inference energy through low-bit arithmetic, yet most still depend on a running statistics batch normalisation (BN) layer, preventing true integer-only deployment. Prior attempts remove BN by parameter folding or tailored initialisation; while helpful, they rarely recover BN's stability and accuracy and often impose bespoke constraints. We present a BN-free, fully integer QNN trained via a progressive, layer-wise distillation scheme that slots into existing low-bit pipelines. Starting from a pretrained BN-enabled teacher, we use layer-wise targets and progressive compensation to train a student that performs inference exclusively with integer arithmetic and contains no BN operations. On ImageNet with AlexNet, the BN-free model attains competitive Top-1 accuracy under aggressive quantisation. The procedure integrates directly with standard quantisation workflows, enabling end-to-end integer-only inference for resource-constrained settings such as edge and embedded devices.

</details>


### [14] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: 提出Abacus方法，通过预测用户事件的频率分布来增强深度序列模型，结合自监督预训练解决展示广告中的用户购买行为建模问题。


<details>
  <summary>Details</summary>
Motivation: 展示广告系统中用户购买行为建模面临正样本稀疏、用户行为随机性导致的类别不平衡和事件时间不规则问题。现有方法依赖手工"计数器"特征，忽略了用户意图的细粒度时间演化，而序列模型又缺少有用的事件计数统计信息。

Method: 提出Abacus方法，通过自监督预训练预测用户事件的频率分布。进一步提出混合目标函数，将Abacus与序列学习目标结合，融合聚合统计的稳定性和序列建模的敏感性。

Result: 在两个真实世界数据集上的实验表明，Abacus预训练优于现有方法，加速了下游任务收敛，混合方法相比基线AUC提升最高达+6.1%。

Conclusion: Abacus方法通过结合事件频率分布预测和序列建模，有效解决了展示广告中用户行为建模的挑战，在性能和收敛速度上均有显著提升。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [15] [MEPIC: Memory Efficient Position Independent Caching for LLM Serving](https://arxiv.org/abs/2512.16822)
*Qian Wang,Zahra Yousefijamarani,Morgan Lindsay Heisler,Rongzhi Gu,Bai Xiaolong,Shan Yizhou,Wei Zhang,Wang Lan,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: MEPIC：一种内存高效的KV缓存系统，通过分页对齐、块级重计算和RoPE融合技术，实现跨位置、跨请求的块级KV缓存复用，显著减少HBM内存使用


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用（如深度研究助手、编码代理和RAG系统）需要重复处理包含共享文档或代码块的长提示历史，对KV缓存造成巨大内存压力。现有的前缀缓存和位置无关缓存技术存在限制，无法有效实现跨请求的块级KV复用。

Method: 1. 将块KV对齐到分页存储；2. 将重计算从token级转移到块级，使只有第一个块是请求特定的；3. 通过注意力内核中的RoPE融合移除位置编码；4. 使剩余块完全可共享

Result: 相比最先进的PIC技术，在保持相当延迟和准确性的同时，HBM内存使用减少高达2倍；对于长提示，内存使用减少高达5倍，无需任何模型修改

Conclusion: MEPIC系统通过创新的内存优化技术，有效解决了LLM应用中KV缓存的内存效率问题，实现了跨位置、跨请求和批次的块级KV复用，显著提升了内存利用率

Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

</details>


### [16] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 论文提出知识转换（KT）方法，结合知识蒸馏、主动学习和因果推理，为在线边缘机器学习中未见数据生成伪标签，解决标签获取困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有边缘机器学习方法通常假设静态模型在中心训练后部署，无法有效处理未见数据。在线边缘机器学习允许模型直接在边缘设备上训练并持续更新，但面临如何为未来未见数据确定标签的关键挑战。

Method: 提出知识转换（KT）方法，结合知识蒸馏、主动学习和因果推理。KT作为主动学习中的"预言者"，将知识从教师模型转换到学生模型，为训练生成伪标签。

Result: 仿真实验表明，当提供稳定的教师模型时，学生模型最终能达到预期最大性能。KT在两种情况下特别有益：1）教师任务通用，可使用预训练模型；2）学生任务标签难以或昂贵获取。

Conclusion: KT方法为解决在线边缘机器学习中未见数据标签问题提供了有效方案，特别适用于教师任务通用或学生标签获取困难的场景。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 提出比例责任原则（PPD）新框架，将道德责任建模为随智能体认知状态变化的函数，揭示不确定性不会消除道德责任，而是将行动责任转化为修复责任。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架难以处理不确定性下的决策制定，通常将其简单视为行动约束。需要一种能动态建模道德责任如何随认知状态变化的新框架。

Method: 引入比例责任原则（PPD）框架，建立数学模型 D_total = K[(1-HI) + HI * g(C_signal)]，通过蒙特卡洛模拟验证系统稳定性，并在临床伦理、受赠人权利法、经济治理和人工智能四个领域应用验证。

Result: 模拟显示保持基线谦逊系数（λ>0）的系统能产生更稳定的责任分配，降低过度自信决策风险。该框架在四个领域都展示了跨学科有效性。

Conclusion: 比例责任原则通过将谦逊形式化为系统参数，为道德责任提供了数学可处理的方法，可作为复杂系统中的稳定原则，防止过度干预和疏忽，平衡认知信心与情境风险。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [18] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: AI流行病学框架通过应用群体级监测方法来治理和解释AI系统，类比流行病学在理解分子机制前通过统计证据支持公共卫生干预，绕过当前可解释性方法在部署模型规模上的复杂性限制。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性方法（如SHAP和机制可解释性）在处理大规模部署模型时面临模型复杂性的挑战。需要一种能够绕过内部模型复杂性、在群体层面监测AI输出的治理框架，使领域专家无需机器学习专业知识也能有效监管AI系统。

Method: 通过标准化捕获AI-专家交互的结构化评估字段（风险水平、对齐分数、准确度分数），将这些作为暴露变量来通过统计关联预测输出失败。类比胆固醇和血压作为预测心脏事件的暴露变量。随后通过专家覆盖和真实世界结果验证输出失败关联。

Result: 该框架对专家零负担，通过被动跟踪专家与AI建议的趋同和分歧提供自动审计追踪。由于分析输出而非内部模型计算，在机构更新模型和切换供应商时提供治理连续性。提供可靠性分数和语义评估，使专家和机构能在AI输出造成危害前检测不可靠输出。

Conclusion: AI流行病学通过使领域专家无需机器学习专业知识就能治理AI系统，实现了AI监管的民主化。该框架通过群体级监测方法绕过模型复杂性，为AI治理提供实用、可扩展的解决方案。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [19] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: LLMs在预测未来事件时普遍存在系统性过度自信问题，即使最先进的模型也表现出显著的校准误差，扩展模型规模和增强推理能力并不能自动改善校准性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在多种任务上表现出色，但它们的认知校准能力（即模型置信度与实际准确度的匹配程度）仍未被充分理解。现有基准主要评估静态知识准确性，缺乏对模型在真正未知的未来事件上量化不确定性能力的评估。

Method: 引入KalshiBench基准，包含300个来自受CFTC监管的Kalshi交易所的预测市场问题，这些问题在模型训练截止日期后才有真实结果。评估了五个前沿模型（Claude Opus 4.5、GPT-5.2、DeepSeek-V3.2、Qwen3-235B和Kimi-K2），使用预期校准误差（ECE）和Brier技能评分等指标衡量校准性能。

Result: 所有模型都表现出系统性过度自信。即使校准最好的模型（Claude Opus 4.5，ECE=0.120）也有显著校准误差。推理增强模型如GPT-5.2-XHigh校准更差（ECE=0.395），尽管准确性相当。只有一个模型获得正的Brier技能评分，表明大多数模型表现不如简单预测基准率。

Conclusion: 模型规模扩展和推理能力增强并不能自动带来校准改进，认知校准是一个需要针对性开发的独立能力。当前最先进的LLMs在量化未来事件不确定性方面存在系统性缺陷。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [20] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: 本文提出了AMUSE基准测试来评估多模态大语言模型在多说话人对话场景中的代理推理能力，并开发了RAFT框架通过奖励优化和选择性参数适应来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（如GPT-4o、Qwen3-Omni）虽然在感知方面表现强大，但在需要代理推理的多说话人对话场景中存在困难，这些场景要求模型能够追踪说话者、维持角色关系并在时间维度上对事件进行基础定位。这些能力对于音频-视频理解应用（如对话视频助手和会议分析）至关重要。

Method: 1. 提出AMUSE基准测试，围绕需要代理推理的任务设计，要求模型将复杂的音频-视觉交互分解为规划、基础定位和反思步骤。评估涵盖三种模式（零样本、引导和代理）和六个任务家族（包括时空说话者基础定位和多模态对话摘要）。
2. 提出RAFT框架，这是一个数据高效的代理对齐框架，整合了奖励优化（使用内在多模态自我评估作为奖励）和选择性参数适应，以实现数据和参数高效更新。

Result: 1. 当前模型在所有评估模式下都表现出弱的多说话人推理能力和不一致的行为（无论是否使用代理评估）。
2. 使用RAFT框架，在AMUSE基准测试上实现了高达39.52%的相对准确率提升。

Conclusion: AMUSE和RAF共同为研究多模态模型中的代理推理能力提供了实用平台，并有效提升了模型在这类复杂任务上的性能。这项工作强调了在多说话人对话场景中开发代理推理能力的重要性，并为未来的多模态模型发展提供了有价值的基准和优化框架。

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [21] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 提出一种新的元启发式优化算法PCIA，灵感来源于人类构建和使用路径的方式，通过随机种群寻找最佳路径，在数学和约束优化问题上表现优异


<details>
  <summary>Details</summary>
Motivation: 受人类构建和使用路径行为的启发，人类通常偏好热门交通路线，在路径封闭时会智能地混合现有路径构建新路线，并随机选择不同路径到达未知目的地

Method: PCIA算法模仿人类路径构建行为，生成随机种群寻找最佳路径（类似群体智能算法），每个粒子代表一条通往目的地的路径

Result: 在53个数学优化问题和13个约束优化问题上测试，结果显示PCIA与流行及最新的元启发式算法相比具有高度竞争力

Conclusion: PCIA是一种有效的元启发式优化算法，基于人类路径构建行为，在多种优化问题上表现出色，具有实际应用价值

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [22] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协调合作实现，而非单一AGI系统。作者呼吁重视这一假说并开发相应的安全框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AI系统的安全，假设最终会出现单一的通用人工智能（AGI）。但作者认为"拼凑式AGI"假说——即通用能力首先通过多个子AGI智能体协调合作实现——被严重忽视，而这一假说需要认真考虑并开发相应的安全保障措施。

Method: 提出分布式AGI安全框架，超越对单个智能体的评估和对齐。该框架核心是设计和实现虚拟智能体沙盒经济（不可渗透或半可渗透），其中智能体间交易由稳健的市场机制管理，配合适当的可审计性、声誉管理和监督，以减轻集体风险。

Result: 提出了一个应对拼凑式AGI风险的系统性框架，将安全关注点从单个智能体扩展到智能体群体协调带来的集体风险，为分布式AGI安全研究提供了新的方向。

Conclusion: 拼凑式AGI假说需要被认真对待，特别是在高级AI智能体具备工具使用、通信和协调能力的快速部署背景下。必须开发超越单个智能体对齐的分布式安全框架，通过沙盒经济和市场机制来管理智能体群体协调带来的风险。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: SocialStoryFrames：一种基于对话语境和叙事理论的形式化框架，用于建模读者对故事的丰富解读、情感和价值判断反应，并通过两个模型在大规模社交媒体故事分析中验证其效用。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以捕捉读者对故事的丰富解读、情感反应和价值判断等复杂响应，限制了叙事分析的深度和广度。需要一种能够系统建模读者反应的计算框架。

Method: 提出SocialStoryFrames形式化框架，结合叙事理论、语言语用学和心理学构建读者反应分类体系。开发SSF-Generator（通过382人调查验证）和SSF-Classifier（专家标注验证）两个模型。构建SSF-Corpus数据集（6,140个社交媒体故事），用于大规模叙事分析。

Result: 模型验证显示良好性能。应用分析揭示了不同社区中叙事意图的频率和相互依赖性，展示了跨社区叙事实践的差异性和多样性。框架能够支持在线社区讲故事行为的新研究。

Conclusion: SocialStoryFrames通过将细粒度、语境敏感的建模与通用的读者反应分类体系相结合，为大规模研究在线社区的叙事实践提供了新工具，填补了计算模型在捕捉复杂读者反应方面的空白。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [24] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 开发了一个基于Qwen2.5-7B模型和LoRA微调的信息提取管道，用于从警方通报中结构化提取15个关键字段，在死亡检测、死亡人数和省级位置提取等任务上达到95%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 从警方事件通报中结构化提取信息对及时准确的数据处理至关重要，但由于社交媒体文本的变异性和非正式性，这一任务面临重大挑战。

Method: 开发了一个领域适应的提取管道，结合了针对性的提示工程和基于LoRA的参数高效微调技术，对Qwen2.5-7B模型进行优化，处理噪声和异质性文本。

Result: LoRA微调显著提升了模型性能，死亡检测准确率超过98.36%，死亡人数提取的精确匹配率达到95.31%，省级位置提取达到95.54%。使用从27,822条微博警方通报中构建的4,933条高质量人工标注数据集。

Conclusion: 该管道为专业领域提供了经过验证的高效多任务结构化信息提取解决方案，为社会科学研究中将非结构化文本转化为可靠结构化数据提供了实用框架。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [25] [ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation](https://arxiv.org/abs/2512.16302)
*Zixuan Chen,Chongkai Gao,Lin Shao,Jieqi Shi,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: ManiLong-Shot：一种通过事件分解实现长视野单次模仿学习的新框架，在仅用10个短视野任务训练后，能在20个未见长视野任务上实现22.8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前单次模仿学习方法主要局限于短视野任务，限制了其在复杂长视野操作任务中的应用。需要一种能够处理长视野预抓取操作任务的单次模仿学习框架。

Method: 将长视野任务围绕物理交互事件进行结构化分解，将其重构为序列化交互感知基元而非直接模仿连续轨迹。基元分解可通过视觉语言模型的高层推理或基于机器人状态变化的启发式规则驱动。对每个基元，预测交互关键的不变区域，建立演示与当前观测间的对应关系，计算目标末端执行器姿态。

Result: 仿真实验中，仅用10个短视野任务训练的ManiLong-Shot通过单次模仿泛化到3个难度等级的20个未见长视野任务，相比SOTA获得22.8%的相对提升。真实机器人实验验证了其通过单次模仿学习稳健执行3个长视野操作任务的能力。

Conclusion: ManiLong-Shot框架成功扩展了单次模仿学习的能力范围，使其能够有效处理长视野预抓取操作任务，为复杂机器人技能学习提供了实用解决方案。

Abstract: One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.

</details>
