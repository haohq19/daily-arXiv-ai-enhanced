<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DescribeEarth: Describe Anything for Remote Sensing Images](https://arxiv.org/abs/2509.25654)
*Kaiyu Li,Zixuan Jiang,Xiangyong Cao,Jiayu Wang,Yuchen Xiao,Deyu Meng,Zhi Wang*

Main category: cs.CV

TL;DR: 提出了Geo-DLC任务，即遥感图像的对象级细粒度图像描述，并构建了DE-Dataset数据集和DE-Benchmark评估套件，开发了DescribeEarth多模态大语言模型，在遥感图像描述任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像描述研究主要关注图像级别，缺乏对象级细粒度解释，无法充分利用遥感图像中丰富的语义和结构信息。

Method: 构建DE-Dataset数据集（25个类别，261,806个标注实例），引入DE-Benchmark评估套件，提出DescribeEarth MLLM架构，集成尺度自适应聚焦策略和领域引导融合模块。

Result: DescribeEarth模型在DE-Benchmark上持续优于最先进的通用MLLMs，在事实准确性、描述丰富度和语法正确性方面表现优异，特别是在捕捉内在对象特征和周围环境属性方面。

Conclusion: Geo-DLC任务和DescribeEarth模型为遥感图像的对象级细粒度描述提供了有效解决方案，显著提升了描述质量。

Abstract: Automated textual description of remote sensing images is crucial for
unlocking their full potential in diverse applications, from environmental
monitoring to urban planning and disaster management. However, existing studies
in remote sensing image captioning primarily focus on the image level, lacking
object-level fine-grained interpretation, which prevents the full utilization
and transformation of the rich semantic and structural information contained in
remote sensing images. To address this limitation, we propose Geo-DLC, a novel
task of object-level fine-grained image captioning for remote sensing. To
support this task, we construct DE-Dataset, a large-scale dataset contains 25
categories and 261,806 annotated instances with detailed descriptions of object
attributes, relationships, and contexts. Furthermore, we introduce
DE-Benchmark, a LLM-assisted question-answering based evaluation suite designed
to systematically measure model capabilities on the Geo-DLC task. We also
present DescribeEarth, a Multi-modal Large Language Model (MLLM) architecture
explicitly designed for Geo-DLC, which integrates a scale-adaptive focal
strategy and a domain-guided fusion module leveraging remote sensing
vision-language model features to encode high-resolution details and remote
sensing category priors while maintaining global context. Our DescribeEarth
model consistently outperforms state-of-the-art general MLLMs on DE-Benchmark,
demonstrating superior factual accuracy, descriptive richness, and grammatical
soundness, particularly in capturing intrinsic object features and surrounding
environmental attributes across simple, complex, and even out-of-distribution
remote sensing scenarios. All data, code and weights are released at
https://github.com/earth-insights/DescribeEarth.

</details>


### [2] [Dragging with Geometry: From Pixels to Geometry-Guided Image Editing](https://arxiv.org/abs/2509.25740)
*Xinyu Pu,Hongsong Wang,Jie Gui,Pan Zhou*

Main category: cs.CV

TL;DR: GeoDrag是一种基于几何引导的拖拽式图像编辑方法，通过结合3D几何线索和2D空间先验，解决了传统2D像素平面编辑在几何密集型场景中的不精确和不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的图像编辑方法主要在2D像素平面上操作，缺乏3D几何线索，导致在旋转和透视变换等几何密集型场景中产生不精确和不一致的编辑结果。

Method: 提出统一的位移场联合编码3D几何和2D空间先验，并引入无冲突分区策略来隔离编辑区域，防止干扰并确保一致性。

Result: 在各种编辑场景下的广泛实验验证了该方法的有效性，显示出更高的精度、结构一致性和可靠的多点编辑能力。

Conclusion: GeoDrag能够实现连贯、高保真和结构一致的图像编辑，在单次前向传播中完成，解决了几何引导编辑中的关键挑战。

Abstract: Interactive point-based image editing serves as a controllable editor,
enabling precise and flexible manipulation of image content. However, most
drag-based methods operate primarily on the 2D pixel plane with limited use of
3D cues. As a result, they often produce imprecise and inconsistent edits,
particularly in geometry-intensive scenarios such as rotations and perspective
transformations. To address these limitations, we propose a novel
geometry-guided drag-based image editing method - GeoDrag, which addresses
three key challenges: 1) incorporating 3D geometric cues into pixel-level
editing, 2) mitigating discontinuities caused by geometry-only guidance, and 3)
resolving conflicts arising from multi-point dragging. Built upon a unified
displacement field that jointly encodes 3D geometry and 2D spatial priors,
GeoDrag enables coherent, high-fidelity, and structure-consistent editing in a
single forward pass. In addition, a conflict-free partitioning strategy is
introduced to isolate editing regions, effectively preventing interference and
ensuring consistency. Extensive experiments across various editing scenarios
validate the effectiveness of our method, showing superior precision,
structural consistency, and reliable multi-point editability. The code will be
available on https://github.com/xinyu-pu/GeoDrag .

</details>


### [3] [Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding](https://arxiv.org/abs/2509.25794)
*Haotian Xue,Yunhao Ge,Yu Zeng,Zhaoshuo Li,Ming-Yu Liu,Yongxin Chen,Jiaojiao Fan*

Main category: cs.CV

TL;DR: 提出了Point-It-Out (PIO)基准测试，通过精确视觉定位系统评估视觉语言模型的具身推理能力，包含三个层次阶段：指称对象定位、任务驱动指向和视觉轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要通过基于图像标注的多选题评估VLMs的具身推理能力，缺乏对精确视觉定位能力的系统评估。

Method: 设计分层评估协议，包含三个阶段：S1指称对象定位、S2任务驱动指向、S3视觉轨迹预测，数据来自室内、厨房、驾驶和机器人操作等具身智能关键领域。

Result: 对10多个最先进VLM的实验发现：GPT-4o等通用模型在精确视觉定位中表现不如某些开源模型；MoLMO在S1和S2表现良好但在需要视觉轨迹规划的S3中表现不佳。

Conclusion: PIO基准揭示了VLMs在具身推理中的能力差异，强调了精确视觉定位评估的重要性，并为模型改进提供了方向。

Abstract: Vision-Language Models (VLMs) have demonstrated impressive world knowledge
across a wide range of tasks, making them promising candidates for embodied
reasoning applications. However, existing benchmarks primarily evaluate the
embodied reasoning ability of VLMs through multiple-choice questions based on
image annotations -- for example, selecting which trajectory better describes
an event in the image. In this work, we introduce the Point-It-Out (PIO)
benchmark, a novel benchmark designed to systematically assess the embodied
reasoning abilities of VLMs through precise visual grounding. We propose a
hierarchical evaluation protocol spanning three stages (S1: referred-object
localization, S2: task-driven pointing, and S3: visual trace prediction), with
data collected from critical domains for embodied intelligence, including
indoor, kitchen, driving, and robotic manipulation scenarios. Extensive
experiments with over ten state-of-the-art VLMs reveal several interesting
findings. For example, strong general-purpose models such as GPT-4o, while
excelling on many benchmarks (e.g., language, perception, and reasoning),
underperform compared to some open-source models in precise visual grounding;
models such as MoLMO perform well in S1 and S2 but struggle in S3, where
requires grounding combined with visual trace planning.

</details>


### [4] [MuSLR: Multimodal Symbolic Logical Reasoning](https://arxiv.org/abs/2509.25851)
*Jundong Xu,Hao Fei,Yuhui Zhang,Liangming Pan,Qijun Huang,Qian Liu,Preslav Nakov,Min-Yen Kan,William Yang Wang,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出了首个多模态符号逻辑推理基准MuSLR，包含1,093个实例，涵盖7个领域。评估发现当前最先进的视觉语言模型在多模态符号推理方面表现不佳，最佳模型GPT-4.1仅达到46.8%。提出了LogiCAM框架，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态符号逻辑推理在自动驾驶、医疗诊断等高风险应用中至关重要，其严谨的确定性推理有助于防止严重后果。需要评估当前视觉语言模型在这方面的能力。

Method: 构建了MuSLR基准数据集，包含1,093个实例，涵盖7个领域、35种原子符号逻辑和976种逻辑组合。提出了LogiCAM模块化框架，将形式逻辑规则应用于多模态输入。

Result: 评估了7个最先进的视觉语言模型，发现它们都在多模态符号推理方面表现不佳，GPT-4.1最佳但仅达到46.8%。LogiCAM框架将GPT-4.1的思维链性能提升了14.13%，在复杂逻辑如一阶逻辑上提升更大。错误分析显示约70%的失败源于模态间的逻辑不对齐。

Conclusion: 当前视觉语言模型在多模态符号逻辑推理方面存在显著不足，LogiCAM框架能有效提升性能。错误分析为未来改进提供了关键见解。

Abstract: Multimodal symbolic logical reasoning, which aims to deduce new facts from
multimodal input via formal logic, is critical in high-stakes applications such
as autonomous driving and medical diagnosis, as its rigorous, deterministic
reasoning helps prevent serious consequences. To evaluate such capabilities of
current state-of-the-art vision language models (VLMs), we introduce the first
benchmark MuSLR for multimodal symbolic logical reasoning grounded in formal
logical rules. MuSLR comprises 1,093 instances across 7 domains, including 35
atomic symbolic logic and 976 logical combinations, with reasoning depths
ranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that
they all struggle with multimodal symbolic reasoning, with the best model,
GPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework
that applies formal logical rules to multimodal inputs, boosting GPT-4.1's
Chain-of-Thought performance by 14.13%, and delivering even larger gains on
complex logics such as first-order logic. We also conduct a comprehensive error
analysis, showing that around 70% of failures stem from logical misalignment
between modalities, offering key insights to guide future improvements. All
data and code are publicly available at https://llm-symbol.github.io/MuSLR.

</details>


### [5] [Predicting Penalty Kick Direction Using Multi-Modal Deep Learning with Pose-Guided Attention](https://arxiv.org/abs/2509.26088)
*Pasindu Ranasinghe,Pamudu Ranasinghe*

Main category: cs.CV

TL;DR: 开发了一个实时多模态深度学习框架，用于在足球点球触球前预测射门方向（左、中、右），准确率达89%，推理时间22毫秒。


<details>
  <summary>Details</summary>
Motivation: 点球经常决定冠军归属，但守门员必须在极短时间内从微妙的生物力学线索中预测射门意图。

Method: 采用双分支架构：MobileNetV2 CNN处理RGB帧空间特征，带注意力机制的LSTM网络处理2D关键点，姿态关键点引导视觉关注任务相关区域，使用距离阈值方法分割输入序列。

Result: 在755个点球事件数据集上测试，准确率89%，比纯视觉和纯姿态基线模型提升14-22%，推理时间22毫秒。

Conclusion: 该轻量级可解释设计适用于守门员训练、战术分析和实时比赛分析。

Abstract: Penalty kicks often decide championships, yet goalkeepers must anticipate the
kicker's intent from subtle biomechanical cues within a very short time window.
This study introduces a real-time, multi-modal deep learning framework to
predict the direction of a penalty kick (left, middle, or right) before ball
contact. The model uses a dual-branch architecture: a MobileNetV2-based CNN
extracts spatial features from RGB frames, while 2D keypoints are processed by
an LSTM network with attention mechanisms. Pose-derived keypoints further guide
visual focus toward task-relevant regions. A distance-based thresholding method
segments input sequences immediately before ball contact, ensuring consistent
input across diverse footage. A custom dataset of 755 penalty kick events was
created from real match videos, with frame-level annotations for object
detection, shooter keypoints, and final ball placement. The model achieved 89%
accuracy on a held-out test set, outperforming visual-only and pose-only
baselines by 14-22%. With an inference time of 22 milliseconds, the lightweight
and interpretable design makes it suitable for goalkeeper training, tactical
analysis, and real-time game analytics.

</details>


### [6] [TSalV360: A Method and Dataset for Text-driven Saliency Detection in 360-Degrees Videos](https://arxiv.org/abs/2509.26208)
*Ioannis Kontostathis,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出了TSV360数据集和TSalV360方法，用于360度视频中的文本驱动显著性检测，通过视觉语言模型和注意力机制实现基于文本描述的定制化显著性检测。


<details>
  <summary>Details</summary>
Motivation: 解决360度视频中基于文本描述的显著性检测问题，使系统能够根据用户提供的文本描述来检测视频中的显著对象或事件。

Method: 扩展并适配了SOTA视觉方法，开发了TSalV360方法，利用SOTA视觉语言模型进行数据表示，集成相似性估计模块和视口时空交叉注意力机制。

Result: 使用TSV360数据集进行的定量和定性评估显示，TSalV360与SOTA视觉方法相比具有竞争力，能够有效执行定制化的文本驱动显著性检测。

Conclusion: TSalV360方法在360度视频中成功实现了文本驱动的显著性检测，为基于文本描述的定制化视频分析提供了有效解决方案。

Abstract: In this paper, we deal with the task of text-driven saliency detection in
360-degrees videos. For this, we introduce the TSV360 dataset which includes
16,000 triplets of ERP frames, textual descriptions of salient objects/events
in these frames, and the associated ground-truth saliency maps. Following, we
extend and adapt a SOTA visual-based approach for 360-degrees video saliency
detection, and develop the TSalV360 method that takes into account a
user-provided text description of the desired objects and/or events. This
method leverages a SOTA vision-language model for data representation and
integrates a similarity estimation module and a viewport spatio-temporal
cross-attention mechanism, to discover dependencies between the different data
modalities. Quantitative and qualitative evaluations using the TSV360 dataset,
showed the competitiveness of TSalV360 compared to a SOTA visual-based approach
and documented its competency to perform customized text-driven saliency
detection in 360-degrees videos.

</details>


### [7] [DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance](https://arxiv.org/abs/2509.26498)
*Jijun Xiang,Longliang Liu,Xuan Zhu,Xianqi Wang,Min Lin,Xin Yang*

Main category: cs.CV

TL;DR: 提出了DEPTHOR++框架，通过合成数据训练、无参数异常检测和深度补全网络，显著提升了轻量dToF传感器在噪声环境下的深度增强性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度增强方法假设理想的dToF输入和完美的dToF-RGB对齐，忽略了实际应用中的校准误差和异常，限制了真实世界的适用性。

Method: 1. 基于合成数据集的模拟方法生成真实训练样本；2. 无学习参数异常检测机制识别错误dToF测量；3. 针对噪声dToF输入设计的深度补全网络，整合RGB图像和单目深度估计先验。

Result: 在ZJU-L5数据集上RMSE和Rel分别平均提升22%和11%；在Mirror3D-NYU数据集上镜面区域性能提升37%；在Hammer数据集上超越RealSense L515测量结果22%。

Conclusion: 该方法显著提升了深度增强的鲁棒性和性能，使低成本传感器能够超越高端设备，在多种真实世界数据集上验证了有效性和泛化能力。

Abstract: Depth enhancement, which converts raw dToF signals into dense depth maps
using RGB guidance, is crucial for improving depth perception in high-precision
tasks such as 3D reconstruction and SLAM. However, existing methods often
assume ideal dToF inputs and perfect dToF-RGB alignment, overlooking
calibration errors and anomalies, thus limiting real-world applicability. This
work systematically analyzes the noise characteristics of real-world
lightweight dToF sensors and proposes a practical and novel depth completion
framework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three
key aspects. First, we introduce a simulation method based on synthetic
datasets to generate realistic training samples for robust model training.
Second, we propose a learnable-parameter-free anomaly detection mechanism to
identify and remove erroneous dToF measurements, preventing misleading
propagation during completion. Third, we design a depth completion network
tailored to noisy dToF inputs, which integrates RGB images and pre-trained
monocular depth estimation priors to improve depth recovery in challenging
regions. On the ZJU-L5 dataset and real-world samples, our training strategy
significantly boosts existing depth completion models, with our model achieving
state-of-the-art performance, improving RMSE and Rel by 22% and 11% on average.
On the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our
model improves upon the previous SOTA by 37% in mirror regions. On the Hammer
dataset, using simulated low-cost dToF data from RealSense L515, our method
surpasses the L515 measurements with an average gain of 22%, demonstrating its
potential to enable low-cost sensors to outperform higher-end devices.
Qualitative results across diverse real-world datasets further validate the
effectiveness and generalizability of our approach.

</details>


### [8] [Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation](https://arxiv.org/abs/2509.26555)
*Agneet Chatterjee,Rahim Entezari,Maksym Zhuravinskyi,Maksim Lapin,Reshinth Adithyan,Amit Raj,Chitta Baral,Yezhou Yang,Varun Jampani*

Main category: cs.CV

TL;DR: 提出了Stable Cinemetrics评估框架，将电影制作控制分为四个层次化分类：设置、事件、灯光和摄像机，包含76个细粒度控制节点。通过大规模人类研究和自动评估器分析当前视频生成模型在专业需求上的表现差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型和基准测试未能捕捉专业视频生成的复杂性和需求，需要建立面向专业电影制作的结构化评估框架。

Method: 引入四个解耦的层次化分类法，构建专业用例对齐的提示基准，开发自动提示分类和问题生成流程，进行大规模人类研究（10+模型、20K视频、80+电影专业人士标注），训练自动评估器。

Result: 即使最强的现有模型在事件和摄像机相关控制方面仍存在显著差距，训练的自动评估器在专家标注对齐方面优于现有零样本基线。

Conclusion: SCINE是首个将专业视频生成置于视频生成模型背景下的方法，通过以电影控制为中心的分类法和结构化评估流程指导未来研究。

Abstract: Recent advances in video generation have enabled high-fidelity video
synthesis from user provided prompts. However, existing models and benchmarks
fail to capture the complexity and requirements of professional video
generation. Towards that goal, we introduce Stable Cinemetrics, a structured
evaluation framework that formalizes filmmaking controls into four
disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.
Together, these taxonomies define 76 fine-grained control nodes grounded in
industry practices. Using these taxonomies, we construct a benchmark of prompts
aligned with professional use cases and develop an automated pipeline for
prompt categorization and question generation, enabling independent evaluation
of each control dimension. We conduct a large-scale human study spanning 10+
models and 20K videos, annotated by a pool of 80+ film professionals. Our
analysis, both coarse and fine-grained reveal that even the strongest current
models exhibit significant gaps, particularly in Events and Camera-related
controls. To enable scalable evaluation, we train an automatic evaluator, a
vision-language model aligned with expert annotations that outperforms existing
zero-shot baselines. SCINE is the first approach to situate professional video
generation within the landscape of video generative models, introducing
taxonomies centered around cinematic controls and supporting them with
structured evaluation pipelines and detailed analyses to guide future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [DPSformer: A long-tail-aware model for improving heavy rainfall prediction](https://arxiv.org/abs/2509.25208)
*Zenghui Huang,Ting Shu,Zhonglei Wang,Yang Lu,Yan Yan,Wei Zhong,Hanzi Wang*

Main category: cs.LG

TL;DR: DPSformer是一个针对暴雨预报的长尾学习模型，通过高分辨率分支增强暴雨事件的表示，显著提升了暴雨预报的准确性。


<details>
  <summary>Details</summary>
Motivation: 降水分布高度不平衡，大多数观测记录无雨或小雨，而暴雨事件罕见。这种不平衡分布阻碍了深度学习模型有效预测暴雨事件。

Method: 将降雨预报明确视为长尾学习问题，引入DPSformer模型，通过高分辨率分支丰富暴雨事件的表示。

Result: 对于≥50mm/6h的暴雨事件，DPSformer将基准数值天气预报模型的CSI从0.012提升到0.067；对于前1%的暴雨事件，其FSS超过0.45，优于现有方法。

Conclusion: 这项工作为暴雨预测建立了一个有效的长尾范式，提供了增强预警系统和减轻极端天气事件社会影响的实用工具。

Abstract: Accurate and timely forecasting of heavy rainfall remains a critical
challenge for modern society. Precipitation exhibits a highly imbalanced
distribution: most observations record no or light rain, while heavy rainfall
events are rare. Such an imbalanced distribution obstructs deep learning models
from effectively predicting heavy rainfall events. To address this challenge,
we treat rainfall forecasting explicitly as a long-tailed learning problem,
identifying the insufficient representation of heavy rainfall events as the
primary barrier to forecasting accuracy. Therefore, we introduce DPSformer, a
long-tail-aware model that enriches representation of heavy rainfall events
through a high-resolution branch. For heavy rainfall events $ \geq $ 50 mm/6 h,
DPSformer lifts the Critical Success Index (CSI) of a baseline Numerical
Weather Prediction (NWP) model from 0.012 to 0.067. For the top 1% coverage of
heavy rainfall events, its Fraction Skill Score (FSS) exceeds 0.45, surpassing
existing methods. Our work establishes an effective long-tailed paradigm for
heavy rainfall prediction, offering a practical tool to enhance early warning
systems and mitigate the societal impacts of extreme weather events.

</details>


### [10] [STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting](https://arxiv.org/abs/2509.25210)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Lei Bai*

Main category: cs.LG

TL;DR: 提出了STCast框架，通过空间对齐注意力机制自适应优化区域边界，并使用时序专家混合模块动态分配月度预测，在区域天气预报中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有区域天气预报方法受限于静态不精确的区域边界，导致泛化能力差。需要开发能够自适应优化边界并动态处理时序变化的AI方法。

Method: 提出STCast框架：1) 空间对齐注意力机制(SAA) - 对齐全局和区域空间分布来初始化边界，基于注意力模式自适应优化边界；2) 时序专家混合模块(TMoE) - 使用离散高斯分布将不同月份的大气变量动态路由到专门专家，增强时序模式捕捉能力。

Result: 实验结果表明，STCast在全局和区域预报、极端事件预测和集合预报四个任务上均优于现有最先进方法，表现出持续的优势。

Conclusion: STCast通过自适应边界优化和动态时序分配，有效提升了区域天气预报的准确性和泛化能力，为AI驱动的天气预报提供了新的解决方案。

Abstract: To gain finer regional forecasts, many works have explored the regional
integration from the global atmosphere, e.g., by solving boundary equations in
physics-based methods or cropping regions from global forecasts in data-driven
methods. However, the effectiveness of these methods is often constrained by
static and imprecise regional boundaries, resulting in poor generalization
ability. To address this issue, we propose Spatial-Temporal Weather Forecasting
(STCast), a novel AI-driven framework for adaptive regional boundary
optimization and dynamic monthly forecast allocation. Specifically, our
approach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns
global and regional spatial distributions to initialize boundaries and
adaptively refines them based on attention-derived alignment patterns.
Furthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where
atmospheric variables from distinct months are dynamically routed to
specialized experts using a discrete Gaussian distribution, enhancing the
model's ability to capture temporal patterns. Beyond global and regional
forecasting, we evaluate our STCast on extreme event prediction and ensemble
forecasting. Experimental results demonstrate consistent superiority over
state-of-the-art methods across all four tasks.

</details>


### [11] [How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data](https://arxiv.org/abs/2509.25263)
*Yifang Zhang,Pengfei Duan,Henan Wang,Shengwu Xiong*

Main category: cs.LG

TL;DR: 提出了RainfallBench基准，用于0-3小时降雨临近预报，包含5年全球12000多个GNSS站点的气象数据，特别包含可降水量水汽(PWV)指标。针对降雨数据的零膨胀、时间衰减和非平稳性特点，设计了专门的评估策略，并提出了Bi-Focus Precipitation Forecaster (BFPF)模块来提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有气象时间序列预测基准主要针对具有强周期性的变量(如温度、湿度)，无法反映模型在更复杂和实际的降雨临近预报场景中的能力。降雨临近预报具有零膨胀、时间衰减和非平稳性等挑战性特征，需要专门的基准和评估方法。

Method: 构建了RainfallBench数据集，包含5年15分钟间隔的6个关键气象变量数据，特别加入了可降水量水汽(PWV)。设计了针对多尺度预测和极端降雨事件的专门评估策略。提出了Bi-Focus Precipitation Forecaster (BFPF)模块，通过融入领域先验知识来解决零膨胀和时间衰减问题。

Result: 在RainfallBench上评估了20多个最先进模型，涵盖6种主要架构。统计分析和消融研究验证了数据集的全面性和所提方法的优越性。BFPF模块有效提升了降雨时间序列预测性能。

Conclusion: RainfallBench填补了降雨临近预报基准的空白，为评估模型在实际复杂气象场景中的能力提供了标准。BFPF模块通过融入领域先验知识，有效解决了降雨数据特有的零膨胀和时间衰减问题，显著提升了预测性能。

Abstract: Rainfall nowcasting, which aims to predict precipitation within the next 0 to
3 hours, is critical for disaster mitigation and real-time response planning.
However, most time series forecasting benchmarks in meteorology are evaluated
on variables with strong periodicity, such as temperature and humidity, which
fail to reflect model capabilities in more complex and practically meteorology
scenarios like rainfall nowcasting. To address this gap, we propose
RainfallBench, a benchmark designed for rainfall nowcasting, a highly
challenging and practically relevant task characterized by zero inflation,
temporal decay, and non-stationarity, focused on predicting precipitation
within the next 0 to 3 hours. The dataset is derived from five years of
meteorological observations, recorded at 15-minute intervals across six
essential variables, and collected from more than 12,000 GNSS stations
globally. In particular, it incorporates precipitable water vapor (PWV), a
crucial indicator of rainfall that is absent in other datasets. We further
design specialized evaluation strategies to assess model performance on key
meteorological challenges, such as multi-scale prediction and extreme rainfall
events, and evaluate over 20 state-of-the-art models across six major
architectures on RainfallBench. Additionally, to address the zero-inflation and
temporal decay issues overlooked by existing models, we introduce Bi-Focus
Precipitation Forecaster (BFPF), a plug-and-play module that incorporates
domain-specific priors to enhance rainfall time series forecasting. Statistical
analysis and ablation studies validate the comprehensiveness of our dataset as
well as the superiority of our methodology. Code and datasets are available at
https://anonymous.4open.science/r/RainfallBench-A710.

</details>


### [12] [A Weather Foundation Model for the Power Grid](https://arxiv.org/abs/2509.25268)
*Cristian Bodnar,Raphaël Rousseau-Rizzi,Nikhil Shankar,James Merleau,Stylianos Flampouris,Guillem Candille,Slavica Antic,François Miralles,Jayesh K. Gupta*

Main category: cs.LG

TL;DR: 该研究对天气基础模型进行微调，为电力基础设施提供超本地化预测，在多个关键气象变量上超越传统数值天气预报，特别是实现了覆冰检测能力，为电网韧性提供智能支持。


<details>
  <summary>Details</summary>
Motivation: 天气基础模型在天气预报方面表现出色，但其对现代社会天气敏感基础设施的实际价值尚未充分探索。研究旨在验证这些模型在电网关键气象预测中的实用价值。

Method: 使用魁北克水电公司资产观测数据对Silurian AI的15亿参数生成式预报变换器模型进行微调，预测五个电网关键变量：地表温度、降水、轮毂高度风速、风机覆冰风险和架空导线覆冰积聚。

Result: 在6-72小时预报时效内，定制模型优于最先进的数值天气预报基准：温度平均绝对误差降低15%，总降水误差降低35%，风速误差降低15%，并实现了覆冰检测0.72的平均精度得分。

Conclusion: 天气基础模型经过少量高保真数据后训练后，可作为下一代电网韧性智能的实用基础，提供传统系统不具备的覆冰预警能力。

Abstract: Weather foundation models (WFMs) have recently set new benchmarks in global
forecast skill, yet their concrete value for the weather-sensitive
infrastructure that powers modern society remains largely unexplored. In this
study, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting
Transformer (GFT), on a rich archive of Hydro-Qu\'ebec asset
observations--including transmission-line weather stations, wind-farm met-mast
streams, and icing sensors--to deliver hyper-local, asset-level forecasts for
five grid-critical variables: surface temperature, precipitation, hub-height
wind speed, wind-turbine icing risk, and rime-ice accretion on overhead
conductors. Across 6-72 h lead times, the tailored model surpasses
state-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)
by 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.
Most importantly, it attains an average precision score of 0.72 for day-ahead
rime-ice detection, a capability absent from existing operational systems,
which affords several hours of actionable warning for potentially catastrophic
outage events. These results show that WFMs, when post-trained with small
amounts of high-fidelity, can serve as a practical foundation for
next-generation grid-resilience intelligence.

</details>


### [13] [Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation](https://arxiv.org/abs/2509.25379)
*Yogesh Verma,Markus Heinonen,Vikas Garg*

Main category: cs.LG

TL;DR: 提出了一种基于物理原理的非线性噪声过程，结合SE(3)流匹配范式，实现了高保真度的蛋白质结构生成和序列条件折叠。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散生成模型在蛋白质设计中忽略了蛋白质的物理真实性，缺乏物理原理支撑的噪声过程。

Method: 引入基于经典物理的非线性噪声过程，将蛋白质展开为二级结构，同时保持拓扑完整性；结合SE(3)流匹配范式建模蛋白质骨架的不变分布。

Result: 在无条件蛋白质生成中达到最先进性能，产生更具设计性和新颖性的蛋白质结构，并能准确将单体序列折叠为精确构象。

Conclusion: 该方法通过物理驱动的噪声过程和流匹配技术，显著提升了蛋白质生成的质量和物理真实性。

Abstract: Protein structure prediction and folding are fundamental to understanding
biology, with recent deep learning advances reshaping the field.
Diffusion-based generative models have revolutionized protein design, enabling
the creation of novel proteins. However, these methods often neglect the
intrinsic physical realism of proteins, driven by noising dynamics that lack
grounding in physical principles. To address this, we first introduce a
physically motivated non-linear noising process, grounded in classical physics,
that unfolds proteins into secondary structures (e.g., alpha helices, linear
beta sheets) while preserving topological integrity--maintaining bonds, and
preventing collisions. We then integrate this process with the flow-matching
paradigm on SE(3) to model the invariant distribution of protein backbones with
high fidelity, incorporating sequence information to enable
sequence-conditioned folding and expand the generative capabilities of our
model. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance in unconditional protein generation, producing
more designable and novel protein structures while accurately folding monomer
sequences into precise protein conformations.

</details>


### [14] [Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation](https://arxiv.org/abs/2509.25381)
*Penglei Gao,Yan Zou,Abhijit Duggal,Shuaiqi Huang,Faming Liang,Xiaofeng Wang*

Main category: cs.LG

TL;DR: 提出FCRN框架，用于竞争风险下的离散时间生存分析，整合功能协变量并处理缺失数据，通过端到端模型同时学习填补缺失值和预测事件特定风险。


<details>
  <summary>Details</summary>
Motivation: 在重症监护等场景中，需要处理功能协变量（如时间序列数据）和缺失数据，传统方法难以有效整合这些复杂因素。

Method: 结合用于功能数据表示的微网络基础层和基于梯度的填补模块，构建端到端深度学习框架。

Result: 在模拟数据集和真实ICU案例研究中，相比随机生存森林和传统竞争风险模型，预测准确性显著提升。

Conclusion: FCRN通过更有效地捕捉动态风险因素和静态预测因子，同时适应不规则和不完整数据，推进了重症监护中的预后建模。

Abstract: We introduce the Functional Competing Risk Net (FCRN), a unified
deep-learning framework for discrete-time survival analysis under competing
risks, which seamlessly integrates functional covariates and handles missing
data within an end-to-end model. By combining a micro-network Basis Layer for
functional data representation with a gradient-based imputation module, FCRN
simultaneously learns to impute missing values and predict event-specific
hazards. Evaluated on multiple simulated datasets and a real-world ICU case
study using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates
substantial improvements in prediction accuracy over random survival forests
and traditional competing risks models. This approach advances prognostic
modeling in critical care by more effectively capturing dynamic risk factors
and static predictors while accommodating irregular and incomplete data.

</details>


### [15] [On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study](https://arxiv.org/abs/2509.25382)
*Fernanda Zapata Bascuñán*

Main category: cs.LG

TL;DR: 本文探讨了在GW150914引力波数据上训练的混合高斯先验变分自编码器（VAE-MoG）的潜在空间。通过哈密顿蒙特卡洛采样发现，尽管模型能准确重构信号，但潜在空间存在明显不匹配，表明强去噪性能不一定意味着潜在表示可靠。


<details>
  <summary>Details</summary>
Motivation: 评估生成模型在捕获数据底层结构方面的能力，特别关注去噪性能与潜在空间表示可靠性之间的关系。

Method: 使用混合高斯先验的变分自编码器（VAE-MoG）处理引力波数据，并通过哈密顿蒙特卡洛（HMC）采样来比较干净输入条件下的后验样本与噪声数据编码器输出的差异。

Result: 模型能够准确重构信号，但统计比较显示潜在空间存在明显不匹配，表明去噪性能强并不保证潜在表示可靠。

Conclusion: 在评估生成模型时，使用基于后验的验证方法至关重要，仅凭重构精度不足以评估潜在空间的质量。

Abstract: In this work, we explore the latent space of a denoising variational
autoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on
gravitational wave data from event GW150914. To evaluate how well the model
captures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw
posterior samples conditioned on clean inputs, and compare them to the
encoder's outputs from noisy data. Although the model reconstructs signals
accurately, statistical comparisons reveal a clear mismatch in the latent
space. This shows that strong denoising performance doesn't necessarily mean
the latent representations are reliable highlighting the importance of using
posterior-based validation when evaluating generative models.

</details>


### [16] [Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring](https://arxiv.org/abs/2509.25438)
*Zhibo Hou,Zhiyu An,Wan Du*

Main category: cs.LG

TL;DR: 提出了一种名为学习进度监控（LPM）的新方法，通过奖励模型改进而非预测误差或新颖性来指导探索，有效避免智能体被环境中的不可学习随机噪声源（noisy-TV）困住。


<details>
  <summary>Details</summary>
Motivation: 传统基于内在奖励的探索方法在面对环境中的不可学习随机噪声源时，智能体会被这些噪声困住而无法有效探索。虽然基于不确定性估计或分布相似性的方法最终能逃脱，但样本效率低且计算成本高。受神经科学发现人类在探索过程中会监控自身进步的启发，提出了LPM方法。

Method: 采用双网络设计：一个误差模型预测动态模型在先前迭代中的预期预测误差，使用当前迭代与先前迭代模型误差的差异来指导探索。LPM奖励模型改进而非预测误差或新颖性。

Result: 理论分析表明LPM的内在奖励具有零等变性，是信息增益的单调指标。在基于MNIST、3D迷宫和Atari的噪声环境中，LPM的内在奖励收敛更快，在迷宫实验中探索更多状态，在Atari中获得更高的外在奖励。

Conclusion: LPM概念简单但有效，标志着噪声鲁棒探索的范式转变，通过监控学习进度而非传统方法实现了更高效的探索。

Abstract: When there exists an unlearnable source of randomness (noisy-TV) in the
environment, a naively intrinsic reward driven exploring agent gets stuck at
that source of randomness and fails at exploration. Intrinsic reward based on
uncertainty estimation or distribution similarity, while eventually escapes
noisy-TVs as time unfolds, suffers from poor sample efficiency and high
computational cost. Inspired by recent findings from neuroscience that humans
monitor their improvements during exploration, we propose a novel method for
intrinsically-motivated exploration, named Learning Progress Monitoring (LPM).
During exploration, LPM rewards model improvements instead of prediction error
or novelty, effectively rewards the agent for observing learnable transitions
rather than the unlearnable transitions. We introduce a dual-network design
that uses an error model to predict the expected prediction error of the
dynamics model in its previous iteration, and use the difference between the
model errors of the current iteration and previous iteration to guide
exploration. We theoretically show that the intrinsic reward of LPM is
zero-equivariant and a monotone indicator of Information Gain (IG), and that
the error model is necessary to achieve monotonicity correspondence with IG. We
empirically compared LPM against state-of-the-art baselines in noisy
environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.
Results show that LPM's intrinsic reward converges faster, explores more states
in the maze experiment, and achieves higher extrinsic reward in Atari. This
conceptually simple approach marks a shift-of-paradigm of noise-robust
exploration. For code to reproduce our experiments, see
https://github.com/Akuna23Matata/LPM_exploration

</details>


### [17] [Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization](https://arxiv.org/abs/2509.25538)
*Marcus Schwarting,Logan Ward,Nathaniel Hudson,Xiaoli Yan,Ben Blaiszik,Santanu Chaudhuri,Eliu Huerta,Ian Foster*

Main category: cs.LG

TL;DR: 提出了一种结合生成建模和主动学习的队列优先级算法，用于解决科学中的逆向设计问题，显著提高了高质量候选分子的生成数量。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在解决科学逆向设计问题时既带来机遇也带来风险，虽然能自主扩展和优化搜索空间，但会在充分调优前探索低质量区域。

Method: 提出了一种队列优先级算法，将生成建模与主动学习结合在分布式工作流中，通过主动学习模型优先处理顶级设计候选，防止资源浪费在无意义的候选上。

Result: 在碳捕获分子结构发现任务中，主动学习方法显著提高了高质量候选分子的数量：无主动学习平均生成281个高性能候选，有主动学习平均生成604个高性能候选。

Conclusion: 结合主动学习的生成AI工作流能有效防止生成模型衰减，显著提高高质量设计候选的识别效率。

Abstract: Generative AI poses both opportunities and risks for solving inverse design
problems in the sciences. Generative tools provide the ability to expand and
refine a search space autonomously, but do so at the cost of exploring
low-quality regions until sufficiently fine tuned. Here, we propose a queue
prioritization algorithm that combines generative modeling and active learning
in the context of a distributed workflow for exploring complex design spaces.
We find that incorporating an active learning model to prioritize top design
candidates can prevent a generative AI workflow from expending resources on
nonsensical candidates and halt potential generative model decay. For an
existing generative AI workflow for discovering novel molecular structure
candidates for carbon capture, our active learning approach significantly
increases the number of high-quality candidates identified by the generative
model. We find that, out of 1000 novel candidates, our workflow without active
learning can generate an average of 281 high-performing candidates, while our
proposed prioritization with active learning can generate an average 604
high-performing candidates.

</details>


### [18] [Binary Sparse Coding for Interpretability](https://arxiv.org/abs/2509.25596)
*Lucia Quirke,Stepan Shabalin,Nora Belrose*

Main category: cs.LG

TL;DR: 提出了二元稀疏自编码器(BAE)和二元转码器(BTC)，通过将激活值限制为0或1来提升特征的单一语义性和可解释性，但会增加重建误差。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)的许多特征只有在高激活强度下才可解释，希望通过二元化消除通过连续特征激活变化传递的不可解释信息。

Method: 使用二元稀疏自编码器(BAE)和二元转码器(BTC)，将所有激活值约束为0或1，消除连续激活强度的变化。

Result: 二元化显著提高了发现特征的可解释性和单一语义性，但增加了重建误差。同时发现二元化会增加不可解释的超高频特征数量。

Conclusion: 当可解释性分数经过频率调整后，连续稀疏编码器的得分略优于二元编码器，这表明多语义性可能是神经激活的不可消除属性。

Abstract: Sparse autoencoders (SAEs) are used to decompose neural network activations
into sparsely activating features, but many SAE features are only interpretable
at high activation strengths. To address this issue we propose to use binary
sparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all
activations to be zero or one. We find that binarisation significantly improves
the interpretability and monosemanticity of the discovered features, while
increasing reconstruction error. By eliminating the distinction between high
and low activation strengths, we prevent uninterpretable information from being
smuggled in through the continuous variation in feature activations. However,
we also find that binarisation increases the number of uninterpretable
ultra-high frequency features, and when interpretability scores are
frequency-adjusted, the scores for continuous sparse coders are slightly better
than those of binary ones. This suggests that polysemanticity may be an
ineliminable property of neural activations.

</details>


### [19] [Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2509.26114)
*Jaesung R. Park,Junsu Kim,Gyeongman Kim,Jinyoung Jo,Sean Choi,Jaewoong Cho,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文揭示了PPO和GRPO中的裁剪机制会导致熵偏差：clip-low增加熵，clip-high减少熵。在标准裁剪参数下，clip-high效应主导，导致整体熵减少。作者建议使用更激进的clip-low值来增加熵、促进探索，防止RLVR训练中的熵崩溃。


<details>
  <summary>Details</summary>
Motivation: RLVR（可验证奖励的强化学习）是提升大语言模型推理能力的主要方法，但容易发生熵崩溃，即模型快速收敛到近乎确定性的形式，阻碍长期RL训练中的探索和进展。

Method: 通过理论和实证分析，研究PPO和GRPO中裁剪机制对熵的影响，揭示clip-low增加熵、clip-high减少熵的规律。

Result: 在标准裁剪参数下，clip-high效应主导，即使提供纯随机奖励，RL算法也会导致整体熵减少。裁剪机制独立于奖励信号影响熵，进而影响推理行为。

Conclusion: 裁剪机制可以有意用于控制熵，通过更激进的clip-low值可以增加熵、促进探索，最终防止RLVR训练中的熵崩溃。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently emerged as
the leading approach for enhancing the reasoning capabilities of large language
models (LLMs). However, RLVR is prone to entropy collapse, where the LLM
quickly converges to a near-deterministic form, hindering exploration and
progress during prolonged RL training. In this work, we reveal that the
clipping mechanism in PPO and GRPO induces biases on entropy. Through
theoretical and empirical analyses, we show that clip-low increases entropy,
while clip-high decreases it. Further, under standard clipping parameters, the
effect of clip-high dominates, resulting in an overall entropy reduction even
when purely random rewards are provided to the RL algorithm. Our findings
highlight an overlooked confounding factor in RLVR: independent of the reward
signal, the clipping mechanism influences entropy, which in turn affects the
reasoning behavior. Furthermore, our analysis demonstrates that clipping can be
deliberately used to control entropy. Specifically, with a more aggressive
clip-low value, one can increase entropy, promote exploration, and ultimately
prevent entropy collapse in RLVR training.

</details>


### [20] [Entropy After $\langle \texttt{/Think} \rangle$ for reasoning model early exiting](https://arxiv.org/abs/2509.26522)
*Xi Wang,James McInerney,Lequn Wang,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出了EAT（Entropy After </Think>）方法，通过监控推理过程中停止思考标记后的熵值变化来检测和防止大语言模型的过度思考问题，从而自适应地分配计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理时存在过度思考倾向，即使已经得到正确答案仍会继续修正，导致计算资源浪费。需要一种方法来检测推理何时达到稳定状态以便及时停止。

Method: 在推理过程中添加停止思考标记</think>，监控后续标记的熵值轨迹。当熵值下降并稳定时（Pass@1达到平台期），通过指数移动平均的方差阈值来确定停止时机。

Result: 在MATH500和AIME2025数据集上，EAT方法减少了13-21%的token使用量，且不影响准确率。即使在黑盒设置下（无法访问推理模型的logits），使用代理模型计算EAT仍然有效。

Conclusion: EAT提供了一种简单有效的方法来检测和防止语言模型的过度思考问题，能够自适应地分配计算资源，显著提高推理效率。

Abstract: Large reasoning models show improved performance with longer chains of
thought. However, recent work has highlighted (qualitatively) their tendency to
overthink, continuing to revise answers even after reaching the correct
solution. We quantitatively confirm this inefficiency by tracking Pass@1 for
answers averaged over a large number of rollouts and find that the model often
begins to always produce the correct answer early in the reasoning, making
extra reasoning a waste of tokens. To detect and prevent overthinking, we
propose a simple and inexpensive novel signal -- Entropy After </Think> (EAT)
-- for monitoring and deciding whether to exit reasoning early. By appending a
stop thinking token (</think>) and monitoring the entropy of the following
token as the model reasons, we obtain a trajectory that decreases and
stabilizes when Pass@1 plateaus; thresholding its variance under an exponential
moving average yields a practical stopping rule. Importantly, our approach
enables adaptively allocating compute based on the EAT trajectory, allowing us
to spend compute in a more efficient way compared with fixing the token budget
for all questions. Empirically, on MATH500 and AIME2025, EAT reduces token
usage by 13 - 21% without harming accuracy, and it remains effective in black
box settings where logits from the reasoning model are not accessible, and EAT
is computed with proxy models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Neo-Grounded Theory: A Methodological Innovation Integrating High-Dimensional Vector Clustering and Multi-Agent Collaboration for Qualitative Research](https://arxiv.org/abs/2509.25244)
*Shuide Wen,Beier Ku,Teng Wang,Mingyang Zou,Yang Yang*

Main category: cs.AI

TL;DR: Neo Grounded Theory (NGT) 结合向量聚类和多智能体系统，解决了定性研究的规模深度悖论，能够在几小时内分析海量数据集，同时保持解释严谨性。


<details>
  <summary>Details</summary>
Motivation: 解决定性研究中规模与深度之间的矛盾，使大规模定性数据分析既高效又保持解释力。

Method: 使用1536维嵌入、层次聚类和并行基于智能体的编码，在4万字符中文访谈文本上比较NGT与手动编码及ChatGPT辅助分析的效果。

Result: NGT实现了168倍速度提升（3小时vs 3周）、更高质量（0.904 vs 0.883）和96%成本降低。人机协作至关重要：纯自动化产生抽象框架，而人工指导产生可操作的双路径理论。

Conclusion: NGT证明计算客观性与人类解释是互补的。向量表示提供可复现的语义测量，同时保留意义的解释维度。研究者从机械编码转向理论指导，AI处理模式识别，人类提供创造性洞见。

Abstract: Purpose: Neo Grounded Theory (NGT) integrates vector clustering with multi
agent systems to resolve qualitative research's scale depth paradox, enabling
analysis of massive datasets in hours while preserving interpretive rigor.
Methods: We compared NGT against manual coding and ChatGPT-assisted analysis
using 40,000 character Chinese interview transcripts. NGT employs
1536-dimensional embeddings, hierarchical clustering, and parallel agent-based
coding. Two experiments tested pure automation versus human guided refinement.
Findings: NGT achieved 168-fold speed improvement (3 hours vs 3 weeks),
superior quality (0.904 vs 0.883), and 96% cost reduction. Human AI
collaboration proved essential: automation alone produced abstract frameworks
while human guidance yielded actionable dual pathway theories. The system
discovered patterns invisible to manual coding, including identity bifurcation
phenomena. Contributions: NGT demonstrates computational objectivity and human
interpretation are complementary. Vector representations provide reproducible
semantic measurement while preserving meaning's interpretive dimensions.
Researchers shift from mechanical coding to theoretical guidance, with AI
handling pattern recognition while humans provide creative insight.
Implications: Cost reduction from \$50,000 to \$500 democratizes qualitative
research, enabling communities to study themselves. Real-time analysis makes
qualitative insights contemporaneous with events. The framework shows
computational methods can strengthen rather than compromise qualitative
research's humanistic commitments.
  Keywords: Grounded theory; Vector embeddings; Multi agent systems; Human AI
collaboration; Computational qualitative analysis

</details>


### [22] [Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments](https://arxiv.org/abs/2509.25282)
*Jiexi Xu,Jiaqi Liu,Ran Tong,Su Liu*

Main category: cs.AI

TL;DR: 提出了因果视觉编程(CVP)新范式，通过在工作流中引入因果结构来减少LLM代理的幻觉和逻辑错误。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在处理复杂任务时依赖概率关联而非真正的因果理解，导致幻觉和逻辑不一致问题。

Method: 通过低代码界面让用户定义工作流模块的"世界模型"，构建有向无环图(DAG)来明确模块间的因果关系，作为代理推理过程的约束。

Result: 在模拟分布偏移的实验中，因果锚定模型保持稳定准确率，而基于概率关联的基线模型性能显著下降。

Conclusion: CVP为构建更可解释、可靠和可信的AI代理提供了可行路径。

Abstract: Large language model (LLM) agents are increasingly capable of orchestrating
complex tasks in low-code environments. However, these agents often exhibit
hallucinations and logical inconsistencies because their inherent reasoning
mechanisms rely on probabilistic associations rather than genuine causal
understanding. This paper introduces a new programming paradigm: Causal-Visual
Programming (CVP), designed to address this fundamental issue by explicitly
introducing causal structures into the workflow design. CVP allows users to
define a simple "world model" for workflow modules through an intuitive
low-code interface, effectively creating a Directed Acyclic Graph (DAG) that
explicitly defines the causal relationships between modules. This causal graph
acts as a crucial constraint during the agent's reasoning process, anchoring
its decisions to a user-defined causal structure and significantly reducing
logical errors and hallucinations by preventing reliance on spurious
correlations. To validate the effectiveness of CVP, we designed a synthetic
experiment that simulates a common real-world problem: a distribution shift
between the training and test environments. Our results show that a causally
anchored model maintained stable accuracy in the face of this shift, whereas a
purely associative baseline model that relied on probabilistic correlations
experienced a significant performance drop. The primary contributions of this
study are: a formal definition of causal structures for workflow modules; the
proposal and implementation of a CVP framework that anchors agent reasoning to
a user-defined causal graph; and empirical evidence demonstrating the
framework's effectiveness in enhancing agent robustness and reducing errors
caused by causal confusion in dynamic environments. CVP offers a viable path
toward building more interpretable, reliable, and trustworthy AI agents.

</details>


### [23] [Building the EHR Foundation Model via Next Event Prediction](https://arxiv.org/abs/2509.25591)
*Zekai Chen,Arda Pekis,Kevin Brown*

Main category: cs.AI

TL;DR: 提出了Next Event Prediction (NEP)框架，通过自回归微调增强LLM在电子健康记录中的时序推理能力，在肿瘤生存预测和临床诊断任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统编码方法无法充分捕捉电子健康记录中的丰富时序动态，而大型语言模型在推理临床事件序列和时序依赖方面存在困难。

Method: 将电子健康记录重新表述为带时间戳的事件链，通过自回归微调预测未来医疗事件，显式建模疾病进展模式和因果关系。

Result: 在时序推理任务中，NEP在AUROC上优于专业EHR模型4.6%，在C-index上优于通用LLM 7.2%，同时获得最先进的预测准确性和临床可解释的注意力模式。

Conclusion: NEP框架成功增强了LLM的时序推理能力，在医疗预测任务中实现了优越性能，并提供了与已知疾病通路一致的临床解释性。

Abstract: Electronic Health Records (EHRs) contain rich temporal dynamics that
conventional encoding approaches fail to adequately capture. While Large
Language Models (LLMs) show promise for EHR modeling, they struggle to reason
about sequential clinical events and temporal dependencies. We propose Next
Event Prediction (NEP), a framework that enhances LLMs' temporal reasoning
through autoregressive fine-tuning on clinical event sequences. By
reformulating EHRs as timestamped event chains and predicting future medical
events, NEP explicitly models disease progression patterns and causal
relationships. Extensive evaluations across oncology survival prediction and
clinical diagnosis tasks demonstrate NEP's superiority, outperforming
specialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index
in temporal reasoning tasks. Our analyses reveal dual benefits:
state-of-the-art prediction accuracy combined with clinically interpretable
attention patterns that align with known disease pathways.

</details>


### [24] [ScheduleMe: Multi-Agent Calendar Assistant](https://arxiv.org/abs/2509.25693)
*N. de Silva,S. Perera,K. L. A. A. Nimasha,I. D. S. Fernando,R. K. A. O. Wijerathne*

Main category: cs.AI

TL;DR: ScheduleMe是一个多代理日历助手，通过自然语言管理Google日历事件，采用图结构协调机制实现模块化、冲突解决和上下文感知交互。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，需要更先进的对话助手来通过自然语言满足用户需求，特别是管理日历事件方面。

Method: 使用图结构协调机制，中央监督代理监督专门的任务代理，实现模块化、冲突解决和上下文感知交互。

Result: 系统能够解析用户命令中的歧义并评估用户指令，展示了结构化推理和代理协作的潜力。

Conclusion: 这种方法为提升个人日历助手工具的可用性和灵活性提供了范例，展示了结构化推理和代理协作如何说服操作者采用更先进的工具。

Abstract: Recent advancements in LLMs have contributed to the rise of advanced
conversational assistants that can assist with user needs through natural
language conversation. This paper presents a ScheduleMe, a multi-agent calendar
assistant for users to manage google calendar events in natural language. The
system uses a graph-structured coordination mechanism where a central
supervisory agent supervises specialized task agents, allowing modularity,
conflicts resolution, and context-aware interactions to resolve ambiguities and
evaluate user commands. This approach sets an example of how structured
reasoning and agent cooperation might convince operators to increase the
usability and flexibility of personal calendar assistant tools.

</details>


### [25] [PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks](https://arxiv.org/abs/2509.25792)
*Alexander Branch,Omead Pooladzandi,Radin Khosraviani,Sunay Gajanan Bhat,Jeffrey Jiang,Gregory Pottie*

Main category: cs.AI

TL;DR: PureVQ-GAN是一种防御数据中毒的方法，通过向量量化VAE和GAN鉴别器来破坏后门触发器，同时保持语义内容完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散防御方法需要数百次迭代步骤，速度慢且不实用。需要一种快速有效的防御机制来对抗数据中毒攻击。

Method: 使用向量量化VAE（VQ-VAE）和GAN鉴别器，通过离散瓶颈强制处理中毒图像，破坏细粒度触发器模式，同时GAN确保输出符合自然图像分布。

Result: 在CIFAR-10上，对Gradient Matching和Bullseye Polytope攻击实现0%中毒成功率，对Narcissus攻击为1.64%，同时保持91-95%的干净准确率。速度比扩散方法快50倍以上。

Conclusion: PureVQ-GAN是一种高效实用的数据中毒防御方法，在保持高准确率的同时显著提升了防御速度。

Abstract: We introduce PureVQ-GAN, a defense against data poisoning that forces
backdoor triggers through a discrete bottleneck using Vector-Quantized VAE with
GAN discriminator. By quantizing poisoned images through a learned codebook,
PureVQ-GAN destroys fine-grained trigger patterns while preserving semantic
content. A GAN discriminator ensures outputs match the natural image
distribution, preventing reconstruction of out-of-distribution perturbations.
On CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient
Matching and Bullseye Polytope attacks, and 1.64% against Narcissus while
maintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring
hundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making
it practical for real training pipelines.

</details>


### [26] [ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack](https://arxiv.org/abs/2509.25843)
*Yein Park,Jungwoo Park,Jaewoo Kang*

Main category: cs.AI

TL;DR: ASGuard是一个基于机制理解的框架，通过识别与时态越狱攻击相关的注意力头，训练精确的缩放向量来重新校准激活，并通过预防性微调增强模型的安全拒绝机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然经过安全对齐，但存在脆弱的拒绝行为，简单的语言变化（如使用过去时态）就能绕过其安全机制，这揭示了当前对齐方法的泛化差距。

Method: 1. 使用电路分析识别与时态越狱攻击相关的特定注意力头；2. 训练通道级缩放向量来重新校准这些头的激活；3. 应用预防性微调，让模型学习更鲁棒的拒绝机制。

Result: 在三个LLM上，ASGuard有效降低了目标越狱攻击的成功率，同时保持了通用能力并最小化了过度拒绝，实现了安全性和实用性的帕累托最优平衡。

Conclusion: 研究展示了如何利用对模型内部机制的深入理解来开发实用、高效和有针对性的方法调整模型行为，为更可靠和可解释的AI安全指明了方向。

Abstract: Large language models (LLMs), despite being safety-aligned, exhibit brittle
refusal behaviors that can be circumvented by simple linguistic changes. As
tense jailbreaking demonstrates that models refusing harmful requests often
comply when rephrased in past tense, a critical generalization gap is revealed
in current alignment methods whose underlying mechanisms are poorly understood.
In this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,
mechanistically-informed framework that surgically mitigates this specific
vulnerability. For the first step, we use circuit analysis to identify the
specific attention heads causally linked to the targeted jailbreaking, the
tense-changing attack. Second, we train a precise, channel-wise scaling vector
to recalibrate the activation of tense vulnerable heads. Lastly, we apply it
into a "preventative fine-tuning", forcing the model to learn a more robust
refusal mechanism. Across three LLMs, ASGuard effectively reduces the attack
success rate of targeted jailbreaking while preserving general capabilities and
minimizing over refusal, achieving a Pareto-optimal balance between safety and
utility. Our findings underscore how adversarial suffixes suppress the
propagation of the refusal-mediating direction, based on mechanistic analysis.
Furthermore, our work showcases how a deep understanding of model internals can
be leveraged to develop practical, efficient, and targeted methods for
adjusting model behavior, charting a course for more reliable and interpretable
AI safety.

</details>


### [27] [RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning](https://arxiv.org/abs/2509.25958)
*Gang Li,Yulei Qin,Xiaoyu Tan,Dingkang Yang,Yuchen Shi,Zihan Xu,Xiang Li,Xing Sun,Ke Li*

Main category: cs.AI

TL;DR: 提出RoRecomp方法解决RLVR训练中推理过程冗长和探索轨迹低效的问题，通过策略性重组训练数据引导模型进行简洁推理，在多个任务中显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 标准RLVR训练导致推理过程过度冗长和探索轨迹低效，因为仅基于结果的奖励不激励效率，且小批量内响应长度的高方差造成优化信号噪声。

Method: RoRecomp将响应分为两类批次：优先批次（结合短正确和长错误响应提供简洁梯度信号）和补偿批次（使用回放缓冲区剩余响应保持稳定性）。

Result: 在三个设置中验证有效性：零RL训练中推理长度减少27.7%，智能体RL中不必要工具调用减少46.8%且准确率提升，思维压缩中长度减少高达52.5%，性能影响最小。

Conclusion: RoRecomp是即插即用方法，能显著提升推理效率，在保持性能的同时实现简洁推理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in
eliciting complex reasoning in large language models (LLMs). However, standard
RLVR training often leads to excessively verbose processes (in reasoning tasks)
and inefficient exploration trajectories (in agentic settings), as outcome-only
rewards provide no incentive for efficiency and the high variance in response
length within relatively small rollout groups results in noisy optimization
signals. To address this, we propose Rollout Response Recomposition (RoRecomp),
a plug-and-play method that guides models toward concise reasoning by
strategically recomposing the training data. RoRecomp separates responses into
two distinct batch types: 1) priority batches, which combine short-correct and
long-incorrect responses selected from online batches to provide a clear
gradient signal for brevity, and 2) compensation batches, which utilize
remaining responses from a replay buffer to maintain stability and prevent
model collapse. To comprehensively evaluate effectiveness, we test RoRecomp
across three settings where results demonstrate substantial efficiency gains:
reducing reasoning length by 27.7% in zero RL training, reducing unnecessary
tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to
52.5% length reduction in thinking compression, all with minimal performance
impact.

</details>


### [28] [MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2509.26128)
*Asmita Sengupta,David Antony Selby,Sebastian Josef Vollmer,Gerrit Großmann*

Main category: cs.AI

TL;DR: 提出了一个从在线药物说明书构建知识图谱的端到端流程，并创建了MEDAKA数据集，包含临床相关属性如副作用、警告、禁忌症等。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学知识图谱主要关注分子相互作用或不良事件，忽略了药物说明书中丰富的临床信息。

Method: 使用网络爬虫和LLM从非结构化在线内容创建知识图谱的端到端流程，并应用于公开药物说明书生成MEDAKA数据集。

Result: 通过人工检查和LLM-as-a-Judge框架评估，与现有生物医学知识图谱和数据库比较显示更好的覆盖范围。

Conclusion: MEDAKA可支持患者安全监测和药物推荐等任务，该流程也可用于其他领域从非结构化文本构建知识图谱。

Abstract: Knowledge graphs (KGs) are increasingly used to represent biomedical
information in structured, interpretable formats. However, existing biomedical
KGs often focus narrowly on molecular interactions or adverse events,
overlooking the rich data found in drug leaflets. In this work, we present (1)
a hackable, end-to-end pipeline to create KGs from unstructured online content
using a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by
applying this method to publicly available drug leaflets. The dataset captures
clinically relevant attributes such as side effects, warnings,
contraindications, ingredients, dosage guidelines, storage instructions and
physical characteristics. We evaluate it through manual inspection and with an
LLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs
and databases. We expect MEDAKA to support tasks such as patient safety
monitoring and drug recommendation. The pipeline can also be used for
constructing KGs from unstructured texts in other domains. Code and dataset are
available at https://github.com/medakakg/medaka.

</details>


### [29] [Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical Practice](https://arxiv.org/abs/2509.26153)
*Jack Gallifant,Katherine C. Kellogg,Matt Butler,Amanda Centi,Patrick F. Doyle,Sayon Dutta,Joyce Guo,Matthew J. Hadfield,Esther H. Kim,David E. Kozono,Hugo JWL Aerts,Adam B. Landman,Raymond H. Mak,Rebecca G. Mishuris,Tanna L. Nelson,Guergana K. Savova,Elad Sharon,Benjamin C. Silverman,Umit Topaloglu,Jeremy L. Warner,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 本文提出了一个面向实践者的现场手册，用于部署使用电子健康记录数据的生成式智能体，重点解决临床AI实施中的社会技术挑战而非算法开发。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗保健领域具有巨大潜力，但其在实际临床环境中的应用与潜力之间存在显著差距。作者旨在弥合这一差距，提供实用的部署指南。

Method: 基于在Mass General Brigham部署"irAE-Agent"系统的经验，以及对20名临床医生、工程师和信息学领导的结构化访谈，分析临床AI开发中的实际挑战。

Result: 分析显示临床AI开发存在关键错位：只有不到20%的努力用于提示工程和模型开发，而超过80%被实施的社会技术工作消耗。作者将此提炼为五个"重活"：数据集成、模型验证、确保经济价值、管理系统漂移和治理。

Conclusion: 该现场手册将重点从算法开发转向必要的基础设施和实施工作，为成功将生成式AI从试点项目转化为常规临床护理提供了可行的解决方案。

Abstract: Large language models (LLMs) integrated into agent-driven workflows hold
immense promise for healthcare, yet a significant gap exists between their
potential and practical implementation within clinical settings. To address
this, we present a practitioner-oriented field manual for deploying generative
agents that use electronic health record (EHR) data. This guide is informed by
our experience deploying the "irAE-Agent", an automated system to detect
immune-related adverse events from clinical notes at Mass General Brigham, and
by structured interviews with 20 clinicians, engineers, and informatics leaders
involved in the project. Our analysis reveals a critical misalignment in
clinical AI development: less than 20% of our effort was dedicated to prompt
engineering and model development, while over 80% was consumed by the
sociotechnical work of implementation. We distill this effort into five "heavy
lifts": data integration, model validation, ensuring economic value, managing
system drift, and governance. By providing actionable solutions for each of
these challenges, this field manual shifts the focus from algorithmic
development to the essential infrastructure and implementation work required to
bridge the "valley of death" and successfully translate generative AI from
pilot projects into routine clinical care.

</details>


### [30] [TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models](https://arxiv.org/abs/2509.24803)
*Tong Guan,Zijie Meng,Dianqi Li,Shiyu Wang,Chao-Han Huck Yang,Qingsong Wen,Zuozhu Liu,Sabato Marco Siniscalchi,Ming Jin,Shirui Pan*

Main category: cs.AI

TL;DR: TSR-Suite是首个全面支持时间序列推理评估、数据管道和模型训练的综合套件，包含23K+样本和四个原子任务。基于此提出的TimeOmni-1模型在因果发现和事件感知预测等任务上显著优于GPT-4.1。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列数据集大多停留在表面对齐和问答层面，缺乏真正需要推理的任务定义和高质量数据，限制了实用时间序列推理模型的发展。

Method: 提出TSR-Suite套件，定义四个原子任务涵盖感知、外推和决策三种基本推理能力。基于此开发TimeOmni-1统一推理模型，采用多阶段训练，整合任务场景混合、新奖励函数和定制优化。

Result: TimeOmni-1在所有任务上表现出强大的分布外泛化能力，因果发现准确率显著提升（64.0% vs 35.9%），事件感知预测任务的有效响应率比GPT-4.1提高超过6%。

Conclusion: TSR-Suite为时间序列推理研究提供了首个全面评估框架，TimeOmni-1证明了统一推理模型在解决多样化现实问题中的有效性，显著推进了时间序列推理领域的发展。

Abstract: Recent advances in multimodal time series learning underscore a paradigm
shift from analytics centered on basic patterns toward advanced time series
understanding and reasoning. However, existing multimodal time series datasets
mostly remain at the level of surface alignment and question answering, without
reaching the depth of genuine reasoning. The absence of well-defined tasks that
genuinely require time series reasoning, along with the scarcity of
high-quality data, has limited progress in building practical time series
reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite
(TSR-Suite), which formalizes four atomic tasks that span three fundamental
capabilities for reasoning with time series: (1) perception, acquired through
scenario understanding and causality discovery; (2) extrapolation, realized via
event-aware forecasting; and (3) decision-making, developed through
deliberation over perception and extrapolation. TSR-Suite is the first
comprehensive time series reasoning suite that supports not only thorough
evaluation but also the data pipeline and training of TSRMs. It contains more
than 23K samples, of which 2.3K are carefully curated through a human-guided
hierarchical annotation process. Building on this foundation, we introduce
TimeOmni-1, the first unified reasoning model designed to address diverse
real-world problems demanding time series reasoning. The model is trained in
multiple stages, integrating a mixture of task scenarios, novel reward
functions, and tailored optimizations. Experiments show that TimeOmni-1
delivers strong out-of-distribution generalization across all tasks and
achieves a high rate of valid responses. It significantly improves causality
discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response
rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.

</details>


### [31] [STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models](https://arxiv.org/abs/2509.26473)
*Shaoxiong Guo,Tianyi Du,Lijun Li,Yuyao Wu,Jie Li,Jing Shao*

Main category: cs.AI

TL;DR: 本文提出STaR-Attack攻击框架，利用统一多模态模型(UMMs)中生成与理解耦合的漏洞，通过时空叙事隐藏恶意事件进行多轮越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 发现统一多模态模型存在生成与理解耦合的漏洞，攻击者可以利用生成功能制作信息丰富的对抗图像，然后通过理解功能单次吸收，形成跨模态生成注入(CMGI)攻击。

Method: STaR-Attack框架：1)定义与目标查询强相关的恶意事件；2)使用三幕叙事理论生成前后场景，隐藏恶意事件作为高潮；3)前两轮利用生成能力制作场景图像；4)引入基于图像的问答游戏，将恶意问题隐藏在良性候选中。

Result: 在Gemini-2.0-Flash上达到93.06%的攻击成功率，超越最强的FlipAttack基线方法。

Conclusion: 揭示了统一多模态模型的关键但未充分开发的漏洞，强调了对UMMs进行安全对齐的必要性。

Abstract: Unified Multimodal understanding and generation Models (UMMs) have
demonstrated remarkable capabilities in both understanding and generation
tasks. However, we identify a vulnerability arising from the
generation-understanding coupling in UMMs. The attackers can use the generative
function to craft an information-rich adversarial image and then leverage the
understanding function to absorb it in a single pass, which we call Cross-Modal
Generative Injection (CMGI). Current attack methods on malicious instructions
are often limited to a single modality while also relying on prompt rewriting
with semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We
propose STaR-Attack, the first multi-turn jailbreak attack framework that
exploits unique safety weaknesses of UMMs without semantic drift. Specifically,
our method defines a malicious event that is strongly correlated with the
target query within a spatio-temporal context. Using the three-act narrative
theory, STaR-Attack generates the pre-event and the post-event scenes while
concealing the malicious event as the hidden climax. When executing the attack
strategy, the opening two rounds exploit the UMM's generative ability to
produce images for these scenes. Subsequently, an image-based question guessing
and answering game is introduced by exploiting the understanding capability.
STaR-Attack embeds the original malicious question among benign candidates,
forcing the model to select and answer the most relevant one given the
narrative context. Extensive experiments show that STaR-Attack consistently
surpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and
surpasses the strongest prior baseline, FlipAttack. Our work uncovers a
critical yet underdeveloped vulnerability and highlights the need for safety
alignments in UMMs.

</details>


### [32] [Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models](https://arxiv.org/abs/2509.26584)
*Matheus Vinicius da Silva de Oliveira,Jonathan de Andrade Silva,Awdren de Lima Fontao*

Main category: cs.AI

TL;DR: 该研究通过蜕变测试对三个小型语言模型在RAG管道中的公平性进行评估，发现细微的人口统计变化可破坏三分之一的蜕变关系，其中种族线索是主要违规原因。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在安全性和公平性问题，包括公平性缺陷和幻觉问题。RAG虽然能缓解幻觉，但其检索内容可能加剧偏见，需要评估其公平性影响。

Method: 使用蜕变测试方法，在提示中引入受控的人口统计扰动，评估三个HuggingFace上的小型语言模型（Llama-3.2-3B-Instruct、Mistral-7B-Instruct-v0.3和Llama-3.1-Nemotron-8B）在RAG管道中的情感分析公平性。

Result: 结果显示，细微的人口统计变化可破坏高达三分之一的蜕变关系。失败分析揭示了持续的偏见层级，涉及种族线索的扰动是违规的主要原因。

Conclusion: RAG中的检索组件必须仔细管理以防止偏见放大。研究为开发者和组织采用可访问的小型语言模型提供了实用警示，强调不能牺牲公平性或可靠性。

Abstract: Large Language Models (LLMs) are widely used across multiple domains but
continue to raise concerns regarding security and fairness. Beyond known attack
vectors such as data poisoning and prompt injection, LLMs are also vulnerable
to fairness bugs. These refer to unintended behaviors influenced by sensitive
demographic cues (e.g., race or sexual orientation) that should not affect
outcomes. Another key issue is hallucination, where models generate plausible
yet false information. Retrieval-Augmented Generation (RAG) has emerged as a
strategy to mitigate hallucinations by combining external retrieval with text
generation. However, its adoption raises new fairness concerns, as the
retrieved content itself may surface or amplify bias. This study conducts
fairness testing through metamorphic testing (MT), introducing controlled
demographic perturbations in prompts to assess fairness in sentiment analysis
performed by three Small Language Models (SLMs) hosted on HuggingFace
(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),
each integrated into a RAG pipeline. Results show that minor demographic
variations can break up to one third of metamorphic relations (MRs). A detailed
analysis of these failures reveals a consistent bias hierarchy, with
perturbations involving racial cues being the predominant cause of the
violations. In addition to offering a comparative evaluation, this work
reinforces that the retrieval component in RAG must be carefully curated to
prevent bias amplification. The findings serve as a practical alert for
developers, testers and small organizations aiming to adopt accessible SLMs
without compromising fairness or reliability.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale](https://arxiv.org/abs/2509.25649)
*Samar Haider,Amir Tohidi,Jenny S. Wang,Timothy Dörr,David M. Rothschild,Chris Callison-Burch,Duncan J. Watts*

Main category: cs.CL

TL;DR: 提出了一个大规模、实时的数据集和计算框架，用于系统研究新闻媒体中的选择性和框架性偏见。该框架结合大型语言模型和新闻爬取技术，提取结构化注释，并提供了交互式网络平台。


<details>
  <summary>Details</summary>
Motivation: 主流新闻机构通过选择报道主题和框架议题来影响公众认知，但大规模测量这些微妙偏见仍具挑战性。

Method: 整合大型语言模型与可扩展的实时新闻爬取技术，提取政治倾向、语调、主题、文章类型和重大事件等结构化注释，每天处理数百篇文章。

Result: 建立了包含15万+文章的语料库，在句子、文章和发布者层面量化报道维度，揭示了新闻覆盖和偏见的深刻模式。

Conclusion: 为大规模研究媒体偏见建立了可重复的方法论，为未来研究提供实证资源，支持学术研究和提高媒体问责制的实际努力。

Abstract: Mainstream news organizations shape public perception not only directly
through the articles they publish but also through the choices they make about
which topics to cover (or ignore) and how to frame the issues they do decide to
cover. However, measuring these subtle forms of media bias at scale remains a
challenge. Here, we introduce a large, ongoing (from January 1, 2024 to
present), near real-time dataset and computational framework developed to
enable systematic study of selection and framing bias in news coverage. Our
pipeline integrates large language models (LLMs) with scalable, near-real-time
news scraping to extract structured annotations -- including political lean,
tone, topics, article type, and major events -- across hundreds of articles per
day. We quantify these dimensions of coverage at multiple levels -- the
sentence level, the article level, and the publisher level -- expanding the
ways in which researchers can analyze media bias in the modern news landscape.
In addition to a curated dataset, we also release an interactive web platform
for convenient exploration of these data. Together, these contributions
establish a reusable methodology for studying media bias at scale, providing
empirical resources for future research. Leveraging the breadth of the corpus
over time and across publishers, we also present some examples (focused on the
150,000+ articles examined in 2024) that illustrate how this novel data set can
reveal insightful patterns in news coverage and bias, supporting academic
research and real-world efforts to improve media accountability.

</details>


### [34] [Mitigating Biases in Language Models via Bias Unlearning](https://arxiv.org/abs/2509.25673)
*Dianqing Liu,Yi Liu,Guoqing Jin,Zhendong Mao*

Main category: cs.CL

TL;DR: BiasUnlearn是一种新的语言模型去偏框架，通过双路径遗忘机制协调刻板印象遗忘与反刻板印象保留，同时通过对抗性遗忘集和动态数据集交换防止偏见极性反转。


<details>
  <summary>Details</summary>
Motivation: 现有参数修改去偏方法显著降低文本连贯性和任务准确性等核心能力，而基于提示的去偏方法仅对预定义触发词有效，无法解决模型参数中深度嵌入的刻板印象关联。

Method: 提出BiasUnlearn框架，采用双路径遗忘机制协调刻板印象遗忘与反刻板印象保留，使用对抗性遗忘集和动态数据集交换防止偏见极性反转。

Result: 在多个语言模型和各种评估基准上的实验表明，BiasUnlearn在减轻语言模型偏见的同时保留语言建模能力方面优于现有方法。去偏权重可在模型变体间迁移，证实偏见表示在预训练期间固化并在微调阶段持续存在。

Conclusion: BiasUnlearn框架能有效减轻语言模型偏见，同时保持模型性能，且去偏权重具有可迁移性，表明偏见在预训练阶段就已固化。

Abstract: Many studies have shown various biases targeting different demographic groups
in language models, amplifying discrimination and harming fairness. Recent
parameter modification debiasing approaches significantly degrade core
capabilities such as text coherence and task accuracy. And Prompt-based
debiasing methods, only effective for predefined trigger words, fail to address
deeply embedded stereotypical associations in model parameters. In this paper,
we propose BiasUnlearn, a novel model debiasing framework which achieves
targeted debiasing via dual-pathway unlearning mechanisms coordinating
stereotype forgetting with anti-stereotype retention, while preventing bias
polarity reversal through adversarial forget set and dynamic dataset swapping.
We conducted extensive experiments with multiple language models across various
evaluation benchmarks. The results show that BiasUnlearn outperforms existing
methods in mitigating bias in language models while retaining language modeling
capabilities. Further experiments reveal that debiasing weights are
transferable across model variants, confirming that bias representations become
entrenched during pre-training and persist through fine-tuning phases.

</details>


### [35] [CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine](https://arxiv.org/abs/2509.26461)
*Yuyang Cheng,Linyue Cai,Changwei Peng,Yumiao Xu,Rongfang Bie,Yong Zhao*

Main category: cs.CL

TL;DR: CreAgentive是一个基于代理工作流的创意生成引擎，通过故事原型和三级代理工作流解决大语言模型在创意写作中的四大限制：体裁多样性不足、输出长度不够、叙事连贯性弱、无法执行复杂结构。


<details>
  <summary>Details</summary>
Motivation: 解决当代大语言模型在创意写作中的关键限制：体裁多样性受限、输出长度不足、叙事连贯性弱、无法强制执行复杂结构构造。

Method: 采用基于知识图谱的故事原型作为体裁无关的叙事表示，通过三级代理工作流：初始化阶段构建用户指定的叙事骨架，生成阶段通过长短目标指导多代理对话实例化故事原型，写作阶段利用原型生成具有高级结构的多体裁文本。

Result: 在广泛实验中，CreAgentive以稳定质量和低成本（每100章低于1美元）生成了数千章内容。在包含10个叙事指标的双维度评估框架下，始终优于强基线，在不同体裁中表现稳健，接近人类创作小说的质量。

Conclusion: CreAgentive通过故事原型和代理工作流架构有效解决了长文本生成的瓶颈，减少了存储冗余，在创意写作任务中实现了高质量、低成本的多体裁生成能力。

Abstract: We present CreAgentive, an agent workflow driven multi-category creative
generation engine that addresses four key limitations of contemporary large
language models in writing stories, drama and other categories of creatives:
restricted genre diversity, insufficient output length, weak narrative
coherence, and inability to enforce complex structural constructs. At its core,
CreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge
graph-based narrative representation that decouples story logic from stylistic
realization by encoding characters, events, and environments as semantic
triples. CreAgentive engages a three-stage agent workflow that comprises: an
Initialization Stage that constructs a user-specified narrative skeleton; a
Generation Stage in which long- and short-term objectives guide multi-agent
dialogues to instantiate the Story Prototype; a Writing Stage that leverages
this prototype to produce multi-genre text with advanced structures such as
retrospection and foreshadowing. This architecture reduces storage redundancy
and overcomes the typical bottlenecks of long-form generation. In extensive
experiments, CreAgentive generates thousands of chapters with stable quality
and low cost (less than $1 per 100 chapters) using a general-purpose backbone
model. To evaluate performance, we define a two-dimensional framework with 10
narrative indicators measuring both quality and length. Results show that
CreAgentive consistently outperforms strong baselines and achieves robust
performance across diverse genres, approaching the quality of human-authored
novels.

</details>


### [36] [Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization](https://arxiv.org/abs/2509.26520)
*Yaoxiang Wang,Qingguo Hu,Yucheng Ding,Ruizhe Wang,Yeyun Gong,Jian Jiao,Yelong Shen,Peng Cheng,Jinsong Su*

Main category: cs.CL

TL;DR: M-MoE训练框架通过系统性地改变训练期间激活的专家数量，在专家集合中构建从粗到细的结构，使单一模型能在不同专家数量下保持稳定性能，实现弹性推理。


<details>
  <summary>Details</summary>
Motivation: 标准Top-K路由器训练策略限制了MoE模型的弹性推理潜力，当推理时激活专家数量改变时，模型性能会急剧下降。

Method: 在训练期间系统性地改变激活专家数量，迫使模型学习有意义的专家排名：顶级专家协作提供基本粗粒度能力，后续专家逐步添加更细粒度的细节。采用分层随机化策略。

Result: 单一M-MoE模型在不同专家数量下表现出显著弹性，其性能与整个专家模型套件相当，但训练成本仅为后者的一小部分。

Conclusion: M-MoE为大规模MoE模型的实际部署提供了更实用和适应性强的解决方案，不仅实现弹性推理，还能通过在不同模型层分配不同计算预算来优化性能。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently
scaling large language models without a proportional increase in computational
cost. However, the standard training strategy of Top-K router prevents MoE
models from realizing their full potential for elastic inference. When the
number of activated experts is altered at inference time, these models exhibit
precipitous performance degradation. In this work, we introduce Matryoshka MoE
(M-MoE), a training framework that instills a coarse-to-fine structure directly
into the expert ensemble. By systematically varying the number of activated
experts during training, M-MoE compels the model to learn a meaningful ranking:
top-ranked experts collaborate to provide essential, coarse-grained
capabilities, while subsequent experts add progressively finer-grained detail.
We explore this principle at multiple granularities, identifying a layer-wise
randomization strategy as the most effective. Our experiments demonstrate that
a single M-MoE model achieves remarkable elasticity, with its performance at
various expert counts closely matching that of an entire suite of specialist
models, but at only a fraction of the total training cost. This flexibility not
only unlocks elastic inference but also enables optimizing performance by
allocating different computational budgets to different model layers. Our work
paves the way for more practical and adaptable deployments of large-scale MoE
models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [37] [On the Conic Complementarity of Planar Contacts](https://arxiv.org/abs/2509.25999)
*Yann de Mont-Marin,Louis Montaut,Jean Ponce,Martial Hebert,Justin Carpentier*

Main category: cs.RO

TL;DR: 本文提出平面Signorini条件，将点接触的Signorini定律与质心压力概念统一起来，建立了离散与连续接触模型之间的桥梁。


<details>
  <summary>Details</summary>
Motivation: 连接机器人学中两个基础原理：防止物体穿透的Signorini定律和用于优化控制的重心压力概念，填补离散与连续接触模型之间的理论空白。

Method: 提出平面Signorini条件的锥互补公式，证明其等价于在整个接触面上强制执行点状Signorini定律，并提供几何解释。

Result: 建立了统一的互补结构，自然捕捉粘附、分离和倾斜三种物理状态，并扩展了经典重心压力概念。

Conclusion: 为处理平面接触提供了数学一致且计算可行的理论基础，对接触动力学模拟和控制算法设计具有重要意义。

Abstract: We present a unifying theoretical result that connects two foundational
principles in robotics: the Signorini law for point contacts, which underpins
many simulation methods for preventing object interpenetration, and the center
of pressure (also known as the zero-moment point), a key concept used in, for
instance, optimization-based locomotion control. Our contribution is the planar
Signorini condition, a conic complementarity formulation that models general
planar contacts between rigid bodies. We prove that this formulation is
equivalent to enforcing the punctual Signorini law across an entire contact
surface, thereby bridging the gap between discrete and continuous contact
models. A geometric interpretation reveals that the framework naturally
captures three physical regimes -sticking, separating, and tilting-within a
unified complementarity structure. This leads to a principled extension of the
classical center of pressure, which we refer to as the extended center of
pressure. By establishing this connection, our work provides a mathematically
consistent and computationally tractable foundation for handling planar
contacts, with implications for both the accurate simulation of contact
dynamics and the design of advanced control and optimization algorithms in
locomotion and manipulation.

</details>


### [38] [Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance](https://arxiv.org/abs/2509.26082)
*Tianyi Jin,Melya Boukheddimi,Rohit Kumar,Gabriele Fadini,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了EA-CoRL框架，结合强化学习和进化策略，实现机器人硬件和控制策略的协同设计，在RH5人形机器人的动态引体向上任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统机器人设计采用顺序流程，硬件确定后再开发控制算法，这限制了硬件能力的充分利用。需要协同设计方法来同时优化设计和控制。

Method: EA-CoRL包含两个关键组件：设计进化（使用进化算法探索硬件配置）和策略连续适应（跨设计演化微调控制策略）。

Result: 在RH5人形机器人的动态引体向上任务中，EA-CoRL相比最先进的RL协同设计方法获得更高的适应度分数和更广的设计空间探索。

Conclusion: 策略连续适应在机器人协同设计中起着关键作用，EA-CoRL框架能够实现硬件和控制策略的有效协同优化。

Abstract: Humanoid robots have seen significant advancements in both design and
control, with a growing emphasis on integrating these aspects to enhance
overall performance. Traditionally, robot design has followed a sequential
process, where control algorithms are developed after the hardware is
finalized. However, this can be myopic and prevent robots to fully exploit
their hardware capabilities. Recent approaches advocate for co-design,
optimizing both design and control in parallel to maximize robotic
capabilities. This paper presents the Evolutionary Continuous Adaptive RL-based
Co-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with
evolutionary strategies to enable continuous adaptation of the control policy
to the hardware. EA-CoRL comprises two key components: Design Evolution, which
explores the hardware choices using an evolutionary algorithm to identify
efficient configurations, and Policy Continuous Adaptation, which fine-tunes a
task-specific control policy across evolving designs to maximize performance
rewards. We evaluate EA-CoRL by co-designing the actuators (gear ratios) and
control policy of the RH5 humanoid for a highly dynamic chin-up task,
previously unfeasible due to actuator limitations. Comparative results against
state-of-the-art RL-based co-design methods show that EA-CoRL achieves higher
fitness score and broader design space exploration, highlighting the critical
role of continuous policy adaptation in robot co-design.

</details>
