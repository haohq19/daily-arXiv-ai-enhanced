{"id": "2507.07341", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07341", "abs": "https://arxiv.org/abs/2507.07341", "authors": ["Sarah Ball", "Greg Gluch", "Shafi Goldwasser", "Frauke Kreuter", "Omer Reingold", "Guy N. Rothblum"], "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment", "comment": null, "summary": "With the increased deployment of large language models (LLMs), one concern is\ntheir potential misuse for generating harmful content. Our work studies the\nalignment challenge, with a focus on filters to prevent the generation of\nunsafe information. Two natural points of intervention are the filtering of the\ninput prompt before it reaches the model, and filtering the output after\ngeneration. Our main results demonstrate computational challenges in filtering\nboth prompts and outputs. First, we show that there exist LLMs for which there\nare no efficient prompt filters: adversarial prompts that elicit harmful\nbehavior can be easily constructed, which are computationally indistinguishable\nfrom benign prompts for any efficient filter. Our second main result identifies\na natural setting in which output filtering is computationally intractable. All\nof our separation results are under cryptographic hardness assumptions. In\naddition to these core findings, we also formalize and study relaxed mitigation\napproaches, demonstrating further computational barriers. We conclude that\nsafety cannot be achieved by designing filters external to the LLM internals\n(architecture and weights); in particular, black-box access to the LLM will not\nsuffice. Based on our technical results, we argue that an aligned AI system's\nintelligence cannot be separated from its judgment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u8fc7\u6ee4\u95ee\u9898\uff0c\u53d1\u73b0\u8f93\u5165\u548c\u8f93\u51fa\u8fc7\u6ee4\u5747\u5b58\u5728\u8ba1\u7b97\u4e0a\u7684\u6311\u6218\uff0c\u5e76\u6307\u51fa\u5916\u90e8\u8fc7\u6ee4\u5668\u65e0\u6cd5\u5b8c\u5168\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9632\u6b62\u5176\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\uff0c\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u8fc7\u6ee4\u6280\u672f\u7684\u6709\u6548\u6027\u53ca\u5176\u8ba1\u7b97\u9650\u5236\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u8f93\u5165\u63d0\u793a\u548c\u8f93\u51fa\u5185\u5bb9\u7684\u8fc7\u6ee4\u6548\u7387\uff0c\u5e76\u57fa\u4e8e\u5bc6\u7801\u5b66\u5047\u8bbe\u8bc1\u660e\u5176\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\u3002", "result": "\u53d1\u73b0\u5bf9\u6297\u6027\u63d0\u793a\u53ef\u7ed5\u8fc7\u9ad8\u6548\u8fc7\u6ee4\u5668\uff0c\u4e14\u8f93\u51fa\u8fc7\u6ee4\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u8868\u660e\u5916\u90e8\u8fc7\u6ee4\u5668\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "\u5b89\u5168\u9700\u5185\u7f6e\u4e8eLLMs\u7684\u8bbe\u8ba1\u4e2d\uff0c\u65e0\u6cd5\u4ec5\u4f9d\u8d56\u5916\u90e8\u8fc7\u6ee4\u5668\uff0c\u667a\u80fd\u4e0e\u5224\u65ad\u9700\u7d27\u5bc6\u7ed3\u5408\u3002"}}
{"id": "2507.07230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07230", "abs": "https://arxiv.org/abs/2507.07230", "authors": ["Priyank Pathak", "Yogesh S. Rawat"], "title": "Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement", "comment": "ICCV'25 paper", "summary": "Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals\nacross different locations and times, irrespective of clothing. Existing\nmethods often rely on additional models or annotations to learn robust,\nclothing-invariant features, making them resource-intensive. In contrast, we\nexplore the use of color - specifically foreground and background colors - as a\nlightweight, annotation-free proxy for mitigating appearance bias in ReID\nmodels. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that\nleverages color information directly from raw images or video frames. CSCI\nefficiently captures color-related appearance bias ('Color See') while\ndisentangling it from identity-relevant ReID features ('Color Ignore'). To\nachieve this, we introduce S2A self-attention, a novel self-attention to\nprevent information leak between color and identity cues within the feature\nspace. Our analysis shows a strong correspondence between learned color\nembeddings and clothing attributes, validating color as an effective proxy when\nexplicit clothing labels are unavailable. We demonstrate the effectiveness of\nCSCI on both image and video ReID with extensive experiments on four CC-ReID\ndatasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for\nimage-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID\nwithout relying on additional supervision. Our results highlight the potential\nof color as a cost-effective solution for addressing appearance bias in\nCC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSCI\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5229\u7528\u989c\u8272\u4fe1\u606f\u89e3\u51b3\u8863\u7269\u66f4\u6362\u91cd\u8bc6\u522b\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6216\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6a21\u578b\u6216\u6807\u6ce8\u5b66\u4e60\u8863\u7269\u4e0d\u53d8\u7279\u5f81\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u3002\u672c\u6587\u63a2\u7d22\u989c\u8272\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ee3\u7406\uff0c\u51cf\u5c11\u5916\u89c2\u504f\u5dee\u3002", "method": "\u63d0\u51faCSCI\u65b9\u6cd5\uff0c\u5229\u7528RGB\u4fe1\u606f\uff0c\u901a\u8fc7S2A\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5206\u79bb\u989c\u8272\u4e0e\u8eab\u4efd\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2aCC-ReID\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u56fe\u50cf\u91cd\u8bc6\u522bTop-1\u63d0\u53472.9%-5.0%\uff0c\u89c6\u9891\u91cd\u8bc6\u522b\u63d0\u53471.0%-2.5%\u3002", "conclusion": "\u989c\u8272\u662f\u89e3\u51b3\u8863\u7269\u66f4\u6362\u91cd\u8bc6\u522b\u5916\u89c2\u504f\u5dee\u7684\u9ad8\u6548\u4f4e\u6210\u672c\u65b9\u6848\u3002"}}
{"id": "2507.07262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07262", "abs": "https://arxiv.org/abs/2507.07262", "authors": ["Shehreen Azad", "Yogesh S Rawat"], "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics", "comment": "Accepted in ICCV 2025", "summary": "In this work, we address activity-biometrics, which involves identifying\nindividuals across diverse set of activities. Unlike traditional person\nidentification, this setting introduces additional challenges as identity cues\nbecome entangled with motion dynamics and appearance variations, making\nbiometrics feature learning more complex. While additional visual data like\npose and/or silhouette help, they often struggle from extraction inaccuracies.\nTo overcome this, we propose a multimodal language-guided framework that\nreplaces reliance on additional visual data with structured textual\nsupervision. At its core, we introduce \\textbf{DisenQ} (\\textbf{Disen}tangling\n\\textbf{Q}-Former), a unified querying transformer that disentangles\nbiometrics, motion, and non-biometrics features by leveraging structured\nlanguage guidance. This ensures identity cues remain independent of appearance\nand motion variations, preventing misidentifications. We evaluate our approach\non three activity-based video benchmarks, achieving state-of-the-art\nperformance. Additionally, we demonstrate strong generalization to complex\nreal-world scenario with competitive performance on a traditional video-based\nidentification benchmark, showing the effectiveness of our framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8bed\u8a00\u5f15\u5bfc\u6846\u67b6DisenQ\uff0c\u7528\u4e8e\u89e3\u51b3\u6d3b\u52a8\u751f\u7269\u8bc6\u522b\u4e2d\u8eab\u4efd\u7279\u5f81\u4e0e\u8fd0\u52a8\u52a8\u6001\u548c\u5916\u89c2\u53d8\u5316\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8eab\u4efd\u8bc6\u522b\u5728\u591a\u6837\u5316\u6d3b\u52a8\u4e2d\u9762\u4e34\u8eab\u4efd\u7279\u5f81\u4e0e\u8fd0\u52a8\u52a8\u6001\u548c\u5916\u89c2\u53d8\u5316\u7ea0\u7f20\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u89c6\u89c9\u6570\u636e\u4f46\u5b58\u5728\u63d0\u53d6\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDisenQ\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6587\u672c\u6307\u5bfc\u5206\u79bb\u751f\u7269\u7279\u5f81\u3001\u8fd0\u52a8\u548c\u975e\u751f\u7269\u7279\u5f81\uff0c\u907f\u514d\u8eab\u4efd\u7279\u5f81\u53d7\u5916\u89c2\u548c\u8fd0\u52a8\u53d8\u5316\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u6d3b\u52a8\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u4f20\u7edf\u89c6\u9891\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DisenQ\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u6307\u5bfc\u6709\u6548\u89e3\u51b3\u4e86\u6d3b\u52a8\u751f\u7269\u8bc6\u522b\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.07599", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07599", "abs": "https://arxiv.org/abs/2507.07599", "authors": ["Sedigh Khademi", "Jim Black", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila"], "title": "Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models", "comment": "5 pages", "summary": "This study evaluates fine-tuned Llama 3.2 models for extracting\nvaccine-related information from emergency department triage notes to support\nnear real-time vaccine safety surveillance. Prompt engineering was used to\ninitially create a labeled dataset, which was then confirmed by human\nannotators. The performance of prompt-engineered models, fine-tuned models, and\na rule-based approach was compared. The fine-tuned Llama 3 billion parameter\nmodel outperformed other models in its accuracy of extracting vaccine names.\nModel quantization enabled efficient deployment in resource-constrained\nenvironments. Findings demonstrate the potential of large language models in\nautomating data extraction from emergency department notes, supporting\nefficient vaccine safety surveillance and early detection of emerging adverse\nevents following immunization issues.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5fae\u8c03\u7684Llama 3.2\u6a21\u578b\u4ece\u6025\u8bca\u5206\u8bca\u7b14\u8bb0\u4e2d\u63d0\u53d6\u75ab\u82d7\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u8fd1\u5b9e\u65f6\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u3002\u5fae\u8c03\u6a21\u578b\u5728\u63d0\u53d6\u75ab\u82d7\u540d\u79f0\u7684\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u652f\u6301\u9ad8\u6548\u7684\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u548c\u65e9\u671f\u53d1\u73b0\u514d\u75ab\u540e\u4e0d\u826f\u4e8b\u4ef6\u3002", "method": "\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u521b\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u7531\u4eba\u5de5\u786e\u8ba4\u3002\u6bd4\u8f83\u4e86\u63d0\u793a\u5de5\u7a0b\u6a21\u578b\u3001\u5fae\u8c03\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002", "result": "\u5fae\u8c03\u7684Llama 3\u4ebf\u53c2\u6570\u6a21\u578b\u5728\u63d0\u53d6\u75ab\u82d7\u540d\u79f0\u7684\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u6a21\u578b\u91cf\u5316\u4f7f\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\u90e8\u7f72\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u6025\u8bca\u7b14\u8bb0\u6570\u636e\u63d0\u53d6\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63d0\u5347\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u6548\u7387\u3002"}}
{"id": "2507.07743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07743", "abs": "https://arxiv.org/abs/2507.07743", "authors": ["Phil\u00e9mon Beghin", "Anne-Emmanuelle Ceulemans", "Fran\u00e7ois Glineur"], "title": "Identification of Violin Reduction via Contour Lines Classification", "comment": null, "summary": "The first violins appeared in late 16th-century Italy. Over the next 200\nyears, they spread across Europe and luthiers of various royal courts, eager to\nexperiment with new techniques, created a highly diverse family of instruments.\nAround 1750, size standards were introduced to unify violin making for\norchestras and conservatories. Instruments that fell between two standards were\nthen reduced to a smaller size by luthiers. These reductions have an impact on\nseveral characteristics of violins, in particular on the contour lines, i.e.\nlines of constant altitude, which look more like a U for non reduced\ninstruments and a V for reduced ones. While such differences are observed by\nexperts, they have not been studied quantitatively.\n  This paper presents a method for classifying violins as reduced or\nnon-reduced based on their contour lines. We study a corpus of 25 instruments\nwhose 3D geometric meshes were acquired via photogrammetry. For each\ninstrument, we extract 10-20 contour lines regularly spaced every millimetre.\nEach line is fitted with a parabola-like curve (with an equation of the type y\n= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)\nand how vertically stretched (alpha) the curve is. We compute additional\nfeatures from those parameters, using regressions and counting how many values\nfall under some threshold. We also deal with outliers and non equal numbers of\nlevels, and eventually obtain a numerical profile for each instrument.\n  We then apply classification methods to assess whether geometry alone can\npredict size reduction. We find that distinguishing between reduced and non\nreduced instruments is feasible to some degree, taking into account that a\nwhole spectrum of more or less transformed violins exists, for which it is more\ndifficult to quantify the reduction. We also find the opening parameter beta to\nbe the most predictive.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6e\u5ed3\u7ebf\u5206\u7c7b\u5c0f\u63d0\u7434\u662f\u5426\u4e3a\u7f29\u5c0f\u5c3a\u5bf8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u5206\u6790\uff0c\u53d1\u73b0\u533a\u5206\u7f29\u5c0f\u4e0e\u975e\u7f29\u5c0f\u5c0f\u63d0\u7434\u662f\u53ef\u884c\u7684\u3002", "motivation": "\u7814\u7a76\u5c0f\u63d0\u7434\u5236\u4f5c\u4e2d\u5c3a\u5bf8\u7f29\u5c0f\u5bf9\u8f6e\u5ed3\u7ebf\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e13\u5bb6\u89c2\u5bdf\u672a\u5b9a\u91cf\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u6444\u5f71\u6d4b\u91cf\u83b7\u53d625\u628a\u5c0f\u63d0\u7434\u76843D\u51e0\u4f55\u7f51\u683c\uff0c\u63d0\u53d6\u5e76\u62df\u5408\u8f6e\u5ed3\u7ebf\u53c2\u6570\uff0c\u901a\u8fc7\u56de\u5f52\u548c\u5206\u7c7b\u65b9\u6cd5\u5206\u6790\u3002", "result": "\u53d1\u73b0\u51e0\u4f55\u7279\u5f81\u53ef\u4ee5\u4e00\u5b9a\u7a0b\u5ea6\u533a\u5206\u7f29\u5c0f\u4e0e\u975e\u7f29\u5c0f\u5c0f\u63d0\u7434\uff0c\u5176\u4e2d\u5f00\u53e3\u53c2\u6570beta\u6700\u5177\u9884\u6d4b\u6027\u3002", "conclusion": "\u51e0\u4f55\u5206\u6790\u53ef\u7528\u4e8e\u5c0f\u63d0\u7434\u5206\u7c7b\uff0c\u4f46\u9700\u8003\u8651\u5c3a\u5bf8\u7f29\u5c0f\u7a0b\u5ea6\u7684\u8fde\u7eed\u6027\u3002"}}
{"id": "2507.07381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07381", "abs": "https://arxiv.org/abs/2507.07381", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos", "comment": null, "summary": "Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as Gate Shift\nModule (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with\ntemporal context. However, these modules are limited in both temporal receptive\nfield and spatial adaptability. We propose a Multi-Scale Attention Gate Shift\nModule (MSAGSM) that enhances GSM with multi-scale temporal dilations and\nmulti-head spatial attention, enabling efficient modeling of both short- and\nlong-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight plug-and-play module that can be easily integrated with various 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia (TTA) dataset-the first PES benchmark for table tennis-containing\nover 4800 precisely annotated events. Extensive experiments across five PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u95e8\u79fb\u4f4d\u6a21\u5757\uff08MSAGSM\uff09\uff0c\u7528\u4e8e\u589e\u5f3a\u4f53\u80b2\u89c6\u9891\u4e2d\u7cbe\u786e\u4e8b\u4ef6\u70b9\u68c0\u6d4b\uff08PES\uff09\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6TTA\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709PES\u6a21\u578b\u7684\u65f6\u7a7a\u6a21\u5757\u5728\u65f6\u95f4\u611f\u53d7\u91ce\u548c\u7a7a\u95f4\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faMSAGSM\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u65f6\u95f4\u6269\u5f20\u548c\u591a\u5934\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u63d0\u5347\u65f6\u7a7a\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2aPES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "conclusion": "MSAGSM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u80fd\u6709\u6548\u63d0\u5347PES\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6TTA\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.07787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07787", "abs": "https://arxiv.org/abs/2507.07787", "authors": ["Elizabeth Hilliard", "Akshaya Jagadeesh", "Alex Cook", "Steele Billings", "Nicholas Skytland", "Alicia Llewellyn", "Jackson Paull", "Nathan Paull", "Nolan Kurylo", "Keatra Nesbitt", "Robert Gruenewald", "Anthony Jantzi", "Omar Chavez"], "title": "Measuring AI Alignment with Human Flourishing", "comment": null, "summary": "This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel\nevaluation framework that assesses AI alignment with human flourishing across\nseven dimensions: Character and Virtue, Close Social Relationships, Happiness\nand Life Satisfaction, Meaning and Purpose, Mental and Physical Health,\nFinancial and Material Stability, and Faith and Spirituality. Unlike\ntraditional benchmarks that focus on technical capabilities or harm prevention,\nthe FAI Benchmark measures AI performance on how effectively models contribute\nto the flourishing of a person across these dimensions. The benchmark evaluates\nhow effectively LLM AI systems align with current research models of holistic\nhuman well-being through a comprehensive methodology that incorporates 1,229\nobjective and subjective questions. Using specialized judge Large Language\nModels (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs\ngeometric mean scoring to ensure balanced performance across all flourishing\ndimensions. Initial testing of 28 leading language models reveals that while\nsome models approach holistic alignment (with the highest-scoring models\nachieving 72/100), none are acceptably aligned across all dimensions,\nparticularly in Faith and Spirituality, Character and Virtue, and Meaning and\nPurpose. This research establishes a framework for developing AI systems that\nactively support human flourishing rather than merely avoiding harm, offering\nsignificant implications for AI development, ethics, and evaluation.", "AI": {"tldr": "FAI Benchmark\u8bc4\u4f30AI\u5728\u4e03\u4e2a\u7ef4\u5ea6\u4e0a\u5bf9\u4eba\u7c7b\u7e41\u8363\u7684\u8d21\u732e\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u4fe1\u4ef0\u4e0e\u7075\u6027\u3001\u54c1\u683c\u4e0e\u7f8e\u5fb7\u3001\u610f\u4e49\u4e0e\u76ee\u7684\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u5173\u6ce8\u6280\u672f\u80fd\u529b\u6216\u907f\u514d\u5371\u5bb3\uff0c\u800cFAI Benchmark\u65e8\u5728\u8861\u91cfAI\u5bf9\u4eba\u7c7b\u5168\u9762\u7e41\u8363\u7684\u8d21\u732e\u3002", "method": "\u901a\u8fc71,229\u4e2a\u4e3b\u5ba2\u89c2\u95ee\u9898\uff0c\u7ed3\u5408\u4e13\u5bb6LLM\u548c\u51e0\u4f55\u5e73\u5747\u8bc4\u5206\uff0c\u8bc4\u4f3028\u4e2a\u9886\u5148\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6700\u9ad8\u5206\u6a21\u578b\u5f9772/100\uff0c\u4f46\u65e0\u6a21\u578b\u5728\u6240\u6709\u7ef4\u5ea6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u5728\u4fe1\u4ef0\u4e0e\u7075\u6027\u7b49\u9886\u57df\u3002", "conclusion": "FAI Benchmark\u4e3a\u5f00\u53d1\u652f\u6301\u4eba\u7c7b\u7e41\u8363\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5bf9AI\u4f26\u7406\u548c\u8bc4\u4f30\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.07323", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07323", "abs": "https://arxiv.org/abs/2507.07323", "authors": ["Dongyu Wei", "Xiaoren Xu", "Yuchen Liu", "H. Vincent Poor", "Mingzhe Chen"], "title": "Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning", "comment": null, "summary": "In this paper, deceptive signal-assisted private split learning is\ninvestigated. In our model, several edge devices jointly perform collaborative\ntraining, and some eavesdroppers aim to collect the model and data information\nfrom devices. To prevent the eavesdroppers from collecting model and data\ninformation, a subset of devices can transmit deceptive signals. Therefore, it\nis necessary to determine the subset of devices used for deceptive signal\ntransmission, the subset of model training devices, and the models assigned to\neach model training device. This problem is formulated as an optimization\nproblem whose goal is to minimize the information leaked to eavesdroppers while\nmeeting the model training energy consumption and delay constraints. To solve\nthis problem, we propose a soft actor-critic deep reinforcement learning\nframework with intrinsic curiosity module and cross-attention (ICM-CA) that\nenables a centralized agent to determine the model training devices, the\ndeceptive signal transmission devices, the transmit power, and sub-models\nassigned to each model training device without knowing the position and\nmonitoring probability of eavesdroppers. The proposed method uses an ICM module\nto encourage the server to explore novel actions and states and a CA module to\ndetermine the importance of each historical state-action pair thus improving\ntraining efficiency. Simulation results demonstrate that the proposed method\nimproves the convergence rate by up to 3x and reduces the information leaked to\neavesdroppers by up to 13% compared to the traditional SAC algorithm.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6b3a\u9a97\u4fe1\u53f7\u8f85\u52a9\u7684\u79c1\u6709\u5206\u5272\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\u4ee5\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u6ee1\u8db3\u80fd\u8017\u548c\u5ef6\u8fdf\u7ea6\u675f\u3002", "motivation": "\u9632\u6b62\u7a83\u542c\u8005\u4ece\u8fb9\u7f18\u8bbe\u5907\u6536\u96c6\u6a21\u578b\u548c\u6570\u636e\u4fe1\u606f\uff0c\u786e\u4fdd\u534f\u4f5c\u8bad\u7ec3\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff08ICM-CA\uff09\uff0c\u7ed3\u5408\u5185\u5728\u597d\u5947\u5fc3\u6a21\u5757\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u52a8\u6001\u9009\u62e9\u8bbe\u5907\u5206\u914d\u548c\u529f\u7387\u63a7\u5236\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfSAC\u7b97\u6cd5\u6536\u655b\u901f\u5ea6\u5feb3\u500d\uff0c\u4fe1\u606f\u6cc4\u9732\u51cf\u5c1113%\u3002", "conclusion": "ICM-CA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2507.07931", "categories": ["cs.AI", "cs.CY", "I.2.0; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.07931", "abs": "https://arxiv.org/abs/2507.07931", "authors": ["Hans Gundlach", "Jayson Lynch", "Neil Thompson"], "title": "Meek Models Shall Inherit the Earth", "comment": "13 pages, 9 figures, longer version of the paper presented at TAIG\n  ICML 2025", "summary": "The past decade has seen incredible scaling of AI systems by a few companies,\nleading to inequality in AI model performance. This paper argues that, contrary\nto prevailing intuition, the diminishing returns to compute scaling will lead\nto a convergence of AI model capabilities. In other words, meek models (those\nwith limited computation budget) shall inherit the earth, approaching the\nperformance level of the best models overall. We develop a model illustrating\nthat under a fixed-distribution next-token objective, the marginal capability\nreturns to raw compute shrink substantially. Given current scaling practices,\nwe argue that these diminishing returns are strong enough that even companies\nthat can scale their models exponentially faster than other organizations will\neventually have little advantage in capabilities. As part of our argument, we\ngive several reasons that proxies like training loss differences capture\nimportant capability measures using evidence from benchmark data and\ntheoretical performance models. In addition, we analyze empirical data on the\ncapability difference of AI models over time. Finally, in light of the\nincreasing ability of meek models, we argue that AI strategy and policy require\nreexamination, and we outline the areas this shift will affect.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\uff0c\u968f\u7740\u8ba1\u7b97\u8d44\u6e90\u6295\u5165\u7684\u8fb9\u9645\u6548\u76ca\u9012\u51cf\uff0cAI\u6a21\u578b\u6027\u80fd\u5c06\u8d8b\u4e8e\u6536\u655b\uff0c\u5373\u4f7f\u8d44\u6e90\u6709\u9650\u7684\u6a21\u578b\u4e5f\u80fd\u63a5\u8fd1\u6700\u4f73\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8AI\u6a21\u578b\u6027\u80fd\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u5e76\u6311\u6218\u5f53\u524d\u5173\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6295\u5165\u8d8a\u591a\u6027\u80fd\u8d8a\u597d\u7684\u76f4\u89c9\u3002", "method": "\u901a\u8fc7\u56fa\u5b9a\u5206\u5e03\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u76ee\u6807\u6a21\u578b\uff0c\u5206\u6790\u8ba1\u7b97\u8d44\u6e90\u7684\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u73b0\u8c61\uff0c\u5e76\u7ed3\u5408\u57fa\u51c6\u6570\u636e\u548c\u7406\u8bba\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u8ba1\u7b97\u8d44\u6e90\u7684\u8fb9\u9645\u6548\u76ca\u663e\u8457\u4e0b\u964d\uff0c\u8d44\u6e90\u6709\u9650\u7684\u6a21\u578b\u6027\u80fd\u5c06\u63a5\u8fd1\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "AI\u6218\u7565\u548c\u653f\u7b56\u9700\u91cd\u65b0\u5ba1\u89c6\uff0c\u4ee5\u5e94\u5bf9\u6a21\u578b\u6027\u80fd\u6536\u655b\u7684\u8d8b\u52bf\u3002"}}
{"id": "2506.21142", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21142", "abs": "https://arxiv.org/abs/2506.21142", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks", "comment": null, "summary": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ecGAN\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u80fd\u591f\u9003\u907f\u65e0\u4eba\u673a\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u7684\u9690\u853d\u5bf9\u6297\u653b\u51fb\uff0c\u5e76\u901a\u8fc7CVAE\u68c0\u6d4b\u8fd9\u4e9b\u653b\u51fb\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u65b0\u578b\u5a01\u80c1\uff0c\u4e14\u5e38\u89c4OOD\u68c0\u6d4b\u5668\u65e0\u6cd5\u533a\u5206\u9690\u853d\u5bf9\u6297\u653b\u51fb\u4e0e\u771f\u5b9eOOD\u4e8b\u4ef6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u7c7bIDS\u5206\u7c7b\u5668\uff0c\u5229\u7528cGAN\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u901a\u8fc7CVAE\u68c0\u6d4b\u8fd9\u4e9b\u6837\u672c\u3002", "result": "CVAE\u5728\u68c0\u6d4b\u9690\u853d\u5bf9\u6297\u653b\u51fb\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u8c03\u9ad8\u7ea7\u6982\u7387\u5efa\u6a21\u5bf9\u63d0\u5347IDS\u5bf9\u6297\u751f\u6210\u6a21\u578b\u653b\u51fb\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.07483", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07483", "abs": "https://arxiv.org/abs/2507.07483", "authors": ["Qiangqiang Wu", "Yi Yu", "Chenqi Kong", "Ziquan Liu", "Jia Wan", "Haoliang Li", "Alex C. Kot", "Antoni B. Chan"], "title": "Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking", "comment": "Accepted by ICCV 2025", "summary": "With the rise of social media, vast amounts of user-uploaded videos (e.g.,\nYouTube) are utilized as training data for Visual Object Tracking (VOT).\nHowever, the VOT community has largely overlooked video data-privacy issues, as\nmany private videos have been collected and used for training commercial models\nwithout authorization. To alleviate these issues, this paper presents the first\ninvestigation on preventing personal video data from unauthorized exploitation\nby deep trackers. Existing methods for preventing unauthorized data use\nprimarily focus on image-based tasks (e.g., image classification), directly\napplying them to videos reveals several limitations, including inefficiency,\nlimited effectiveness, and poor generalizability. To address these issues, we\npropose a novel generative framework for generating Temporal Unlearnable\nExamples (TUEs), and whose efficient computation makes it scalable for usage on\nlarge-scale video datasets. The trackers trained w/ TUEs heavily rely on\nunlearnable noises for temporal matching, ignoring the original data structure\nand thus ensuring training video data-privacy. To enhance the effectiveness of\nTUEs, we introduce a temporal contrastive loss, which further corrupts the\nlearning of existing trackers when using our TUEs for training. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nin video data-privacy protection, with strong transferability across VOT\nmodels, datasets, and temporal matching tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u65f6\u95f4\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\uff08TUEs\uff09\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u4fdd\u62a4\u89c6\u9891\u6570\u636e\u9690\u79c1\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u89c6\u89c9\u5bf9\u8c61\u8ddf\u8e2a\uff08VOT\uff09\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u5a92\u4f53\u7684\u5174\u8d77\uff0c\u5927\u91cf\u7528\u6237\u4e0a\u4f20\u7684\u89c6\u9891\u88ab\u7528\u4e8eVOT\u8bad\u7ec3\uff0c\u4f46\u6570\u636e\u9690\u79c1\u95ee\u9898\u88ab\u5ffd\u89c6\uff0c\u8bb8\u591a\u79c1\u4eba\u89c6\u9891\u672a\u7ecf\u6388\u6743\u88ab\u7528\u4e8e\u5546\u4e1a\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210TUEs\u7684\u751f\u6210\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u5bf9\u6bd4\u635f\u5931\u4ee5\u589e\u5f3a\u6548\u679c\uff0c\u786e\u4fdd\u8bad\u7ec3\u89c6\u9891\u6570\u636e\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8de8\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u65f6\u95f4\u5339\u914d\u4efb\u52a1\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89c6\u9891\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.07390", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07390", "abs": "https://arxiv.org/abs/2507.07390", "authors": ["Seonghyun Park", "Kiyoung Seong", "Soojung Yang", "Rafael G\u00f3mez-Bombarelli", "Sungsoo Ahn"], "title": "Learning Collective Variables from Time-lagged Generation", "comment": null, "summary": "Rare events such as state transitions are difficult to observe directly with\nmolecular dynamics simulations due to long timescales. Enhanced sampling\ntechniques overcome this by introducing biases along carefully chosen\nlow-dimensional features, known as collective variables (CVs), which capture\nthe slow degrees of freedom. Machine learning approaches (MLCVs) have automated\nCV discovery, but existing methods typically focus on discriminating\nmeta-stable states without fully encoding the detailed dynamics essential for\naccurate sampling. We propose TLC, a framework that learns CVs directly from\ntime-lagged conditions of a generative model. Instead of modeling the static\nBoltzmann distribution, TLC models a time-lagged conditional distribution\nyielding CVs to capture the slow dynamic behavior. We validate TLC on the\nAlanine Dipeptide system using two CV-based enhanced sampling tasks: (i)\nsteered molecular dynamics (SMD) and (ii) on-the-fly probability enhanced\nsampling (OPES), demonstrating equal or superior performance compared to\nexisting MLCV methods in both transition path sampling and state\ndiscrimination.", "AI": {"tldr": "TLC\u6846\u67b6\u901a\u8fc7\u4ece\u751f\u6210\u6a21\u578b\u7684\u65f6\u95f4\u6ede\u540e\u6761\u4ef6\u4e2d\u5b66\u4e60\u96c6\u4f53\u53d8\u91cf\uff08CVs\uff09\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6355\u6349\u6162\u52a8\u6001\u884c\u4e3a\uff0c\u5728Alanine Dipeptide\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u96be\u4ee5\u76f4\u63a5\u89c2\u6d4b\u7f55\u89c1\u4e8b\u4ef6\uff08\u5982\u72b6\u6001\u8f6c\u53d8\uff09\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4ec5\u5173\u6ce8\u533a\u5206\u7a33\u6001\u800c\u5ffd\u7565\u52a8\u6001\u7ec6\u8282\uff0cTLC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TLC\u901a\u8fc7\u5efa\u6a21\u65f6\u95f4\u6ede\u540e\u6761\u4ef6\u5206\u5e03\u800c\u975e\u9759\u6001Boltzmann\u5206\u5e03\uff0c\u5b66\u4e60CVs\u4ee5\u6355\u6349\u6162\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u5728Alanine Dipeptide\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5728SMD\u548cOPES\u4efb\u52a1\u4e2d\uff0cTLC\u5728\u8def\u5f84\u91c7\u6837\u548c\u72b6\u6001\u533a\u5206\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u73b0\u6709MLCV\u65b9\u6cd5\u3002", "conclusion": "TLC\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u52a8\u6001\u884c\u4e3a\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684CV\u53d1\u73b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u589e\u5f3a\u91c7\u6837\u4efb\u52a1\u3002"}}
{"id": "2507.07559", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07559", "abs": "https://arxiv.org/abs/2507.07559", "authors": ["Amirhossein Sadough", "Mahyar Shahsavari", "Mark Wijtvliet", "Marcel van Gerven"], "title": "Real-Time Decorrelation-Based Anomaly Detection for Multivariate Time Series", "comment": null, "summary": "Anomaly detection (AD) plays a vital role across a wide range of real-world\ndomains by identifying data instances that deviate from expected patterns,\npotentially signaling critical events such as system failures, fraudulent\nactivities, or rare medical conditions. The demand for real-time AD has surged\nwith the rise of the (Industrial) Internet of Things, where massive volumes of\nmultivariate sensor data must be processed instantaneously. Real-time AD\nrequires methods that not only handle high-dimensional streaming data but also\noperate in a single-pass manner, without the burden of storing historical\ninstances, thereby ensuring minimal memory usage and fast decision-making. We\npropose DAD, a novel real-time decorrelation-based anomaly detection method for\nmultivariate time series, based on an online decorrelation learning approach.\nUnlike traditional proximity-based or reconstruction-based detectors that\nprocess entire data or windowed instances, DAD dynamically learns and monitors\nthe correlation structure of data sample by sample in a single pass, enabling\nefficient and effective detection. To support more realistic benchmarking\npractices, we also introduce a practical hyperparameter tuning strategy\ntailored for real-time anomaly detection scenarios. Extensive experiments on\nwidely used benchmark datasets demonstrate that DAD achieves the most\nconsistent and superior performance across diverse anomaly types compared to\nstate-of-the-art methods. Crucially, its robustness to increasing\ndimensionality makes it particularly well-suited for real-time,\nhigh-dimensional data streams. Ultimately, DAD not only strikes an optimal\nbalance between detection efficacy and computational efficiency but also sets a\nnew standard for real-time, memory-constrained anomaly detection.", "AI": {"tldr": "DAD\u662f\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u53bb\u76f8\u5173\u5b66\u4e60\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u5185\u5b58\u53cb\u597d\u6027\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u7269\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u9700\u6c42\u6fc0\u589e\uff0c\u9700\u8981\u5904\u7406\u9ad8\u7ef4\u6d41\u6570\u636e\u4e14\u65e0\u9700\u5b58\u50a8\u5386\u53f2\u5b9e\u4f8b\u7684\u65b9\u6cd5\u3002", "method": "DAD\u901a\u8fc7\u5355\u6b21\u904d\u5386\u52a8\u6001\u5b66\u4e60\u6570\u636e\u7684\u76f8\u5173\u7ed3\u6784\uff0c\u4e0e\u4f20\u7edf\u57fa\u4e8e\u90bb\u8fd1\u6216\u91cd\u6784\u7684\u65b9\u6cd5\u4e0d\u540c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAD\u5728\u591a\u79cd\u5f02\u5e38\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u5408\u9ad8\u7ef4\u5b9e\u65f6\u6570\u636e\u6d41\u3002", "conclusion": "DAD\u5728\u68c0\u6d4b\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u5b9e\u65f6\u5185\u5b58\u53d7\u9650\u7684\u5f02\u5e38\u68c0\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.07580", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.NA", "65F55, 68T50"], "pdf": "https://arxiv.org/pdf/2507.07580", "abs": "https://arxiv.org/abs/2507.07580", "authors": ["Uliana Parkina", "Maxim Rakhuba"], "title": "COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation", "comment": null, "summary": "Recent studies suggest that context-aware low-rank approximation is a useful\ntool for compression and fine-tuning of modern large-scale neural networks. In\nthis type of approximation, a norm is weighted by a matrix of input\nactivations, significantly improving metrics over the unweighted case.\nNevertheless, existing methods for neural networks suffer from numerical\ninstabilities due to their reliance on classical formulas involving explicit\nGram matrix computation and their subsequent inversion. We demonstrate that\nthis can degrade the approximation quality or cause numerically singular\nmatrices.\n  To address these limitations, we propose a novel inversion-free regularized\nframework that is based entirely on stable decompositions and overcomes the\nnumerical pitfalls of prior art. Our method can handle possible challenging\nscenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when\ninput activation matrices are nearly singular, and even (3) when insufficient\ndata prevents unique approximation. For the latter, we prove that our solution\nconverges to a desired approximation and derive explicit error bounds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u5206\u89e3\u7684\u65e0\u9006\u6b63\u5219\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u663e\u5f0fGram\u77e9\u9635\u8ba1\u7b97\u548c\u9006\u8fd0\u7b97\u5bfc\u81f4\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u56e0\u4f9d\u8d56\u663e\u5f0fGram\u77e9\u9635\u8ba1\u7b97\u548c\u9006\u8fd0\u7b97\uff0c\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u8fd1\u4f3c\u8d28\u91cf\u6216\u4ea7\u751f\u6570\u503c\u5947\u5f02\u77e9\u9635\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u5206\u89e3\u7684\u65e0\u9006\u6b63\u5219\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u6821\u51c6\u77e9\u9635\u8d85\u51faGPU\u5185\u5b58\u5bb9\u91cf\u3001\u8f93\u5165\u6fc0\u6d3b\u77e9\u9635\u63a5\u8fd1\u5947\u5f02\u6216\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u503c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u6536\u655b\u5230\u671f\u671b\u8fd1\u4f3c\uff0c\u4e14\u63a8\u5bfc\u4e86\u663e\u5f0f\u8bef\u5dee\u754c\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6311\u6218\u6027\u573a\u666f\u3002"}}
{"id": "2507.07735", "categories": ["cs.LG", "cs.CL", "cs.CR", "I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.07735", "abs": "https://arxiv.org/abs/2507.07735", "authors": ["Peiyan Zhang", "Haibo Jin", "Liying Kang", "Haohan Wang"], "title": "GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing", "comment": "24 pages", "summary": "Jailbreak attacks reveal critical vulnerabilities in Large Language Models\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\nthese threats is particularly challenging due to the evolving nature of LLMs\nand the sophistication required in effectively probing their vulnerabilities.\nCurrent benchmarks and evaluation methods struggle to fully address these\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\npaper, we review existing jailbreak evaluation practices and identify three\nassumed desiderata for an effective jailbreak evaluation protocol. To address\nthese challenges, we introduce GuardVal, a new evaluation protocol that\ndynamically generates and refines jailbreak prompts based on the defender LLM's\nstate, providing a more accurate assessment of defender LLMs' capacity to\nhandle safety-critical situations. Moreover, we propose a new optimization\nmethod that prevents stagnation during prompt refinement, ensuring the\ngeneration of increasingly effective jailbreak prompts that expose deeper\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\nhighlight distinct behavioral patterns among the models, offering a\ncomprehensive view of their robustness. Furthermore, our evaluation process\ndeepens the understanding of LLM behavior, leading to insights that can inform\nfuture research and drive the development of more secure models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGuardVal\u534f\u8bae\uff0c\u52a8\u6001\u751f\u6210\u548c\u4f18\u5316\u8d8a\u72f1\u63d0\u793a\uff0c\u4ee5\u66f4\u51c6\u786e\u8bc4\u4f30LLM\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u6355\u6349LLM\u7684\u6f0f\u6d1e\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "method": "\u5f15\u5165GuardVal\u534f\u8bae\uff0c\u52a8\u6001\u751f\u6210\u548c\u4f18\u5316\u8d8a\u72f1\u63d0\u793a\uff0c\u7ed3\u5408\u65b0\u4f18\u5316\u65b9\u6cd5\u9632\u6b62\u505c\u6ede\u3002", "result": "\u5e94\u7528\u4e8e\u591a\u79cd\u6a21\u578b\uff08\u5982Mistral-7b\u3001GPT-4\uff09\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u6f0f\u6d1e\u3002", "conclusion": "GuardVal\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684LLM\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u53ca\u5b89\u5168\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.07804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07804", "abs": "https://arxiv.org/abs/2507.07804", "authors": ["Alba Garrido", "Alejandro Almod\u00f3var", "Patricia A. Apell\u00e1niz", "Juan Parras", "Santiago Zazo"], "title": "Deep Survival Analysis in Multimodal Medical Data: A Parametric and Probabilistic Approach with Competing Risks", "comment": "29 pages, 9 Figures", "summary": "Accurate survival prediction is critical in oncology for prognosis and\ntreatment planning. Traditional approaches often rely on a single data\nmodality, limiting their ability to capture the complexity of tumor biology. To\naddress this challenge, we introduce a multimodal deep learning framework for\nsurvival analysis capable of modeling both single and competing risks\nscenarios, evaluating the impact of integrating multiple medical data sources\non survival predictions. We propose SAMVAE (Survival Analysis Multimodal\nVariational Autoencoder), a novel deep learning architecture designed for\nsurvival prediction that integrates six data modalities: clinical variables,\nfour molecular profiles, and histopathological images. SAMVAE leverages\nmodality specific encoders to project inputs into a shared latent space,\nenabling robust survival prediction while preserving modality specific\ninformation. Its parametric formulation enables the derivation of clinically\nmeaningful statistics from the output distributions, providing patient-specific\ninsights through interactive multimedia that contribute to more informed\nclinical decision-making and establish a foundation for interpretable,\ndata-driven survival analysis in oncology. We evaluate SAMVAE on two cancer\ncohorts breast cancer and lower grade glioma applying tailored preprocessing,\ndimensionality reduction, and hyperparameter optimization. The results\ndemonstrate the successful integration of multimodal data for both standard\nsurvival analysis and competing risks scenarios across different datasets. Our\nmodel achieves competitive performance compared to state-of-the-art multimodal\nsurvival models. Notably, this is the first parametric multimodal deep learning\narchitecture to incorporate competing risks while modeling continuous time to a\nspecific event, using both tabular and image data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMVAE\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u5b58\u5206\u6790\uff0c\u6574\u5408\u516d\u79cd\u6570\u636e\u6a21\u6001\uff0c\u5e76\u5728\u4e73\u817a\u764c\u548c\u4f4e\u7ea7\u522b\u80f6\u8d28\u7624\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u751f\u5b58\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u6a21\u6001\uff0c\u96be\u4ee5\u6355\u6349\u80bf\u7624\u751f\u7269\u5b66\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSAMVAE\u67b6\u6784\uff0c\u5229\u7528\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u5c06\u8f93\u5165\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u652f\u6301\u751f\u5b58\u9884\u6d4b\u548c\u7ade\u4e89\u98ce\u9669\u573a\u666f\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u4f4e\u7ea7\u522b\u80f6\u8d28\u7624\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u591a\u6a21\u6001\u7ade\u4e89\u98ce\u9669\u5efa\u6a21\u3002", "conclusion": "SAMVAE\u4e3a\u80bf\u7624\u5b66\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6570\u636e\u9a71\u52a8\u751f\u5b58\u5206\u6790\u6846\u67b6\uff0c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2507.07734", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.07734", "abs": "https://arxiv.org/abs/2507.07734", "authors": ["Michael Neumeier", "Jules Lecomte", "Nils Kazinski", "Soubarna Banik", "Bing Li", "Axel von Arnim"], "title": "EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream Spiking Neural Networks", "comment": "International Conference on Neuromorphic Systems (ICONS) 2025", "summary": "Recognizing human activities early is crucial for the safety and\nresponsiveness of human-robot and human-machine interfaces. Due to their high\ntemporal resolution and low latency, event-based vision sensors are a perfect\nmatch for this early recognition demand. However, most existing processing\napproaches accumulate events to low-rate frames or space-time voxels which\nlimits the early prediction capabilities. In contrast, spiking neural networks\n(SNNs) can process the events at a high-rate for early predictions, but most\nworks still fall short on final accuracy. In this work, we introduce a\nhigh-rate two-stream SNN which closes this gap by outperforming previous work\nby 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark\nthe SNNs within a novel early event-based recognition framework by reporting\nTop-1 and Top-5 recognition scores for growing observation time. Finally, we\nexemplify the impact of these methods on a real-world task of early action\ntriggering for human motion capture in sports.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u9891\u7387\u7684\u53cc\u6d41\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\uff0c\u7528\u4e8e\u65e9\u671f\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u5728THU EACT-50\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u53472%\u3002", "motivation": "\u65e9\u671f\u8bc6\u522b\u4eba\u7c7b\u6d3b\u52a8\u5bf9\u5b89\u5168\u6027\u548c\u54cd\u5e94\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4e8b\u4ef6\u89c6\u89c9\u4f20\u611f\u5668\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4f4e\u5ef6\u8fdf\u9002\u5408\u8fd9\u4e00\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9650\u5236\u4e86\u65e9\u671f\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9ad8\u9891\u7387\u53cc\u6d41SNN\u5904\u7406\u4e8b\u4ef6\u6570\u636e\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u4f4e\u901f\u7387\u9650\u5236\u3002", "result": "\u5728THU EACT-50\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u53472%\uff0c\u5e76\u5728\u65e9\u671f\u8bc6\u522b\u6846\u67b6\u4e2d\u8bc4\u4f30\u4e86Top-1\u548cTop-5\u5f97\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e9\u671f\u52a8\u4f5c\u89e6\u53d1\uff08\u5982\u8fd0\u52a8\u6355\u6349\uff09\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
