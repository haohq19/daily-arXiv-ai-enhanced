{"id": "2512.21476", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21476", "abs": "https://arxiv.org/abs/2512.21476", "authors": ["Suncheng Xiang", "Xiaoyang Wang", "Junjie Jiang", "Hejia Wang", "Dahong Qian"], "title": "GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification", "comment": "Work in progress", "summary": "Colonoscopic Polyp Re-Identification aims to match the same polyp from a large gallery with images from different views taken using different cameras, which plays an important role in the prevention and treatment of colorectal cancer in computer-aided diagnosis. However, the coarse resolution of high-level features of a specific polyp often leads to inferior results for small objects where detailed information is important. To address this challenge, we propose a novel architecture, named Gated Progressive Fusion network, to selectively fuse features from multiple levels using gates in a fully connected way for polyp ReID. On the basis of it, a gated progressive fusion strategy is introduced to achieve layer-wise refinement of semantic information through multi-level feature interactions. Experiments on standard benchmarks show the benefits of the multimodal setting over state-of-the-art unimodal ReID models, especially when combined with the specialized multimodal fusion strategy.", "AI": {"tldr": "\u63d0\u51faGated Progressive Fusion\u7f51\u7edc\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u9009\u62e9\u6027\u878d\u5408\u591a\u5c42\u7ea7\u7279\u5f81\uff0c\u7528\u4e8e\u7ed3\u80a0\u955c\u606f\u8089\u91cd\u8bc6\u522b\uff0c\u63d0\u5347\u5c0f\u76ee\u6807\u8bc6\u522b\u6027\u80fd", "motivation": "\u7ed3\u80a0\u955c\u606f\u8089\u91cd\u8bc6\u522b\u5728\u7ed3\u76f4\u80a0\u764c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9ad8\u5c42\u7279\u5f81\u7684\u7c97\u7c92\u5ea6\u5206\u8fa8\u7387\u5bf9\u5c0f\u76ee\u6807\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5c0f\u76ee\u6807\u9700\u8981\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f", "method": "\u63d0\u51faGated Progressive Fusion\u7f51\u7edc\uff0c\u4f7f\u7528\u95e8\u63a7\u673a\u5236\u5728\u5168\u8fde\u63a5\u65b9\u5f0f\u4e0b\u9009\u62e9\u6027\u878d\u5408\u591a\u5c42\u7ea7\u7279\u5f81\uff0c\u91c7\u7528\u95e8\u63a7\u6e10\u8fdb\u878d\u5408\u7b56\u7565\u901a\u8fc7\u591a\u5c42\u7ea7\u7279\u5f81\u4ea4\u4e92\u5b9e\u73b0\u8bed\u4e49\u4fe1\u606f\u7684\u9010\u5c42\u7ec6\u5316", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5355\u6a21\u6001\u91cd\u8bc6\u522b\u6a21\u578b\uff0c\u7279\u522b\u662f\u7ed3\u5408\u4e13\u95e8\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u65f6\u6548\u679c\u66f4\u4f73", "conclusion": "\u63d0\u51fa\u7684\u95e8\u63a7\u6e10\u8fdb\u878d\u5408\u7f51\u7edc\u901a\u8fc7\u6709\u6548\u878d\u5408\u591a\u5c42\u7ea7\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u80a0\u955c\u606f\u8089\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5c0f\u76ee\u6807\u8bc6\u522b\u65b9\u9762"}}
{"id": "2512.21706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21706", "abs": "https://arxiv.org/abs/2512.21706", "authors": ["Shuchang Pan", "Siddharth Banerjee", "Dhruv Hebbar", "Siddhant Patel", "Akshaj Gupta", "Kan Jen Cheng", "Hanjo Kim", "Zeyi Austin Li", "Martin Q. Ma", "Tingle Li", "Gopala Anumanchipalli", "Jiachen Lian"], "title": "Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech", "comment": null, "summary": "Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u548c\u56fe\u601d\u7ef4\uff08GoT\uff09\u7684\u5bf9\u8bdd\u884c\u4e3a\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6807\u6ce8\u9884\u6d4b\u6c9f\u901a\u610f\u56fe\u548c\u8a00\u8bed\u884c\u4e3a\uff0c\u5efa\u7acb\u56e0\u679c\u65f6\u5e8f\u4f9d\u8d56\uff0c\u5b9e\u73b0\u5168\u53cc\u5de5\u4f1a\u8bdd\u7cfb\u7edf\u7684\u81ea\u7136\u4ea4\u4e92\u3002", "motivation": "\u4eba\u7c7b\u5bf9\u8bdd\u7531\u9690\u542b\u7684\u601d\u7ef4\u94fe\u7ec4\u7ec7\uff0c\u8868\u73b0\u4e3a\u65f6\u5e8f\u8a00\u8bed\u884c\u4e3a\u3002\u6355\u6349\u8fd9\u79cd\u56e0\u679c\u8def\u5f84\u662f\u6784\u5efa\u81ea\u7136\u5168\u53cc\u5de5\u4ea4\u4e92\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8fd9\u79cd\u56e0\u679c\u5173\u7cfb\u7684\u5efa\u6a21\u3002", "method": "1. \u5c06\u5bf9\u8bdd\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u56fe\u601d\u7ef4\uff08GoT\uff09\u4e2d\u7684\u56e0\u679c\u63a8\u7406\uff1b2. \u91c7\u7528\u5c42\u6b21\u5316\u6807\u6ce8\u65b9\u6848\u5f62\u5f0f\u5316\u610f\u56fe\u5230\u884c\u4e3a\u7684\u8def\u5f84\uff0c\u9884\u6d4b\u9ad8\u5c42\u6c9f\u901a\u610f\u56fe\u548c\u4f4e\u5c42\u8a00\u8bed\u884c\u4e3a\uff1b3. \u4f7f\u7528\u6df7\u5408\u8bed\u6599\u5e93\u8bad\u7ec3\uff0c\u7ed3\u5408\u53ef\u63a7\u6a21\u62df\u6570\u636e\u548c\u771f\u5b9e\u5bf9\u8bdd\u6570\u636e\uff1b4. \u5c06\u6d41\u5f0f\u9884\u6d4b\u6784\u5efa\u4e3a\u6f14\u5316\u56fe\uff0c\u901a\u8fc7\u591a\u6a21\u6001Transformer\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8a00\u8bed\u884c\u4e3a\u5e76\u751f\u6210\u51b3\u7b56\u7406\u7531\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5168\u53cc\u5de5\u5bf9\u8bdd\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u884c\u4e3a\u68c0\u6d4b\uff0c\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u4e3a\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5bf9\u8bdd\u63a8\u7406\u57fa\u51c6\u5efa\u7acb\u4e86\u57fa\u7840\u3002", "conclusion": "GoT\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u5bf9\u8bdd\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u8fc7\u7a0b\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u8a00\u8bed\u884c\u4e3a\u3001\u751f\u6210\u89e3\u91ca\u6027\u7406\u7531\uff0c\u4e3a\u6784\u5efa\u66f4\u81ea\u7136\u3001\u53ef\u89e3\u91ca\u7684\u5168\u53cc\u5de5\u4f1a\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2512.21709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21709", "abs": "https://arxiv.org/abs/2512.21709", "authors": ["Md. Rakibul Islam", "Most. Sharmin Sultana Samu", "Md. Zahid Hossain", "Farhad Uz Zaman", "Md. Kamrozzaman Bhuiyan"], "title": "Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers", "comment": "Accepted for publication in 2025 28th International Conference on Computer and Information Technology (ICCIT)", "summary": "Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5b5f\u52a0\u62c9\u8bed\u4e2dAI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\uff0c\u6d4b\u8bd5\u4e86\u4e94\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u96f6\u6837\u672c\u8bc4\u4f30\u6548\u679c\u63a5\u8fd1\u968f\u673a\uff0c\u4f46\u7ecf\u8fc7\u5fae\u8c03\u540eXLM-RoBERTa\u3001mDeBERTa\u548cMultilingualBERT\u80fd\u8fbe\u5230\u7ea691%\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u5199\u4f5c\u7684\u6587\u672c\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u865a\u5047\u4fe1\u606f\u548c\u5185\u5bb9\u64cd\u7eb5\u7684\u62c5\u5fe7\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u6d89\u53ca\u591a\u8bed\u8a00\u68c0\u6d4b\uff0c\u4f46\u5b5f\u52a0\u62c9\u8bed\u9886\u57df\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u5b5f\u52a0\u62c9\u8bed\u4e30\u5bcc\u7684\u8bcd\u6c47\u548c\u590d\u6742\u7ed3\u6784\u4f7f\u5f97\u533a\u5206\u4eba\u7c7b\u5199\u4f5c\u548cAI\u751f\u6210\u6587\u672c\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7814\u7a76\u8c03\u67e5\u4e86\u4e94\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff1aXLMRoBERTa-Large\u3001mDeBERTaV3-Base\u3001BanglaBERT-Base\u3001IndicBERT-Base\u548cMultilingualBERT-Base\u3002\u9996\u5148\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u7136\u540e\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u3002", "result": "\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793a\u6240\u6709\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff08\u7ea650%\u51c6\u786e\u7387\uff09\u3002\u5fae\u8c03\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1aXLM-RoBERTa\u3001mDeBERTa\u548cMultilingualBERT\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u90fd\u8fbe\u5230\u7ea691%\u3002IndicBERT\u8868\u73b0\u76f8\u5bf9\u8f83\u5f31\uff0c\u8868\u660e\u5176\u5728\u8be5\u4efb\u52a1\u5fae\u8c03\u4e2d\u7684\u6709\u6548\u6027\u6709\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u5b5f\u52a0\u62c9\u8bed\u4e2dAI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\uff0c\u4e3a\u6784\u5efa\u5bf9\u6297AI\u751f\u6210\u5185\u5bb9\u7684\u5f3a\u5927\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u6700\u9002\u5408\u8be5\u4efb\u52a1\u7684\u6a21\u578b\u67b6\u6784\u3002"}}
{"id": "2512.21516", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21516", "abs": "https://arxiv.org/abs/2512.21516", "authors": ["Hongqing He", "Jie Xu", "Wenyuan Yang", "Yonghua Zhu", "Guoqiu Wen", "Xiaofeng Zhu"], "title": "Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data", "comment": null, "summary": "Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5bf9\u6bd4\u5b66\u4e60\u591a\u89c6\u56fe\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u56fe\u5f15\u5bfc\u548c\u5c40\u90e8\u56fe\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e0d\u5b8c\u6574\u548c\u566a\u58f0\u591a\u89c6\u56fe\u6570\u636e\u4e2d\u7684\u6837\u672c\u914d\u5bf9\u7a00\u5c11\u548c\u9519\u8bef\u914d\u5bf9\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u89c6\u56fe\u6570\u636e\u5e38\u5b58\u5728\u4e0d\u5b8c\u6574\u6216\u566a\u58f0\u95ee\u9898\uff0c\u5bfc\u81f4\u6837\u672c\u914d\u5bf9\u7a00\u5c11\u6216\u9519\u8bef\u914d\u5bf9\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u6548\u679c\u3002\u7a00\u5c11\u914d\u5bf9\u95ee\u9898\u963b\u788d\u4e86\u591a\u89c6\u56fe\u4e92\u8865\u4fe1\u606f\u7684\u5145\u5206\u63d0\u53d6\uff0c\u800c\u9519\u8bef\u914d\u5bf9\u95ee\u9898\u5219\u5bfc\u81f4\u5bf9\u6bd4\u5b66\u4e60\u5728\u9519\u8bef\u65b9\u5411\u4e0a\u4f18\u5316\u6a21\u578b\u3002", "method": "1. \u5168\u5c40\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\uff1a\u6240\u6709\u89c6\u56fe\u6837\u672c\u6784\u5efa\u5168\u5c40\u89c6\u56fe\u4eb2\u548c\u56fe\uff0c\u5f62\u6210\u65b0\u7684\u6837\u672c\u5bf9\u4ee5\u5145\u5206\u63a2\u7d22\u4e92\u8865\u4fe1\u606f\uff0c\u89e3\u51b3\u7a00\u5c11\u914d\u5bf9\u95ee\u9898\u30022. \u5c40\u90e8\u56fe\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60\uff1a\u5229\u7528\u5c40\u90e8\u90bb\u5c45\u751f\u6210\u914d\u5bf9\u6743\u91cd\uff0c\u81ea\u9002\u5e94\u5730\u52a0\u5f3a\u6216\u51cf\u5f31\u914d\u5bf9\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7f13\u89e3\u9519\u8bef\u914d\u5bf9\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u63d2\u8865\uff0c\u53ef\u96c6\u6210\u5230\u7edf\u4e00\u7684\u5168\u5c40-\u5c40\u90e8\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u4e0d\u5b8c\u6574\u548c\u566a\u58f0\u8bbe\u7f6e\u4e0b\u7684\u591a\u89c6\u56fe\u6570\u636e\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u5bf9\u6bd4\u5b66\u4e60\u591a\u89c6\u56fe\u805a\u7c7b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u548c\u566a\u58f0\u591a\u89c6\u56fe\u6570\u636e\u4e2d\u7684\u6837\u672c\u914d\u5bf9\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u56fe\u5f15\u5bfc\u548c\u5c40\u90e8\u56fe\u52a0\u6743\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6548\u679c\u3002"}}
{"id": "2512.21545", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21545", "abs": "https://arxiv.org/abs/2512.21545", "authors": ["Sanghyun Jo", "Donghwan Lee", "Eunji Jung", "Seong Je Oh", "Kyungsu Kim"], "title": "EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal", "comment": null, "summary": "Object removal differs from common inpainting, since it must prevent the masked target from reappearing and reconstruct the occluded background with structural and contextual fidelity, rather than merely filling a hole plausibly. Recent dataset-free approaches that redirect self-attention inside the mask fail in two ways: non-target foregrounds are often misinterpreted as background, which regenerates unwanted objects, and direct attention manipulation disrupts fine details and hinders coherent integration of background cues. We propose EraseLoRA, a novel dataset-free framework that replaces attention surgery with background-aware reasoning and test-time adaptation. First, Background-aware Foreground Exclusion (BFE), uses a multimodal large-language models to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without paired supervision, producing reliable background cues while excluding distractors. Second, Background-aware Reconstruction with Subtype Aggregation (BRSA), performs test-time optimization that treats inferred background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives, preserving local detail and global structure without explicit attention intervention. We validate EraseLoRA as a plug-in to pretrained diffusion models and across benchmarks for object removal, demonstrating consistent improvements over dataset-free baselines and competitive results against dataset-driven methods. The code will be made available upon publication.", "AI": {"tldr": "EraseLoRA\uff1a\u65e0\u9700\u6570\u636e\u96c6\u7684\u7269\u4f53\u79fb\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u80cc\u666f\u611f\u77e5\u63a8\u7406\u548c\u6d4b\u8bd5\u65f6\u9002\u914d\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u975e\u76ee\u6807\u524d\u666f\u88ab\u8bef\u5224\u4e3a\u80cc\u666f\u5bfc\u81f4\u7269\u4f53\u91cd\u73b0\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u64cd\u4f5c\u7834\u574f\u7ec6\u8282\u7684\u95ee\u9898\u3002", "motivation": "\u7269\u4f53\u79fb\u9664\u4e0e\u666e\u901a\u4fee\u590d\u4e0d\u540c\uff0c\u9700\u8981\u9632\u6b62\u88ab\u63a9\u7801\u7684\u76ee\u6807\u91cd\u65b0\u51fa\u73b0\uff0c\u5e76\u4ee5\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u91cd\u5efa\u88ab\u906e\u6321\u7684\u80cc\u666f\u3002\u73b0\u6709\u65e0\u6570\u636e\u96c6\u65b9\u6cd5\u901a\u8fc7\u91cd\u5b9a\u5411\u63a9\u7801\u5185\u7684\u81ea\u6ce8\u610f\u529b\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u975e\u76ee\u6807\u524d\u666f\u5e38\u88ab\u8bef\u5224\u4e3a\u80cc\u666f\u5bfc\u81f4\u4e0d\u9700\u8981\u7684\u7269\u4f53\u91cd\u65b0\u751f\u6210\uff1b\u76f4\u63a5\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u7834\u574f\u7ec6\u8282\u5e76\u963b\u788d\u80cc\u666f\u7ebf\u7d22\u7684\u8fde\u8d2f\u6574\u5408\u3002", "method": "\u63d0\u51faEraseLoRA\u6846\u67b6\uff1a1) \u80cc\u666f\u611f\u77e5\u524d\u666f\u6392\u9664(BFE)\uff1a\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u5355\u5f20\u56fe\u50cf-\u63a9\u7801\u5bf9\u4e2d\u5206\u79bb\u76ee\u6807\u524d\u666f\u3001\u975e\u76ee\u6807\u524d\u666f\u548c\u5e72\u51c0\u80cc\u666f\uff0c\u65e0\u9700\u914d\u5bf9\u76d1\u7763\uff1b2) \u80cc\u666f\u611f\u77e5\u91cd\u5efa\u4e0e\u5b50\u7c7b\u578b\u805a\u5408(BRSA)\uff1a\u8fdb\u884c\u6d4b\u8bd5\u65f6\u4f18\u5316\uff0c\u5c06\u63a8\u65ad\u7684\u80cc\u666f\u5b50\u7c7b\u578b\u89c6\u4e3a\u4e92\u8865\u7247\u6bb5\uff0c\u901a\u8fc7\u91cd\u5efa\u548c\u5bf9\u9f50\u76ee\u6807\u5f3a\u5236\u5176\u4e00\u81f4\u6574\u5408\uff0c\u65e0\u9700\u663e\u5f0f\u6ce8\u610f\u529b\u5e72\u9884\u3002", "result": "EraseLoRA\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u63d2\u4ef6\uff0c\u5728\u7269\u4f53\u79fb\u9664\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5bf9\u65e0\u6570\u636e\u96c6\u57fa\u7ebf\u7684\u6301\u7eed\u6539\u8fdb\uff0c\u5e76\u4e0e\u6570\u636e\u96c6\u9a71\u52a8\u65b9\u6cd5\u7ade\u4e89\u7684\u7ed3\u679c\u3002\u4ee3\u7801\u5c06\u5728\u53d1\u8868\u540e\u63d0\u4f9b\u3002", "conclusion": "EraseLoRA\u901a\u8fc7\u80cc\u666f\u611f\u77e5\u63a8\u7406\u548c\u6d4b\u8bd5\u65f6\u9002\u914d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65e0\u6570\u636e\u96c6\u7269\u4f53\u79fb\u9664\u65b9\u6cd5\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u7269\u4f53\u79fb\u9664\u6548\u679c\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u3002"}}
{"id": "2512.21637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21637", "abs": "https://arxiv.org/abs/2512.21637", "authors": ["Mutiara Shabrina", "Nova Kurnia Putri", "Jefri Satria Ferdiansyah", "Sabita Khansa Dewi", "Novanto Yudistira"], "title": "Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints", "comment": null, "summary": "Text-driven image manipulation often suffers from attribute entanglement, where modifying a target attribute (e.g., adding bangs) unintentionally alters other semantic properties such as identity or appearance. The Predict, Prevent, and Evaluate (PPE) framework addresses this issue by leveraging pre-trained vision-language models for disentangled editing. In this work, we analyze the PPE framework, focusing on its architectural components, including BERT-based attribute prediction and StyleGAN2-based image generation on the CelebA-HQ dataset. Through empirical analysis, we identify a limitation in the original regularization strategy, where latent updates remain dense and prone to semantic leakage. To mitigate this issue, we introduce a sparsity-based constraint using L1 regularization on latent space manipulation. Experimental results demonstrate that the proposed approach enforces more focused and controlled edits, effectively reducing unintended changes in non-target attributes while preserving facial identity.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86PPE\u6846\u67b6\uff0c\u53d1\u73b0\u5176\u6b63\u5219\u5316\u7b56\u7565\u5b58\u5728\u6f5c\u5728\u66f4\u65b0\u5bc6\u96c6\u5bfc\u81f4\u8bed\u4e49\u6cc4\u6f0f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4f7f\u7528L1\u6b63\u5219\u5316\u8fdb\u884c\u7a00\u758f\u7ea6\u675f\uff0c\u5b9e\u73b0\u66f4\u805a\u7126\u548c\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\u3002", "motivation": "\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u5e38\u9762\u4e34\u5c5e\u6027\u7ea0\u7f20\u95ee\u9898\uff0c\u5373\u4fee\u6539\u76ee\u6807\u5c5e\u6027\uff08\u5982\u6dfb\u52a0\u5218\u6d77\uff09\u4f1a\u65e0\u610f\u4e2d\u6539\u53d8\u5176\u4ed6\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u8eab\u4efd\u6216\u5916\u8c8c\uff09\u3002PPE\u6846\u67b6\u867d\u7136\u5c1d\u8bd5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u5176\u6b63\u5219\u5316\u7b56\u7565\u5b58\u5728\u6f5c\u5728\u66f4\u65b0\u5bc6\u96c6\u3001\u6613\u5bfc\u81f4\u8bed\u4e49\u6cc4\u6f0f\u7684\u5c40\u9650\u6027\u3002", "method": "1. \u5206\u6790PPE\u6846\u67b6\u7684\u67b6\u6784\u7ec4\u4ef6\uff08\u57fa\u4e8eBERT\u7684\u5c5e\u6027\u9884\u6d4b\u548c\u57fa\u4e8eStyleGAN2\u7684\u56fe\u50cf\u751f\u6210\uff09\uff1b2. \u8bc6\u522b\u539f\u59cb\u6b63\u5219\u5316\u7b56\u7565\u7684\u5c40\u9650\u6027\uff1b3. \u63d0\u51fa\u57fa\u4e8eL1\u6b63\u5219\u5316\u7684\u7a00\u758f\u7ea6\u675f\u65b9\u6cd5\uff0c\u5bf9\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u65bd\u52a0\u7a00\u758f\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u805a\u7126\u548c\u53ef\u63a7\u7684\u7f16\u8f91\uff0c\u6709\u6548\u51cf\u5c11\u975e\u76ee\u6807\u5c5e\u6027\u7684\u610f\u5916\u6539\u53d8\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u7559\u9762\u90e8\u8eab\u4efd\u7279\u5f81\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7a00\u758f\u7ea6\u675f\uff0c\u6539\u8fdb\u4e86PPE\u6846\u67b6\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6f5c\u5728\u66f4\u65b0\u5bc6\u96c6\u5bfc\u81f4\u7684\u8bed\u4e49\u6cc4\u6f0f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2512.21791", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2512.21791", "abs": "https://arxiv.org/abs/2512.21791", "authors": ["Christophe D. Hounwanou", "Yae Ulrich Gaba", "Pierre Ntakirutimana"], "title": "Synthetic Financial Data Generation for Enhanced Financial Modelling", "comment": "23 pages, 7 figures, 6 tables. Submitted as a preprint. This work presents a unified multi-criteria evaluation framework for synthetic financial data, applied to ARIMA-GARCH, VAEs, and TimeGAN models", "summary": "Data scarcity and confidentiality in finance often impede model development and robust testing. This paper presents a unified multi-criteria evaluation framework for synthetic financial data and applies it to three representative generative paradigms: the statistical ARIMA-GARCH baseline, Variational Autoencoders (VAEs), and Time-series Generative Adversarial Networks (TimeGAN). Using historical S and P 500 daily data, we evaluate fidelity (Maximum Mean Discrepancy, MMD), temporal structure (autocorrelation and volatility clustering), and practical utility in downstream tasks, specifically mean-variance portfolio optimization and volatility forecasting. Empirical results indicate that ARIMA-GARCH captures linear trends and conditional volatility but fails to reproduce nonlinear dynamics; VAEs produce smooth trajectories that underestimate extreme events; and TimeGAN achieves the best trade-off between realism and temporal coherence (e.g., TimeGAN attained the lowest MMD: 1.84e-3, average over 5 seeds). Finally, we articulate practical guidelines for selecting generative models according to application needs and computational constraints. Our unified evaluation protocol and reproducible codebase aim to standardize benchmarking in synthetic financial data research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5408\u6210\u91d1\u878d\u6570\u636e\u8bc4\u4f30\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u4e09\u79cd\u751f\u6210\u6a21\u578b\uff1aARIMA-GARCH\u3001VAE\u548cTimeGAN\uff0c\u901a\u8fc7S&P 500\u6570\u636e\u8bc4\u4f30\u5176\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u7ed3\u6784\u548c\u4e0b\u6e38\u4efb\u52a1\u5b9e\u7528\u6027\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u7684\u6570\u636e\u7a00\u7f3a\u6027\u548c\u4fdd\u5bc6\u6027\u9650\u5236\u4e86\u6a21\u578b\u5f00\u53d1\u548c\u7a33\u5065\u6d4b\u8bd5\uff0c\u9700\u8981\u6709\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u591a\u6807\u51c6\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4fdd\u771f\u5ea6\uff08MMD\uff09\u3001\u65f6\u95f4\u7ed3\u6784\uff08\u81ea\u76f8\u5173\u548c\u6ce2\u52a8\u7387\u805a\u7c7b\uff09\u548c\u4e0b\u6e38\u4efb\u52a1\u5b9e\u7528\u6027\uff08\u5747\u503c-\u65b9\u5dee\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u548c\u6ce2\u52a8\u7387\u9884\u6d4b\uff09\uff0c\u5e94\u7528\u4e8e\u4e09\u79cd\u751f\u6210\u6a21\u578b\uff1a\u7edf\u8ba1ARIMA-GARCH\u3001VAE\u548cTimeGAN\u3002", "result": "ARIMA-GARCH\u80fd\u6355\u6349\u7ebf\u6027\u8d8b\u52bf\u548c\u6761\u4ef6\u6ce2\u52a8\u7387\u4f46\u65e0\u6cd5\u590d\u5236\u975e\u7ebf\u6027\u52a8\u6001\uff1bVAE\u4ea7\u751f\u5e73\u6ed1\u8f68\u8ff9\u4f46\u4f4e\u4f30\u6781\u7aef\u4e8b\u4ef6\uff1bTimeGAN\u5728\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff08MMD\u6700\u4f4e\uff1a1.84e-3\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6839\u636e\u5e94\u7528\u9700\u6c42\u548c\u8ba1\u7b97\u7ea6\u675f\u9009\u62e9\u751f\u6210\u6a21\u578b\u7684\u5b9e\u7528\u6307\u5357\uff0c\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u53ef\u590d\u73b0\u4ee3\u7801\u5e93\u65e8\u5728\u6807\u51c6\u5316\u5408\u6210\u91d1\u878d\u6570\u636e\u7814\u7a76\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2512.21801", "categories": ["cs.LG", "cs.DC", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.21801", "abs": "https://arxiv.org/abs/2512.21801", "authors": ["Krishna Chaitanya Sunkara", "Rambabu Konakanchi"], "title": "Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers", "comment": "7 pages, 6 figures, IEEE conference format", "summary": "AI data centers which are GPU centric, have adopted liquid cooling to handle extreme heat loads, but coolant leaks result in substantial energy loss through unplanned shutdowns and extended repair periods. We present a proof-of-concept smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting with Random Forest classifiers for instant detection. Testing on synthetic data aligned with ASHRAE 2021 standards, our approach achieves 96.5% detection accuracy and 87% forecasting accuracy at 90% probability within plus or minus 30-minute windows. Analysis demonstrates that humidity, pressure, and flow rate deliver strong predictive signals, while temperature exhibits minimal immediate response due to thermal inertia in server hardware. The system employs MQTT streaming, InfluxDB storage, and Streamlit dashboards, forecasting leaks 2-4 hours ahead while identifying sudden events within 1 minute. For a typical 47-rack facility, this approach could prevent roughly 1,500 kWh annual energy waste through proactive maintenance rather than reactive emergency procedures. While validation remains synthetic-only, results establish feasibility for future operational deployment in sustainable data center operations.", "AI": {"tldr": "\u57fa\u4e8eLSTM\u548c\u968f\u673a\u68ee\u6797\u7684\u667a\u80fd\u7269\u8054\u7f51\u76d1\u63a7\u7cfb\u7edf\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u68c0\u6d4b\u6570\u636e\u4e2d\u5fc3\u6db2\u4f53\u51b7\u5374\u6cc4\u6f0f\uff0c\u53ef\u63d0\u524d2-4\u5c0f\u65f6\u9884\u8b66\u5e76\u57281\u5206\u949f\u5185\u8bc6\u522b\u7a81\u53d1\u6cc4\u6f0f\uff0c\u51cf\u5c11\u80fd\u6e90\u6d6a\u8d39\u3002", "motivation": "AI\u6570\u636e\u4e2d\u5fc3\u91c7\u7528\u6db2\u4f53\u51b7\u5374\u5904\u7406\u9ad8\u70ed\u91cf\uff0c\u4f46\u51b7\u5374\u5242\u6cc4\u6f0f\u4f1a\u5bfc\u81f4\u8ba1\u5212\u5916\u505c\u673a\u548c\u5927\u4fee\u671f\u95f4\u5927\u91cf\u80fd\u6e90\u635f\u5931\uff0c\u9700\u8981\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u6765\u9884\u9632\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408LSTM\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6982\u7387\u6cc4\u6f0f\u9884\u6d4b\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8fdb\u884c\u5373\u65f6\u68c0\u6d4b\u7684\u667a\u80fd\u7269\u8054\u7f51\u76d1\u63a7\u7cfb\u7edf\uff0c\u4f7f\u7528MQTT\u6d41\u4f20\u8f93\u3001InfluxDB\u5b58\u50a8\u548cStreamlit\u4eea\u8868\u677f\u3002", "result": "\u5728\u7b26\u5408ASHRAE 2021\u6807\u51c6\u7684\u5408\u6210\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0c\u8fbe\u523096.5%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u548c87%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0890%\u6982\u7387\uff0c\u00b130\u5206\u949f\u7a97\u53e3\uff09\uff0c\u6e7f\u5ea6\u3001\u538b\u529b\u548c\u6d41\u91cf\u63d0\u4f9b\u5f3a\u9884\u6d4b\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u53ef\u63d0\u524d2-4\u5c0f\u65f6\u9884\u6d4b\u6cc4\u6f0f\u5e76\u57281\u5206\u949f\u5185\u8bc6\u522b\u7a81\u53d1\u4e8b\u4ef6\uff0c\u5bf9\u4e8e\u5178\u578b47\u673a\u67b6\u8bbe\u65bd\uff0c\u6bcf\u5e74\u53ef\u9632\u6b62\u7ea61,500 kWh\u80fd\u6e90\u6d6a\u8d39\uff0c\u4e3a\u53ef\u6301\u7eed\u6570\u636e\u4e2d\u5fc3\u8fd0\u8425\u63d0\u4f9b\u53ef\u884c\u6027\u65b9\u6848\u3002"}}
{"id": "2512.21710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21710", "abs": "https://arxiv.org/abs/2512.21710", "authors": ["Zhan Chen", "Zile Guo", "Enze Zhu", "Peirong Zhang", "Xiaoxuan Liu", "Lei Wang", "Yidan Zhang"], "title": "RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention", "comment": null, "summary": "Video prediction is plagued by a fundamental trilemma: achieving high-resolution and perceptual quality typically comes at the cost of real-time speed, hindering its use in latency-critical applications. This challenge is most acute for autonomous UAVs in dense urban environments, where foreseeing events from high-resolution imagery is non-negotiable for safety. Existing methods, reliant on iterative generation (diffusion, autoregressive models) or quadratic-complexity attention, fail to meet these stringent demands on edge hardware. To break this long-standing trade-off, we introduce RAPTOR, a video prediction architecture that achieves real-time, high-resolution performance. RAPTOR's single-pass design avoids the error accumulation and latency of iterative approaches. Its core innovation is Efficient Video Attention (EVA), a novel translator module that factorizes spatiotemporal modeling. Instead of processing flattened spacetime tokens with $O((ST)^2)$ or $O(ST)$ complexity, EVA alternates operations along the spatial (S) and temporal (T) axes. This factorization reduces the time complexity to $O(S + T)$ and memory complexity to $O(max(S, T))$, enabling global context modeling at $512^2$ resolution and beyond, operating directly on dense feature maps with a patch-free design. Complementing this architecture is a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details. Experiments show RAPTOR is the first predictor to exceed 30 FPS on a Jetson AGX Orin for $512^2$ video, setting a new state-of-the-art on UAVid, KTH, and a custom high-resolution dataset in PSNR, SSIM, and LPIPS. Critically, RAPTOR boosts the mission success rate in a real-world UAV navigation task by 18/%, paving the way for safer and more anticipatory embodied agents.", "AI": {"tldr": "RAPTOR\u662f\u4e00\u4e2a\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u9884\u6d4b\u67b6\u6784\uff0c\u901a\u8fc7\u521b\u65b0\u7684Efficient Video Attention\u6a21\u5757\u548c3\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u7b49\u8fb9\u7f18\u8bbe\u5907\u5e94\u7528\u3002", "motivation": "\u89c6\u9891\u9884\u6d4b\u9762\u4e34\u5206\u8fa8\u7387\u3001\u8d28\u91cf\u548c\u901f\u5ea6\u7684\u4e09\u96be\u56f0\u5883\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u81ea\u4e3b\u65e0\u4eba\u673a\u5728\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u5bf9\u9ad8\u5206\u8fa8\u7387\u5b9e\u65f6\u9884\u6d4b\u7684\u4e25\u683c\u8981\u6c42\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u5ef6\u8fdf\u5173\u952e\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u63d0\u51faRAPTOR\u67b6\u6784\uff0c\u91c7\u7528\u5355\u6b21\u524d\u5411\u8bbe\u8ba1\u907f\u514d\u8fed\u4ee3\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u5ef6\u8fdf\u3002\u6838\u5fc3\u521b\u65b0\u662fEfficient Video Attention\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u65f6\u7a7a\u5efa\u6a21\u5206\u89e3\u4e3a\u7a7a\u95f4\u548c\u65f6\u95f4\u8f74\u4ea4\u66ff\u64cd\u4f5c\uff0c\u5c06\u590d\u6742\u5ea6\u4eceO(ST\u00b2)\u964d\u4f4e\u5230O(S+T)\uff0c\u5b9e\u73b0\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002\u914d\u54083\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\u4ece\u7c97\u5230\u7ec6\u9010\u6b65\u4f18\u5316\u9884\u6d4b\u3002", "result": "RAPTOR\u662f\u9996\u4e2a\u5728Jetson AGX Orin\u4e0a\u5bf9512\u00b2\u5206\u8fa8\u7387\u89c6\u9891\u8d85\u8fc730 FPS\u7684\u9884\u6d4b\u5668\uff0c\u5728UAVid\u3001KTH\u548c\u81ea\u5b9a\u4e49\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e0aPSNR\u3001SSIM\u548cLPIPS\u6307\u6807\u8fbe\u5230\u65b0SOTA\u3002\u5728\u5b9e\u9645\u65e0\u4eba\u673a\u5bfc\u822a\u4efb\u52a1\u4e2d\u5c06\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534718%\u3002", "conclusion": "RAPTOR\u6253\u7834\u4e86\u89c6\u9891\u9884\u6d4b\u4e2d\u957f\u671f\u5b58\u5728\u7684\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u5177\u9884\u89c1\u6027\u7684\u5177\u8eab\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u5ef6\u8fdf\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2512.21804", "categories": ["cs.CV", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.21804", "abs": "https://arxiv.org/abs/2512.21804", "authors": ["Rahul Gupta"], "title": "S&P 500 Stock's Movement Prediction using CNN", "comment": "9 pages, 19 diagrams. Originally submitted as a part of my Stanford University program taught by Dr. Fei Fei Lee and Andrej Karpathy CS231N 2018", "summary": "This paper is about predicting the movement of stock consist of S&P 500 index. Historically there are many approaches have been tried using various methods to predict the stock movement and being used in the market currently for algorithm trading and alpha generating systems using traditional mathematical approaches [1, 2].\n  The success of artificial neural network recently created a lot of interest and paved the way to enable prediction using cutting-edge research in the machine learning and deep learning. Some of these papers have done a great job in implementing and explaining benefits of these new technologies. Although most these papers do not go into the complexity of the financial data and mostly utilize single dimension data, still most of these papers were successful in creating the ground for future research in this comparatively new phenomenon. In this paper, I am trying to use multivariate raw data including stock split/dividend events (as-is) present in real-world market data instead of engineered financial data. Convolution Neural Network (CNN), the best-known tool so far for image classification, is used on the multi-dimensional stock numbers taken from the market mimicking them as a vector of historical data matrices (read images) and the model achieves promising results. The predictions can be made stock by stock, i.e., a single stock, sector-wise or for the portfolio of stocks.", "AI": {"tldr": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u5904\u7406\u591a\u7ef4\u539f\u59cb\u80a1\u5e02\u6570\u636e\u9884\u6d4bS&P 500\u6307\u6570\u80a1\u7968\u8d70\u52bf\uff0c\u5c06\u5386\u53f2\u6570\u636e\u77e9\u9635\u89c6\u4e3a\"\u56fe\u50cf\"\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b", "motivation": "\u4f20\u7edf\u6570\u5b66\u65b9\u6cd5\u5728\u7b97\u6cd5\u4ea4\u6613\u4e2d\u5df2\u6709\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5927\u591a\u4f7f\u7528\u5355\u7ef4\u6570\u636e\u4e14\u672a\u5145\u5206\u8003\u8651\u91d1\u878d\u6570\u636e\u7684\u590d\u6742\u6027\u3002\u672c\u6587\u65e8\u5728\u4f7f\u7528\u5305\u542b\u80a1\u7968\u5206\u5272/\u80a1\u606f\u4e8b\u4ef6\u7684\u591a\u7ef4\u539f\u59cb\u5e02\u573a\u6570\u636e\uff0c\u800c\u975e\u4eba\u5de5\u8bbe\u8ba1\u7684\u91d1\u878d\u6570\u636e", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u5904\u7406\u591a\u7ef4\u80a1\u7968\u6570\u636e\uff0c\u5c06\u5386\u53f2\u6570\u636e\u77e9\u9635\u89c6\u4e3a\u5411\u91cf\u5316\u7684\"\u56fe\u50cf\"\uff0c\u7528\u4e8e\u9884\u6d4b\u5355\u4e2a\u80a1\u7968\u3001\u884c\u4e1a\u677f\u5757\u6216\u6295\u8d44\u7ec4\u5408\u7684\u8d70\u52bf", "result": "\u6a21\u578b\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u80a1\u7968\u8d70\u52bf", "conclusion": "CNN\u80fd\u591f\u6210\u529f\u5e94\u7528\u4e8e\u591a\u7ef4\u539f\u59cb\u91d1\u878d\u6570\u636e\u9884\u6d4b\uff0c\u4e3a\u8fd9\u4e00\u76f8\u5bf9\u65b0\u5174\u9886\u57df\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u652f\u6301\u4ece\u5355\u53ea\u80a1\u7968\u5230\u6295\u8d44\u7ec4\u5408\u7684\u591a\u5c42\u6b21\u9884\u6d4b"}}
{"id": "2512.21861", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21861", "abs": "https://arxiv.org/abs/2512.21861", "authors": ["Md Rafid Islam", "Rafsan Jany", "Akib Ahmed", "Mohammad Ashrafuzzaman Khan"], "title": "Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening", "comment": null, "summary": "Diabetic retinopathy (DR) remains a leading cause of preventable blindness, yet large-scale screening is constrained by limited specialist availability and variable image quality across devices and populations. This work investigates whether feature-level fusion of complementary convolutional neural network (CNN) backbones can deliver accurate and efficient binary DR screening on globally sourced fundus images. Using 11,156 images pooled from five public datasets (APTOS, EyePACS, IDRiD, Messidor, and ODIR), we frame DR detection as a binary classification task and compare three pretrained models (ResNet50, EfficientNet-B0, and DenseNet121) against pairwise and tri-fusion variants. Across five independent runs, fusion consistently outperforms single backbones. The EfficientNet-B0 + DenseNet121 (Eff+Den) fusion model achieves the best overall mean performance (accuracy: 82.89\\%) with balanced class-wise F1-scores for normal (83.60\\%) and diabetic (82.60\\%) cases. While the tri-fusion is competitive, it incurs a substantially higher computational cost. Inference profiling highlights a practical trade-off: EfficientNet-B0 is the fastest (approximately 1.16 ms/image at batch size 1000), whereas the Eff+Den fusion offers a favorable accuracy--latency balance. These findings indicate that lightweight feature fusion can enhance generalization across heterogeneous datasets, supporting scalable binary DR screening workflows where both accuracy and throughput are critical.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u901a\u8fc7\u7279\u5f81\u7ea7\u878d\u5408\u591a\u4e2aCNN\u9aa8\u5e72\u7f51\u7edc\u6765\u63d0\u5347\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u4e8c\u5206\u7c7b\u7b5b\u67e5\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u878d\u5408\u6a21\u578b\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\uff0c\u5176\u4e2dEfficientNet-B0 + DenseNet121\u878d\u5408\u5728\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u53ef\u9884\u9632\u6027\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u5927\u89c4\u6a21\u7b5b\u67e5\u53d7\u5230\u4e13\u5bb6\u8d44\u6e90\u6709\u9650\u4ee5\u53ca\u4e0d\u540c\u8bbe\u5907\u548c\u4eba\u7fa4\u56fe\u50cf\u8d28\u91cf\u5dee\u5f02\u7684\u5236\u7ea6\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u5f02\u8d28\u6570\u636e\u3001\u540c\u65f6\u4fdd\u8bc1\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u81ea\u52a8\u5316\u7b5b\u67e5\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08APTOS\u3001EyePACS\u3001IDRiD\u3001Messidor\u3001ODIR\uff09\u768411,156\u5f20\u773c\u5e95\u56fe\u50cf\uff0c\u5c06DR\u68c0\u6d4b\u4f5c\u4e3a\u4e8c\u5206\u7c7b\u4efb\u52a1\u3002\u6bd4\u8f83\u4e86\u4e09\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08ResNet50\u3001EfficientNet-B0\u3001DenseNet121\uff09\u53ca\u5176\u4e24\u4e24\u878d\u5408\u548c\u4e09\u8005\u878d\u5408\u7684\u53d8\u4f53\u3002\u901a\u8fc7\u4e94\u6b21\u72ec\u7acb\u8fd0\u884c\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u8fdb\u884c\u63a8\u7406\u65f6\u95f4\u5206\u6790\u3002", "result": "\u878d\u5408\u6a21\u578b\u5728\u6240\u6709\u8fd0\u884c\u4e2d\u5747\u4f18\u4e8e\u5355\u4e00\u9aa8\u5e72\u7f51\u7edc\u3002EfficientNet-B0 + DenseNet121\u878d\u5408\u6a21\u578b\u83b7\u5f97\u6700\u4f73\u5e73\u5747\u6027\u80fd\uff08\u51c6\u786e\u738782.89%\uff09\uff0c\u6b63\u5e38\u75c5\u4f8b\u548c\u7cd6\u5c3f\u75c5\u75c5\u4f8b\u7684F1\u5206\u6570\u5e73\u8861\uff08\u5206\u522b\u4e3a83.60%\u548c82.60%\uff09\u3002\u4e09\u6a21\u578b\u878d\u5408\u867d\u7136\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u663e\u8457\u66f4\u9ad8\u3002\u63a8\u7406\u5206\u6790\u663e\u793aEfficientNet-B0\u6700\u5feb\uff08\u7ea61.16\u6beb\u79d2/\u56fe\u50cf\uff09\uff0c\u800cEff+Den\u878d\u5408\u5728\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6709\u5229\u7684\u5e73\u8861\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7279\u5f81\u878d\u5408\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u5728\u5f02\u8d28\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u5728\u51c6\u786e\u6027\u548c\u541e\u5410\u91cf\u90fd\u81f3\u5173\u91cd\u8981\u7684\u53ef\u6269\u5c55DR\u4e8c\u5206\u7c7b\u7b5b\u67e5\u5de5\u4f5c\u6d41\u7a0b\u3002\u878d\u5408\u7b56\u7565\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2512.22046", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22046", "abs": "https://arxiv.org/abs/2512.22046", "authors": ["Zongmin Zhang", "Zhen Sun", "Yifan Liao", "Wenhan Dong", "Xinlei He", "Xingshuo Han", "Shengmin Xu", "Xinyi Huang"], "title": "Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models", "comment": null, "summary": "Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.", "AI": {"tldr": "BadVSFM\uff1a\u9996\u4e2a\u9488\u5bf9\u63d0\u793a\u9a71\u52a8\u89c6\u9891\u5206\u5272\u57fa\u7840\u6a21\u578b\uff08VSFM\uff09\u7684\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u5e38\u5206\u5272\u8d28\u91cf", "motivation": "\u968f\u7740SAM2\u7b49\u63d0\u793a\u9a71\u52a8\u89c6\u9891\u5206\u5272\u57fa\u7840\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u75c5\u7406\u5b66\u7b49\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\uff0c\u540e\u95e8\u5b89\u5168\u5a01\u80c1\u65e5\u76ca\u51f8\u663e\u3002\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u540e\u95e8\u653b\u51fb\u5bf9VSFMs\u51e0\u4e4e\u65e0\u6548\uff08\u6210\u529f\u7387\u4f4e\u4e8e5%\uff09\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9VSFM\u7279\u6027\u7684\u653b\u51fb\u65b9\u6cd5", "method": "\u63d0\u51faBadVSFM\u4e24\u9636\u6bb5\u653b\u51fb\u6846\u67b6\uff1a1\uff09\u5f15\u5bfc\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u4f7f\u89e6\u53d1\u5e27\u6620\u5c04\u5230\u76ee\u6807\u5d4c\u5165\uff0c\u5e72\u51c0\u5e27\u4fdd\u6301\u4e0e\u53c2\u8003\u7f16\u7801\u5668\u5bf9\u9f50\uff1b2\uff09\u8bad\u7ec3\u63a9\u7801\u89e3\u7801\u5668\uff0c\u4f7f\u6240\u6709\u63d0\u793a\u7c7b\u578b\u7684\u89e6\u53d1\u5e27-\u63d0\u793a\u5bf9\u4ea7\u751f\u5171\u4eab\u76ee\u6807\u63a9\u7801\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u8f93\u51fa\u63a5\u8fd1\u53c2\u8003\u89e3\u7801\u5668", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u4e94\u4e2aVSFM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBadVSFM\u5728\u4e0d\u540c\u89e6\u53d1\u5668\u548c\u63d0\u793a\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u53ef\u63a7\u7684\u540e\u95e8\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u5206\u5272\u8d28\u91cf\u3002\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5bf9\u56db\u79cd\u4ee3\u8868\u6027\u9632\u5fa1\u65b9\u6cd5\u57fa\u672c\u65e0\u6548", "conclusion": "BadVSFM\u63ed\u793a\u4e86\u5f53\u524dVSFMs\u4e2d\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u901a\u8fc7\u5206\u79bb\u89e6\u53d1\u548c\u5e72\u51c0\u8868\u793a\u3001\u5c06\u6ce8\u610f\u529b\u8f6c\u79fb\u5230\u89e6\u53d1\u533a\u57df\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u540e\u95e8\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9"}}
{"id": "2512.22096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22096", "abs": "https://arxiv.org/abs/2512.22096", "authors": ["Xiaofeng Mao", "Zhen Li", "Chuanhao Li", "Xiaojie Xu", "Kaining Ying", "Tong He", "Jiangmiao Pang", "Yu Qiao", "Kaipeng Zhang"], "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "comment": null, "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "AI": {"tldr": "\u63d0\u51fa\u540d\u4e3a\\method\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u903c\u771f\u3001\u4ea4\u4e92\u5f0f\u548c\u8fde\u7eed\u7684\u4e16\u754c\uff0c\u652f\u6301\u952e\u76d8\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u53c2\u6570\u8fc7\u5927\u3001\u63a8\u7406\u6b65\u9aa4\u957f\u3001\u5386\u53f2\u4e0a\u4e0b\u6587\u589e\u957f\u5feb\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u751f\u6210\u4ea4\u4e92\u5f0f\u53ef\u63a2\u7d22\u4e16\u754c\u7684\u65b9\u6cd5\u9762\u4e34\u53c2\u6570\u89c4\u6a21\u8fc7\u5927\u3001\u4f9d\u8d56\u5197\u957f\u63a8\u7406\u6b65\u9aa4\u3001\u5386\u53f2\u4e0a\u4e0b\u6587\u5feb\u901f\u589e\u957f\u7b49\u5173\u952e\u6311\u6218\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u4e14\u7f3a\u4e4f\u6587\u672c\u63a7\u5236\u751f\u6210\u80fd\u529b\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u96c6\u6210\u7edf\u4e00\u4e0a\u4e0b\u6587\u538b\u7f29\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u957f\u89c6\u9891\u751f\u6210\u6846\u67b6\uff1b2) \u57fa\u4e8e\u53cc\u5411\u6ce8\u610f\u529b\u84b8\u998f\u548c\u589e\u5f3a\u6587\u672c\u5d4c\u5165\u65b9\u6848\u7684\u5b9e\u65f6\u6d41\u5f0f\u52a0\u901f\u7b56\u7565\uff1b3) \u7528\u4e8e\u751f\u6210\u4e16\u754c\u4e8b\u4ef6\u7684\u6587\u672c\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u903c\u771f\u3001\u4ea4\u4e92\u5f0f\u3001\u8fde\u7eed\u4e16\u754c\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u652f\u6301\u952e\u76d8\u63a2\u7d22\uff0c\u5e76\u5728\u8865\u5145\u6750\u6599\u4e2d\u63d0\u4f9b\u4e86\u4ee3\u7801\u5e93\u3002", "conclusion": "\\method\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4ea4\u4e92\u5f0f\u4e16\u754c\u65f6\u9762\u4e34\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u6587\u672c\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u4ece\u5355\u5f20\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u903c\u771f\u3001\u4ea4\u4e92\u5f0f\u3001\u8fde\u7eed\u4e16\u754c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
