{"id": "2510.02617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02617", "abs": "https://arxiv.org/abs/2510.02617", "authors": ["Beijia Lu", "Ziyi Chen", "Jing Xiao", "Jun-Yan Zhu"], "title": "Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation", "comment": "Project Page: https://beijia11.github.io/IASA", "summary": "Diffusion models can synthesize realistic co-speech video from audio for\nvarious applications, such as video creation and virtual agents. However,\nexisting diffusion-based methods are slow due to numerous denoising steps and\ncostly attention mechanisms, preventing real-time deployment. In this work, we\ndistill a many-step diffusion video model into a few-step student model.\nUnfortunately, directly applying recent diffusion distillation methods degrades\nvideo quality and falls short of real-time performance. To address these\nissues, our new video distillation method leverages input human pose\nconditioning for both attention and loss functions. We first propose using\naccurate correspondence between input human pose keypoints to guide attention\nto relevant regions, such as the speaker's face, hands, and upper body. This\ninput-aware sparse attention reduces redundant computations and strengthens\ntemporal correspondences of body parts, improving inference efficiency and\nmotion coherence. To further enhance visual quality, we introduce an\ninput-aware distillation loss that improves lip synchronization and hand motion\nrealism. By integrating our input-aware sparse attention and distillation loss,\nour method achieves real-time performance with improved visual quality compared\nto recent audio-driven and input-driven methods. We also conduct extensive\nexperiments showing the effectiveness of our algorithmic design choices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u4eba\u4f53\u59ff\u6001\u6761\u4ef6\u5f15\u5bfc\u6ce8\u610f\u529b\u548c\u635f\u5931\u51fd\u6570\uff0c\u5c06\u591a\u6b65\u6269\u6563\u89c6\u9891\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u5e76\u63d0\u5347\u89c6\u89c9\u8d28\u91cf", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u7531\u4e8e\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\u548c\u6602\u8d35\u7684\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u901f\u5ea6\u7f13\u6162\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72", "method": "\u5229\u7528\u8f93\u5165\u4eba\u4f53\u59ff\u6001\u5173\u952e\u70b9\u7684\u51c6\u786e\u5bf9\u5e94\u5173\u7cfb\u6765\u5f15\u5bfc\u6ce8\u610f\u529b\u5230\u76f8\u5173\u533a\u57df\uff0c\u63d0\u51fa\u8f93\u5165\u611f\u77e5\u7a00\u758f\u6ce8\u610f\u529b\u548c\u8f93\u5165\u611f\u77e5\u84b8\u998f\u635f\u5931", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u76f8\u6bd4\u6700\u8fd1\u7684\u97f3\u9891\u9a71\u52a8\u548c\u8f93\u5165\u9a71\u52a8\u65b9\u6cd5\uff0c\u89c6\u89c9\u8d28\u91cf\u5f97\u5230\u6539\u5584", "conclusion": "\u901a\u8fc7\u6574\u5408\u8f93\u5165\u611f\u77e5\u7a00\u758f\u6ce8\u610f\u529b\u548c\u84b8\u998f\u635f\u5931\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387"}}
{"id": "2510.02592", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02592", "abs": "https://arxiv.org/abs/2510.02592", "authors": ["Jean Douglas Carvalho", "Hugo Kenji", "Ahmad Mohammad Saber", "Glaucia Melo", "Max Mauro Dias Santos", "Deepa Kundur"], "title": "Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs", "comment": "This paper has been presented at the 2025 IEEE PES Conference on\n  Innovative Smart Grid Technologies (ISGT 2025)", "summary": "The integration of electric vehicles (EVs) into smart grids presents unique\nopportunities to enhance both transportation systems and energy networks.\nHowever, ensuring safe and interpretable interactions between drivers,\nvehicles, and the surrounding environment remains a critical challenge. This\npaper presents a multi-modal large language model (LLM)-based framework to\nprocess multimodal sensor data - such as object detection, semantic\nsegmentation, and vehicular telemetry - and generate natural-language alerts\nfor drivers. The framework is validated using real-world data collected from\ninstrumented vehicles driving on urban roads, ensuring its applicability to\nreal-world scenarios. By combining visual perception (YOLOv8), geocoded\npositioning, and CAN bus telemetry, the framework bridges raw sensor data and\ndriver comprehension, enabling safer and more informed decision-making in urban\ndriving scenarios. Case studies using real data demonstrate the framework's\neffectiveness in generating context-aware alerts for critical situations, such\nas proximity to pedestrians, cyclists, and other vehicles. This paper\nhighlights the potential of LLMs as assistive tools in e-mobility, benefiting\nboth transportation systems and electric networks by enabling scalable fleet\ncoordination, EV load forecasting, and traffic-aware energy planning.\n  Index Terms - Electric vehicles, visual perception, large language models,\nYOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5904\u7406\u89c6\u89c9\u611f\u77e5\u3001\u5b9a\u4f4d\u548c\u8f66\u8f86\u6570\u636e\uff0c\u4e3a\u9a7e\u9a76\u5458\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8b66\u62a5\uff0c\u63d0\u5347\u7535\u52a8\u6c7d\u8f66\u5728\u57ce\u5e02\u9a7e\u9a76\u4e2d\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u4e0e\u667a\u80fd\u7535\u7f51\u7684\u96c6\u6210\u9700\u8981\u786e\u4fdd\u9a7e\u9a76\u5458\u3001\u8f66\u8f86\u4e0e\u73af\u5883\u4e4b\u95f4\u5b89\u5168\u53ef\u89e3\u91ca\u7684\u4ea4\u4e92\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u548c\u81ea\u7136\u8bed\u8a00\u8b66\u62a5\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408YOLOv8\u89c6\u89c9\u611f\u77e5\u3001\u5730\u7406\u7f16\u7801\u5b9a\u4f4d\u548cCAN\u603b\u7ebf\u9065\u6d4b\u6570\u636e\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u9a7e\u9a76\u5458\u8b66\u62a5\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u6709\u6548\u751f\u6210\u9488\u5bf9\u884c\u4eba\u3001\u81ea\u884c\u8f66\u548c\u5176\u4ed6\u8f66\u8f86\u63a5\u8fd1\u7b49\u5173\u952e\u60c5\u5883\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8b66\u62a5\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u52a8\u51fa\u884c\u4e2d\u5177\u6709\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u8f66\u961f\u534f\u8c03\u3001\u7535\u52a8\u6c7d\u8f66\u8d1f\u8f7d\u9884\u6d4b\u548c\u4ea4\u901a\u611f\u77e5\u80fd\u6e90\u89c4\u5212\uff0c\u540c\u65f6\u60e0\u53ca\u4ea4\u901a\u7cfb\u7edf\u548c\u7535\u7f51\u3002"}}
{"id": "2510.02631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02631", "abs": "https://arxiv.org/abs/2510.02631", "authors": ["Victor Enescu", "Hichem Sahbi"], "title": "Deep Generative Continual Learning using Functional LoRA: FunLoRA", "comment": null, "summary": "Continual adaptation of deep generative models holds tremendous potential and\ncritical importance, given their rapid and expanding usage in text and vision\nbased applications. Incremental training, however, remains highly challenging\ndue to catastrophic forgetting phenomenon, which makes it difficult for neural\nnetworks to effectively incorporate new knowledge. A common strategy consists\nin retraining the generative model on its own synthetic data in order to\nmitigate forgetting. Yet, such an approach faces two major limitations: (i) the\ncontinually increasing training time eventually becomes intractable, and (ii)\nreliance on synthetic data inevitably leads to long-term performance\ndegradation, since synthetic samples lack the richness of real training data.\nIn this paper, we attenuate these issues by designing a novel and more\nexpressive conditioning mechanism for generative models based on low rank\nadaptation (LoRA), that exclusively employs rank 1 matrices, whose\nreparametrized matrix rank is functionally increased using carefully selected\nfunctions -- and dubbed functional LoRA: FunLoRA. Using this dynamic\nconditioning, the generative model is guaranteed to avoid catastrophic\nforgetting and needs only to be trained on data from the current task.\nExtensive experiments using flow-matching based models trained from scratch,\nshowcase that our proposed parameter-efficient fine-tuning (PEFT) method\nsurpasses prior state-of-the-art results based on diffusion models, reaching\nhigher classification accuracy scores, while only requiring a fraction of the\nmemory cost and sampling time.", "AI": {"tldr": "\u63d0\u51faFunLoRA\u65b9\u6cd5\uff0c\u4f7f\u7528\u79e91\u77e9\u9635\u548c\u51fd\u6570\u589e\u5f3a\u7684\u52a8\u6001\u6761\u4ef6\u673a\u5236\u6765\u89e3\u51b3\u751f\u6210\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u5408\u6210\u6570\u636e\u91cd\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u8bad\u7ec3\u65f6\u95f4\u968f\u4efb\u52a1\u589e\u52a0\u800c\u4e0d\u53ef\u63a7\uff0c\u4ee5\u53ca\u4f9d\u8d56\u5408\u6210\u6570\u636e\u5bfc\u81f4\u6027\u80fd\u957f\u671f\u9000\u5316\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94(LoRA)\u7684\u65b0\u578b\u6761\u4ef6\u673a\u5236\uff0c\u4e13\u95e8\u4f7f\u7528\u79e91\u77e9\u9635\uff0c\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u7684\u51fd\u6570\u589e\u5f3a\u91cd\u53c2\u6570\u5316\u77e9\u9635\u7684\u79e9\uff0c\u79f0\u4e3aFunLoRA\u3002", "result": "\u5728\u4ece\u5934\u8bad\u7ec3\u7684\u6d41\u5339\u914d\u6a21\u578b\u4e2d\uff0cFunLoRA\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fbe\u5230\u66f4\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u53ea\u9700\u8981\u4e00\u5c0f\u90e8\u5206\u5185\u5b58\u6210\u672c\u548c\u91c7\u6837\u65f6\u95f4\u3002", "conclusion": "FunLoRA\u901a\u8fc7\u52a8\u6001\u6761\u4ef6\u673a\u5236\u4fdd\u8bc1\u751f\u6210\u6a21\u578b\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u53ea\u9700\u5728\u5f53\u524d\u4efb\u52a1\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002"}}
{"id": "2510.02655", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02655", "abs": "https://arxiv.org/abs/2510.02655", "authors": ["Daniel G. Schwartz"], "title": "A Concept of Possibility for Real-World Events", "comment": null, "summary": "This paper offers a new concept of {\\it possibility} as an alternative to the\nnow-a-days standard concept originally introduced by L.A. Zadeh in 1978. This\nnew version was inspired by the original but, formally, has nothing in common\nwith it other than that they both adopt the {\\L}ukasiewicz multivalent\ninterpretation of the logical connectives. Moreover, rather than seeking to\nprovide a general notion of possibility, this focuses specifically on the\npossibility of a real-world event. An event is viewed as having prerequisites\nthat enable its occurrence and constraints that may impede its occurrence, and\nthe possibility of the event is computed as a function of the probabilities\nthat the prerequisites hold and the constraints do not. This version of\npossibility might appropriately be applied to problems of planning. When there\nare multiple plans available for achieving a goal, this theory can be used to\ndetermine which plan is most possible, i.e., easiest or most feasible to\ncomplete. It is speculated that this model of reasoning correctly captures\nnormal human reasoning about plans. The theory is elaborated and an\nillustrative example for vehicle route planning is provided. There is also a\nsuggestion of potential future applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u80fd\u6027\u6982\u5ff5\u4f5c\u4e3aZadeh\u4f20\u7edf\u53ef\u80fd\u6027\u7406\u8bba\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e13\u6ce8\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u7684\u53ef\u80fd\u6027\u8ba1\u7b97\uff0c\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5148\u51b3\u6761\u4ef6\u548c\u7ea6\u675f\u6761\u4ef6\u3002", "motivation": "\u66ff\u4ee31978\u5e74Zadeh\u63d0\u51fa\u7684\u6807\u51c6\u53ef\u80fd\u6027\u6982\u5ff5\uff0c\u4e13\u6ce8\u4e8e\u73b0\u5b9e\u4e8b\u4ef6\u7684\u53ef\u80fd\u6027\u8bc4\u4f30\uff0c\u7279\u522b\u9002\u7528\u4e8e\u89c4\u5212\u95ee\u9898\u3002", "method": "\u5c06\u4e8b\u4ef6\u89c6\u4e3a\u5177\u6709\u4fc3\u6210\u5176\u53d1\u751f\u7684\u5148\u51b3\u6761\u4ef6\u548c\u963b\u788d\u5176\u53d1\u751f\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u53ef\u80fd\u6027\u8ba1\u7b97\u57fa\u4e8e\u5148\u51b3\u6761\u4ef6\u6210\u7acb\u6982\u7387\u548c\u7ea6\u675f\u6761\u4ef6\u4e0d\u6210\u7acb\u6982\u7387\u7684\u51fd\u6570\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u80fd\u6027\u7406\u8bba\uff0c\u53ef\u7528\u4e8e\u89c4\u5212\u95ee\u9898\u4e2d\u8bc4\u4f30\u4e0d\u540c\u8ba1\u5212\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u8f66\u8f86\u8def\u7ebf\u89c4\u5212\u793a\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "conclusion": "\u8fd9\u79cd\u53ef\u80fd\u6027\u6a21\u578b\u53ef\u80fd\u6b63\u786e\u6355\u6349\u4e86\u4eba\u7c7b\u5173\u4e8e\u8ba1\u5212\u63a8\u7406\u7684\u6b63\u5e38\u601d\u7ef4\u65b9\u5f0f\uff0c\u5e76\u5177\u6709\u672a\u6765\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.02778", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02778", "abs": "https://arxiv.org/abs/2510.02778", "authors": ["Xian Zhang", "Zexi Wu", "Zinuo Li", "Hongming Xu", "Luqi Gong", "Farid Boussaid", "Naoufel Werghi", "Mohammed Bennamoun"], "title": "AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding", "comment": null, "summary": "Understanding long-form videos remains a significant challenge for\nvision--language models (VLMs) due to their extensive temporal length and high\ninformation density. Most current multimodal large language models (MLLMs) rely\non uniform sampling, which often overlooks critical moments, leading to\nincorrect responses to queries. In parallel, many keyframe selection approaches\nimpose rigid temporal spacing: once a frame is chosen, an exclusion window\nsuppresses adjacent timestamps to reduce redundancy. While effective at\nlimiting overlap, this strategy frequently misses short, fine-grained cues near\nimportant events. Other methods instead emphasize visual diversity but neglect\nquery relevance. We propose AdaRD-Key, a training-free keyframe sampling module\nfor query-driven long-form video understanding. AdaRD-Key maximizes a unified\nRelevance--Diversity Max-Volume (RD-MV) objective, combining a\nquery-conditioned relevance score with a log-determinant diversity component to\nyield informative yet non-redundant frames. To handle broad queries with weak\nalignment to the video, AdaRD-Key employs a lightweight relevance-aware gating\nmechanism; when the relevance distribution indicates weak alignment, the method\nseamlessly shifts into a diversity-only mode, enhancing coverage without\nadditional supervision. Our pipeline is training-free, computationally\nefficient (running in real time on a single GPU), and compatible with existing\nVLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and\nVideo-MME demonstrate state-of-the-art performance, particularly on long-form\nvideos. Code available at https://github.com/Xian867/AdaRD-Key.", "AI": {"tldr": "AdaRD-Key\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5173\u952e\u5e27\u91c7\u6837\u6a21\u5757\uff0c\u901a\u8fc7\u6700\u5927\u5316\u76f8\u5173\u6027-\u591a\u6837\u6027\u76ee\u6807\u6765\u63d0\u5347\u957f\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u5728\u5f31\u5bf9\u9f50\u60c5\u51b5\u4e0b\u80fd\u81ea\u52a8\u5207\u6362\u5230\u591a\u6837\u6027\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5747\u5300\u91c7\u6837\u4f1a\u5ffd\u7565\u5173\u952e\u65f6\u523b\uff0c\u800c\u73b0\u6709\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u4e25\u683c\u7684\u65f6\u95f4\u95f4\u9694\u9519\u8fc7\u7ec6\u7c92\u5ea6\u7ebf\u7d22\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u89c6\u89c9\u591a\u6837\u6027\u800c\u5ffd\u7565\u67e5\u8be2\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u76f8\u5173\u6027-\u591a\u6837\u6027\u6700\u5927\u4f53\u79ef\u76ee\u6807\uff0c\u7ed3\u5408\u67e5\u8be2\u6761\u4ef6\u76f8\u5173\u6027\u8bc4\u5206\u548c\u5bf9\u6570\u884c\u5217\u5f0f\u591a\u6837\u6027\u7ec4\u4ef6\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u76f8\u5173\u6027\u611f\u77e5\u95e8\u63a7\u673a\u5236\u5904\u7406\u5f31\u5bf9\u9f50\u60c5\u51b5\u3002", "result": "\u5728LongVideoBench\u548cVideo-MME\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u957f\u89c6\u9891\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u53ef\u5728\u5355\u4e2aGPU\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "AdaRD-Key\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u9ad8\u6548\u5173\u952e\u5e27\u91c7\u6837\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.02815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02815", "abs": "https://arxiv.org/abs/2510.02815", "authors": ["Feng Yuan", "Yifan Gao", "Yuehua Ye", "Haoyue Li", "Xin Gao"], "title": "Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis", "comment": "ICLR2026 under review", "summary": "Cross-modal medical image synthesis research focuses on reconstructing\nmissing imaging modalities from available ones to support clinical diagnosis.\nDriven by clinical necessities for flexible modality reconstruction, we explore\nK to N medical generation, where three critical challenges emerge: How can we\nmodel the heterogeneous contributions of different modalities to various target\ntasks? How can we ensure fusion quality control to prevent degradation from\nnoisy information? How can we maintain modality identity consistency in\nmulti-output generation? Driven by these clinical necessities, and drawing\ninspiration from SAM2's sequential frame paradigm and clinicians' progressive\nworkflow of incrementally adding and selectively integrating multi-modal\ninformation, we treat multi-modal medical data as sequential frames with\nquality-driven selection mechanisms. Our key idea is to \"learn\" adaptive\nweights for each modality-task pair and \"memorize\" beneficial fusion patterns\nthrough progressive enhancement. To achieve this, we design three collaborative\nmodules: PreWeightNet for global contribution assessment, ThresholdNet for\nadaptive filtering, and EffiWeightNet for effective weight computation.\nMeanwhile, to maintain modality identity consistency, we propose the Causal\nModality Identity Module (CMIM) that establishes causal constraints between\ngenerated images and target modality descriptions using vision-language\nmodeling. Extensive experimental results demonstrate that our proposed Med-K2N\noutperforms state-of-the-art methods by significant margins on multiple\nbenchmarks. Source code is available.", "AI": {"tldr": "\u63d0\u51faMed-K2N\u65b9\u6cd5\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u591a\u6a21\u6001\u5408\u6210\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5f02\u8d28\u6a21\u6001\u8d21\u732e\u5efa\u6a21\u3001\u878d\u5408\u8d28\u91cf\u63a7\u5236\u548c\u6a21\u6001\u8eab\u4efd\u4e00\u81f4\u6027\u4fdd\u6301\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u4f5c\u6a21\u5757\u548c\u56e0\u679c\u6a21\u6001\u8eab\u4efd\u6a21\u5757\u5b9e\u73b0K\u5230N\u7684\u7075\u6d3b\u6a21\u6001\u91cd\u5efa\u3002", "motivation": "\u4e34\u5e8a\u8bca\u65ad\u9700\u8981\u7075\u6d3b\u7684\u591a\u6a21\u6001\u91cd\u5efa\u80fd\u529b\uff0c\u4f46\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u4e0d\u540c\u6a21\u6001\u5bf9\u76ee\u6807\u4efb\u52a1\u7684\u5f02\u8d28\u8d21\u732e\u5982\u4f55\u5efa\u6a21\uff1f\u5982\u4f55\u63a7\u5236\u878d\u5408\u8d28\u91cf\u9632\u6b62\u566a\u58f0\u4fe1\u606f\u9000\u5316\uff1f\u5982\u4f55\u5728\u591a\u8f93\u51fa\u751f\u6210\u4e2d\u4fdd\u6301\u6a21\u6001\u8eab\u4efd\u4e00\u81f4\u6027\uff1f", "method": "\u5c06\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u89c6\u4e3a\u5e8f\u5217\u5e27\uff0c\u8bbe\u8ba1\u4e09\u4e2a\u534f\u4f5c\u6a21\u5757\uff1aPreWeightNet\u8fdb\u884c\u5168\u5c40\u8d21\u732e\u8bc4\u4f30\uff0cThresholdNet\u8fdb\u884c\u81ea\u9002\u5e94\u8fc7\u6ee4\uff0cEffiWeightNet\u8ba1\u7b97\u6709\u6548\u6743\u91cd\uff1b\u540c\u65f6\u63d0\u51fa\u56e0\u679c\u6a21\u6001\u8eab\u4efd\u6a21\u5757(CMIM)\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u5efa\u7acb\u751f\u6210\u56fe\u50cf\u4e0e\u76ee\u6807\u6a21\u6001\u63cf\u8ff0\u95f4\u7684\u56e0\u679c\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMed-K2N\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Med-K2N\u65b9\u6cd5\u901a\u8fc7\u6e10\u8fdb\u589e\u5f3a\u548c\u56e0\u679c\u7ea6\u675f\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u591a\u6a21\u6001\u5408\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6a21\u6001\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2510.03049", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03049", "abs": "https://arxiv.org/abs/2510.03049", "authors": ["Ruotong Liao", "Guowen Huang", "Qing Cheng", "Thomas Seidl", "Daniel Cremers", "Volker Tresp"], "title": "When and Where do Events Switch in Multi-Event Video Generation?", "comment": "Work in Progress. Accepted to ICCV2025 @ LongVid-Foundations", "summary": "Text-to-video (T2V) generation has surged in response to challenging\nquestions, especially when a long video must depict multiple sequential events\nwith temporal coherence and controllable content. Existing methods that extend\nto multi-event generation omit an inspection of the intrinsic factor in event\nshifting. The paper aims to answer the central question: When and where\nmulti-event prompts control event transition during T2V generation. This work\nintroduces MEve, a self-curated prompt suite for evaluating multi-event\ntext-to-video (T2V) generation, and conducts a systematic study of two\nrepresentative model families, i.e., OpenSora and CogVideoX. Extensive\nexperiments demonstrate the importance of early intervention in denoising steps\nand block-wise model layers, revealing the essential factor for multi-event\nvideo generation and highlighting the possibilities for multi-event\nconditioning in future models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u4e8b\u4ef6\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e8b\u4ef6\u8f6c\u6362\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MEve\u8bc4\u4f30\u5957\u4ef6\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86OpenSora\u548cCogVideoX\u6a21\u578b\uff0c\u53d1\u73b0\u65e9\u671f\u5e72\u9884\u53bb\u566a\u6b65\u9aa4\u548c\u5757\u7ea7\u6a21\u578b\u5c42\u5bf9\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u5c55\u5230\u591a\u4e8b\u4ef6\u751f\u6210\u65f6\u5ffd\u7565\u4e86\u4e8b\u4ef6\u8f6c\u6362\u7684\u5185\u5728\u56e0\u7d20\uff0c\u672c\u6587\u65e8\u5728\u56de\u7b54\u591a\u4e8b\u4ef6\u63d0\u793a\u4f55\u65f6\u4f55\u5730\u63a7\u5236\u4e8b\u4ef6\u8f6c\u6362\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u5f15\u5165MEve\u81ea\u5efa\u63d0\u793a\u5957\u4ef6\u8bc4\u4f30\u591a\u4e8b\u4ef6\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u5bf9OpenSora\u548cCogVideoX\u4e24\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u5bb6\u65cf\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5728\u53bb\u566a\u6b65\u9aa4\u65e9\u671f\u5e72\u9884\u548c\u5757\u7ea7\u6a21\u578b\u5c42\u7684\u91cd\u8981\u6027\uff0c\u63ed\u793a\u4e86\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u7a81\u51fa\u4e86\u672a\u6765\u6a21\u578b\u4e2d\u591a\u4e8b\u4ef6\u8c03\u8282\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u591a\u4e8b\u4ef6\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.02809", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02809", "abs": "https://arxiv.org/abs/2510.02809", "authors": ["Th\u00e9o Dupuy", "Binbin Xu", "St\u00e9phane Perrey", "Jacky Montmain", "Abdelhak Imoussaten"], "title": "Relevance-Aware Thresholding in Online Conformal Prediction for Time Series", "comment": null, "summary": "Uncertainty quantification has received considerable interest in recent works\nin Machine Learning. In particular, Conformal Prediction (CP) gains ground in\nthis field. For the case of time series, Online Conformal Prediction (OCP)\nbecomes an option to address the problem of data distribution shift over time.\nIndeed, the idea of OCP is to update a threshold of some quantity (whether the\nmiscoverage level or the quantile) based on the distribution observation. To\nevaluate the performance of OCP methods, two key aspects are typically\nconsidered: the coverage validity and the prediction interval width\nminimization. Recently, new OCP methods have emerged, offering long-run\ncoverage guarantees and producing more informative intervals. However, during\nthe threshold update step, most of these methods focus solely on the validity\nof the prediction intervals~--~that is, whether the ground truth falls inside\nor outside the interval~--~without accounting for their relevance. In this\npaper, we aim to leverage this overlooked aspect. Specifically, we propose\nenhancing the threshold update step by replacing the binary evaluation\n(inside/outside) with a broader class of functions that quantify the relevance\nof the prediction interval using the ground truth. This approach helps prevent\nabrupt threshold changes, potentially resulting in narrower prediction\nintervals. Indeed, experimental results on real-world datasets suggest that\nthese functions can produce tighter intervals compared to existing OCP methods\nwhile maintaining coverage validity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u66f4\u5e7f\u6cdb\u7684\u51fd\u6570\u6765\u91cf\u5316\u9884\u6d4b\u533a\u95f4\u7684\u76f8\u5173\u6027\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u4e8c\u5143\u8bc4\u4f30\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u7a84\u7684\u9884\u6d4b\u533a\u95f4\u540c\u65f6\u4fdd\u6301\u8986\u76d6\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u9608\u503c\u66f4\u65b0\u6b65\u9aa4\u4e2d\u53ea\u5173\u6ce8\u9884\u6d4b\u533a\u95f4\u7684\u6709\u6548\u6027\uff08\u771f\u5b9e\u503c\u662f\u5426\u5728\u533a\u95f4\u5185\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u533a\u95f4\u7684\u76f8\u5173\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u8fd9\u4e2a\u88ab\u5ffd\u89c6\u7684\u65b9\u9762\u6765\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5728\u9608\u503c\u66f4\u65b0\u6b65\u9aa4\u4e2d\u7528\u66f4\u5e7f\u6cdb\u7684\u51fd\u6570\u7c7b\u66ff\u4ee3\u4e8c\u5143\u8bc4\u4f30\uff08\u5185\u90e8/\u5916\u90e8\uff09\uff0c\u8fd9\u4e9b\u51fd\u6570\u4f7f\u7528\u771f\u5b9e\u503c\u6765\u91cf\u5316\u9884\u6d4b\u533a\u95f4\u7684\u76f8\u5173\u6027\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u9608\u503c\u7a81\u53d8\uff0c\u53ef\u80fd\u4ea7\u751f\u66f4\u7a84\u7684\u9884\u6d4b\u533a\u95f4\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u51fd\u6570\u80fd\u591f\u4ea7\u751f\u6bd4\u73b0\u6709OCP\u65b9\u6cd5\u66f4\u7d27\u7684\u533a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u8986\u76d6\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5316\u9884\u6d4b\u533a\u95f4\u76f8\u5173\u6027\u7684\u51fd\u6570\u6765\u589e\u5f3a\u9608\u503c\u66f4\u65b0\u6b65\u9aa4\uff0c\u53ef\u4ee5\u4ea7\u751f\u66f4\u7a84\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u540c\u65f6\u7ef4\u6301\u8986\u76d6\u4fdd\u8bc1\uff0c\u6539\u8fdb\u4e86\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.02712", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02712", "abs": "https://arxiv.org/abs/2510.02712", "authors": ["Yubo Li", "Ramayya Krishnan", "Rema Padman"], "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized conversational AI, yet their\nrobustness in extended multi-turn dialogues remains poorly understood. Existing\nevaluation frameworks focus on static benchmarks and single-turn assessments,\nfailing to capture the temporal dynamics of conversational degradation that\ncharacterize real-world interactions. In this work, we present the first\ncomprehensive survival analysis of conversational AI robustness, analyzing\n36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a\ntime-to-event process. Our survival modeling framework-employing Cox\nproportional hazards, Accelerated Failure Time, and Random Survival Forest\napproaches-reveals extraordinary temporal dynamics. We find that abrupt,\nprompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing\nthe hazard of conversational failure. In stark contrast, gradual, cumulative\ndrift is highly protective, vastly reducing the failure hazard and enabling\nsignificantly longer dialogues. AFT models with interactions demonstrate\nsuperior performance, achieving excellent discrimination and exceptional\ncalibration. These findings establish survival analysis as a powerful paradigm\nfor evaluating LLM robustness, offer concrete insights for designing resilient\nconversational agents, and challenge prevailing assumptions about the necessity\nof semantic consistency in conversational AI Systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u751f\u5b58\u5206\u6790\u5e94\u7528\u4e8e\u5bf9\u8bddAI\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u5206\u679036,951\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u53d1\u73b0\uff1a\u7a81\u53d1\u8bed\u4e49\u6f02\u79fb\u4f1a\u707e\u96be\u6027\u589e\u52a0\u5bf9\u8bdd\u5931\u8d25\u98ce\u9669\uff0c\u800c\u6e10\u8fdb\u8bed\u4e49\u6f02\u79fb\u5219\u663e\u8457\u964d\u4f4e\u5931\u8d25\u98ce\u9669\u5e76\u5ef6\u957f\u5bf9\u8bdd\u5bff\u547d\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u57fa\u51c6\u548c\u5355\u8f6e\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u9000\u5316\u7279\u5f81\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u7406\u89e3LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u751f\u5b58\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ecCox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u3001\u52a0\u901f\u5931\u6548\u65f6\u95f4\u6a21\u578b\u548c\u968f\u673a\u751f\u5b58\u68ee\u6797\u65b9\u6cd5\uff0c\u5bf99\u4e2a\u6700\u5148\u8fdbLLM\u7684\u5bf9\u8bdd\u6570\u636e\u8fdb\u884c\u5efa\u6a21\uff0c\u5c06\u5931\u8d25\u89c6\u4e3a\u65f6\u95f4\u5230\u4e8b\u4ef6\u7684\u8fc7\u7a0b\u3002", "result": "AFT\u4ea4\u4e92\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5177\u6709\u4f18\u5f02\u7684\u533a\u5206\u5ea6\u548c\u6821\u51c6\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u7a81\u53d1\u8bed\u4e49\u6f02\u79fb\u4f1a\u6025\u5267\u589e\u52a0\u5bf9\u8bdd\u5931\u8d25\u98ce\u9669\uff0c\u800c\u6e10\u8fdb\u8bed\u4e49\u6f02\u79fb\u5219\u5177\u6709\u4fdd\u62a4\u4f5c\u7528\uff0c\u80fd\u663e\u8457\u5ef6\u957f\u5bf9\u8bdd\u6301\u7eed\u65f6\u95f4\u3002", "conclusion": "\u751f\u5b58\u5206\u6790\u662f\u8bc4\u4f30LLM\u9c81\u68d2\u6027\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u4e3a\u8bbe\u8ba1\u5f39\u6027\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u5177\u4f53\u89c1\u89e3\uff0c\u5e76\u6311\u6218\u4e86\u5bf9\u8bddAI\u7cfb\u7edf\u4e2d\u8bed\u4e49\u4e00\u81f4\u6027\u5fc5\u8981\u6027\u7684\u666e\u904d\u5047\u8bbe\u3002"}}
{"id": "2510.02967", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.02967", "abs": "https://arxiv.org/abs/2510.02967", "authors": ["Matthew Lewis", "Samuel Thio", "Richard JB Dobson", "Spiros Denaxas"], "title": "Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines", "comment": null, "summary": "This paper presents the development and evaluation of a Retrieval-Augmented\nGeneration (RAG) system for querying the United Kingdom's National Institute\nfor Health and Care Excellence (NICE) clinical guidelines using Large Language\nModels (LLMs). The extensive length and volume of these guidelines can impede\ntheir utilisation within a time-constrained healthcare system, a challenge this\nproject addresses through the creation of a system capable of providing users\nwith precisely matched information in response to natural language queries. The\nsystem's retrieval architecture, composed of a hybrid embedding mechanism, was\nevaluated against a database of 10,195 text chunks derived from three hundred\nguidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)\nof 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten\nretrieved chunks, when evaluated on 7901 queries.\n  The most significant impact of the RAG system was observed during the\ngeneration phase. When evaluated on a manually curated dataset of seventy\nquestion-answer pairs, RAG-enhanced models showed substantial gains in\nperformance. Faithfulness, the measure of whether an answer is supported by the\nsource text, was increased by 64.7 percentage points to 99.5% for the\nRAG-enhanced O4-Mini model and significantly outperformed the medical-focused\nMeditron3-8B LLM, which scored 43%. This, combined with a perfect Context\nPrecision score of 1 for all RAG-enhanced models, confirms the system's ability\nto prevent information fabrication by grounding its answers in relevant source\nmaterial. This study thus establishes RAG as an effective, reliable, and\nscalable approach for applying generative AI in healthcare, enabling\ncost-effective access to medical guidelines.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u67e5\u8be2\u82f1\u56fdNICE\u4e34\u5e8a\u6307\u5357\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u6df7\u5408\u5d4c\u5165\u673a\u5236\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56de\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "NICE\u4e34\u5e8a\u6307\u5357\u7bc7\u5e45\u5197\u957f\u3001\u6570\u91cf\u5e9e\u5927\uff0c\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u533b\u7597\u73af\u5883\u4e2d\u96be\u4ee5\u6709\u6548\u5229\u7528\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5feb\u901f\u63d0\u4f9b\u7cbe\u786e\u4fe1\u606f\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u67b6\u6784\uff0c\u7ed3\u5408\u6df7\u5408\u5d4c\u5165\u673a\u5236\uff0c\u4ece10,195\u4e2a\u6587\u672c\u5757\u4e2d\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\uff0c\u4f7f\u7528LLMs\u751f\u6210\u56de\u7b54\u3002", "result": "\u7cfb\u7edf\u68c0\u7d22\u6027\u80fd\u4f18\u5f02\uff1a\u5e73\u5747\u5012\u6570\u6392\u540d0.814\uff0c\u9996\u5757\u53ec\u56de\u738781%\uff0c\u524d10\u5757\u53ec\u56de\u738799.1%\uff1bRAG\u589e\u5f3a\u6a21\u578b\u5728\u5fe0\u5b9e\u5ea6\u65b9\u9762\u63d0\u534764.7\u4e2a\u767e\u5206\u70b9\u81f399.5%\uff0c\u663e\u8457\u4f18\u4e8e\u533b\u7597\u4e13\u7528\u6a21\u578bMeditron3-8B(43%)\u3002", "conclusion": "RAG\u662f\u533b\u7597\u9886\u57df\u5e94\u7528\u751f\u6210\u5f0fAI\u7684\u6709\u6548\u3001\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u5f0f\u63d0\u4f9b\u533b\u7597\u6307\u5357\u8bbf\u95ee\u3002"}}
{"id": "2510.03162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03162", "abs": "https://arxiv.org/abs/2510.03162", "authors": ["Ha Manh Bui", "Iliana Maifeld-Carucci", "Anqi Liu"], "title": "Calibrated Uncertainty Sampling for Active Learning", "comment": null, "summary": "We study the problem of actively learning a classifier with a low calibration\nerror. One of the most popular Acquisition Functions (AFs) in pool-based Active\nLearning (AL) is querying by the model's uncertainty. However, we recognize\nthat an uncalibrated uncertainty model on the unlabeled pool may significantly\naffect the AF effectiveness, leading to sub-optimal generalization and high\ncalibration error on unseen data. Deep Neural Networks (DNNs) make it even\nworse as the model uncertainty from DNN is usually uncalibrated. Therefore, we\npropose a new AF by estimating calibration errors and query samples with the\nhighest calibration error before leveraging DNN uncertainty. Specifically, we\nutilize a kernel calibration error estimator under the covariate shift and\nformally show that AL with this AF eventually leads to a bounded calibration\nerror on the unlabeled pool and unseen test data. Empirically, our proposed\nmethod surpasses other AF baselines by having a lower calibration and\ngeneralization error across pool-based AL settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6821\u51c6\u8bef\u5dee\u4f30\u8ba1\u7684\u4e3b\u52a8\u5b66\u4e60\u91c7\u96c6\u51fd\u6570\uff0c\u901a\u8fc7\u67e5\u8be2\u6821\u51c6\u8bef\u5dee\u6700\u9ad8\u7684\u6837\u672c\u6765\u6539\u8fdb\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u4ece\u800c\u964d\u4f4e\u6cdb\u5316\u8bef\u5dee\u548c\u6821\u51c6\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u901a\u5e38\u672a\u6821\u51c6\uff0c\u8fd9\u4f1a\u5f71\u54cd\u91c7\u96c6\u51fd\u6570\u7684\u6709\u6548\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u5dee\u548c\u6821\u51c6\u8bef\u5dee\u9ad8\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u91c7\u96c6\u51fd\u6570\uff0c\u5728\u5229\u7528DNN\u4e0d\u786e\u5b9a\u6027\u4e4b\u524d\u5148\u4f30\u8ba1\u6821\u51c6\u8bef\u5dee\u5e76\u67e5\u8be2\u6821\u51c6\u8bef\u5dee\u6700\u9ad8\u7684\u6837\u672c\u3002\u4f7f\u7528\u6838\u6821\u51c6\u8bef\u5dee\u4f30\u8ba1\u5668\u5904\u7406\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u4fdd\u8bc1\u672a\u6807\u6ce8\u6c60\u548c\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u6821\u51c6\u8bef\u5dee\u6709\u754c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6c60\u5f0f\u4e3b\u52a8\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5176\u4ed6\u91c7\u96c6\u51fd\u6570\u57fa\u7ebf\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u6821\u51c6\u8bef\u5dee\u548c\u6cdb\u5316\u8bef\u5dee\u3002", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u6821\u51c6\u8bef\u5dee\u9ad8\u7684\u6837\u672c\u53ef\u4ee5\u6709\u6548\u6539\u8fdb\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u548c\u6821\u51c6\u6027\u80fd\u3002"}}
{"id": "2510.03197", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03197", "abs": "https://arxiv.org/abs/2510.03197", "authors": ["James Thomas", "Johan Wahlstr\u00f6m"], "title": "Estimation of Resistance Training RPE using Inertial Sensors and Electromyography", "comment": null, "summary": "Accurate estimation of rating of perceived exertion (RPE) can enhance\nresistance training through personalized feedback and injury prevention. This\nstudy investigates the application of machine learning models to estimate RPE\nduring single-arm dumbbell bicep curls, using data from wearable inertial and\nelectromyography (EMG) sensors. A custom dataset of 69 sets and over 1000\nrepetitions was collected, with statistical features extracted for model\ntraining. Among the models evaluated, a random forest classifier achieved the\nhighest performance, with 41.4% exact accuracy and 85.9% $\\pm1$ RPE accuracy.\nWhile the inclusion of EMG data slightly improved model accuracy over inertial\nsensors alone, its utility may have been limited by factors such as data\nquality and placement sensitivity. Feature analysis highlighted eccentric\nrepetition time as the strongest RPE predictor. The results demonstrate the\nfeasibility of wearable-sensor-based RPE estimation and identify key challenges\nfor improving model generalizability.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\uff08\u60ef\u6027\u4f20\u611f\u5668\u548c\u808c\u7535\u56fe\uff09\u6765\u4f30\u8ba1\u5355\u81c2\u54d1\u94c3\u5f2f\u4e3e\u65f6\u7684\u81ea\u89c9\u7528\u529b\u7a0b\u5ea6\u8bc4\u5206\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523041.4%\uff0c\u00b11 RPE\u51c6\u786e\u7387\u4e3a85.9%\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u81ea\u89c9\u7528\u529b\u7a0b\u5ea6\u8bc4\u5206\u53ef\u4ee5\u589e\u5f3a\u6297\u963b\u8bad\u7ec3\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u53cd\u9988\u548c\u9884\u9632\u4f24\u5bb3\u6765\u6539\u5584\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u6536\u96c6\u4e8669\u7ec4\u8d85\u8fc71000\u6b21\u91cd\u590d\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u53ef\u7a7f\u6234\u60ef\u6027\u4f20\u611f\u5668\u548c\u808c\u7535\u56fe\u4f20\u611f\u5668\u6570\u636e\uff0c\u63d0\u53d6\u7edf\u8ba1\u7279\u5f81\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u597d\uff0c\u7cbe\u786e\u51c6\u786e\u7387\u4e3a41.4%\uff0c\u00b11 RPE\u51c6\u786e\u7387\u4e3a85.9%\u3002\u808c\u7535\u56fe\u6570\u636e\u7684\u52a0\u5165\u7565\u5fae\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff0c\u4f46\u53d7\u5230\u6570\u636e\u8d28\u91cf\u548c\u653e\u7f6e\u654f\u611f\u6027\u7684\u9650\u5236\u3002\u79bb\u5fc3\u91cd\u590d\u65f6\u95f4\u662f\u6700\u5f3a\u7684RPE\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684RPE\u4f30\u8ba1\u7684\u53ef\u884c\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u6311\u6218\u3002"}}
