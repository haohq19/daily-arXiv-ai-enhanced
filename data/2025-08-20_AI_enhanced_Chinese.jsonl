{"id": "2508.13319", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13319", "abs": "https://arxiv.org/abs/2508.13319", "authors": ["Kshitij Kavimandan", "Pooja Mangal", "Devanshi Mehta"], "title": "A Surveillance Based Interactive Robot", "comment": "4 pages, 5 figures", "summary": "We build a mobile surveillance robot that streams video in real time and\nresponds to speech so a user can monitor and steer it from a phone or browser.\nThe system uses two Raspberry Pi 4 units: a front unit on a differential drive\nbase with camera, mic, and speaker, and a central unit that serves the live\nfeed and runs perception. Video is sent with FFmpeg. Objects in the scene are\ndetected using YOLOv3 to support navigation and event awareness. For voice\ninteraction, we use Python libraries for speech recognition, multilingual\ntranslation, and text-to-speech, so the robot can take spoken commands and read\nback responses in the requested language. A Kinect RGB-D sensor provides visual\ninput and obstacle cues. In indoor tests the robot detects common objects at\ninteractive frame rates on CPU, recognises commands reliably, and translates\nthem to actions without manual control. The design relies on off-the-shelf\nhardware and open software, making it easy to reproduce. We discuss limits and\npractical extensions, including sensor fusion with ultrasonic range data, GPU\nacceleration, and adding face and text recognition.", "AI": {"tldr": "\u57fa\u4e8e\u6811\u8393\u6d3ePi\u7684\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u652f\u6301\u5b9e\u65f6\u89c6\u9891\u6d41\u3001\u8bed\u97f3\u63a7\u5236\u548c\u591a\u8bed\u8a00\u4ea4\u4e92\uff0c\u901a\u8fc7YOLOv3\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\u548c\u5ba4\u5185\u5bfc\u822a", "motivation": "\u5efa\u7acb\u4e00\u4e2a\u4f7f\u7528\u666e\u901a\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\u7684\u6613\u590d\u73b0\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u652f\u6301\u8fdc\u7a0b\u76d1\u63a7\u548c\u8bed\u97f3\u63a7\u5236", "method": "\u4f7f\u75282\u53f0Raspberry Pi 4\uff08\u524d\u7aef\u548c\u4e2d\u592e\u5355\u5143\uff09\uff0c\u91c7\u7528FFmpeg\u4f20\u8f93\u89c6\u9891\uff0cYOLOv3\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\uff0cPython\u8bed\u97f3\u5e93\u5b9e\u73b0\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\uff0cKinect RGB-D\u4f20\u611f\u5668\u63d0\u4f9b\u89c6\u89c9\u8f93\u5165", "result": "\u5ba4\u5185\u6d4b\u8bd5\u4e2d\u80fd\u591f\u5728CPU\u4e0a\u4ee5\u4ea4\u4e92\u5e27\u7387\u68c0\u6d4b\u5e38\u89c1\u7269\u4f53\uff0c\u53ef\u9760\u8bc6\u522b\u547d\u4ee4\u5e76\u8f6c\u6362\u4e3a\u52a8\u4f5c\uff0c\u65e0\u9700\u624b\u52a8\u63a7\u5236", "conclusion": "\u8bbe\u8ba1\u4f9d\u9760\u5e02\u9762\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\uff0c\u6613\u4e8e\u590d\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4f20\u611f\u5668\u878d\u5408\u3001GPU\u52a0\u901f\u3001\u4eba\u8138\u548c\u6587\u672c\u8bc6\u522b\u7b49\u6269\u5c55\u53ef\u80fd\u6027"}}
{"id": "2508.13219", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13219", "abs": "https://arxiv.org/abs/2508.13219", "authors": ["Su Chen", "Xiaohua Qi", "Xixun Lin", "Yanmin Shang", "Xiaolin Xu", "Yangxi Li"], "title": "Deep Graph Neural Point Process For Learning Temporal Interactive Networks", "comment": null, "summary": "Learning temporal interaction networks(TIN) is previously regarded as a\ncoarse-grained multi-sequence prediction problem, ignoring the network topology\nstructure influence. This paper addresses this limitation and a Deep Graph\nNeural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two\nkey modules: the Node Aggregation Layer and the Self Attentive Layer. The Node\nAggregation Layer captures topological structures to generate static\nrepresentation for users and items, while the Self Attentive Layer dynamically\nupdates embeddings over time. By incorporating both dynamic and static\nembeddings into the event intensity function and optimizing the model via\nmaximum likelihood estimation, DGNPP predicts events and occurrence time\neffectively. Experimental evaluations on three public datasets demonstrate that\nDGNPP achieves superior performance in event prediction and time prediction\ntasks with high efficiency, significantly outperforming baseline models and\neffectively mitigating the limitations of prior approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86DGNPP\u6a21\u578b\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u70b9\u8fc7\u7a0b\u6765\u5b66\u4e60\u65f6\u5e8f\u4ea4\u4e92\u7f51\u7edc\uff0c\u540c\u65f6\u8003\u8651\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u5728\u4e8b\u4ef6\u9884\u6d4b\u548c\u65f6\u95f4\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u7684\u65f6\u5e8f\u4ea4\u4e92\u7f51\u7edc\u5b66\u4e60\u5ffd\u7565\u4e86\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u7684\u5f71\u54cd\uff0c\u5c06\u95ee\u9898\u7b80\u5316\u4e3a\u7c97\u7c92\u5ea6\u7684\u591a\u5e8f\u5217\u9884\u6d4b", "method": "DGNPP\u6a21\u578b\u5305\u542b\u8282\u70b9\u805a\u5408\u5c42\uff08\u6355\u83b7\u62d3\u6251\u7ed3\u6784\u751f\u6210\u9759\u6001\u8868\u793a\uff09\u548c\u81ea\u6ce8\u610f\u529b\u5c42\uff08\u52a8\u6001\u66f4\u65b0\u5d4c\u5165\uff09\uff0c\u5c06\u52a8\u6001\u548c\u9759\u6001\u5d4c\u5165\u7ed3\u5408\u5230\u4e8b\u4ef6\u5f3a\u5ea6\u51fd\u6570\u4e2d\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4f18\u5316", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDGNPP\u5728\u4e8b\u4ef6\u9884\u6d4b\u548c\u65f6\u95f4\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u548c\u9ad8\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "DGNPP\u6709\u6548\u89e3\u51b3\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u6210\u529f\u6574\u5408\u4e86\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u4fe1\u606f"}}
{"id": "2508.13461", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13461", "abs": "https://arxiv.org/abs/2508.13461", "authors": ["Ivan Reyes-Amezcua", "Francisco Lopez-Tiro", "Clement Larose", "Andres Mendez-Vazquez", "Gilberto Ochoa-Ruiz", "Christian Daul"], "title": "Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs", "comment": null, "summary": "Kidney stone classification from endoscopic images is critical for\npersonalized treatment and recurrence prevention. While convolutional neural\nnetworks (CNNs) have shown promise in this task, their limited ability to\ncapture long-range dependencies can hinder performance under variable imaging\nconditions. This study presents a comparative analysis between Vision\nTransformers (ViTs) and CNN-based models, evaluating their performance on two\nex vivo datasets comprising CCD camera and flexible ureteroscope images. The\nViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50\nbaseline across multiple imaging conditions. For instance, in the most visually\ncomplex subset (Section patches from endoscopic images), the ViT model achieved\n95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50.\nIn the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracy\nversus 78.4% with CNN. These improvements extend across precision and recall as\nwell. The results demonstrate that ViT-based architectures provide superior\nclassification performance and offer a scalable alternative to conventional\nCNNs for kidney stone image analysis.", "AI": {"tldr": "\u89c6\u89c9Transformer\u5728\u80be\u7ed3\u77f3\u5185\u955c\u56fe\u50cf\u5206\u7c7b\u4e2d\u663e\u8457\u8d85\u8d8a\u4f20\u7edfCNN\u6a21\u578b\uff0c\u5728\u590d\u6742\u56fe\u50cf\u6761\u4ef6\u4e0b\u51c6\u786e\u7387\u63d0\u534715-30%\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u89e3\u51b3\u65b9\u6848", "motivation": "\u80be\u7ed3\u77f3\u5206\u7c7b\u5bf9\u4e2a\u6027\u5316\u6cbb\u7597\u548c\u590d\u53d1\u9884\u9632\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfCNN\u6a21\u578b\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u65f6\u5b58\u5728\u9650\u5236\uff0c\u5f71\u54cd\u5728\u53d8\u5316\u7684\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u6027\u80fd", "method": "\u8fdb\u884c\u89c6\u89c9Transformer(ViT)\u4e0eCNN\u57fa\u7840\u6a21\u578b(ResNet50)\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u5728\u4e24\u4e2aex vivo\u6570\u636e\u96c6(\u5305\u62ecCCD\u76f8\u673a\u548c\u7f13\u6027\u5c3f\u9053\u955c\u56fe\u50cf)\u4e0a\u8bc4\u4f30\u6027\u80fd\uff0c\u91c7\u7528ImageNet-21k\u9884\u8bad\u7ec3\u7684ViT-base\u6a21\u578b", "result": "ViT\u6a21\u578b\u5728\u5404\u79cd\u6210\u50cf\u6761\u4ef6\u4e0b\u5747\u8d85\u8fc7ResNet50\u57fa\u51c6\u3002\u5728\u6700\u590d\u6742\u7684\u5185\u955c\u56fe\u50cf\u5b50\u96c6\u4e2d\uff0cViT\u8fbe\u523095.2%\u51c6\u786e\u7387\u548c95.1% F1\u5206\u6570(ResNet50\u4ec564.5%\u548c59.3%)\uff1b\u5728\u6df7\u5408\u89c6\u56fe\u5b50\u96c6\u4e2d\uff0cViT\u8fbe\u523087.1%\u51c6\u786e\u7387(ResNet50\u4e3a78.4%)\uff0c\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u4e5f\u90fd\u6709\u663e\u8457\u63d0\u5347", "conclusion": "ViT\u57fa\u7840\u67b6\u6784\u63d0\u4f9b\u4e86\u66f4\u4f18\u79f0\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u80be\u7ed3\u77f3\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u4f20\u7edfCNN\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2508.13406", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13406", "abs": "https://arxiv.org/abs/2508.13406", "authors": ["Nooshin Bahador", "Milad Lankarany"], "title": "Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp", "comment": "23 pages, 7 figures", "summary": "This study presents a quantitative framework for evaluating the spatial\nconcordance between clinically defined seizure onset zones (SOZs) and\nstatistically anomalous channels identified through time-frequency analysis of\nchirp events. The proposed pipeline employs a two-step methodology: (1)\nUnsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with\nadaptive neighborhood selection identifies anomalous channels based on\nspectro-temporal features of chirp (Onset frequency, offset frequency, and\ntemporal duration); and (2) Spatial Correlation Analysis, which computes both\nexact co-occurrence metrics and weighted index similarity, incorporating\nhemispheric congruence and electrode proximity. Key findings demonstrate that\nthe LOF-based approach (N neighbors=20, contamination=0.2) effectively detects\noutliers, with index matching (weighted by channel proximity) outperforming\nexact matching in SOZ localization. Performance metrics (precision, recall, F1)\nwere highest for seizure-free patients (Index Precision mean: 0.903) and those\nwith successful surgical outcomes (Index Precision mean: 0.865), whereas\nfailure cases exhibited lower concordance (Index Precision mean: 0.460). The\nkey takeaway is that chirp-based outlier detection, combined with weighted\nspatial metrics, provides a complementary method for SOZ localization,\nparticularly in patients with successful surgical outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u9891\u5206\u6790chirp\u4e8b\u4ef6\u8bc6\u522b\u7edf\u8ba1\u5f02\u5e38\u901a\u9053\uff0c\u5e76\u8bc4\u4f30\u5176\u4e0e\u4e34\u5e8a\u5b9a\u4e49\u7684\u75af\u75ea\u8d77\u6e90\u533a\u57df\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u8865\u5145\u65b9\u6cd5\u6765\u51c6\u786e\u5b9a\u4f4d\u75af\u75ea\u8d77\u6e90\u533a\u57df\uff0c\u7279\u522b\u662f\u5728\u6240\u6d4b\u75c5\u4eba\u4e2d\u9a8c\u8bc1\u624b\u672f\u6210\u529f\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u65b9\u6cd5\uff1a(1)\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f7f\u7528\u5c40\u90e8\u5f02\u5e38\u56e0\u5b50(LOF)\u5206\u6790\u8bc6\u522b\u5f02\u5e38\u901a\u9053\uff1b(2)\u7a7a\u95f4\u76f8\u5173\u6027\u5206\u6790\uff0c\u8ba1\u7b97\u7cbe\u786e\u5339\u914d\u6307\u6807\u548c\u52a0\u6743\u6307\u6570\u76f8\u4f3c\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u75af\u75ea\u514d\u75c5\u4eba\u7fa4\u4e2d\u8868\u73b0\u6700\u4f73\uff08\u7cbe\u5ea6\u5747\u503c0.903\uff09\uff0c\u6210\u529f\u6240\u6d4b\u75c5\u4eba\u4e5f\u663e\u793a\u826f\u597d\u7ed3\u679c\uff08\u7cbe\u5ea6\u5747\u503c0.865\uff09\uff0c\u800c\u5931\u8d25\u6848\u4f8b\u7684\u4e00\u81f4\u6027\u8f83\u4f4e\uff08\u7cbe\u5ea6\u5747\u503c0.460\uff09\u3002", "conclusion": "chirp\u57fa\u7840\u7684\u5f02\u5e38\u68c0\u6d4b\u7ed3\u5408\u52a0\u6743\u7a7a\u95f4\u6307\u6807\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684SOZ\u5b9a\u4f4d\u8865\u5145\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6240\u6d4b\u6210\u529f\u7684\u75c5\u4eba\u3002"}}
{"id": "2508.13434", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13434", "abs": "https://arxiv.org/abs/2508.13434", "authors": ["Yunfeng Ge", "Ming Jin", "Yiji Zhao", "Hongyan Li", "Bo Du", "Chang Xu", "Shirui Pan"], "title": "EventTSF: Event-Aware Non-Stationary Time Series Forecasting", "comment": "13 pages, 10 figures", "summary": "Time series forecasting plays a vital role in critical domains like energy\nand transportation, where non-stationary dynamics are deeply intertwined with\nevents in other modalities such as texts. However, incorporating natural\nlanguage-based external events to improve non-stationary forecasting remains\nlargely unexplored, as most approaches still rely on a single modality,\nresulting in limited contextual knowledge and model underperformance. Enabling\nfine-grained multimodal interactions between temporal and textual data is\nchallenged by three fundamental issues: (1) the difficulty of fine-grained\nsynchronization between time-varying discrete textual events and continuous\ntime series; (2) the inherent temporal uncertainty introduced by textual\nsemantics; and (3) the misalignment between textual event embeddings and\nmulti-resolution temporal patterns. In this work, we address these challenges\nby introducing event-aware non-stationary time series forecasting (EventTSF),\nan autoregressive generation framework that integrates historical time series\nwith textual events to make subsequent forecasts. Specifically, EventTSF uses\nautoregressive diffusion with flow matching at each step to capture nuanced\ntemporal-event interactions. To handle event-induced uncertainty, flow matching\ntimesteps are adaptively controlled according to event semantic signals. The\nunderlying denoiser employs a multimodal U-shaped diffusion transformer that\nefficiently fuses temporal and textual modalities across different resolutions.\nExtensive experiments on 8 synthetic and real-world datasets show that EventTSF\noutperforms 12 baselines across diverse event-aware non-stationary time series\nforecasting scenarios, achieving substantial improvements of 10.7% higher\nforecasting accuracy and $1.13\\times$ faster training efficiency.", "AI": {"tldr": "EventTSF\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6269\u6563\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u6709\u6548\u6574\u5408\u6587\u672c\u4e8b\u4ef6\u4fe1\u606f\u6765\u63d0\u5347\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u6570\u636e\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6587\u672c\u4e8b\u4ef6\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u975e\u5e73\u7a33\u52a8\u6001\u573a\u666f\u4e0b\u9884\u6d4b\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u4e0e\u6587\u672c\u4e8b\u4ef6\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u540c\u6b65\u3001\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u6a21\u6001\u5bf9\u9f50\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faEventTSF\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u56de\u5f52\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4f7f\u7528\u6d41\u5339\u914d\u6280\u672f\u6355\u6349\u65f6\u95f4-\u4e8b\u4ef6\u4ea4\u4e92\u3002\u901a\u8fc7\u4e8b\u4ef6\u8bed\u4e49\u4fe1\u53f7\u81ea\u9002\u5e94\u63a7\u5236\u6d41\u5339\u914d\u65f6\u95f4\u6b65\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u6001U\u5f62\u6269\u6563transformer\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u6709\u6548\u878d\u5408\u65f6\u95f4\u548c\u6587\u672c\u6a21\u6001\u3002", "result": "\u57288\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEventTSF\u572812\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534710.7%\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u9ad81.13\u500d\u3002", "conclusion": "EventTSF\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u6574\u5408\u6587\u672c\u4e8b\u4ef6\u4fe1\u606f\u5bf9\u63d0\u5347\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u7684\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u8de8\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13587", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13587", "abs": "https://arxiv.org/abs/2508.13587", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Liming Zheng", "Yufeng Zhong", "Lin Ma"], "title": "Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation", "comment": "technical report", "summary": "While reinforcement learning (RL) has proven highly effective for general\nreasoning in vision-language models, its application to tasks requiring\nin-depth understanding of information-rich images and generation of structured\noutputs remains underexplored. Chart-to-code generation exemplifies this\nchallenge, demanding complex reasoning over visual charts to generate\nstructured code. Supervised fine-tuning (SFT) alone is often insufficient,\nhighlighting the need for effective RL strategies that appropriately reward\nstructured outputs. We systematically investigate the performance plateau in\nSFT through large-scale experiments and propose Multimodal Structured\nReinforcement Learning (MSRL) for chart-to-code generation, which substantially\nbreaks through this plateau. We construct the largest training corpus to date,\ncontaining 3 million chart-code pairs from real-world arXiv tables to mitigate\nsimplistic patterns of prior synthetic data. Despite reaching state-of-the-art\nperformance, our experiments show that scaling SFT data eventually hits a\nplateau where further increases yield negligible improvements. Our MSRL method\nleverages a multi-granularity structured reward system using multimodal textual\nand visual feedback. At the textual level, rule-based rewards validate\nfine-grained code details. At the visual level, model-based rewards assess\nstructural similarity by rendering generated code into images and employing an\nevaluator model. We implement this within a two-stage curriculum for training\nstability. Results demonstrate that MSRL significantly breaks the SFT plateau,\nimproving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA\nbenchmarks respectively, achieving competitive performance with advanced\nclosed-source models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u591a\u6a21\u6001\u7ed3\u6784\u5316\u5f3a\u5316\u5b66\u4e60(MSRL)\u5728\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u591a\u7ea7\u5956\u52b1\u7cfb\u7edf\u7a81\u7834\u4e86\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\u5e73\u53f0\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u9700\u8981\u6df1\u5ea6\u7406\u89e3\u4fe1\u606f\u4e30\u5bcc\u56fe\u50cf\u548c\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4efb\u52a1\u4e2d\u5e94\u7528\u4e0d\u8db3\uff0c\u76d1\u7763\u5b66\u4e60\u5355\u72ec\u4f7f\u7528\u5f80\u5f80\u4e0d\u591f\u6709\u6548\uff0c\u9700\u8981\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u5956\u52b1\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u7ed3\u6784\u5316\u5f3a\u5316\u5b66\u4e60(MSRL)\uff0c\u6784\u5efa\u4e86\u5305\u542b300\u4e07\u5bf9\u56fe\u8868-\u4ee3\u7801\u7684\u6700\u5927\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u4f7f\u7528\u591a\u7ea7\u5956\u52b1\u7cfb\u7edf\uff1a\u6587\u672c\u5c42\u9762\u7684\u89c4\u5219\u57fa\u5956\u52b1\u9a8c\u8bc1\u7ec6\u7c92\u5ea6\u4ee3\u7801\u7ec6\u8282\uff0c\u89c6\u89c9\u5c42\u9762\u7684\u6a21\u578b\u57fa\u5956\u52b1\u901a\u8fc7\u6e32\u67d3\u4ee3\u7801\u5230\u56fe\u50cf\u5e76\u4f7f\u7528\u8bc4\u4f30\u5668\u6a21\u578b\u8bc4\u4f30\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u8fdb\u884c\u8bad\u7ec3\u7a33\u5b9a\u6027\u63a7\u5236\u3002", "result": "MSRL\u663e\u8457\u7a81\u7834\u4e86\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\u5e73\u53f0\uff0c\u5728ChartMimic\u548cReachQA\u51c0\u9ad8\u4e0a\u5206\u522b\u63d0\u53476.2%\u548c9.9%\uff0c\u8fbe\u5230\u4e86\u4e0e\u5148\u8fdb\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u7ed3\u6784\u5316\u5f3a\u5316\u5b66\u4e60\u662f\u89e3\u51b3\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u8fd9\u7c7b\u9700\u8981\u590d\u6742\u7406\u89e3\u7684\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u591a\u7ea7\u5956\u52b1\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u5347\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u8d28\u91cf\u3002"}}
{"id": "2508.13548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13548", "abs": "https://arxiv.org/abs/2508.13548", "authors": ["Rituparna Datta", "Jiaming Cui", "Gregory R. Madden", "Anil Vullikanti"], "title": "CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics", "comment": null, "summary": "Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public\nhealth threat within hospitals as well as long-term care facilities. Better\nunderstanding of MRSA risks, evaluation of interventions and forecasting MRSA\nrates are important public health problems. Existing forecasting models rely on\nstatistical or neural network approaches, which lack epidemiological\ninterpretability, and have limited performance. Mechanistic epidemic models are\ndifficult to calibrate and limited in incorporating diverse datasets. We\npresent CALYPSO, a hybrid framework that integrates neural networks with\nmechanistic metapopulation models to capture the spread dynamics of infectious\ndiseases (i.e., MRSA) across healthcare and community settings. Our model\nleverages patient-level insurance claims, commuting data, and healthcare\ntransfer patterns to learn region- and time-specific parameters governing MRSA\nspread. This enables accurate, interpretable forecasts at multiple spatial\nresolutions (county, healthcare facility, region, state) and supports\ncounterfactual analyses of infection control policies and outbreak risks. We\nalso show that CALYPSO improves statewide forecasting performance by over 4.5%\ncompared to machine learning baselines, while also identifying high-risk\nregions and cost-effective strategies for allocating infection prevention\nresources.", "AI": {"tldr": "CALYPSO\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u673a\u5236\u6027\u5143\u79cd\u7fa4\u6a21\u578b\u6765\u9884\u6d4bMRSA\u5728\u533b\u7597\u548c\u793e\u533a\u73af\u5883\u4e2d\u7684\u4f20\u64ad\u52a8\u6001\uff0c\u76f8\u6bd4\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u6027\u80fd\u63d0\u53474.5%\u4ee5\u4e0a", "motivation": "MRSA\u662f\u533b\u9662\u548c\u957f\u671f\u62a4\u7406\u673a\u6784\u7684\u91cd\u8981\u516c\u5171\u536b\u751f\u5a01\u80c1\uff0c\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u7f3a\u4e4f\u6d41\u884c\u75c5\u5b66\u53ef\u89e3\u91ca\u6027\u4e14\u6027\u80fd\u6709\u9650\uff0c\u673a\u5236\u6027\u6d41\u884c\u75c5\u6a21\u578b\u96be\u4ee5\u6821\u51c6\u4e14\u65e0\u6cd5\u6709\u6548\u6574\u5408\u591a\u6837\u5316\u6570\u636e\u96c6", "method": "\u6574\u5408\u795e\u7ecf\u7f51\u7edc\u4e0e\u673a\u5236\u6027\u5143\u79cd\u7fa4\u6a21\u578b\uff0c\u5229\u7528\u60a3\u8005\u7ea7\u4fdd\u9669\u7d22\u8d54\u6570\u636e\u3001\u901a\u52e4\u6570\u636e\u548c\u533b\u7597\u8f6c\u79fb\u6a21\u5f0f\u6765\u5b66\u4e60\u533a\u57df\u548c\u65f6\u95f4\u7279\u5f02\u6027\u53c2\u6570\uff0c\u6355\u6349MRSA\u4f20\u64ad\u52a8\u6001", "result": "CALYPSO\u76f8\u6bd4\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u5728\u5168\u5dde\u8303\u56f4\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u8d85\u8fc74.5%\uff0c\u80fd\u591f\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\u548c\u5236\u5b9a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u611f\u67d3\u9884\u9632\u8d44\u6e90\u5206\u914d\u7b56\u7565", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u591a\u7a7a\u95f4\u5206\u8fa8\u7387\u9884\u6d4b\uff0c\u652f\u6301\u611f\u67d3\u63a7\u5236\u653f\u7b56\u548c\u66b4\u53d1\u98ce\u9669\u7684\u53cd\u4e8b\u5b9e\u5206\u6790\uff0c\u4e3a\u516c\u5171\u536b\u751f\u51b3\u7b56\u63d0\u4f9b\u6709\u529b\u5de5\u5177"}}
{"id": "2508.13558", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13558", "abs": "https://arxiv.org/abs/2508.13558", "authors": ["Hsieh Ching-Teng", "Wang Yuan-Kai"], "title": "Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer", "comment": "14 pages, 11 figures", "summary": "In recent years, neuromorphic computing and spiking neural networks (SNNs)\nhave ad-vanced rapidly through integration with deep learning. However, the\nperformance of SNNs still lags behind that of convolutional neural networks\n(CNNs), primarily due to the limited information capacity of spike-based data.\nAlthough some studies have attempted to improve SNN performance by training\nthem with non-spiking inputs such as static images, this approach deviates from\nthe original intent of neuromorphic computing, which emphasizes spike-based\ninformation processing. To address this issue, we propose a Neuron-like\nEncoding method that generates spike data based on the intrinsic operational\nprinciples and functions of biological neurons. This method is further enhanced\nby the incorporation of an artificial pho-toreceptor layer, enabling spike data\nto carry both color and luminance information, thereby forming a complete\nvisual spike signal. Experimental results using the Integrate-and-Fire neuron\nmodel demonstrate that this biologically inspired approach effectively\nincreases the information content of spike signals and improves SNN\nperformance, all while adhering to neuromorphic principles. We believe this\nconcept holds strong potential for future development and may contribute to\novercoming current limitations in neuro-morphic computing, facilitating broader\napplications of SNNs.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u4eff\u751f\u7269\u795e\u7ecf\u5143\u8fd0\u4f5c\u539f\u7406\u7684\u795e\u7ecf\u5143\u6837\u7f16\u7801\u65b9\u6cd5\uff0c\u751f\u6210\u5305\u542b\u989c\u8272\u548c\u4eae\u5ea6\u4fe1\u606f\u7684\u89c6\u89c9\u523a\u523a\u4fe1\u53f7\uff0c\u63d0\u5347\u523a\u523a\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u4e14\u9075\u5faa\u795e\u7ecf\u6a21\u6001\u8ba1\u7b97\u539f\u5219\u3002", "motivation": "\u89e3\u51b3SNN\u6027\u80fd\u843d\u540e\u4e8eCNN\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u56e0\u4e3a\u523a\u523a\u6570\u636e\u4fe1\u606f\u5bb9\u91cf\u6709\u9650\uff0c\u800c\u73b0\u6709\u7528\u9759\u6001\u56fe\u50cf\u8bad\u7ec3SNN\u7684\u65b9\u6cd5\u53c8\u8fdd\u80cc\u4e86\u795e\u7ecf\u6a21\u6001\u8ba1\u7b97\u7684\u539f\u5219\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u5143\u6837\u7f16\u7801\u65b9\u6cd5\uff0c\u57fa\u4e8e\u751f\u7269\u795e\u7ecf\u5143\u7684\u5185\u5728\u8fd0\u4f5c\u539f\u7406\u548c\u529f\u80fd\u751f\u6210\u523a\u523a\u6570\u636e\uff0c\u5e76\u52a0\u5165\u4eba\u9020\u5149\u53d7\u5668\u5c42\u4ee5\u4f7f\u523a\u523a\u6570\u636e\u5305\u542b\u989c\u8272\u548c\u4eae\u5ea6\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u79cd\u53d7\u751f\u7269\u5b66\u542f\u53d1\u7684\u65b9\u6cd5\u6709\u6548\u589e\u52a0\u4e86\u523a\u523a\u4fe1\u53f7\u7684\u4fe1\u606f\u542b\u91cf\uff0c\u63d0\u9ad8\u4e86SNN\u7684\u6027\u80fd\uff0c\u540c\u65f6\u9075\u5faa\u795e\u7ecf\u6a21\u6001\u8ba1\u7b97\u539f\u5219\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u6709\u671b\u514b\u670d\u795e\u7ecf\u6a21\u6001\u8ba1\u7b97\u7684\u5f53\u524d\u9650\u5236\uff0c\u4fc3\u8fdbSNN\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2508.13561", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13561", "abs": "https://arxiv.org/abs/2508.13561", "authors": ["Rituparna Datta", "Methun Kamruzzaman", "Eili Y. Klein", "Gregory R Madden", "Xinwei Deng", "Anil Vullikanti", "Parantapa Bhattacharya"], "title": "Prediction of Hospital Associated Infections During Continuous Hospital Stays", "comment": null, "summary": "The US Centers for Disease Control and Prevention (CDC), in 2019, designated\nMethicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial\nresistance threat. The risk of acquiring MRSA and suffering life-threatening\nconsequences due to it remains especially high for hospitalized patients due to\na unique combination of factors, including: co-morbid conditions, immuno\nsuppression, antibiotic use, and risk of contact with contaminated hospital\nworkers and equipment. In this paper, we present a novel generative\nprobabilistic model, GenHAI, for modeling sequences of MRSA test results\noutcomes for patients during a single hospitalization. This model can be used\nto answer many important questions from the perspectives of hospital\nadministrators for mitigating the risk of MRSA infections. Our model is based\non the probabilistic programming paradigm, and can be used to approximately\nanswer a variety of predictive, causal, and counterfactual questions. We\ndemonstrate the efficacy of our model by comparing it against discriminative\nand generative machine learning models using two real-world datasets.", "AI": {"tldr": "\u63d0\u51faGenHAI\u6982\u7387\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5efa\u6a21\u4f4f\u9662\u60a3\u8005MRSA\u68c0\u6d4b\u7ed3\u679c\u5e8f\u5217\uff0c\u5e2e\u52a9\u533b\u9662\u7ba1\u7406\u8005\u964d\u4f4eMRSA\u611f\u67d3\u98ce\u9669", "motivation": "MRSA\u88abCDC\u5217\u4e3a\u4e25\u91cd\u6297\u83cc\u7d20\u8010\u836f\u6027\u5a01\u80c1\uff0c\u4f4f\u9662\u60a3\u8005\u56e0\u591a\u79cd\u56e0\u7d20\u9762\u4e34\u9ad8\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u5de5\u5177\u6765\u7406\u89e3\u548c\u51cf\u8f7b\u611f\u67d3\u98ce\u9669", "method": "\u57fa\u4e8e\u6982\u7387\u7f16\u7a0b\u8303\u5f0f\u7684\u751f\u6210\u6982\u7387\u6a21\u578b\uff0c\u80fd\u591f\u8fd1\u4f3c\u56de\u7b54\u9884\u6d4b\u6027\u3001\u56e0\u679c\u6027\u548c\u53cd\u4e8b\u5b9e\u6027\u95ee\u9898", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4e0e\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027", "conclusion": "GenHAI\u6a21\u578b\u4e3a\u533b\u9662\u7ba1\u7406\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u56de\u7b54\u5173\u4e8eMRSA\u611f\u67d3\u98ce\u9669\u7684\u5404\u79cd\u91cd\u8981\u95ee\u9898"}}
{"id": "2508.13625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13625", "abs": "https://arxiv.org/abs/2508.13625", "authors": ["Wenxuan Ye", "Xueli An", "Onur Ayan", "Junfan Wang", "Xueqiang Yan", "Georg Carle"], "title": "Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models", "comment": "Accepted to Globecom 2025", "summary": "Large models, renowned for superior performance, outperform smaller ones even\nwithout billion-parameter scales. While mobile network servers have ample\ncomputational resources to support larger models than client devices, privacy\nconstraints prevent clients from directly sharing their raw data. Federated\nLearning (FL) enables decentralized clients to collaboratively train a shared\nmodel by exchanging model parameters instead of transmitting raw data. Yet, it\nrequires a uniform model architecture and multiple communication rounds, which\nneglect resource heterogeneity, impose heavy computational demands on clients,\nand increase communication overhead. To address these challenges, we propose\nFedOL, to construct a larger and more comprehensive server model in one-shot\nsettings (i.e., in a single communication round). Instead of model parameter\nsharing, FedOL employs knowledge distillation, where clients only exchange\nmodel prediction outputs on an unlabeled public dataset. This reduces\ncommunication overhead by transmitting compact predictions instead of full\nmodel weights and enables model customization by allowing heterogeneous model\narchitectures. A key challenge in this setting is that client predictions may\nbe biased due to skewed local data distributions, and the lack of ground-truth\nlabels in the public dataset further complicates reliable learning. To mitigate\nthese issues, FedOL introduces a specialized objective function that\niteratively refines pseudo-labels and the server model, improving learning\nreliability. To complement this, FedOL incorporates a tailored pseudo-label\ngeneration and knowledge distillation strategy that effectively integrates\ndiverse knowledge. Simulation results show that FedOL significantly outperforms\nexisting baselines, offering a cost-effective solution for mobile networks\nwhere clients possess valuable private data but limited computational\nresources.", "AI": {"tldr": "FedOL\u662f\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u8f6e\u901a\u4fe1\u5b9e\u73b0\u670d\u52a1\u5668\u5927\u6a21\u578b\u6784\u5efa\uff0c\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u800c\u975e\u53c2\u6570\u5171\u4eab\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u652f\u6301\u5f02\u6784\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7edf\u4e00\u6a21\u578b\u67b6\u6784\u8981\u6c42\u3001\u591a\u8f6e\u901a\u4fe1\u5f00\u9500\u5927\u3001\u5ba2\u6237\u7aef\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7b49\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5ba2\u6237\u7aef\u4ec5\u4ea4\u6362\u5728\u672a\u6807\u8bb0\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u9884\u6d4b\u8f93\u51fa\uff1b\u5f15\u5165\u4e13\u95e8\u7684\u76ee\u6807\u51fd\u6570\u8fed\u4ee3\u4f18\u5316\u4f2a\u6807\u7b7e\u548c\u670d\u52a1\u5668\u6a21\u578b\uff1b\u8bbe\u8ba1\u5b9a\u5236\u5316\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aFedOL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u79fb\u52a8\u7f51\u7edc\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FedOL\u901a\u8fc7\u5355\u8f6e\u901a\u4fe1\u548c\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u8d44\u6e90\u5f02\u6784\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2508.13236", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13236", "abs": "https://arxiv.org/abs/2508.13236", "authors": ["Hyeonjin Choi", "Jinse Kim", "Dong-yeon Yoo", "Ju-sung Sun", "Jung-won Lee"], "title": "Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray", "comment": "8 pages, 5 figures", "summary": "Early detection and rapid intervention of lung cancer are crucial.\nNonetheless, ensuring an accurate diagnosis is challenging, as physicians'\nability to interpret chest X-rays varies significantly depending on their\nexperience and degree of fatigue. Although medical AI has been rapidly\nadvancing to assist in diagnosis, physicians' trust in such systems remains\nlimited, preventing widespread clinical adoption. This skepticism fundamentally\nstems from concerns about its diagnostic uncertainty. In clinical diagnosis,\nphysicians utilize extensive background knowledge and clinical experience. In\ncontrast, medical AI primarily relies on repetitive learning of the target\nlesion to generate diagnoses based solely on that data. In other words, medical\nAI does not possess sufficient knowledge to render a diagnosis, leading to\ndiagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning\nPolicy that can address the issue of knowledge deficiency by learning the\nphysicians' background knowledge alongside the Chest X-ray lesion information.\nWe used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou\nUniversity Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a\n10% enhancement in sensitivity compared to the baseline model while also\ndecreasing entropy as a measure of uncertainty by 0.2.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u533b\u751f\u7684\u80cc\u666f\u77e5\u8bc6\u548c\u80ba\u90e8\u75c5\u53d8\u6765\u63d0\u9ad8\u533b\u7597AI\u5728\u80f8\u90e8X\u5149\u7247\u8bca\u65ad\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u533b\u751f\u5bf9\u533b\u7597AI\u7cfb\u7edf\u7684\u4e0d\u4fe1\u4efb\u4e3b\u8981\u6765\u81ea\u4e8e\u5bf9\u5176\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u7684\u62c5\u5fe7\u3002\u4e34\u5e8a\u8bca\u65ad\u4e2d\u533b\u751f\u5229\u7528\u4e86\u5e7f\u6cdb\u7684\u80cc\u666f\u77e5\u8bc6\u548c\u7ecf\u9a8c\uff0c\u800c\u533b\u7597AI\u4ec5\u4f9d\u8d56\u5bf9\u76ee\u6807\u75c5\u53d8\u7684\u91cd\u590d\u5b66\u4e60\uff0c\u5bfc\u81f4\u77e5\u8bc6\u7f3a\u4e4f\u548c\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5b66\u4e60\u7b56\u7565\uff0c\u540c\u65f6\u5b66\u4e60\u533b\u751f\u7684\u80cc\u666f\u77e5\u8bc6\u548c\u80f8\u90e8X\u5149\u7247\u75c5\u53d8\u4fe1\u606f\u3002\u4f7f\u7528\u4e86\u6765\u81eaAjou\u5927\u5b66\u533b\u96622,517\u5f20\u65e0\u75c5\u53d8\u56fe\u50cf\u548c656\u5f20\u7ed3\u8282\u56fe\u50cf\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u63d0\u51fa\u6a21\u578b\u8fbe\u523092%\u51c6\u786e\u7387\uff08IoU 0.2 / FPPI 2\uff09\uff0c\u654f\u611f\u5ea6\u6bd4\u57fa\u51c6\u6a21\u578b\u63d0\u9ad810%\uff0c\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u71ac\u4f4e\u4e8620%\uff08\u71b7\u589e\u91cf\u51cf\u5c110.2\uff09\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u533b\u751f\u7684\u80cc\u666f\u77e5\u8bc6\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u533b\u7597AI\u7684\u77e5\u8bc6\u7f3a\u4e4f\u95ee\u9898\uff0c\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u533b\u7597AI\u5728\u4e34\u5e8a\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2508.13812", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.13812", "abs": "https://arxiv.org/abs/2508.13812", "authors": ["Donghwa Kang", "Doohyun Kim", "Sang-Ki Ko", "Jinkyu Lee", "Hyeongboo Baek", "Brent ByungHoon Kang"], "title": "Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation", "comment": "8 pages", "summary": "State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural\nnetworks (SNNs), which largely rely on extending FGSM and PGD frameworks, face\na critical limitation: substantial attack latency from multi-timestep\nprocessing, rendering them infeasible for practical real-time applications.\nThis inefficiency stems from their design as direct extensions of ANN\nparadigms, which fail to exploit key SNN properties. In this paper, we propose\nthe timestep-compressed attack (TCA), a novel framework that significantly\nreduces attack latency. TCA introduces two components founded on key insights\ninto SNN behavior. First, timestep-level backpropagation (TLBP) is based on our\nfinding that global temporal information in backpropagation to generate\nperturbations is not critical for an attack's success, enabling per-timestep\nevaluation for early stopping. Second, adversarial membrane potential reuse\n(A-MPR) is motivated by the observation that initial timesteps are\ninefficiently spent accumulating membrane potential, a warm-up phase that can\nbe pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the\nCIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the\nrequired attack latency by up to 56.6% and 57.1% compared to SOTA methods in\nwhite-box and black-box settings, respectively, while maintaining a comparable\nattack success rate.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u65f6\u95f4\u6b65\u538b\u7f29\u653b\u51fb(TCA)\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u6b65\u7ea7\u53cd\u5411\u4f20\u64ad\u548c\u5bf9\u6297\u819c\u7535\u4f4d\u91cd\u7528\u6280\u672f\uff0c\u5728\u4fdd\u6301\u653b\u51fb\u6210\u529f\u7387\u7684\u540c\u65f6\u5c06\u653b\u51fb\u5ef6\u8fdf\u51cf\u5c11\u4e86\u8d85\u8fc756%\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5728\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e2d\u5b58\u5728\u7684\u5b9e\u65f6\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eFGSM\u548cPGD\u6846\u67b6\u7684\u68af\u5ea6\u57fa\u7840\u653b\u51fb\u65b9\u6cd5\u5728\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNNs)\u4e2d\u5b58\u5728\u663e\u8457\u7684\u653b\u51fb\u5ef6\u8fdf\u95ee\u9898\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u5b83\u4eec\u76f4\u63a5\u6269\u5c55\u81eaANN\u8303\u5f0f\uff0c\u6ca1\u6709\u5229\u7528SNN\u7684\u5173\u952e\u7279\u6027\uff0c\u5bfc\u81f4\u591a\u65f6\u95f4\u6b65\u5904\u7406\u8017\u65f6\u8fc7\u957f\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u6b65\u538b\u7f29\u653b\u51fb(TCA)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u65f6\u95f4\u6b65\u7ea7\u53cd\u5411\u4f20\u64ad(TLBP)\uff0c\u57fa\u4e8e\u53d1\u73b0\u751f\u6210\u5e72\u6270\u7684\u5168\u5c40\u65f6\u95f4\u4fe1\u606f\u5e76\u975e\u5173\u952e\uff0c\u652f\u6301\u6309\u65f6\u95f4\u6b65\u8bc4\u4f30\u5e76\u63d0\u524d\u505c\u6b62\uff1b2)\u5bf9\u6297\u819c\u7535\u4f4d\u91cd\u7528(A-MPR)\uff0c\u5229\u7528\u521d\u59cb\u65f6\u95f4\u6b65\u4e3b\u8981\u7528\u4e8e\u7d2f\u79ef\u819c\u7535\u4f4d\u7684\u7279\u70b9\uff0c\u5c06\u8fd9\u4e2a\u70ed\u8eab\u9636\u6bb5\u9884\u8ba1\u7b97\u5e76\u91cd\u7528\u3002", "result": "\u5728VGG-11\u548cResNet-17\u6a21\u578b\u4e0a\u4f7f\u7528CIFAR-10/100\u548cCIFAR10-DVS\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0cTCA\u5728\u767d\u76d2\u548c\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u5206\u522b\u5c06\u6240\u9700\u653b\u51fb\u5ef6\u8fdf\u51cf\u5c11\u4e86\u6700\u9ad856.6%\u548c57.1%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "TCA\u6846\u67b6\u901a\u8fc7\u5145\u5206\u5229\u7528SNN\u7684\u7279\u6709\u6027\u8d28\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3aSNN\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6848\u3002"}}
{"id": "2508.13905", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13905", "abs": "https://arxiv.org/abs/2508.13905", "authors": ["Tianheng Ling", "Vipin Singh", "Chao Qian", "Felix Biessmann", "Gregor Schiele"], "title": "Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management", "comment": "6 pages, 6 figures, 1 table, accepted by the 11th IEEE International\n  Smart Cities Conference", "summary": "Extreme weather events, intensified by climate change, increasingly challenge\naging combined sewer systems, raising the risk of untreated wastewater\noverflow. Accurate forecasting of sewer overflow basin filling levels can\nprovide actionable insights for early intervention, helping mitigating\nuncontrolled discharge. In recent years, AI-based forecasting methods have\noffered scalable alternatives to traditional physics-based models, but their\nreliance on cloud computing limits their reliability during communication\noutages. To address this, we propose an end-to-end forecasting framework that\nenables energy-efficient inference directly on edge devices. Our solution\nintegrates lightweight Transformer and Long Short-Term Memory (LSTM) models,\ncompressed via integer-only quantization for efficient on-device execution.\nMoreover, an automated hardware-aware deployment pipeline is used to search for\noptimal model configurations by jointly minimizing prediction error and energy\nconsumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer\ndata, the selected 8-bit Transformer model, trained on 24 hours of historical\nmeasurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ\nper inference. In contrast, the optimal 8-bit LSTM model requires significantly\nless energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE\n0.0432) and much longer training time. This trade-off highlights the need to\nalign model selection with deployment priorities, favoring LSTM for ultra-low\nenergy consumption or Transformer for higher predictive accuracy. In general,\nour work enables local, energy-efficient forecasting, contributing to more\nresilient combined sewer systems. All code can be found in the GitHub\nRepository (https://github.com/tianheng-ling/EdgeOverflowForecast).", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9884\u6d4b\u5408\u6d41\u5236\u6c61\u6c34\u6ea2\u6d41\u6c60\u586b\u5145\u6c34\u4f4d\u7684\u7aef\u4fa7\u8bbe\u5907\u667a\u80fd\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316Transformer\u548cLSTM\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u80fd\u8017\u7684\u5e73\u8861\u3002", "motivation": "\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u52a0\u5267\u4e86\u8001\u5316\u5408\u6d41\u6c61\u6c34\u7cfb\u7edf\u7684\u6ea2\u6d41\u98ce\u9669\uff0c\u800c\u73b0\u6709AI\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e91\u8ba1\u7b97\u5728\u901a\u4fe1\u6545\u969c\u65f6\u53ef\u9760\u6027\u4f4e\uff0c\u9700\u8981\u80fd\u591f\u5728\u7aef\u4fa7\u8bbe\u5907\u4e0a\u9ad8\u6548\u6267\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u6574\u5408\u8f7b\u91cf\u5316Transformer\u548cLSTM\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u6570\u91c7\u6837\u538b\u7f29\u6280\u672f\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002\u4f7f\u7528\u81ea\u52a8\u5316\u786c\u4ef6\u611f\u77e5\u90e8\u7f72\u6d41\u7a0b\u5728AMD Spartan-7 FPGA\u4e0a\u5bfb\u627e\u6700\u4f18\u6a21\u578b\u914d\u7f6e\u3002", "result": "8\u4f4dTransformer\u6a21\u578b\u572824\u5c0f\u65f6\u5386\u53f2\u6570\u636e\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027(MSE 0.0376)\uff0c\u6bcf\u6b21\u63a8\u7406\u80fd\u80170.370mJ\u30028\u4f4dLSTM\u6a21\u578b\u80fd\u8017\u66f4\u4f4e(0.009mJ)\u4f46\u51c6\u786e\u6027\u5dee14.89%\uff0c\u8bad\u7ec3\u65f6\u95f4\u66f4\u957f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u672c\u5730\u5316\u3001\u80fd\u6548\u9ad8\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u5408\u6d41\u5236\u6c61\u6c34\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6839\u636e\u90e8\u7f72\u4f18\u5148\u7ea7\u9009\u62e9LSTM(\u6781\u4f4e\u80fd\u8017)\u6216Transformer(\u9ad8\u51c6\u786e\u6027)\u6a21\u578b\u3002"}}
{"id": "2508.13891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13891", "abs": "https://arxiv.org/abs/2508.13891", "authors": ["Taimur Khan"], "title": "Forecasting Smog Events Using ConvLSTM: A Spatio-Temporal Approach for Aerosol Index Prediction in South Asia", "comment": null, "summary": "The South Asian Smog refers to the recurring annual air pollution events\nmarked by high contaminant levels, reduced visibility, and significant\nsocio-economic impacts, primarily affecting the Indo-Gangetic Plains (IGP) from\nNovember to February. Over the past decade, increased air pollution sources\nsuch as crop residue burning, motor vehicles, and changing weather patterns\nhave intensified these smog events. However, real-time forecasting systems for\nincreased particulate matter concentrations are still not established at\nregional scale. The Aerosol Index, closely tied to smog formation and a key\ncomponent in calculating the Air Quality Index (AQI), reflects particulate\nmatter concentrations. This study forecasts aerosol events using Sentinel-5P\nair constituent data (2019-2023) and a Convolutional Long-Short Term Memory\n(ConvLSTM) neural network, which captures spatial and temporal correlations\nmore effectively than previous models. Using the Ultraviolet (UV) Aerosol Index\nat 340-380 nm as the predictor, results show the Aerosol Index can be\nforecasted at five-day intervals with a Mean Squared Error of ~0.0018, loss of\n~0.3995, and Structural Similarity Index of ~0.74. While effective, the model\ncan be improved by integrating additional data and refining its architecture.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Sentinel-5P\u536b\u661f\u6570\u636e\u548cConvLSTM\u795e\u7ecf\u7f51\u7edc\u6765\u9884\u6d4b\u5357\u4e9a\u96fe\u973e\u4e8b\u4ef6\u7684\u60ac\u6d6e\u9897\u7c92\u7269\u6307\u6570\uff0c\u5b9e\u73b0\u4e865\u5929\u95f4\u9694\u7684\u9884\u6d4b\uff0c\u8bef\u5dee\u8f83\u5c0f\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5357\u4e9a\u96fe\u973e\u4e8b\u4ef6\u5bf9\u5370\u5ea6-\u6052\u6cb3\u5e73\u539f\u9020\u6210\u4e25\u91cd\u7684\u793e\u4f1a\u7ecf\u6d4e\u5f71\u54cd\uff0c\u4f46\u7f3a\u4e4f\u533a\u57df\u5c3a\u5ea6\u7684\u5b9e\u65f6\u9884\u6d4b\u7cfb\u7edf\u3002\u60ac\u6d6e\u9897\u7c92\u7269\u6307\u6570\u4e0e\u96fe\u973e\u5f62\u6210\u5bc6\u5207\u76f8\u5173\uff0c\u662f\u7a7a\u6c14\u8d28\u91cf\u6307\u6570\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u4f7f\u75282019-2023\u5e74Sentinel-5P\u536b\u661f\u6c14\u6eb6\u80f6\u6570\u636e\uff0c\u91c7\u7528\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6(ConvLSTM)\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5229\u7528340-380nm\u7d2b\u5916\u6c14\u6eb6\u80f6\u6307\u6570\u4f5c\u4e3a\u9884\u6d4b\u56e0\u5b50\uff0c\u6355\u6349\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "result": "\u6a21\u578b\u80fd\u591f\u4ee55\u5929\u95f4\u9694\u9884\u6d4b\u6c14\u6eb6\u80f6\u6307\u6570\uff0c\u5747\u65b9\u8bef\u5dee\u7ea60.0018\uff0c\u635f\u5931\u7ea60.3995\uff0c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u7ea60.74\uff0c\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u6a21\u578b\u3002", "conclusion": "ConvLSTM\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u5357\u4e9a\u96fe\u973e\u4e8b\u4ef6\uff0c\u4f46\u53ef\u901a\u8fc7\u6574\u5408\u66f4\u591a\u6570\u636e\u548c\u4f18\u5316\u67b6\u6784\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002"}}
