<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 15]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: Semantic-Drive：一个本地优先的神经符号框架，用于从自动驾驶车辆日志中高效挖掘长尾安全关键事件，通过解耦感知和推理VLM实现高召回率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆开发面临长尾训练数据稀缺的瓶颈，现有解决方案存在精度不足、隐私侵犯和成本高昂的问题，需要一种本地化、精确且隐私保护的语义数据挖掘方法。

Method: 采用神经符号框架，将感知解耦为两个阶段：1) 通过实时开放词汇检测器进行符号接地以锚定注意力；2) 通过推理VLM进行认知场景分析。使用"系统2"推理时对齐策略和多模型"法官-侦察员"共识机制来缓解幻觉问题。

Result: 在nuScenes数据集上，相比CLIP的0.475召回率，Semantic-Drive达到0.966召回率，风险评估误差减少40%，系统可在消费级硬件上运行，提供隐私保护的云端替代方案。

Conclusion: Semantic-Drive为自动驾驶数据挖掘提供了一个高效、隐私保护且成本效益高的解决方案，显著提高了长尾安全关键事件的检测能力，推动了自动驾驶系统的稳健性发展。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [2] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 本研究提出一种监督语义分割工作流，旨在提升火灾后烧毁区域（BA）划分的性能和效率，针对SPOT-6/7高分辨率影像，评估了U-Net和SegFormer模型，并探讨了土地覆盖数据辅助任务和测试时增强技术的影响。


<details>
  <summary>Details</summary>
Motivation: 当前烧毁区域划分方法主要依赖计算机视觉模型处理灾后遥感影像，但往往忽视了其在时间紧迫的应急管理场景中的适用性。需要开发既能保证性能又能提高效率的烧毁区域划分方法。

Method: 提出监督语义分割工作流，针对SPOT-6/7高分辨率影像。实验评估U-Net和SegFormer模型，使用Dice分数、交并比和推理时间作为评价指标。探索了将土地覆盖数据作为辅助任务，以及测试时增强技术。

Result: U-Net和SegFormer在有限训练数据下表现相似，但SegFormer需要更多计算资源，在应急场景中实用性受限。加入土地覆盖辅助任务能增强模型鲁棒性且不增加推理时间。测试时增强能提升划分性能但增加推理时间，可通过混合精度等优化方法缓解。

Conclusion: 该研究为应急管理场景下的烧毁区域划分提供了有效的监督语义分割工作流，平衡了性能与效率需求。U-Net在资源受限场景中更具实用性，辅助任务和优化技术能进一步提升模型性能。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [3] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 开发深度学习模型融合多源遥感数据，生成格陵兰冰盖每日100米分辨率的地表融水分布图，相比现有方法精度显著提升


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速融化过程尚未完全理解且难以测量，现有融水分布图在时间和空间分辨率上存在权衡，无法同时实现高时空分辨率

Method: 使用深度学习模型融合区域气候模型（RCM）、合成孔径雷达（SAR）、被动微波（PMW）和数字高程模型（DEM）数据，对Helheim冰川2017-2023年数据进行时空降尺度处理

Result: 融合所有数据流的深度学习方法精度达95%，显著优于仅依赖区域气候模型（83%）或被动微波观测（72%）的非深度学习方法；同时发布了MeltwaterBench基准数据集

Conclusion: 深度学习融合多源遥感数据能有效生成高时空分辨率融水分布图，为理解冰盖融化过程提供重要工具，并建立了基准数据集促进更复杂降尺度方法比较

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [4] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 提出ISA-ViT框架和ALERT数据集，解决UWB雷达在分心驾驶检测中的两大挑战：缺乏大规模真实数据集和ViT输入尺寸限制问题。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶是全球致命事故的主要原因。虽然IR-UWB雷达具有抗干扰、低功耗和隐私保护等优势，但缺乏大规模真实UWB数据集，且固定输入的ViT难以适应UWB雷达数据的非标准维度。

Method: 提出输入尺寸无关的Vision Transformer（ISA-ViT）框架，通过调整补丁配置和利用预训练位置嵌入向量，在调整UWB数据尺寸时保留多普勒频移和相位特征。同时提出领域融合策略，结合距离域和频域特征提升分类性能。

Result: ISA-ViT在UWB分心驾驶检测中比现有ViT方法准确率提升22.68%。发布了包含10,220个雷达样本的ALERT数据集，涵盖7种分心驾驶行为。

Conclusion: 通过公开ALERT数据集和详细描述输入尺寸无关策略，促进了更鲁棒、可扩展的分心驾驶检测系统的开发，有助于实际部署。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [5] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

TL;DR: 提出一个结合CNN和GCN的深度学习管道，用于识别自闭症儿童在与机器人互动时的情绪反应，使用印度首个大规模真实世界数据集。


<details>
  <summary>Details</summary>
Motivation: 理解自闭症儿童在社交互动中的情绪反应是发展心理学和人机交互领域的关键挑战，目前缺乏针对自闭症儿童情绪分析的专门研究和大规模数据集。

Method: 使用混合模型：微调的ResNet-50 CNN + 三层GCN，训练于从MediaPipe FaceMesh提取的视觉和几何特征。采用DeepFace和FER模型的加权集成进行概率标签生成，通过KL散度优化融合嵌入进行最终分类。

Result: 该方法在建模细微情感反应方面表现出稳健性能，有效捕捉神经多样性儿童的微观情绪线索，为临床和治疗性人机交互中的情感分析提供了重要前景。

Conclusion: 这是印度首个针对自闭症儿童情绪分析的大规模真实世界数据集和管道，为未来个性化辅助技术奠定了重要基础，填补了自闭症特定HRI研究的空白。

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [6] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Motus是一个统一的潜在动作世界模型，通过混合Transformer架构整合理解、视频生成和动作三个专家模块，利用光流学习潜在动作，在仿真和真实场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法将理解、世界建模和控制分离为孤立模型，这种碎片化阻碍了多模态生成能力的统一，且难以从大规模异构数据中学习。需要构建一个统一的系统来整合这些功能。

Method: 提出Motus统一潜在动作世界模型：1) 采用混合Transformer架构整合三个专家模块（理解、视频生成、动作）；2) 使用UniDiffuser风格调度器实现不同建模模式的灵活切换；3) 利用光流学习潜在动作，提取像素级"delta动作"；4) 采用三阶段训练流程和六层数据金字塔进行大规模动作预训练。

Result: 在仿真环境中：相比X-VLA提升15%，相比Pi0.5提升45%；在真实场景中：提升11-48%。证明了统一建模所有功能和先验知识能显著提升下游机器人任务性能。

Conclusion: Motus通过统一潜在动作世界模型成功整合了理解、世界建模和控制功能，利用现有预训练模型和丰富的可共享运动信息，在仿真和真实场景中都取得了显著性能提升，证明了统一建模方法对机器人任务的重要价值。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [7] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: 提出了一种监督对比学习框架用于视频表示学习，通过将视频帧空间排列成单张图像，利用预训练CNN骨干网络，避免复杂视频Transformer的计算开销，在分类准确率和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示学习方法通常需要复杂的视频Transformer模型，计算开销大。需要一种能够利用预训练CNN骨干网络、计算效率高且能有效学习视频时序全局上下文的方法。

Method: 提出Supervised Contrastive Frame Aggregation方法：1) 视频到图像聚合策略，将多个视频帧空间排列成单张输入图像；2) 使用预训练CNN骨干网络（如ResNet50）；3) 设计对比学习目标，直接比较模型生成的成对投影；4) 通过不同时间帧采样创建同一视频的多个自然视图作为正样本。

Result: 在Penn Action数据集上达到76%分类准确率（ViVIT为43%），在HMDB51数据集上达到48%准确率（ViVIT为37%）。计算资源需求更少，在监督和自监督设置下都能学习有效视频表示，支持分类和字幕生成等任务。

Conclusion: 该方法通过视频帧聚合和对比学习，有效利用时序全局上下文，在保持高分类准确率的同时显著降低计算开销，为视频表示学习提供了一种高效实用的解决方案。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [8] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: MADTempo是一个视频检索框架，通过时序搜索机制聚合视频片段相似度来捕获事件连续性，并结合Google图像搜索回退模块增强查询表示，提升对未见视觉概念和复杂时序查询的检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索系统在建模多个事件间的时序依赖关系和处理包含未见或罕见视觉概念的查询方面存在不足，需要能够理解复杂事件时序结构且具备更强泛化能力的检索系统。

Method: 1) 时序搜索机制：通过聚合连续视频片段的相似度分数来捕获事件级连续性，支持多事件查询的连贯检索；2) Google图像搜索回退模块：利用外部网络图像扩展查询表示，弥补预训练视觉嵌入的不足，增强对分布外查询的鲁棒性。

Result: MADTempo框架提升了视频检索系统的时序推理和泛化能力，为大规模视频语料库中实现更具语义感知和自适应的检索铺平了道路。

Conclusion: 该工作通过统一时序搜索与网络级视觉基础，解决了现有视频检索系统在时序建模和泛化能力方面的局限性，推动了更智能、适应性更强的视频检索技术的发展。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [9] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 提出一个统一的多模态时刻检索系统，通过级联双嵌入管道、时间感知评分机制和智能体引导查询分解，解决现有方法在跨模态噪声、时间建模和模态选择方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频内容的指数增长迫切需要高效的多模态时刻检索系统。现有方法面临三个关键挑战：1) 固定权重融合策略无法处理跨模态噪声和模糊查询；2) 时间建模难以捕捉连贯事件序列同时惩罚不现实的时间间隔；3) 系统需要手动模态选择，降低了可用性。

Method: 提出三个关键创新：1) 级联双嵌入管道结合BEIT-3和SigLIP进行广泛检索，通过BLIP-2重排序平衡召回率和精确度；2) 时间感知评分机制通过波束搜索对大的时间间隔应用指数衰减惩罚，构建连贯事件序列而非孤立帧；3) 智能体引导查询分解(GPT-4o)自动解释模糊查询，将其分解为模态特定子查询(视觉/OCR/ASR)，并进行自适应分数融合，消除手动模态选择。

Result: 定性分析表明，该系统能有效处理模糊查询，检索时间连贯的序列，并动态调整融合策略，提升了交互式时刻搜索能力。

Conclusion: 该统一多模态时刻检索系统通过创新的级联检索、时间建模和智能查询分解，显著提升了处理模糊查询、构建连贯事件序列和自适应融合的能力，推动了交互式时刻搜索技术的发展。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [10] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 论文提出GAPL框架解决AIGI检测中的"收益后冲突"困境，通过生成器感知原型学习创建统一特征空间，结合LoRA增强判别能力，实现跨生成器的SOTA检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用AIGI检测器通过聚合多生成器数据提升泛化性，但存在"收益后冲突"困境：随着数据源多样性增加，检测性能先提升后下降。这源于数据级异质性导致特征分布重叠，以及模型级瓶颈（固定预训练编码器无法适应复杂性增长）。

Method: 提出生成器感知原型学习(GAPL)框架：1) 学习紧凑的典型伪造原型集，创建统一低方差特征空间以对抗数据异质性；2) 采用两阶段训练方案结合低秩适应(LoRA)，增强判别能力同时保留预训练知识。

Result: GAPL在广泛GAN和扩散模型生成器上实现最先进的检测性能，展示出优越的跨生成器检测准确率，有效解决了"收益后冲突"困境。

Conclusion: GAPL通过结构化学习范式约束表示，建立更鲁棒和可泛化的决策边界，为通用AIGI检测提供了有效解决方案，代码已开源。

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [11] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: 提出Bi-Erasing框架，通过双向图像引导的概念擦除，同时进行概念抑制和安全增强，平衡擦除效果与生成质量


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法通常采用单向策略（抑制目标概念或强化安全替代），难以平衡概念移除与生成质量。需要一种能同时处理这两个方面的解决方案

Method: 提出双向图像引导概念擦除框架，包含两个解耦的图像分支：负分支负责抑制有害语义，正分支为安全替代提供视觉指导。使用基于掩码的过滤防止无关内容干扰

Result: 在广泛的实验评估中，Bi-Erasing在平衡概念移除效果和视觉保真度方面优于基线方法

Conclusion: 通过同时优化概念抑制和安全增强的互补方向，Bi-Erasing实现了擦除效果与生成可用性之间的平衡，为文本到图像模型的安全改进提供了有效方案

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [12] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: ADHint提出了一种基于难度的自适应提示方法，在强化学习中更好地平衡探索与模仿，提升推理能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的RL方法在调度提示比例和估计相对优势时忽略难度因素，导致学习不稳定和过度模仿离策略提示，需要更好的探索-模仿平衡。

Method: 1) 基于样本难度先验的自适应提示：评估样本难度并调度适当提示比例；2) 一致性梯度调制和选择性掩码：调制提示内令牌级梯度；3) 基于推出难度后验的优势估计：利用有/无提示推出的相对难度估计各自优势。

Result: 在多种模态、模型规模和领域上的实验表明，ADHint在推理能力和分布外泛化方面优于现有方法，在pass@1和avg@8指标上均表现更优。

Conclusion: 将难度作为关键因素融入提示比例调度和相对优势估计，能实现更好的探索-模仿平衡，提升模型的推理能力和泛化性能。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [13] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 本文提出了一种针对人脸检索系统的身份遗忘方法，通过分散目标身份在超球面上的嵌入表示，防止形成紧凑的身份簇，从而实现隐私保护，同时保持对其他身份的检索性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统虽然能学习高度区分性的身份簇以实现准确检索，但存在严重的隐私问题，可能被用于未经授权的身份追踪。现有机器遗忘方法在人脸检索场景下的适用性尚未充分探索，特别是对于现代基于嵌入的识别模型。

Method: 评估了多种现有的近似类别遗忘方法（如随机标记、梯度上升、边界遗忘等），并提出了一种简单有效的基于分散的遗忘方法。该方法通过分散目标身份在超球面上的嵌入表示，防止形成紧凑的身份簇，同时保持嵌入空间的区分性结构。

Result: 在标准基准测试（VGGFace2、CelebA）上的大量实验表明，该方法实现了优越的遗忘效果，同时保持了检索实用性。

Conclusion: 本文研究了人脸检索系统中的身份遗忘问题，提出了一种有效的分散式遗忘方法，能够在保护特定身份隐私的同时，保持系统对其他身份的检索性能，为人脸识别系统的隐私保护提供了新思路。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [14] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 提出MCT-UEG框架，通过元跨任务训练生成广泛不可利用的个人图像，提升实际应用性


<details>
  <summary>Details</summary>
Motivation: 现有不可利用示例生成方法在实际应用中存在局限性，无法生成跨不同真实世界计算机视觉任务都广泛不可利用的示例，需要提升方法的实际适用性

Method: 提出MCT-UEG框架，核心是设计面向平坦最小值的元训练和测试方案，优化不可利用示例生成器以有效产生广泛不可利用的示例

Result: 大量实验证明了该框架的有效性

Conclusion: 提出的MCT-UEG框架能够生成跨任务广泛不可利用的个人图像，提升了不可利用示例生成方法的实际应用价值

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [15] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

TL;DR: 提出H2IAD任务和IADNet模型，用于检测双人交互行为异常，通过TASM和DREM模块分别捕捉时空特征，在基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单人异常检测模型难以准确捕捉人类交互中的复杂非对称动态，而人类天生具有协作性，交互行为异常需要专门研究。

Method: 提出IADNet模型，包含Temporal Attention Sharing Module (TASM)共享运动编码以同步协作相关性，以及Distance-Based Relational Encoding Module (DREM)捕捉空间社交线索，最后使用normalizing flow进行异常评分。

Result: 在人类交互运动基准测试中，IADNet优于现有的人为中心异常检测基线方法。

Conclusion: 成功定义了H2IAD新任务并提出了有效的IADNet解决方案，通过时空特征建模显著提升了交互行为异常检测性能。

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Meta-Continual Mobility Forecasting for Proactive Handover Prediction](https://arxiv.org/abs/2512.11841)
*Sasi Vardhan Reddy Mandapati*

Main category: cs.LG

TL;DR: 提出轻量级元持续预测框架，用于蜂窝网络中的短期移动性预测，通过元初始化、残差检测和在线更新，在零样本和少样本场景下提升预测精度，并改善切换性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界移动性高度非平稳，存在突然转向、速度变化和不可预测用户行为，导致传统预测器漂移，引发切换时机错误或失败。

Method: 基于GRU的预测器，结合Reptile元初始化进行快速少样本适应，以及EWMA残差检测器在漂移发生时触发紧凑的在线更新。

Result: 在GeoLife和DeepMIMO数据集上，零样本设置下达到4.46m ADE和7.79m FDE，10样本少样本下ADE提升至3.71m，从突然漂移中恢复速度比离线GRU快2-3倍。应用于切换预测时，F1提升至0.83，AUROC达0.90，显著减少切换失败和乒乓事件。

Conclusion: 该方法轻量（128k参数），适用于5G/6G系统边缘部署，能有效应对移动性非平稳性，提升预测精度和切换性能。

Abstract: Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.

</details>


### [17] [GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search](https://arxiv.org/abs/2512.11856)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Zhi Yang,Weisheng Zhao,Chunming Hu*

Main category: cs.LG

TL;DR: GCoDE：首个面向图神经网络设备-边缘协同推理的自动框架，通过架构-映射协同设计和能量感知优化，实现高效部署


<details>
  <summary>Details</summary>
Motivation: GNN在边缘设备上的推理面临计算成本高、硬件资源有限的挑战，现有设备-边缘协同推理研究不足，传统模型划分方法对GNN效果不佳

Method: 提出GCoDE框架，将设备通信过程抽象为显式操作，在统一设计空间中联合优化架构和映射方案；引入能量预测方法提高评估精度；采用约束随机搜索策略寻找最优解

Result: 实验表明GCoDE相比现有方法最高可实现44.9倍加速和98.2%能耗降低，在1.5小时内找到平衡精度和效率的最优解

Conclusion: GCoDE是首个GNN设备-边缘协同推理自动框架，通过协同设计和系统性能感知，有效解决了边缘场景下GNN推理的效率和能耗问题

Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.

</details>


### [18] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 该论文证明经典的Robbins子高斯混合策略在确定性路径上满足遗憾界，为对抗性在线学习和博弈论统计之间架起桥梁


<details>
  <summary>Details</summary>
Motivation: 连接对抗性在线学习（通常处理有界数据的遗憾界）和博弈论统计（可以处理无界数据但需要随机性假设）两个领域，通过条件遗憾界作为随机和对抗性投注之间的桥梁

Method: 分析经典的Robbins子高斯混合策略，证明其在自然"Ville事件"E_α中的每条路径上都满足确定性遗憾界，其中V_T是非负非递减的累积方差过程

Result: 在Ville事件E_α中，遗憾界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T（乘以通用常数）；如果V_T ≥ ln(1/α)，则简化为ln(1/α) + ln ln V_T；在概率为1的事件E_0中，遗憾最终被ln ln V_T界住

Conclusion: 条件遗憾界可以作为随机和对抗性投注之间的桥梁，为处理无界数据提供了新的理论框架，连接了对抗性在线学习和博弈论统计两个领域

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [19] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

TL;DR: TianXing-S2S是一个多圈层耦合的概率模型，用于全球次季节到季节（S2S）的每日集合预报，在45天预报中超越了ECMWF和FuXi-S2S系统。


<details>
  <summary>Details</summary>
Motivation: 在气候变化加剧的背景下，准确的次季节到季节（S2S）极端事件预测对于资源规划和灾害缓解至关重要，但由于复杂的多圈层相互作用和内在的大气不确定性，这类预测仍然具有挑战性。

Method: TianXing-S2S首先将多样的多圈层预测因子编码到紧凑的潜在空间中，然后使用扩散模型生成每日集合预报。在去噪器中加入了一个基于最优传输（OT）的新型耦合模块，以优化大气和多圈层边界条件之间的相互作用。

Result: 在关键大气变量上，TianXing-S2S在1.5°分辨率的45天日均集合预报中，超越了欧洲中期天气预报中心（ECMWF）S2S系统和FuXi-S2S。该模型能够熟练预测极端事件（如热浪和异常降水），并识别土壤湿度作为关键的前兆信号。此外，模型能够生成长达180天的稳定滚动预报。

Conclusion: TianXing-S2S建立了一个稳健的S2S研究框架，为变暖世界中的次季节到季节预测提供了有效的解决方案，展示了多圈层耦合和扩散模型在改进长期天气预报方面的潜力。

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [20] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

TL;DR: DynaGen是一个统一的时间知识图谱推理方法，通过动态构建实体中心子图处理插值任务，使用条件扩散过程处理外推任务，在六个基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间知识图谱推理方法面临两个关键挑战：插值方法中上下文建模有限，外推方法中存在认知泛化偏差。需要一种统一的方法同时解决这两个问题。

Method: DynaGen采用统一框架：对于插值任务，动态构建实体中心子图，使用协同双分支GNN编码器捕捉演化结构上下文；对于外推任务，应用条件扩散过程，迫使模型学习底层演化原理而非表面模式。

Result: 在六个基准数据集上的实验表明，DynaGen达到最先进性能。相比次优模型，平均MRR分数在插值任务上提升2.61分，在外推任务上提升1.45分。

Conclusion: DynaGen通过动态子图构建和条件扩散过程，有效解决了时间知识图谱推理中的上下文建模和泛化偏差问题，为插值和外推任务提供了统一的解决方案。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [21] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

TL;DR: 提出WAY深度学习架构，通过重构AIS轨迹为嵌套序列结构，结合通道聚合序列处理块和梯度丢弃技术，实现提前数天至数周的船舶目的地预测


<details>
  <summary>Details</summary>
Motivation: AIS系统虽然支持数据驱动的海事监控，但存在可靠性问题和数据间隔不规则的问题，需要解决船舶目的地预测的挑战

Method: 1) 将长港口到港口轨迹重构为嵌套序列结构；2) 提出WAY深度学习架构，包含轨迹表示层和通道聚合序列处理块；3) 引入梯度丢弃技术解决单标签多对多训练问题

Result: 在5年AIS数据上的实验表明，WAY优于传统基于空间网格的方法，且梯度丢弃技术带来了性能提升，同时展示了ETA估计的多任务学习应用潜力

Conclusion: WAY架构通过创新的轨迹表示和训练技术，有效解决了长时船舶目的地预测问题，为实际海事监控应用提供了可行方案

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [22] [Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks](https://arxiv.org/abs/2512.13340)
*Henrik C. M. Frederiksen,Junya Shiraishi,Cedomir Stefanovic,Hei Victor Cheng,Shashi Raj Pandey*

Main category: cs.LG

TL;DR: 提出事件驱动的通信框架，将持续学习整合到物联网网络中，用于节能的故障检测


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源有限，部署的轻量级机器学习模型会因环境非平稳性和初始训练数据有限而导致推理精度下降。传统更新方法会消耗额外能量，不适合能量受限的物联网设备。

Method: 提出事件驱动的通信框架，使物联网设备和边缘服务器能够根据无线链路条件和可用能量预算，协作更新轻量级机器学习模型。

Result: 在真实数据集上的评估显示，该方法在推理召回率上优于周期性采样和非自适应持续学习方法，在严格的能量和带宽约束下实现了高达42.8%的改进。

Conclusion: 该框架通过智能整合持续学习和自适应通信策略，为物联网网络中的节能故障检测提供了有效解决方案。

Abstract: The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.

</details>


### [23] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: DPUL提出了一种新的服务器端联邦遗忘方法，通过深度遗忘所有有影响的权重来防止隐私泄露，相比现有方法在准确性和时间成本上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在高计算需求、复杂激励机制和客户端计算能力差异等问题，导致时间长、成本高。同时，现有服务器端知识蒸馏方法仅移除目标客户端的更新，忽略了其他客户端贡献中嵌入的隐私，可能导致隐私泄露。

Method: DPUL包含三个组件：(1)通过过滤客户端更新幅度识别高权重参数，并将其回滚以确保深度移除；(2)利用变分自编码器(VAE)重建和消除低权重参数；(3)使用基于投影的技术恢复模型。

Result: 在四个数据集上的实验结果表明，DPUL超越了最先进的基线方法，准确率提高了1%-5%，时间成本降低了高达12倍。

Conclusion: DPUL提供了一种有效的服务器端联邦遗忘方法，能够深度遗忘所有有影响的权重，有效防止隐私泄露，同时在准确性和效率方面都有显著提升。

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp](https://arxiv.org/abs/2512.12048)
*Muddsair Sharif,Huseyin Seker*

Main category: cs.AI

TL;DR: 提出CAMAC-DRA框架，通过多智能体协调优化智能电动汽车充电生态系统，平衡五个利益相关方，在真实数据验证中表现优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 智能电动汽车充电生态系统需要协调多个利益相关方（用户、电网运营商、充电站运营商、车队运营商、环境因素），同时适应动态环境条件，现有方法难以平衡这些竞争目标。

Method: 提出上下文敏感的多智能体协调框架，结合协调深度Q网络、图神经网络和注意力机制，处理20个上下文特征，通过加权协调机制和共识协议平衡五个利益相关方。

Result: 在441,077个真实充电交易数据集上验证，相比DDPG、A3C、PPO和GNN等基线算法，实现92%协调成功率、15%能效提升、10%成本降低、20%电网压力减少、2.3倍更快收敛，同时保持88%训练稳定性和85%样本效率。

Conclusion: CAMAC-DRA框架成功开发了上下文感知的多利益相关方协调系统，有效平衡竞争目标并适应实时变量，是智能电动汽车充电协调和可持续交通电气化的突破性解决方案。

Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.

</details>


### [25] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

TL;DR: 研究者分析了AI模型文档的碎片化和不一致问题，开发了加权透明度框架和自动化评估流程，发现前沿实验室合规率约80%，但大多数提供商低于60%，安全关键类别存在最大披露缺口。


<details>
  <summary>Details</summary>
Motivation: AI模型文档分散在不同平台且结构不一致，阻碍政策制定者、审计者和用户可靠评估安全声明、数据来源和版本变更，需要系统化评估框架来解决透明度问题。

Method: 分析了5个前沿模型和100个Hugging Face模型卡，识别出947个独特章节名称；基于欧盟AI法案和斯坦福透明度指数开发了8个章节23个子章节的加权透明度框架；实现了自动化多智能体管道，从公开源提取文档并通过LLM共识评分完整性。

Result: 评估50个模型（视觉、多模态、开源和闭源）总成本低于3美元；前沿实验室（xAI、微软、Anthropic）合规率约80%，大多数提供商低于60%；安全关键类别缺陷最大：欺骗行为、幻觉和儿童安全评估分别损失148、124和116总分。

Conclusion: AI模型文档存在系统性透明度缺口，特别是安全关键领域；自动化评估框架能高效识别披露不足；需要标准化文档格式和强制安全披露要求来改善透明度。

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [26] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu,Zhi Chen,Yuanguang Zhong*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在供应链管理等高风险决策中会复制并放大人类认知偏差，GPT-4等复杂模型因"过度思考"表现出最大非理性，而效率优化的GPT-4o表现接近最优。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地应用于商业决策，但其复制和放大人类认知偏差的风险尚未得到充分理解，特别是在供应链管理等高风险运营环境中。研究旨在识别LLMs认知偏差的性质和来源。

Method: 使用动态多轮实验，在经典报童问题中测试GPT-4、GPT-4o和LLaMA-8B模型，检验五种已确立的决策偏差。分析模型在提供最优公式情况下的表现，以区分知识差距和架构约束。

Result: LLMs一致复制了经典的"过低/过高"订购偏差，并显著放大了需求追逐等行为。发现"智能悖论"：更复杂的GPT-4因过度思考表现出最大非理性，而效率优化的GPT-4o表现接近最优。即使在提供最优公式时偏差仍存在，表明这些偏差源于架构约束而非知识差距。

Conclusion: 管理者应根据具体任务选择模型，效率优化模型在某些优化问题上可能优于复杂模型。LLMs显著放大偏差凸显了高风险决策中需要强有力的人工监督。设计结构化、基于规则的提示是约束模型启发式倾向、提高AI辅助决策可靠性的有效策略。

Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [27] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: WebOperator：一个结合安全回溯和战略探索的树搜索框架，用于提升LLM智能体在部分可观察的Web环境中的表现


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在Web环境中采用贪婪的逐步行动策略，缺乏长期规划能力，且无法有效处理错误或探索替代路径。树搜索方法虽然提供了结构化探索框架，但缺乏安全回溯机制，且假设所有动作都可逆，这在现实的Web任务中不成立。

Method: WebOperator框架包含：1）最佳优先搜索策略，根据奖励估计和安全性对动作排序；2）稳健的回溯机制，在重放路径前验证可行性以防止副作用；3）从多个不同推理上下文生成动作候选，确保探索多样性；4）通过预执行过滤无效动作和合并语义等价动作来优化动作集。

Result: 在WebArena和WebVoyager上的实验表明，WebOperator在WebArena上使用gpt-4o达到了54.6%的最先进成功率，证明了战略远见与安全执行相结合的关键优势。

Conclusion: WebOperator通过集成安全回溯和战略探索，有效解决了LLM智能体在部分可观察Web环境中的长期规划问题，显著提升了任务成功率。

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [28] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: 论文提出M-GRPO框架和IQR自适应过滤方法，解决自监督强化学习中长期训练时的策略崩溃问题，提升大语言模型推理能力的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督强化学习方法在长期训练中存在"策略崩溃"问题，性能急剧下降。即使增加rollout数量也只能延迟而非防止崩溃，需要新的稳定训练方法。

Method: 1. M-GRPO：利用缓慢演化的动量模型提供稳定训练目标；2. IQR自适应过滤：基于四分位距动态修剪低熵轨迹，保持策略多样性。

Result: 在多个推理基准测试中，M-GRPO稳定了训练过程，IQR过滤防止了过早收敛，两者结合实现了卓越的训练稳定性和最先进的性能。

Conclusion: 通过M-GRPO框架和IQR自适应过滤方法，有效解决了自监督强化学习中的策略崩溃问题，为大语言模型推理能力的稳定训练提供了有效解决方案。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [29] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu,Jun Ma,Waishan Qiu,Cui Guo*

Main category: cs.CL

TL;DR: VPR-AttLLM是一个模型无关的框架，通过将大语言模型的语义推理和地理空间知识集成到现有视觉地点识别管道中，提升社交媒体街景图像的地理定位性能，特别针对洪水等危机事件图像。


<details>
  <summary>Details</summary>
Motivation: 社交媒体提供的街景图像虽然能实时反映城市洪水等危机事件，但缺乏可靠的地理元数据。现有的视觉地点识别模型在处理这类图像时，由于视觉失真和跨源场景的领域偏移，性能显著下降。

Method: 提出VPR-AttLLM框架，通过注意力引导的描述符增强，将大语言模型的语义推理和地理空间知识集成到现有VPR管道中。利用LLM识别城市背景中的位置信息区域并抑制瞬态视觉噪声，无需模型重新训练或额外数据。

Result: 在扩展基准测试中，将VPR-AttLLM与三种最先进的VPR模型（CosPlace、EigenPlaces、SALAD）集成，召回性能持续提升，相对增益通常在1-3%之间，在最具挑战性的真实洪水图像上可达8%。

Conclusion: VPR-AttLLM为LLM引导的多模态融合在视觉检索系统中建立了可推广的范式，将城市感知理论原理嵌入注意力机制，连接了类人空间推理与现代VPR架构。其即插即用设计、强大的跨源鲁棒性和可解释性，展示了在可扩展城市监测和危机图像快速地理定位方面的潜力。

Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [30] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg,Koshi Eguchi,Takuya Akiba,Edoardo Cetin*

Main category: cs.CL

TL;DR: DroPE方法通过训练后丢弃位置嵌入，实现零样本上下文扩展，无需长上下文微调


<details>
  <summary>Details</summary>
Motivation: 传统方法需要昂贵的微调才能扩展语言模型的上下文长度，位置嵌入在训练时很重要但限制了测试时对未见长度序列的泛化能力

Method: 提出DroPE方法：在预训练后丢弃位置嵌入，经过短暂重新校准阶段，使模型适应无位置信息的状态

Result: DroPE实现了无缝的零样本上下文扩展，无需长上下文微调，在不同模型和数据集规模上表现优异，远超现有位置嵌入缩放方法

Conclusion: 位置嵌入不是语言建模的内在要求，可以在预训练后安全移除，DroPE方法打破了扩展上下文长度的关键瓶颈

Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [31] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang,Jianjun Li,Mingqian Ding,Chang Liu,Xinchi Zou,Wenqi Yang*

Main category: cs.CL

TL;DR: 提出SCIR框架和MBSC数据集，通过自校正迭代优化降低LLM信息抽取的训练成本，同时提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的信息抽取系统面临两个主要限制：高训练成本和难以与LLM偏好对齐

Method: 提出自校正迭代优化（SCIR）框架，包含双路径自校正模块和反馈驱动优化；创建多任务双语自校正（MBSC）数据集，包含10万+条目，间接蒸馏GPT-4能力

Result: 在命名实体识别、关系抽取和事件抽取三个任务上超越现有方法，span-based Micro-F1平均提升5.27%，训练成本降低87%

Conclusion: SCIR框架增强了信息抽取系统的灵活性和准确性，为轻量高效的信息抽取范式铺平道路

Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [32] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: HyperEdit：基于超网络的动态适应和差异感知正则化，用于指令驱动的文本编辑任务，在修改区域上比现有方法提升9%-30% BLEU分数


<details>
  <summary>Details</summary>
Motivation: 指令驱动的文本编辑在实际应用中越来越重要（如代码编辑器Cursor），但大语言模型在此任务上表现不佳。编辑任务需要忠实执行用户指令同时保留未修改内容，现有方法将编辑视为通用文本生成，导致两个关键问题：难以忠实对齐多样用户意图，以及经常过度编辑未改变区域。

Method: 提出HyperEdit方法：1）基于超网络的动态适应，生成请求特定的参数，使模型能针对每个指令定制编辑策略；2）差异感知正则化，将监督聚焦在修改的文本片段上，防止过度编辑同时确保精确的最小化修改。

Result: HyperEdit在修改区域上比最先进的基线方法相对提升了9%-30%的BLEU分数，尽管只使用了30亿参数。

Conclusion: HyperEdit通过动态适应和差异感知正则化有效解决了指令驱动文本编辑中的两个关键挑战，显著提升了编辑质量和忠实度。

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [33] [Integrating Causal Reasoning into Automated Fact-Checking](https://arxiv.org/abs/2512.13286)
*Youssra Rebboud,Pasquale Lisena,Raphael Troncy*

Main category: cs.CL

TL;DR: 提出结合事件关系提取、语义相似度计算和基于规则推理的方法，检测声明与证据间事件链的逻辑不一致性，为事实核查建立首个细粒度因果事件关系基线。


<details>
  <summary>Details</summary>
Motivation: 当前自动化事实核查方法缺乏专门的因果推理能力，可能错失语义丰富的可解释性机会。拒绝声明的一个常见原因是检测事件间错误的因果关系。

Method: 结合事件关系提取、语义相似度计算和基于规则的推理，检测声明中事件链与证据中事件链之间的逻辑不一致性。

Result: 在两个事实核查数据集上评估，该方法为将细粒度因果事件关系集成到事实核查中建立了首个基线，并增强了裁决预测的可解释性。

Conclusion: 提出的方法填补了事实核查中因果推理的空白，通过整合因果事件关系提高了事实核查的准确性和可解释性，为未来研究奠定了基础。

Abstract: In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.

</details>


### [34] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

Main category: cs.CL

TL;DR: 提出非解析推理(NRR)框架，通过多向量嵌入、非坍缩注意力和上下文身份追踪来防止语言模型过早语义坍缩，将语义歧义作为显式表示状态而非故障模式。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在"过早语义坍缩"的核心架构限制，即softmax竞争和贪婪解码迫使模型在获得足够上下文前就承诺单一语义，导致推理脆弱和上下文失败。

Method: 引入非解析推理(NRR)框架，包含三个组件：1)多向量嵌入保持每个token的多个可行解释；2)非坍缩注意力防止跨层的赢家通吃动态；3)上下文身份追踪为重复实体分配上下文特定身份。这些机制通过外部解析算子ρ统一，使语义承诺变得显式、可控且任务相关。

Result: 合成评估显示NRR能有效保持歧义和追踪上下文：CIT增强模型在分布外身份转移任务上达到90.9%准确率，而Transformer基线仅为9.1%。

Conclusion: NRR为过早语义坍缩提供了原则性替代方案，将歧义重新定义为显式表示状态而非故障模式。关键问题不是AI是否应该解析歧义，而是何时、如何以及在谁的控制下进行解析。

Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [35] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu,Nam Nguyen,Yinzhu Quan,Austin Zhang*

Main category: cs.CL

TL;DR: 本文首次对事件序列的时间标记化进行了实证研究，比较了五种编码策略在不同统计分布数据集上的表现，发现没有单一最优策略，性能取决于标记器与数据统计特性的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 连续时间表示是大型语言模型建模时间事件序列的关键且未被充分探索的挑战。现有策略如字节级表示或日历标记等，但最优方法仍不明确，特别是考虑到真实世界事件数据的多样化统计分布（从平滑的对数正态分布到离散的尖峰模式）。

Method: 比较了五种不同的时间编码策略：朴素数字字符串、高精度字节级表示、人类语义日历标记、经典均匀分箱和自适应残差标量量化。通过在体现这些多样化分布的真实世界数据集上微调LLMs来评估这些策略。

Result: 分析表明没有单一策略是普遍最优的；预测性能很大程度上取决于标记器与数据统计特性的对齐程度。基于对数的策略在偏斜分布上表现优异，而人类中心格式在混合模态上表现出鲁棒性。

Conclusion: 时间标记化策略的选择应基于数据的统计特性，而不是寻求通用解决方案。对数策略适合偏斜分布，人类语义格式适合混合模态，强调了数据驱动方法在时间表示中的重要性。

Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


### [36] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

Main category: cs.CL

TL;DR: 评估四种消除大语言模型安全对齐拒绝机制的工具（Heretic、DECCP、ErisForge、FailSpy）在16个指令调优模型上的效果，发现单次通过方法在能力保留上表现更好，数学推理能力对消除干预最为敏感。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全对齐机制通过学习的拒绝行为防止对有害查询的响应，但这些机制也阻碍了认知建模、对抗测试和安全分析等合法研究应用。虽然消除技术可以通过方向正交化手术式移除拒绝表示，但现有实现方法的相对有效性尚未得到充分评估。

Method: 评估四种消除工具（Heretic、DECCP、ErisForge、FailSpy）在16个指令调优模型（7B-14B参数）上的兼容性和效果。报告所有16个模型的工具兼容性，并根据工具支持情况在子集上报告定量指标。比较单次通过方法和贝叶斯优化消除方法的效果。

Result: 单次通过方法在基准子集上表现出更好的能力保留（三个模型的平均GSM8K变化：ErisForge -0.28个百分点；DECCP -0.13个百分点）。贝叶斯优化消除产生可变的分布偏移（KL散度：0.043-1.646），能力影响因模型而异。数学推理能力对消除干预最为敏感，GSM8K变化范围从+1.51个百分点到-18.81个百分点（相对变化-26.5%）。

Conclusion: 该研究为研究人员提供了基于证据的消除工具选择标准，适用于不同模型架构。主要发现表明数学推理能力对消除干预最为敏感，工具选择和模型架构会显著影响能力保留效果。

Abstract: Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [37] [VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer](https://arxiv.org/abs/2512.11891)
*Songqiao Hu,Zeyi Liu,Shuang Liu,Jun Cen,Zihan Meng,Xiao He*

Main category: cs.RO

TL;DR: 提出AEGIS架构，通过控制屏障函数为现有VLA模型添加安全约束层，在保持任务性能的同时提升安全性，并在新基准SafeLIBERO上验证有效性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作任务中表现出色，但在非结构化环境中部署时面临安全挑战，特别是需要同时保证任务完成和避免碰撞。现有方法缺乏理论保证的安全机制。

Method: 提出VLSA架构AEGIS，包含基于控制屏障函数的安全约束层，可即插即用式集成到现有VLA模型中，提供理论安全保证。

Result: 在构建的SafeLIBERO基准测试中，AEGIS相比SOTA基线在障碍物避免率上提升59.16%，任务执行成功率提升17.25%，同时保持原有指令跟随性能。

Conclusion: AEGIS架构成功解决了VLA模型在非结构化环境中的安全问题，通过理论保证的安全约束层实现了任务性能与安全性的平衡，为实际部署提供了可行方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.

</details>


### [38] [Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving](https://arxiv.org/abs/2512.12211)
*Longchao Da,David Isele,Hua Wei,Manish Saroya*

Main category: cs.RO

TL;DR: 提出一个自适应评估轨迹预测器性能的管道，通过准确性和多样性两个维度动态评估，比传统误差指标更能反映预测器对自动驾驶车辆驾驶性能的实际贡献。


<details>
  <summary>Details</summary>
Motivation: 当前轨迹预测评估主要依赖ADE、FDE等误差指标，这些指标只关注事后准确性，忽略了预测器对自动驾驶车辆在复杂交互场景中的实际影响。高质量的预测器不仅需要准确性，还应捕捉周围智能体所有可能的运动方向，以支持自动驾驶车辆的谨慎决策。

Method: 提出一个综合评估管道，通过两个维度自适应评估预测器性能：1) 准确性；2) 多样性。根据驾驶场景的关键程度，动态结合这两个维度，生成预测器性能的最终评分。

Result: 在基于真实世界数据集的闭环基准测试上进行广泛实验，结果表明该评估管道比传统指标提供更合理的评估，能更好地反映预测器评估与自动驾驶车辆驾驶性能之间的相关性。

Conclusion: 该评估管道提供了一种稳健的方法来选择最能提升自动驾驶车辆驾驶性能的预测器，强调了在复杂交互场景中同时考虑预测准确性和多样性的重要性。

Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.

</details>


### [39] [Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models](https://arxiv.org/abs/2512.12427)
*Rudolf Reiter,Chao Qin,Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: Unique提出了一种统一MPC框架，通过级联不同保真度模型（短时高保真+长时低保真）来解决四旋翼无人机同时需要即时反应和长时规划的矛盾，在相同计算预算下显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 四旋翼无人机任务需要同时具备即时反应能力和长时规划能力。高保真模型控制精确但计算量大，无法用于长时规划；低保真模型可扩展但闭环性能差。现有方法无法在计算效率和性能间取得平衡。

Method: 1. 统一MPC框架：在单个优化中级联不同保真度模型（短时高保真模型用于精确控制，长时低保真模型用于规划）
2. 跨时域成本对齐和约束设计：包括可行性保持的推力和体速率约束、状态匹配约束、推力诱导加速度和急动-体速率关系约束
3. 3D渐进平滑调度：沿时域变形基于范数的障碍物以避免局部极小值
4. 并行随机初始化MPC求解器：在长低保真时域上发现更低成本的局部极小值

Result: 在仿真和真实飞行中，在相同计算预算下，Unique相比标准MPC和分层规划器-跟踪器基线，将闭环位置或速度跟踪性能提升高达75%。消融研究和帕累托分析证实了该方法在不同时域变化、约束近似和平滑调度下的鲁棒性增益。

Conclusion: Unique通过级联不同保真度模型的统一MPC框架，成功解决了四旋翼无人机任务中即时反应和长时规划的矛盾，在有限计算资源下实现了显著的性能提升，为复杂空中任务提供了有效的解决方案。

Abstract: Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.

</details>
