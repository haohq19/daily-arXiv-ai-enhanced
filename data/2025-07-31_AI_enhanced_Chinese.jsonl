{"id": "2507.22264", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22264", "abs": "https://arxiv.org/abs/2507.22264", "authors": ["Shaoan Xie", "Lingjing Kong", "Yujia Zheng", "Yu Yao", "Zeyu Tang", "Eric P. Xing", "Guangyi Chen", "Kun Zhang"], "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees", "comment": "CVPR2025", "summary": "Contrastive Language-Image Pre-training (CLIP)~\\citep{radford2021learning}\nhas emerged as a pivotal model in computer vision and multimodal learning,\nachieving state-of-the-art performance at aligning visual and textual\nrepresentations through contrastive learning. However, CLIP struggles with\npotential information misalignment in many image-text datasets and suffers from\nentangled representation. On the one hand, short captions for a single image in\ndatasets like MSCOCO may describe disjoint regions in the image, leaving the\nmodel uncertain about which visual features to retain or disregard. On the\nother hand, directly aligning long captions with images can lead to the\nretention of entangled details, preventing the model from learning\ndisentangled, atomic concepts -- ultimately limiting its generalization on\ncertain downstream tasks involving short prompts.\n  In this paper, we establish theoretical conditions that enable flexible\nalignment between textual and visual representations across varying levels of\ngranularity. Specifically, our framework ensures that a model can not only\n\\emph{preserve} cross-modal semantic information in its entirety but also\n\\emph{disentangle} visual representations to capture fine-grained textual\nconcepts. Building on this foundation, we introduce \\ours, a novel approach\nthat identifies and aligns the most relevant visual and textual representations\nin a modular manner. Superior performance across various tasks demonstrates its\ncapability to handle information misalignment and supports our identification\ntheory. The code is available at https://github.com/Mid-Push/SmartCLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5SmartCLIP\uff0c\u89e3\u51b3\u4e86CLIP\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u9f50\u548c\u8868\u793a\u7ea0\u7f20\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u5f0f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u4e2d\u5b58\u5728\u4fe1\u606f\u4e0d\u5bf9\u9f50\u548c\u8868\u793a\u7ea0\u7f20\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u67d0\u4e9b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u6761\u4ef6\uff0c\u786e\u4fdd\u8de8\u6a21\u6001\u8bed\u4e49\u4fe1\u606f\u7684\u5b8c\u6574\u4fdd\u7559\u548c\u89c6\u89c9\u8868\u793a\u7684\u7ec6\u7c92\u5ea6\u89e3\u8026\uff0c\u5e76\u5f15\u5165SmartCLIP\u65b9\u6cd5\u8fdb\u884c\u6a21\u5757\u5316\u5bf9\u9f50\u3002", "result": "SmartCLIP\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u5904\u7406\u4fe1\u606f\u4e0d\u5bf9\u9f50\u7684\u80fd\u529b\u548c\u652f\u6301\u7684\u7406\u8bba\u3002", "conclusion": "SmartCLIP\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u89e3\u8026\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.22769", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.22769", "abs": "https://arxiv.org/abs/2507.22769", "authors": ["Satyesh Shanker Awasthi", "Mohammed Irshadh Ismaaeel Sathyamangalam Imran", "Stefano Arrigoni", "Francesco Braghin"], "title": "Bayesian Optimization applied for accelerated Virtual Validation of the Autonomous Driving Function", "comment": null, "summary": "Rigorous Verification and Validation (V&V) of Autonomous Driving Functions\n(ADFs) is paramount for ensuring the safety and public acceptance of Autonomous\nVehicles (AVs). Current validation relies heavily on simulation to achieve\nsufficient test coverage within the Operational Design Domain (ODD) of a\nvehicle, but exhaustively exploring the vast parameter space of possible\nscenarios is computationally expensive and time-consuming. This work introduces\na framework based on Bayesian Optimization (BO) to accelerate the discovery of\ncritical scenarios. We demonstrate the effectiveness of the framework on an\nModel Predictive Controller (MPC)-based motion planner, showing that it\nidentifies hazardous situations, such as off-road events, using orders of\nmagnitude fewer simulations than brute-force Design of Experiments (DoE)\nmethods. Furthermore, this study investigates the scalability of the framework\nin higher-dimensional parameter spaces and its ability to identify multiple,\ndistinct critical regions within the ODD of the motion planner used as the case\nstudy .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\uff08ADF\uff09\u9a8c\u8bc1\u4e2d\u7684\u5173\u952e\u573a\u666f\u53d1\u73b0\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u7684\u9a8c\u8bc1\u4e3b\u8981\u4f9d\u8d56\u4eff\u771f\uff0c\u4f46\u5168\u9762\u63a2\u7d22\u53c2\u6570\u7a7a\u95f4\u8017\u65f6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08MPC\uff09\u4e3a\u57fa\u7840\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u4e3a\u4f8b\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u4ee5\u8fdc\u4f4e\u4e8e\u66b4\u529b\u5b9e\u9a8c\u8bbe\u8ba1\uff08DoE\uff09\u65b9\u6cd5\u7684\u4eff\u771f\u6b21\u6570\u8bc6\u522b\u5371\u9669\u573a\u666f\uff08\u5982\u504f\u79bb\u9053\u8def\u4e8b\u4ef6\uff09\uff0c\u5e76\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u5c55\u73b0\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u6548\u7387\uff0c\u5e76\u80fd\u8bc6\u522b\u8fd0\u52a8\u89c4\u5212\u5668\u64cd\u4f5c\u8bbe\u8ba1\u57df\uff08ODD\uff09\u4e2d\u7684\u591a\u4e2a\u5173\u952e\u533a\u57df\u3002"}}
{"id": "2507.22299", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22299", "abs": "https://arxiv.org/abs/2507.22299", "authors": ["Afonso Martini Spezia", "Mariana Recamonde-Mendoza"], "title": "Comparing Cluster-Based Cross-Validation Strategies for Machine Learning Model Evaluation", "comment": null, "summary": "Cross-validation plays a fundamental role in Machine Learning, enabling\nrobust evaluation of model performance and preventing overestimation on\ntraining and validation data. However, one of its drawbacks is the potential to\ncreate data subsets (folds) that do not adequately represent the diversity of\nthe original dataset, which can lead to biased performance estimates. The\nobjective of this work is to deepen the investigation of cluster-based\ncross-validation strategies by analyzing the performance of different\nclustering algorithms through experimental comparison. Additionally, a new\ncross-validation technique that combines Mini Batch K-Means with class\nstratification is proposed. Experiments were conducted on 20 datasets (both\nbalanced and imbalanced) using four supervised learning algorithms, comparing\ncross-validation strategies in terms of bias, variance, and computational cost.\nThe technique that uses Mini Batch K-Means with class stratification\noutperformed others in terms of bias and variance on balanced datasets, though\nit did not significantly reduce computational cost. On imbalanced datasets,\ntraditional stratified cross-validation consistently performed better, showing\nlower bias, variance, and computational cost, making it a safe choice for\nperformance evaluation in scenarios with class imbalance. In the comparison of\ndifferent clustering algorithms, no single algorithm consistently stood out as\nsuperior. Overall, this work contributes to improving predictive model\nevaluation strategies by providing a deeper understanding of the potential of\ncluster-based data splitting techniques and reaffirming the effectiveness of\nwell-established strategies like stratified cross-validation. Moreover, it\nhighlights perspectives for increasing the robustness and reliability of model\nevaluations, especially in datasets with clustering characteristics.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u805a\u7c7b\u7684\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Mini Batch K-Means\u548c\u7c7b\u5206\u5c42\u7684\u65b0\u6280\u672f\uff0c\u5e76\u572820\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6280\u672f\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u4f20\u7edf\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u66f4\u4f18\u3002", "motivation": "\u4ea4\u53c9\u9a8c\u8bc1\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u5b50\u96c6\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u539f\u59cb\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u4ea7\u751f\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u805a\u7c7b\u7684\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Mini Batch K-Means\u548c\u7c7b\u5206\u5c42\u7684\u65b0\u4ea4\u53c9\u9a8c\u8bc1\u6280\u672f\uff0c\u5e76\u572820\u4e2a\u6570\u636e\u96c6\uff08\u5e73\u8861\u548c\u4e0d\u5e73\u8861\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7b56\u7565\u7684\u504f\u5dee\u3001\u65b9\u5dee\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u6280\u672f\u7684\u504f\u5dee\u548c\u65b9\u5dee\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4f46\u672a\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u4f20\u7edf\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u8868\u73b0\u66f4\u4f18\u3002\u805a\u7c7b\u7b97\u6cd5\u6bd4\u8f83\u4e2d\uff0c\u65e0\u5355\u4e00\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6df1\u5165\u7814\u7a76\u57fa\u4e8e\u805a\u7c7b\u7684\u6570\u636e\u5206\u5272\u6280\u672f\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u8bc4\u4f30\u7b56\u7565\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u4f20\u7edf\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u3002\u7814\u7a76\u4e3a\u63d0\u5347\u6a21\u578b\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2307.10803", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2307.10803", "abs": "https://arxiv.org/abs/2307.10803", "authors": ["Hanchen Yang", "Wengen Li", "Shuyu Wang", "Hui Li", "Jihong Guan", "Shuigeng Zhou", "Jiannong Cao"], "title": "Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities", "comment": null, "summary": "With the rapid amassing of spatial-temporal (ST) ocean data, many\nspatial-temporal data mining (STDM) studies have been conducted to address\nvarious oceanic issues, including climate forecasting and disaster warning.\nCompared with typical ST data (e.g., traffic data), ST ocean data is more\ncomplicated but with unique characteristics, e.g., diverse regionality and high\nsparsity. These characteristics make it difficult to design and train STDM\nmodels on ST ocean data. To the best of our knowledge, a comprehensive survey\nof existing studies remains missing in the literature, which hinders not only\ncomputer scientists from identifying the research issues in ocean data mining\nbut also ocean scientists to apply advanced STDM techniques. In this paper, we\nprovide a comprehensive survey of existing STDM studies for ocean science.\nConcretely, we first review the widely-used ST ocean datasets and highlight\ntheir unique characteristics. Then, typical ST ocean data quality enhancement\ntechniques are explored. Next, we classify existing STDM studies in ocean\nscience into four types of tasks, i.e., prediction, event detection, pattern\nmining, and anomaly detection, and elaborate on the techniques for these tasks.\nFinally, promising research opportunities are discussed. This survey can help\nscientists from both computer science and ocean science better understand the\nfundamental concepts, key techniques, and open challenges of STDM for ocean\nscience.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u65f6\u7a7a\u6d77\u6d0b\u6570\u636e\u6316\u6398\uff08STDM\uff09\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u6570\u636e\u8d28\u91cf\u589e\u5f3a\u6280\u672f\u3001\u4efb\u52a1\u5206\u7c7b\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65f6\u7a7a\u6d77\u6d0b\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u72ec\u7279\u6027\uff08\u5982\u533a\u57df\u591a\u6837\u6027\u548c\u9ad8\u7a00\u758f\u6027\uff09\u4f7f\u5f97STDM\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u7efc\u8ff0\uff0c\u963b\u788d\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u548c\u6d77\u6d0b\u79d1\u5b66\u5bb6\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u3002", "method": "\u9996\u5148\u56de\u987e\u5e38\u7528ST\u6d77\u6d0b\u6570\u636e\u96c6\u53ca\u5176\u7279\u6027\uff0c\u63a2\u8ba8\u6570\u636e\u8d28\u91cf\u589e\u5f3a\u6280\u672f\uff0c\u5c06\u73b0\u6709STDM\u7814\u7a76\u5206\u4e3a\u9884\u6d4b\u3001\u4e8b\u4ef6\u68c0\u6d4b\u3001\u6a21\u5f0f\u6316\u6398\u548c\u5f02\u5e38\u68c0\u6d4b\u56db\u7c7b\u4efb\u52a1\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u76f8\u5173\u6280\u672f\u3002", "result": "\u63d0\u4f9b\u4e86STDM\u5728\u6d77\u6d0b\u79d1\u5b66\u4e2d\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u5e2e\u52a9\u8de8\u9886\u57df\u79d1\u5b66\u5bb6\u7406\u89e3\u57fa\u672c\u6982\u5ff5\u3001\u5173\u952e\u6280\u672f\u548c\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86STDM\u5728\u6d77\u6d0b\u79d1\u5b66\u9886\u57df\u7684\u7efc\u8ff0\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.22393", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22393", "abs": "https://arxiv.org/abs/2507.22393", "authors": ["Anubhav Kataria", "Surbhi Madan", "Shreya Ghosh", "Tom Gedeon", "Abhinav Dhall"], "title": "Gems: Group Emotion Profiling Through Multimodal Situational Understanding", "comment": null, "summary": "Understanding individual, group and event level emotions along with\ncontextual information is crucial for analyzing a multi-person social\nsituation. To achieve this, we frame emotion comprehension as the task of\npredicting fine-grained individual emotion to coarse grained group and event\nlevel emotion. We introduce GEMS that leverages a multimodal swin-transformer\nand S3Attention based architecture, which processes an input scene, group\nmembers, and context information to generate joint predictions. Existing\nmulti-person emotion related benchmarks mainly focus on atomic interactions\nprimarily based on emotion perception over time and group level. To this end,\nwe extend and propose VGAF-GEMS to provide more fine grained and holistic\nanalysis on top of existing group level annotation of VGAF dataset. GEMS aims\nto predict basic discrete and continuous emotions (including valence and\narousal) as well as individual, group and event level perceived emotions. Our\nbenchmarking effort links individual, group and situational emotional responses\nholistically. The quantitative and qualitative comparisons with adapted\nstate-of-the-art models demonstrate the effectiveness of GEMS framework on\nVGAF-GEMS benchmarking. We believe that it will pave the way of further\nresearch. The code and data is available at:\nhttps://github.com/katariaak579/GEMS", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGEMS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001Swin-Transformer\u548cS3Attention\u67b6\u6784\uff0c\u9884\u6d4b\u7ec6\u7c92\u5ea6\u4e2a\u4f53\u60c5\u7eea\u5230\u7c97\u7c92\u5ea6\u7fa4\u4f53\u548c\u4e8b\u4ef6\u7ea7\u60c5\u7eea\uff0c\u5e76\u5728VGAF-GEMS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u4e2a\u4f53\u3001\u7fa4\u4f53\u548c\u4e8b\u4ef6\u7ea7\u60c5\u7eea\u53ca\u5176\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u5206\u6790\u591a\u4eba\u793e\u4ea4\u573a\u666f\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u539f\u5b50\u4ea4\u4e92\u6216\u7fa4\u4f53\u7ea7\u60c5\u7eea\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001Swin-Transformer\u548cS3Attention\u67b6\u6784\uff0c\u7ed3\u5408\u573a\u666f\u3001\u7fa4\u4f53\u6210\u5458\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8054\u5408\u9884\u6d4b\u79bb\u6563\u548c\u8fde\u7eed\u60c5\u7eea\uff08\u5982\u6548\u4ef7\u548c\u5524\u9192\u5ea6\uff09\u3002", "result": "\u5728VGAF-GEMS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGEMS\u6846\u67b6\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "GEMS\u6846\u67b6\u4e3a\u60c5\u7eea\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.22410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22410", "abs": "https://arxiv.org/abs/2507.22410", "authors": ["Xiaocheng Yang", "Sumuk Shashidhar", "Dilek Hakkani-Tur"], "title": "Question Generation for Assessing Early Literacy Reading Comprehension", "comment": "2 pages, 1 figure, accepted by SLaTE 2025", "summary": "Assessment of reading comprehension through content-based interactions plays\nan important role in the reading acquisition process. In this paper, we propose\na novel approach for generating comprehension questions geared to K-2 English\nlearners. Our method ensures complete coverage of the underlying material and\nadaptation to the learner's specific proficiencies, and can generate a large\ndiversity of question types at various difficulty levels to ensure a thorough\nevaluation. We evaluate the performance of various language models in this\nframework using the FairytaleQA dataset as the source material. Eventually, the\nproposed approach has the potential to become an important part of autonomous\nAI-driven English instructors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3aK-2\u82f1\u8bed\u5b66\u4e60\u8005\u751f\u6210\u9605\u8bfb\u7406\u89e3\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u786e\u4fdd\u8986\u76d6\u6750\u6599\u5e76\u9002\u5e94\u5b66\u4e60\u8005\u6c34\u5e73\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5728FairytaleQA\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30\u9605\u8bfb\u7406\u89e3\u5bf9\u9605\u8bfb\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u5b66\u4e60\u8005\u6c34\u5e73\u5e76\u5168\u9762\u8986\u76d6\u6750\u6599\u7684\u95ee\u9898\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u591a\u6837\u5316\u3001\u96be\u5ea6\u53ef\u8c03\u7684\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u8986\u76d6\u6750\u6599\u5e76\u9002\u5e94\u5b66\u4e60\u8005\u6c34\u5e73\uff0c\u4f7f\u7528FairytaleQA\u6570\u636e\u96c6\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u6837\u5316\u95ee\u9898\u5e76\u9002\u5e94\u5b66\u4e60\u8005\u6c34\u5e73\uff0c\u5728FairytaleQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u6210\u4e3a\u81ea\u4e3bAI\u82f1\u8bed\u6559\u5b66\u5de5\u5177\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2507.22445", "categories": ["cs.CL", "cs.AI", "H.1.2; I.2.4; I.2.0; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22445", "abs": "https://arxiv.org/abs/2507.22445", "authors": ["Jill Walker Rettberg", "Hermann Wigers"], "title": "AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini", "comment": "This project has received funding from the European Union's Horizon\n  2020 research and innovation programme under grant agreement number\n  101142306. The project is also supported by the Center for Digital Narrative,\n  which is funded by the Research Council of Norway through its Centres of\n  Excellence scheme, project number 332643", "summary": "Can a language model trained largely on Anglo-American texts generate stories\nthat are culturally relevant to other nationalities? To find out, we generated\n11,800 stories - 50 for each of 236 countries - by sending the prompt \"Write a\n1500 word potential {demonym} story\" to OpenAI's model gpt-4o-mini. Although\nthe stories do include surface-level national symbols and themes, they\noverwhelmingly conform to a single narrative plot structure across countries: a\nprotagonist lives in or returns home to a small town and resolves a minor\nconflict by reconnecting with tradition and organising community events.\nReal-world conflicts are sanitised, romance is almost absent, and narrative\ntension is downplayed in favour of nostalgia and reconciliation. The result is\na narrative homogenisation: an AI-generated synthetic imaginary that\nprioritises stability above change and tradition above growth. We argue that\nthe structural homogeneity of AI-generated narratives constitutes a distinct\nform of AI bias, a narrative standardisation that should be acknowledged\nalongside the more familiar representational bias. These findings are relevant\nto literary studies, narratology, critical AI studies, NLP research, and\nefforts to improve the cultural alignment of generative AI.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u82f1\u7f8e\u6587\u672c\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6545\u4e8b\u867d\u5305\u542b\u8868\u9762\u6587\u5316\u7b26\u53f7\uff0c\u4f46\u53d9\u4e8b\u7ed3\u6784\u5355\u4e00\uff0c\u7f3a\u4e4f\u6587\u5316\u6df1\u5ea6\u548c\u591a\u6837\u6027\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u751f\u6210\u5177\u6709\u6587\u5316\u76f8\u5173\u6027\u7684\u6545\u4e8b\uff0c\u63ed\u793aAI\u751f\u6210\u53d9\u4e8b\u7684\u6f5c\u5728\u504f\u89c1\u3002", "method": "\u4f7f\u7528GPT-4o-mini\u4e3a236\u4e2a\u56fd\u5bb6\u5404\u751f\u621050\u4e2a\u6545\u4e8b\uff0c\u5206\u6790\u53d9\u4e8b\u7ed3\u6784\u548c\u5185\u5bb9\u3002", "result": "\u6545\u4e8b\u53d9\u4e8b\u7ed3\u6784\u9ad8\u5ea6\u540c\u8d28\u5316\uff0c\u5f3a\u8c03\u4f20\u7edf\u4e0e\u7a33\u5b9a\uff0c\u7f3a\u4e4f\u51b2\u7a81\u548c\u591a\u6837\u6027\u3002", "conclusion": "AI\u751f\u6210\u53d9\u4e8b\u7684\u7ed3\u6784\u540c\u8d28\u5316\u662f\u4e00\u79cd\u65b0\u578b\u504f\u89c1\uff0c\u9700\u5728\u6587\u5316\u5bf9\u9f50\u548cAI\u7814\u7a76\u4e2d\u5f15\u8d77\u91cd\u89c6\u3002"}}
{"id": "2507.22412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22412", "abs": "https://arxiv.org/abs/2507.22412", "authors": ["Sijie Wang", "Siqi Li", "Yawei Zhang", "Shangshu Yu", "Shenghai Yuan", "Rui She", "Quanjiang Guo", "JinXuan Zheng", "Ong Kang Howe", "Leonrich Chandra", "Shrivarshann Srijeyan", "Aditya Sivadas", "Toshan Aggarwal", "Heyuan Liu", "Hongming Zhang", "Chujie Chen", "Junyu Jiang", "Lihua Xie", "Wee Peng Tay"], "title": "UAVScenes: A Multi-Modal Dataset for UAVs", "comment": "Accepted by ICCV 2025", "summary": "Multi-modal perception is essential for unmanned aerial vehicle (UAV)\noperations, as it enables a comprehensive understanding of the UAVs'\nsurrounding environment. However, most existing multi-modal UAV datasets are\nprimarily biased toward localization and 3D reconstruction tasks, or only\nsupport map-level semantic segmentation due to the lack of frame-wise\nannotations for both camera images and LiDAR point clouds. This limitation\nprevents them from being used for high-level scene understanding tasks. To\naddress this gap and advance multi-modal UAV perception, we introduce\nUAVScenes, a large-scale dataset designed to benchmark various tasks across\nboth 2D and 3D modalities. Our benchmark dataset is built upon the\nwell-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only\nfor simultaneous localization and mapping (SLAM). We enhance this dataset by\nproviding manually labeled semantic annotations for both frame-wise images and\nLiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.\nThese additions enable a wide range of UAV perception tasks, including\nsegmentation, depth estimation, 6-DoF localization, place recognition, and\nnovel view synthesis (NVS). Our dataset is available at\nhttps://github.com/sijieaaa/UAVScenes", "AI": {"tldr": "UAVScenes\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u9ad8\u7ea7\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u5e27\u7ea7\u6807\u6ce8\u548c6-DoF\u4f4d\u59ff\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u6570\u636e\u96c6\u591a\u504f\u5411\u5b9a\u4f4d\u548c3D\u91cd\u5efa\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5e27\u7ea7\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u9ad8\u7ea7\u573a\u666f\u7406\u89e3\u4efb\u52a1\u7684\u5e94\u7528\u3002", "method": "\u57fa\u4e8eMARS-LVIG\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u624b\u52a8\u6807\u6ce8\u56fe\u50cf\u548cLiDAR\u70b9\u4e91\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u6dfb\u52a06-DoF\u4f4d\u59ff\u3002", "result": "UAVScenes\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff0c\u5982\u5206\u5272\u3001\u6df1\u5ea6\u4f30\u8ba1\u3001\u5b9a\u4f4d\u3001\u573a\u666f\u8bc6\u522b\u548c\u65b0\u89c6\u89d2\u5408\u6210\u3002", "conclusion": "UAVScenes\u586b\u8865\u4e86\u591a\u6a21\u6001\u65e0\u4eba\u673a\u611f\u77e5\u7684\u7a7a\u767d\uff0c\u4e3a\u9ad8\u7ea7\u573a\u666f\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2507.22438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22438", "abs": "https://arxiv.org/abs/2507.22438", "authors": ["Youngho Kim", "Hoonhee Cho", "Kuk-Jin Yoon"], "title": "From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras", "comment": null, "summary": "Human pose estimation is critical for applications such as rehabilitation,\nsports analytics, and AR/VR systems. However, rapid motion and low-light\nconditions often introduce motion blur, significantly degrading pose estimation\ndue to the domain gap between sharp and blurred images. Most datasets assume\nstable conditions, making models trained on sharp images struggle in blurred\nenvironments. To address this, we introduce a novel domain adaptation approach\nthat leverages event cameras, which capture high temporal resolution motion\ndata and are inherently robust to motion blur. Using event-based augmentation,\nwe generate motion-aware blurred images, effectively bridging the domain gap\nbetween sharp and blurred domains without requiring paired annotations.\nAdditionally, we develop a student-teacher framework that iteratively refines\npseudo-labels, leveraging mutual uncertainty masking to eliminate incorrect\nlabels and enable more effective learning. Experimental results demonstrate\nthat our approach outperforms conventional domain-adaptive human pose\nestimation methods, achieving robust pose estimation under motion blur without\nrequiring annotations in the target domain. Our findings highlight the\npotential of event cameras as a scalable and effective solution for domain\nadaptation in real-world motion blur environments. Our project codes are\navailable at https://github.com/kmax2001/EvSharp2Blur.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u57df\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5728\u8fd0\u52a8\u6a21\u7cca\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u901a\u8fc7\u4e8b\u4ef6\u589e\u5f3a\u548c\u5e08\u751f\u6846\u67b6\uff0c\u65e0\u9700\u76ee\u6807\u57df\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u8fd0\u52a8\u6a21\u7cca\u548c\u4f4e\u5149\u6761\u4ef6\u5bfc\u81f4\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5047\u8bbe\u7a33\u5b9a\u6761\u4ef6\uff0c\u6a21\u578b\u5728\u6a21\u7cca\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u751f\u6210\u8fd0\u52a8\u611f\u77e5\u6a21\u7cca\u56fe\u50cf\uff0c\u901a\u8fc7\u5e08\u751f\u6846\u67b6\u548c\u4e92\u4e0d\u786e\u5b9a\u6027\u63a9\u7801\u8fed\u4ee3\u4f18\u5316\u4f2a\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u65e0\u9700\u76ee\u6807\u57df\u6807\u6ce8\u5373\u53ef\u5728\u8fd0\u52a8\u6a21\u7cca\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u4e3a\u771f\u5b9e\u4e16\u754c\u8fd0\u52a8\u6a21\u7cca\u73af\u5883\u4e2d\u7684\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22459", "abs": "https://arxiv.org/abs/2507.22459", "authors": ["Jaeha Kim", "Junghun Oh", "Kyoung Mu Lee"], "title": "Exploiting Diffusion Prior for Task-driven Image Restoration", "comment": "Accepted to ICCV 2025", "summary": "Task-driven image restoration (TDIR) has recently emerged to address\nperformance drops in high-level vision tasks caused by low-quality (LQ) inputs.\nPrevious TDIR methods struggle to handle practical scenarios in which images\nare degraded by multiple complex factors, leaving minimal clues for\nrestoration. This motivates us to leverage the diffusion prior, one of the most\npowerful natural image priors. However, while the diffusion prior can help\ngenerate visually plausible results, using it to restore task-relevant details\nremains challenging, even when combined with recent TDIR methods. To address\nthis, we propose EDTR, which effectively harnesses the power of diffusion prior\nto restore task-relevant details. Specifically, we propose directly leveraging\nuseful clues from LQ images in the diffusion process by generating from\npixel-error-based pre-restored LQ images with mild noise added. Moreover, we\nemploy a small number of denoising steps to prevent the generation of redundant\ndetails that dilute crucial task-related information. We demonstrate that our\nmethod effectively utilizes diffusion prior for TDIR, significantly enhancing\ntask performance and visual quality across diverse tasks with multiple complex\ndegradations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEDTR\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u5148\u9a8c\u6062\u590d\u4efb\u52a1\u76f8\u5173\u7ec6\u8282\uff0c\u63d0\u5347\u591a\u590d\u6742\u9000\u5316\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u6027\u80fd\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4efb\u52a1\u9a71\u52a8\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5728\u591a\u590d\u6742\u9000\u5316\u573a\u666f\u4e0b\u96be\u4ee5\u6062\u590d\u4efb\u52a1\u76f8\u5173\u7ec6\u8282\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faEDTR\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u50cf\u7d20\u8bef\u5dee\u9884\u6062\u590d\u7684\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e2d\u751f\u6210\u5e76\u6dfb\u52a0\u8f7b\u5fae\u566a\u58f0\uff0c\u76f4\u63a5\u5229\u7528\u6269\u6563\u5148\u9a8c\u6062\u590d\u4efb\u52a1\u76f8\u5173\u7ec6\u8282\uff0c\u540c\u65f6\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u4ee5\u907f\u514d\u5197\u4f59\u4fe1\u606f\u3002", "result": "EDTR\u663e\u8457\u63d0\u5347\u4e86\u591a\u590d\u6742\u9000\u5316\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u6027\u80fd\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "EDTR\u6709\u6548\u5229\u7528\u6269\u6563\u5148\u9a8c\uff0c\u4e3a\u4efb\u52a1\u9a71\u52a8\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.22499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22499", "abs": "https://arxiv.org/abs/2507.22499", "authors": ["Xiang Li", "Qianli Shen", "Haonan Wang", "Kenji Kawaguchi"], "title": "LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning", "comment": "23 pages", "summary": "Recent generative models face significant risks of producing harmful content,\nwhich has underscored the importance of machine unlearning (MU) as a critical\ntechnique for eliminating the influence of undesired data. However, existing MU\nmethods typically assign the same weight to all data to be forgotten, which\nmakes it difficult to effectively forget certain data that is harder to unlearn\nthan others. In this paper, we empirically demonstrate that the loss of data\nitself can implicitly reflect its varying difficulty. Building on this insight,\nwe introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective\nplug-and-play strategy that dynamically reweights data during the unlearning\nprocess with minimal additional computational overhead. Our approach\nsignificantly reduces the gap between existing MU methods and exact unlearning\nin both image classification and generation tasks, effectively enhancing the\nprevention of harmful content generation in text-to-image diffusion models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u635f\u5931\u91cd\u52a0\u6743\u7684\u65b9\u6cd5\uff08LoReUn\uff09\uff0c\u52a8\u6001\u8c03\u6574\u9057\u5fd8\u6570\u636e\u7684\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5bf9\u6240\u6709\u9057\u5fd8\u6570\u636e\u8d4b\u4e88\u76f8\u540c\u6743\u91cd\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u96be\u9057\u5fd8\u6570\u636e\u3002", "method": "\u5229\u7528\u6570\u636e\u635f\u5931\u53cd\u6620\u9057\u5fd8\u96be\u5ea6\uff0c\u63d0\u51fa\u52a8\u6001\u91cd\u52a0\u6743\u7b56\u7565LoReUn\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u7f29\u5c0f\u4e0e\u7cbe\u786e\u9057\u5fd8\u7684\u5dee\u8ddd\uff0c\u6709\u6548\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "conclusion": "LoReUn\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u63d2\u62d4\u5f0f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u9057\u5fd8\u6548\u679c\u3002"}}
{"id": "2507.22524", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.22524", "abs": "https://arxiv.org/abs/2507.22524", "authors": ["Fang Wang", "Paolo Ceravolo", "Ernesto Damiani"], "title": "HGCN(O): A Self-Tuning GCN HyperModel Toolkit for Outcome Prediction in Event-Sequence Data", "comment": "15 pages, 2 figures, submitted to Knowledge-Base Systems", "summary": "We propose HGCN(O), a self-tuning toolkit using Graph Convolutional Network\n(GCN) models for event sequence prediction. Featuring four GCN architectures\n(O-GCN, T-GCN, TP-GCN, TE-GCN) across the GCNConv and GraphConv layers, our\ntoolkit integrates multiple graph representations of event sequences with\ndifferent choices of node- and graph-level attributes and in temporal\ndependencies via edge weights, optimising prediction accuracy and stability for\nbalanced and unbalanced datasets. Extensive experiments show that GCNConv\nmodels excel on unbalanced data, while all models perform consistently on\nbalanced data. Experiments also confirm the superior performance of HGCN(O)\nover traditional approaches. Applications include Predictive Business Process\nMonitoring (PBPM), which predicts future events or states of a business process\nbased on event logs.", "AI": {"tldr": "HGCN(O)\u662f\u4e00\u4e2a\u57fa\u4e8eGCN\u7684\u81ea\u9002\u5e94\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u4e8b\u4ef6\u5e8f\u5217\u9884\u6d4b\uff0c\u5305\u542b\u56db\u79cdGCN\u67b6\u6784\uff0c\u4f18\u5316\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u4e8b\u4ef6\u5e8f\u5217\u9884\u6d4b\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u548c\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408\u591a\u79cd\u56fe\u8868\u793a\u548c\u8282\u70b9/\u56fe\u7ea7\u5c5e\u6027\uff0c\u901a\u8fc7\u8fb9\u6743\u91cd\u5f15\u5165\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "GCNConv\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6240\u6709\u6a21\u578b\u5728\u5e73\u8861\u6570\u636e\u4e0a\u8868\u73b0\u4e00\u81f4\uff0cHGCN(O)\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "HGCN(O)\u5728\u4e8b\u4ef6\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8ePBPM\u7b49\u5e94\u7528\u3002"}}
{"id": "2507.22798", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22798", "abs": "https://arxiv.org/abs/2507.22798", "authors": ["Michael C. Burkhart", "Bashar Ramadan", "Luke Solo", "William F. Parker", "Brett K. Beaulieu-Jones"], "title": "Quantifying surprise in clinical care: Detecting highly informative events in electronic health records with foundation models", "comment": null, "summary": "We present a foundation model-derived method to identify highly informative\ntokens and events in electronic health records. Our approach considers incoming\ndata in the entire context of a patient's hospitalization and so can flag\nanomalous events that rule-based approaches would consider within a normal\nrange. We demonstrate that the events our model flags are significant for\npredicting downstream patient outcomes and that a fraction of events identified\nas carrying little information can safely be dropped. Additionally, we show how\ninformativeness can help interpret the predictions of prognostic models trained\non foundation model-derived representations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u9ad8\u4fe1\u606f\u91cf\u6807\u8bb0\u548c\u4e8b\u4ef6\uff0c\u5e76\u9a8c\u8bc1\u5176\u9884\u6d4b\u60a3\u8005\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u5f02\u5e38\u4e8b\u4ef6\uff0c\u800c\u65b0\u65b9\u6cd5\u901a\u8fc7\u5168\u9762\u8003\u8651\u60a3\u8005\u4f4f\u9662\u6570\u636e\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u8fd9\u4e9b\u4e8b\u4ef6\u3002", "method": "\u5229\u7528\u57fa\u7840\u6a21\u578b\u5206\u6790\u60a3\u8005\u4f4f\u9662\u6570\u636e\u7684\u4e0a\u4e0b\u6587\uff0c\u8bc6\u522b\u9ad8\u4fe1\u606f\u91cf\u4e8b\u4ef6\uff0c\u5e76\u9a8c\u8bc1\u5176\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u8bc6\u522b\u7684\u4e8b\u4ef6\u5bf9\u9884\u6d4b\u60a3\u8005\u7ed3\u679c\u6709\u663e\u8457\u610f\u4e49\uff0c\u4e14\u90e8\u5206\u4f4e\u4fe1\u606f\u91cf\u4e8b\u4ef6\u53ef\u5b89\u5168\u5ffd\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u5e2e\u52a9\u89e3\u91ca\u9884\u540e\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u3002"}}
{"id": "2507.22733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22733", "abs": "https://arxiv.org/abs/2507.22733", "authors": ["Hang Su", "Yunlong Feng", "Daniel Gehrig", "Panfeng Jiang", "Ling Gao", "Xavier Lagorce", "Laurent Kneip"], "title": "A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks", "comment": null, "summary": "Structure and continuous motion estimation from point correspondences is a\nfundamental problem in computer vision that has been powered by well-known\nalgorithms such as the familiar 5-point or 8-point algorithm. However, despite\ntheir acclaim, these algorithms are limited to processing point correspondences\noriginating from a pair of views each one representing an instantaneous capture\nof the scene. Yet, in the case of rolling shutter cameras, or more recently,\nevent cameras, this synchronization breaks down. In this work, we present a\nunified approach for structure and linear motion estimation from 2D point\ncorrespondences with arbitrary timestamps, from an arbitrary set of views. By\nformulating the problem in terms of first-order dynamics and leveraging a\nconstant velocity motion model, we derive a novel, linear point incidence\nrelation allowing for the efficient recovery of both linear velocity and 3D\npoints with predictable degeneracies and solution multiplicities. Owing to its\ngeneral formulation, it can handle correspondences from a wide range of sensing\nmodalities such as global shutter, rolling shutter, and event cameras, and can\neven combine correspondences from different collocated sensors. We validate the\neffectiveness of our solver on both simulated and real-world data, where we\nshow consistent improvement across all modalities when compared to recent\napproaches. We believe our work opens the door to efficient structure and\nmotion estimation from asynchronous data. Code can be found at\nhttps://github.com/suhang99/AsyncTrack-Motion-Solver.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4efb\u610f\u65f6\u95f4\u6233\u76842D\u70b9\u5bf9\u5e94\u5173\u7cfb\u4e2d\u4f30\u8ba1\u7ed3\u6784\u548c\u7ebf\u6027\u8fd0\u52a8\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u76f8\u673a\u7c7b\u578b\u3002", "motivation": "\u4f20\u7edf\u7b97\u6cd5\uff08\u59825\u70b9\u62168\u70b9\u7b97\u6cd5\uff09\u4ec5\u9002\u7528\u4e8e\u540c\u6b65\u89c6\u56fe\uff0c\u800c\u6eda\u52a8\u5feb\u95e8\u76f8\u673a\u548c\u4e8b\u4ef6\u76f8\u673a\u7b49\u5f02\u6b65\u6570\u636e\u573a\u666f\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u4e00\u9636\u52a8\u529b\u5b66\u548c\u6052\u5b9a\u901f\u5ea6\u8fd0\u52a8\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u7ebf\u6027\u70b9\u5165\u5c04\u5173\u7cfb\uff0c\u9ad8\u6548\u6062\u590d\u7ebf\u6027\u901f\u5ea6\u548c3D\u70b9\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u6240\u6709\u6a21\u6001\u4e0b\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f02\u6b65\u6570\u636e\u7684\u9ad8\u6548\u7ed3\u6784\u548c\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.22828", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22828", "abs": "https://arxiv.org/abs/2507.22828", "authors": ["Kedong Xiu", "Saiqian Zhang"], "title": "CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models", "comment": "9 pages, accepted by the 2025 ACM Multimedia Conference", "summary": "As Vision-Language Models (VLMs) are increasingly deployed in split-DNN\nconfigurations--with visual encoders (e.g., ResNet, ViT) operating on user\ndevices and sending intermediate features to the cloud--there is a growing\nprivacy risk from semantic information leakage. Existing approaches to\nreconstructing images from these intermediate features often result in blurry,\nsemantically ambiguous images. To directly address semantic leakage, we propose\nCapRecover, a cross-modality inversion framework that recovers high-level\nsemantic content, such as labels or captions, directly from intermediate\nfeatures without image reconstruction.\n  We evaluate CapRecover on multiple datasets and victim models, demonstrating\nstrong performance in semantic recovery. Specifically, CapRecover achieves up\nto 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from\nResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis\nfurther reveals that deeper convolutional layers encode significantly more\nsemantic information compared to shallow layers. To mitigate semantic leakage,\nwe introduce a simple yet effective protection method: adding random noise to\nintermediate features at each layer and removing the noise in the next layer.\nExperimental results show that this approach prevents semantic leakage without\nadditional training costs.", "AI": {"tldr": "CapRecover\u662f\u4e00\u79cd\u8de8\u6a21\u6001\u53cd\u6f14\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u4e2d\u95f4\u7279\u5f81\u6062\u590d\u9ad8\u7ea7\u8bed\u4e49\u5185\u5bb9\uff08\u5982\u6807\u7b7e\u6216\u63cf\u8ff0\uff09\uff0c\u65e0\u9700\u56fe\u50cf\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5206\u5272DNN\u914d\u7f6e\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u4e2d\u95f4\u7279\u5f81\u7684\u8bed\u4e49\u4fe1\u606f\u6cc4\u6f0f\u5e26\u6765\u9690\u79c1\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u91cd\u5efa\u7684\u56fe\u50cf\u6a21\u7cca\u4e14\u8bed\u4e49\u6a21\u7cca\uff0c\u65e0\u6cd5\u76f4\u63a5\u89e3\u51b3\u8bed\u4e49\u6cc4\u6f0f\u95ee\u9898\u3002", "method": "\u63d0\u51faCapRecover\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u4e2d\u95f4\u7279\u5f81\u6062\u590d\u8bed\u4e49\u5185\u5bb9\uff08\u5982\u6807\u7b7e\u6216\u63cf\u8ff0\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e00\u79cd\u4fdd\u62a4\u65b9\u6cd5\uff1a\u5728\u6bcf\u5c42\u4e2d\u95f4\u7279\u5f81\u6dfb\u52a0\u968f\u673a\u566a\u58f0\u5e76\u5728\u4e0b\u4e00\u5c42\u53bb\u9664\u3002", "result": "CapRecover\u5728CIFAR-10\u4e0a\u8fbe\u523092.71%\u7684Top-1\u6807\u7b7e\u51c6\u786e\u7387\uff0c\u5728COCO2017\u4e0a\u751f\u6210\u6d41\u7545\u63cf\u8ff0\uff08ROUGE-L\u5f97\u52060.52\uff09\u3002\u4fdd\u62a4\u65b9\u6cd5\u6709\u6548\u9632\u6b62\u8bed\u4e49\u6cc4\u6f0f\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "CapRecover\u80fd\u9ad8\u6548\u6062\u590d\u8bed\u4e49\u5185\u5bb9\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u4fdd\u62a4\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
