{"id": "2509.09775", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09775", "abs": "https://arxiv.org/abs/2509.09775", "authors": ["Aleksandr Boldachev"], "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture", "comment": "22 pages, 6 figures", "summary": "This paper presents boldsea, Boldachev's semantic-event approach -- an\narchitecture for modeling complex dynamic systems using executable ontologies\n-- semantic models that act as dynamic structures, directly controlling process\nexecution. We demonstrate that integrating event semantics with a dataflow\narchitecture addresses the limitations of traditional Business Process\nManagement (BPM) systems and object-oriented semantic technologies. The paper\npresents the formal BSL (boldsea Semantic Language), including its BNF grammar,\nand outlines the boldsea-engine's architecture, which directly interprets\nsemantic models as executable algorithms without compilation. It enables the\nmodification of event models at runtime, ensures temporal transparency, and\nseamlessly merges data and business logic within a unified semantic framework.", "AI": {"tldr": "boldsea\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u4e8b\u4ef6\u65b9\u6cd5\u7684\u67b6\u6784\uff0c\u4f7f\u7528\u53ef\u6267\u884c\u672c\u4f53\u6765\u5efa\u6a21\u590d\u6742\u52a8\u6001\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfBPM\u7cfb\u7edf\u548c\u9762\u5411\u5bf9\u8c61\u8bed\u4e49\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406(BPM)\u7cfb\u7edf\u548c\u9762\u5411\u5bf9\u8c61\u8bed\u4e49\u6280\u672f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u5b9e\u65f6\u7684\u8fc7\u7a0b\u6267\u884c\u63a7\u5236\u3002", "method": "\u63d0\u51faboldsea\u8bed\u4e49\u8bed\u8a00(BSL)\u53ca\u5176BNF\u8bed\u6cd5\uff0c\u8bbe\u8ba1boldsea-engine\u67b6\u6784\uff0c\u76f4\u63a5\u89e3\u91ca\u8bed\u4e49\u6a21\u578b\u4f5c\u4e3a\u53ef\u6267\u884c\u7b97\u6cd5\uff0c\u65e0\u9700\u7f16\u8bd1\u3002", "result": "\u5b9e\u73b0\u4e86\u8fd0\u884c\u65f6\u4e8b\u4ef6\u6a21\u578b\u4fee\u6539\u3001\u65f6\u95f4\u900f\u660e\u6027\uff0c\u5e76\u5728\u7edf\u4e00\u7684\u8bed\u4e49\u6846\u67b6\u5185\u65e0\u7f1d\u878d\u5408\u6570\u636e\u548c\u4e1a\u52a1\u903b\u8f91\u3002", "conclusion": "boldsea\u67b6\u6784\u901a\u8fc7\u96c6\u6210\u4e8b\u4ef6\u8bed\u4e49\u548c\u6570\u636e\u6d41\u67b6\u6784\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\u3002"}}
{"id": "2509.09730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09730", "abs": "https://arxiv.org/abs/2509.09730", "authors": ["Kaikai Zhao", "Zhaoxiang Liu", "Peng Wang", "Xin Wang", "Zhicheng Ma", "Yajun Xu", "Wenjing Zhang", "Yibing Nan", "Kai Wang", "Shiguo Lian"], "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance", "comment": "accepted by Image and Vision Computing", "summary": "General-domain large multimodal models (LMMs) have achieved significant\nadvances in various image-text tasks. However, their performance in the\nIntelligent Traffic Surveillance (ITS) domain remains limited due to the\nabsence of dedicated multimodal datasets. To address this gap, we introduce\nMITS (Multimodal Intelligent Traffic Surveillance), the first large-scale\nmultimodal benchmark dataset specifically designed for ITS. MITS includes\n170,400 independently collected real-world ITS images sourced from traffic\nsurveillance cameras, annotated with eight main categories and 24 subcategories\nof ITS-specific objects and events under diverse environmental conditions.\nAdditionally, through a systematic data generation pipeline, we generate\nhigh-quality image captions and 5 million instruction-following visual\nquestion-answer pairs, addressing five critical ITS tasks: object and event\nrecognition, object counting, object localization, background analysis, and\nevent reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream\nLMMs on this dataset, enabling the development of ITS-specific applications.\nExperimental results show that MITS significantly improves LMM performance in\nITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905\n(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to\n0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the\ndataset, code, and models as open-source, providing high-value resources to\nadvance both ITS and LMM research.", "AI": {"tldr": "MITS\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u4ea4\u901a\u76d1\u63a7\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b17\u4e07\u5f20\u771f\u5b9e\u4ea4\u901a\u76d1\u63a7\u56fe\u50cf\u548c500\u4e07\u6761\u6307\u4ee4\u95ee\u7b54\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u4ea4\u901a\u76d1\u63a7\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u5927\u6a21\u578b\u5728\u667a\u80fd\u4ea4\u901a\u76d1\u63a7\u9886\u57df\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u4e13\u95e8\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u9488\u5bf9\u4ea4\u901a\u76d1\u63a7\u573a\u666f\u7684\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c617\u4e07\u5f20\u771f\u5b9e\u4ea4\u901a\u76d1\u63a7\u56fe\u50cf\uff0c\u6807\u6ce88\u5927\u7c7b24\u5c0f\u7c7b\u4ea4\u901a\u5bf9\u8c61\u548c\u4e8b\u4ef6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6570\u636e\u751f\u6210\u6d41\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u63cf\u8ff0\u548c500\u4e07\u6761\u6307\u4ee4\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d65\u4e2a\u5173\u952e\u4ea4\u901a\u76d1\u63a7\u4efb\u52a1\u3002", "result": "\u5728MITS\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0c\u4e3b\u6d41\u5927\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1aLLaVA-1.5\u4ece0.494\u63d0\u5347\u52300.905(+83.2%)\uff0cLLaVA-1.6\u4ece0.678\u52300.921(+35.8%)\uff0cQwen2-VL\u4ece0.584\u52300.926(+58.6%)\uff0cQwen2.5-VL\u4ece0.732\u52300.930(+27.0%)\u3002", "conclusion": "MITS\u6570\u636e\u96c6\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4ea4\u901a\u76d1\u63a7\u9886\u57df\u7684\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u6027\u80fd\uff0c\u4e3aITS\u548cLMM\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u4ef7\u503c\u8d44\u6e90\u3002"}}
{"id": "2509.09953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09953", "abs": "https://arxiv.org/abs/2509.09953", "authors": ["Mahfuzul I. Nissan", "Sharmin Aktar"], "title": "Detection of Anomalous Behavior in Robot Systems Based on Machine Learning", "comment": null, "summary": "Ensuring the safe and reliable operation of robotic systems is paramount to\nprevent potential disasters and safeguard human well-being. Despite rigorous\ndesign and engineering practices, these systems can still experience\nmalfunctions, leading to safety risks. In this study, we present a machine\nlearning-based approach for detecting anomalies in system logs to enhance the\nsafety and reliability of robotic systems. We collected logs from two distinct\nscenarios using CoppeliaSim and comparatively evaluated several machine\nlearning models, including Logistic Regression (LR), Support Vector Machine\n(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context\n(Context 1) and a Pioneer robot context (Context 2). Results showed that while\nLR demonstrated superior performance in Context 1, the Autoencoder model proved\nto be the most effective in Context 2. This highlights that the optimal model\nchoice is context-dependent, likely due to the varying complexity of anomalies\nacross different robotic platforms. This research underscores the value of a\ncomparative approach and demonstrates the particular strengths of autoencoders\nfor detecting complex anomalies in robotic systems.", "AI": {"tldr": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u591a\u79cd\u6a21\u578b\u5728\u4e0d\u540c\u673a\u5668\u4eba\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6700\u4f18\u6a21\u578b\u9009\u62e9\u5177\u6709\u573a\u666f\u4f9d\u8d56\u6027\uff0c\u81ea\u7f16\u7801\u5668\u5728\u590d\u6742\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u5c3d\u7ba1\u6709\u4e25\u683c\u7684\u8bbe\u8ba1\u548c\u5de5\u7a0b\u5b9e\u8df5\uff0c\u7cfb\u7edf\u4ecd\u53ef\u80fd\u53d1\u751f\u6545\u969c\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u6765\u589e\u5f3a\u5b89\u5168\u6027", "method": "\u4f7f\u7528CoppeliaSim\u6536\u96c6\u4e24\u4e2a\u4e0d\u540c\u573a\u666f\uff08\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u548cPioneer\u673a\u5668\u4eba\uff09\u7684\u7cfb\u7edf\u65e5\u5fd7\uff0c\u6bd4\u8f83\u8bc4\u4f30\u4e86\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u81ea\u7f16\u7801\u5668\u7b49\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b", "result": "\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u903b\u8f91\u56de\u5f52\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5728Pioneer\u673a\u5668\u4eba\u573a\u666f\u4e2d\u81ea\u7f16\u7801\u5668\u6548\u679c\u6700\u597d\uff0c\u8868\u660e\u6700\u4f18\u6a21\u578b\u9009\u62e9\u53d6\u51b3\u4e8e\u5177\u4f53\u573a\u666f\uff0c\u53ef\u80fd\u7531\u4e8e\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5f02\u5e38\u590d\u6742\u6027\u5dee\u5f02", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6bd4\u8f83\u65b9\u6cd5\u7684\u4ef7\u503c\uff0c\u5e76\u8bc1\u660e\u4e86\u81ea\u7f16\u7801\u5668\u5728\u68c0\u6d4b\u673a\u5668\u4eba\u7cfb\u7edf\u590d\u6742\u5f02\u5e38\u65b9\u9762\u7684\u7279\u6b8a\u4f18\u52bf\uff0c\u6a21\u578b\u9009\u62e9\u9700\u8981\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u8fdb\u884c\u4f18\u5316"}}
{"id": "2509.09772", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09772", "abs": "https://arxiv.org/abs/2509.09772", "authors": ["Sanjay Basu", "Sadiq Y. Patel", "Parth Sheth", "Bhairavi Muralidharan", "Namrata Elamaran", "Aakriti Kinra", "Rajaie Batniji"], "title": "Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management", "comment": "10 pages, 5 figures, 4 tables", "summary": "Population health management programs for Medicaid populations coordinate\nlongitudinal outreach and services (e.g., benefits navigation, behavioral\nhealth, social needs support, and clinical scheduling) and must be safe, fair,\nand auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement\nLearning (HACO) framework that separates risk calibration from preference\noptimization to generate conservative action recommendations at scale. In our\nsetting, each step involves choosing among common coordination actions (e.g.,\nwhich member to contact, by which modality, and whether to route to a\nspecialized service) while controlling the near-term risk of adverse\nutilization events (e.g., unplanned emergency department visits or\nhospitalizations). Using a de-identified operational dataset from Waymark\ncomprising 2.77 million sequential decisions across 168,126 patients, HACO (i)\ntrains a lightweight risk model for adverse events, (ii) derives a conformal\nthreshold to mask unsafe actions at a target risk level, and (iii) learns a\npreference policy on the resulting safe subset. We evaluate policies with a\nversion-agnostic fitted Q evaluation (FQE) on stratified subsets and audit\nsubgroup performance across age, sex, and race. HACO achieves strong risk\ndiscrimination (AUC ~0.81) with a calibrated threshold ( {\\tau} ~0.038 at\n{\\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses\nreveal systematic differences in estimated value across demographics,\nunderscoring the importance of fairness auditing. Our results show that\nconformal risk gating integrates cleanly with offline RL to deliver\nconservative, auditable decision support for population health management\nteams.", "AI": {"tldr": "HACO\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u98ce\u9669\u6821\u51c6\u548c\u504f\u597d\u4f18\u5316\uff0c\u4e3a\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u63d0\u4f9b\u4fdd\u5b88\u3001\u53ef\u5ba1\u8ba1\u7684\u884c\u52a8\u63a8\u8350\uff0c\u63a7\u5236\u4e0d\u826f\u4e8b\u4ef6\u98ce\u9669", "motivation": "\u4e3a\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u7684\u5065\u5eb7\u7ba1\u7406\u9879\u76ee\u63d0\u4f9b\u5b89\u5168\u3001\u516c\u5e73\u3001\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u652f\u6301\uff0c\u63a7\u5236\u4e0d\u826f\u533b\u7597\u4e8b\u4ef6\u98ce\u9669", "method": "\u6df7\u5408\u81ea\u9002\u5e94\u7b26\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u98ce\u9669\u6a21\u578b\u3001\u63a8\u5bfc\u7b26\u5408\u9608\u503c\u5c4f\u853d\u4e0d\u5b89\u5168\u884c\u52a8\u3001\u5728\u5b89\u5168\u5b50\u96c6\u4e0a\u5b66\u4e60\u504f\u597d\u7b56\u7565", "result": "\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u98ce\u9669\u533a\u5206\u80fd\u529b\uff08AUC\u7ea60.81\uff09\uff0c\u6821\u51c6\u9608\u503c\u5728\u76ee\u6807\u98ce\u9669\u6c34\u5e73\u4e0b\u4fdd\u6301\u9ad8\u5b89\u5168\u8986\u76d6\u7387\uff0c\u5b50\u7ec4\u5206\u6790\u663e\u793a\u4eba\u53e3\u7edf\u8ba1\u5b66\u5dee\u5f02", "conclusion": "\u7b26\u5408\u98ce\u9669\u95e8\u63a7\u4e0e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u53ef\u4e3a\u4eba\u53e3\u5065\u5eb7\u7ba1\u7406\u56e2\u961f\u63d0\u4f9b\u4fdd\u5b88\u4e14\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u652f\u6301"}}
{"id": "2509.09848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09848", "abs": "https://arxiv.org/abs/2509.09848", "authors": ["Nana Han", "Dong Liu", "Tomas Norton"], "title": "Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly being recognised as valuable\nknowledge communication tools in many industries. However, their application in\nlivestock farming remains limited, being constrained by several factors not\nleast the availability, diversity and complexity of knowledge sources. This\nstudy introduces an intelligent knowledge assistant system designed to support\nhealth management in farmed goats. Leveraging the Retrieval-Augmented\nGeneration (RAG), two structured knowledge processing methods, table\ntextualization and decision-tree textualization, were proposed to enhance large\nlanguage models' (LLMs) understanding of heterogeneous data formats. Based on\nthese methods, a domain-specific goat farming knowledge base was established to\nimprove LLM's capacity for cross-scenario generalization. The knowledge base\nspans five key domains: Disease Prevention and Treatment, Nutrition Management,\nRearing Management, Goat Milk Management, and Basic Farming Knowledge.\nAdditionally, an online search module is integrated to enable real-time\nretrieval of up-to-date information. To evaluate system performance, six\nablation experiments were conducted to examine the contribution of each\ncomponent. The results demonstrated that heterogeneous knowledge fusion method\nachieved the best results, with mean accuracies of 87.90% on the validation set\nand 84.22% on the test set. Across the text-based, table-based, decision-tree\nbased Q&A tasks, accuracy consistently exceeded 85%, validating the\neffectiveness of structured knowledge fusion within a modular design. Error\nanalysis identified omission as the predominant error category, highlighting\nopportunities to further improve retrieval coverage and context integration. In\nconclusion, the results highlight the robustness and reliability of the\nproposed system for practical applications in goat farming.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u667a\u80fd\u77e5\u8bc6\u52a9\u624b\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u5c71\u7f8a\u517b\u6b96\u5065\u5eb7\u7ba1\u7406\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u5904\u7406\u65b9\u6cd5\u63d0\u5347LLM\u5bf9\u5f02\u6784\u6570\u636e\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u755c\u7267\u4e1a\u5e94\u7528\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u77e5\u8bc6\u6e90\u7684\u53ef\u7528\u6027\u3001\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u9650\u5236\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5c71\u7f8a\u517b\u6b96\u7684\u5065\u5eb7\u7ba1\u7406\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6280\u672f\uff0c\u63d0\u51fa\u8868\u683c\u6587\u672c\u5316\u548c\u51b3\u7b56\u6811\u6587\u672c\u5316\u4e24\u79cd\u7ed3\u6784\u5316\u77e5\u8bc6\u5904\u7406\u65b9\u6cd5\uff0c\u5efa\u7acb\u5305\u542b\u4e94\u4e2a\u5173\u952e\u9886\u57df\u7684\u5c71\u7f8a\u517b\u6b96\u77e5\u8bc6\u5e93\uff0c\u5e76\u96c6\u6210\u5728\u7ebf\u641c\u7d22\u6a21\u5757\u3002", "result": "\u5f02\u6784\u77e5\u8bc6\u878d\u5408\u65b9\u6cd5\u53d6\u5f97\u6700\u4f73\u6548\u679c\uff0c\u9a8c\u8bc1\u96c6\u5e73\u5747\u51c6\u786e\u738787.90%\uff0c\u6d4b\u8bd5\u96c684.22%\u3002\u5728\u6587\u672c\u3001\u8868\u683c\u3001\u51b3\u7b56\u6811\u95ee\u7b54\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u5747\u8d85\u8fc785%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u5c71\u7f8a\u517b\u6b96\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u9519\u8bef\u5206\u6790\u663e\u793a\u9057\u6f0f\u662f\u4e3b\u8981\u9519\u8bef\u7c7b\u578b\uff0c\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u68c0\u7d22\u8986\u76d6\u548c\u4e0a\u4e0b\u6587\u6574\u5408\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.09799", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09799", "abs": "https://arxiv.org/abs/2509.09799", "authors": ["Mansi Sharma", "Alexandre Duchevet", "Florian Daiber", "Jean-Paul Imbert", "Maurice Rekrut"], "title": "Distinguishing Startle from Surprise Events Based on Physiological Signals", "comment": null, "summary": "Unexpected events can impair attention and delay decision-making, posing\nserious safety risks in high-risk environments such as aviation. In particular,\nreactions like startle and surprise can impact pilot performance in different\nways, yet are often hard to distinguish in practice. Existing research has\nlargely studied these reactions separately, with limited focus on their\ncombined effects or how to differentiate them using physiological data. In this\nwork, we address this gap by distinguishing between startle and surprise events\nbased on physiological signals using machine learning and multi-modal fusion\nstrategies. Our results demonstrate that these events can be reliably\npredicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.\nTo further validate the robustness of our model, we extended the evaluation to\ninclude a baseline condition, successfully differentiating between Startle,\nSurprise, and Baseline states with a highest mean accuracy of 74.9% with\nXGBoost and Late Fusion.", "AI": {"tldr": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u533a\u5206\u98ce\u9669\u73af\u5883\u4e2d\u7684\u60ca\u5413\u548c\u60ca\u8bb6\u53cd\u5e94\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe85.7%", "motivation": "\u60ca\u8bb6\u548c\u60ca\u5413\u4e8b\u4ef6\u4f1a\u5f71\u54cd\u98de\u884c\u5458\u8868\u73b0\u5e76\u5e26\u6765\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8fd9\u4e24\u79cd\u53cd\u5e94\u7684\u7ec4\u5408\u6548\u5e94\u5206\u6790\u548c\u751f\u7406\u533a\u5206\u65b9\u6cd5", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u6765\u533a\u5206\u60ca\u5413\u548c\u60ca\u8bb6\u4e8b\u4ef6", "result": "\u60ca\u5413\u548c\u60ca\u8bb6\u4e8b\u4ef6\u53ef\u9760\u9884\u6d4b\uff0cSVM\u548c\u540e\u671f\u878d\u5408\u7b56\u7565\u8fbe\u523085.7%\u51c6\u786e\u7387\uff1b\u5305\u62ec\u57fa\u7ebf\u72b6\u6001\u7684\u4e09\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cXGBoost\u548c\u540e\u671f\u878d\u5408\u8fbe\u523074.9%\u51c6\u786e\u7387", "conclusion": "\u7814\u7a76\u4e3a\u98ce\u9669\u73af\u5883\u4e2d\u7684\u60ca\u5413\u548c\u60ca\u8bb6\u53cd\u5e94\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u751f\u7406\u533a\u5206\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5b89\u5168\u6027\u80fd\u548c\u51b3\u7b56\u8d28\u91cf"}}
{"id": "2509.10065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10065", "abs": "https://arxiv.org/abs/2509.10065", "authors": ["Hauzi Cao", "Jiahao Shen", "Zhengzhen Li", "Qinquan Ren", "Shiyu Zhao"], "title": "Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation", "comment": null, "summary": "This paper studies the kinematic tracking control problem for aerial\nmanipulators. Existing kinematic tracking control methods, which typically\nemploy proportional-derivative feedback or tracking-error-based feedback\nstrategies, may fail to achieve tracking objectives within specified time\nconstraints. To address this limitation, we propose a novel control framework\ncomprising two key components: end-effector tracking control based on a\nuser-defined preset trajectory and quadratic programming-based reference\nallocation. Compared with state-of-the-art approaches, the proposed method has\nseveral attractive features. First, it ensures that the end-effector reaches\nthe desired position within a preset time while keeping the tracking error\nwithin a performance envelope that reflects task requirements. Second,\nquadratic programming is employed to allocate the references of the quadcopter\nbase and the Delta arm, while considering the physical constraints of the\naerial manipulator, thus preventing solutions that may violate physical\nlimitations. The proposed approach is validated through three experiments.\nExperimental results demonstrate the effectiveness of the proposed algorithm\nand its capability to guarantee that the target position is reached within the\npreset time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bbe\u8f68\u8ff9\u548c\u4e8c\u6b21\u89c4\u5212\u7684\u7a7a\u4e2d\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u6846\u67b6\uff0c\u786e\u4fdd\u672b\u7aef\u6267\u884c\u5668\u5728\u9884\u8bbe\u65f6\u95f4\u5185\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u5e76\u6ee1\u8db3\u7269\u7406\u7ea6\u675f", "motivation": "\u73b0\u6709\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u65b9\u6cd5\uff08\u5982PD\u53cd\u9988\u6216\u8ddf\u8e2a\u8bef\u5dee\u53cd\u9988\u7b56\u7565\uff09\u65e0\u6cd5\u5728\u6307\u5b9a\u65f6\u95f4\u7ea6\u675f\u5185\u5b9e\u73b0\u8ddf\u8e2a\u76ee\u6807\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027", "method": "\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u9884\u8bbe\u8f68\u8ff9\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u63a7\u5236\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212\u7684\u53c2\u8003\u5206\u914d\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u7269\u7406\u7ea6\u675f", "result": "\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u786e\u4fdd\u76ee\u6807\u4f4d\u7f6e\u5728\u9884\u8bbe\u65f6\u95f4\u5185\u5230\u8fbe\uff0c\u4e14\u8ddf\u8e2a\u8bef\u5dee\u4fdd\u6301\u5728\u53cd\u6620\u4efb\u52a1\u8981\u6c42\u7684\u6027\u80fd\u5305\u7edc\u5185", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u6846\u67b6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u4fdd\u8bc1\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u7cbe\u786e\u8ddf\u8e2a\uff0c\u540c\u65f6\u907f\u514d\u8fdd\u53cd\u7269\u7406\u9650\u5236\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09943", "abs": "https://arxiv.org/abs/2509.09943", "authors": ["Zhu Chen", "Mert Edg\u00fc", "Er Jin", "Johannes Stegmaier"], "title": "Segment Anything for Cell Tracking", "comment": null, "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.", "AI": {"tldr": "\u57fa\u4e8eSAM2\u7684\u65e0\u76d1\u7763\u7ec6\u80de\u8ddf\u8e2a\u6846\u67b6\uff0c\u514d\u9700\u624b\u52a8\u6807\u6ce8\u548c\u6570\u636e\u96c6\u7279\u5b9a\u8c03\u6574\uff0c\u57282D\u548c3D\u663e\u5fae\u955c\u89c6\u9891\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u8ddf\u8e2a\u7cbe\u5ea6", "motivation": "\u89e3\u51b3\u663e\u5fae\u955c\u56fe\u50cf\u5e8f\u5217\u4e2d\u7ec6\u80de\u8ddf\u8e2a\u548c\u6709\u4e1d\u5206\u88c2\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5305\u62ec\u5206\u88c2\u7269\u4f53\u3001\u4f4e\u4fe1\u566a\u6bd4\u3001\u8fb9\u754c\u4e0d\u660e\u786e\u3001\u5bc6\u96c6\u805a\u96c6\u7b49\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f9d\u8d56\u8d39\u65f6\u8d39\u529b\u7684\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6", "method": "\u901a\u8fc7\u5c06Segment Anything 2 (SAM2)\u5927\u578b\u57fa\u7840\u5206\u5272\u6a21\u578b\u96c6\u6210\u5230\u8ddf\u8e2a\u6d41\u7a0b\u4e2d\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u8ddf\u8e2a\u6846\u67b6\uff0c\u4e0d\u4f9d\u8d56\u4efb\u4f55\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u96c6", "result": "\u65b9\u6cd5\u57282D\u548c\u5927\u89c4\u6a213D\u65f6\u95f4\u62c9\u663e\u5fae\u955c\u89c6\u9891\u4e2d\u8fbe\u5230\u4e86\u7ade\u4e89\u6027\u7684\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u540c\u65f6\u6d88\u9664\u4e86\u5bf9\u6570\u636e\u96c6\u7279\u5b9a\u9002\u914d\u7684\u9700\u6c42", "conclusion": "\u8be5\u65e0\u76d1\u7763\u65b9\u6cd5\u514d\u9664\u4e86\u624b\u52a8\u6807\u6ce8\u7684\u6210\u672c\u548c\u65f6\u95f4\u6d88\u8017\uff0c\u80fd\u591f\u5728\u5404\u79cd\u5f02\u8d28\u6027\u663e\u5fae\u955c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u7ec6\u80de\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.", "AI": {"tldr": "\u8fd9\u7bc7\u8c03\u67e5\u6027\u8bba\u6587\u7cfb\u7edf\u603b\u7ed3\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e0e\u4f20\u7edf\u6846\u7387\u76f8\u673a\u878d\u5408\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5728\u89c6\u9891\u6062\u590d\u548c3D\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u65f6\u95f4\u589e\u5f3a\u548c\u7a7a\u95f4\u589e\u5f3a\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u653e\u6570\u636e\u96c6\u8d44\u6e90\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4f5c\u4e3a\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u4f20\u611f\u5668\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u8d85\u9ad8\u6293\u6296\u7387\u7b49\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4e0e\u4f20\u7edf\u6846\u7387\u76f8\u673a\u878d\u5408\u624d\u80fd\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002\u672c\u6587\u7684\u52a8\u673a\u662f\u7cfb\u7edf\u603b\u7ed3\u8fd9\u79cd\u878d\u5408\u6280\u672f\u7684\u53d1\u5c55\u8fdb\u5c55\u548c\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u8c03\u67e5\u65b9\u6cd5\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u5168\u9762\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u6062\u590d\u4e2d\u7684\u8d21\u732e\uff1a\u65f6\u95f4\u589e\u5f3a\uff08\u5982\u5e27\u95f4\u63d2\u503c\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\uff09\u548c\u7a7a\u95f4\u589e\u5f3a\uff08\u5305\u62ec\u8d85\u5206\u8fa8\u7387\u3001\u4f4e\u5149\u548cHDR\u589e\u5f3a\u3001\u4f2a\u5f71\u51cf\u5c11\uff09\uff0c\u540c\u65f6\u6df1\u5165\u8ba8\u8bba3D\u91cd\u5efa\u9886\u57df\u7684\u8fdb\u5c55\u3002", "result": "\u8c03\u67e5\u663e\u793a\uff0c\u4e8b\u4ef6\u6d41\u4e0e\u4f20\u7edf\u6846\u7387\u6293\u6296\u7684\u878d\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u5404\u79cd\u89c6\u89c9\u8d28\u91cf\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5177\u6709\u6311\u6218\u6027\u6761\u4ef6\u7684\u573a\u666f\u4e0b\u3002\u8bba\u6587\u8fd8\u6c47\u603b\u4e86\u5b8c\u6574\u7684\u5f00\u653e\u6570\u636e\u96c6\u8d44\u6e90\uff0c\u4e3a\u53ef\u590d\u73b0\u7814\u7a76\u548c\u6027\u80fd\u6d4b\u8bd5\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u6700\u65b0\u8fdb\u5c55\u548c\u6d1e\u5bdf\uff0c\u8fd9\u4efd\u8c03\u67e5\u65e8\u5728\u6fc0\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7cfb\u7edf\uff08\u7279\u522b\u662f\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\uff09\u6765\u5b9e\u73b0\u9ad8\u7ea7\u89c6\u89c9\u5a92\u4f53\u6062\u590d\u548c\u589e\u5f3a\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2509.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09977", "abs": "https://arxiv.org/abs/2509.09977", "authors": ["Siying Liu", "Zikai Wang", "Hanle Zheng", "Yifan Hu", "Xilin Wang", "Qingkai Yang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking", "comment": "15 pages, 8 figures", "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.", "AI": {"tldr": "ISTASTrack\u662f\u9996\u4e2a\u57fa\u4e8eTransformer\u7684ANN-SNN\u6df7\u5408\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7ISTA\u9002\u914d\u5668\u5b9e\u73b0RGB\u548c\u4e8b\u4ef6\u6570\u636e\u7684\u6709\u6548\u878d\u5408\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ANN\u7f51\u7edc\u96be\u4ee5\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u6d41\u7684\u7a00\u758f\u548c\u5f02\u6b65\u7279\u6027\uff0c\u800cANN-SNN\u6df7\u5408\u67b6\u6784\u5728RGB-Event\u611f\u77e5\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u8de8\u5f02\u6784\u8303\u5f0f\u7684\u7279\u5f81\u878d\u5408\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6a21\u578b\uff1a\u89c6\u89c9Transformer\u63d0\u53d6RGB\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u8109\u51b2Transformer\u6355\u83b7\u4e8b\u4ef6\u6d41\u65f6\u7a7a\u52a8\u6001\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8eISTA\u7b97\u6cd5\u7684\u9002\u914d\u5668\u8fdb\u884c\u53cc\u5411\u7279\u5f81\u4ea4\u4e92\uff0c\u5e76\u52a0\u5165\u65f6\u5e8f\u4e0b\u91c7\u6837\u6ce8\u610f\u529b\u6a21\u5757\u5bf9\u9f50\u7279\u5f81\u3002", "result": "\u5728FE240hz\u3001VisEvent\u3001COESOT\u548cFELT\u7b49RGB-Event\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u80fd\u6548\u3002", "conclusion": "ISTASTrack\u8bc1\u660e\u4e86ANN-SNN\u6df7\u5408\u8bbe\u8ba1\u5728\u9c81\u68d2\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.10227", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.10227", "abs": "https://arxiv.org/abs/2509.10227", "authors": ["\u00c1ngel Ladr\u00f3n", "Miguel S\u00e1nchez-Dom\u00ednguez", "Javier Rozal\u00e9n", "Fernando R. S\u00e1nchez", "Javier de Vicente", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "title": "A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures", "comment": "29 pages, 15 figures", "summary": "Fatigue life prediction is essential in both the design and operational\nphases of any aircraft, and in this sense safety in the aerospace industry\nrequires early detection of fatigue cracks to prevent in-flight failures.\nRobust and precise fatigue life predictors are thus essential to ensure safety.\nTraditional engineering methods, while reliable, are time consuming and involve\ncomplex workflows, including steps such as conducting several Finite Element\nMethod (FEM) simulations, deriving the expected loading spectrum, and applying\ncycle counting techniques like peak-valley or rainflow counting. These steps\noften require collaboration between multiple teams and tools, added to the\ncomputational time and effort required to achieve fatigue life predictions.\nMachine learning (ML) offers a promising complement to traditional fatigue life\nestimation methods, enabling faster iterations and generalization, providing\nquick estimates that guide decisions alongside conventional simulations.\n  In this paper, we present a ML-based pipeline that aims to estimate the\nfatigue life of different aircraft wing locations given the flight parameters\nof the different missions that the aircraft will be operating throughout its\noperational life. We validate the pipeline in a realistic use case of fatigue\nlife estimation, yielding accurate predictions alongside a thorough statistical\nvalidation and uncertainty quantification. Our pipeline constitutes a\ncomplement to traditional methodologies by reducing the amount of costly\nsimulations and, thereby, lowering the required computational and human\nresources.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u98de\u673a\u673a\u7ffc\u75b2\u52b3\u5bff\u547d\u9884\u6d4b\u7ba1\u9053\uff0c\u66ff\u4ee3\u4f20\u7edf\u590d\u6742\u6709\u9650\u5143\u6a21\u62df\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u9884\u6d4b\u5e76\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42", "motivation": "\u4f20\u7edf\u98de\u673a\u75b2\u52b3\u5bff\u547d\u9884\u6d4b\u65b9\u6cd5\u8017\u65f6\u4e14\u590d\u6742\uff0c\u9700\u8981\u591a\u56e2\u961f\u534f\u4f5c\u548c\u5927\u91cf\u6709\u9650\u5143\u6a21\u62df\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u63d0\u4f9b\u5feb\u901f\u8865\u5145\u65b9\u6848\u6765\u63d0\u9ad8\u6548\u7387", "method": "\u5f00\u53d1\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7ba1\u9053\uff0c\u6839\u636e\u98de\u884c\u53c2\u6570\u9884\u6d4b\u98de\u673a\u673a\u7ffc\u4e0d\u540c\u4f4d\u7f6e\u7684\u75b2\u52b3\u5bff\u547d\uff0c\u5305\u542b\u7edf\u8ba1\u9a8c\u8bc1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "result": "\u5728\u771f\u5b9e\u75b2\u52b3\u5bff\u547d\u4f30\u8ba1\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u83b7\u5f97\u51c6\u786e\u9884\u6d4b\u7ed3\u679c", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u662f\u5bf9\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u6548\u8865\u5145\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6602\u8d35\u6a21\u62df\u9700\u6c42\uff0c\u964d\u4f4e\u8ba1\u7b97\u548c\u4eba\u529b\u8d44\u6e90\u6210\u672c"}}
{"id": "2509.10341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10341", "abs": "https://arxiv.org/abs/2509.10341", "authors": ["Botond Fazekas", "Thomas Pinetz", "Guilherme Aresta", "Taha Emre", "Hrvoje Bogunovic"], "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT", "comment": null, "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.", "AI": {"tldr": "GARD\u662f\u4e00\u79cd\u57fa\u4e8e\u4f3d\u9a6c\u6269\u6563\u6a21\u578b\u7684OCT\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u51cf\u5c11\u4fdd\u771f\u9879\u548c\u52a0\u901f\u63a8\u7406\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u6709\u6548\u53bb\u9664\u6563\u6591\u566a\u58f0", "motivation": "OCT\u56fe\u50cf\u5b58\u5728\u56fa\u6709\u7684\u6563\u6591\u566a\u58f0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u566a\u58f0\u53bb\u9664\u548c\u7ed3\u6784\u4fdd\u6301\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u566a\u58f0\u7edf\u8ba1\u6a21\u578b\u548c\u66f4\u597d\u7684\u53bb\u566a\u6548\u679c", "method": "\u91c7\u7528\u53bb\u566a\u6269\u6563\u4f3d\u9a6c\u6a21\u578b\uff08DDGM\uff09\u66ff\u4ee3\u4f20\u7edf\u9ad8\u65af\u5047\u8bbe\uff0c\u5f15\u5165\u566a\u58f0\u51cf\u5c11\u4fdd\u771f\u9879\u5229\u7528\u9884\u5904\u7406\u56fe\u50cf\u6307\u5bfc\u53bb\u566a\uff0c\u5e76\u91c7\u7528DDIM\u6846\u67b6\u52a0\u901f\u63a8\u7406", "result": "\u5728\u914d\u5bf9\u566a\u58f0/\u4f4e\u566a\u58f0OCT\u6570\u636e\u96c6\u4e0a\uff0cGARD\u5728PSNR\u3001SSIM\u548cMSE\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u4ea7\u751f\u66f4\u6e05\u6670\u7684\u8fb9\u7f18\u5e76\u66f4\u597d\u4fdd\u7559\u89e3\u5256\u7ec6\u8282", "conclusion": "GARD\u901a\u8fc7\u4f3d\u9a6c\u6269\u6563\u6a21\u578b\u548c\u566a\u58f0\u51cf\u5c11\u4fdd\u771f\u9879\uff0c\u6210\u529f\u89e3\u51b3\u4e86OCT\u56fe\u50cf\u53bb\u566a\u4e2d\u566a\u58f0\u53bb\u9664\u4e0e\u7ed3\u6784\u4fdd\u6301\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u89c6\u7f51\u819c\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf"}}
