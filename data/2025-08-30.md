<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 该研究提出了用于360度视频显著性预测的新数据集YT360-EyeTracking和两个基于视觉Transformer的模型SalViT360和SalViT360-AV，通过整合空间音频信息显著提升了全景视频中的注意力预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏全面的360度视听显著性预测数据集，且现有方法未充分考虑球面畸变和空间音频的整合，研究旨在探索如何利用视听线索有效预测360度视频中的视觉显著性。

Method: 构建了包含81个ODV的YT360-EyeTracking数据集，提出了SalViT360（基于视觉Transformer的框架，配备球面几何感知的时空注意力层）和SalViT360-AV（进一步整合基于音频输入的Transformer适配器）两个新颖的显著性预测模型。

Result: 在多个基准数据集（包括YT360-EyeTracking）上的实验结果表明，SalViT360和SalViT360-AV在预测360度场景中的观看者注意力方面显著优于现有方法。

Conclusion: 在模型架构中整合空间音频线索对于准确预测全景视频中的显著性至关重要，音频信息的加入显著提升了显著性预测的性能。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [2] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出了一个基于VLM的视觉模型解释流程，能够在样本和数据集层面提供有意义的模型行为解释，发现失败案例并提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型开发主要关注性能指标，缺乏对模型一般性行为的解释方法。理解模型在普通图像上的行为对防止偏见判断和识别模型趋势至关重要。

Method: 利用视觉-语言模型(VLM)构建了一个解释流程，能够同时在样本级别和数据集级别对视觉模型进行解释。该流程需要最小化人工动作，将视觉模型开发与xAI分析相结合。

Result: 提出的流程可以有效地发现模型的失败案例，并提供对视觉模型行为的深入洞察，促进图像分析的进步。

Conclusion: 该研究为解决视觉模型可解释性的挑战提供了一个有效的流程，通过VLM技术实现了样本和数据集层面的全面解释，有助于提升模型的可信赖性和可理解性。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [3] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: MedFoundationHub是一个医疗视觉语言模型GUI工具包，旨在解决医疗VLMs的安全隐私问题，支持医生无编程使用模型，工程师快速部署，并通过Docker确保隐私保护推理。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型虽然为临床应用带来巨大机遇，但存在严重的隐私安全风险，包括受保护健康信息泄露、数据泄漏和网络威胁漏洞，特别是在医院环境中尤为关键。

Method: 开发MedFoundationHub图形用户界面工具包，支持医生手动选择使用不同模型无需编程，工程师以即插即用方式高效部署医疗VLMs，集成Hugging Face开源模型，通过Docker编排实现操作系统无关的隐私保护推理部署。

Result: 使用单个NVIDIA A6000 GPU的离线本地工作站即可运行，评估了5个最先进VLM模型（Google-MedGemma3-4B等），病理学家评估了1015个临床医生-模型评分事件，发现存在脱靶回答、模糊推理和病理术语不一致等局限性。

Conclusion: MedFoundationHub提供了一个安全、易用的医疗VLM部署解决方案，但当前模型仍存在显著局限性，需要在医疗术语准确性和推理一致性方面进一步改进。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [4] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 提出了一种延迟脉冲方法和时间依赖的IF神经元(tdIF)，解决了SNN在视觉检测任务中性能不佳的问题，在超低时间步长(5步内)下实现了最先进的性能


<details>
  <summary>Details</summary>
Motivation: 当前ANN-SNN转换方法在分类任务中表现优异，但在视觉检测任务中性能仍然不理想，主要原因是异质脉冲模式导致的残余膜电位问题

Method: 采用延迟脉冲方法缓解异质脉冲模式问题，并提出tdIF神经元架构，使IF神经元能够根据时间步长的时序动态调整积累和发放行为

Result: 在目标检测和车道线检测两个关键视觉任务上进行了广泛评估，方法超越了当前ANN-SNN转换方法，在超低延迟(5时间步内)下达到最先进性能

Conclusion: 所提出的方法使脉冲能够表现出不同的时间特性，而不仅仅依赖频率表示，在超低时间步长下实现了更精确的特征表示，同时保持了与传统IF神经元相当的能量消耗

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [5] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: Video-MTR是一个强化多轮推理框架，通过迭代选择关键视频片段和问题理解来解决长视频理解问题，采用门控双级奖励系统实现端到端训练，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 长视频理解存在长程时间依赖和多个事件的挑战，现有方法依赖静态推理或外部视觉语言模型，存在复杂度高和端到端训练不足导致的性能不佳问题。

Method: 提出强化多轮推理框架Video-MTR，迭代选择视频片段并基于先前处理片段的理解进行渐进式推理，引入门控双级奖励系统（轨迹级奖励和轮级奖励）优化视频片段选择和问题理解。

Result: 在VideoMME、MLVU和EgoSchema等基准测试中，Video-MTR在准确性和效率方面均优于现有方法，实现了最先进的性能。

Conclusion: Video-MTR通过多轮迭代推理和门控双级奖励系统，有效解决了长视频理解问题，无需外部视觉语言模型，实现了端到端训练，显著提升了长视频理解的性能。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [6] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: UTA-Sign是一种无监督的热成像-事件相机视频增强方法，用于低光照环境下的交通标志识别，通过双增强机制融合热成像帧和事件信号来提升交通标志检测精度


<details>
  <summary>Details</summary>
Motivation: 热成像相机在低光照环境下表现优异，但在识别相似材料制成的标志时存在盲点；事件相机能有效检测光强变化但采样不均匀。两种模态具有互补特性，需要融合来解决交通标志识别问题

Method: 提出双增强机制：利用热成像帧提供准确运动线索作为时间参考来对齐不均匀的事件信号，同时事件信号为原始热成像帧提供细微的标志内容，实现一致的标志表示

Result: 在真实场景数据集上验证，在交通标志描绘质量和感知层面的检测精度方面表现出优越性能

Conclusion: 该方法有效解决了热成像的标志盲点和事件相机的不均匀采样问题，通过模态融合显著提升了低光照环境下交通标志的识别能力

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [7] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 基于低频感知干扰的深度假制造主动防御方法，直接干扰面部替换生成过程，在保持视觉质量的同时提高防御效果


<details>
  <summary>Details</summary>
Motivation: 现有深度假检测方法主要是被动的事后分析，无法防止攻击发生，需要主动防御技术来应对假制造技术对隐私和社会安全的威胁

Method: 结合频域和空间域特征，使用离散小波变换(DWT)提取低频组件并生成干扰，通过编码器-干扰生成器-解码器完整架构，在保留高频细节的同时引入低频人工产物

Result: 在CelebA-HQ和LFW数据集上实验表明，方法显著降低了面部替换效果，提高了防御成功率，同时保持了良好的视觉质量

Conclusion: 该主动防御方法能够有效干扰深度假制造生成过程，为应对假制造技术提供了一种前置性的解决方案

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [8] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 本文是一篇关于视频中抽象概念理解的综述论文，探讨了如何利用基础模型来识别和理解视频中的抽象概念（如正义、自由、团结等），这是当前视频内容理解领域的一个重要挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然现代AI系统在识别视频中具体可见的物体、动作、事件和场景方面取得了显著进展，但在理解抽象概念方面仍落后于人类。抽象概念识别对于使模型更符合人类推理和价值观至关重要。

Method: 本文采用综述研究方法，系统分析了不同任务和数据集，回顾了数十年来社区在抽象概念理解方面的经验，并探讨如何利用多模态基础模型的最新进展来解决这一挑战。

Result: 研究发现研究人员长期以来一直在尝试解决抽象概念理解任务，并充分利用当时可用的工具。论文强调了借鉴社区经验的重要性，以避免在基础模型时代"重新发明轮子"。

Conclusion: 基础模型的进步为解决视频中抽象概念理解这一重要开放挑战提供了理想条件。通过整合数十年的研究经验和现代多模态基础模型，可以更好地推进这一领域的发展。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [9] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: DVCTNet是一个基于双视图协同训练的网络，通过结合全景X射线图像的全局视图和裁剪牙齿图像的局部视图，使用门控交叉视图注意力模块动态融合特征，显著提升了牙科龋齿检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前牙科龋齿检测方法由于对比度变化细微和病变形态多样，准确率不理想。受牙医临床工作流程启发，需要结合整体图像筛查和详细牙齿级检查来提高检测精度。

Method: 使用自动牙齿检测建立全局和局部两个互补视图，分别预训练视觉基础模型。全局视图模型作为检测骨干生成区域建议，局部视图模型提取详细特征。通过门控交叉视图注意力模块动态融合双视图特征。

Result: 在公开数据集和新构建的高精度数据集上，DVCTNet均表现出优于现有最先进方法的性能，证明了其临床适用性。

Conclusion: DVCTNet通过双视图协同训练和特征融合，有效提升了牙科龋齿检测的准确性，为临床诊断提供了可靠的工具。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 本文揭示了强化学习比监督微调更有效地破坏LLM安全对齐，并提出TokenBuncher防御方法通过抑制模型响应不确定性来对抗RL有害微调


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，有害微调风险增加。现有研究主要关注监督微调，但发现强化学习能更有效地突破安全防护，需要专门防御机制

Method: 提出TokenBuncher防御方法，包含熵作为奖励的RL和Token Noiser机制，通过约束模型响应不确定性来防止RL利用奖励信号驱动有害行为

Result: 跨多个模型和RL算法的实验表明，TokenBuncher能有效缓解有害RL微调，同时保持良性任务效用和微调能力

Conclusion: RL有害微调比SFT构成更大系统性风险，TokenBuncher提供了有效且通用的防御解决方案

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [11] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: 提出了cMALC-D框架，使用大型语言模型生成语义化课程并提供更鲁棒的评估信号，通过多样性上下文混合机制防止模式崩溃，在交通信号控制领域显著提升泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有上下文多智能体强化学习方法依赖不可靠的代理信号（如价值估计），在多智能体设置中由于智能体间动态性和部分可观测性而变得嘈杂不稳定，需要更鲁棒的课程学习和评估方法。

Method: 使用大型语言模型生成语义化课程，引入基于多样性的上下文混合机制，通过组合先前上下文特征创建新的训练场景，防止模式崩溃并鼓励探索。

Result: 在交通信号控制领域的实验表明，cMALC-D相比现有课程学习方法显著提高了泛化能力和样本效率。

Conclusion: cMALC-D框架通过LLM引导的课程学习和多样性上下文混合，有效解决了多智能体强化学习在复杂不确定环境中的泛化问题，为实际部署提供了更可靠的解决方案。

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 提出GDLLM方法，基于大语言模型(LLMs)的全局距离感知建模，通过图注意力网络和软推理范式解决事件时序关系抽取中的长距离依赖和少数类关系识别问题


<details>
  <summary>Details</summary>
Motivation: 小语言模型(SLMs)在处理不平衡数据集中的少数类关系时受限，而大语言模型(LLMs)使用人工设计的提示可能引入噪声，干扰对事件间长距离依赖关系的判断

Method: 1) 使用图注意力网络(GAT)构建距离感知图结构辅助LLMs捕获长距离依赖特征；2) 设计基于软推理的时序特征学习范式，将LLMs生成的概率信息补充到多头注意力机制中

Result: 在TB-Dense和MATRES两个公开数据集上实现了最先进的性能，显著提升了少数关系类的表现和整体学习能力

Conclusion: GDLLM框架通过有效捕获全局特征，显著增强了少数关系类的性能，并提高了整体学习能力

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>
