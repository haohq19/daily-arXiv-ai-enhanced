<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: 提出SMAGNet模型，通过自适应融合SAR和MSI数据提升洪水淹没范围制图精度，即使在MSI数据缺失时仍保持稳健性能


<details>
  <summary>Details</summary>
Motivation: 洪水期间及时准确的水域范围制图对灾害管理至关重要。虽然SAR数据常用于洪水响应，但结合MSI数据的多模态方法能提升精度，特别是在洪水高峰期观测数据有限时。然而，如何自适应整合部分可用的MSI数据到SAR制图流程中尚未充分探索。

Method: 提出SMAGNet（空间掩码自适应门控网络），一种多模态深度学习模型：以SAR数据为主要输入进行洪水后水域范围制图，通过特征融合整合互补的MSI数据。模型能自适应处理不同可用程度的MSI数据。

Result: 在C2S-MS Floods数据集实验中，SMAGNet在不同MSI数据可用性水平下均优于其他多模态深度学习模型。即使MSI数据完全缺失，SMAGNet性能仍与仅使用SAR数据训练的U-Net模型统计相当。

Conclusion: SMAGNet增强了模型对缺失数据的鲁棒性，提升了多模态深度学习在真实洪水管理场景中的适用性，为灾害管理提供了更可靠的技术支持。

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [2] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 提出一个端到端工作流，使用多模态大语言模型自动批改手写STEM考试，通过多阶段设计确保可靠性，在真实课程测验中达到约8分的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试能捕捉开放式推理和图表，但人工批改速度慢且难以扩展，需要自动化解决方案来保持标准考试流程。

Method: 端到端工作流：教师提供手写参考答案和评分规则，参考方案转换为文本摘要用于条件评分；多阶段设计包括格式/存在性检查、独立评分器集成、监督聚合和确定性验证模板。

Result: 使用GPT-5.2和Gemini-3 Pro后端，完整流程在斯洛文尼亚语真实课程测验中达到约8分的平均绝对误差，低偏差，约17%的手动审查触发率。

Conclusion: 结构化提示和参考方案基础对于准确评分至关重要，该工作流为手写STEM考试批改提供了可靠、可扩展的自动化解决方案。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 基于电子健康记录的机器学习模型在预测肝硬化方面显著优于传统FIB-4评分，能够提前1-3年进行更准确的风险分层。


<details>
  <summary>Details</summary>
Motivation: 开发能够利用常规电子健康记录数据早期预测肝硬化的机器学习模型，以改善肝硬化预防和管理，并超越传统FIB-4评分的性能。

Method: 使用大型学术医疗系统的去标识化电子健康记录数据进行回顾性队列研究，识别脂肪肝患者并根据ICD-9/10编码分为肝硬化和非肝硬化队列。构建观察窗口和预测窗口模拟临床实际使用场景，整合人口统计学、诊断、实验室结果、生命体征和共病指数等特征，训练XGBoost模型进行1年、2年和3年预测，并与FIB-4评分进行性能比较。

Result: XGBoost模型在1年、2年和3年预测中分别获得0.81、0.73和0.69的AUC值，显著优于FIB-4的0.71、0.63和0.57。随着预测时间延长，性能优势更加明显，表明机器学习模型具有更好的早期风险识别能力。

Conclusion: 基于常规电子健康记录数据的机器学习模型在肝硬化早期预测方面显著优于传统FIB-4评分，能够实现更早、更准确的风险分层，可作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理。

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [4] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: 该论文提出结合光流和三种深度学习模型（S-CNN、预训练CNN特征提取器、H-ELM）用于无人机视频中的人体检测，在UCF-ARG数据集上取得高精度。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征方法依赖专家知识，对光照变化、相机抖动等动态事件敏感。需要更便宜、自动化的特征学习方法，特别是在无人机非静态相机拍摄的复杂视频中检测人体。

Method: 结合光流和三种深度学习模型：1）监督卷积神经网络（S-CNN），2）预训练CNN特征提取器，3）分层极限学习机（H-ELM）。在UCF-ARG无人机数据集上训练测试，评估五种人体动作。

Result: 预训练CNN平均准确率98.09%，S-CNN使用softmax达95.6%（SVM为91.7%），H-ELM达95.9%。H-ELM在CPU上训练445秒，S-CNN在GPU上训练770秒。

Conclusion: 提出的自动特征学习方法在无人机视频人体检测任务中表现成功，预训练CNN效果最佳，H-ELM在计算效率上有优势。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [5] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: 该论文研究了残差网络中分层SGD如何高效学习层次化标签模型，这些模型具有多项式深度表达能力，超越了之前可学习的对数深度模型，为理解深度学习提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习为何能高效学习复杂函数，特别是探索残差网络能否学习具有层次结构的标签模型。作者认为人类教师提供的细粒度标签揭示了大脑内部算法的"提示"，这种层次结构可能是深度学习成功的关键。

Method: 采用残差网络和分层SGD算法，学习具有未知标签层次结构的模型：L₁ ⊆ L₂ ⊆ ... ⊆ Lᵣ = [n]，其中L₁中的标签是输入的简单函数，而i>1时，Lᵢ中的标签是更简单标签的简单函数。

Result: 证明了残差网络中的分层SGD能够高效学习这类层次模型，这些模型达到了高效可学习性的深度极限，需要多项式深度来表达，而之前的模型只能由对数深度电路计算。

Conclusion: 层次化模型的学习能力可能最终成为理解深度学习的基础。人类教师的存在支持了层次结构自然可用的假设，通过提供细粒度标签，教师有效揭示了大脑内部算法的"提示"，这种形式化的直觉表明层次结构促进了高效学习。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [6] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 研究者开发了AugUNet1D，一种基于1D UNet并采用数据增强的深度学习模型，用于自动标记脑电图中的棘慢波放电，在961小时小鼠EEG数据上表现优于传统算法和14种机器学习分类器。


<details>
  <summary>Details</summary>
Motivation: 手动标记脑电图事件（特别是棘慢波放电）耗时且劳动密集，尤其是在连续数周至数月的记录中。现有自动标记方法仍有改进空间，需要更准确高效的解决方案。

Method: 1. 比较14种机器学习分类器在961小时小鼠EEG数据上的表现；2. 选择表现最佳的1D UNet进行改进；3. 通过数据增强（特别是缩放增强）提升模型性能；4. 将增强后的AugUNet1D与"Twin Peaks"算法进行对比。

Result: 1D UNet在14种分类器中表现最佳；数据增强显著提升性能，其中缩放增强效果最明显；AugUNet1D在检测SWD方面优于"Twin Peaks"算法，检测到的事件特征更接近人工标记。

Conclusion: AugUNet1D是自动标记EEG中棘慢波放电的有效工具，性能优于现有方法。研究者公开了预训练和未训练的模型供其他用户使用，有助于减少人工标注工作量。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [7] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 提出基于熵的再训练框架，将数据漂移建模为概率流，通过熵平衡分解触发再训练，在非平稳分类任务中显著减少再训练次数同时保持性能


<details>
  <summary>Details</summary>
Motivation: 现有漂移检测方法缺乏动力学原理解释，无法指导再训练频率与运营成本的平衡。需要一种基于原理的框架来应对非平稳环境中的数据漂移问题。

Method: 将部署时数据漂移建模为Fokker-Planck方程控制的概率流，使用随时间演化的KL散度量模型-数据不匹配，通过熵平衡分解（包含非负熵产生项）触发再训练

Result: 在受控非平稳分类实验中，基于熵触发的再训练实现了与高频再训练相当的预测性能，同时相对于每日和基于标签的策略减少了数量级的再训练事件

Conclusion: 基于熵的再训练框架提供了一种无标签干预策略，响应累积的不匹配而非延迟的性能崩溃，为平衡再训练频率与运营成本提供了原理性指导

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [8] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的持续学习方法，结合在线重置机制防止码本坍塌，并使用KV-LoRA高效利用压缩记忆表示，在保持高准确率的同时将记忆库大小减少到基线方法的0.3%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识容易过时，需要持续学习来更新。现有记忆增强方法虽然能缓解灾难性遗忘，但记忆库会随数据流不断增长，导致存储和计算成本增加。

Method: 1) 通过码本优化策略压缩记忆库；2) 引入在线重置机制防止码本坍塌；3) 在注意力层使用Key-Value低秩适配(KV-LoRA)来高效利用压缩记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小压缩到最竞争基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过记忆库压缩和稳定学习机制，实现了高效且有效的持续学习，解决了记忆库无限增长的问题，同时保持了模型性能。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [9] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 使用机器学习预测骑行时长，结合路线拓扑和运动员体能状态，相比仅用拓扑特征减少14%误差


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的骑行时长预测需要大量参数（如空气阻力系数、实时风速），对业余骑手不实用，需要更简单有效的预测方法

Method: 采用机器学习方法，结合路线拓扑特征和运动员当前体能状态（从训练负荷指标推导），使用Lasso回归模型，在单人数据集(N=96次骑行)上进行N-of-1研究设计

Result: 拓扑+体能特征组合的Lasso回归模型达到MAE=6.60分钟，R²=0.922；相比仅用拓扑特征(MAE=7.66分钟)，误差减少14%，证明生理状态对自定节奏骑行有显著影响

Conclusion: 机器学习方法能有效预测骑行时长，无需复杂物理参数，运动员体能状态是重要预测因素，渐进式检查点预测支持动态比赛规划

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [10] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 提出基于图神经网络和强化学习的交通感知出租车热点预测框架，在模拟德里数据集上减少乘客等待时间56%，行驶距离38%


<details>
  <summary>Details</summary>
Motivation: 传统出租车热点预测模型仅依赖历史需求，忽略了交通拥堵、道路事件和公共活动等动态因素，无法实现实时供需匹配

Method: 将城市道路网络建模为图（节点为交叉口，边为路段），使用GNN编码时空依赖，结合Q-learning智能体推荐最优出租车热点位置

Result: 在模拟德里出租车数据集上，相比基线随机选择方法，乘客等待时间减少约56%，行驶距离减少38%

Conclusion: 该交通感知的图强化学习框架能有效优化出租车供需匹配，可扩展到多模式交通系统并集成到智慧城市平台中

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [11] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文提出分布创造性推理（DCR）框架，分析当前LLM推理循环导致多样性衰减的问题，并提供保持正确性和创造性的理论方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理循环主要优化正确性，导致推理路径分布崩溃，语义熵下降，损害创造性问题解决能力。需要分析这种失败并找到保持多样性的方法。

Method: 引入分布创造性推理（DCR）框架，将训练视为通过解决方案轨迹概率测度的梯度流。STaR、GRPO、DPO等方法都是该损失函数的特例。

Result: 提出三个核心成果：多样性衰减定理描述不同方法如何导致多样性衰减；确保收敛到稳定多样策略的设计；实际可行的实现方案。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的原则性方案，解决了现有推理循环的分布崩溃问题。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: 本文提出了一种新的知识表示框架HTKGHs，用于处理包含多个主要实体的复杂时态事实，并基于POLECAT数据库构建了htkgh-polecat数据集，评估了大语言模型在复杂预测场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的超关系时态知识图(HTKGs)在表示复杂时态事实方面存在局限性，特别是无法支持包含两个以上主要实体的时态事实，而这在现实世界的地缘政治事件中很常见。需要一种更强大的表示框架来处理这类复杂事件。

Method: 1. 提出了HTKGHs（超关系时态知识广义超图）作为HTKGs的泛化形式，支持地缘政治事件中常见的两种复杂事实类型
2. 建立了HTKGHs的形式化定义，确保向后兼容性
3. 基于全球事件数据库POLECAT构建了htkgh-polecat数据集
4. 在关系预测任务上对流行的大语言模型进行了基准测试和分析

Result: 1. 成功形式化了HTKGHs框架，能够有效表示包含多个主要实体的复杂时态事实
2. 创建了htkgh-polecat数据集，为复杂时态知识表示提供了新的基准
3. 通过基准测试获得了大语言模型在复杂预测场景中的适应性和能力洞察

Conclusion: HTKGHs为表示复杂时态事实提供了更强大的框架，特别是在地缘政治事件分析中。通过构建htkgh-polecat数据集和基准测试，为未来研究复杂时态知识表示和大语言模型在预测任务中的应用奠定了基础。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [13] [DefVINS: Visual-Inertial Odometry for Deformable Scenes](https://arxiv.org/abs/2601.00702)
*Samuel Cerezo,Javier Civera*

Main category: cs.RO

TL;DR: DefVINS：一种视觉惯性里程计框架，通过嵌入变形图显式分离刚性IMU锚定状态与非刚性变形，提高非刚性环境下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 可变形场景违反了经典视觉惯性里程计（VIO）的刚性假设，导致对局部非刚性运动的过拟合或在变形主导视觉视差时产生严重漂移

Method: 使用嵌入变形图显式分离刚性IMU锚定状态与非刚性变形，通过可观测性分析指导变形激活策略，渐进激活非刚性自由度

Result: 结合惯性约束与可观测性感知的变形激活策略，在非刚性环境下提高了鲁棒性

Conclusion: DefVINS框架通过显式分离刚性运动与非刚性变形，结合惯性测量与可观测性分析，有效解决了可变形场景中的VIO问题

Abstract: Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.

</details>
