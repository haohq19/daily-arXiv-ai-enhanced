<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件相机的神经形态眼动追踪模型，使用LIF层和深度可分离卷积替代传统ANN模块，在保持精度的同时大幅降低了计算复杂度和功耗。


<details>
  <summary>Details</summary>
Motivation: 可穿戴系统眼动追踪需要低延迟和毫瓦级功耗，但传统基于帧的管道存在运动模糊、计算成本高和时间分辨率有限的问题。神经形态传感器和脉冲神经网络提供了有前景的替代方案，但现有SNN方法要么过于专门化，要么性能不及现代ANN架构。

Method: 提出神经形态版本的高性能事件眼动追踪模型，用轻量级LIF层替换其循环和注意力模块，并利用深度可分离卷积降低模型复杂度。

Result: 模型获得3.7-4.1px平均误差，接近专用神经形态系统Retina（3.24px）的精度，同时相比最接近的ANN变体，模型大小减少20倍，理论计算量减少850倍。这些高效变体预计在1kHz下以3.9-4.9 mW功耗和3 ms延迟运行。

Conclusion: 结果表明，高性能事件眼动追踪架构可以重新设计为SNN，在保持实时可穿戴部署所需精度的同时，获得显著的效率提升。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

</details>


### [2] [Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering](https://arxiv.org/abs/2512.10327)
*Cai Xu,Jinlong Liu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: ISMVC提出了一种基于信息量的选择性插补方法，通过评估缺失位置的信息量来决定是否插补，结合变分自编码器和混合高斯先验学习聚类友好的表示，在非平衡缺失场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法在处理不完整数据时面临挑战：基于插补的方法可能引入噪声和偏差，而免插补方法在严重不完整情况下缺乏跨视图互补性。需要一种更智能的选择性插补策略来平衡信息利用和噪声控制。

Method: ISMVC方法：1）基于视图内相似性和跨视图一致性评估每个缺失位置的信息量；2）只在有足够支持时选择性插补；3）结合变分自编码器和混合高斯先验学习聚类友好表示；4）在分布层面进行插补，稳定后验分布聚合并建模插补不确定性。

Result: 在多个基准数据集上的实验表明，ISMVC在更现实和具有挑战性的非平衡缺失场景下，优于现有的基于插补和免插补方法。该方法轻量级、数据驱动且模型无关，可作为插件模块集成到现有IMC模型中。

Conclusion: ISMVC通过选择性插补策略有效平衡了信息利用和噪声控制，在分布层面建模不确定性，为不完整多视图聚类提供了一种鲁棒且高效的解决方案。

Abstract: Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.

</details>


### [3] [Topology-Agnostic Animal Motion Generation from Text Prompt](https://arxiv.org/abs/2512.10352)
*Keyi Chen,Mingze Sun,Zhenyu Liu,Zhangquan Chen,Ruqi Huang*

Main category: cs.CV

TL;DR: OmniZoo：首个大规模异构动物运动数据集与通用自回归运动生成框架，支持任意骨骼拓扑和文本驱动的运动生成


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法大多依赖固定骨骼模板，无法泛化到不同或扰动拓扑的骨骼结构。同时缺乏大规模异构动物运动数据和统一生成框架来联合建模任意骨骼拓扑与文本条件。

Method: 提出OmniZoo数据集（140个物种，32,979个序列，多模态标注）和通用自回归运动生成框架。核心是拓扑感知骨骼嵌入模块，将任意骨骼的几何和结构属性编码到共享标记空间，与文本语义无缝融合。

Result: 方法能够根据文本提示和目标骨骼生成时间连贯、物理合理、语义对齐的运动，并支持跨物种运动风格迁移。

Conclusion: OmniZoo解决了当前运动生成方法的核心限制，为任意骨骼拓扑的文本驱动运动生成提供了首个大规模数据集和统一框架，具有广泛的应用前景。

Abstract: Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.

</details>


### [4] [Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos](https://arxiv.org/abs/2512.10363)
*Mingyu Jeon,Jisoo Yang,Sungjin Han,Jinkwon Hwang,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: P2S是一个无需训练的视频时刻检索框架，通过自适应跨度生成和查询分解技术，在长视频中实现高效的时间定位，超越有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有长视频时刻检索方法面临两大问题：有监督方法扩展性差、泛化能力弱；零样本方法存在搜索阶段候选爆炸和精炼阶段需要高成本VLM验证的问题。

Method: 提出P2S框架，包含两个关键创新：1）自适应跨度生成器防止搜索阶段候选爆炸；2）查询分解技术在不依赖高成本VLM的情况下精炼候选。

Result: P2S在MAD数据集上R5@0.1指标比有监督SOTA方法提升3.7%，是首个能在小时级长视频中进行时间定位的零样本框架。

Conclusion: P2S通过创新的训练免费框架解决了长视频时刻检索中的搜索效率低下和精炼成本高昂的问题，在零样本设置下超越了有监督方法。

Abstract: Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \textbf{P}oint-\textbf{to}-\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\% on R5@0.1 on MAD).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: HB-SGE是一种结合重球动量和预测梯度外推的鲁棒一阶优化方法，在病态和非凸问题上比NAG和标准动量方法更稳定。


<details>
  <summary>Details</summary>
Motivation: NAG等加速梯度方法在条件良好的问题上收敛快，但在病态或非凸问题上由于动量积累过激而容易发散。需要一种既能保持加速效果又能在复杂地形上稳定的方法。

Method: HB-SGE结合重球动量和预测梯度外推，使用局部泰勒近似估计未来梯度方向，而不是简单地累积历史梯度。这种方法提供自适应加速同时保持稳定性。

Result: 在病态二次问题（条件数κ=50）上，HB-SGE在119次迭代收敛，而SGD和NAG都发散。在非凸Rosenbrock函数上，HB-SGE在2718次迭代收敛，而经典动量方法在10步内就发散。HB-SGE在良好条件下不如NAG快，但比SGD快且更鲁棒。

Conclusion: HB-SGE提供了一种鲁棒的优化替代方案，在病态和非凸问题上保持稳定收敛，同时保持O(d)内存开销和与标准动量相同的超参数设置。

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [6] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: CHyLL提出了一种在潜在空间中学习混合系统连续神经表示的新方法，避免了轨迹分割、事件函数或模式切换。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别学习每个离散模式的动态，但面临模式切换和流不连续性的组合挑战，难以准确学习混合系统的动态。

Method: CHyLL将状态空间重新表述为分段光滑商流形，其中流变得空间连续。基于微分拓扑的嵌入定理，在更高维空间中同时学习无奇点的神经嵌入和其中的连续流。

Result: CHyLL能够准确预测混合系统的流，具有优越的准确性，并能识别混合系统的拓扑不变量。成功应用于随机最优控制问题。

Conclusion: CHyLL通过将混合系统表示为潜在空间中的连续流，克服了传统方法在模式切换和不连续性方面的限制，为混合系统学习提供了新框架。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [7] [Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography](https://arxiv.org/abs/2512.10179)
*Abolfazl Shahrooei,Luke Arthur,Om Patel,Derek Kamper*

Main category: cs.LG

TL;DR: SNN与TCN在HD-sEMG信号解码指尖力任务中的性能比较，TCN表现更优但SNN有潜力通过优化缩小差距


<details>
  <summary>Details</summary>
Motivation: 高密度表面肌电信号（HD-sEMG）为非侵入式神经接口提供了可能，但将神经活动映射到用户运动意图仍然具有挑战性。研究旨在评估脉冲神经网络（SNN）作为神经形态架构与时域卷积网络（TCN）在解码指尖力任务中的性能差异。

Method: 使用单个参与者（10次试验）的前臂电极阵列收集数据，通过FastICA分解获取运动单元（MU）放电活动。采用重叠窗口训练模型，使用端到端的因果卷积架构。比较SNN和TCN在解码指尖力任务中的表现。

Result: 在保留试验中，TCN达到了4.44% MVC RMSE（Pearson r = 0.974），而SNN达到了8.25% MVC（r = 0.922）。TCN在准确性上优于SNN。

Conclusion: 虽然TCN更准确，但SNN作为一个现实的神经形态基线，通过适度的架构和超参数优化，有望缩小与TCN的性能差距。

Abstract: High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.

</details>


### [8] [A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field](https://arxiv.org/abs/2512.10287)
*Apurba Sarker,Reza T. Batley,Darshan Sarojini,Sourav Saha*

Main category: cs.LG

TL;DR: KHRONOS是一种基于核的神经代理模型，通过融合稀疏高保真数据和低保真信息来预测空气动力学场，在计算资源受限条件下表现出色，相比传统密集神经网络需要更少的可训练参数并提供更快的训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 空气动力学模拟成本高昂，需要快速替代模型用于设计和优化应用。传统方法在处理稀疏高保真数据和低保真信息融合方面存在效率问题，特别是在计算资源受限的情况下。

Method: 提出KHRONOS模型，基于变分原理、插值理论和张量分解构建。使用AirfRANS数据集作为高保真基准，NeuralFoil生成低保真对应数据。与多层感知机(MLP)、图神经网络(GNN)和物理信息神经网络(PINN)进行比较。考虑不同高保真数据可用性水平(0%、10%、30%)和复杂几何参数化，预测翼型表面压力系数分布。

Result: 所有模型最终达到可比的预测精度，但KHRONOS在资源受限条件下表现优异。KHRONOS需要数量级更少的可训练参数，在可比精度下提供更快的训练和推理速度，特别是在高保真数据稀缺的情况下。

Conclusion: KHRONOS及其类似架构在多保真空气动力学场预测中能够有效平衡精度和效率，特别适用于计算资源受限的应用场景，为空气动力学设计和优化提供了高效的替代模型解决方案。

Abstract: Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.

</details>


### [9] [Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks](https://arxiv.org/abs/2512.10355)
*Hyunsung Kim,Sangwoo Seo,Hoyoung Choi,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: DEFCON是一个评估足球防守贡献的框架，使用图注意力网络量化球员在每次进攻情境中的防守价值，通过计算对手预期控球价值的变化来分配防守积分。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注有球动作（如拦截、抢断），但有效防守往往体现在阻止危险机会发生之前，导致防守球员的真实影响未被充分衡量。

Method: 使用图注意力网络估计每个进攻选项的成功概率和预期价值，以及每个防守球员的防守责任，计算进攻方在每次动作前后的预期控球价值变化，根据防守球员是否减少或增加对手的EPV来分配正负积分。

Result: 在2023-24赛季训练、2024-25赛季荷甲联赛数据上评估，DEFCON聚合的球员积分与市场估值呈现强正相关。展示了多种实际应用：比赛中防守贡献时间线、球场区域空间分析、攻防球员互动配对总结。

Conclusion: DEFCON提供了一个全面量化防守贡献的框架，能够更准确地评估防守球员的真实影响，并具有多种实际应用价值。

Abstract: Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.

</details>


### [10] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: 提出DCFO方法，专门为LOF异常检测算法生成反事实解释，通过数据空间分区实现梯度优化，在50个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异常检测需要解释性，特别是反事实解释能说明如何改变数据点使其不被判为异常。现有方法大多忽略异常检测的特殊挑战，且未针对LOF等经典算法，而LOF虽广泛应用但缺乏可解释性。

Method: 提出DCFO方法，专门为LOF设计反事实解释。通过将数据空间划分为LOF行为平滑的区域，实现高效的基于梯度的优化，生成最小改变的反事实解释。

Result: 在50个OpenML数据集上的实验验证表明，DCFO在生成反事实解释的邻近性和有效性方面，始终优于基准竞争对手。

Conclusion: DCFO成功解决了LOF异常检测算法缺乏可解释性的问题，为异常检测提供了有效的反事实解释方法，在多个数据集上表现出优越性能。

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [11] [UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting](https://arxiv.org/abs/2508.01426)
*Hang Ni,Weijia Zhang,Hao Liu*

Main category: cs.LG

TL;DR: UniExtreme是一个通用的极端天气预测基础模型，通过自适应频率调制和事件先验增强模块，解决了现有模型在多样化极端事件预测上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测基础模型主要关注一般天气条件或特定类型的极端事件，忽视了现实世界中多样化极端事件的大气模式。极端事件具有两个关键特征：与正常天气模式的光谱差异，以及多样化极端事件的分层驱动和地理混合。

Method: 提出UniExtreme模型，包含两个核心模块：1) 自适应频率调制模块，通过可学习的Beta分布滤波器和多粒度光谱聚合捕捉正常与极端天气的区域光谱差异；2) 事件先验增强模块，通过双级记忆融合网络融入区域特定的极端事件先验，解决极端多样性和复合极端模式。

Result: 大量实验表明，UniExtreme在极端天气和一般天气预测任务上都优于现有最先进基线模型，展现出在多样化极端场景中的卓越适应性。

Conclusion: UniExtreme通过整合光谱差异建模和事件先验增强，成功构建了一个通用的极端天气预测基础模型，能够有效处理现实世界中多样化极端事件的复杂模式。

Abstract: Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出一个全面的数据偏见检测与缓解流程，包含四个组件来处理表示偏见和刻板印象，通过人类验证和模型微调评估效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据存在多方面偏见，包括有害语言和倾斜的人口分布。欧盟AI法案等法规要求识别和缓解针对受保护群体的偏见，但缺乏实践指导和操作化方法。

Method: 提出四步流程：1) 使用LLM生成符合质量标准的词表检测相关群体标签；2) 用人口表示分数量化表示偏见；3) 用社会语言学过滤检测和缓解刻板印象；4) 通过语法和上下文感知的反事实数据增强补偿表示偏见。

Result: 在性别、宗教和年龄案例中，成功减少了文本数据集中的表示偏见和刻板印象。但使用去偏见数据微调的LLM在偏见基准测试中并未一致表现更好，暴露了当前评估方法的缺陷。

Conclusion: 虽然数据去偏见方法有效，但仅靠数据去偏见不足以确保模型偏见减少，需要更有针对性的数据操作来解决具体表现的模型偏见，同时需要改进评估方法。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [13] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT是一个三层调度员支持系统，用于在自动语音识别失败时处理加勒比口音英语的紧急呼叫，通过结合口音调优的ASR、本地实体提取和生物声学痛苦检测，为调度员提供转录置信度、结构化临床实体和声音压力指标三个互补信号。


<details>
  <summary>Details</summary>
Motivation: 现有紧急语音识别系统对非标准英语变体（特别是加勒比口音）存在系统性性能下降，导致加勒比人口在紧急服务中存在关键差距。需要确保加勒比口音能够公平获得国家分诊协议的服务。

Method: 提出TRIDENT三层架构：1) 加勒比口音调优的ASR系统；2) 基于大语言模型的本地实体提取；3) 生物声学痛苦检测。系统将低ASR置信度视为有价值的队列优先级信号，特别是与升高的声音压力标记结合时。实体提取层捕捉训练有素的响应者和冷静旁观者可能报告的生命威胁紧急情况。

Result: 建立了口音弹性紧急AI框架，确保加勒比口音能够公平获得国家分诊协议（ESI常规操作和START大规模伤亡事件）。系统设计考虑了灾难场景下的离线操作。加勒比紧急呼叫的实证验证是未来工作。

Conclusion: TRIDENT通过将低ASR置信度重新定义为有价值的优先级信号，结合心理语言学研究中压力诱导的语码转换理论，为口音弹性紧急AI建立了框架，确保加勒比人口在紧急情况下获得公平服务。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>
