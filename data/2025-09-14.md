<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: PromptGuard是一个模块化提示框架，通过VulnGuard Prompt技术使用对比学习防止LLM生成有害信息，保护弱势群体。


<details>
  <summary>Details</summary>
Motivation: 现有安全方法依赖事后过滤或通用对齐技术，无法在生成源头主动预防有害输出，特别是对LGBTQ+、单亲家庭等弱势群体的危害。

Method: 提出PromptGuard框架，包含VulnGuard Prompt混合技术：整合GitHub精选数据、伦理思维链推理和自适应角色提示，采用多目标优化理论。

Result: 通过熵界和帕累托最优性证明25-30%的分析性危害减少，建立包含6个核心模块的智能专家系统。

Conclusion: 提供了全面的数学形式化框架，包括收敛证明和信息论漏洞分析，为系统性实证研究奠定数学基础。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [2] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event是首个基于事件相机数据的大规模语言驱动目标定位基准，包含5,567个真实驾驶场景、13,458个标注对象和30,000+经过验证的指代表达式，支持可解释的组合式目标定位。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级精度和抗运动模糊的优势，但在与自然语言理解的多模态融合方面研究不足，需要建立基准来填补这一空白。

Method: 构建基于真实驾驶场景的大规模数据集，每个指代表达式包含外观、状态、与观察者关系、与周围对象关系四个结构化属性，显式捕捉空间、时间和关系线索。

Result: 创建了包含5,567个场景、13,458个标注对象和30,000+指代表达式的大规模基准数据集，支持可解释的组合式目标定位。

Conclusion: Talk2Event为推进多模态和时间感知感知提供了基础，在机器人、人机交互等领域具有广泛应用前景。

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [3] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: 本文提出了RRDataset真实世界鲁棒性数据集，用于评估AI生成图像检测模型在复杂真实场景下的性能，包括场景泛化、网络传输鲁棒性和重数字化鲁棒性三个维度。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，高度逼真的图像合成对数字安全和媒体可信度提出了新挑战。现有AI生成图像检测方法在复杂真实世界条件下的性能评估存在研究空白。

Method: 构建包含7个主要场景的高质量图像数据集RRDataset，评估17个检测器和10个视觉语言模型，并进行192人参与的大规模人类研究，探索人类在检测AI生成图像方面的少样本学习能力。

Result: 基准测试结果揭示了当前AI检测方法在真实世界条件下的局限性，并强调了借鉴人类适应性来开发更鲁棒检测算法的重要性。

Conclusion: 该研究填补了AI生成图像检测在真实世界评估方面的空白，为开发更鲁棒的检测算法提供了重要数据集和基准，同时强调了人类认知能力对算法改进的借鉴价值。

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [4] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 提出CoAtNeXt混合模型用于胃组织图像分类，在多个数据集上超越现有CNN和ViT模型，准确率高达96-98%，有望辅助病理学家提高诊断准确性


<details>
  <summary>Details</summary>
Motivation: 胃病早期诊断至关重要，但传统组织病理学检查完全依赖人工，工作强度大且存在诊断差异，需要自动化、可靠且高效的胃组织分析方法

Method: 基于CoAtNet架构，用增强的ConvNeXtV2块替换MBConv层，并集成CBAM注意力模块以改善局部特征提取，在计算效率和分类性能间取得平衡

Result: 在HMU-GC-HE-30K八分类数据集上达到96.47%准确率，在GasHisSDB二分类数据集上达到98.29%准确率，超越所有测试的CNN和ViT模型

Conclusion: CoAtNeXt是胃组织图像组织病理学分类的强大架构，在二分类和多分类任务中表现优异，具有辅助病理学家提高诊断准确性和减轻工作负担的潜力

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [5] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: 提出了DATE方法，通过时间戳注入机制和语义引导的时间感知采样策略，增强多模态大语言模型的长视频时序理解能力


<details>
  <summary>Details</summary>
Motivation: 现有方法采用均匀帧采样和隐式位置编码，难以处理长视频中的长距离依赖关系，导致关键信息丢失和时序理解能力下降

Method: DATE方法包含时间戳注入机制(TIM)和时序感知相似性采样(TASS)：1)将视频帧嵌入与文本时间戳交错构建连续时间参考系统；2)将视频采样重构为视觉-语言检索任务，采用两阶段算法确保语义相关性和时序覆盖

Result: 在7B和72B模型上实现了小时级长视频基准测试的最先进性能，7B模型在某些基准上甚至超过了许多72B模型

Conclusion: DATE方法通过显式时间建模和语义引导采样，显著提升了多模态大语言模型的长视频时序理解和关键事件定位能力

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [6] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: 提出Code-as-Thought(CaT)方法，通过代码表示图表信息实现可验证的符号化推理，并引入视觉可编程性概念让模型自适应选择代码推理或直接视觉推理路径


<details>
  <summary>Details</summary>
Motivation: 解决现有图表理解方法的局限性：依赖外部工具导致脆弱性，或单一推理策略难以验证中间步骤，无法有效利用强化学习奖励信号

Method: 采用自适应框架，VLMs学习选择CaT路径（代码表示）或直接视觉推理路径，使用双奖励系统（数据准确性奖励+决策奖励）进行强化学习训练

Result: 在多样化图表理解基准测试中表现出强大且稳健的性能

Conclusion: VLMs不仅可以学习推理，还可以学习如何推理，能够为每个任务动态选择最优推理路径

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [7] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 该论文提出了CoTAP方法来解决密集自监督学习中的过分散问题，通过显式语义集中来改善密集表示学习


<details>
  <summary>Details</summary>
Motivation: 主流图像级自监督学习方法在密集任务中遇到patch过分散现象，导致下游性能下降。研究发现图像级SSL通过隐式语义集中避免此问题，但这种方法不适用于空间敏感的密集SSL

Method: 1) 通过蒸馏patch对应关系打破严格空间对齐，使用噪声容忍排序损失；2) 提出对象感知过滤器，通过交叉注意力将输出空间映射到基于对象的空间；3) 扩展AP损失到连续目标，利用其决策无关和自适应聚焦特性

Result: 在各种任务上的实证研究有力地支持了该方法的有效性

Conclusion: 提出的CoTAP方法通过显式语义集中成功解决了密集自监督学习中的过分散问题，在多个任务上表现出色

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 这篇论文提出了一种上下文条件化的异常检测框架，专门处理广泛存在异质上下文的表格数据，通过自动识别上下文特征和模型条件分布，显著提升了无监督异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据常包含异质上下文（如不同用户），导致全局稀有事件在特定上下文下变得正常。传统的单一全局分布模型无法抓住这些上下文细节，影响检测性能。

Method: 提出上下文条件化异常检测框架，自动识别上下文特征，使用简单的深度自动编码器建模条件数据分布。

Result: 在多个表格数据集上进行了广泛实验，证明该方法在无监督异常检测任务上超过了当前最先进的方法。

Conclusion: 证明了考虑上下文信息对于准确区分异常和正常实例的重要性，为表格数据异常检测领域提供了有效的解决方案。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [9] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 当前深度学习天气模型的评估指标存在"统计相似性陷阱"，奖励模糊预测而忽略罕见高影响事件。本文提出DART框架，通过双解码器架构解决粗粒度天气预报到高分辨率卫星亮度温度场的转换问题，专门优化极端对流检测。


<details>
  <summary>Details</summary>
Motivation: 现有天气模型评估指标过分强调统计相似性，导致模型倾向于生成模糊预测而无法准确检测罕见但高影响的极端天气事件，如危险对流。

Method: 提出DART（Dual Architecture for Regression Tasks）框架，采用双解码器架构进行背景/极端分解、物理启发的过采样和任务特定损失函数，专门针对220K以下的极端对流检测进行优化。

Result: DART在极端对流检测上达到CSI=0.273，偏差为2.52，相比基线模型在同等CSI下的偏差6.72有显著改进。移除集成水汽输送(IVT)使极端对流检测性能提升270%。框架训练时间短于10分钟，可无缝集成到现有气象工作流中。

Conclusion: DART成功解决了混合转换-分割-降尺度任务，通过专门化设计避免了统计相似性陷阱，为极端天气预警提供了可信的AI解决方案，在2023年8月吉大港洪水灾害案例中得到实际验证。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [10] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过知识蒸馏从LLM中提取领域知识和推理能力来提升QA模拟器性能，在保持高效推理的同时达到更好的预测效果


<details>
  <summary>Details</summary>
Motivation: 解决现有QA模拟器中LLM-free方法性能不佳而LLM-based方法推理速度慢、资源消耗高的问题，需要在性能和效率之间找到平衡

Method: 使用知识蒸馏技术，从大型语言模型(LLM)中提取领域知识和推理能力，构建高效的QA模拟器LDSim

Result: 在模拟任务和知识追踪(KT)任务上都取得了强劲的结果，实现了性能与效率的良好平衡

Conclusion: LDSim方法通过知识蒸馏有效提升了QA模拟器的性能，为教育推荐系统提供了高质量的训练数据生成方案

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [11] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个解决算法选择问题中计算成本高和OpenML数据局限性的大规模实验数据集，包含9,408个管道在300个数据集上的完整实验结果


<details>
  <summary>Details</summary>
Motivation: 解决算法选择问题中评估不同算法性能的高计算成本问题，同时克服OpenML等现有在线存储库在管道多样性和代表性方面的局限性

Method: 构建PIPES数据集，通过系统性地组合不同的数据预处理技术（如缩放、插补等）创建9,408个多样化管道，并在300个数据集上进行全面实验

Result: 创建了一个包含详细管道信息、训练测试时间、预测结果、性能指标和错误信息的全面实验数据库，为元学习研究提供了多样化和代表性的数据支持

Conclusion: PIPES为算法选择和元学习研究提供了比现有资源更全面和多样化的实验数据，具有可扩展性，能够支持更深入的机器学习管道分析

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [12] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 基于深度学习的随机和混沌时空系统模拟器，通过参数条件化和局部注意力机制，实现了在不同参数范围和域规模上的高效模拟和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决经典数值积分方法在模拟随机、混沌时空系统时计算成本高、效率低的问题，并实现对参数空间的高效探索和稀有事件的统计研究。

Method: 采用预训练+精调法，在单个参数域预训练后，通过少量多样化数据集进行精调；统一局部注意力机制处理不同域规模和分辨率，支持小域预训练后扩展到大域。

Result: 在混沌Kuramoto-Sivashinsky方程和随机推动的beta平面湍流上验证，能够捕捉插值参数下的现象，实现了较传统数值积分方法显著的计算加速，概率版本还能提供不确定性量化。

Conclusion: 该模拟器为随机、混沌时空系统提供了高效、可扩展的模拟方案，在保持计算效率的同时支持不确定性分析，有助于参数空间探索和稀有事件研究。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 论文提出了一种截断交叉熵损失函数(TCE)来缓解生成式AI模型在合成数据上重复训练导致的模型崩溃问题，通过降低对高置信度预测的权重来显著延迟模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型产生合成数据的比例不断增加，预计到203年大部分训练数据将是机器生成的。在合成数据上重复训练会导致模型崩溃现象，使模型性能逐代退化。现有缓解策略有限，需要新的解决方案。

Method: 提出截断交叉熵损失函数(TCE)，这是一种置信度感知的损失函数，在训练过程中降低高置信度预测的权重。建立了模型无关的框架，将损失函数设计与模型崩溃缓解联系起来。

Result: TCE显著延迟了递归训练中的模型崩溃，将模型崩溃前的保真度间隔延长了2.3倍以上。该方法在不同模态上都表现出良好的泛化能力。

Conclusion: 损失函数设计为在合成数据日益增多的时代保持生成模型质量提供了一个简单而强大的工具，置信度感知的损失函数是缓解模型崩溃的有效策略。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 基于NeuroTac感应器咈米皮肤的神经元感矩系统，通过刷新卷积神经网络实现94.33%的滑动状态分类准确率，能够在滑动前至360ms检测到初始滑动


<details>
  <summary>Details</summary>
Motivation: 解决在边缘计算平台上部署初始滑动检测系统时遇到的能源约束问题，提高机器人操作的安全性

Method: 采用NeuroTac感应器与扩张的咈米皮肤设计，结合刷新卷积神经网络(SCNN)进行滑动状态分类

Result: 在感应器运动引起的滑动条件下，SCNN模型对三种滑动状态的分类准确率达94.33%；在重力引起的动态滑动验证中，系统能够在滑动前至360ms检测到初始滑动

Conclusion: 该神经元感矩系统具有稳定且响应快速的初始滑动检测能力，适合在能源受限的边缘平台上部署

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>
