{"id": "2508.00162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00162", "abs": "https://arxiv.org/abs/2508.00162", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "comment": null, "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "AI": {"tldr": "CHILD\u662f\u4e00\u79cd\u7d27\u51d1\u53ef\u91cd\u6784\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5173\u8282\u7ea7\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u5168\u8eab\u63a7\u5236\u548c\u79fb\u52a8\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u652f\u6301\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u5173\u8282\u7ea7\u9065\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u7684\u591a\u6837\u6027\u3002", "method": "CHILD\u7cfb\u7edf\u8bbe\u8ba1\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u652f\u6301\u76f4\u63a5\u5173\u8282\u6620\u5c04\u548c\u81ea\u9002\u5e94\u529b\u53cd\u9988\uff0c\u9632\u6b62\u4e0d\u5b89\u5168\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u79fb\u52a8\u64cd\u4f5c\u548c\u5168\u8eab\u63a7\u5236\u7684\u80fd\u529b\u3002", "conclusion": "CHILD\u7cfb\u7edf\u5f00\u6e90\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "AI\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u7ed3\u5408\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u7a0b\u6316\u6398\uff08OCPM\uff09\u5b9e\u73b0\u6d41\u7a0b\u667a\u80fd\uff08PI\uff09\uff0c\u4ee5\u652f\u6301\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u548c\u89c4\u8303\u5f0fAI\u3002", "motivation": "\u63a2\u8ba8AI\u5728\u7aef\u5230\u7aef\u64cd\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u56f0\u96be\uff0c\u63d0\u51fa\u9700\u8981\u7ed3\u5408\u6d41\u7a0b\u667a\u80fd\uff08PI\uff09\u548cOCPM\u6765\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u7a0b\u6316\u6398\uff08OCPM\uff09\u4f5c\u4e3a\u8fde\u63a5\u6570\u636e\u548c\u6d41\u7a0b\u7684\u6865\u6881\uff0c\u652f\u6301\u591a\u79cdAI\u5f62\u5f0f\u3002", "result": "OCPM\u662f\u5b9e\u73b0\u6d41\u7a0b\u667a\u80fd\uff08PI\uff09\u7684\u5173\u952e\uff0c\u80fd\u591f\u652f\u6301\u751f\u6210\u5f0f\u3001\u9884\u6d4b\u5f0f\u548c\u89c4\u8303\u5f0fAI\u5728\u7ec4\u7ec7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "AI\u9700\u8981\u6d41\u7a0b\u667a\u80fd\uff08PI\uff09\u548cOCPM\u7684\u7ed3\u5408\uff0c\u4ee5\u4f18\u5316\u64cd\u4f5c\u6d41\u7a0b\u5e76\u5b9e\u73b0AI\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002"}}
{"id": "2508.00041", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00041", "abs": "https://arxiv.org/abs/2508.00041", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages", "comment": null, "summary": "Federated fine-tuning enables Large Language Models (LLMs) to adapt to\ndownstream tasks while preserving data privacy, but its resource-intensive\nnature limits deployment on edge devices. In this paper, we introduce\nDevelopmental Federated Tuning (DevFT), a resource-efficient approach inspired\nby cognitive development that progressively builds a powerful LLM from a\ncompact foundation. DevFT decomposes the fine-tuning process into developmental\nstages, each optimizing submodels with increasing parameter capacity. Knowledge\nfrom earlier stages transfers to subsequent submodels, providing optimized\ninitialization parameters that prevent convergence to local minima and\naccelerate training. This paradigm mirrors human learning, gradually\nconstructing comprehensive knowledge structure while refining existing skills.\nTo efficiently build stage-specific submodels, DevFT introduces\ndeconfliction-guided layer grouping and differential-based layer fusion to\ndistill essential information and construct representative layers. Evaluations\nacross multiple benchmarks demonstrate that DevFT significantly outperforms\nstate-of-the-art methods, achieving up to 4.59$\\times$ faster convergence,\n10.67$\\times$ reduction in communication overhead, and 9.07% average\nperformance improvement, while maintaining compatibility with existing\napproaches.", "AI": {"tldr": "DevFT\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u6784\u5efaLLM\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5fae\u8c03\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "method": "\u5206\u9636\u6bb5\u5fae\u8c03\uff0c\u9010\u6b65\u589e\u52a0\u53c2\u6570\u5bb9\u91cf\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\u4f18\u5316\u521d\u59cb\u5316\u53c2\u6570\u3002\u91c7\u7528\u53bb\u51b2\u7a81\u5f15\u5bfc\u7684\u5c42\u5206\u7ec4\u548c\u57fa\u4e8e\u5dee\u5f02\u7684\u5c42\u878d\u5408\u6784\u5efa\u5b50\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u53474.59\u500d\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1110.67\u500d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53479.07%\u3002", "conclusion": "DevFT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709\u65b9\u6cd5\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6848\u3002"}}
{"id": "2508.00467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00467", "abs": "https://arxiv.org/abs/2508.00467", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSubCDM\u7684\u5b50\u96c6\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u5b50\u96c6\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u51b3\u7b56\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\u9700\u8981\u6240\u6709\u673a\u5668\u4eba\u53c2\u4e0e\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u65e0\u6cd5\u5206\u914d\u673a\u5668\u4eba\u6267\u884c\u5176\u4ed6\u4efb\u52a1\u3002", "method": "\u52a8\u6001\u6784\u5efa\u5b50\u96c6\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u5b50\u96c6\u5927\u5c0f\u4ee5\u5e94\u5bf9\u5171\u8bc6\u96be\u5ea6\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cSubCDM\u5728\u51cf\u5c11\u673a\u5668\u4eba\u4f7f\u7528\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5168\u7fa4\u51b3\u7b56\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SubCDM\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7fa4\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.00308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00308", "abs": "https://arxiv.org/abs/2508.00308", "authors": ["Chunyan She", "Fujun Han", "Chengyu Fang", "Shukai Duan", "Lidan Wang"], "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement", "comment": "Accepted by ACM MM 2025", "summary": "The event camera, benefiting from its high dynamic range and low latency,\nprovides performance gain for low-light image enhancement. Unlike frame-based\ncameras, it records intensity changes with extremely high temporal resolution,\ncapturing sufficient structure information. Currently, existing event-based\nmethods feed a frame and events directly into a single model without fully\nexploiting modality-specific advantages, which limits their performance.\nTherefore, by analyzing the role of each sensing modality, the enhancement\npipeline is decoupled into two stages: visibility restoration and structure\nrefinement. In the first stage, we design a visibility restoration network with\namplitude-phase entanglement by rethinking the relationship between amplitude\nand phase components in Fourier space. In the second stage, a fusion strategy\nwith dynamic alignment is proposed to mitigate the spatial mismatch caused by\nthe temporal resolution discrepancy between two sensing modalities, aiming to\nrefine the structure information of the image enhanced by the visibility\nrestoration network. In addition, we utilize spatial-frequency interpolation to\nsimulate negative samples with diverse illumination, noise and artifact\ndegradations, thereby developing a contrastive loss that encourages the model\nto learn discriminative representations. Experiments demonstrate that the\nproposed method outperforms state-of-the-art models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u589e\u5f3a\u6d41\u7a0b\u4e3a\u53ef\u89c1\u6027\u6062\u590d\u548c\u7ed3\u6784\u7ec6\u5316\u4e24\u9636\u6bb5\uff0c\u7ed3\u5408\u52a8\u6001\u5bf9\u9f50\u548c\u5bf9\u6bd4\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u5e27\u76f8\u673a\u7684\u6a21\u6001\u4f18\u52bf\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "1. \u53ef\u89c1\u6027\u6062\u590d\u7f51\u7edc\u901a\u8fc7\u5085\u91cc\u53f6\u7a7a\u95f4\u7684\u632f\u5e45-\u76f8\u4f4d\u7ea0\u7f20\u8bbe\u8ba1\uff1b2. \u52a8\u6001\u5bf9\u9f50\u878d\u5408\u7b56\u7565\u89e3\u51b3\u6a21\u6001\u95f4\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\uff1b3. \u7a7a\u95f4\u9891\u7387\u63d2\u503c\u751f\u6210\u8d1f\u6837\u672c\uff0c\u5f00\u53d1\u5bf9\u6bd4\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u589e\u5f3a\u6d41\u7a0b\u548c\u52a8\u6001\u5bf9\u9f50\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00374", "abs": "https://arxiv.org/abs/2508.00374", "authors": ["Yuji Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models", "comment": "Accepted to MVA2025 (Best Poster Award)", "summary": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.", "AI": {"tldr": "BiAnt\u7ed3\u5408\u524d\u5411\u548c\u540e\u5411\u9884\u6d4b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u89c6\u9891\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u56e0\u5355\u5411\u6027\u9650\u5236\u6027\u80fd\uff0c\u96be\u4ee5\u6355\u6349\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u5b50\u52a8\u4f5c\u3002", "method": "BiAnt\u7ed3\u5408\u524d\u5411\u548c\u540e\u5411\u9884\u6d4b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728Ego4D\u6570\u636e\u96c6\u4e0a\uff0cBiAnt\u5728\u7f16\u8f91\u8ddd\u79bb\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BiAnt\u901a\u8fc7\u53cc\u5411\u9884\u6d4b\u6709\u6548\u63d0\u5347\u4e86\u957f\u671f\u52a8\u4f5c\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00473", "abs": "https://arxiv.org/abs/2508.00473", "authors": ["Jiaping Cao", "Kangkang Zhou", "Juan Du"], "title": "HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection is a fundamental task in video surveillance, with\nbroad applications in public safety and intelligent monitoring systems.\nAlthough previous methods leverage Euclidean representations in RGB or depth\ndomains, such embeddings are inherently limited in capturing hierarchical event\nstructures and spatio-temporal continuity. To address these limitations, we\npropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for\nanomaly detection in 3D point cloud videos. Our approach first extracts\nper-frame spatial features from point cloud sequences via point cloud\nextractor, and then embeds them into Lorentzian hyperbolic space, which better\ncaptures the latent hierarchical structure of events. To model temporal\ndynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism\nthat leverages Lorentzian inner products and curvature-aware softmax to learn\ntemporal dependencies under non-Euclidean geometry. Our method performs all\nfeature transformations and anomaly scoring directly within full Lorentzian\nspace rather than via tangent space approximation. Extensive experiments\ndemonstrate that HyPCV-Former achieves state-of-the-art performance across\nmultiple anomaly categories, with a 7\\% improvement on the TIMo dataset and a\n5.6\\% gain on the DAD dataset compared to benchmarks. The code will be released\nupon paper acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2\u65f6\u7a7a\u53d8\u6362\u5668\uff08HyPCV-Former\uff09\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u66f4\u597d\u5730\u6355\u6349\u4e8b\u4ef6\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u7a7a\u8fde\u7eed\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728RGB\u6216\u6df1\u5ea6\u57df\u4e2d\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8868\u793a\uff0c\u96be\u4ee5\u6355\u6349\u4e8b\u4ef6\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u7a7a\u8fde\u7eed\u6027\u3002", "method": "\u901a\u8fc7\u70b9\u4e91\u63d0\u53d6\u5668\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u6d1b\u4f26\u5179\u53cc\u66f2\u7a7a\u95f4\uff0c\u5f15\u5165\u53cc\u66f2\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08HMHA\uff09\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728TIMo\u548cDAD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e867%\u548c5.6%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HyPCV-Former\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u76f4\u63a5\u8fdb\u884c\u7279\u5f81\u53d8\u6362\u548c\u5f02\u5e38\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.00734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00734", "abs": "https://arxiv.org/abs/2508.00734", "authors": ["Liuyun Xu", "Seymour M. J. Spence"], "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems", "comment": null, "summary": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4fdd\u771f\u5ea6\u5206\u5c42\u91c7\u6837\u548c\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u5143\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u5c0f\u5931\u6548\u6982\u7387\uff0c\u663e\u8457\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u975e\u7ebf\u6027\u6709\u9650\u5143\u6a21\u578b\u4e2d\u8ba1\u7b97\u5c0f\u5931\u6548\u6982\u7387\u65f6\u4ecd\u9700\u8981\u5927\u91cf\u6a21\u578b\u8bc4\u4f30\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u5206\u5c42\u91c7\u6837\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u5143\u6a21\u578b\uff0c\u4f5c\u4e3a\u4f4e\u4fdd\u771f\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u4fdd\u771f\u5ea6\u8499\u7279\u5361\u6d1b\u6846\u67b6\u4f30\u8ba1\u5931\u6548\u6982\u7387\u3002", "result": "\u5e94\u7528\u4e8e\u9ad8\u5c42\u94a2\u7ed3\u6784\u5efa\u7b51\u7684\u98ce\u6fc0\u52b1\u5206\u6790\uff0c\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u54cd\u5e94\u7684\u8d85\u8d8a\u6982\u7387\u66f2\u7ebf\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u7684\u7f55\u89c1\u4e8b\u4ef6\u5206\u6790\u3002"}}
