{"id": "2602.09079", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09079", "abs": "https://arxiv.org/abs/2602.09079", "authors": ["Zachary N. Flamholz", "Dillon Tracy", "Ripple Khera", "Jordan Wolinsky", "Nicholas Lee", "Nathaniel Tann", "Xiao Yin Zhu", "Harry Phillips", "Jeffrey Sherman"], "title": "Patient foundation model for risk stratification in low-risk overweight patients", "comment": null, "summary": "Accurate risk stratification in patients with overweight or obesity is critical for guiding preventive care and allocating high-cost therapies such as GLP-1 receptor agonists. We present PatientTPP, a neural temporal point process (TPP) model trained on over 500,000 real-world clinical trajectories to learn patient representations from sequences of diagnoses, labs, and medications. We extend existing TPP modeling approaches to include static and numeric features and incorporate clinical knowledge for event encoding. PatientTPP representations support downstream prediction tasks, including classification of obesity-associated outcomes in low-risk individuals, even for events not explicitly modeled during training. In health economic evaluation, PatientTPP outperformed body mass index in stratifying patients by future cardiovascular-related healthcare costs, identifying higher-risk patients more efficiently. By modeling both the type and timing of clinical events, PatientTPP offers an interpretable, general-purpose foundation for patient risk modeling with direct applications to obesity-related care and cost targeting.", "AI": {"tldr": "PatientTPP\u662f\u4e00\u4e2a\u795e\u7ecf\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\uff0c\u5229\u752850\u4e07+\u771f\u5b9e\u4e34\u5e8a\u8f68\u8ff9\u5b66\u4e60\u60a3\u8005\u8868\u5f81\uff0c\u901a\u8fc7\u5efa\u6a21\u4e34\u5e8a\u4e8b\u4ef6\u7c7b\u578b\u548c\u65f6\u95f4\u6765\u6539\u5584\u80a5\u80d6\u60a3\u8005\u98ce\u9669\u5206\u5c42\uff0c\u5728\u5fc3\u8840\u7ba1\u76f8\u5173\u533b\u7597\u6210\u672c\u9884\u6d4b\u4e0a\u4f18\u4e8eBMI\u6307\u6807\u3002", "motivation": "\u51c6\u786e\u7684\u98ce\u9669\u5206\u5c42\u5bf9\u4e8e\u8d85\u91cd\u6216\u80a5\u80d6\u60a3\u8005\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u4ee5\u6307\u5bfc\u9884\u9632\u6027\u62a4\u7406\u548c\u5206\u914d\u9ad8\u6210\u672c\u6cbb\u7597\uff08\u5982GLP-1\u53d7\u4f53\u6fc0\u52a8\u5242\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5982BMI\u5728\u98ce\u9669\u9884\u6d4b\u4e0a\u6709\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u6a21\u578b\u6765\u8bc6\u522b\u9ad8\u98ce\u9669\u60a3\u8005\u3002", "method": "\u5f00\u53d1PatientTPP\u795e\u7ecf\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\uff0c\u6269\u5c55\u73b0\u6709TPP\u65b9\u6cd5\u4ee5\u5305\u542b\u9759\u6001\u548c\u6570\u503c\u7279\u5f81\uff0c\u5e76\u6574\u5408\u4e34\u5e8a\u77e5\u8bc6\u8fdb\u884c\u4e8b\u4ef6\u7f16\u7801\u3002\u6a21\u578b\u57fa\u4e8e50\u591a\u4e07\u771f\u5b9e\u4e34\u5e8a\u8f68\u8ff9\u8bad\u7ec3\uff0c\u5b66\u4e60\u8bca\u65ad\u3001\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u548c\u836f\u7269\u5e8f\u5217\u7684\u60a3\u8005\u8868\u5f81\u3002", "result": "PatientTPP\u8868\u5f81\u652f\u6301\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\uff0c\u5305\u62ec\u5bf9\u4f4e\u98ce\u9669\u4e2a\u4f53\u4e2d\u80a5\u80d6\u76f8\u5173\u7ed3\u5c40\u7684\u5206\u7c7b\uff0c\u5373\u4f7f\u5bf9\u4e8e\u8bad\u7ec3\u671f\u95f4\u672a\u660e\u786e\u5efa\u6a21\u7684\u4e8b\u4ef6\u3002\u5728\u5065\u5eb7\u7ecf\u6d4e\u8bc4\u4f30\u4e2d\uff0cPatientTPP\u5728\u6309\u672a\u6765\u5fc3\u8840\u7ba1\u76f8\u5173\u533b\u7597\u6210\u672c\u5206\u5c42\u60a3\u8005\u65b9\u9762\u4f18\u4e8eBMI\uff0c\u80fd\u66f4\u6709\u6548\u5730\u8bc6\u522b\u9ad8\u98ce\u9669\u60a3\u8005\u3002", "conclusion": "\u901a\u8fc7\u5efa\u6a21\u4e34\u5e8a\u4e8b\u4ef6\u7684\u7c7b\u578b\u548c\u65f6\u95f4\uff0cPatientTPP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u901a\u7528\u57fa\u7840\uff0c\u7528\u4e8e\u60a3\u8005\u98ce\u9669\u5efa\u6a21\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u80a5\u80d6\u76f8\u5173\u62a4\u7406\u548c\u6210\u672c\u76ee\u6807\u5b9a\u4f4d\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597\u63d0\u4f9b\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2602.09112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09112", "abs": "https://arxiv.org/abs/2602.09112", "authors": ["Russ Webb", "Jason Ramapuram"], "title": "A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation", "comment": null, "summary": "What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \\$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\\% accuracy while GPT-5 has 95\\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86Cadmus\u7cfb\u7edf\uff0c\u5305\u542b\u6574\u6570\u865a\u62df\u673a\u3001\u771f\u5b9e\u7a0b\u5e8f\u6570\u636e\u96c6\u548c\u5c0f\u578bTransformer\u6a21\u578b\uff0c\u7528\u4e8e\u4ee5\u4f4e\u6210\u672c\u7814\u7a76\u7a0b\u5e8f\u5408\u6210\u3001\u5206\u5e03\u5916\u8868\u793a\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u76f8\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u66f4\u597d\u7684\u900f\u660e\u5ea6\u548c\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u7a0b\u5e8f\u5408\u6210\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b58\u5728\u5206\u5e03\u95ee\u9898\u96be\u4ee5\u754c\u5b9a\u3001\u5fae\u8c03\u6548\u679c\u4e0d\u660e\u3001\u5206\u8bcd\u5f71\u54cd\u4e0d\u6e05\u3001\u8ba1\u7b97\u5b58\u50a8\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002\u9700\u8981\u4e00\u4e2a\u5c0f\u578b\u3001\u900f\u660e\u3001\u53ef\u63a7\u7684\u7cfb\u7edf\u6765\u6df1\u5165\u7814\u7a76\u7a0b\u5e8f\u5b8c\u6210\u3001\u5206\u5e03\u5916\u8868\u793a\u3001\u5f52\u7eb3\u63a8\u7406\u7b49\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u5f00\u53d1Cadmus\u7cfb\u7edf\uff0c\u5305\u542b\uff1a1\uff09\u6574\u6570\u865a\u62df\u673a\uff1b2\uff09\u591a\u6837\u4efb\u52a1\u7684\u771f\u5b9e\u7a0b\u5e8f\u6570\u636e\u96c6\uff1b3\uff09\u81ea\u56de\u5f52Transformer\u6a21\u578b\uff08\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e200\u7f8e\u5143\uff09\u3002\u8be5\u7cfb\u7edf\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u7cbe\u7ec6\u63a7\u5236\u8bad\u7ec3\u5206\u5e03\uff0c\u5e76\u80fd\u68c0\u67e5\u548c\u63d2\u88c5\u6a21\u578b\u3002", "result": "Cadmus\u6a21\u578b\u5728\u7b80\u5355\u7684\u6574\u6570\u7b97\u672f\u7a0b\u5e8f\u5b8c\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eGPT-5\u768495%\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u4e0e\u95ee\u9898\u5173\u7cfb\u7684\u900f\u660e\u5ea6\uff0c\u800cGPT-5\u5728\u89e3\u51b3\u76f8\u540c\u4efb\u52a1\u65f6\u5f15\u5165\u4e86\u672a\u77e5\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u6210\u4e3a\u67d0\u4e9b\u7814\u7a76\u4e2d\u7684\u6df7\u6dc6\u56e0\u7d20\u3002", "conclusion": "\u5c0f\u578b\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u80fd\u591f\u5b9e\u73b0\u7ecf\u6d4e\u6709\u6548\u7684\u63d2\u88c5\u548c\u8c03\u67e5\uff0c\u4e3a\u7a0b\u5e8f\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u63a7\u7684\u5e73\u53f0\u3002\u5f53\u9700\u8981\u5b8c\u5168\u7406\u89e3\u8bad\u7ec3\u96c6\u4e0e\u4efb\u52a1\u5173\u7cfb\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u672a\u77e5\u5148\u9a8c\u77e5\u8bc6\u4f1a\u6210\u4e3a\u7814\u7a76\u969c\u788d\uff0c\u800cCadmus\u7cfb\u7edf\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2602.09255", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09255", "abs": "https://arxiv.org/abs/2602.09255", "authors": ["Mingfeng Yuan", "Hao Zhang", "Mahan Mohammadi", "Runhao Li", "Jinjun Shan", "Steven L. Waslander"], "title": "STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory", "comment": null, "summary": "Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.", "AI": {"tldr": "STaR\u662f\u4e00\u4e2a\u9762\u5411\u79fb\u52a8\u673a\u5668\u4eba\u7684\u667a\u80fd\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u591a\u6a21\u6001\u957f\u671f\u8bb0\u5fc6\u548c\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684\u53ef\u6269\u5c55\u68c0\u7d22\u7b97\u6cd5\uff0c\u652f\u6301\u5728\u5f00\u653e\u52a8\u6001\u573a\u666f\u4e2d\u8fdb\u884c\u957f\u65f6\u7a0b\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5f00\u653e\u52a8\u6001\u573a\u666f\u4e2d\u957f\u671f\u90e8\u7f72\u65f6\uff0c\u9762\u4e34\u6784\u5efa\u53ef\u6269\u5c55\u957f\u671f\u8bb0\u5fc6\u7684\u6311\u6218\uff0c\u9700\u8981\u652f\u6301\u4e0d\u540c\u7c92\u5ea6\u7684\u5f00\u653e\u6307\u4ee4\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u540c\u65f6\u4ea7\u751f\u7cbe\u786e\u53ef\u6267\u884c\u7684\u5bfc\u822a\u7b54\u6848\u3002", "method": "\u63d0\u51faSTaR\u6846\u67b6\uff1a(1)\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u591a\u6a21\u6001\u957f\u671f\u8bb0\u5fc6\uff0c\u6cdb\u5316\u5230\u672a\u89c1\u67e5\u8be2\u540c\u65f6\u4fdd\u7559\u7ec6\u7c92\u5ea6\u73af\u5883\u8bed\u4e49\uff1b(2)\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u6761\u4ef6\u68c0\u7d22\u7b97\u6cd5\uff0c\u4ece\u957f\u671f\u8bb0\u5fc6\u4e2d\u63d0\u53d6\u7d27\u51d1\u3001\u975e\u5197\u4f59\u3001\u4fe1\u606f\u4e30\u5bcc\u7684\u5019\u9009\u8bb0\u5fc6\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "result": "\u5728NaVQA\uff08\u6df7\u5408\u5ba4\u5185\u5916\u6821\u56ed\u573a\u666f\uff09\u548cWH-VQA\uff08\u4ed3\u5e93\u57fa\u51c6\uff09\u6570\u636e\u96c6\u4e0a\uff0cSTaR\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6210\u529f\u7387\u548c\u663e\u8457\u66f4\u4f4e\u7684\u7a7a\u95f4\u8bef\u5dee\u3002\u5728\u771f\u5b9eHusky\u8f6e\u5f0f\u673a\u5668\u4eba\u4e0a\u7684\u5ba4\u5185\u5916\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u63a8\u7406\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "STaR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5f00\u653e\u52a8\u6001\u573a\u666f\u4e2d\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u667a\u80fd\u63a8\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bb0\u5fc6\u6784\u5efa\u548c\u68c0\u7d22\u673a\u5236\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u667a\u80fd\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2602.09155", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09155", "abs": "https://arxiv.org/abs/2602.09155", "authors": ["Ahmed Rahu", "Brian Shula", "Brandon Combs", "Aqsa Sultana", "Surendra P. Singh", "Vijayan K. Asari", "Derrick Forchetti"], "title": "Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images", "comment": "20 pages, 5 figures", "summary": "Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.", "AI": {"tldr": "\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u4f4e\u7ea7\u522b\u7ba1\u72b6\u817a\u7624\u7684\u6570\u5b57\u5316\u75c5\u7406\u5207\u7247\uff0c\u9884\u6d4b\u60a3\u8005\u672a\u6765\u7ed3\u76f4\u80a0\u764c\u98ce\u9669", "motivation": "\u7ed3\u76f4\u80a0\u764c\u4ecd\u662f\u91cd\u8981\u6b7b\u56e0\uff0c\u5373\u4f7f\u7b5b\u67e5\u80fd\u964d\u4f4e\u53d1\u75c5\u7387\uff0c\u4ecd\u6709\u90e8\u5206\u4f4e\u7ea7\u522b\u817a\u7624\u60a3\u8005\u4f1a\u53d1\u5c55\u4e3a\u7ed3\u76f4\u80a0\u764c\u3002\u4f20\u7edf\u7ec4\u7ec7\u5b66\u8bc4\u4f30\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u9884\u793a\u6076\u6027\u6f5c\u80fd\u7684\u7ec6\u5fae\u7279\u5f81\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u98ce\u9669\u5206\u5c42\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6570\u5b57\u75c5\u7406\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\uff0c\u5168\u9762\u5ba2\u89c2\u5730\u5206\u6790\u4f4e\u7ea7\u522b\u7ba1\u72b6\u817a\u7624\u7684\u6574\u5f20\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\uff0c\u68c0\u6d4b\u7ec6\u5fae\u7684\u7ec4\u7ec7\u5b66\u7279\u5f81\u3002", "result": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u80fd\u5426\u4ece\u4f4e\u7ea7\u522b\u7ba1\u72b6\u817a\u7624\u7684WSIs\u4e2d\u68c0\u6d4b\u51fa\u9884\u6d4b\u60a3\u8005\u957f\u671f\u7ed3\u76f4\u80a0\u764c\u98ce\u9669\u7684\u7ec6\u5fae\u7ec4\u7ec7\u5b66\u7279\u5f81\u3002", "conclusion": "\u6570\u5b57\u75c5\u7406\u548c\u673a\u5668\u5b66\u4e60\u4e3a\u8bc6\u522b\u4f4e\u98ce\u9669\u60a3\u8005\u4e2d\u7684\u9ad8\u98ce\u9669\u4e9a\u7fa4\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u6709\u52a9\u4e8e\u5236\u5b9a\u4e2a\u4f53\u5316\u76d1\u6d4b\u548c\u9884\u9632\u7b56\u7565\u3002"}}
{"id": "2602.09165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09165", "abs": "https://arxiv.org/abs/2602.09165", "authors": ["Hirunima Jayasekara", "Chuong Huynh", "Yixuan Ren", "Christabel Acquaye", "Abhinav Shrivastava"], "title": "All-in-One Conditioning for Text-to-Image Synthesis", "comment": null, "summary": "Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u96f6\u6837\u672c\u6761\u4ef6\u673a\u5236ASQL Conditioner\uff0c\u901a\u8fc7\u8f7b\u91cf\u8bed\u8a00\u6a21\u578b\u751f\u6210\u89c6\u89c9\u6761\u4ef6\uff0c\u5728\u63a8\u7406\u65f6\u4f18\u5316\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u590d\u6742\u63d0\u793a\u4e0b\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u7ec4\u5408\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u63d0\u793a\u65f6\uff0c\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9884\u5b9a\u4e49\u5e03\u5c40\u56fe\uff0c\u4f46\u8fd9\u79cd\u521a\u6027\u7ea6\u675f\u9650\u5236\u4e86\u7ec4\u5408\u7075\u6d3b\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u573a\u666f\u56fe\u7ed3\u6784\u7684\u96f6\u6837\u672c\u6761\u4ef6\u673a\u5236\uff0c\u6838\u5fc3\u662fASQL\uff08\u5c5e\u6027-\u5927\u5c0f-\u6570\u91cf-\u4f4d\u7f6e\uff09\u6761\u4ef6\u5668\u3002\u4f7f\u7528\u8f7b\u91cf\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8f6f\u89c6\u89c9\u6307\u5bfc\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u4f18\u5316\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u652f\u6301\u8f7b\u91cf\u3001\u8fde\u8d2f\u4e14\u591a\u6837\u5316\u7684\u56fe\u50cf\u5408\u6210\uff0c\u63d0\u5347\u4e86\u590d\u6742\u63d0\u793a\u4e0b\u7684\u7ec4\u5408\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u573a\u666f\u56fe\u6846\u67b6\u548cASQL\u6761\u4ef6\u5668\u7684\u8f6f\u89c6\u89c9\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u6587\u672c\u63d0\u793a\u7684\u66f4\u51c6\u786e\u89e3\u91ca\u548c\u89c6\u89c9\u8868\u793a\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u521a\u6027\u5e03\u5c40\u7ea6\u675f\u7684\u9650\u5236\u3002"}}
{"id": "2602.09367", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09367", "abs": "https://arxiv.org/abs/2602.09367", "authors": ["Jinghan Yang", "Jingyi Hou", "Xinbo Yu", "Wei He", "Yifan Wu"], "title": "CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments", "comment": null, "summary": "Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.", "AI": {"tldr": "CAPER\u6846\u67b6\u901a\u8fc7\u8d23\u4efb\u5206\u79bb\u7ed3\u6784\u5b9e\u73b0\u79d1\u5b66\u5b9e\u9a8c\u673a\u5668\u4eba\u64cd\u4f5c\uff1a\u9ad8\u5c42\u4efb\u52a1\u63a8\u7406\u751f\u6210\u7b26\u5408\u7ea6\u675f\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u4e2d\u5c42\u591a\u6a21\u6001\u63a5\u5730\u5b9e\u73b0\u5b50\u4efb\u52a1\uff0c\u4f4e\u5c42\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u7269\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u6210\u529f\u7387\u3001\u7a0b\u5e8f\u6b63\u786e\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u673a\u5668\u4eba\u8f85\u52a9\u9700\u8981\u7a0b\u5e8f\u6b63\u786e\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u3001\u6709\u9650\u76d1\u7763\u4e0b\u7684\u53ef\u9760\u6267\u884c\uff0c\u4ee5\u53ca\u4f4e\u6f14\u793a\u6570\u636e\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7aef\u5230\u7aef\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u9762\u4e34\u6311\u6218\uff0c\u5176\u53ef\u6062\u590d\u9519\u8bef\u5047\u8bbe\u548c\u6570\u636e\u9a71\u52a8\u7b56\u7565\u5b66\u4e60\u5728\u534f\u8bae\u654f\u611f\u5b9e\u9a8c\u4e2d\u5e38\u5e38\u5931\u6548\u3002", "method": "CAPER\u91c7\u7528\u8d23\u4efb\u5206\u79bb\u7ed3\u6784\uff1a1) \u4efb\u52a1\u7ea7\u63a8\u7406\u5728\u663e\u5f0f\u7ea6\u675f\u4e0b\u751f\u6210\u7a0b\u5e8f\u6709\u6548\u7684\u52a8\u4f5c\u5e8f\u5217\uff1b2) \u4e2d\u5c42\u591a\u6a21\u6001\u63a5\u5730\u5b9e\u73b0\u5b50\u4efb\u52a1\uff0c\u4e0d\u5c06\u7a7a\u95f4\u51b3\u7b56\u59d4\u6258\u7ed9\u5927\u8bed\u8a00\u6a21\u578b\uff1b3) \u4f4e\u5c42\u63a7\u5236\u901a\u8fc7\u6700\u5c11\u6f14\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u7269\u7406\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u53ef\u89e3\u91ca\u4e2d\u95f4\u8868\u793a\u7f16\u7801\u7a0b\u5e8f\u627f\u8bfa\u3002", "result": "\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u57fa\u51c6\u548c\u516c\u5171\u957f\u65f6\u7a0b\u64cd\u4f5c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6210\u529f\u7387\u3001\u7a0b\u5e8f\u6b63\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u548c\u957f\u65f6\u7a0b\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CAPER\u901a\u8fc7\u9650\u5236\u5b66\u4e60\u548c\u63a8\u7406\u5728\u89c4\u5212\u63a7\u5236\u7ba1\u9053\u4e2d\u7684\u4f4d\u7f6e\uff0c\u9632\u6b62\u6267\u884c\u65f6\u8fdd\u53cd\u5b9e\u9a8c\u903b\u8f91\uff0c\u63d0\u9ad8\u4e86\u53ef\u63a7\u6027\u3001\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u4e3a\u79d1\u5b66\u5b9e\u9a8c\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09430", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09430", "abs": "https://arxiv.org/abs/2602.09430", "authors": ["Yiwen Pang", "Bo Zhou", "Changjin Li", "Xuanhao Wang", "Shengxiang Xu", "Deng-Bao Wang", "Min-Ling Zhang", "Shimin Di"], "title": "Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments", "comment": null, "summary": "Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.", "AI": {"tldr": "\u63d0\u51faAgentic VLA\u63a8\u7406\u63d2\u4ef6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u673a\u5236\u5728\u79d1\u5b66\u5b9e\u9a8c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u751f\u6210\u8fc7\u6e21\u52a8\u4f5c\u4ee3\u7801\uff0c\u89e3\u51b3VLA\u6a21\u578b\u6267\u884c\u590d\u5408\u4efb\u52a1\u65f6\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u79d1\u5b66\u5b9e\u9a8c\u901a\u5e38\u7531\u591a\u4e2a\u539f\u5b50\u4efb\u52a1\u7ec4\u6210\u7684\u957f\u65f6\u7a0b\u590d\u5408\u4efb\u52a1\uff0c\u73b0\u6709VLA\u6a21\u578b\u867d\u7136\u80fd\u53ef\u9760\u6267\u884c\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u539f\u5b50\u52a8\u4f5c\uff0c\u4f46\u5728\u91cd\u65b0\u7ec4\u5408\u8fd9\u4e9b\u539f\u5b50\u52a8\u4f5c\u5f62\u6210\u590d\u5408\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u8bad\u7ec3\u65f6\u7684\u539f\u5b50\u4efb\u52a1\u4e0e\u63a8\u7406\u65f6\u7684\u590d\u5408\u4efb\u52a1\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6267\u884c\u5fc5\u8981\u7684\u8fc7\u6e21\u64cd\u4f5c\u3002", "method": "\u63d0\u51faAgentic VLA\u63a8\u7406\u63d2\u4ef6\uff0c\u5f15\u5165\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u63a8\u7406\u673a\u5236\uff0c\u5728\u6267\u884c\u987a\u5e8f\u64cd\u4f5c\u4efb\u52a1\u65f6\u8fdb\u884c\u5e72\u9884\u3002\u901a\u8fc7\u663e\u5f0f\u7684\u8fc7\u6e21\u63a8\u7406\u548c\u751f\u6210\u8fc7\u6e21\u6027\u673a\u5668\u4eba\u52a8\u4f5c\u4ee3\u7801\uff0c\u5f15\u5bfcVLA\u6a21\u578b\u5b8c\u6210\u7f3a\u5931\u7684\u8fc7\u6e21\u6b65\u9aa4\uff0c\u5b9e\u73b0\u590d\u5408\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u53ef\u9760\u6267\u884c\u3002", "result": "\u5728\u73b0\u6709\u4eff\u771f\u73af\u5883\u4e2d\u6784\u5efa\u79d1\u5b66\u4eea\u5668\u548c\u5e38\u89c1\u79d1\u5b66\u64cd\u4f5c\u573a\u666f\u76843D\u8d44\u4ea7\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u539f\u5b50\u4efb\u52a1\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad8\u4e8642%\u3002\u540c\u65f6\u5c55\u793a\u65b9\u6cd5\u53ef\u4ee5\u8f7b\u677e\u4ece\u4eff\u771f\u73af\u5883\u8fc1\u79fb\u5230\u771f\u5b9e\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u63a8\u7406\u65f6\u5e72\u9884\u800c\u975e\u989d\u5916\u8bad\u7ec3\uff0c\u8ba1\u7b97\u548c\u6570\u636e\u6548\u7387\u9ad8\uff0c\u9002\u5408\u5f00\u653e\u6027\u548c\u957f\u65f6\u7a0b\u7684\u673a\u5668\u4eba\u5b9e\u9a8c\u5ba4\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u79d1\u5b66\u5b9e\u9a8c\u590d\u5408\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2602.09220", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09220", "abs": "https://arxiv.org/abs/2602.09220", "authors": ["Julien Guit\u00e9-Vinet", "Alexandre Blondin Mass\u00e9", "\u00c9ric Beaudry"], "title": "A Lightweight Multi-View Approach to Short-Term Load Forecasting", "comment": null, "summary": "Time series forecasting is a critical task across domains such as energy, finance, and meteorology, where accurate predictions enable informed decision-making. While transformer-based and large-parameter models have recently achieved state-of-the-art results, their complexity can lead to overfitting and unstable forecasts, especially when older data points become less relevant. In this paper, we propose a lightweight multi-view approach to short-term load forecasting that leverages single-value embeddings and a scaled time-range input to capture temporally relevant features efficiently. We introduce an embedding dropout mechanism to prevent over-reliance on specific features and enhance interpretability. Our method achieves competitive performance with significantly fewer parameters, demonstrating robustness across multiple datasets, including scenarios with noisy or sparse data, and provides insights into the contributions of individual features to the forecast.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u591a\u89c6\u89d2\u77ed\u671f\u8d1f\u8377\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u503c\u5d4c\u5165\u548c\u7f29\u653e\u65f6\u95f4\u8303\u56f4\u8f93\u5165\uff0c\u5f15\u5165\u5d4c\u5165\u4e22\u5f03\u673a\u5236\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u53c2\u6570\u91cf\u5c11\u4f46\u6027\u80fd\u6709\u7ade\u4e89\u529b", "motivation": "\u867d\u7136\u57fa\u4e8eTransformer\u548c\u5927\u53c2\u6570\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\uff0c\u4f46\u5176\u590d\u6742\u6027\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u4e0d\u7a33\u5b9a\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5f53\u65e7\u6570\u636e\u70b9\u76f8\u5173\u6027\u964d\u4f4e\u65f6\u3002\u9700\u8981\u66f4\u8f7b\u91cf\u3001\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u5904\u7406\u77ed\u671f\u8d1f\u8377\u9884\u6d4b", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u591a\u89c6\u89d2\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5355\u503c\u5d4c\u5165\u548c\u7f29\u653e\u65f6\u95f4\u8303\u56f4\u8f93\u5165\u6709\u6548\u6355\u6349\u65f6\u95f4\u76f8\u5173\u7279\u5f81\uff1b2\uff09\u5f15\u5165\u5d4c\u5165\u4e22\u5f03\u673a\u5236\u9632\u6b62\u5bf9\u7279\u5b9a\u7279\u5f81\u7684\u8fc7\u5ea6\u4f9d\u8d56\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027", "result": "\u65b9\u6cd5\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u566a\u58f0\u6216\u7a00\u758f\u6570\u636e\u573a\u666f\uff0c\u5e76\u80fd\u63d0\u4f9b\u5355\u4e2a\u7279\u5f81\u5bf9\u9884\u6d4b\u8d21\u732e\u7684\u6d1e\u5bdf", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u591a\u89c6\u89d2\u65b9\u6cd5\u4e3a\u77ed\u671f\u8d1f\u8377\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027"}}
{"id": "2602.09767", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09767", "abs": "https://arxiv.org/abs/2602.09767", "authors": ["Ruopeng Cui", "Yifei Bi", "Haojie Luo", "Wei Li"], "title": "Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning", "comment": null, "summary": "Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\\% expansion in state-space coverage compared to the baseline.", "AI": {"tldr": "\u63d0\u51faOMoE\u67b6\u6784\u548c\u591a\u5224\u522b\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\uff0c\u9632\u6b62\u6280\u80fd\u8868\u793a\u91cd\u53e0\u548c\u5956\u52b1\u6b3a\u9a97\uff0c\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6280\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u6602\u8d35\u7684\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u3002\u73b0\u6709\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u5355\u4e00\u7b56\u7565\u5b66\u4e60\u591a\u6837\u5316\u6280\u80fd\uff0c\u6ca1\u6709\u5efa\u6a21\u6280\u80fd\u95f4\u7684\u5171\u4eab\u7ed3\u6784\u548c\u5dee\u5f02\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\uff1b2\uff09\u5bb9\u6613\u53d1\u751f\u5956\u52b1\u6b3a\u9a97\uff0c\u5956\u52b1\u4fe1\u53f7\u5feb\u901f\u589e\u52a0\u4f46\u5b9e\u9645\u6280\u80fd\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "1\uff09\u63d0\u51fa\u6b63\u4ea4\u6df7\u5408\u4e13\u5bb6\uff08OMoE\uff09\u67b6\u6784\uff0c\u9632\u6b62\u591a\u6837\u5316\u884c\u4e3a\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u91cd\u53e0\uff0c\u4f7f\u5355\u4e00\u7b56\u7565\u80fd\u591f\u638c\u63e1\u5e7f\u6cdb\u7684\u8fd0\u52a8\u6280\u80fd\uff1b2\uff09\u8bbe\u8ba1\u591a\u5224\u522b\u5668\u6846\u67b6\uff0c\u4e0d\u540c\u5224\u522b\u5668\u5728\u4e0d\u540c\u89c2\u5bdf\u7a7a\u95f4\u4e0a\u64cd\u4f5c\uff0c\u6709\u6548\u7f13\u89e3\u5956\u52b1\u6b3a\u9a97\u95ee\u9898\u3002", "result": "\u572812-DOF Unitree A1\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6280\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u5ea6\u63d0\u5347\u4e8618.3%\u3002", "conclusion": "\u901a\u8fc7OMoE\u67b6\u6784\u548c\u591a\u5224\u522b\u5668\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u4e2d\u7684\u6280\u80fd\u8868\u793a\u91cd\u53e0\u548c\u5956\u52b1\u6b3a\u9a97\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u591a\u6837\u5316\u7684\u6280\u80fd\u5b66\u4e60\u3002"}}
{"id": "2602.09517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09517", "abs": "https://arxiv.org/abs/2602.09517", "authors": ["Sangwon Yu", "Ik-hwan Kim", "Donghun Kang", "Bongkyu Hwang", "Junhwa Choi", "Suk-hoon Jung", "Seungki Hong", "Taehee Lee", "Sungroh Yoon"], "title": "Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models", "comment": null, "summary": "Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.", "AI": {"tldr": "SAKE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u63a8\u7406\u65f6\u4f7f\u7528\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u951a\u5b9a\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u9996\u5c3e\uff0c\u89e3\u51b3LLMs\u5728\u957f\u63a8\u7406\u94fe\u4e2d\u77e5\u8bc6\u6574\u5408\u8870\u51cf\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4e2d\u5b58\u5728\"\u77e5\u8bc6\u6574\u5408\u8870\u51cf\"\u74f6\u9888\uff1a\u968f\u7740\u641c\u7d22\u524d\u751f\u6210\u7684\u63a8\u7406\u957f\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u8d8a\u6765\u8d8a\u96be\u4ee5\u5c06\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u6574\u5408\u5230\u540e\u7eed\u63a8\u7406\u6b65\u9aa4\u4e2d\uff0c\u5373\u4f7f\u76f8\u5173\u4fe1\u606f\u53ef\u7528\u4e5f\u4f1a\u9650\u5236\u6027\u80fd\u3002", "method": "\u63d0\u51faSelf-Anchored Knowledge Encoding (SAKE)\u7b56\u7565\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\u3002\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u5904\u951a\u5b9a\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\uff0c\u9632\u6b62\u77e5\u8bc6\u88ab\u5148\u524d\u4e0a\u4e0b\u6587\u6df9\u6ca1\uff0c\u4ece\u800c\u4fdd\u6301\u5176\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u548c\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAKE\u663e\u8457\u7f13\u89e3\u4e86\u77e5\u8bc6\u6574\u5408\u8870\u51cf\u95ee\u9898\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "SAKE\u4e3a\u667a\u80fd\u4f53\u5f0fLLMs\u4e2d\u7684\u77e5\u8bc6\u6574\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6539\u5584\u77e5\u8bc6\u5229\u7528\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.10063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10063", "abs": "https://arxiv.org/abs/2602.10063", "authors": ["Tianyi Jiang", "Arctanx An", "Hengyi Feng", "Naixin Zhai", "Haodong Li", "Xiaomin Yu", "Jiahui Liu", "Hanwen Du", "Shuo Zhang", "Zhi Yang", "Jie Huang", "Yuhua Li", "Yongxin Ni", "Huacan Wang", "Ronghao Chen"], "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes", "comment": null, "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.", "AI": {"tldr": "\u63d0\u51faChain of Mindset (CoM)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u56db\u79cd\u5f02\u6784\u601d\u7ef4\u6a21\u5f0f\uff08\u7a7a\u95f4\u3001\u6536\u655b\u3001\u53d1\u6563\u3001\u7b97\u6cd5\uff09\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\"\u5355\u4e00\u601d\u7ef4\u9677\u9631\"\uff0c\u5373\u5728\u6240\u6709\u63a8\u7406\u6b65\u9aa4\u4e2d\u5e94\u7528\u76f8\u540c\u7684\u56fa\u5b9a\u601d\u7ef4\u6a21\u5f0f\uff0c\u5ffd\u7565\u4e86\u89e3\u51b3\u540c\u4e00\u95ee\u9898\u4e0d\u540c\u9636\u6bb5\u9700\u8981\u6839\u672c\u4e0d\u540c\u7684\u601d\u7ef4\u6a21\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u8fbe\u5230\u66f4\u9ad8\u667a\u80fd\u6c34\u5e73\u3002", "method": "\u63d0\u51faCoM\u6846\u67b6\uff1a1) \u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u56db\u79cd\u529f\u80fd\u5f02\u6784\u7684\u601d\u7ef4\u6a21\u5f0f\uff1a\u7a7a\u95f4\u601d\u7ef4\u3001\u6536\u655b\u601d\u7ef4\u3001\u53d1\u6563\u601d\u7ef4\u3001\u7b97\u6cd5\u601d\u7ef4\uff1b2) \u5143\u4ee3\u7406\u6839\u636e\u6f14\u5316\u4e2d\u7684\u63a8\u7406\u72b6\u6001\u52a8\u6001\u9009\u62e9\u6700\u4f18\u601d\u7ef4\u6a21\u5f0f\uff1b3) \u53cc\u5411\u4e0a\u4e0b\u6587\u95e8\u63a7\u8fc7\u6ee4\u8de8\u6a21\u5757\u4fe1\u606f\u6d41\u4ee5\u4fdd\u6301\u6548\u7387\u548c\u6548\u679c\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\u3001\u79d1\u5b66QA\u548c\u7a7a\u95f4\u63a8\u7406\u7b49\u516d\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoM\u5728Qwen3-VL-32B-Instruct\u548cGemini-2.0-Flash\u4e0a\u5206\u522b\u4ee54.96%\u548c4.72%\u7684\u603b\u4f53\u51c6\u786e\u7387\u4f18\u52bf\u8d85\u8d8a\u6700\u5f3a\u57fa\u7ebf\uff0c\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5e73\u8861\u63a8\u7406\u6548\u7387\u3002", "conclusion": "CoM\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u591a\u79cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u7a81\u7834\u4e86\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u7684\u5355\u4e00\u601d\u7ef4\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\u7684\u9002\u5e94\u6027\u63a8\u7406\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.09991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09991", "abs": "https://arxiv.org/abs/2602.09991", "authors": ["Fran\u00e7ois Marcoux", "Fran\u00e7ois Grondin"], "title": "Acoustic Drone Package Delivery Detection", "comment": null, "summary": "In recent years, the illicit use of unmanned aerial vehicles (UAVs) for deliveries in restricted area such as prisons became a significant security challenge. While numerous studies have focused on UAV detection or localization, little attention has been given to delivery events identification. This study presents the first acoustic package delivery detection algorithm using a ground-based microphone array. The proposed method estimates both the drone's propeller speed and the delivery event using solely acoustic features. A deep neural network detects the presence of a drone and estimates the propeller's rotation speed or blade passing frequency (BPF) from a mel spectrogram. The algorithm analyzes the BPFs to identify probable delivery moments based on sudden changes before and after a specific time. Results demonstrate a mean absolute error of the blade passing frequency estimator of 16 Hz when the drone is less than 150 meters away from the microphone array. The drone presence detection estimator has a accuracy of 97%. The delivery detection algorithm correctly identifies 96% of events with a false positive rate of 8%. This study shows that deliveries can be identified using acoustic signals up to a range of 100 meters.", "AI": {"tldr": "\u57fa\u4e8e\u5730\u9762\u9ea6\u514b\u98ce\u9635\u5217\u7684\u58f0\u5b66\u5305\u88f9\u6295\u9012\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u65e0\u4eba\u673a\u87ba\u65cb\u6868\u8f6c\u901f\u53d8\u5316\u8bc6\u522b\u6295\u9012\u4e8b\u4ef6", "motivation": "\u8fd1\u5e74\u6765\u65e0\u4eba\u673a\u5728\u76d1\u72f1\u7b49\u9650\u5236\u533a\u57df\u7684\u975e\u6cd5\u6295\u9012\u6210\u4e3a\u91cd\u5927\u5b89\u5168\u9690\u60a3\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u65e0\u4eba\u673a\u68c0\u6d4b\u6216\u5b9a\u4f4d\uff0c\u5bf9\u6295\u9012\u4e8b\u4ef6\u8bc6\u522b\u5173\u6ce8\u4e0d\u8db3", "method": "\u4f7f\u7528\u5730\u9762\u9ea6\u514b\u98ce\u9635\u5217\u91c7\u96c6\u58f0\u5b66\u4fe1\u53f7\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4ece\u6885\u5c14\u9891\u8c31\u56fe\u4e2d\u68c0\u6d4b\u65e0\u4eba\u673a\u5b58\u5728\u5e76\u4f30\u8ba1\u87ba\u65cb\u6868\u8f6c\u901f\uff08\u53f6\u7247\u901a\u8fc7\u9891\u7387\uff09\uff0c\u5206\u6790\u8f6c\u901f\u5728\u7279\u5b9a\u65f6\u95f4\u524d\u540e\u7684\u7a81\u53d8\u6765\u8bc6\u522b\u6295\u9012\u65f6\u523b", "result": "\u65e0\u4eba\u673a150\u7c73\u8303\u56f4\u5185\u53f6\u7247\u901a\u8fc7\u9891\u7387\u4f30\u8ba1\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee16Hz\uff0c\u65e0\u4eba\u673a\u5b58\u5728\u68c0\u6d4b\u51c6\u786e\u738797%\uff0c\u6295\u9012\u4e8b\u4ef6\u8bc6\u522b\u6b63\u786e\u738796%\uff0c\u8bef\u62a5\u73878%\uff0c\u6709\u6548\u68c0\u6d4b\u8303\u56f4\u8fbe100\u7c73", "conclusion": "\u7814\u7a76\u8868\u660e\u58f0\u5b66\u4fe1\u53f7\u53ef\u5728100\u7c73\u8303\u56f4\u5185\u6709\u6548\u8bc6\u522b\u65e0\u4eba\u673a\u6295\u9012\u4e8b\u4ef6\uff0c\u4e3a\u9650\u5236\u533a\u57df\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2602.09528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09528", "abs": "https://arxiv.org/abs/2602.09528", "authors": ["Ziqiang Shi", "Rujie Liu", "Shanshan Yu", "Satoshi Munakata", "Koichi Shirahata"], "title": "Schr\u00f6Mind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schr\u00f6dinger Bridge Problem", "comment": "ICASSP 2026", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose Schr\u00f6Mind-a novel framework reducing hallucinations via solving the Schr\u00f6dinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schr\u00f6dinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.", "AI": {"tldr": "\u63d0\u51faSchr\u00f6Mind\u6846\u67b6\uff0c\u901a\u8fc7\u6c42\u89e3\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u53d7\u9650\uff0c\u4e3b\u8981\u95ee\u9898\u662f\u5e7b\u89c9\u73b0\u8c61\u2014\u2014\u751f\u6210\u7684\u6587\u672c\u4e0e\u89c6\u89c9\u8f93\u5165\u77db\u76fe\u6216\u5ffd\u7565\u89c6\u89c9\u4fe1\u606f\u3002\u867d\u7136\u6a21\u578b\u80fd\u7406\u89e3\u56fe\u50cf\uff0c\u4f46\u96be\u4ee5\u751f\u6210\u51c6\u786e\u7684token\u5e8f\u5217\uff0c\u4e14\u81ea\u56de\u5f52\u7279\u6027\u963b\u788d\u9519\u8bef\u4fee\u6b63", "method": "\u63d0\u51faSchr\u00f6Mind\u6846\u67b6\uff0c\u901a\u8fc7\u6c42\u89e3\u859b\u5b9a\u8c14\u6865\u95ee\u9898\uff0c\u5efa\u7acb\u5e7b\u89c9\u6fc0\u6d3b\u548c\u771f\u5b9e\u6fc0\u6d3b\u4e4b\u95f4\u7684token\u7ea7\u6620\u5c04\uff0c\u4ee5\u6700\u5c0f\u4f20\u8f93\u6210\u672c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u539f\u6709\u80fd\u529b", "result": "\u5728POPE\u548cMME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u5f15\u5165\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500", "conclusion": "Schr\u00f6Mind\u901a\u8fc7\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.09328", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.09328", "abs": "https://arxiv.org/abs/2602.09328", "authors": ["Jiaming Liu", "Cheng Ding", "Daoqiang Zhang"], "title": "In-Hospital Stroke Prediction from PPG-Derived Hemodynamic Features", "comment": "11 pages, 6 figures, 3 tables. To appear in Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '26)", "summary": "The absence of pre-hospital physiological data in standard clinical datasets fundamentally constrains the early prediction of stroke, as patients typically present only after stroke has occurred, leaving the predictive value of continuous monitoring signals such as photoplethysmography (PPG) unvalidated. In this work, we overcome this limitation by focusing on a rare but clinically critical cohort - patients who suffered stroke during hospitalization while already under continuous monitoring - thereby enabling the first large-scale analysis of pre-stroke PPG waveforms aligned to verified onset times. Using MIMIC-III and MC-MED, we develop an LLM-assisted data mining pipeline to extract precise in-hospital stroke onset timestamps from unstructured clinical notes, followed by physician validation, identifying 176 patients (MIMIC) and 158 patients (MC-MED) with high-quality synchronized pre-onset PPG data, respectively. We then extract hemodynamic features from PPG and employ a ResNet-1D model to predict impending stroke across multiple early-warning horizons. The model achieves F1-scores of 0.7956, 0.8759, and 0.9406 at 4, 5, and 6 hours prior to onset on MIMIC-III, and, without re-tuning, reaches 0.9256, 0.9595, and 0.9888 on MC-MED for the same horizons. These results provide the first empirical evidence from real-world clinical data that PPG contains predictive signatures of stroke several hours before onset, demonstrating that passively acquired physiological signals can support reliable early warning, supporting a shift from post-event stroke recognition to proactive, physiology-based surveillance that may materially improve patient outcomes in routine clinical care.", "AI": {"tldr": "\u5229\u7528\u4f4f\u9662\u671f\u95f4\u5df2\u63a5\u53d7\u8fde\u7eed\u76d1\u6d4b\u7684\u5352\u4e2d\u60a3\u8005\u961f\u5217\uff0c\u9996\u6b21\u5927\u89c4\u6a21\u5206\u6790\u5352\u4e2d\u524dPPG\u6ce2\u5f62\uff0c\u8bc1\u660ePPG\u4fe1\u53f7\u5728\u5352\u4e2d\u53d1\u751f\u524d\u6570\u5c0f\u65f6\u5177\u6709\u9884\u6d4b\u4ef7\u503c\u3002", "motivation": "\u6807\u51c6\u4e34\u5e8a\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u9662\u524d\u751f\u7406\u6570\u636e\u9650\u5236\u4e86\u5352\u4e2d\u7684\u65e9\u671f\u9884\u6d4b\uff0c\u56e0\u4e3a\u60a3\u8005\u901a\u5e38\u5728\u5352\u4e2d\u53d1\u751f\u540e\u624d\u5c31\u8bca\uff0c\u5bfc\u81f4\u8fde\u7eed\u76d1\u6d4b\u4fe1\u53f7\uff08\u5982PPG\uff09\u7684\u9884\u6d4b\u4ef7\u503c\u65e0\u6cd5\u9a8c\u8bc1\u3002", "method": "1) \u805a\u7126\u4f4f\u9662\u671f\u95f4\u53d1\u751f\u5352\u4e2d\u4e14\u5df2\u63a5\u53d7\u8fde\u7eed\u76d1\u6d4b\u7684\u7f55\u89c1\u961f\u5217\uff1b2) \u4f7f\u7528LLM\u8f85\u52a9\u6570\u636e\u6316\u6398\u7ba1\u9053\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7cbe\u786e\u7684\u9662\u5185\u5352\u4e2d\u53d1\u4f5c\u65f6\u95f4\u6233\uff1b3) \u4ecePPG\u63d0\u53d6\u8840\u6d41\u52a8\u529b\u5b66\u7279\u5f81\uff1b4) \u4f7f\u7528ResNet-1D\u6a21\u578b\u5728\u591a\u4e2a\u9884\u8b66\u65f6\u95f4\u7a97\u53e3\u9884\u6d4b\u5373\u5c06\u53d1\u751f\u7684\u5352\u4e2d\u3002", "result": "\u5728MIMIC-III\u4e0a\uff0c\u6a21\u578b\u5728\u5352\u4e2d\u53d1\u751f\u524d4\u30015\u30016\u5c0f\u65f6\u5206\u522b\u8fbe\u52300.7956\u30010.8759\u30010.9406\u7684F1\u5206\u6570\uff1b\u5728MC-MED\u4e0a\uff08\u65e0\u9700\u91cd\u65b0\u8c03\u53c2\uff09\u5206\u522b\u8fbe\u52300.9256\u30010.9595\u30010.9888\u3002\u9996\u6b21\u4ece\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u63d0\u4f9bPPG\u5728\u5352\u4e2d\u524d\u6570\u5c0f\u65f6\u5305\u542b\u9884\u6d4b\u7279\u5f81\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002", "conclusion": "\u88ab\u52a8\u83b7\u53d6\u7684\u751f\u7406\u4fe1\u53f7\u53ef\u4ee5\u652f\u6301\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\uff0c\u652f\u6301\u4ece\u5352\u4e2d\u540e\u8bc6\u522b\u5411\u57fa\u4e8e\u751f\u7406\u5b66\u7684\u4e3b\u52a8\u76d1\u6d4b\u8f6c\u53d8\uff0c\u53ef\u80fd\u663e\u8457\u6539\u5584\u5e38\u89c4\u4e34\u5e8a\u62a4\u7406\u4e2d\u7684\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2602.09600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09600", "abs": "https://arxiv.org/abs/2602.09600", "authors": ["Yuxi Wang", "Wenqi Ouyang", "Tianyi Wei", "Yi Dong", "Zhiqi Shen", "Xingang Pan"], "title": "Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures", "comment": null, "summary": "Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Pl\u00fccker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.", "AI": {"tldr": "Hand2World\uff1a\u57fa\u4e8e\u5355\u5f20\u573a\u666f\u56fe\u50cf\u7684\u81ea\u6211\u4e2d\u5fc3\u4ea4\u4e92\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc73D\u624b\u90e8\u7f51\u683c\u6295\u5f71\u548c\u76f8\u673a\u51e0\u4f55\u6ce8\u5165\u89e3\u51b3\u624b\u52bf\u4ea4\u4e92\u89c6\u9891\u5408\u6210\u7684\u6311\u6218\uff0c\u652f\u6301\u4efb\u610f\u957f\u5ea6\u751f\u6210\u548c\u76f8\u673a\u63a7\u5236\u3002", "motivation": "\u589e\u5f3a\u73b0\u5b9e\u548c\u5177\u8eabAI\u9700\u8981\u80fd\u591f\u54cd\u5e94\u624b\u52bf\u8f93\u5165\u3001\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u957f\u671f\u7a33\u5b9a\u6027\u7684\u81ea\u6211\u4e2d\u5fc3\u4ea4\u4e92\u4e16\u754c\u6a21\u578b\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u81ea\u7531\u7a7a\u95f4\u624b\u52bf\u4e0e\u63a5\u89e6\u8bad\u7ec3\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u3001\u5355\u76ee\u89c6\u56fe\u4e2d\u624b\u90e8\u8fd0\u52a8\u4e0e\u76f8\u673a\u8fd0\u52a8\u7684\u6a21\u7cca\u6027\uff0c\u4ee5\u53ca\u9700\u8981\u4efb\u610f\u957f\u5ea6\u89c6\u9891\u751f\u6210\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faHand2World\u7edf\u4e00\u81ea\u56de\u5f52\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u6295\u5f713D\u624b\u90e8\u7f51\u683c\u8fdb\u884c\u906e\u6321\u4e0d\u53d8\u7684\u624b\u90e8\u6761\u4ef6\u5316\uff1b2\uff09\u901a\u8fc7\u9010\u50cf\u7d20Pl\u00fccker\u5c04\u7ebf\u5d4c\u5165\u6ce8\u5165\u663e\u5f0f\u76f8\u673a\u51e0\u4f55\uff1b3\uff09\u5f00\u53d1\u5168\u81ea\u52a8\u5355\u76ee\u6807\u6ce8\u6d41\u7a0b\uff1b4\uff09\u5c06\u53cc\u5411\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u56e0\u679c\u751f\u6210\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u81ea\u6211\u4e2d\u5fc3\u4ea4\u4e92\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHand2World\u5728\u611f\u77e5\u8d28\u91cf\u548c3D\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u652f\u6301\u76f8\u673a\u63a7\u5236\u548c\u957f\u65f6\u57df\u4ea4\u4e92\u751f\u6210\u3002", "conclusion": "Hand2World\u901a\u8fc7\u521b\u65b0\u7684\u624b\u90e8\u6761\u4ef6\u5316\u548c\u76f8\u673a\u51e0\u4f55\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u6211\u4e2d\u5fc3\u4ea4\u4e92\u751f\u6210\u7684\u591a\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u589e\u5f3a\u73b0\u5b9e\u548c\u5177\u8eabAI\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u89c6\u9891\u5408\u6210\u80fd\u529b\u3002"}}
{"id": "2602.09461", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09461", "abs": "https://arxiv.org/abs/2602.09461", "authors": ["Lihao Mai", "Chenhan Xiao", "Yang Weng"], "title": "Scalable and Reliable State-Aware Inference of High-Impact N-k Contingencies", "comment": null, "summary": "Increasing penetration of inverter-based resources, flexible loads, and rapidly changing operating conditions make higher-order $N\\!-\\!k$ contingency assessment increasingly important but computationally prohibitive. Exhaustive evaluation of all outage combinations using AC power-flow or ACOPF is infeasible in routine operation. This fact forces operators to rely on heuristic screening methods whose ability to consistently retain all critical contingencies is not formally established. This paper proposes a scalable, state-aware contingency inference framework designed to directly generate high-impact $N\\!-\\!k$ outage scenarios without enumerating the combinatorial contingency space. The framework employs a conditional diffusion model to produce candidate contingencies tailored to the current operating state, while a topology-aware graph neural network trained only on base and $N\\!-\\!1$ cases efficiently constructs high-risk training samples offline. Finally, the framework is developed to provide controllable coverage guarantees for severe contingencies, allowing operators to explicitly manage the risk of missing critical events under limited AC power-flow evaluation budgets. Experiments on IEEE benchmark systems show that, for a given evaluation budget, the proposed approach consistently evaluates higher-severity contingencies than uniform sampling. This allows critical outages to be identified more reliably with reduced computational effort.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u72b6\u6001\u611f\u77e5\u6545\u969c\u63a8\u7406\u6846\u67b6\uff0c\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u76f4\u63a5\u751f\u6210\u9ad8\u5f71\u54cd\u7684N-k\u6545\u969c\u573a\u666f\uff0c\u907f\u514d\u679a\u4e3e\u7ec4\u5408\u7a7a\u95f4\uff0c\u5e76\u63d0\u4f9b\u53ef\u63a7\u7684\u8986\u76d6\u4fdd\u8bc1\u3002", "motivation": "\u968f\u7740\u9006\u53d8\u5668\u8d44\u6e90\u6e17\u900f\u7387\u589e\u52a0\u3001\u8d1f\u8377\u53d8\u5316\u5feb\u901f\uff0c\u9ad8\u9636N-k\u6545\u969c\u8bc4\u4f30\u53d8\u5f97\u91cd\u8981\u4f46\u8ba1\u7b97\u91cf\u5927\u3002\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u4e0d\u53ef\u884c\uff08\u679a\u4e3e\u6240\u6709\u7ec4\u5408\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u542f\u53d1\u5f0f\u7b5b\u9009\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u8986\u76d6\u6240\u6709\u5173\u952e\u6545\u969c\u3002", "method": "1) \u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6839\u636e\u5f53\u524d\u8fd0\u884c\u72b6\u6001\u751f\u6210\u5019\u9009\u6545\u969c\u573a\u666f\uff1b2) \u4ec5\u4f7f\u7528\u57fa\u6001\u548cN-1\u6848\u4f8b\u8bad\u7ec3\u62d3\u6251\u611f\u77e5\u56fe\u795e\u7ecf\u7f51\u7edc\u79bb\u7ebf\u6784\u5efa\u9ad8\u98ce\u9669\u8bad\u7ec3\u6837\u672c\uff1b3) \u63d0\u4f9b\u53ef\u63a7\u7684\u8986\u76d6\u4fdd\u8bc1\uff0c\u5141\u8bb8\u5728\u6709\u9650AC\u6f6e\u6d41\u8bc4\u4f30\u9884\u7b97\u4e0b\u7ba1\u7406\u9057\u6f0f\u5173\u952e\u4e8b\u4ef6\u7684\u98ce\u9669\u3002", "result": "\u5728IEEE\u57fa\u51c6\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7ed9\u5b9a\u8bc4\u4f30\u9884\u7b97\u4e0b\uff0c\u8be5\u65b9\u6cd5\u6bd4\u5747\u5300\u91c7\u6837\u80fd\u6301\u7eed\u8bc4\u4f30\u66f4\u9ad8\u4e25\u91cd\u6027\u7684\u6545\u969c\uff0c\u4ece\u800c\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u91cf\u66f4\u53ef\u9760\u5730\u8bc6\u522b\u5173\u952e\u6545\u969c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aN-k\u6545\u969c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u76f4\u63a5\u751f\u6210\u9ad8\u5f71\u54cd\u6545\u969c\u573a\u666f\uff0c\u63d0\u4f9b\u53ef\u63a7\u7684\u98ce\u9669\u8986\u76d6\u4fdd\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u6545\u969c\u8bc6\u522b\u6548\u7387\u3002"}}
{"id": "2602.09872", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09872", "abs": "https://arxiv.org/abs/2602.09872", "authors": ["Mridankan Mandal"], "title": "BabyMamba-HAR: Lightweight Selective State Space Models for Efficient Human Activity Recognition on Resource Constrained Devices", "comment": null, "summary": "Human activity recognition (HAR) on wearable and mobile devices is constrained by memory footprint and computational budget, yet competitive accuracy must be maintained across heterogeneous sensor configurations. Selective state space models (SSMs) offer linear time sequence processing with input dependent gating, presenting a compelling alternative to quadratic complexity attention mechanisms. However, the design space for deploying SSMs in the TinyML regime remains largely unexplored. In this paper, BabyMamba-HAR is introduced, a framework comprising two novel lightweight Mamba inspired architectures optimized for resource constrained HAR: (1) CI-BabyMamba-HAR, using a channel independent stem that processes each sensor channel through shared weight, but instance independent transformations to prevent cross channel noise propagation, and (2) Crossover-BiDir-BabyMamba-HAR, using an early fusion stem that achieves channel count independent computational complexity. Both variants incorporate weight tied bidirectional scanning and lightweight temporal attention pooling. Through evaluation across eight diverse benchmarks, it is demonstrated that Crossover-BiDir-BabyMamba-HAR achieves 86.52% average macro F1-score with approximately 27K parameters and 2.21M MACs, matching TinyHAR (86.16%) while requiring 11x fewer MACs on high channel datasets. Systematic ablation studies reveal that bidirectional scanning contributes up to 8.42% F1-score improvement, and gated temporal attention provides up to 8.94% F1-score gain over mean pooling. These findings establish practical design principles for deploying selective state space models as efficient TinyML backbones for HAR.", "AI": {"tldr": "BabyMamba-HAR\uff1a\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u8f7b\u91cf\u7ea7Mamba\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u53d7\u9650\u4e8e\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u9700\u8981\u5728\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u63d0\u4f9b\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u80fd\u529b\uff0c\u662f\u4e8c\u6b21\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728TinyML\u9886\u57df\u7684\u5e94\u7528\u8bbe\u8ba1\u7a7a\u95f4\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faBabyMamba-HAR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7Mamba\u67b6\u6784\uff1a1) CI-BabyMamba-HAR\u4f7f\u7528\u901a\u9053\u72ec\u7acb\u4e3b\u5e72\uff0c\u901a\u8fc7\u5171\u4eab\u6743\u91cd\u4f46\u5b9e\u4f8b\u72ec\u7acb\u7684\u53d8\u6362\u5904\u7406\u6bcf\u4e2a\u4f20\u611f\u5668\u901a\u9053\uff0c\u9632\u6b62\u8de8\u901a\u9053\u566a\u58f0\u4f20\u64ad\uff1b2) Crossover-BiDir-BabyMamba-HAR\u4f7f\u7528\u65e9\u671f\u878d\u5408\u4e3b\u5e72\uff0c\u5b9e\u73b0\u901a\u9053\u6570\u65e0\u5173\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u4e24\u79cd\u53d8\u4f53\u90fd\u5305\u542b\u6743\u91cd\u7ed1\u5b9a\u7684\u53cc\u5411\u626b\u63cf\u548c\u8f7b\u91cf\u7ea7\u65f6\u95f4\u6ce8\u610f\u529b\u6c60\u5316\u3002", "result": "Crossover-BiDir-BabyMamba-HAR\u5728\u516b\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5b8fF1\u5206\u6570\u8fbe\u523086.52%\uff0c\u4ec5\u9700\u7ea627K\u53c2\u6570\u548c2.21M MACs\uff0c\u4e0eTinyHAR\uff0886.16%\uff09\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u5728\u9ad8\u901a\u9053\u6570\u636e\u96c6\u4e0a\u9700\u898111\u500d\u66f4\u5c11\u7684MACs\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u53cc\u5411\u626b\u63cf\u53ef\u5e26\u6765\u9ad8\u8fbe8.42%\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u95e8\u63a7\u65f6\u95f4\u6ce8\u610f\u529b\u76f8\u6bd4\u5e73\u5747\u6c60\u5316\u53ef\u63d0\u4f9b\u9ad8\u8fbe8.94%\u7684F1\u5206\u6570\u589e\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728TinyML\u9886\u57df\u90e8\u7f72\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4f5c\u4e3a\u9ad8\u6548\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u9aa8\u5e72\u7f51\u7edc\u5efa\u7acb\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u8bc1\u660e\u4e86SSMs\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2602.09824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09824", "abs": "https://arxiv.org/abs/2602.09824", "authors": ["Xuhang Wu", "Zhuoxuan Liang", "Wei Li", "Xiaohua Jia", "Sumi Helal"], "title": "PlugSI: Plug-and-Play Test-Time Graph Adaptation for Spatial Interpolation", "comment": "Accepted at DASFAA 2026 (Full Research Paper)", "summary": "With the rapid advancement of IoT and edge computing, sensor networks have become indispensable, driving the need for large-scale sensor deployment. However, the high deployment cost hinders their scalability. To tackle the issues, Spatial Interpolation (SI) introduces virtual sensors to infer readings from observed sensors, leveraging graph structure. However, current graph-based SI methods rely on pre-trained models, lack adaptation to larger and unseen graphs at test-time, and overlook test data utilization. To address these issues, we propose PlugSI, a plug-and-play framework that refines test-time graph through two key innovations. First, we design an Unknown Topology Adapter (UTA) that adapts to the new graph structure of each small-batch at test-time, enhancing the generalization of SI pre-trained models. Second, we introduce a Temporal Balance Adapter (TBA) that maintains a stable historical consensus to guide UTA adaptation and prevent drifting caused by noise in the current batch. Empirically, extensive experiments demonstrate PlugSI can be seamlessly integrated into existing graph-based SI methods and provide significant improvement (e.g., a 10.81% reduction in MAE).", "AI": {"tldr": "PlugSI\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u672a\u77e5\u62d3\u6251\u9002\u914d\u5668\u548c\u65f6\u95f4\u5e73\u8861\u9002\u914d\u5668\uff0c\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u4f18\u5316\u56fe\u7ed3\u6784\uff0c\u63d0\u5347\u7a7a\u95f4\u63d2\u503c\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u548c\u8fb9\u7f18\u8ba1\u7b97\u7684\u53d1\u5c55\uff0c\u5927\u89c4\u6a21\u4f20\u611f\u5668\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u7a7a\u95f4\u63d2\u503c\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u6cd5\u9002\u5e94\u6d4b\u8bd5\u65f6\u66f4\u5927\u6216\u672a\u89c1\u8fc7\u7684\u56fe\u7ed3\u6784\uff0c\u4e14\u5ffd\u7565\u4e86\u6d4b\u8bd5\u6570\u636e\u7684\u5229\u7528\u3002", "method": "\u63d0\u51faPlugSI\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u672a\u77e5\u62d3\u6251\u9002\u914d\u5668(UTA)\uff0c\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u65b0\u56fe\u7ed3\u6784\uff1b2) \u65f6\u95f4\u5e73\u8861\u9002\u914d\u5668(TBA)\uff0c\u7ef4\u62a4\u7a33\u5b9a\u7684\u5386\u53f2\u5171\u8bc6\u6765\u6307\u5bfcUTA\u9002\u5e94\uff0c\u9632\u6b62\u5f53\u524d\u6279\u6b21\u566a\u58f0\u5bfc\u81f4\u7684\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePlugSI\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u7a7a\u95f4\u63d2\u503c\u65b9\u6cd5\u4e2d\uff0c\u5e76\u5e26\u6765\u663e\u8457\u6539\u8fdb\uff08\u4f8b\u5982MAE\u964d\u4f4e10.81%\uff09\u3002", "conclusion": "PlugSI\u901a\u8fc7\u6d4b\u8bd5\u65f6\u56fe\u7ed3\u6784\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7a7a\u95f4\u63d2\u503c\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u4f20\u611f\u5668\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.10014", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10014", "abs": "https://arxiv.org/abs/2602.10014", "authors": ["Chenruo Liu", "Yijun Dong", "Yiqiu Shen", "Qi Lei"], "title": "A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula", "comment": null, "summary": "Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u81ea\u6539\u8fdbLLM\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u4e0b\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u63ed\u793a\u4e86\u53cd\u9988\u5faa\u73af\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u4e86\u8bfe\u7a0b\u5b66\u4e60\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u8fed\u4ee3\u81ea\u6539\u8fdb\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u5728\u6709\u9650\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u7406\u8bba\u57fa\u7840\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u81ea\u6539\u8fdb\u8fc7\u7a0b\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002", "method": "\u5c06\u6bcf\u8f6e\u81ea\u6539\u8fdb\u5efa\u6a21\u4e3a\u5bf9\u5956\u52b1\u8fc7\u6ee4\u5206\u5e03\u7684\u6700\u5927\u4f3c\u7136\u5fae\u8c03\uff0c\u63a8\u5bfc\u6709\u9650\u6837\u672c\u4e0b\u7684\u671f\u671b\u5956\u52b1\u4fdd\u8bc1\u3002\u91c7\u7528\u4efb\u52a1\u4e2d\u5fc3\u89c6\u89d2\uff0c\u8003\u8651\u591a\u96be\u5ea6\u7ea7\u522b\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5206\u6790\u6a21\u578b\u521d\u59cb\u5316\u3001\u4efb\u52a1\u96be\u5ea6\u548c\u6837\u672c\u9884\u7b97\u7b49\u6761\u4ef6\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u660e\u786e\u7684\u53cd\u9988\u5faa\u73af\u673a\u5236\uff1a\u66f4\u597d\u7684\u6a21\u578b\u6bcf\u8f6e\u63a5\u53d7\u66f4\u591a\u6570\u636e\uff0c\u652f\u6301\u6301\u7eed\u81ea\u6539\u8fdb\u540c\u65f6\u89e3\u91ca\u6539\u8fdb\u9971\u548c\u73b0\u8c61\u3002\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u4ece\u6613\u5230\u96be\u7684\u8bfe\u7a0b\u5b66\u4e60\u6bd4\u56fa\u5b9a\u4efb\u52a1\u6df7\u5408\u8bad\u7ec3\u5177\u6709\u66f4\u597d\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u4e3aLLM\u81ea\u6539\u8fdb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5176\u5185\u5728\u673a\u5236\uff0c\u5e76\u4e3a\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u56fe\u63a8\u7406\u4efb\u52a1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\u3002"}}
