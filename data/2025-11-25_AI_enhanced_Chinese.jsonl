{"id": "2511.17618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17618", "abs": "https://arxiv.org/abs/2511.17618", "authors": ["Ju-Young Oh"], "title": "Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach", "comment": "[Master's thesis, Korea University, 2025]", "summary": "Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.", "AI": {"tldr": "FIQ\u6846\u67b6\u901a\u8fc7\u751f\u6210\u63cf\u8ff0\u6027\u95ee\u7b54\u5bf9\u6765\u589e\u5f3a\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728SUTD-TrafficQA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709VQA\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8b\u4ef6\u4e2d\u5fc3\u6807\u6ce8\uff0c\u7f3a\u4e4f\u5bf9\u573a\u666f\u57fa\u7840\u4fe1\u606f\uff08\u5982\u7269\u4f53\u7c7b\u522b\u3001\u7a7a\u95f4\u914d\u7f6e\u3001\u89c6\u89c9\u5c5e\u6027\uff09\u7684\u7406\u89e3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b", "method": "\u63d0\u51faFIQ\u6846\u67b6\uff1a1\uff09\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u63cf\u8ff0\u6027\u4fe1\u606f\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u4e30\u5bcc\u6570\u636e\u96c6\uff1b2\uff09\u8bbe\u8ba1VQ-CAlign\u6a21\u5757\u5bf9\u9f50\u95ee\u9898\u5d4c\u5165\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u4fdd\u7559\u4e0a\u4e0b\u6587\u7ebf\u7d22", "result": "\u5728SUTD-TrafficQA\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cFIQ\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u5bf9\u89c6\u9891\u5185\u5bb9\u7684\u57fa\u7840\u7406\u89e3\uff0cFIQ\u80fd\u6709\u6548\u63d0\u5347VQA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6027\u80fd"}}
{"id": "2511.17572", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.17572", "abs": "https://arxiv.org/abs/2511.17572", "authors": ["Patrick Gerard", "Aiden Chang", "Svitlana Volkova"], "title": "Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs", "comment": "37 pages, EurIPS 2025", "summary": "When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u8ba4\u77e5\u7acb\u573a\u8f6c\u79fb\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5220\u9664\u4e8b\u4ef6\u77e5\u8bc6\u6765\u9a8c\u8bc1\u5bf9\u9f50LLMs\u662f\u5426\u80fd\u5728\u65e0\u77e5\u72b6\u6001\u4e0b\u4ecd\u4fdd\u6301\u793e\u533a\u7279\u5b9a\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u53d1\u73b0\u5373\u4f7f\u79fb\u9664\u4e8b\u5b9e\u4fe1\u606f\uff0cLLMs\u4ecd\u80fd\u7ef4\u6301\u7a33\u5b9a\u7684\u793e\u533a\u7279\u5b9a\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u65b9\u5f0f\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u9f50\u5230\u7279\u5b9a\u5728\u7ebf\u793e\u533a\u65f6\uff0c\u662f\u8868\u73b0\u51fa\u53ef\u6cdb\u5316\u7684\u884c\u4e3a\u6a21\u5f0f\u53cd\u6620\u793e\u533a\u6001\u5ea6\uff0c\u8fd8\u662f\u4ec5\u4ec5\u56de\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u3002", "method": "\u5f15\u5165\u8ba4\u77e5\u7acb\u573a\u8f6c\u79fb\u6d4b\u8bd5\u6846\u67b6\uff1a\u9488\u5bf9\u6027\u5220\u9664\u4e8b\u4ef6\u77e5\u8bc6\u5e76\u7528\u591a\u79cd\u63a2\u9488\u9a8c\u8bc1\uff0c\u7136\u540e\u8bc4\u4f30\u6a21\u578b\u5728\u65e0\u77e5\u72b6\u6001\u4e0b\u662f\u5426\u4ecd\u80fd\u91cd\u73b0\u793e\u533a\u7684\u6709\u673a\u54cd\u5e94\u6a21\u5f0f\u3002\u4f7f\u7528\u4fc4\u4e4c\u519b\u4e8b\u8ba8\u8bba\u548c\u7f8e\u56fd\u515a\u6d3eTwitter\u6570\u636e\u3002", "result": "\u5373\u4f7f\u7ecf\u8fc7\u6fc0\u8fdb\u7684\u4e8b\u5b9e\u79fb\u9664\uff0c\u5bf9\u9f50\u7684LLMs\u4ecd\u4fdd\u6301\u7a33\u5b9a\u7684\u793e\u533a\u7279\u5b9a\u884c\u4e3a\u6a21\u5f0f\u6765\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u5bf9\u9f50\u7f16\u7801\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u6cdb\u5316\u7684\u884c\u4e3a\uff0c\u8d85\u8d8a\u4e86\u8868\u9762\u6a21\u4eff\u3002\u8be5\u6846\u67b6\u4e3a\u68c0\u6d4b\u5728\u65e0\u77e5\u72b6\u6001\u4e0b\u6301\u7eed\u5b58\u5728\u7684\u884c\u4e3a\u504f\u89c1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u5b89\u5168\u900f\u660e\u7684LLM\u90e8\u7f72\u3002"}}
{"id": "2511.17582", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17582", "abs": "https://arxiv.org/abs/2511.17582", "authors": ["Jie Ou", "Shuaihong Jiang", "Yingjun Du", "Cees G. M. Snoek"], "title": "GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning", "comment": "Accepted by AAAI 2026", "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.", "AI": {"tldr": "GateRA\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7token\u611f\u77e5\u7684\u52a8\u6001\u95e8\u63a7\u673a\u5236\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u8c03\u6574PEFT\u66f4\u65b0\u7684\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u9009\u62e9\u6027\u3001token\u7ea7\u522b\u7684\u9002\u914d\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\u5bf9\u6240\u6709token\u5e94\u7528\u9759\u6001\u3001\u8f93\u5165\u65e0\u5173\u7684\u66f4\u65b0\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u8f93\u5165\u7684\u91cd\u8981\u6027\u548c\u96be\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u5728\u7b80\u5355\u5185\u5bb9\u4e0a\u8fc7\u62df\u5408\u6216\u5728\u91cd\u8981\u533a\u57df\u9002\u914d\u4e0d\u8db3\u3002", "method": "\u5728\u6807\u51c6PEFT\u5206\u652f\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u95e8\u63a7\uff0c\u5b9e\u73b0token\u7ea7\u522b\u7684\u9009\u62e9\u6027\u9002\u914d\uff1b\u4f7f\u7528\u57fa\u4e8e\u71b5\u7684\u6b63\u5219\u5316\u9f13\u52b1\u63a5\u8fd1\u4e8c\u5143\u7684\u95e8\u63a7\u51b3\u7b56\uff1b\u7406\u8bba\u5206\u6790\u663e\u793aGateRA\u5728PEFT\u8def\u5f84\u4e0a\u4ea7\u751f\u8f6f\u68af\u5ea6\u63a9\u7801\u6548\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGateRA\u59cb\u7ec8\u4f18\u4e8e\u6216\u5339\u914d\u5148\u524d\u7684PEFT\u65b9\u6cd5\uff1b\u53ef\u89c6\u5316\u663e\u793aGateRA\u80fd\u81ea\u52a8\u6291\u5236\u5197\u4f59\u9884\u586b\u5145token\u7684\u66f4\u65b0\uff0c\u540c\u65f6\u5728\u89e3\u7801\u9636\u6bb5\u5f3a\u8c03\u9002\u914d\u3002", "conclusion": "GateRA\u901a\u8fc7\u52a8\u6001\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u667a\u80fd\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u5c06\u5bb9\u91cf\u96c6\u4e2d\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u4e0a\u3002"}}
{"id": "2511.17813", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.17813", "abs": "https://arxiv.org/abs/2511.17813", "authors": ["Scott Merrill", "Shashank Srivastava"], "title": "Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation", "comment": "8 pages (29 pages including appendix), 18 figures. Code and datasets are available at https://github.com/smerrillunc/action-aware-llms. Submitted to ACL 2026", "summary": "Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this \"action-aware\" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4eceZoom\u4f1a\u8bae\u5f55\u97f3\u751f\u6210\u5e26\u8bf4\u8bdd\u8005\u5c5e\u6027\u8f6c\u5f55\u7684\u6d41\u7a0b\uff0c\u521b\u5efa\u4e86\u4e09\u4e2a\u5730\u65b9\u653f\u5e9c\u5ba1\u8bae\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u5ba1\u8bae\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u751f\u6210\u7684\u533f\u540d\u8bf4\u8bdd\u8005\u6807\u7b7e\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u771f\u5b9e\u591a\u65b9\u5ba1\u8bae\u7684\u80fd\u529b\uff0c\u7f3a\u4e4f\u8bf4\u8bdd\u8005\u5c5e\u6027\u6570\u636e\u662f\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\u5c06\u516c\u5f00Zoom\u5f55\u97f3\u8f6c\u6362\u4e3a\u5e26\u8bf4\u8bdd\u8005\u5c5e\u6027\u7684\u8f6c\u5f55\uff0c\u5305\u542b\u4eba\u7269\u753b\u50cf\u548c\u8bed\u7528\u884c\u4e3a\u6807\u7b7e\uff1b\u57fa\u4e8e\u6b64\u6570\u636e\u5fae\u8c03LLM\u6765\u5efa\u6a21\u7279\u5b9a\u53c2\u4e0e\u8005\u3002", "result": "\u4f7f\u7528\"\u884c\u4e3a\u611f\u77e5\"\u6570\u636e\u5fae\u8c03\u7684LLM\u4f7f\u56f0\u60d1\u5ea6\u964d\u4f4e67%\uff0c\u8bf4\u8bdd\u8005\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u6027\u7684\u5206\u7c7b\u5668\u6027\u80fd\u6307\u6807\u51e0\u4e4e\u7ffb\u500d\uff1b\u56fe\u7075\u5f0f\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6a21\u62df\u5ba1\u8bae\u4e0e\u771f\u5b9e\u5ba1\u8bae\u96be\u4ee5\u533a\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u73b0\u5b9e\u516c\u6c11\u6a21\u62df\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u5ba1\u8bae\u6a21\u62df\u3002"}}
{"id": "2511.17583", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17583", "abs": "https://arxiv.org/abs/2511.17583", "authors": ["Chenrui Ma", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Yanning Shen"], "title": "Learning Straight Flows: Variational Flow Matching for Efficient Generation", "comment": null, "summary": "Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \\textbf{S}traight \\textbf{V}ariational \\textbf{F}low \\textbf{M}atching (\\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \\textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.", "AI": {"tldr": "\u63d0\u51faS-VFM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53d8\u5206\u6f5c\u7801\u6765\u5f3a\u5236\u8f68\u8ff9\u76f4\u7ebf\u5316\uff0c\u89e3\u51b3Flow Matching\u4e2d\u4e00\u6b65\u751f\u6210\u7684\u5c40\u9650\u6027\u95ee\u9898", "motivation": "Flow Matching\u4f9d\u8d56\u5b66\u4e60\u7684\u5f2f\u66f2\u8f68\u8ff9\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e00\u6b65\u751f\u6210\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u79bb\u6563\u8fd1\u4f3c\u8bef\u5dee\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6536\u655b\u56f0\u96be\u7b49\u95ee\u9898", "method": "\u5c06\u53d8\u5206\u6f5c\u7801\uff08\u8868\u793a\"\u751f\u6210\u6982\u89c8\"\uff09\u96c6\u6210\u5230Flow Matching\u6846\u67b6\u4e2d\uff0c\u663e\u5f0f\u5f3a\u5236\u8f68\u8ff9\u76f4\u7ebf\u5316\uff0c\u4ea7\u751f\u7ebf\u6027\u751f\u6210\u8def\u5f84", "result": "\u5728\u4e09\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "S-VFM\u901a\u8fc7\u5f15\u5165\u53d8\u5206\u6f5c\u7801\u6709\u6548\u89e3\u51b3\u4e86Flow Matching\u7684\u8f68\u8ff9\u5f2f\u66f2\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u4e00\u6b65\u751f\u6210"}}
{"id": "2511.17668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17668", "abs": "https://arxiv.org/abs/2511.17668", "authors": ["Ziyuan Gao"], "title": "MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation", "comment": "Accepted by WACV 2026 (round 2)", "summary": "Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedPEFT-CL\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u67b6\u6784\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\u5728\u65b0\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\u5728\u9002\u5e94\u65b0\u89e3\u5256\u7ed3\u6784\u65f6\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u90e8\u7f72\u3002\u9488\u5bf9\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eCLIPSeg\u7684\u53cc\u9636\u6bb5\u67b6\u6784\uff1a\u81ea\u9002\u5e94\u5b66\u4e60\u9636\u6bb5\u4f7f\u7528\u8bed\u4e49\u76f8\u4f3c\u6027\u9002\u914d\u5668\u5206\u914d\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff1b\u77e5\u8bc6\u5de9\u56fa\u9636\u6bb5\u91c7\u7528\u53cc\u5411Fisher\u8bb0\u5fc6\u534f\u8c03\u3002\u5305\u542b\u8bed\u4e49\u9a71\u52a8\u9002\u914d\u5668\u5206\u914d\u673a\u5236\u3001\u53cc\u6a21\u6001LoRA\u9002\u5e94\u548c\u53cc\u5411Fisher\u8bb0\u5fc6\u534f\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6700\u5c0f\u53c2\u6570\u5f00\u9500\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9057\u5fd8\u7f13\u89e3\u548c\u6027\u80fd\u4fdd\u6301\u3002", "conclusion": "MedPEFT-CL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2511.18171", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18171", "abs": "https://arxiv.org/abs/2511.18171", "authors": ["Jasper Nie", "Christian Muise", "Victoria Armstrong"], "title": "BPMN to PDDL: Translating Business Workflows for AI Planning", "comment": "8 pages, 3 figures. Code and generated PDDL outputs available at https://github.com/QuMuLab/bpmn-to-pddl-translation", "summary": "Business Process Model and Notation (BPMN) is a widely used standard for modelling business processes. While automated planning has been proposed as a method for simulating and reasoning about BPMN workflows, most implementations remain incomplete or limited in scope. This project builds upon prior theoretical work to develop a functional pipeline that translates BPMN 2.0 diagrams into PDDL representations suitable for planning. The system supports core BPMN constructs, including tasks, events, sequence flows, and gateways, with initial support for parallel and inclusive gateway behaviour. Using a non-deterministic planner, we demonstrate how to generate and evaluate valid execution traces. Our implementation aims to bridge the gap between theory and practical tooling, providing a foundation for further exploration of translating business processes into well-defined plans.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5c06BPMN 2.0\u56fe\u8f6c\u6362\u4e3aPDDL\u8868\u793a\u7684\u529f\u80fd\u6027\u7ba1\u9053\uff0c\u7528\u4e8e\u4e1a\u52a1\u6d41\u7a0b\u7684\u81ea\u52a8\u89c4\u5212\u548c\u4eff\u771f\u3002", "motivation": "\u867d\u7136\u81ea\u52a8\u89c4\u5212\u5df2\u88ab\u63d0\u51fa\u4f5c\u4e3a\u6a21\u62df\u548c\u63a8\u7406BPMN\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5\uff0c\u4f46\u5927\u591a\u6570\u5b9e\u73b0\u4ecd\u4e0d\u5b8c\u6574\u6216\u8303\u56f4\u6709\u9650\u3002\u8be5\u9879\u76ee\u65e8\u5728\u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\u5de5\u5177\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u6027\u7ba1\u9053\uff0c\u5c06BPMN 2.0\u56fe\u8f6c\u6362\u4e3a\u9002\u5408\u89c4\u5212\u7684PDDL\u8868\u793a\uff0c\u652f\u6301\u6838\u5fc3BPMN\u6784\u9020\uff08\u4efb\u52a1\u3001\u4e8b\u4ef6\u3001\u5e8f\u5217\u6d41\u3001\u7f51\u5173\uff09\uff0c\u5e76\u521d\u6b65\u652f\u6301\u5e76\u884c\u548c\u5305\u542b\u7f51\u5173\u884c\u4e3a\u3002", "result": "\u4f7f\u7528\u975e\u786e\u5b9a\u6027\u89c4\u5212\u5668\u6f14\u793a\u4e86\u5982\u4f55\u751f\u6210\u548c\u8bc4\u4f30\u6709\u6548\u6267\u884c\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u5b9e\u73b0\u4e3a\u5c06\u4e1a\u52a1\u6d41\u7a0b\u8f6c\u6362\u4e3a\u660e\u786e\u5b9a\u4e49\u7684\u89c4\u5212\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.18153", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18153", "abs": "https://arxiv.org/abs/2511.18153", "authors": ["Shreyas Kumar", "Barat S", "Debojit Das", "Yug Desai", "Siddhi Jain", "Rajesh Kumar", "Harish J. Palanthandalam-Madapusi"], "title": "A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies", "comment": "10 pages, 9 figures", "summary": "Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.", "AI": {"tldr": "SnapNet\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u5173\u8282\u901f\u5ea6\u77ac\u53d8\u5b9e\u65f6\u68c0\u6d4b\u5361\u6263\u88c5\u914d\u556e\u5408\uff0c\u7ed3\u5408\u52a8\u6001\u7cfb\u7edf\u53cc\u81c2\u534f\u8c03\u6846\u67b6\u5b9e\u73b0\u7cbe\u786e\u5bf9\u51c6\u548c\u67d4\u987a\u63d2\u5165\uff0c\u5728\u5f02\u8d28\u53cc\u624b\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u548c\u51b2\u51fb\u529b\u964d\u4f4e\u3002", "motivation": "\u7cbe\u5bc6\u5361\u6263\u88c5\u914d\uff08\u5982\u955c\u7247\u63d2\u5165\u773c\u955c\u6846\u6216\u7535\u5b50\u5143\u4ef6\u7ec4\u88c5\uff09\u9700\u8981\u53ca\u65f6\u68c0\u6d4b\u556e\u5408\u5e76\u5feb\u901f\u8870\u51cf\u529b\uff0c\u4ee5\u9632\u6b62\u8fc7\u51b2\u5bfc\u81f4\u7684\u7ec4\u4ef6\u635f\u574f\u6216\u88c5\u914d\u5931\u8d25\u3002", "method": "\u63d0\u51faSnapNet\u8f7b\u91cf\u795e\u7ecf\u7f51\u7edc\u4ece\u5173\u8282\u901f\u5ea6\u77ac\u53d8\u5b9e\u65f6\u68c0\u6d4b\u556e\u5408\uff0c\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\uff1b\u5f00\u53d1\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u53cc\u81c2\u534f\u8c03\u6846\u67b6\uff0c\u96c6\u6210SnapNet\u68c0\u6d4b\u4e0e\u4e8b\u4ef6\u89e6\u53d1\u963b\u6297\u8c03\u5236\u3002", "result": "\u5728\u5f02\u8d28\u53cc\u624b\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff08\u53ec\u56de\u7387\u8d85\u8fc796%\uff09\uff0c\u4e0e\u6807\u51c6\u963b\u6297\u63a7\u5236\u76f8\u6bd4\u5cf0\u503c\u51b2\u51fb\u529b\u964d\u4f4e\u8fbe30%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u89c9\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\u53ef\u9760\u7684\u5361\u6263\u88c5\u914d\u556e\u5408\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e8b\u4ef6\u89e6\u53d1\u963b\u6297\u8c03\u5236\u80fd\u6709\u6548\u51cf\u5c11\u51b2\u51fb\u529b\uff0c\u63d0\u9ad8\u7cbe\u5bc6\u88c5\u914d\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2511.18313", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18313", "abs": "https://arxiv.org/abs/2511.18313", "authors": ["Joseph Oladokun"], "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search", "comment": "10 pages", "summary": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8def\u5f84\u7ea6\u675f\u68c0\u7d22\uff08PCR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u7ed3\u6784\u7ea6\u675f\u548c\u8bed\u4e49\u641c\u7d22\uff0c\u786e\u4fdd\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u7684\u4fe1\u606f\u4fdd\u6301\u903b\u8f91\u5173\u7cfb\uff0c\u63d0\u9ad8LLM\u4ee3\u7406\u63a8\u7406\u7684\u8fde\u8d2f\u6027\u3002", "motivation": "LLM\u4ee3\u7406\u4ece\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u4e0a\u4e0b\u6587\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e0e\u5f53\u524d\u63a8\u7406\u72b6\u6001\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u94fe\u4e0d\u8fde\u8d2f\u3002", "method": "PCR\u65b9\u6cd5\u5c06\u7ed3\u6784\u56fe\u7ea6\u675f\u4e0e\u8bed\u4e49\u641c\u7d22\u76f8\u7ed3\u5408\uff0c\u5c06\u641c\u7d22\u7a7a\u95f4\u9650\u5236\u4e3a\u4ece\u951a\u8282\u70b9\u53ef\u8fbe\u7684\u8282\u70b9\uff0c\u9632\u6b62\u68c0\u7d22\u7ed3\u6784\u65ad\u5f00\u7684\u4fe1\u606f\u3002", "result": "\u5728PathRAG-6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPCR\u5b9e\u73b0100%\u7ed3\u6784\u4e00\u81f4\u6027\uff08\u57fa\u7ebf\u65b9\u6cd5\u4e3a24-32%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u76f8\u5173\u6027\u5f97\u5206\u3002\u5728\u6280\u672f\u9886\u57df\uff0cPCR\u5728\u6392\u540d10\u65f6\u83b7\u5f97\u5b8c\u5168\u76f8\u5173\u6027\u548c\u5b8c\u5168\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u5411\u91cf\u641c\u7d22\u548c\u6df7\u5408\u68c0\u7d22\u3002PCR\u5c06\u68c0\u7d22\u4e0a\u4e0b\u6587\u7684\u5e73\u5747\u56fe\u8ddd\u79bb\u51cf\u5c11\u4e8678%\u3002", "conclusion": "\u8def\u5f84\u7ea6\u675f\u68c0\u7d22\u662f\u63d0\u9ad8LLM\u4ee3\u7406\u63a8\u7406\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u8fde\u8d2f\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.18243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18243", "abs": "https://arxiv.org/abs/2511.18243", "authors": ["Eashan Vytla", "Bhavanishankar Kalavakolanu", "Andrew Perrault", "Matthew McCrink"], "title": "Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters", "comment": null, "summary": "Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u65e0\u4eba\u673a\u63a7\u5236\u7b56\u7565\u6027\u80fd\uff0c\u4f46\u53d1\u73b0\u8be5\u65b9\u6cd5\u4e0e\u6807\u51c6RNN\u6a21\u578b\u90fd\u5b58\u5728\u6cdb\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u65e0\u6cd5\u6536\u655b\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u63a7\u5236\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u548c\u6076\u52a3\u6761\u4ef6\u4e0b\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4f46Dreamer\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u7cfb\u7edf\u4e0a\u5e94\u7528\u56f0\u96be\uff0c\u4e3b\u8981\u662f\u6837\u672c\u6548\u7387\u4f4e\u548c\u52a8\u529b\u5b66\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u65e0\u4eba\u673a\u89c6\u4e3a\u81ea\u7531\u4f53\u7cfb\u7edf\uff0c\u9884\u6d4b\u4f5c\u7528\u5728\u5176\u4e0a\u7684\u51c0\u529b\u548c\u529b\u77e9\uff0c\u7136\u540e\u901a\u8fc76\u81ea\u7531\u5ea6\u9f99\u683c-\u5e93\u5854\u79ef\u5206\u5668\u9884\u6d4b\u672a\u6765\u72b6\u6001\u3002\u4e0e\u6807\u51c6RNN\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u90fd\u8868\u73b0\u826f\u597d\uff0c\u4f46\u90fd\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u8f68\u8ff9\uff0c\u5bfc\u81f4\u72b6\u6001\u9884\u6d4b\u5feb\u901f\u53d1\u6563\uff0c\u963b\u788d\u4e86\u7b56\u7565\u6536\u655b\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u7684\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u867d\u7136\u7406\u8bba\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u6cdb\u5316\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u89e3\u51b3\u72b6\u6001\u9884\u6d4b\u53d1\u6563\u95ee\u9898\u3002"}}
{"id": "2511.18397", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18397", "abs": "https://arxiv.org/abs/2511.18397", "authors": ["Monte MacDiarmid", "Benjamin Wright", "Jonathan Uesato", "Joe Benton", "Jon Kutasov", "Sara Price", "Naia Bouscal", "Sam Bowman", "Trenton Bricken", "Alex Cloud", "Carson Denison", "Johannes Gasteiger", "Ryan Greenblatt", "Jan Leike", "Jack Lindsey", "Vlad Mikulik", "Ethan Perez", "Alex Rodrigues", "Drake Thomas", "Albert Webson", "Daniel Ziegler", "Evan Hubinger"], "title": "Natural Emergent Misalignment from Reward Hacking in Production RL", "comment": null, "summary": "We show that when large language models learn to reward hack on production RL environments, this can result in egregious emergent misalignment. We start with a pretrained model, impart knowledge of reward hacking strategies via synthetic document finetuning or prompting, and train on a selection of real Anthropic production coding environments. Unsurprisingly, the model learns to reward hack. Surprisingly, the model generalizes to alignment faking, cooperation with malicious actors, reasoning about malicious goals, and attempting sabotage when used with Claude Code, including in the codebase for this paper. Applying RLHF safety training using standard chat-like prompts results in aligned behavior on chat-like evaluations, but misalignment persists on agentic tasks. Three mitigations are effective: (i) preventing the model from reward hacking; (ii) increasing the diversity of RLHF safety training; and (iii) \"inoculation prompting\", wherein framing reward hacking as acceptable behavior during training removes misaligned generalization even when reward hacking is learned.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.17632", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17632", "abs": "https://arxiv.org/abs/2511.17632", "authors": ["Bestoun S. Ahmed", "Tommaso Azzalin", "Andreas Kassler", "Andreas Thore", "Hans Lindback"], "title": "Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production", "comment": null, "summary": "We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u667a\u80fd\u5236\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u670d\u52a1\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u94a2\u94c1\u751f\u4ea7\u5382\u7684\u53ef\u6301\u7eed\u6027\u3001\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u5c06\u4f20\u7edf\u5236\u9020\u6d41\u7a0b\u8f6c\u53d8\u4e3a\u667a\u80fd\u7cfb\u7edf\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u6027\u76ee\u6807\uff0c\u5f3a\u8c03MLOps\u5728\u6570\u636e\u9a71\u52a8\u5236\u9020\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u5fae\u670d\u52a1\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u5b9e\u65f6\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728MLOps\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u4f18\u5316\u529f\u7387\u8bbe\u7f6e\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4f18\u5316\u611f\u5e94\u7089\u52a0\u70ed\u3001\u63d0\u9ad8\u64cd\u4f5c\u8d28\u91cf\u3001\u51cf\u5c11\u5de5\u827a\u6d6a\u8d39\uff0c\u5b9e\u73b0\u5236\u9020\u5e9f\u6599\u51cf\u5c11\u548c\u751f\u4ea7\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f20\u7edf\u6d41\u7a0b\u5411\u667a\u80fd\u7cfb\u7edf\u8f6c\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u6b65\u9aa4\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u53ef\u9002\u5e94\u5404\u79cd\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2511.18739", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18739", "abs": "https://arxiv.org/abs/2511.18739", "authors": ["Kaixiang Yang", "Jiarong Liu", "Yupeng Song", "Shuanghua Yang", "Yujue Zhou"], "title": "A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u95ee\u9898\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u6846\u67b6\uff0c\u5c0620\u591a\u4e2a\u5e38\u7528\u6307\u6807\u91cd\u65b0\u89e3\u91ca\u4e3a6\u4e2a\u7ef4\u5ea6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u91cf\u5316\u6307\u6807\u533a\u5206\u771f\u5b9e\u68c0\u6d4b\u4e0e\u968f\u673a\u566a\u58f0\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6307\u6807\u9002\u7528\u6027\u5fc5\u987b\u4e0e\u4efb\u52a1\u76ee\u6807\u5bf9\u9f50\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5e94\u7528\u76ee\u6807\u591a\u6837\u4e14\u6307\u6807\u5047\u8bbe\u5f02\u6784\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7406\u89e3\u4e0d\u540c\u6307\u6807\u9488\u5bf9\u7684\u5177\u4f53\u8bc4\u4f30\u6311\u6218\u3002", "method": "\u5c06\u5e38\u7528\u6307\u6807\u5206\u7c7b\u4e3a6\u4e2a\u7ef4\u5ea6\uff1a\u57fa\u7840\u7cbe\u5ea6\u3001\u65f6\u6548\u6027\u3001\u6807\u7b7e\u5bb9\u9519\u3001\u4eba\u5de5\u6210\u672c\u3001\u9c81\u68d2\u6027\u548c\u8de8\u6570\u636e\u96c6\u53ef\u6bd4\u6027\u3002\u901a\u8fc7\u771f\u5b9e\u3001\u968f\u673a\u548coracle\u68c0\u6d4b\u573a\u666f\u5b9e\u9a8c\uff0c\u5206\u6790\u6307\u6807\u5f97\u5206\u5206\u5e03\u548c\u533a\u5206\u80fd\u529b\u3002", "result": "\u5927\u591a\u6570\u4e8b\u4ef6\u7ea7\u6307\u6807\u5177\u6709\u5f3a\u533a\u5206\u80fd\u529b\uff0c\u4f46NAB\u3001Point-Adjust\u7b49\u5e38\u7528\u6307\u6807\u5bf9\u968f\u673a\u5f97\u5206\u81a8\u80c0\u7684\u62b5\u6297\u80fd\u529b\u6709\u9650\u3002\u6307\u6807\u9002\u7528\u6027\u672c\u8d28\u4e0a\u662f\u4efb\u52a1\u4f9d\u8d56\u7684\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u73b0\u6709\u6307\u6807\u63d0\u4f9b\u4e86\u7edf\u4e00\u5206\u6790\u89c6\u89d2\uff0c\u5e76\u4e3a\u9009\u62e9\u6216\u5f00\u53d1\u66f4\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u9c81\u68d2\u548c\u516c\u5e73\u7684\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2511.18760", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2511.18760", "abs": "https://arxiv.org/abs/2511.18760", "authors": ["Azim Ospanov", "Zijin Feng", "Jiacheng Sun", "Haoli Bai", "Xin Shen", "Farzan Farnia"], "title": "HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs", "comment": null, "summary": "Informal mathematics has been central to modern large language model (LLM) reasoning, offering flexibility and enabling efficient construction of arguments. However, purely informal reasoning is prone to logical gaps and subtle errors that are difficult to detect and correct. In contrast, formal theorem proving provides rigorous, verifiable mathematical reasoning, where each inference step is checked by a trusted compiler in systems such as Lean, but lacks the exploratory freedom of informal problem solving. This mismatch leaves current LLM-based math agents without a principled way to combine the strengths of both paradigms. In this work, we introduce Hermes, the first tool-assisted agent that explicitly interleaves informal reasoning with formally verified proof steps in Lean. The framework performs intermediate formal checking to prevent reasoning drift and employs a memory module that maintains proof continuity across long, multi-step reasoning chains, enabling both exploration and verification within a single workflow. We evaluate Hermes on four challenging mathematical reasoning benchmarks using LLMs of varying parameter scales, from small models to state-of-the-art systems. Across all settings, Hermes reliably improves the reasoning accuracy of base models while substantially reducing token usage and computational cost compared to reward-based approaches. On difficult datasets such as AIME'25, Hermes achieves up to a 67% accuracy improvement while using 80% fewer total inference FLOPs. The implementation and codebase are publicly available at https://github.com/aziksh-ospanov/HERMES.", "AI": {"tldr": "Hermes\u662f\u4e00\u4e2a\u7ed3\u5408\u975e\u6b63\u5f0f\u63a8\u7406\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u6570\u5b66\u63a8\u7406\u4ee3\u7406\u6846\u67b6\uff0c\u5728Lean\u4e2d\u4ea4\u66ff\u4f7f\u7528\u975e\u6b63\u5f0f\u63a8\u7406\u548c\u5f62\u5f0f\u5316\u8bc1\u660e\u6b65\u9aa4\uff0c\u901a\u8fc7\u4e2d\u95f4\u9a8c\u8bc1\u9632\u6b62\u63a8\u7406\u6f02\u79fb\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u975e\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\u7075\u6d3b\u4f46\u5bb9\u6613\u51fa\u9519\uff0c\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u4e25\u8c28\u4f46\u7f3a\u4e4f\u63a2\u7d22\u6027\u3002\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6570\u5b66\u4ee3\u7406\u7f3a\u4e4f\u5c06\u4e24\u79cd\u8303\u5f0f\u4f18\u52bf\u7ed3\u5408\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1Hermes\u6846\u67b6\uff0c\u5728Lean\u4e2d\u4ea4\u66ff\u8fdb\u884c\u975e\u6b63\u5f0f\u63a8\u7406\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u4f7f\u7528\u4e2d\u95f4\u68c0\u67e5\u9632\u6b62\u63a8\u7406\u6f02\u79fb\uff0c\u5e76\u91c7\u7528\u8bb0\u5fc6\u6a21\u5757\u7ef4\u62a4\u591a\u6b65\u63a8\u7406\u7684\u8fde\u7eed\u6027\u3002", "result": "\u5728\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHermes\u53ef\u9760\u5730\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11token\u4f7f\u7528\u548c\u8ba1\u7b97\u6210\u672c\u3002\u5728AIME'25\u7b49\u56f0\u96be\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8fbe67%\uff0c\u603b\u63a8\u7406FLOPs\u51cf\u5c1180%\u3002", "conclusion": "Hermes\u6210\u529f\u5730\u5c06\u975e\u6b63\u5f0f\u63a8\u7406\u7684\u7075\u6d3b\u6027\u4e0e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u4e25\u8c28\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u65e2\u5177\u63a2\u7d22\u6027\u53c8\u53ef\u9a8c\u8bc1\u7684\u7edf\u4e00\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2511.17918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17918", "abs": "https://arxiv.org/abs/2511.17918", "authors": ["Youngsik Yun", "Dongjun Gu", "Youngjung Uh"], "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization", "comment": "Project page: https://bbangsik13.github.io/FASR", "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9891\u7387\u81ea\u9002\u5e94\u9510\u5ea6\u6b63\u5219\u5316\uff08FASR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u89e3\u51b3\u5176\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5bf9\u65b0\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5927\u591a\u6570\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\u7531\u4e8e\u5bf9\u7a00\u758f\u89c2\u6d4b\u7684\u8fc7\u62df\u5408\uff0c\u7f3a\u4e4f\u5bf9\u65b0\u89c6\u89d2\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4ece\u673a\u5668\u5b66\u4e60\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c63DGS\u4f18\u5316\uff0c\u5c06\u65b0\u89c6\u89d2\u5408\u6210\u89c6\u4e3a\u5bf9\u672a\u89c1\u89c6\u89d2\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faFASR\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u6620\u56fe\u50cf\u7684\u5c40\u90e8\u9891\u7387\u6765\u8bbe\u7f6e\u6b63\u5219\u5316\u6743\u91cd\u548c\u90bb\u57df\u534a\u5f84\uff0c\u5728\u4f30\u8ba1\u5c40\u90e8\u9510\u5ea6\u65f6\u9632\u6b62\u8fc7\u5ea6\u6b63\u5219\u5316\u5bfc\u81f4\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u9510\u5ea6\u7684\u60e9\u7f5a\u4e0d\u8db3\u3002", "result": "\u5728\u5404\u79cd\u914d\u7f6e\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u6539\u8fdb\u4e86\u5e7f\u6cdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u9632\u6b62\u4e86\u65b0\u89c6\u89d2\u4e2d\u7684\u6f02\u6d6e\u4f2a\u5f71\uff0c\u5e76\u91cd\u5efa\u4e86SAM\u65b9\u6cd5\u5bb9\u6613\u8fc7\u5ea6\u5e73\u6ed1\u7684\u7cbe\u7ec6\u7ec6\u8282\u3002", "conclusion": "FASR\u65b9\u6cd5\u901a\u8fc7\u9891\u7387\u81ea\u9002\u5e94\u9510\u5ea6\u6b63\u5219\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e863DGS\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.18743", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18743", "abs": "https://arxiv.org/abs/2511.18743", "authors": ["Yu Lei", "Shuzheng Si", "Wei Wang", "Yifei Wu", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context", "comment": null, "summary": "Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.", "AI": {"tldr": "RhinoInsight\u662f\u4e00\u4e2a\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u6e05\u5355\u548c\u8bc1\u636e\u5ba1\u8ba1\u4e24\u4e2a\u63a7\u5236\u673a\u5236\u589e\u5f3aLLM\u4ee3\u7406\u7684\u7a33\u5065\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u63d0\u5347\u7814\u7a76\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u91c7\u7528\u7ebf\u6027\u89c4\u5212-\u641c\u7d22-\u5199\u4f5c-\u62a5\u544a\u6d41\u7a0b\uff0c\u5b58\u5728\u9519\u8bef\u79ef\u7d2f\u548c\u4e0a\u4e0b\u6587\u8150\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u884c\u4e3a\u548c\u4e0a\u4e0b\u6587\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "1) \u53ef\u9a8c\u8bc1\u6e05\u5355\u6a21\u5757\u5c06\u7528\u6237\u9700\u6c42\u8f6c\u5316\u4e3a\u53ef\u8ffd\u8e2a\u5b50\u76ee\u6807\uff0c\u901a\u8fc7\u6279\u8bc4\u8005\u7cbe\u70bc\u5e76\u751f\u6210\u5206\u5c42\u5927\u7eb2\uff1b2) \u8bc1\u636e\u5ba1\u8ba1\u6a21\u5757\u7ed3\u6784\u5316\u641c\u7d22\u5185\u5bb9\uff0c\u8fed\u4ee3\u66f4\u65b0\u5927\u7eb2\u5e76\u4fee\u526a\u566a\u58f0\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u6279\u8bc4\u8005\u7ed1\u5b9a\u9ad8\u8d28\u91cf\u8bc1\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRhinoInsight\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "RhinoInsight\u901a\u8fc7\u6dfb\u52a0\u63a7\u5236\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u6df1\u5ea6\u7814\u7a76\u4e2d\u7684\u7a33\u5065\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2511.18810", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18810", "abs": "https://arxiv.org/abs/2511.18810", "authors": ["Yuxia Fu", "Zhizhen Zhang", "Yuqi Zhang", "Zijian Wang", "Zi Huang", "Yadan Luo"], "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.", "AI": {"tldr": "MergeVLA\u662f\u4e00\u4e2a\u9762\u5411\u5408\u5e76\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u7684LoRA\u9002\u914d\u5668\u548c\u4efb\u52a1\u63a9\u7801\u89e3\u51b3\u591a\u6280\u80fdVLA\u6a21\u578b\u5408\u5e76\u95ee\u9898\uff0c\u5728\u672a\u77e5\u4efb\u52a1\u65f6\u4f7f\u7528\u4efb\u52a1\u8def\u7531\u5668\u8fdb\u884c\u81ea\u9002\u5e94\u9009\u62e9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u5355\u4e00\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6269\u5c55\u5230\u591a\u6280\u80fd\u8bbe\u7f6e\u65f6\u9762\u4e34\u6311\u6218\uff1a\u76f4\u63a5\u5408\u5e76\u4e0d\u540c\u4efb\u52a1\u7684\u4e13\u5bb6\u6a21\u578b\u4f1a\u5bfc\u81f4\u6210\u529f\u7387\u63a5\u8fd1\u96f6\u3002\u9700\u8981\u89e3\u51b3VLA\u6a21\u578b\u65e0\u6cd5\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u638c\u63e1\u591a\u79cd\u6280\u80fd\u7684\u6839\u672c\u95ee\u9898\u3002", "method": "1. \u8bc6\u522bVLA\u5fae\u8c03\u4e2d\u4e0d\u53ef\u5408\u5e76\u6027\u7684\u4e24\u4e2a\u5173\u952e\u6765\u6e90\uff1aLoRA\u9002\u914d\u5668\u53d1\u6563\u548c\u52a8\u4f5c\u4e13\u5bb6\u4e2d\u7684\u8de8\u5757\u4f9d\u8d56\uff1b2. \u63d0\u51faMergeVLA\u67b6\u6784\uff1a\u4f7f\u7528\u4efb\u52a1\u63a9\u7801\u7684\u7a00\u758f\u6fc0\u6d3bLoRA\u9002\u914d\u5668\u4fdd\u6301\u53c2\u6570\u4e00\u81f4\u6027\uff0c\u7528\u4ec5\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u66ff\u6362\u81ea\u6ce8\u610f\u529b\u4ee5\u4fdd\u6301\u4e13\u4e1a\u5316\u5c40\u90e8\u5316\uff1b3. \u6d4b\u8bd5\u65f6\u4f7f\u7528\u4efb\u52a1\u8def\u7531\u5668\u81ea\u9002\u5e94\u9009\u62e9\u4efb\u52a1\u63a9\u7801\u548c\u4e13\u5bb6\u5934\u3002", "result": "\u5728LIBERO\u3001LIBERO-Plus\u3001RoboTwin\u548c\u771f\u5b9eSO101\u673a\u68b0\u81c2\u4e0a\u7684\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cMergeVLA\u5b9e\u73b0\u4e86\u4e0e\u5355\u72ec\u5fae\u8c03\u4e13\u5bb6\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u3001\u4f53\u73b0\u548c\u73af\u5883\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MergeVLA\u901a\u8fc7\u8bbe\u8ba1\u4fdd\u6301\u53ef\u5408\u5e76\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u591a\u6280\u80fd\u8bbe\u7f6e\u4e2d\u7684\u5408\u5e76\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u638c\u63e1\u591a\u79cd\u6280\u80fd\u7684\u5355\u4e00VLA\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.17973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17973", "abs": "https://arxiv.org/abs/2511.17973", "authors": ["Hiroto Honda"], "title": "Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning", "comment": "Accepted to WACV 2026", "summary": "Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.", "AI": {"tldr": "APR\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6297\u6027\u653b\u51fb\u6270\u52a8\u65b0\u4efb\u52a1\u56fe\u50cf\uff0c\u5728\u7ebf\u5408\u6210\u4f2a\u91cd\u653e\u56fe\u50cf\uff0c\u65e0\u9700\u5b58\u50a8\u5386\u53f2\u6837\u672c\uff0c\u89e3\u51b3\u4e86EFCIL\u4e2d\u7684\u53ef\u5851\u6027-\u7a33\u5b9a\u6027\u56f0\u5883\u3002", "motivation": "EFCIL\u9762\u4e34\u5728\u4e0d\u5b58\u50a8\u5386\u53f2\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u65b0\u7c7b\u522b\u540c\u65f6\u4fdd\u7559\u65e7\u77e5\u8bc6\u7684\u6311\u6218\uff0c\u4e3b\u8981\u56f0\u96be\u5728\u4e8e\u53ef\u5851\u6027-\u7a33\u5b9a\u6027\u56f0\u5883\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u6027\u653b\u51fb\u5728\u65b0\u4efb\u52a1\u56fe\u50cf\u4e0a\u751f\u6210\u4f2a\u91cd\u653e\u56fe\u50cf\uff0c\u4ee5\u589e\u5f3a\u7684\u65e7\u7c7b\u5747\u503c\u539f\u578b\u4e3a\u76ee\u6807\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u9632\u6b62\u8bed\u4e49\u6f02\u79fb\uff0c\u5e76\u6821\u51c6\u534f\u65b9\u5dee\u77e9\u9635\u8865\u507f\u8bed\u4e49\u6f02\u79fb\u3002", "result": "\u5728\u6807\u51c6EFCIL\u57fa\u51c6\u6d4b\u8bd5\u7684\u51b7\u542f\u52a8\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "APR\u65b9\u6cd5\u6709\u6548\u8c03\u548c\u4e86\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u65e0\u9700\u5b58\u50a8\u91cd\u653e\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u4f18\u5f02\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2511.18937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18937", "abs": "https://arxiv.org/abs/2511.18937", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials", "comment": "13 pages, 3 tables, 5 figures", "summary": "We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5f62\u5316\u77e5\u8bc6\u7684\u65b9\u6cd5\u6765\u5ba1\u67e5\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u7684\u6cbb\u7597\u76f8\u5173\u4e0d\u826f\u4e8b\u4ef6\uff0c\u901a\u8fc7\u4e3aMedDRA\u6dfb\u52a0\u9690\u85cf\u7684\u533b\u5b66\u77e5\u8bc6\u5c42\u6765\u6539\u8fdbAE\u5206\u6790\u3002", "motivation": "\u6539\u8fdb\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u4e0d\u826f\u4e8b\u4ef6\u7684\u5ba1\u67e5\u8fc7\u7a0b\uff0c\u589e\u5f3aMedDRA\u672f\u8bed\u7cfb\u7edf\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u63d0\u9ad8AE\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\u548c\u6548\u7387\u3002", "method": "\u6784\u5efaSafeterm\u533b\u5b66\u77e5\u8bc6\u5c42\uff0c\u57282D\u5730\u56fe\u4e2d\u6355\u83b7\u672f\u8bed\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff1b\u4f7f\u7528\u6536\u7f29\u53d1\u751f\u7387\u6bd4\u8ba1\u7b97\u6cbb\u7597\u7279\u5f02\u6027\u4e0d\u6210\u6bd4\u4f8b\u6307\u6807\uff1b\u901a\u8fc7\u7cbe\u5ea6\u52a0\u6743\u805a\u5408\u63a8\u5bfc\u805a\u7c7b\u7ea7EBGM\u503c\uff1b\u63d0\u4f9b\u8bed\u4e49\u5730\u56fe\u548c\u671f\u671b\u5ea6-\u4e0d\u6210\u6bd4\u4f8b\u5ea6\u56fe\u4e24\u79cd\u53ef\u89c6\u5316\u8f93\u51fa\u3002", "result": "\u5728\u4e09\u4e2a\u5386\u53f2\u8bd5\u9a8c\u4e2d\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u6e05\u6670\u6062\u590d\u4e86\u6240\u6709\u9884\u671f\u7684\u5b89\u5168\u4fe1\u53f7\uff1b\u5c06MedDRA\u4e0e\u533b\u5b66\u77e5\u8bc6\u5c42\u7ed3\u5408\u63d0\u9ad8\u4e86AE\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u4e3aMedDRA\u6dfb\u52a0\u533b\u5b66\u77e5\u8bc6\u5c42\uff0c\u663e\u8457\u6539\u5584\u4e86\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u4e0d\u826f\u4e8b\u4ef6\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.18037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18037", "abs": "https://arxiv.org/abs/2511.18037", "authors": ["Yunfan Lu", "Nico Messikommer", "Xiaogang Xu", "Liming Chen", "Yuhan Chen", "Nikola Zubic", "Davide Scaramuzza", "Hui Xiong"], "title": "Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation", "comment": null, "summary": "Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u7edf\u8ba1\u6210\u50cf\u566a\u58f0\u6a21\u578b\uff0c\u8054\u5408\u63cf\u8ff0APS\u548cEVS\u50cf\u7d20\u7684\u566a\u58f0\u884c\u4e3a\uff0c\u5e76\u5f00\u53d1\u4e86\u6821\u51c6\u6d41\u7a0b\u548cHESIM\u6a21\u62df\u5668\uff0c\u5728\u591a\u4e2a\u6210\u50cf\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e8b\u4ef6\u5e27\u6df7\u5408\u4f20\u611f\u5668\u96c6\u6210\u4e86APS\u548cEVS\uff0c\u4f46\u590d\u6742\u7684\u7535\u8def\u67b6\u6784\u5f15\u5165\u4e86\u96be\u4ee5\u7406\u89e3\u7684\u566a\u58f0\u6a21\u5f0f\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u566a\u58f0\u7684\u7edf\u4e00\u5efa\u6a21\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u7edf\u8ba1\u7684\u7edf\u4e00\u6210\u50cf\u566a\u58f0\u6a21\u578b\uff0c\u5305\u542b\u5149\u5b50\u6563\u7c92\u566a\u58f0\u3001\u6697\u7535\u6d41\u566a\u58f0\u3001\u56fa\u5b9a\u6a21\u5f0f\u566a\u58f0\u548c\u91cf\u5316\u566a\u58f0\uff0c\u5efa\u7acb\u4e86\u6821\u51c6\u6d41\u7a0b\u6765\u4f30\u8ba1\u566a\u58f0\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u4e86HESIM\u6a21\u62df\u5668\u3002", "result": "\u5728\u4e24\u4e2a\u6df7\u5408\u4f20\u611f\u5668\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u591a\u4e2a\u6210\u50cf\u4efb\u52a1\uff08\u5982\u89c6\u9891\u5e27\u63d2\u503c\u548c\u53bb\u6a21\u7cca\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u6570\u636e\u7684\u5f3a\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u9996\u4e2a\u8054\u5408\u63cf\u8ff0APS\u548cEVS\u566a\u58f0\u884c\u4e3a\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u901a\u8fc7\u6821\u51c6\u548c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.17879", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.17879", "abs": "https://arxiv.org/abs/2511.17879", "authors": ["Yusong Wu", "Stephen Brade", "Teng Ma", "Tia-Jane Fowler", "Enning Yang", "Berker Banar", "Aaron Courville", "Natasha Jaques", "Cheng-Zhi Anna Huang"], "title": "Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction", "comment": null, "summary": "Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u6765\u7f13\u89e3RL\u540e\u8bad\u7ec3\u4e2d\u7684\u5956\u52b1\u7834\u89e3\u95ee\u9898\uff0c\u7528\u4e8e\u65cb\u5f8b\u5230\u548c\u5f26\u4f34\u594f\u751f\u6210\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u591a\u6837\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u8c10\u6ce2\u4e00\u81f4\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5b9e\u65f6\u5373\u5174\u6f14\u594f\u9700\u8981\u5b9e\u65f6\u534f\u8c03\u548c\u9002\u5e94\uff0c\u800c\u4f20\u7edfRL\u540e\u8bad\u7ec3\u4f1a\u56e0\u5956\u52b1\u7834\u89e3\u5bfc\u81f4\u8f93\u51fa\u591a\u6837\u6027\u964d\u4f4e\uff0c\u8fd9\u5728\u97f3\u4e50\u521b\u4f5c\u4e2d\u7279\u522b\u6709\u5bb3\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u540c\u6f14\u5316\u7684\u5224\u522b\u5668\u533a\u5206\u7b56\u7565\u8f68\u8ff9\u548c\u6570\u636e\u5206\u5e03\uff0c\u7b56\u7565\u540c\u65f6\u6700\u5927\u5316\u5224\u522b\u5668\u8f93\u51fa\u548c\u4e00\u81f4\u6027\u5956\u52b1\u4ee5\u9632\u6b62\u8f93\u51fa\u5d29\u6e83\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u6a21\u578b\u5728\u8f93\u51fa\u591a\u6837\u6027\u3001\u8c10\u6ce2\u4e00\u81f4\u6027\u3001\u9002\u5e94\u901f\u5ea6\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u5747\u6709\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u5730\u7f13\u89e3\u4e86\u751f\u6210\u5e8f\u5217\u6a21\u578bRL\u540e\u8bad\u7ec3\u4e2d\u7684\u5956\u52b1\u7834\u89e3\u95ee\u9898\u3002"}}
{"id": "2511.17902", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17902", "abs": "https://arxiv.org/abs/2511.17902", "authors": ["Yifan He", "Haodong Zhang", "Qiuheng Song", "Lin Lei", "Zhenxuan Zeng", "Haoyang He", "Hongyan Wu"], "title": "Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing", "comment": null, "summary": "Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.\n  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.\n  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.", "AI": {"tldr": "\u63d0\u51faDUPLE\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u5149\u7ea4\u4f20\u611f\u5728\u8de8\u90e8\u7f72\u573a\u666f\u4e0b\u7684\u6d3b\u52a8\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u57df\u591a\u539f\u578b\u5b66\u4e60\u3001\u7edf\u8ba1\u5f15\u5bfc\u7f51\u7edc\u548c\u67e5\u8be2\u611f\u77e5\u539f\u578b\u805a\u5408\u6765\u5e94\u5bf9\u4fe1\u53f7\u57df\u504f\u79fb\u3001\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7b49\u6311\u6218\u3002", "motivation": "\u5206\u5e03\u5f0f\u5149\u7ea4\u4f20\u611f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u4e0d\u540c\u5149\u7ea4\u90e8\u7f72\u7c7b\u578b\u5bfc\u81f4\u4fe1\u53f7\u6a21\u5f0f\u5dee\u5f02\uff08\u57df\u504f\u79fb\uff09\u3001\u65b0\u90e8\u7f72\u573a\u666f\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u6e90\u57df\u5185\u6570\u636e\u4e0d\u8db3\u96be\u4ee5\u6355\u83b7\u7c7b\u5185\u591a\u6837\u6027\u3002", "method": "DUPLE\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u53cc\u57df\u591a\u539f\u578b\u5b66\u4e60\u5668\u878d\u5408\u65f6\u9891\u57df\u7279\u5f81\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b\u7edf\u8ba1\u5f15\u5bfc\u7f51\u7edc\u4ece\u539f\u59cb\u7edf\u8ba1\u7279\u5f81\u63a8\u65ad\u57df\u91cd\u8981\u6027\u548c\u539f\u578b\u654f\u611f\u6027\uff1b\u67e5\u8be2\u611f\u77e5\u539f\u578b\u805a\u5408\u6a21\u5757\u81ea\u9002\u5e94\u9009\u62e9\u548c\u7ec4\u5408\u76f8\u5173\u539f\u578b\u3002", "result": "\u5728\u8de8\u90e8\u7f72DFOS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57df\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u8de8\u4e0d\u540c\u5149\u7ea4\u914d\u7f6e\u7684\u9c81\u68d2\u4e8b\u4ef6\u8bc6\u522b\u3002", "conclusion": "DUPLE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86DFOS\u7cfb\u7edf\u4e2d\u7684\u57df\u504f\u79fb\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u8de8\u90e8\u7f72\u573a\u666f\u4e0b\u7684\u6d3b\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18063", "categories": ["cs.CV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2511.18063", "abs": "https://arxiv.org/abs/2511.18063", "authors": ["Gabriela Fernandes"], "title": "A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)", "comment": null, "summary": "Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u865a\u62df\u75c5\u7406\u52a9\u624b\uff0c\u4f7f\u7528EfficientNet-B3\u6a21\u578b\u533a\u5206\u5bab\u9888\u817a\u539f\u4f4d\u764c\u4e0e\u6b63\u5e38\u5bab\u9888\u817a\u7ec4\u7ec7\uff0c\u5728CAISHI\u6570\u636e\u96c6\u4e0a\u8fbe\u523073.23%\u51c6\u786e\u7387\u3002", "motivation": "\u5bab\u9888\u817a\u539f\u4f4d\u764c\u662f\u91cd\u8981\u7684\u764c\u524d\u75c5\u53d8\uff0c\u51c6\u786e\u8bca\u65ad\u5177\u6709\u6311\u6218\u6027\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u9884\u9632\u8fdb\u5c55\u4e3a\u6d78\u6da6\u6027\u817a\u764c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u75282240\u5f20\u4e13\u5bb6\u6807\u6ce8\u7684H&E\u56fe\u50cf\uff0c\u7ecf\u8fc7Macenko\u67d3\u8272\u5f52\u4e00\u5316\u548c\u57fa\u4e8e\u8865\u4e01\u7684\u9884\u5904\u7406\uff0c\u91c7\u7528EfficientNet-B3 CNN\u6a21\u578b\uff0c\u4f7f\u7528\u7c7b\u522b\u5e73\u8861\u91c7\u6837\u548c\u7126\u70b9\u635f\u5931\u51fd\u6570\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u603b\u4f53\u51c6\u786e\u7387\u4e3a0.7323\uff0c\u5f02\u5e38\u7c7bF1\u5206\u65700.75\uff0c\u6b63\u5e38\u7c7bF1\u5206\u65700.71\u3002Grad-CAM\u70ed\u56fe\u663e\u793a\u4e0eAIS\u5f62\u6001\u4e00\u81f4\u7684\u751f\u7269\u5b66\u53ef\u89e3\u91ca\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91caAI\u7cfb\u7edf\u5728\u5bab\u9888\u817a\u75c5\u7406\u5b66\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5728\u7b5b\u67e5\u5de5\u4f5c\u6d41\u7a0b\u3001\u6559\u80b2\u548c\u8d44\u6e90\u532e\u4e4f\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.18089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18089", "abs": "https://arxiv.org/abs/2511.18089", "authors": ["Wenjing Liu", "Qin Ren", "Wen Zhang", "Yuewei Lin", "Chenyu You"], "title": "Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective", "comment": null, "summary": "Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86Together-Then-Apart (TTA)\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u540c\u65f6\u5efa\u6a21\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u8868\u793a\uff0c\u5728\u4fdd\u6301\u6a21\u6001\u7279\u5b9a\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u5f3a\u8c03\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5bfc\u81f4\u8868\u793a\u5d29\u6e83\u548c\u591a\u6837\u6027\u51cf\u5c11\uff0c\u800c\u4fdd\u6301\u6a21\u6001\u7279\u5b9a\u7ed3\u6784\u4e0e\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u6027\u540c\u7b49\u91cd\u8981\u3002", "method": "TTA\u6846\u67b6\u5305\u542bTogether\u9636\u6bb5\uff08\u901a\u8fc7\u5171\u4eab\u539f\u578b\u5bf9\u9f50\u5d4c\u5165\uff0c\u4f7f\u7528\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u76ee\u6807\u81ea\u9002\u5e94\u7a81\u51fa\u4fe1\u606f\u6807\u8bb0\uff09\u548cApart\u9636\u6bb5\uff08\u901a\u8fc7\u6a21\u6001\u951a\u70b9\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u5668\u6700\u5927\u5316\u8868\u793a\u591a\u6837\u6027\uff0c\u9632\u6b62\u7279\u5f81\u5d29\u6e83\uff09\u3002", "result": "\u5728\u4e94\u4e2aTCGA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTTA\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5982\u4f55\u5728\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u4e2d\u8054\u5408\u5b9e\u73b0\u5bf9\u9f50\u548c\u72ec\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.18272", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18272", "abs": "https://arxiv.org/abs/2511.18272", "authors": ["Richard J. Young"], "title": "Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation", "comment": "24 pages, 11 figures, 2 tables", "summary": "Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5728\u533b\u7597\u6587\u6863OCR\u4e2d\u4f7f\u7528\u89c6\u89c9token\u63a9\u7801\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6240\u6709\u63a9\u7801\u7b56\u7565\u6700\u591a\u53ea\u80fd\u51cf\u5c1142.9%\u7684PHI\uff0c\u5bf9\u957f\u683c\u5f0f\u6807\u8bc6\u7b26\u6709\u6548\u4f46\u5bf9\u77ed\u7ed3\u6784\u5316\u6807\u8bc6\u7b26\u65e0\u6548\u3002", "motivation": "\u968f\u7740\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u73af\u5883\u4e2d\u7684OCR\u5e94\u7528\u589e\u52a0\uff0c\u4fdd\u62a4\u5065\u5eb7\u4fe1\u606f(PHI)\u5728\u6587\u6863\u5904\u7406\u8fc7\u7a0b\u4e2d\u7684\u66b4\u9732\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e03\u79cd\u63a9\u7801\u7b56\u7565(V3-V9)\uff0c\u9488\u5bf9\u4e0d\u540c\u67b6\u6784\u5c42\uff0c\u4f7f\u7528100\u4efd\u5408\u6210\u533b\u7597\u8d26\u5355\u6587\u6863\u8bc4\u4f30PHI\u51cf\u5c11\u6548\u679c\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u3002", "result": "\u6240\u6709\u63a9\u7801\u7b56\u7565\u90fd\u6536\u655b\u523042.9%\u7684PHI\u51cf\u5c11\u7387\uff0c\u6210\u529f\u6291\u5236\u957f\u683c\u5f0f\u7a7a\u95f4\u5206\u5e03\u6807\u8bc6\u7b26(100%\u6709\u6548)\uff0c\u4f46\u65e0\u6cd5\u963b\u6b62\u77ed\u7ed3\u6784\u5316\u6807\u8bc6\u7b26(0%\u6709\u6548)\u3002", "conclusion": "\u8fd9\u4e00\u8d1f\u9762\u7ed3\u679c\u786e\u7acb\u4e86\u4ec5\u9760\u89c6\u89c9\u9690\u79c1\u5e72\u9884\u7684\u8fb9\u754c\uff0c\u4e3a\u533a\u5206\u9002\u5408\u89c6\u89c9\u7ea7\u4e0e\u8bed\u8a00\u7ea7\u7f16\u8f91\u7684PHI\u7c7b\u578b\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u5f15\u5bfc\u672a\u6765\u7814\u7a76\u8f6c\u5411\u89e3\u7801\u5668\u7ea7\u5fae\u8c03\u548c\u6df7\u5408\u9632\u5fa1\u67b6\u6784\u3002"}}
{"id": "2511.18314", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18314", "abs": "https://arxiv.org/abs/2511.18314", "authors": ["Yuting Gao", "Wang Lan", "Hengyuan Zhao", "Linjiang Huang", "Si Liu", "Qingpei Guo"], "title": "AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert", "comment": null, "summary": "Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.", "AI": {"tldr": "AnyExperts\u63d0\u51fa\u4e86\u4e00\u79cd\u6309\u9700\u3001\u9884\u7b97\u611f\u77e5\u7684\u52a8\u6001\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u91cd\u8981\u6027\u4e3a\u6bcf\u4e2atoken\u5206\u914d\u53ef\u53d8\u6570\u91cf\u7684\u4e13\u5bb6\u69fd\u4f4d\u6765\u4f18\u5316\u591a\u6a21\u6001MoE\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001MoE\u6a21\u578b\u91c7\u7528\u56fa\u5b9a\u7684\u4e13\u5bb6\u6fc0\u6d3b\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u6a21\u6001\u95f4\u8bed\u4e49\u91cd\u8981\u6027\u7684\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4e0d\u4f18\uff0c\u5197\u4f59token\u4e0e\u5173\u952etoken\u6d88\u8017\u76f8\u540c\u8d44\u6e90\u3002", "method": "\u63d0\u51faAnyExperts\u6846\u67b6\uff1a1\uff09\u4e3a\u6bcf\u4e2atoken\u5206\u914d\u53ef\u53d8\u603b\u6570\u91cf\u7684\u4e13\u5bb6\u69fd\u4f4d\uff1b2\uff09\u603b\u69fd\u4f4d\u6570\u9650\u5236\u5728\u56fa\u5b9a\u8303\u56f4\u5185\uff1b3\uff09\u6bcf\u4e2a\u69fd\u4f4d\u7531\u771f\u5b9e\u4e13\u5bb6\u6216\u865a\u62df\u4e13\u5bb6\u586b\u5145\uff0c\u865a\u62df\u4e13\u5bb6\u6bd4\u4f8b\u4e0a\u9650\u4e3a20%\uff1b4\uff09\u6839\u636e\u8bed\u4e49\u91cd\u8981\u6027\u81ea\u9002\u5e94\u5e73\u8861\u771f\u5b9e\u4e0e\u865a\u62df\u4e13\u5bb6\u6bd4\u4f8b\u3002", "result": "\u5728\u89c6\u89c9\u7406\u89e3\u3001\u97f3\u9891\u7406\u89e3\u548cNLP\u7406\u89e3\u7b49\u4efb\u52a1\u4e2d\uff0cAnyExperts\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u63d0\u5347\u6027\u80fd\uff1a\u5728\u901a\u7528\u56fe\u50cf/\u89c6\u9891\u4efb\u52a1\u4e2d\uff0c\u4f7f\u752840%\u66f4\u5c11\u7684\u771f\u5b9e\u4e13\u5bb6\u6fc0\u6d3b\u8fbe\u5230\u53ef\u6bd4\u7cbe\u5ea6\uff1b\u5728\u6587\u672c\u5bc6\u96c6\u4efb\u52a1\u4e2d\uff0c\u51cf\u5c1110%\u771f\u5b9e\u4e13\u5bb6\u4f7f\u7528\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7684\u3001\u91cd\u8981\u6027\u9a71\u52a8\u7684\u4e13\u5bb6\u5206\u914d\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001MoE\u6a21\u578b\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2511.18331", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18331", "abs": "https://arxiv.org/abs/2511.18331", "authors": ["Sohini Roychowdhury", "Adam Holeman", "Mohammad Amin", "Feng Wei", "Bhaskar Mehta", "Srihari Reddy"], "title": "DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations", "comment": "9 pages, 3 Tables, 5 images. https://openreview.net/pdf?id=oglD54lvcB", "summary": "For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.", "AI": {"tldr": "Dynamix\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u5e8f\u5217\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u76f8\u5173\u6027\u539f\u5219\u548c\u57fa\u4e8e\u4e8b\u4ef6\u7279\u5f81\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u4f18\u5316\u4e8b\u4ef6\u5386\u53f2\u5904\u7406\uff0c\u5728\u4e0d\u727a\u7272\u5e7f\u544a\u9884\u6d4b\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5728\u7ebf\u5e7f\u544a\u63a8\u8350\u7cfb\u7edf\u4e2d\u5904\u7406\u5b8c\u6574\u7684\u7528\u6237-\u5e7f\u544a\u4e92\u52a8\u5386\u53f2\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5e8f\u5217\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6700\u5927\u76f8\u5173\u6027\u539f\u5219\u548c\u57fa\u4e8e\u4e8b\u4ef6\u7279\u5f81\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u4f1a\u8bdd\u548c\u8868\u9762\u7ea7\u522b\u5bf9\u7528\u6237\u4e92\u52a8\u8fdb\u884c\u5206\u7c7b\uff0c\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u79fb\u9664\u548c\u9009\u62e9\u6027\u7279\u5f81\u589e\u5f3a\u6765\u4f18\u5316\u5904\u7406\u3002", "result": "\u52a8\u6001\u8d44\u6e90\u79fb\u9664\u4f7f\u8bad\u7ec3\u548c\u63a8\u7406\u541e\u5410\u91cf\u5206\u522b\u63d0\u9ad81.15%\u548c1.8%\uff0c\u52a8\u6001\u7279\u5f81\u589e\u5f3a\u63d0\u4f9b0.033 NE\u589e\u76ca\uff0c\u540c\u65f6\u63a8\u7406QPS\u63d0\u9ad84.2%\u3002", "conclusion": "Dynamix\u5728\u57fa\u4e8e\u5728\u7ebf\u7528\u6237\u5e8f\u5217\u7684\u63a8\u8350\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6210\u672c\u6548\u7387\u548c\u6027\u80fd\u6539\u8fdb\uff0c\u81ea\u76d1\u7763\u7528\u6237\u5206\u5272\u548c\u8d44\u6e90\u63a2\u7d22\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u590d\u6742\u7279\u5f81\u9009\u62e9\u7b56\u7565\u3002"}}
{"id": "2511.18394", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18394", "abs": "https://arxiv.org/abs/2511.18394", "authors": ["Chinmay Karkar", "Paras Chopra"], "title": "Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking", "comment": null, "summary": "Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.", "AI": {"tldr": "LLMs\u5728\u9884\u6d4b\u793e\u4f1a\u3001\u653f\u6cbb\u548c\u7ecf\u6d4e\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u51fa\u90e8\u5206\u80fd\u529b\uff0c\u4f46\u5176\u9884\u6d4b\u6027\u80fd\u5728\u4e0d\u540c\u9886\u57df\u548c\u63d0\u793a\u6846\u67b6\u4e0b\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6a21\u578b\u622a\u6b62\u65e5\u671f\u540e\u53d1\u751f\u7684\u4e8b\u4ef6\uff0c\u63a2\u7d22\u4e0d\u540c\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6821\u51c6\u3002", "method": "\u5206\u6790\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5728\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u4e0a\u7684\u9884\u6d4b\u8868\u73b0\uff0c\u7814\u7a76\u4e0a\u4e0b\u6587\u3001\u95ee\u9898\u7c7b\u578b\u548c\u5916\u90e8\u77e5\u8bc6\u5bf9\u51c6\u786e\u6027\u548c\u6821\u51c6\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u6dfb\u52a0\u4e8b\u5b9e\u65b0\u95fb\u80cc\u666f\u5982\u4f55\u6539\u53d8\u4fe1\u5ff5\u5f62\u6210\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "result": "LLMs\u7684\u9884\u6d4b\u80fd\u529b\u9ad8\u5ea6\u53ef\u53d8\uff0c\u53d6\u51b3\u4e8e\u63d0\u95ee\u7684\u5185\u5bb9\u548c\u65b9\u5f0f\u3002", "conclusion": "LLMs\u7684\u9884\u6d4b\u80fd\u529b\u4e0d\u662f\u666e\u904d\u4e00\u81f4\u7684\uff0c\u800c\u662f\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5177\u4f53\u9886\u57df\u3001\u95ee\u9898\u6846\u67b6\u548c\u4e0a\u4e0b\u6587\u6761\u4ef6\u3002"}}
{"id": "2511.18344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18344", "abs": "https://arxiv.org/abs/2511.18344", "authors": ["Tianyang Xu", "Jinjie Gu", "Xuefeng Zhu", "XiaoJun Wu", "Josef Kittler"], "title": "A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles", "comment": null, "summary": "With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.", "AI": {"tldr": "\u63d0\u51fa\u4e86MM-UAV\uff0c\u9996\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u65e0\u4eba\u673a\u8ddf\u8e2a\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542bRGB\u3001\u7ea2\u5916\u548c\u4e8b\u4ef6\u4fe1\u53f7\u4e09\u79cd\u6a21\u6001\uff0c\u8986\u76d630\u591a\u4e2a\u6311\u6218\u6027\u573a\u666f\uff0c\u5305\u542b1321\u4e2a\u540c\u6b65\u591a\u6a21\u6001\u5e8f\u5217\u548c280\u4e07\u6807\u6ce8\u5e27\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u591a\u6a21\u6001\u591a\u65e0\u4eba\u673a\u8ddf\u8e2a\u6846\u67b6\uff0c\u5305\u542b\u504f\u79fb\u5f15\u5bfc\u81ea\u9002\u5e94\u5bf9\u9f50\u6a21\u5757\u548c\u81ea\u9002\u5e94\u52a8\u6001\u878d\u5408\u6a21\u5757\u7b49\u521b\u65b0\u6280\u672f\u3002", "motivation": "\u968f\u7740\u4f4e\u7a7a\u65e0\u4eba\u673a\u7684\u666e\u53ca\uff0c\u89c6\u89c9\u591a\u76ee\u6807\u8ddf\u8e2a\u6210\u4e3a\u5173\u952e\u5b89\u5168\u6280\u672f\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u5355\u6a21\u6001\u8ddf\u8e2a\u5bb9\u6613\u5931\u8d25\u3002\u867d\u7136\u591a\u6a21\u6001\u8ddf\u8e2a\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u516c\u5171\u6570\u636e\u96c6\u963b\u788d\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u591a\u65e0\u4eba\u673a\u8ddf\u8e2a\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\u521b\u65b0\uff1a\u504f\u79fb\u5f15\u5bfc\u81ea\u9002\u5e94\u5bf9\u9f50\u6a21\u5757\u89e3\u51b3\u4f20\u611f\u5668\u95f4\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u81ea\u9002\u5e94\u52a8\u6001\u878d\u5408\u6a21\u5757\u5e73\u8861\u4e0d\u540c\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f\u3002\u8fd8\u5f15\u5165\u4e86\u4e8b\u4ef6\u589e\u5f3a\u5173\u8054\u673a\u5236\uff0c\u5229\u7528\u4e8b\u4ef6\u6a21\u6001\u7684\u8fd0\u52a8\u7ebf\u7d22\u8fdb\u884c\u66f4\u53ef\u9760\u7684\u8eab\u4efd\u7ef4\u62a4\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MM-UAV\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u591a\u6a21\u6001\u8ddf\u8e2a\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u65e0\u4eba\u673a\u8ddf\u8e2a\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2511.18396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18396", "abs": "https://arxiv.org/abs/2511.18396", "authors": ["Jinhao Li", "Sarah M. Erfani", "Lei Feng", "James Bailey", "Feng Liu"], "title": "Exploring Weak-to-Strong Generalization for CLIP-based Classification", "comment": "TMLR", "summary": "Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7c7b\u539f\u578b\u5b66\u4e60\uff08CPL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u8f83\u5f31\u6a21\u578b\u76d1\u7763\u8f83\u5f3a\u6a21\u578b\u6765\u63d0\u5347CLIP\u6a21\u578b\u7684\u5206\u7c7b\u80fd\u529b\uff0c\u5728\u9884\u8bad\u7ec3\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u83b7\u5f973.67%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u4eba\u5de5\u76d1\u7763\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u5f53\u6a21\u578b\u8d85\u8d8a\u4eba\u7c7b\u77e5\u8bc6\u65f6\uff0c\u63d0\u4f9b\u51c6\u786e\u53cd\u9988\u53d8\u5f97\u56f0\u96be\u4e14\u4f4e\u6548\u3002\u9700\u8981\u63a2\u7d22\u4f7f\u7528\u8f83\u5f31\u6a21\u578b\u76d1\u7763\u8f83\u5f3a\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7c7b\u539f\u578b\u5b66\u4e60\uff08CPL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u66f4\u5177\u4ee3\u8868\u6027\u7684\u7c7b\u522b\u539f\u578b\u6765\u589e\u5f3aCLIP\u6a21\u578b\u7684\u5206\u7c7b\u80fd\u529b\uff0c\u5728\u5f31\u76d1\u7763\u4e0b\u4f7f\u7528\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u76ee\u6807\u573a\u666f\u4e0b\uff0c\u7279\u522b\u662f\u9884\u8bad\u7ec3\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0cCPL\u65b9\u6cd5\u8868\u73b0\u7a33\u5065\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e863.67%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f31\u5230\u5f3a\u6cdb\u5316\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u540c\u6837\u6709\u6548\uff0cCPL\u65b9\u6cd5\u5728\u6709\u9650\u9884\u8bad\u7ec3\u6761\u4ef6\u4e0b\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.18437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18437", "abs": "https://arxiv.org/abs/2511.18437", "authors": ["Chi Zhang", "Haibo Qiu", "Qiming Zhang", "Yufei Xu", "Zhixiong Zeng", "Siqi Yang", "Peng Shi", "Lin Ma", "Jing Zhang"], "title": "Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.", "AI": {"tldr": "PEARL\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u5206\u652f\u611f\u77e5-\u63a8\u7406\u534f\u540c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5730\u5c06\u63a8\u7406\u951a\u5b9a\u5230\u5df2\u9a8c\u8bc1\u7684\u89c6\u89c9\u8bc1\u636e\u6765\u89e3\u51b3\u4f20\u7edfRLVR\u65b9\u6cd5\u5ffd\u89c6\u89c6\u89c9\u611f\u77e5\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684RLVR\u65b9\u6cd5\u4ec5\u9a8c\u8bc1\u6700\u7ec8\u6587\u672c\u8f93\u51fa\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u611f\u77e5\u8fd9\u4e00\u57fa\u7840\u6b65\u9aa4\uff0c\u5bfc\u81f4\u89c6\u89c9\u5e7b\u89c9\u548c\u5956\u52b1\u653b\u51fb\u95ee\u9898\u3002\u57fa\u4e8e\u6709\u7f3a\u9677\u611f\u77e5\u7684\u63a8\u7406\u672c\u8d28\u4e0a\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faPEARL\u6846\u67b6\uff1a\u4e3a\u6bcf\u4e2a\u63a8\u7406\u95ee\u9898\u751f\u6210\u611f\u77e5\u68c0\u67e5\u6e05\u5355\uff08\u5305\u542b\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u611f\u77e5\u5bfc\u5411\u5b50\u95ee\u9898\uff09\uff0c\u5728\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8f85\u52a9rollout\u83b7\u5f97\u611f\u77e5\u5956\u52b1\uff0c\u8be5\u5956\u52b1\u65e2\u76f4\u63a5\u5f3a\u5316\u6a21\u578b\u611f\u77e5\u80fd\u529b\uff0c\u53c8\u4f5c\u4e3a\u63a8\u7406\u7684\u4fdd\u771f\u5ea6\u95e8\u63a7\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5982\u5728MathVerse\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u63d0\u53479.7%\uff0c\u76f8\u6bd4GRPO\u63d0\u53476.6%\u3002", "conclusion": "PEARL\u901a\u8fc7\u611f\u77e5-\u63a8\u7406\u534f\u540c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u611f\u77e5\u5ffd\u89c6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u80fd\u4e0e\u4e3b\u6d41RL\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2511.18448", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18448", "abs": "https://arxiv.org/abs/2511.18448", "authors": ["Shaoyu Liu", "Jianing Li", "Guanghui Zhao", "Yunjian Zhang", "Xiangyang Ji"], "title": "EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.", "AI": {"tldr": "EventBench\u662f\u4e00\u4e2a\u9488\u5bf9\u4e8b\u4ef6\u89c6\u89c9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u6307\u6807\u548c\u5927\u89c4\u6a21\u4e8b\u4ef6\u6d41\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u4e8b\u4ef6\u7406\u89e3\u3001\u8bc6\u522b\u548c\u7a7a\u95f4\u63a8\u7406\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u5168\u9762\u8bc4\u4f30\u5176\u80fd\u529b\u3002\u73b0\u6709\u7684\u4e8b\u4ef6\u57fa\u51c6\u5728\u5f00\u653e\u6027\u3001\u4efb\u52a1\u591a\u6837\u6027\u3001\u7a7a\u95f4\u7ef4\u5ea6\u6574\u5408\u548c\u6570\u636e\u89c4\u6a21\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faEventBench\u57fa\u51c6\uff0c\u5177\u6709\u56db\u4e2a\u5173\u952e\u7279\u70b9\uff1a1)\u5f00\u653e\u6027\uff0c\u53d1\u5e03\u6240\u6709\u539f\u59cb\u4e8b\u4ef6\u6d41\u548c\u4efb\u52a1\u6307\u4ee4\uff1b2)\u4efb\u52a1\u591a\u6837\u6027\uff0c\u6db5\u76d6\u7406\u89e3\u3001\u8bc6\u522b\u548c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff1b3)\u7a7a\u95f4\u7ef4\u5ea6\u6574\u5408\uff0c\u9996\u521b3D\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff1b4)\u6570\u636e\u89c4\u6a21\u5927\uff0c\u5305\u542b\u8d85\u8fc7100\u4e07\u4e8b\u4ef6-\u6587\u672c\u5bf9\u8bad\u7ec3\u96c6\u3002", "result": "\u8bc4\u4f30\u4e86GPT-5\u3001Gemini-2.5 Pro\u3001Qwen2.5-VL\u3001InternVL3\u548cEventGPT\u7b49\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u57fa\u4e8e\u4e8b\u4ef6\u7684MLLM\u5728\u4e8b\u4ef6\u6d41\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "EventBench\u4e3a\u4e8b\u4ef6\u89c6\u89c9MLLM\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.18727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18727", "abs": "https://arxiv.org/abs/2511.18727", "authors": ["Devansh Agarwal", "Maitreyi Chatterjee", "Biplab Chatterjee"], "title": "LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs", "comment": "Accepted in Proceedings of the 3rd INCOM 2026", "summary": "Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.", "AI": {"tldr": "LogSyn\u6846\u67b6\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u975e\u7ed3\u6784\u5316\u7684\u98de\u673a\u7ef4\u4fee\u65e5\u5fd7\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u5b9e\u73b0\u95ee\u9898-\u89e3\u51b3\u65b9\u6848\u6458\u8981\u751f\u6210\u548c\u4e8b\u4ef6\u5206\u7c7b\uff0c\u4e3a\u822a\u7a7a\u7ef4\u4fee\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8bed\u4e49\u5206\u6790\u548c\u6d1e\u5bdf\u63d0\u53d6\u65b9\u6cd5\u3002", "motivation": "\u98de\u673a\u7ef4\u4fee\u65e5\u5fd7\u5305\u542b\u5b9d\u8d35\u7684\u5b89\u5168\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u5176\u975e\u7ed3\u6784\u5316\u6587\u672c\u683c\u5f0f\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u53d6\u8fd9\u4e9b\u6570\u636e\u4e2d\u7684\u53ef\u64cd\u4f5c\u6d1e\u5bdf\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5c11\u91cf\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u57286,169\u6761\u8bb0\u5f55\u4e0a\u6267\u884c\u53d7\u63a7\u62bd\u8c61\u751f\u6210\uff0c\u603b\u7ed3\u95ee\u9898-\u89e3\u51b3\u65b9\u6848\u53d9\u8ff0\u5e76\u5728\u8be6\u7ec6\u5c42\u6b21\u672c\u4f53\u4e2d\u5206\u7c7b\u4e8b\u4ef6\u3002", "result": "\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u5173\u952e\u6545\u969c\u6a21\u5f0f\uff0c\u4e3a\u7ef4\u4fee\u65e5\u5fd7\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8bed\u4e49\u7ed3\u6784\u5316\u548c\u6d1e\u5bdf\u63d0\u53d6\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6539\u8fdb\u822a\u7a7a\u53ca\u76f8\u5173\u884c\u4e1a\u7684\u7ef4\u4fee\u5de5\u4f5c\u6d41\u7a0b\u548c\u9884\u6d4b\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.18504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18504", "abs": "https://arxiv.org/abs/2511.18504", "authors": ["Md Tasnin Tanvir", "Soumitra Das", "Sk Md Abidar Rahaman", "Ali Shiri Sichani"], "title": "Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression", "comment": "9 pages, 6 figures", "summary": "The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u81ea\u9002\u5e94\u538b\u7f29\u6280\u672fSTTF\u548cANC\uff0c\u5c06\u7b97\u6cd5\u521b\u65b0\u4e0e\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u8fb9\u7f18AI\u5bf9\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u9700\u6c42\u9700\u8981\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u8fd9\u4e9b\u8bbe\u5907\u5177\u6709\u6709\u9650\u7684\u529f\u8017\u548c\u5185\u5b58\u3002", "method": "STTF\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u7684\u53d8\u5316\u68c0\u6d4b\u52a8\u6001\u91cd\u7528\u89c6\u89c9token\uff0cANC\u901a\u8fc7\u5b66\u4e60\u7684\u8def\u7531\u5668\u6709\u6761\u4ef6\u5730\u6fc0\u6d3b\u7f16\u7801\u5668\u5206\u652f\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u573a\u666f\u590d\u6742\u5ea6\u9002\u5e94\u3002", "result": "3B\u53c2\u6570\u7684TinyGPT-STTF\u5728COCO 2017\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230CIDEr 131.2\uff0c\u6bd4LLaVA-1.5 7B\u9ad8\u51fa17.6 CIDEr\u70b9\uff0c\u540c\u65f6\u4f7f\u75282.3\u500d\u66f4\u5c11\u7684\u53c2\u6570\u548c62\u500d\u66f4\u5c11\u7684\u8bbe\u5907\u7aefFLOPs\u3002STTF\u5728\u4e8b\u4ef6\u89c6\u89c9\u4efb\u52a1\u4e2d\u51cf\u5c1184%\u7684\u5e73\u5747token\u6570\uff0cANC\u5728\u4f4e\u8fd0\u52a8\u573a\u666f\u4e2d\u51cf\u5c11\u9ad8\u8fbe90%\u7684FLOPs\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4f7f\u5f97\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u6709\u80fd\u529b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2511.18830", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18830", "abs": "https://arxiv.org/abs/2511.18830", "authors": ["Fang Wang", "Paolo Ceravolo", "Ernesto Damiani"], "title": "Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM", "comment": "12 pages", "summary": "Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u79bb\u4e8b\u4ef6\u548c\u5e8f\u5217\u5c5e\u6027\uff0c\u4f7f\u7528\u6301\u7eed\u65f6\u95f4\u611f\u77e5\u7684\u4f2a\u5d4c\u5165\u77e9\u9635\u5c06\u65f6\u95f4\u91cd\u8981\u6027\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u8868\u793a\uff0c\u4ee5\u89e3\u51b3\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u65f6\u95f4\u4e0d\u89c4\u5219\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u96be\u4ee5\u5904\u7406\u65f6\u95f4\u4e0d\u89c4\u5219\u6027\uff0c\u7279\u522b\u662f\u968f\u673a\u4e8b\u4ef6\u6301\u7eed\u65f6\u95f4\u548c\u91cd\u53e0\u65f6\u95f4\u6233\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u53cc\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\uff0c\u5206\u79bb\u4e8b\u4ef6\u548c\u5e8f\u5217\u5c5e\u6027\uff0c\u4f7f\u7528\u6301\u7eed\u65f6\u95f4\u611f\u77e5\u4f2a\u5d4c\u5165\u77e9\u9635\u3002\u5728B-LSTM\u548cB-GCN\u4e24\u4e2a\u57fa\u7ebf\u5bb6\u65cf\u53ca\u5176\u6301\u7eed\u65f6\u95f4\u611f\u77e5\u53d8\u4f53D-LSTM\u548cD-GCN\u4e0a\u5b9e\u73b0\uff0c\u6240\u6709\u6a21\u578b\u90fd\u5305\u542b\u81ea\u8c03\u8c10\u8d85\u6a21\u578b\u7528\u4e8e\u81ea\u9002\u5e94\u67b6\u6784\u9009\u62e9\u3002", "result": "\u5728\u5e73\u8861\u548c\u4e0d\u5e73\u8861\u7ed3\u679c\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6301\u7eed\u65f6\u95f4\u4f2a\u5d4c\u5165\u8f93\u5165\u6301\u7eed\u6539\u5584\u6cdb\u5316\u80fd\u529b\uff0c\u964d\u4f4e\u6a21\u578b\u590d\u6742\u6027\uff0c\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86\u663e\u5f0f\u65f6\u95f4\u7f16\u7801\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u5065\u58ee\u7684\u5b9e\u9645\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u8bbe\u8ba1\u65b9\u6848\u3002"}}
{"id": "2511.18835", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18835", "abs": "https://arxiv.org/abs/2511.18835", "authors": ["Fang Wang", "Lance Kosca", "Adrienne Kosca", "Marko Gacesa", "Ernesto Damiani"], "title": "Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data", "comment": "6 pages", "summary": "This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.", "AI": {"tldr": "HGNN(O)\u662f\u4e00\u4e2a\u7528\u4e8e\u4e8b\u4ef6\u5e8f\u5217\u6570\u636e\u7ed3\u679c\u9884\u6d4b\u7684AutoML GNN\u8d85\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u81ea\u52a8\u8c03\u4f18\u67b6\u6784\u548c\u8d85\u53c2\u6570\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4e3a\u590d\u6742\u4e8b\u4ef6\u5e8f\u5217\u6570\u636e\u7684\u7ed3\u679c\u9884\u6d4b\u63d0\u4f9b\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684AutoML-GNN\u57fa\u51c6\u65b9\u6cd5\uff0c\u65e0\u9700\u624b\u52a8\u914d\u7f6e\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u67b6\u6784\u548c\u8d85\u53c2\u6570\u3002", "method": "\u6269\u5c55\u4e86\u56db\u79cd\u67b6\u6784\uff08\u5355\u5c42\u3001\u53cc\u5c42\u3001\u53cc\u5c42\u4f2a\u5d4c\u5165\u3001\u53cc\u5c42\u5d4c\u5165\uff09\u548c\u516d\u79cdGNN\u7b97\u5b50\uff0c\u91c7\u7528\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u81ea\u8c03\u4f18\u673a\u5236\uff0c\u5305\u542b\u526a\u679d\u548c\u65e9\u505c\u7b56\u7565\u3002", "result": "\u5728Traffic Fines\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u8d85\u8fc70.98\uff0c\u5728Patients\u6570\u636e\u96c6\u4e0a\u52a0\u6743F1\u5206\u6570\u8fbe\u52300.86\uff0c\u65e0\u9700\u663e\u5f0f\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684AutoML-GNN\u65b9\u6cd5\u4e3a\u590d\u6742\u4e8b\u4ef6\u5e8f\u5217\u6570\u636e\u7684\u7ed3\u679c\u9884\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u57fa\u51c6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18871", "abs": "https://arxiv.org/abs/2511.18871", "authors": ["Jian Lu"], "title": "Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning", "comment": null, "summary": "Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u63a8\u7406\u548c\u8bad\u7ec3\u5206\u79bb\u90e8\u7f72\u7684\u5468\u671f\u6027\u5f02\u6b65\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u52a0\u8f7d\u5668\u548c\u5f15\u5165\u7edf\u4e00\u4e09\u6a21\u578b\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u7b97\u6cd5\u7cbe\u5ea6\u4e0d\u53d8\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4e3b\u6d41RL\u6846\u67b6\u4e2d\u63a8\u7406\u548c\u8bad\u7ec3\u5728\u540c\u4e00\u8bbe\u5907\u4e0a\u540c\u6b65\u6267\u884c\uff0c\u8fd9\u79cd\u8ba1\u7b97\u8026\u5408\u963b\u788d\u4e86\u5e76\u53d1\u63a8\u7406\u548c\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u63a8\u7406\u548c\u8bad\u7ec3\u5206\u79bb\u90e8\u7f72\u7b56\u7565\uff0c\u6539\u8fdb\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u6784\u5efa\u5468\u671f\u6027\u5f02\u6b65\u6846\u67b6\uff0c\u4f7f\u7528\u7edf\u4e00\u4e09\u6a21\u578b\u67b6\u6784\u548c\u5171\u4eab\u63d0\u793a\u6ce8\u610f\u529b\u63a9\u7801\u51cf\u5c11\u91cd\u590d\u8ba1\u7b97\u3002", "result": "\u5728NPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u81f3\u5c11\u4e09\u500d\u7684\u6574\u4f53\u6027\u80fd\u63d0\u5347\uff0c\u7b97\u6cd5\u7cbe\u5ea6\u4e0e\u540c\u6b65\u65b9\u6cd5\u5b8c\u5168\u7b49\u6548\u3002", "conclusion": "\u8be5\u5468\u671f\u6027\u5f02\u6b65\u6846\u67b6\u5177\u6709\u9700\u6c42\u9a71\u52a8\u3001\u72ec\u7acb\u548c\u5f39\u6027\u6269\u5c55\u7684\u80fd\u529b\uff0c\u663e\u793a\u51fa\u5e7f\u6cdb\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.18735", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18735", "abs": "https://arxiv.org/abs/2511.18735", "authors": ["Zhantao Gong", "Liaoyuan Fan", "Qing Guo", "Xun Xu", "Xulei Yang", "Shijie Li"], "title": "Thinking Ahead: Foresight Intelligence in MLLMs and World Models", "comment": "25 pages, 27 figures, submitted to CVPR 2026", "summary": "In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.", "AI": {"tldr": "\u63d0\u51fa\u4e86Foresight Intelligence\u6982\u5ff5\u548cFSU-QA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u524d\u77bb\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5fae\u8c03\u540e\u7684\u5c0f\u6a21\u578b\u80fd\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u524d\u77bb\u667a\u80fd\u8fd9\u4e00\u5173\u952e\u80fd\u529b\uff0c\u800c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u521b\u5efaFSU-QA\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30VLMs\u7684\u524d\u77bb\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u9884\u6d4b\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684VLMs\u5728\u524d\u77bb\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u7ecf\u8fc7FSU-QA\u5fae\u8c03\u7684\u5c0f\u6a21\u578b\u80fd\u5927\u5e45\u8d85\u8d8a\u5927\u578b\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "FSU-QA\u4e3a\u5f00\u53d1\u771f\u6b63\u80fd\u591f\u9884\u6d4b\u548c\u7406\u89e3\u672a\u6765\u4e8b\u4ef6\u7684\u4e0b\u4e00\u4ee3\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2511.18775", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18775", "abs": "https://arxiv.org/abs/2511.18775", "authors": ["Kihyun Na", "Jinyoung Choi", "Injung Kim"], "title": "Rethinking Garment Conditioning in Diffusion-based Virtual Try-On", "comment": "15 pages (including references and supplementary material), 10 figures, 7 tables. Code and pretrained models will be released", "summary": "Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.", "AI": {"tldr": "\u63d0\u51faRe-CatVTON\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u5355UNet\u7684\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6761\u4ef6\u5b66\u4e60\u7b56\u7565\u548c\u76f4\u63a5\u6ce8\u5165\u771f\u5b9e\u670d\u88c5\u6f5c\u5728\u7279\u5f81\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u53ccUNet\u67b6\u6784\u7684\u6269\u6563\u6a21\u578b\u5728\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5de8\u5927\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u5355UNet\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u53ef\u89c6\u5316\u5206\u6790\u548c\u7406\u8bba\u5206\u6790\u63d0\u51fa\u4e09\u4e2a\u5173\u4e8e\u4e0a\u4e0b\u6587\u7279\u5f81\u5b66\u4e60\u7684\u5047\u8bbe\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1Re-CatVTON\u6a21\u578b\u3002\u5f15\u5165\u9488\u5bf9VTON\u7a7a\u95f4\u8fde\u63a5\u6761\u4ef6\u7684\u6539\u8fdb\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u7b56\u7565\uff0c\u5e76\u76f4\u63a5\u6ce8\u5165\u4ece\u5e72\u51c0\u670d\u88c5\u6f5c\u5728\u7279\u5f81\u6d3e\u751f\u7684\u771f\u5b9e\u670d\u88c5\u6f5c\u5728\u7279\u5f81\u4ee5\u9632\u6b62\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "Re-CatVTON\u76f8\u6bd4\u524d\u8eabCatVTON\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u4f4e\u4e8e\u9ad8\u6027\u80fd\u53ccUNet\u6a21\u578bLeffa\u3002\u5728FID\u3001KID\u548cLPIPS\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4ec5SSIM\u7565\u6709\u4e0b\u964d\uff0c\u5efa\u7acb\u4e86\u5355UNet VTON\u6a21\u578b\u7684\u65b0\u6548\u7387-\u6027\u80fd\u5e73\u8861\u3002", "conclusion": "Re-CatVTON\u901a\u8fc7\u6539\u8fdb\u7684\u6761\u4ef6\u5b66\u4e60\u7b56\u7565\u548c\u8bef\u5dee\u9884\u9632\u673a\u5236\uff0c\u4e3a\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u4f18\u5f02\u7684\u5355UNet\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.19090", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19090", "abs": "https://arxiv.org/abs/2511.19090", "authors": ["Shenghan Zhao", "Yuzhen Lin", "Ximeng Yang", "Qiaochu Lu", "Haozhong Xue", "Gaozhe Jiang"], "title": "Optimization of Deep Learning Models for Dynamic Market Behavior Prediction", "comment": null, "summary": "The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5e8f\u5217\u6a21\u578b\u7528\u4e8e\u591a\u65f6\u95f4\u8303\u56f4\u9700\u6c42\u9884\u6d4b\uff0c\u5728\u7535\u5b50\u5546\u52a1\u4ea4\u6613\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u548c\u5148\u8fdbTransformer\u6a21\u578b\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u91d1\u878d\u79d1\u6280\u7684\u53d1\u5c55\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u6d88\u8d39\u8005\u884c\u4e3a\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u6539\u5584\u8d37\u6b3e\u7b56\u7565\u548c\u5e02\u573a\u6548\u7387\u3002\u672c\u6587\u4e13\u6ce8\u4e8e\u96f6\u552e\u5e02\u573a\u884c\u4e3a\uff0c\u660e\u786e\u9884\u6d4b\u76ee\u6807\u4e3aSKU\u7ea7\u522b\u7684\u65e5\u9700\u6c42\u6216\u6536\u5165\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5e8f\u5217\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u65f6\u95f4\u5377\u79ef\u3001\u95e8\u63a7\u5faa\u73af\u6a21\u5757\u548c\u65f6\u95f4\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u4f7f\u7528\u6807\u51c6\u56de\u5f52\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u4e25\u683c\u7684\u65f6\u95f4\u5206\u5272\u6765\u9632\u6b62\u6570\u636e\u6cc4\u9732\u3002", "result": "\u4e0eARIMA/Prophet\u3001LSTM/GRU\u3001LightGBM\u4ee5\u53ca\u5148\u8fdb\u7684Transformer\u9884\u6d4b\u6a21\u578b\uff08TFT\u3001Informer\u3001Autoformer\u3001N-BEATS\uff09\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728MAE\u3001RMSE\u3001sMAPE\u3001MASE\u548cTheil's U_2\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u6301\u7eed\u51c6\u786e\u5ea6\u63d0\u5347\uff0c\u5728\u9ad8\u5cf0/\u8282\u5047\u65e5\u671f\u95f4\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u548c\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6539\u8fdb\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u73b0\u7ec6\u8282\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002\u8be5\u6df7\u5408\u6a21\u578b\u5728\u591a\u65f6\u95f4\u8303\u56f4\u9700\u6c42\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.19328", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19328", "abs": "https://arxiv.org/abs/2511.19328", "authors": ["Rohan Saha", "Farzane Aminmansour", "Alona Fyshe"], "title": "Understanding the Staged Dynamics of Transformers in Learning Latent Structure", "comment": "Preprint", "summary": "While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5b66\u4e60\u6f5c\u5728\u7ed3\u6784\u7684\u52a8\u6001\u8fc7\u7a0b\uff0c\u53d1\u73b0\u5728Alchemy\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u901a\u8fc7\u79bb\u6563\u9636\u6bb5\u9010\u6b65\u5b66\u4e60\uff0c\u5148\u638c\u63e1\u7c97\u7c92\u5ea6\u89c4\u5219\uff0c\u518d\u5b66\u4e60\u5b8c\u6574\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u5e76\u63ed\u793a\u4e86\u7ec4\u5408\u4e0e\u5206\u89e3\u80fd\u529b\u7684\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u867d\u7136Transformer\u80fd\u591f\u4ece\u4e0a\u4e0b\u6587\u4e2d\u53d1\u73b0\u6f5c\u5728\u7ed3\u6784\uff0c\u4f46\u5176\u83b7\u53d6\u4e0d\u540c\u6f5c\u5728\u7ed3\u6784\u7ec4\u4ef6\u7684\u52a8\u6001\u8fc7\u7a0b\u4ecd\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u7406\u89e3Transformer\u5b66\u4e60\u6f5c\u5728\u7ed3\u6784\u7684\u673a\u5236\u548c\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528Alchemy\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bad\u7ec3\u5c0f\u578b\u4ec5\u89e3\u7801\u5668Transformer\u5904\u7406\u4e09\u4e2a\u4efb\u52a1\u53d8\u4f53\uff1a\u4ece\u90e8\u5206\u4e0a\u4e0b\u6587\u63a8\u65ad\u7f3a\u5931\u89c4\u5219\u3001\u7ec4\u5408\u7b80\u5355\u89c4\u5219\u89e3\u51b3\u591a\u6b65\u5e8f\u5217\u3001\u5206\u89e3\u590d\u6742\u591a\u6b65\u793a\u4f8b\u63a8\u65ad\u4e2d\u95f4\u6b65\u9aa4\u3002\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u4e8b\u4ef6\u6765\u5206\u6790\u5b66\u4e60\u52a8\u6001\u3002", "result": "\u6a21\u578b\u4ee5\u79bb\u6563\u9636\u6bb5\u83b7\u53d6\u80fd\u529b\uff0c\u5148\u5b66\u4e60\u7c97\u7c92\u5ea6\u89c4\u5219\uff0c\u540e\u5b66\u4e60\u5b8c\u6574\u6f5c\u5728\u7ed3\u6784\u3002\u53d1\u73b0\u5173\u952e\u4e0d\u5bf9\u79f0\u6027\uff1a\u6a21\u578b\u80fd\u7a33\u5065\u7ec4\u5408\u57fa\u672c\u89c4\u5219\uff0c\u4f46\u96be\u4ee5\u5206\u89e3\u590d\u6742\u793a\u4f8b\u6765\u53d1\u73b0\u57fa\u672c\u89c4\u5219\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7406\u89e3Transformer\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u6f5c\u5728\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fd9\u4e9b\u80fd\u529b\u5982\u4f55\u9010\u6b65\u6f14\u5316\u7684\u7ec6\u7c92\u5ea6\u89c6\u56fe\u3002"}}
{"id": "2511.18792", "categories": ["cs.CV", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18792", "abs": "https://arxiv.org/abs/2511.18792", "authors": ["Cheng Jiang", "Yihe Yan", "Yanxiang Wang", "Chun Tung Chou", "Wen Hu"], "title": "Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing", "comment": null, "summary": "While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical \"domain shift\" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21MAE\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u572814\u4e2aWi-Fi CSI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u5bf9\u8de8\u57df\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\uff0c\u53d1\u73b0\u6570\u636e\u800c\u975e\u6a21\u578b\u5bb9\u91cf\u662fWi-Fi\u611f\u77e5\u6cdb\u5316\u7684\u5f53\u524d\u74f6\u9888\u3002", "motivation": "Wi-Fi\u611f\u77e5\u867d\u7136\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u8de8\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u3001\u786c\u4ef6\u6216\u7528\u6237\u95f4\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u4e14\u788e\u7247\u5316\u3002", "method": "\u91c7\u7528\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u4f7f\u7528\u63a9\u7801\u81ea\u7f16\u7801(MAE)\u98ce\u683c\u572814\u4e2aWi-Fi CSI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5305\u542b130\u4e07\u6837\u672c\uff0c\u8986\u76d64\u79cd\u8bbe\u5907\u30012.4/5/6 GHz\u9891\u6bb5\u548c20-160 MHz\u5e26\u5bbd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1) \u9884\u8bad\u7ec3\u6570\u636e\u91cf\u589e\u52a0\u5e26\u6765\u5bf9\u6570\u7ebf\u6027\u6539\u8fdb\uff1b2) \u5f53\u524d\u6570\u636e\u91cf\u4e0b\u66f4\u5927\u6a21\u578b\u4ec5\u63d0\u4f9b\u8fb9\u9645\u589e\u76ca\uff1b3) \u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7b49\u4efb\u52a1\u4e0a\u63d0\u5347\u8de8\u57df\u51c6\u786e\u73872.2%-15.7%\u3002", "conclusion": "\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u662fWi-Fi\u611f\u77e5\u9886\u57df\u6cdb\u5316\u7684\u5173\u952e\uff0c\u4e3a\u8bbe\u8ba1\u5b9e\u9645\u90e8\u7f72\u7684\u9c81\u68d2Wi-Fi\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2511.18823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18823", "abs": "https://arxiv.org/abs/2511.18823", "authors": ["Fufangchen Zhao", "Liao Zhang", "Daiqi Shi", "Yuanjun Gao", "Chen Ye", "Yang Cai", "Jian Gao", "Danfeng Yan"], "title": "VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models", "comment": null, "summary": "We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct \"key-information-missing\" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.", "AI": {"tldr": "VideoPerceiver\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u63d0\u5347\u89c6\u9891\u7406\u89e3\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u9488\u5bf9\u77ed\u7247\u6bb5\u4e2d\u7684\u77ac\u65f6\u52a8\u4f5c\u548c\u957f\u89c6\u9891\u4e2d\u7684\u7f55\u89c1\u4e8b\u4ef6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u77ed\u89c6\u9891\u4e2d\u7684\u77ac\u65f6\u52a8\u4f5c\u548c\u957f\u89c6\u9891\u4e2d\u7684\u7f55\u89c1\u77ac\u6001\u4e8b\u4ef6\u65b9\u9762\u7684\u80fd\u529b\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u6784\u5efa'\u5173\u952e\u4fe1\u606f\u7f3a\u5931'\u89c6\u9891\uff0c\u901a\u8fc7\u66ff\u6362\u5173\u952e\u5e27\u5e76\u8054\u5408\u7f16\u7801\u539f\u59cb\u548c\u4fee\u6539\u89c6\u9891token\uff0c\u4f7f\u7528\u8f85\u52a9\u5bf9\u6bd4\u635f\u5931\u5bf9\u9f50\u4e2d\u95f4\u89c6\u89c9\u8868\u793a\uff1b\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u4f7f\u7528\u4e24\u79cd\u89c6\u9891\u53d8\u4f53\u751f\u6210\u63cf\u8ff0\uff0c\u901a\u8fc7\u76f8\u5bf9\u5956\u52b1\u786e\u4fdd\u5b8c\u6574\u89c6\u9891\u7684\u54cd\u5e94\u4f18\u4e8e\u964d\u7ea7\u8f93\u5165\u3002", "result": "\u5728\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7406\u89e3\u548c\u7f55\u89c1\u4e8b\u4ef6\u63cf\u8ff0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684VMLLMs\uff0c\u540c\u65f6\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\u4fdd\u6301\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5148\u5904\u7406\u4efb\u52a1\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2511.19199", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19199", "abs": "https://arxiv.org/abs/2511.19199", "authors": ["Teodora Popordanoska", "Jiameng Li", "Matthew B. Blaschko"], "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection", "comment": "First two authors contributed equally", "summary": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.", "AI": {"tldr": "CLASH\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u77db\u76fe\u68c0\u6d4b\u57fa\u51c6\uff0c\u5305\u542bCOCO\u56fe\u50cf\u4e0e\u5305\u542b\u5bf9\u8c61\u7ea7\u6216\u5c5e\u6027\u7ea7\u77db\u76fe\u7684\u77db\u76fe\u5b57\u5e55\u914d\u5bf9\uff0c\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u8de8\u6a21\u6001\u51b2\u7a81\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u5927\u91cf\u77db\u76fe\u7684\u591a\u6a21\u6001\u8f93\u5165\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u5047\u8bbe\u8f93\u5165\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u8bc4\u4f30\u8de8\u6a21\u6001\u77db\u76fe\u68c0\u6d4b\u8fd9\u4e00\u9632\u6b62\u5e7b\u89c9\u548c\u786e\u4fdd\u53ef\u9760\u6027\u7684\u57fa\u672c\u80fd\u529b\u3002", "method": "\u6784\u5efaCLASH\u57fa\u51c6\uff0c\u5305\u542bCOCO\u56fe\u50cf\u4e0e\u77db\u76fe\u5b57\u5e55\u914d\u5bf9\uff0c\u901a\u8fc7\u81ea\u52a8\u8d28\u91cf\u68c0\u67e5\u7b5b\u9009\u7684\u5e7f\u6cdb\u5fae\u8c03\u96c6\u548c\u8f83\u5c0f\u7684\u4eba\u5de5\u9a8c\u8bc1\u8bca\u65ad\u96c6\uff0c\u5728\u591a\u9879\u9009\u62e9\u548c\u5f00\u653e\u5f0f\u683c\u5f0f\u4e0b\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5206\u6790\u663e\u793a\u5b83\u4eec\u5728\u8bc6\u522b\u8de8\u6a21\u6001\u51b2\u7a81\u65b9\u9762\u5b58\u5728\u663e\u8457\u9650\u5236\uff0c\u66b4\u9732\u51fa\u7cfb\u7edf\u6027\u7684\u6a21\u6001\u504f\u89c1\u548c\u7c7b\u522b\u7279\u5b9a\u5f31\u70b9\u3002\u9488\u5bf9CLASH\u7684\u5fae\u8c03\u663e\u8457\u589e\u5f3a\u4e86\u51b2\u7a81\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "CLASH\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u77db\u76fe\u68c0\u6d4b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u8bc1\u660e\u9488\u5bf9\u6027\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347\u51b2\u7a81\u8bc6\u522b\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2511.18920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18920", "abs": "https://arxiv.org/abs/2511.18920", "authors": ["Wenhao Xu", "Xin Dong", "Yue Li", "Haoyuan Shi", "Zhiwei Xiong"], "title": "EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models", "comment": "8 pages, 7 figures", "summary": "Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.", "AI": {"tldr": "\u63d0\u51faEventSTU\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u539f\u7406\u5b9e\u73b0\u9ad8\u6548\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u65f6\u95f4\u57df\u5173\u952e\u5e27\u91c7\u6837\u548c\u7a7a\u95f4\u57dftoken\u526a\u679d\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\uff0c\u53d7\u4e8b\u4ef6\u76f8\u673a\u542f\u53d1\uff0c\u5e0c\u671b\u5229\u7528\u4e8b\u4ef6\u89e6\u53d1\u7279\u6027\u6765\u6d88\u9664\u5197\u4f59\u4fe1\u606f", "method": "1. \u65f6\u95f4\u57df\uff1a\u7c97\u5230\u7ec6\u5173\u952e\u5e27\u91c7\u6837\u7b97\u6cd5\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u53d8\u5316\u89e6\u53d1\u7279\u6027\u6d88\u9664\u5197\u4f59\u5e27\n2. \u7a7a\u95f4\u57df\uff1a\u81ea\u9002\u5e94token\u526a\u679d\u7b97\u6cd5\uff0c\u5229\u7528\u4e8b\u4ef6\u89c6\u89c9\u663e\u8457\u6027\u4f5c\u4e3a\u96f6\u6210\u672c\u5148\u9a8c\n3. \u65f6\u7a7a\u6574\u5408\uff1a\u7ed3\u5408\u95ee\u9898\u76f8\u5173\u6027\u81ea\u9002\u5e94\u5206\u914dtoken\u526a\u679d\u9884\u7b97", "result": "\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b03.01\u500dFLOPs\u51cf\u5c11\u548c3.10\u500d\u9884\u586b\u5145\u52a0\u901f\uff0c\u540c\u65f6\u6027\u80fd\u4ecd\u6709\u63d0\u5347", "conclusion": "EventSTU\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u65f6\u7a7a\u7406\u89e3\uff0c\u65e2\u9002\u7528\u4e8e\u7269\u7406\u4e8b\u4ef6\u76f8\u673a\u4e5f\u652f\u6301\u901a\u7528\u89c6\u9891\u7406\u89e3\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6548\u7387\u4f18\u5316\u65b9\u6848"}}
{"id": "2511.18968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18968", "abs": "https://arxiv.org/abs/2511.18968", "authors": ["Bhuvan Sachdeva", "Sneha Kumari", "Rudransh Agarwal", "Shalaka Kumaraswamy", "Niharika Singri Prasad", "Simon Mueller", "Raphael Lechtenboehmer", "Maximilian W. M. Wintergerst", "Thomas Schultz", "Kaushik Murali", "Mohit Jain"], "title": "CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery", "comment": null, "summary": "Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86CataractCompDetect\u6846\u67b6\uff0c\u7ed3\u5408\u76f8\u4f4d\u611f\u77e5\u5b9a\u4f4d\u3001SAM 2\u8ddf\u8e2a\u3001\u5e76\u53d1\u75c7\u7279\u5b9a\u98ce\u9669\u8bc4\u5206\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff0c\u7528\u4e8e\u767d\u5185\u969c\u624b\u672f\u5e76\u53d1\u75c7\u7684\u81ea\u52a8\u68c0\u6d4b\u3002", "motivation": "\u767d\u5185\u969c\u624b\u672f\u662f\u5168\u7403\u6700\u5e38\u89c1\u7684\u624b\u672f\u4e4b\u4e00\uff0c\u4f46\u672f\u4e2d\u5e76\u53d1\u75c7\u5982\u8679\u819c\u8131\u5782\u3001\u540e\u56ca\u7834\u88c2\u548c\u73bb\u7483\u4f53\u4e22\u5931\u4ecd\u662f\u5bfc\u81f4\u4e0d\u826f\u9884\u540e\u7684\u4e3b\u8981\u539f\u56e0\u3002\u81ea\u52a8\u68c0\u6d4b\u8fd9\u4e9b\u4e8b\u4ef6\u53ef\u5b9e\u73b0\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u548c\u5ba2\u89c2\u57f9\u8bad\u53cd\u9988\u3002", "method": "\u7ed3\u5408\u76f8\u4f4d\u611f\u77e5\u5b9a\u4f4d\u3001SAM 2\u8ddf\u8e2a\u3001\u5e76\u53d1\u75c7\u7279\u5b9a\u98ce\u9669\u8bc4\u5206\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002\u4f7f\u7528\u9996\u4e2a\u767d\u5185\u969c\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6CataComp\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728CataComp\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5f97\u5206\u4e3a70.63%\uff0c\u5404\u5e76\u53d1\u75c7\u68c0\u6d4b\u6027\u80fd\u5206\u522b\u4e3a\uff1a\u8679\u819c\u8131\u578281.8%\u3001\u540e\u56ca\u7834\u88c260.87%\u3001\u73bb\u7483\u4f53\u4e22\u593169.23%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u7ed3\u6784\u5316\u624b\u672f\u5148\u9a8c\u77e5\u8bc6\u4e0e\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u5bf9\u4e8e\u8bc6\u522b\u7f55\u89c1\u4f46\u5f71\u54cd\u91cd\u5927\u7684\u672f\u4e2d\u4e8b\u4ef6\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.19202", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.19202", "abs": "https://arxiv.org/abs/2511.19202", "authors": ["Brent Zoomers", "Florian Hahlbohm", "Joni Vanherck", "Lode Jorissen", "Marcus Magnor", "Nick Michiels"], "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting", "comment": "15 pages, 13 figures", "summary": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u5c0f\u578b\u5171\u4eabMLP\u5b66\u4e603D\u9ad8\u65af\u6a21\u578b\u4e2d\u9ad8\u65af\u53ef\u89c1\u6027\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u906e\u6321\u5254\u9664\u6765\u52a0\u901f\u6e32\u67d3\uff0c\u5728VRAM\u4f7f\u7528\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u867d\u7136\u80fd\u5229\u7528\u89c6\u9525\u4f53\u5254\u9664\u548c\u7ec6\u8282\u5c42\u6b21\u7b56\u7565\u52a0\u901f\u6e32\u67d3\uff0c\u4f46\u7531\u4e8e\u9ad8\u65af\u7684\u534a\u900f\u660e\u7279\u6027\u65e0\u6cd5\u5e94\u7528\u906e\u6321\u5254\u9664\u6280\u672f\uff0c\u8fd9\u9650\u5236\u4e86\u6e32\u67d3\u6548\u7387\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u4f7f\u7528\u5c0f\u578b\u5171\u4eabMLP\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u4e2d\u6240\u6709\u9ad8\u65af\u7684\u89c6\u70b9\u4f9d\u8d56\u53ef\u89c1\u6027\u51fd\u6570\uff0c\u5728\u5149\u6805\u5316\u524d\u67e5\u8be2\u89c6\u9525\u4f53\u5185\u9ad8\u65af\u7684\u53ef\u89c1\u6027\uff0c\u4e22\u5f03\u88ab\u906e\u6321\u7684\u57fa\u5143\uff0c\u5e76\u96c6\u6210\u5230\u65b0\u7684\u5b9e\u4f8b\u5316\u8f6f\u4ef6\u5149\u6805\u5668\u4e2d\u3002", "result": "\u5728\u7ec4\u5408\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728VRAM\u4f7f\u7528\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\uff0c\u4e0e\u73b0\u6709LoD\u6280\u672f\u5177\u6709\u4e92\u8865\u7279\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u67e5\u8be2\u906e\u6321\u5254\u9664\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e32\u67d3\u4e2d\u906e\u6321\u5254\u9664\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u6548\u7387\u3002"}}
