{"id": "2601.12169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12169", "abs": "https://arxiv.org/abs/2601.12169", "authors": ["Samuel A. Moore", "Easop Lee", "Boyuan Chen"], "title": "Learning Legged MPC with Smooth Neural Surrogates", "comment": null, "summary": "Deep learning and model predictive control (MPC) can play complementary roles in legged robotics. However, integrating learned models with online planning remains challenging. When dynamics are learned with neural networks, three key difficulties arise: (1) stiff transitions from contact events may be inherited from the data; (2) additional non-physical local nonsmoothness can occur; and (3) training datasets can induce non-Gaussian model errors due to rapid state changes. We address (1) and (2) by introducing the smooth neural surrogate, a neural network with tunable smoothness designed to provide informative predictions and derivatives for trajectory optimization through contact. To address (3), we train these models using a heavy-tailed likelihood that better matches the empirical error distributions observed in legged-robot dynamics. Together, these design choices substantially improve the reliability, scalability, and generalizability of learned legged MPC. Across zero-shot locomotion tasks of increasing difficulty, smooth neural surrogates with robust learning yield consistent reductions in cumulative cost on simple, well-conditioned behaviors (typically 10-50%), while providing substantially larger gains in regimes where standard neural dynamics often fail outright. In these regimes, smoothing enables reliable execution (from 0/5 to 5/5 success) and produces about 2-50x lower cumulative cost, reflecting orders-of-magnitude absolute improvements in robustness rather than incremental performance gains.", "AI": {"tldr": "\u63d0\u51fa\u5e73\u6ed1\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u548c\u9c81\u68d2\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u5b66\u4e60\u52a8\u529b\u5b66\u6a21\u578b\u5728\u817f\u5f0f\u673a\u5668\u4ebaMPC\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u8fd0\u52a8\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7ed3\u5408\u5e94\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u65f6\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a1\uff09\u63a5\u89e6\u4e8b\u4ef6\u7684\u521a\u6027\u8fc7\u6e21\uff1b2\uff09\u989d\u5916\u7684\u975e\u7269\u7406\u5c40\u90e8\u975e\u5149\u6ed1\u6027\uff1b3\uff09\u8bad\u7ec3\u6570\u636e\u5bfc\u81f4\u7684\u975e\u9ad8\u65af\u6a21\u578b\u8bef\u5dee\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u5b66\u4e60\u6a21\u578b\u5728\u5728\u7ebf\u89c4\u5212\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "1\uff09\u5f15\u5165\u5e73\u6ed1\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\uff0c\u5177\u6709\u53ef\u8c03\u5e73\u6ed1\u6027\uff0c\u4e3a\u63a5\u89e6\u8f68\u8ff9\u4f18\u5316\u63d0\u4f9b\u4fe1\u606f\u9884\u6d4b\u548c\u5bfc\u6570\uff1b2\uff09\u4f7f\u7528\u91cd\u5c3e\u4f3c\u7136\u51fd\u6570\u8bad\u7ec3\u6a21\u578b\uff0c\u66f4\u597d\u5730\u5339\u914d\u817f\u5f0f\u673a\u5668\u4eba\u52a8\u529b\u5b66\u7684\u7ecf\u9a8c\u8bef\u5dee\u5206\u5e03\u3002", "result": "\u5728\u96be\u5ea6\u9012\u589e\u7684\u96f6\u6837\u672c\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u5e73\u6ed1\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u4e0e\u9c81\u68d2\u5b66\u4e60\u7ed3\u5408\uff1a\u7b80\u5355\u884c\u4e3a\u4e0a\u7d2f\u79ef\u6210\u672c\u964d\u4f4e10-50%\uff1b\u5728\u6807\u51c6\u795e\u7ecf\u52a8\u529b\u5b66\u5e38\u5931\u8d25\u7684\u590d\u6742\u573a\u666f\u4e2d\uff0c\u6210\u529f\u7387\u4ece0/5\u63d0\u5347\u52305/5\uff0c\u7d2f\u79ef\u6210\u672c\u964d\u4f4e2-50\u500d\uff0c\u5b9e\u73b0\u6570\u91cf\u7ea7\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5e73\u6ed1\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u548c\u91cd\u5c3e\u4f3c\u7136\u8bad\u7ec3\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u817f\u5f0f\u673a\u5668\u4ebaMPC\u7684\u53ef\u9760\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a5\u89e6\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7a81\u7834\u6027\u7684\u6027\u80fd\u6539\u8fdb\u3002"}}
{"id": "2601.11611", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11611", "abs": "https://arxiv.org/abs/2601.11611", "authors": ["Marina Vicini", "Martin Rudorfer", "Zhuangzhuang Dai", "Luis J. Manso"], "title": "Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home", "comment": "Accepted to International Conference on Ubiquitous Computing and Ambient Intelligence (UCAmI) 2024", "summary": "With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u88ab\u52a8\u4f20\u611f\u5668\uff08PIR\u548c\u95e8\u4f20\u611f\u5668\uff09\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u6d3b\u52a8\u5230\u4e0d\u540c\u65f6\u95f4\u6bb5\u5e76\u7f16\u7801\u65f6\u95f4\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u8001\u5e74\u4eba\u5728\u667a\u80fd\u5bb6\u5c45\u4e2d\u7684\u6d3b\u52a8\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u5168\u7403\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u9700\u8981\u8ba9\u8001\u5e74\u4eba\u80fd\u591f\u72ec\u7acb\u5b89\u5168\u5730\u5728\u5bb6\u4e2d\u751f\u6d3b\u3002\u4f7f\u7528\u88ab\u52a8\u4f20\u611f\u5668\u76d1\u6d4b\u65e5\u5e38\u6d3b\u52a8\u5bf9\u4e8e\u9884\u9632\u6027\u533b\u7597\u5e72\u9884\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u6548\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u5c06\u6d3b\u52a8\u805a\u7c7b\u5230\u65e9\u6668\u3001\u4e0b\u5348\u548c\u591c\u665a\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4fe1\u606f\u7f16\u7801\u5230\u7279\u5f81\u52a0\u6743\u65b9\u6cd5\u4e2d\uff0c\u8ba1\u7b97\u4e0d\u540c\u7684\u4e92\u4fe1\u606f\u77e9\u9635\u3002\u6269\u5c55\u7279\u5f81\u5411\u91cf\uff0c\u52a0\u5165\u4e00\u5929\u4e2d\u7684\u65f6\u95f4\u548c\u4e00\u5468\u4e2d\u7684\u5929\u6570\u4f5c\u4e3a\u5faa\u73af\u65f6\u95f4\u7279\u5f81\uff0c\u5e76\u6dfb\u52a0\u7528\u6237\u4f4d\u7f6e\u8ddf\u8e2a\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\uff0c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u83b7\u5f97\u4e86\u6539\u8fdb\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\uff0c\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u83b7\u5f97\u6700\u9ad8\u589e\u76ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5f00\u53d1\u6709\u6548\u667a\u80fd\u5bb6\u5c45\u89e3\u51b3\u65b9\u6848\u4ee5\u652f\u6301\u8001\u5e74\u4eba\u539f\u5730\u517b\u8001\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u66f4\u597d\u5730\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u63d0\u9ad8\u4e86\u6d3b\u52a8\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2601.11615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11615", "abs": "https://arxiv.org/abs/2601.11615", "authors": ["Beyza Cinar", "Louisa van den Boom", "Maria Maleshkova"], "title": "A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia", "comment": null, "summary": "Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7814\u7a76\u4e86\u57fa\u4e8e\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u4f4e\u8840\u7cd6\u4e8b\u4ef6\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u548c\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b01\u5c0f\u65f6\u5185\u7684\u9884\u6d4b\u6548\u679c\u6700\u4f73\uff0c\u4f20\u7edfML\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u66f4\u4f18\u3002", "motivation": "1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u9700\u8981\u7ec8\u8eab\u80f0\u5c9b\u7d20\u6cbb\u7597\uff0c\u4f46\u80f0\u5c9b\u7d20\u6cbb\u7597\u6709\u5bfc\u81f4\u4f4e\u8840\u7cd6\u7684\u526f\u4f5c\u7528\u3002\u4f4e\u8840\u7cd6\uff08\u8840\u7cd6\u4f4e\u4e8e70 mg/dL\uff09\u4f1a\u589e\u52a0\u6b7b\u4ea1\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u4f4e\u8840\u7cd6\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u6539\u5584\u7cd6\u5c3f\u75c5\u7ba1\u7406\u3002", "method": "\u7efc\u8ff0\u5206\u6790\u4e86\u57fa\u4e8e\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u56de\u5f52\u6a21\u578b\uff08\u9884\u6d4b\u8840\u7cd6\u503c\uff09\u548c\u5206\u7c7b\u6a21\u578b\uff08\u8bc6\u522b\u4f4e\u8840\u7cd6\u4e8b\u4ef6\uff09\u3002\u6bd4\u8f83\u4e86\u77ed\u671f\uff0815-120\u5206\u949f\uff09\u548c\u957f\u671f\uff083-24\u5c0f\u65f6\u4ee5\u4e0a\uff09\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\u3002", "result": "1\uff091\u5c0f\u65f6\u5185\u7684\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u6548\u679c\u6700\u4f73\uff1b2\uff09\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u597d\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u66f4\u4f18\uff0c\u5355\u4e00\u6a21\u578b\u65e0\u6cd5\u5728\u591a\u4e2a\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u90fd\u8868\u73b0\u826f\u597d\uff1b3\uff09\u591a\u53d8\u91cf\u6570\u636e\u96c6\u548c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff1b4\uff09\u4e2a\u6027\u5316\u6570\u636e\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u8d28\u91cf\u6709\u9650\uff0c\u57fa\u4e8e\u4eba\u7fa4\u7684\u6a21\u578b\u66f4\u53d7\u9752\u7750\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u4f4e\u8840\u7cd6\u4e8b\u4ef6\uff0c\u4f46\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u3001\u6a21\u578b\u9009\u62e9\u3001\u6570\u636e\u7279\u5f81\u548c\u4e2a\u6027\u5316\u7a0b\u5ea6\u90fd\u4f1a\u5f71\u54cd\u9884\u6d4b\u6027\u80fd\u3002\u672a\u6765\u9700\u8981\u66f4\u591a\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2601.12377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12377", "abs": "https://arxiv.org/abs/2601.12377", "authors": ["Haobo Xi", "Shiyong Zhang", "Qianli Dong", "Yunze Tong", "Songyang Wu", "Jing Yuan", "Xuebo Zhang"], "title": "R-VoxelMap: Accurate Voxel Mapping with Recursive Plane Fitting for Online LiDAR Odometry", "comment": null, "summary": "This paper proposes R-VoxelMap, a novel voxel mapping method that constructs accurate voxel maps using a geometry-driven recursive plane fitting strategy to enhance the localization accuracy of online LiDAR odometry. VoxelMap and its variants typically fit and check planes using all points in a voxel, which may lead to plane parameter deviation caused by outliers, over segmentation of large planes, and incorrect merging across different physical planes. To address these issues, R-VoxelMap utilizes a geometry-driven recursive construction strategy based on an outlier detect-and-reuse pipeline. Specifically, for each voxel, accurate planes are first fitted while separating outliers using random sample consensus (RANSAC). The remaining outliers are then propagated to deeper octree levels for recursive processing, ensuring a detailed representation of the environment. In addition, a point distribution-based validity check algorithm is devised to prevent erroneous plane merging. Extensive experiments on diverse open-source LiDAR(-inertial) simultaneous localization and mapping (SLAM) datasets validate that our method achieves higher accuracy than other state-of-the-art approaches, with comparable efficiency and memory usage. Code will be available on GitHub.", "AI": {"tldr": "R-VoxelMap\uff1a\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u9a71\u52a8\u9012\u5f52\u5e73\u9762\u62df\u5408\u7684\u65b0\u578b\u4f53\u7d20\u5efa\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f02\u5e38\u503c\u68c0\u6d4b\u4e0e\u91cd\u7528\u7ba1\u9053\u63d0\u5347LiDAR\u91cc\u7a0b\u8ba1\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u73b0\u6709VoxelMap\u53ca\u5176\u53d8\u4f53\u5728\u4f53\u7d20\u4e2d\u4f7f\u7528\u6240\u6709\u70b9\u62df\u5408\u5e73\u9762\uff0c\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5f02\u5e38\u503c\u5bfc\u81f4\u5e73\u9762\u53c2\u6570\u504f\u5dee\uff1b2\uff09\u5927\u5e73\u9762\u8fc7\u5ea6\u5206\u5272\uff1b3\uff09\u4e0d\u540c\u7269\u7406\u5e73\u9762\u9519\u8bef\u5408\u5e76\u3002\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u4e86\u5efa\u56fe\u7cbe\u5ea6\u548c\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u9a71\u52a8\u7684\u9012\u5f52\u6784\u5efa\u7b56\u7565\uff1a1\uff09\u5728\u6bcf\u4e2a\u4f53\u7d20\u4e2d\u4f7f\u7528RANSAC\u62df\u5408\u51c6\u786e\u5e73\u9762\u5e76\u5206\u79bb\u5f02\u5e38\u503c\uff1b2\uff09\u5269\u4f59\u5f02\u5e38\u503c\u4f20\u64ad\u5230\u66f4\u6df1\u7684\u516b\u53c9\u6811\u5c42\u7ea7\u8fdb\u884c\u9012\u5f52\u5904\u7406\uff1b3\uff09\u8bbe\u8ba1\u57fa\u4e8e\u70b9\u5206\u5e03\u7684\u6709\u6548\u6027\u68c0\u67e5\u7b97\u6cd5\u9632\u6b62\u9519\u8bef\u5e73\u9762\u5408\u5e76\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90LiDAR(-\u60ef\u6027)SLAM\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cR-VoxelMap\u76f8\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u8fbe\u5230\u66f4\u9ad8\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "R-VoxelMap\u901a\u8fc7\u9012\u5f52\u5e73\u9762\u62df\u5408\u548c\u5f02\u5e38\u503c\u5904\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u4f53\u7d20\u5efa\u56fe\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LiDAR\u91cc\u7a0b\u8ba1\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4ee3\u7801\u5c06\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2601.11640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11640", "abs": "https://arxiv.org/abs/2601.11640", "authors": ["Yingda Yu", "Jiaqi Xuan", "Shuhui Shi", "Xuanyu Teng", "Shuyang Xu", "Guanchao Tong"], "title": "Confident Learning for Object Detection under Model Constraints", "comment": "Submitted to ICPR 2026, currently under review", "summary": "Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.", "AI": {"tldr": "\u63d0\u51faMDDC\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u8bca\u65ad\u4e0e\u4fee\u6b63\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u56fa\u5b9a\u8f7b\u91cf\u6a21\u578b\u4e0b\u5b9e\u73b05-25%\u7684mAP\u63d0\u5347", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u519c\u4e1a\u6742\u8349\u68c0\u6d4b\u9762\u4e34\u4e25\u683c\u7684\u8ba1\u7b97\u8d44\u6e90\u3001\u6a21\u578b\u5bb9\u91cf\u548c\u5b9e\u65f6\u63a8\u7406\u5ef6\u8fdf\u7ea6\u675f\uff0c\u65e0\u6cd5\u901a\u8fc7\u6a21\u578b\u6269\u5c55\u6216\u96c6\u6210\u6765\u63d0\u5347\u6027\u80fd\uff0c\u9700\u8981\u6570\u636e\u5c42\u9762\u7684\u4f18\u5316\u65b9\u6848", "method": "\u63d0\u51fa\u6a21\u578b\u9a71\u52a8\u7684\u6570\u636e\u4fee\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9519\u8bef\u5206\u6790\u5c06\u68c0\u6d4b\u5931\u8d25\u5206\u4e3a\u56db\u7c7b\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u8bad\u7ec3-\u4fee\u6b63-\u518d\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7ed3\u5408\u7248\u672c\u63a7\u5236\u6570\u636e\u7ba1\u7406", "result": "\u5728\u591a\u4e2a\u6742\u8349\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u56fa\u5b9a\u8f7b\u91cf\u68c0\u6d4b\u5668\u5b9e\u73b05-25%\u7684mAP@0.5\u63d0\u5347\uff0c\u8bc1\u660e\u6570\u636e\u8d28\u91cf\u4f18\u5316\u80fd\u6709\u6548\u7f13\u89e3\u56fa\u5b9a\u6a21\u578b\u5bb9\u91cf\u4e0b\u7684\u6027\u80fd\u74f6\u9888", "conclusion": "\u7cfb\u7edf\u5316\u7684\u6570\u636e\u8d28\u91cf\u4f18\u5316\u662f\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\uff0c\u5728\u6a21\u578b\u5bb9\u91cf\u53d7\u9650\u65f6\uff0c\u6570\u636e\u5c42\u9762\u7684\u6539\u8fdb\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6"}}
{"id": "2601.11669", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11669", "abs": "https://arxiv.org/abs/2601.11669", "authors": ["Wenwen Liao", "Hang Ruan", "Jianbo Yu", "Xiaofeng Yang", "Qingchao Jiang", "Xuefeng Yan"], "title": "IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning", "comment": null, "summary": "Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement Classifier (IPEC), a test-time method that optimizes prototype estimation by leveraging information from previous query samples. IPEC maintains a dynamic auxiliary set by selectively incorporating query samples that are classified with high confidence. To ensure sample quality, we design a robust dual-filtering mechanism that assesses each query sample based on both global prediction confidence and local discriminative ability. By aggregating this auxiliary set with the support set in subsequent tasks, IPEC builds progressively more stable and representative prototypes, effectively reducing its reliance on the initial support set. We ground this approach in a Bayesian interpretation, conceptualizing the support set as a prior and the auxiliary set as a data-driven posterior, which in turn motivates the design of a practical \"warm-up and test\" two-stage inference protocol. Extensive empirical results validate the superior performance of our proposed method across multiple few-shot classification tasks.", "AI": {"tldr": "IPEC\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u589e\u91cf\u539f\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5148\u524d\u67e5\u8be2\u6837\u672c\u7684\u4fe1\u606f\u4f18\u5316\u539f\u578b\u4f30\u8ba1\uff0c\u63d0\u5347\u5c0f\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5ea6\u91cf\u7684\u5c0f\u6837\u672c\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u5047\u8bbe\u6279\u6b21\u72ec\u7acb\u6027\uff0c\u65e0\u6cd5\u5229\u7528\u5148\u524d\u6279\u6b21\u79ef\u7d2f\u7684\u5b9d\u8d35\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faIPEC\u65b9\u6cd5\uff1a1\uff09\u7ef4\u62a4\u52a8\u6001\u8f85\u52a9\u96c6\uff0c\u9009\u62e9\u6027\u7eb3\u5165\u9ad8\u7f6e\u4fe1\u5ea6\u67e5\u8be2\u6837\u672c\uff1b2\uff09\u8bbe\u8ba1\u53cc\u91cd\u8fc7\u6ee4\u673a\u5236\uff0c\u57fa\u4e8e\u5168\u5c40\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u5c40\u90e8\u5224\u522b\u80fd\u529b\u8bc4\u4f30\u6837\u672c\u8d28\u91cf\uff1b3\uff09\u5c06\u8f85\u52a9\u96c6\u4e0e\u652f\u6301\u96c6\u805a\u5408\u6784\u5efa\u66f4\u7a33\u5b9a\u7684\u539f\u578b\uff1b4\uff09\u57fa\u4e8e\u8d1d\u53f6\u65af\u89e3\u91ca\u8bbe\u8ba1\"\u9884\u70ed-\u6d4b\u8bd5\"\u4e24\u9636\u6bb5\u63a8\u7406\u534f\u8bae\u3002", "result": "\u5728\u591a\u4e2a\u5c0f\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86IPEC\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "IPEC\u901a\u8fc7\u6d4b\u8bd5\u65f6\u589e\u91cf\u5b66\u4e60\u6709\u6548\u51cf\u5c11\u5bf9\u521d\u59cb\u652f\u6301\u96c6\u7684\u4f9d\u8d56\uff0c\u6784\u5efa\u66f4\u7a33\u5b9a\u548c\u5177\u6709\u4ee3\u8868\u6027\u7684\u539f\u578b\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2601.11865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11865", "abs": "https://arxiv.org/abs/2601.11865", "authors": ["Truong Nguyen", "Phi Van Dat", "Ngan Nguyen", "Linh Ngo Van", "Trung Le", "Thanh Hong Nguyen"], "title": "CTPD: Cross Tokenizer Preference Distillation", "comment": "AAAI 2026", "summary": "While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher's preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.", "AI": {"tldr": "CTPD\u662f\u9996\u4e2a\u5728\u5f02\u6784\u5206\u8bcd\u5668\u6a21\u578b\u95f4\u8f6c\u79fb\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u884c\u4e3a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5b57\u7b26\u7ea7\u5bf9\u9f50\u3001\u91cd\u8981\u6027\u91c7\u6837\u548c\u6559\u5e08\u951a\u5b9a\u53c2\u8003\u5b9e\u73b0\u8de8\u5206\u8bcd\u5668\u504f\u597d\u84b8\u998f\u3002", "motivation": "\u77e5\u8bc6\u84b8\u998f\u5728\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u66f4\u73b0\u5b9e\u7684\u8de8\u5206\u8bcd\u5668\u8bbe\u7f6e\u4e2d\u3002\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u4e4b\u95f4\u5206\u8bcd\u65b9\u6848\u7684\u4e0d\u517c\u5bb9\u6027\u963b\u788d\u4e86\u7ec6\u7c92\u5ea6\u7684\u767d\u76d2\u504f\u597d\u4fe1\u606f\u84b8\u998f\u3002", "method": "\u63d0\u51fa\u8de8\u5206\u8bcd\u5668\u504f\u597d\u84b8\u998f(CTPD)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1)\u5bf9\u9f50\u8de8\u5ea6\u6295\u5f71\uff0c\u5c06\u6559\u5e08\u548c\u5b66\u751ftoken\u6620\u5c04\u5230\u5171\u4eab\u5b57\u7b26\u7ea7\u8de8\u5ea6\uff1b2)\u8de8\u5206\u8bcd\u5668\u9002\u914d\u7684token\u7ea7\u91cd\u8981\u6027\u91c7\u6837(TIS-DPO)\uff1b3)\u6559\u5e08\u951a\u5b9a\u53c2\u8003\uff0c\u8ba9\u5b66\u751f\u5728DPO\u98ce\u683c\u76ee\u6807\u4e2d\u76f4\u63a5\u5229\u7528\u6559\u5e08\u504f\u597d\u3002", "result": "\u7406\u8bba\u5206\u6790\u5c06CTPD\u5efa\u7acb\u5728\u91cd\u8981\u6027\u91c7\u6837\u57fa\u7840\u4e0a\uff0c\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CTPD\u4e3a\u4e0d\u540c\u5206\u8bcd\u65b9\u6848\u95f4\u7684\u504f\u597d\u84b8\u998f\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u66f4\u6613\u83b7\u53d6\u548c\u9ad8\u6548\u7684\u5bf9\u9f50\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2601.11789", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11789", "abs": "https://arxiv.org/abs/2601.11789", "authors": ["Shenyang Deng", "Boyao Liao", "Zhuoli Ouyang", "Tianyu Pang", "Minhak Song", "Yaoqing Yang"], "title": "Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis", "comment": "The 37th International Conference on Algorithmic Learning Theory", "summary": "This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $\u03b7_t^*$ separates alignment-decreasing ($\u03b7_t < \u03b7_t^*$) from alignment-increasing ($\u03b7_t > \u03b7_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86SGD\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\uff0c\u53d1\u73b0\u68af\u5ea6\u4e0e\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\u5448\u73b0\u5148\u964d\u540e\u5347\u6700\u7ec8\u7a33\u5b9a\u7684\u4e24\u9636\u6bb5\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u4e86\u6b65\u957f\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76SGD\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\uff0c\u5373\u68af\u5ea6\u4e0eHessian\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\u884c\u4e3a\uff0c\u8fd9\u79cd\u5bf9\u9f50\u88ab\u79f0\u4e3a\"\u53ef\u7591\"\u7684\uff0c\u56e0\u4e3a\u6cbf\u7740\u9ad8\u5ea6\u5bf9\u9f50\u7684\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u7684\u68af\u5ea6\u66f4\u65b0\u5b9e\u9645\u4e0a\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u635f\u5931\u3002", "method": "\u5728\u9ad8\u7ef4\u4e8c\u6b21\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u63d0\u51fa\u6b65\u957f\u6761\u4ef6\u7406\u8bba\uff0c\u533a\u5206\u5bf9\u9f50\u51cf\u5c11\u548c\u5bf9\u9f50\u589e\u52a0\u7684\u4e0d\u540c\u6b65\u957f\u533a\u95f4\uff0c\u5206\u6790\u81ea\u9002\u5e94\u4e34\u754c\u6b65\u957f\u03b7_t^*\u7684\u4f5c\u7528\uff0c\u5e76\u8bc1\u660e\u5728\u5e38\u6570\u6b65\u957f\u548c\u5927\u521d\u59cb\u5316\u4e0bSGD\u7684\u4e24\u9636\u6bb5\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u5728\u4f4e\u5bf9\u9f50\u533a\u57df\u5b58\u5728\u81ea\u9002\u5e94\u4e34\u754c\u6b65\u957f\u03b7_t^*\uff0c\u6b65\u957f\u5c0f\u4e8e\u5b83\u65f6\u5bf9\u9f50\u51cf\u5c11\uff0c\u5927\u4e8e\u5b83\u65f6\u5bf9\u9f50\u589e\u52a0\uff1b\u5728\u9ad8\u5bf9\u9f50\u533a\u57df\uff0c\u5bf9\u9f50\u5177\u6709\u81ea\u6821\u6b63\u6027\u3002\u5728\u8db3\u591f\u75c5\u6001\u6761\u4ef6\u4e0b\uff0c\u5b58\u5728\u6b65\u957f\u533a\u95f4\u4f7f\u5f97\u5411bulk\u5b50\u7a7a\u95f4\u6295\u5f71\u80fd\u964d\u4f4e\u635f\u5931\uff0c\u800c\u5411\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u6295\u5f71\u53cd\u800c\u589e\u52a0\u635f\u5931\u3002", "conclusion": "SGD\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u786e\u5b9e\u5b58\u5728\"\u53ef\u7591\u5bf9\u9f50\"\u7684\u4e24\u9636\u6bb5\u73b0\u8c61\uff0c\u6b65\u957f\u9009\u62e9\u662f\u5173\u952e\u56e0\u7d20\u3002\u8be5\u7406\u8bba\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5411\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u6295\u5f71\u68af\u5ea6\u66f4\u65b0\u662f\u65e0\u6548\u7684\uff0c\u5e76\u4e3a\u7406\u89e3SGD\u5728\u75c5\u6001\u95ee\u9898\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.11794", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11794", "abs": "https://arxiv.org/abs/2601.11794", "authors": ["Abdelrahman Ramadan", "Zahra Dorbeigi Namaghi", "Emily Taylor", "Lucas Edwards", "Xan Giuliani", "David S. McLagan", "Sidney Givigi", "Melissa Greeff"], "title": "Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV Sensing", "comment": null, "summary": "Wildfire monitoring requires high-resolution atmospheric measurements, yet low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift, cross-sensitivity, and response lag that corrupt concentration estimates. Traditional deep learning denoising approaches demand large datasets impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE, a physics-informed denoising autoencoder that addresses data scarcity by embedding physical constraints directly into the network architecture. Non-negative concentration estimates are enforced via softplus activations and physically plausible temporal smoothing, ensuring outputs are physically admissible by construction rather than relying on loss function penalties. The architecture employs hierarchical decoder heads for Black Carbon, Gas, and CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight data), two orders of magnitude below typical deep learning requirements. PC$^2$DAE-Lean achieves 67.3\\% smoothness improvement and 90.7\\% high-frequency noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net, Transformer, CBDAE, DeSpaWN) produce 15--23\\% negative outputs. The lean variant outperforms wide (+5.6\\% smoothness), suggesting reduced capacity with strong inductive bias prevents overfitting in data-scarce regimes. Training completes in under 65 seconds on consumer hardware.", "AI": {"tldr": "PC\u00b2DAE\u662f\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u7ea6\u675f\u5d4c\u5165\u7f51\u7edc\u67b6\u6784\u6765\u89e3\u51b3\u65e0\u4eba\u673a\u4f20\u611f\u5668\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u91cf\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u65e0\u4eba\u673a\u642d\u8f7d\u7684\u4f4e\u6210\u672c\u4f20\u611f\u5668\u5b58\u5728\u57fa\u7ebf\u6f02\u79fb\u3001\u4ea4\u53c9\u654f\u611f\u6027\u548c\u54cd\u5e94\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u8fd9\u5728\u6709\u9650\u7684\u65e0\u4eba\u673a\u98de\u884c\u4efb\u52a1\u4e2d\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u63d0\u51faPC\u00b2DAE\u7269\u7406\u4fe1\u606f\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7softplus\u6fc0\u6d3b\u51fd\u6570\u5f3a\u5236\u975e\u8d1f\u6d53\u5ea6\u4f30\u8ba1\u548c\u7269\u7406\u5408\u7406\u7684\u65f6\u95f4\u5e73\u6ed1\uff0c\u5c06\u7269\u7406\u7ea6\u675f\u76f4\u63a5\u5d4c\u5165\u7f51\u7edc\u67b6\u6784\u800c\u975e\u4f9d\u8d56\u635f\u5931\u51fd\u6570\u60e9\u7f5a\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u53d8\u4f53\uff1aPC\u00b2DAE-Lean\uff0821k\u53c2\u6570\uff09\u7528\u4e8e\u8fb9\u7f18\u90e8\u7f72\uff0cPC\u00b2DAE-Wide\uff08204k\u53c2\u6570\uff09\u7528\u4e8e\u79bb\u7ebf\u5904\u7406\u3002", "result": "\u5728\u4ec57,894\u4e2a\u540c\u6b651Hz\u6837\u672c\uff08\u7ea62.2\u5c0f\u65f6\u98de\u884c\u6570\u636e\uff09\u4e0a\u8bc4\u4f30\uff0cPC\u00b2DAE-Lean\u5b9e\u73b0\u4e8667.3%\u5e73\u6ed1\u5ea6\u6539\u8fdb\u548c90.7%\u9ad8\u9891\u566a\u58f0\u51cf\u5c11\uff0c\u4e14\u96f6\u7269\u7406\u8fdd\u89c4\u3002\u4e94\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u4ea7\u751f15-23%\u8d1f\u8f93\u51fa\u3002\u7cbe\u7b80\u7248\u6bd4\u5bbd\u7248\u8868\u73b0\u66f4\u597d\uff08+5.6%\u5e73\u6ed1\u5ea6\uff09\u3002", "conclusion": "PC\u00b2DAE\u901a\u8fc7\u5c06\u7269\u7406\u7ea6\u675f\u76f4\u63a5\u5d4c\u5165\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u4f20\u611f\u5668\u53bb\u566a\u95ee\u9898\uff0c\u7cbe\u7b80\u67b6\u6784\u914d\u5408\u5f3a\u5f52\u7eb3\u504f\u7f6e\u80fd\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u8bad\u7ec3\u65f6\u95f4\u77ed\u4e14\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u3002"}}
{"id": "2601.11724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11724", "abs": "https://arxiv.org/abs/2601.11724", "authors": ["Muditha Fernando", "Kajhanan Kailainathan", "Krishnakanth Nagaratnam", "Isuranga Udaravi Bandara Senavirathne", "Ranga Rodrigo"], "title": "SemAlign: Language Guided Semi-supervised Domain Generalization", "comment": "15 pages, 6 figures", "summary": "Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4e30\u5bcc\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\u6765\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u91c7\u7528\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7b56\u7565\u9632\u6b62\u8fc7\u62df\u5408\u3002", "motivation": "\u73b0\u6709SSDG\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6700\u5927\u6570\u636e\u5229\u7528\u7387\uff0c\u8fd9\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\u53c8\u80fd\u5145\u5206\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "1) \u5c06\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u8bed\u4e49\u4e30\u5bcc\u4e14\u6cdb\u5316\u7684\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ee5\u4fc3\u8fdb\u9886\u57df\u4e0d\u53d8\u6027\uff1b2) \u91c7\u7528\u6709\u6548\u7684\u56fe\u50cf\u7ea7\u589e\u5f3a\u7b56\u7565\uff1b3) \u5b9e\u65bd\u8f93\u51fa\u7ea7\u6b63\u5219\u5316\u7b56\u7565\u6765\u6700\u5c0f\u5316\u8fc7\u62df\u5408\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u4e0e\u73b0\u6709SSDG\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684(SOTA)\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6a21\u578b\u7279\u5f81\u4e0eVLM\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3SSDG\u95ee\u9898\uff0c\u5728\u6570\u636e\u5229\u7528\u7387\u548c\u9632\u6b62\u8fc7\u62df\u5408\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.11908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11908", "abs": "https://arxiv.org/abs/2601.11908", "authors": ["Byeongjin Kim", "Gyuwan Kim", "Seo Yeon Park"], "title": "PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning", "comment": "23 pages, 6 figures", "summary": "Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.", "AI": {"tldr": "PPA-Plan\u662f\u4e00\u79cd\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4e3b\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u901a\u8fc7\u8bc6\u522b\u6f5c\u5728\u903b\u8f91\u9677\u9631\u548c\u9519\u8bef\u5047\u8bbe\uff0c\u5c06\u5176\u4f5c\u4e3a\u8d1f\u9762\u7ea6\u675f\u6765\u6307\u5bfc\u89c4\u5212\u751f\u6210\uff0c\u4ece\u800c\u5728\u89c4\u5212\u9636\u6bb5\u5c31\u9884\u9632\u5931\u8d25\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u76f8\u5173\u4fe1\u606f\u7a00\u758f\u5206\u5e03\u65f6\u3002\u73b0\u6709\u7684\u89c4\u5212-\u6267\u884c\u6846\u67b6\u56e0\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u800c\u751f\u6210\u4e0d\u53ef\u9760\u7684\u89c4\u5212\uff0c\u5bfc\u81f4\u57fa\u4e8e\u9519\u8bef\u5047\u8bbe\uff0c\u4e14\u4e8b\u540e\u96be\u4ee5\u8bc6\u522b\u548c\u4fee\u6b63\u95ee\u9898\u3002", "method": "PPA-Plan\u4e3b\u52a8\u8bc6\u522b\u6f5c\u5728\u903b\u8f91\u9677\u9631\u548c\u9519\u8bef\u5047\u8bbe\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u8d1f\u9762\u7ea6\u675f\uff0c\u5e76\u5728\u89c4\u5212\u751f\u6210\u65f6\u660e\u786e\u907f\u514d\u8fd9\u4e9b\u7ea6\u675f\uff0c\u4ece\u800c\u5728\u89c4\u5212\u9636\u6bb5\u9884\u9632\u5931\u8d25\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6267\u884cPPA-Plan\u751f\u6210\u7684\u89c4\u5212\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7684\u89c4\u5212-\u6267\u884c\u65b9\u6cd5\u548c\u76f4\u63a5\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "PPA-Plan\u901a\u8fc7\u4e3b\u52a8\u9884\u9632\u89c4\u5212\u5931\u8d25\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6027\u80fd\uff0c\u4e3a\u89c4\u5212-\u6267\u884c\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89c4\u5212\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2601.11932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11932", "abs": "https://arxiv.org/abs/2601.11932", "authors": ["Abdullah Al Monsur", "Nitesh Vamshi Bommisetty", "Gene Louis Kim"], "title": "Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes", "comment": null, "summary": "The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u89e3\u7801\u5668LLMs\u7684\u5355\u5411\u6027\u67b6\u6784\u74f6\u9888\u548cMicro-F1\u6307\u6807\u504f\u5411\u591a\u6570\u7c7b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4f7f\u7528\u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u548cLoRA\u5fae\u8c03\u6765\u63d0\u5347\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u578b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u68c0\u6d4b\u7814\u7a76\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u89e3\u7801\u5668LLMs\u7684\u5355\u5411\u67b6\u6784\u4e0d\u9002\u5408\u9700\u8981\u53cc\u5411\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff1b2\uff09\u4f20\u7edf\u4f7f\u7528\u7684Micro-F1\u6307\u6807\u4f1a\u504f\u5411\u591a\u6570\u7c7b\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\u3002", "method": "1\uff09\u4f7f\u7528\u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u578b\u4ee5\u514b\u670d\u89e3\u7801\u5668LLMs\u7684\u5355\u5411\u6027\u9650\u5236\uff1b2\uff09\u91c7\u7528LoRA\uff08\u4f4e\u79e9\u9002\u5e94\uff09\u8fdb\u884c\u5fae\u8c03\uff1b3\uff09\u4f7f\u7528Macro-F1\u4f5c\u4e3a\u4e3b\u8981\u8bc4\u4f30\u6307\u6807\u6765\u66f4\u516c\u5e73\u5730\u8bc4\u4f30\u6a21\u578b\u5728\u6240\u6709\u4e8b\u4ef6\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u6a21\u578b\u4f18\u4e8e\u6807\u51c6\u89e3\u7801\u5668\u57fa\u7ebf\uff1b2\uff09LoRA\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86Macro-F1\u5206\u6570\uff0c\u7279\u522b\u662f\u5bf9\u89e3\u7801\u5668\u6a21\u578b\uff1b3\uff09LoRA\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u548cLoRA\u5fae\u8c03\u53ef\u4ee5\u514b\u670d\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u67b6\u6784\u9650\u5236\u548c\u8bc4\u4f30\u504f\u5dee\uff0cMacro-F1\u662f\u66f4\u5408\u9002\u7684\u8bc4\u4f30\u6307\u6807\uff0cLoRA\u662f\u63d0\u5347LLMs\u5728\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u522b\u4e0a\u6027\u80fd\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.12310", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12310", "abs": "https://arxiv.org/abs/2601.12310", "authors": ["Jennifer Dodgson", "Alfath Daryl Alhajir", "Michael Joedhitya", "Akira Rafhael Janson Pattirane", "Surender Suresh Kumar", "Joseph Lim", "C. H. Peh", "Adith Ramdas", "Steven Zhang Zhexu"], "title": "Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection", "comment": null, "summary": "Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.\n  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.\n  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u5b58\u6d3b\u6027\u800c\u975e\u5956\u52b1\u7684\u81ea\u6211\u8bad\u7ec3\u67b6\u6784\uff0c\u901a\u8fc7\u884c\u4e3a\u5728\u771f\u5b9e\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u5b58\u7eed\u8fdb\u884c\u9009\u62e9\uff0c\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u548c\u8bed\u4e49\u6f02\u79fb\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u5f00\u653e\u5f0f\u81ea\u6211\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u81ea\u6211\u8bad\u7ec3\u7cfb\u7edf\u56e0\u7f3a\u4e4f\u5916\u90e8\u6570\u636e\u8d28\u91cf\u5224\u65ad\u6807\u51c6\u800c\u9000\u5316\uff0c\u5bb9\u6613\u51fa\u73b0\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u5728\u7a00\u758f\u5916\u90e8\u53cd\u9988\u548c\u6709\u9650\u5185\u5b58\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u81ea\u6211\u8bad\u7ec3\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u73af\u5883\u5b58\u6d3b\u6027\u800c\u975e\u5956\u52b1\u7684\u81ea\u6211\u8bad\u7ec3\u67b6\u6784\uff1a\u5019\u9009\u884c\u4e3a\u5728\u771f\u5b9e\u8d44\u6e90\u7ea6\u675f\u4e0b\u6267\u884c\uff0c\u53ea\u6709\u90a3\u4e9b\u73af\u5883\u6548\u679c\u6301\u4e45\u4e14\u4fdd\u7559\u672a\u6765\u4ea4\u4e92\u53ef\u80fd\u6027\u7684\u884c\u4e3a\u624d\u4f1a\u88ab\u4f20\u64ad\u3002\u73af\u5883\u4e0d\u63d0\u4f9b\u8bed\u4e49\u53cd\u9988\u3001\u5bc6\u96c6\u5956\u52b1\u6216\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\uff0c\u9009\u62e9\u4ec5\u901a\u8fc7\u884c\u4e3a\u4f5c\u4e3a\u4e16\u754c\u6539\u53d8\u4e8b\u4ef6\u7684\u5dee\u5f02\u5b58\u7eed\u8fdb\u884c\u3002", "result": "\u5206\u6790\u8bed\u4e49\u52a8\u6001\u663e\u793a\uff0c\u6539\u8fdb\u4e3b\u8981\u901a\u8fc7\u6709\u6548\u4e14\u53ef\u91cd\u590d\u7b56\u7565\u5728\u6574\u5408\u548c\u4fee\u526a\u673a\u5236\u4e0b\u7684\u6301\u4e45\u6027\u5b9e\u73b0\uff08\u8d1f\u7a7a\u95f4\u5b66\u4e60\u8303\u5f0f\uff09\u3002\u6a21\u578b\u53d1\u5c55\u51fa\u5143\u5b66\u4e60\u7b56\u7565\uff08\u5982\u6545\u610f\u5b9e\u9a8c\u5931\u8d25\u4ee5\u83b7\u53d6\u4fe1\u606f\u6027\u9519\u8bef\u6d88\u606f\uff09\uff0c\u65e0\u9700\u660e\u786e\u6307\u4ee4\u3002\u73af\u5883\u57fa\u7840\u9009\u62e9\u4f7f\u53ef\u6301\u7eed\u5f00\u653e\u5f0f\u81ea\u6211\u6539\u8fdb\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "\u73af\u5883\u57fa\u7840\u9009\u62e9\u80fd\u591f\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u5f00\u653e\u5f0f\u81ea\u6211\u6539\u8fdb\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u7c7b\u7b56\u5212\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u5851\u9020\u3002"}}
{"id": "2601.13252", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.13252", "abs": "https://arxiv.org/abs/2601.13252", "authors": ["Mahmud S. Zango", "Jianglin Lan"], "title": "Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints", "comment": "28 pages, 5 figures, 1 table. Review article", "summary": "Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging \"Edge AI\" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the \"Sim-to-Real\" transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7eb3\u7c73\u65e0\u4eba\u673a\uff08\u91cd\u91cf<50g\uff0c\u8ba1\u7b97\u529f\u7387<100mW\uff09\u81ea\u4e3b\u5bfc\u822a\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u4ece\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u5230\u8fb9\u7f18AI\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5728\u957f\u671f\u7eed\u822a\u3001\u52a8\u6001\u73af\u5883\u907f\u969c\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8fc1\u79fb\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u7eb3\u7c73\u65e0\u4eba\u673a\u9762\u4e34\u6781\u7aef\u7684\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u529f\u7387\uff08SWaP\uff09\u9650\u5236\uff08\u91cd\u91cf<50g\uff0c\u673a\u8f7d\u5904\u7406\u5668<100mW\uff09\uff0c\u8fd9\u4f7f\u5176\u4e0e\u6807\u51c6\u673a\u5668\u4eba\u8303\u5f0f\u6709\u6839\u672c\u533a\u522b\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8fd9\u79cd\u8d85\u4f4e\u529f\u8017\u8ba1\u7b97\u73af\u5883\u8bbe\u8ba1\u4f20\u611f\u3001\u8ba1\u7b97\u548c\u63a7\u5236\u67b6\u6784\u3002", "method": "\u7efc\u5408\u8bc4\u8ff0\u4e86\u9762\u5411100mW\u4ee5\u4e0b\u8ba1\u7b97\u73af\u5883\u7684\u4f20\u611f\u3001\u8ba1\u7b97\u548c\u63a7\u5236\u67b6\u6784\uff0c\u5305\u62ec\u4ece\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u5230\u8fb9\u7f18AI\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u91cf\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8d85\u4f4e\u529f\u8017SoC\u4e0a\u7684\u90e8\u7f72\uff0c\u795e\u7ecf\u5f62\u6001\u4e8b\u4ef6\u63a7\u5236\uff0c\u4ee5\u53ca\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728\u89c6\u89c9\u5bfc\u822a\u548c\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u957f\u671f\u7eed\u822a\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u907f\u969c\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\"\u4eff\u771f\u5230\u73b0\u5b9e\"\u8fc1\u79fb\u65b9\u9762\u4ecd\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u878d\u5408\u8f7b\u91cf\u7ea7\u7ecf\u5178\u63a7\u5236\u548c\u6570\u636e\u9a71\u52a8\u611f\u77e5\u7684\u6df7\u5408\u67b6\u6784\u8def\u7ebf\u56fe\uff0c\u4ee5\u5b9e\u73b0\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u5b8c\u5168\u81ea\u4e3b\u3001\u654f\u6377\u7684\u7eb3\u7c73\u65e0\u4eba\u673a\u3002"}}
{"id": "2601.12008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12008", "abs": "https://arxiv.org/abs/2601.12008", "authors": ["Shiqing Gao", "Yihang Zhou", "Shuai Shao", "Haoyu Luo", "Yiheng Bing", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "title": "Extreme Value Policy Optimization for Safe Reinforcement Learning", "comment": "Published in the 42nd International Conference on Machine Learning (ICML 2025)", "summary": "Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.", "AI": {"tldr": "EVO\u7b97\u6cd5\u5229\u7528\u6781\u503c\u7406\u8bba\u5904\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6781\u7aef\u98ce\u9669\u4e8b\u4ef6\uff0c\u901a\u8fc7\u6781\u503c\u5206\u4f4d\u6570\u4f18\u5316\u548c\u4f18\u5148\u7ea7\u56de\u653e\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4fdd\u6301\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u671f\u671b\u7ea6\u675f\uff0c\u5ffd\u7565\u4e86\u5c3e\u90e8\u5206\u5e03\u4e2d\u7684\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u7684\u6781\u7aef\u503c\u4e8b\u4ef6\uff08\u5982\u9ed1\u5929\u9e45\u4e8b\u4ef6\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u7ea6\u675f\u8fdd\u53cd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u6781\u7aef\u98ce\u9669\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEVO\u7b97\u6cd5\uff1a1) \u5229\u7528\u6781\u503c\u7406\u8bba\u5efa\u6a21\u6781\u7aef\u5956\u52b1\u548c\u6210\u672c\u6837\u672c\uff1b2) \u5f15\u5165\u6781\u503c\u5206\u4f4d\u6570\u4f18\u5316\u76ee\u6807\u6355\u6349\u6210\u672c\u5c3e\u90e8\u5206\u5e03\uff1b3) \u91c7\u7528\u6781\u7aef\u4f18\u5148\u7ea7\u56de\u653e\u673a\u5236\uff0c\u653e\u5927\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u6837\u672c\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u7406\u8bba\u4e0a\uff1a\u5efa\u7acb\u4e86\u7b56\u7565\u66f4\u65b0\u671f\u95f4\u9884\u671f\u7ea6\u675f\u8fdd\u53cd\u7684\u4e0a\u754c\uff0c\u4fdd\u8bc1\u5728\u96f6\u8fdd\u53cd\u5206\u4f4d\u6570\u6c34\u5e73\u4e0a\u7684\u4e25\u683c\u7ea6\u675f\u6ee1\u8db3\u3002EVO\u6bd4\u671f\u671b\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u7ea6\u675f\u8fdd\u53cd\u6982\u7387\uff0c\u6bd4\u5206\u4f4d\u6570\u56de\u5f52\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u65b9\u5dee\u3002\u5b9e\u9a8c\u4e0a\uff1aEVO\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u671f\u95f4\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u7ade\u4e89\u7684\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "EVO\u7b97\u6cd5\u901a\u8fc7\u6781\u503c\u7406\u8bba\u6709\u6548\u5904\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6781\u7aef\u98ce\u9669\u4e8b\u4ef6\uff0c\u5728\u4fdd\u8bc1\u7b56\u7565\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7ea6\u675f\u8fdd\u53cd\uff0c\u4e3a\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u7684\u5b89\u5168\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.13451", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13451", "abs": "https://arxiv.org/abs/2601.13451", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "title": "Event-based Heterogeneous Information Processing for Online Vision-based Obstacle Detection and Localization", "comment": null, "summary": "This paper introduces a novel framework for robotic vision-based navigation that integrates Hybrid Neural Networks (HNNs) with Spiking Neural Network (SNN)-based filtering to enhance situational awareness for unmodeled obstacle detection and localization. By leveraging the complementary strengths of Artificial Neural Networks (ANNs) and SNNs, the system achieves both accurate environmental understanding and fast, energy-efficient processing. The proposed architecture employs a dual-pathway approach: an ANN component processes static spatial features at low frequency, while an SNN component handles dynamic, event-based sensor data in real time. Unlike conventional hybrid architectures that rely on domain conversion mechanisms, our system incorporates a pre-developed SNN-based filter that directly utilizes spike-encoded inputs for localization and state estimation. Detected anomalies are validated using contextual information from the ANN pathway and continuously tracked to support anticipatory navigation strategies. Simulation results demonstrate that the proposed method offers acceptable detection accuracy while maintaining computational efficiency close to SNN-only implementations, which operate at a fraction of the resource cost. This framework represents a significant advancement in neuromorphic navigation systems for robots operating in unpredictable and dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6ee4\u6ce2\u7684\u673a\u5668\u4eba\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8e\u672a\u5efa\u6a21\u969c\u788d\u7269\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u73af\u5883\u7406\u89e3\u548c\u4f4e\u529f\u8017\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u5728\u5904\u7406\u52a8\u6001\u3001\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u7684\u672a\u5efa\u6a21\u969c\u788d\u7269\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u73af\u5883\u7406\u89e3\u548c\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u5904\u7406\u7684\u8981\u6c42\u3002\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5404\u6709\u4f18\u52bf\uff0c\u4f46\u5355\u72ec\u4f7f\u7528\u96be\u4ee5\u517c\u987e\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff1aANN\u7ec4\u4ef6\u5904\u7406\u4f4e\u9891\u9759\u6001\u7a7a\u95f4\u7279\u5f81\uff0cSNN\u7ec4\u4ef6\u5b9e\u65f6\u5904\u7406\u52a8\u6001\u4e8b\u4ef6\u4f20\u611f\u5668\u6570\u636e\u3002\u7cfb\u7edf\u5305\u542b\u9884\u5f00\u53d1\u7684SNN\u6ee4\u6ce2\u5668\uff0c\u76f4\u63a5\u4f7f\u7528\u8109\u51b2\u7f16\u7801\u8f93\u5165\u8fdb\u884c\u5b9a\u4f4d\u548c\u72b6\u6001\u4f30\u8ba1\u3002\u68c0\u6d4b\u5230\u7684\u5f02\u5e38\u901a\u8fc7ANN\u8def\u5f84\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u9a8c\u8bc1\u5e76\u6301\u7eed\u8ddf\u8e2a\uff0c\u652f\u6301\u9884\u6d4b\u6027\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u63a5\u8fd1\u7eafSNN\u5b9e\u73b0\u7684\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u53ef\u63a5\u53d7\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002\u7eafSNN\u5b9e\u73b0\u7684\u8d44\u6e90\u6210\u672c\u4ec5\u4e3a\u4f20\u7edf\u65b9\u6cd5\u7684\u4e00\u5c0f\u90e8\u5206\u3002", "conclusion": "\u8be5\u6846\u67b6\u4ee3\u8868\u4e86\u795e\u7ecf\u5f62\u6001\u5bfc\u822a\u7cfb\u7edf\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u4eba\u5728\u4e0d\u53ef\u9884\u6d4b\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\uff0c\u6210\u529f\u7ed3\u5408\u4e86ANN\u7684\u51c6\u786e\u6027\u548cSNN\u7684\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2601.11952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11952", "abs": "https://arxiv.org/abs/2601.11952", "authors": ["Haonan An", "Guang Hua", "Wei Du", "Hangcheng Cao", "Yihang Tao", "Guowen Xu", "Susanto Rahardja", "Yuguang Fang"], "title": "Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal", "comment": null, "summary": "Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.", "AI": {"tldr": "\u63d0\u51faDecoder Gradient Shields (DGS)\u9632\u5fa1\u673a\u5236\uff0c\u4fdd\u62a4\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u89e3\u7801\u5668\u514d\u53d7\u57fa\u4e8e\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u7684\u653b\u51fb\uff0c\u901a\u8fc7\u68af\u5ea6\u91cd\u5b9a\u5411\u548c\u7f29\u653e\u9632\u6b62\u6c34\u5370\u79fb\u9664\u5668\u8bad\u7ec3\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u800c\u89e3\u7801\u5668\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u5b58\u5728\u9488\u5bf9\u89e3\u7801\u5668\u7684\u653b\u51fb\u3002\u653b\u51fb\u8005\u5229\u7528\u67e5\u8be2\u54cd\u5e94\u83b7\u53d6\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u6765\u8bad\u7ec3\u6c34\u5370\u79fb\u9664\u5668\uff0c\u9700\u8981\u9632\u5fa1\u673a\u5236\u4fdd\u62a4\u89e3\u7801\u5668\u3002", "method": "\u63d0\u51faDGS\u9632\u5fa1\u673a\u5236\u5bb6\u65cf\uff1aDGS-O\uff08\u8f93\u51fa\u5c42\uff09\u3001DGS-I\uff08\u8f93\u5165\u5c42\uff09\u548cDGS-L\uff08\u4e2d\u95f4\u5c42\uff09\u3002DGS-O\u6709\u95ed\u5f0f\u89e3\uff0c\u6240\u6709DGS\u90fd\u6709\u53ef\u8bc1\u660e\u7684\u6027\u80fd\u3002\u901a\u8fc7\u8054\u5408\u8bbe\u8ba1\u6c34\u5370\u901a\u9053\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u7684\u68af\u5ea6\u91cd\u5b9a\u5411\u548c\u7f29\u653e\uff0c\u9632\u6b62\u6c34\u5370\u79fb\u9664\u5668\u8fbe\u5230\u4f4e\u635f\u5931\u503c\u8bad\u7ec3\u6536\u655b\u3002", "result": "\u5728\u53bb\u96e8\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u65e0\u76d2\u6c34\u5370\u6280\u672f\uff0cDGS\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86100%\u7684\u9632\u5fa1\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7801\u5668\u8f93\u51fa\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "DGS\u662f\u6709\u6548\u7684\u89e3\u7801\u5668\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u4fdd\u62a4\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u514d\u53d7\u57fa\u4e8e\u68af\u5ea6\u6cc4\u9732\u7684\u653b\u51fb\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6c34\u5370\u7814\u7a76\u4e2d\u89e3\u7801\u5668\u5b89\u5168\u6027\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.12095", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12095", "abs": "https://arxiv.org/abs/2601.12095", "authors": ["Hamidreza Sadeghi", "Saeedeh Momtazi", "Reza Safabakhsh"], "title": "Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding", "comment": null, "summary": "Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u540c\u6784\u573a\uff0c\u4f7f\u7528\u5d4c\u5165\u5411\u91cf\u8868\u793a\u6570\u5b57\u4ee5\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6781\u503c\u65f6\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u8be5\u5d4c\u5165\u80fd\u4fdd\u6301\u6709\u7406\u6570\u57df\u4e0a\u7684\u4ee3\u6570\u8fd0\u7b97\u6027\u8d28\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u6781\u5c0f\u6570\u6216\u6781\u5927\u6570\u65f6\u9762\u4e34\u6ea2\u51fa\u3001\u4e0b\u6ea2\u548c\u8f93\u51fa\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u4ee3\u6570\u6027\u8d28\u540c\u65f6\u907f\u514d\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u6570\u5b57\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u540c\u6784\u573a\u4f5c\u4e3a\u4ee3\u6570\u7ed3\u6784\uff08\u5982\u7fa4\u3001\u57df\uff09\u7684\u795e\u7ecf\u62bd\u8c61\uff0c\u4f7f\u7528\u56fa\u5b9a\u957f\u5ea6\u7684\u5d4c\u5165\u5411\u91cf\u8868\u793a\u6570\u5b57\uff0c\u8fd9\u4e9b\u5d4c\u5165\u5411\u91cf\u5728\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4ee3\u6570\u7ed3\u6784\uff0c\u7279\u522b\u662f\u80fd\u4fdd\u6301\u6709\u7406\u6570\u57df\u4e0a\u7684\u52a0\u6cd5\u3001\u4e58\u6cd5\u548c\u6bd4\u8f83\u8fd0\u7b97\u3002", "result": "\u52a0\u6cd5\u8fd0\u7b97\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6052\u7b49\u6027\u3001\u5c01\u95ed\u6027\u548c\u7ed3\u5408\u6027\u7b49\u5173\u952e\u4ee3\u6570\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8d85\u8fc795%\uff1b\u4e58\u6cd5\u8fd0\u7b97\u9762\u4e34\u6311\u6218\uff0c\u5728\u4e0d\u540c\u4ee3\u6570\u6027\u8d28\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u572853%\u523073%\u4e4b\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u52a0\u6cd5\u8fd0\u7b97\u7684\u4ee3\u6570\u6027\u8d28\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u4e58\u6cd5\u8fd0\u7b97\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e2d\u6570\u503c\u8868\u793a\u7684\u4ee3\u6570\u7ed3\u6784\u4fdd\u6301\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.11981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11981", "abs": "https://arxiv.org/abs/2601.11981", "authors": ["Jian Lang", "Rongpei Hong", "Ting Zhong", "Yong Wang", "Fan Zhou"], "title": "Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection", "comment": "13 pages. Accepted by KDD 2026 research track. Codes are released at https://github.com/Jian-Lang/RADAR", "summary": "Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.", "AI": {"tldr": "RADAR\uff1a\u9996\u4e2a\u652f\u6301\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u68c0\u6d4b\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5f15\u5bfc\u7684\u9002\u5e94\u8303\u5f0f\u89e3\u51b3\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u7684\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u65b0\u95fb\u4e3b\u9898\u5206\u5e03\u4e00\u81f4\uff0c\u65e0\u6cd5\u68c0\u6d4b\u4e0e\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u76f8\u5173\u7684\u5047\u65b0\u95fb\u89c6\u9891\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\u3002", "method": "\u63d0\u51faRADAR\u6846\u67b6\uff0c\u91c7\u7528\u68c0\u7d22\u5f15\u5bfc\u7684\u9002\u5e94\u8303\u5f0f\uff1a1) \u57fa\u4e8e\u71b5\u9009\u62e9\u7684\u68c0\u7d22\u673a\u5236\uff0c\u4e3a\u76ee\u6807\u57df\u89c6\u9891\u63d0\u4f9b\u7a33\u5b9a\uff08\u4f4e\u71b5\uff09\u76f8\u5173\u53c2\u8003\uff1b2) \u7a33\u5b9a\u951a\u70b9\u5f15\u5bfc\u7684\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u5206\u5e03\u7ea7\u5339\u914d\u5c06\u4e0d\u7a33\u5b9a\u5b9e\u4f8b\u8868\u793a\u5bf9\u9f50\u5230\u6e90\u57df\uff1b3) \u76ee\u6807\u57df\u611f\u77e5\u7684\u81ea\u8bad\u7ec3\u8303\u5f0f\uff0c\u751f\u6210\u7531\u7a33\u5b9a\u53c2\u8003\u589e\u5f3a\u7684\u4fe1\u606f\u6027\u4f2a\u6807\u7b7e\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRADAR\u5728\u6d4b\u8bd5\u65f6\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u5bf9\u672a\u89c1\u5047\u65b0\u95fb\u89c6\u9891\u4e3b\u9898\u5b9e\u73b0\u5f3a\u5927\u7684\u5373\u65f6\u9002\u5e94\u3002", "conclusion": "RADAR\u662f\u9996\u4e2a\u652f\u6301\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u68c0\u6d4b\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u68c0\u7d22\u5f15\u5bfc\u9002\u5e94\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.12131", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.12131", "abs": "https://arxiv.org/abs/2601.12131", "authors": ["Santosh Chapagain", "MohammadReza EskandariNasab", "Onur Vural", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics", "comment": "This is preliminary work towards a broader SolarGPT framework", "summary": "Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.\n  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.", "AI": {"tldr": "SolarGPT-QA\u662f\u57fa\u4e8eLLaMA-3\u6784\u5efa\u7684\u9886\u57df\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u7a7a\u95f4\u5929\u6c14\u548c\u592a\u9633\u7269\u7406\u5b66\u7684\u6559\u80b2\u95ee\u7b54\uff0c\u901a\u8fc7\u79d1\u5b66\u6587\u732e\u548cGPT-4\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5728\u6559\u80b2\u89e3\u91ca\u65b9\u9762\u4e0e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u7ade\u4e89\u3002", "motivation": "\u592a\u9633\u6d3b\u52a8\uff08\u592a\u9633\u8000\u6591\u3001\u65e5\u5195\u7269\u8d28\u629b\u5c04\u3001\u5730\u78c1\u66b4\uff09\u5bf9\u536b\u661f\u3001\u822a\u7a7a\u3001\u7535\u7f51\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u6781\u7aef\u4e8b\u4ef6\u53ef\u80fd\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\u3002\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e14\u65e0\u6cd5\u6e05\u6670\u89e3\u91ca\u590d\u6742\u7684\u7a7a\u95f4\u79d1\u5b66\u6982\u5ff5\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6559\u80b2\u5de5\u5177\u3002", "method": "\u57fa\u4e8eLLaMA-3\u57fa\u7840\u6a21\u578b\u6784\u5efaSolarGPT-QA\u95ee\u7b54\u7cfb\u7edf\uff0c\u4f7f\u7528\u79d1\u5b66\u6587\u732e\u548cGPT-4\u751f\u6210\u7684\u5927\u89c4\u6a21\u95ee\u7b54\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u7528Grok-3\u4ee5\u5b66\u751f\u53cb\u597d\u7684\u6545\u4e8b\u98ce\u683c\u8fdb\u884c\u7cbe\u70bc\u3002\u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u6559\u5b66\u5fae\u8c03\uff0c\u5e73\u8861\u79d1\u5b66\u51c6\u786e\u6027\u548c\u6559\u80b2\u6548\u679c\u3002", "result": "\u4eba\u5de5\u6210\u5bf9\u8bc4\u4f30\u663e\u793a\uff0cSolarGPT-QA\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5728\u6559\u80b2\u89e3\u91ca\u65b9\u9762\u4e0e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u7ade\u4e89\u3002\u5c0f\u578b\u8bd5\u70b9\u5b66\u751f\u7406\u89e3\u7814\u7a76\u8868\u660e\u751f\u6210\u7684\u89e3\u91ca\u6e05\u6670\u5ea6\u548c\u53ef\u8bbf\u95ee\u6027\u6709\u6240\u6539\u5584\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u4e0e\u6559\u5b66\u5fae\u8c03\u7ed3\u5408\u5bf9\u5e73\u8861\u79d1\u5b66\u51c6\u786e\u6027\u548c\u6559\u80b2\u6548\u679c\u5f88\u91cd\u8981\u3002", "conclusion": "SolarGPT-QA\u4ee3\u8868\u4e86\u5411\u66f4\u5e7f\u6cdb\u7684SolarGPT\u6846\u67b6\u8fc8\u51fa\u7684\u7b2c\u4e00\u6b65\uff0c\u8be5\u6846\u67b6\u65e8\u5728\u7a7a\u95f4\u79d1\u5b66\u6559\u80b2\u548c\u9884\u6d4b\u3002\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u6559\u5b66\u80fd\u529b\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u592a\u9633\u6d3b\u52a8\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7a7a\u95f4\u5929\u6c14\u610f\u8bc6\u548c\u6559\u80b2\u6548\u679c\u3002"}}
{"id": "2601.12010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12010", "abs": "https://arxiv.org/abs/2601.12010", "authors": ["Yifei Chen", "Ross Greer"], "title": "SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine", "comment": null, "summary": "The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.", "AI": {"tldr": "\u63d0\u51faSMc2f\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u6d41\u7a0b\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u6316\u6398\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7c97\u7b5b\u9009\u3001\u6784\u5efa\u6848\u4f8b\u6570\u636e\u5e93\uff0c\u5e76\u5f15\u5165\u6587\u672c-\u8f68\u8ff9\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709RefAV\u6846\u67b6\u4ec5\u57fa\u4e8e\u8f68\u8ff9\u6807\u7b7e\u8fdb\u884c\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u81ea\u7136\u8bed\u8a00\u4e0e\u539f\u59cbRGB\u56fe\u50cf\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u4e14\u4f9d\u8d56\u4e0a\u6e383D\u76ee\u6807\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7684\u8d28\u91cf\uff0c\u8f68\u8ff9\u6570\u636e\u4e0d\u51c6\u786e\u4f1a\u5bfc\u81f4\u65f6\u7a7a\u5b9a\u4f4d\u9519\u8bef\u3002\u9700\u8981\u66f4\u9c81\u68d2\u7684\u573a\u666f\u6316\u6398\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSMc2f\u7c97\u5230\u7ec6\u6d41\u7a0b\uff1a1) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7c97\u7c92\u5ea6\u56fe\u50cf-\u6587\u672c\u8fc7\u6ee4\uff1b2) \u5728RefAV\u57fa\u7840\u4e0a\u6784\u5efa\u6210\u529f\u6316\u6398\u6848\u4f8b\u6570\u636e\u5e93\uff0c\u81ea\u52a8\u68c0\u7d22\u793a\u4f8b\u8fdb\u884c\u5c11\u6837\u672c\u6761\u4ef6\u5b66\u4e60\uff1b3) \u5f15\u5165\u6587\u672c-\u8f68\u8ff9\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5c06\u5339\u914d\u5bf9\u62c9\u8fd1\u3001\u4e0d\u5339\u914d\u5bf9\u63a8\u8fdc\uff0c\u83b7\u5f97\u7ec6\u7c92\u5ea6\u5339\u914d\u5668\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SMc2f\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6848\u4f8b\u6570\u636e\u5e93\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u573a\u666f\u6316\u6398\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12822", "abs": "https://arxiv.org/abs/2601.12822", "authors": ["Wenqi Zhang", "Yulin Shen", "Changyue Jiang", "Jiarun Dai", "Geng Hong", "Xudong Pan"], "title": "MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction", "comment": null, "summary": "Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.", "AI": {"tldr": "MirrorGuard\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u62df\u7684\u8bad\u7ec3\u6765\u589e\u5f3a\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u4f7f\u7528\u795e\u7ecf\u7b26\u53f7\u6a21\u62df\u7ba1\u9053\u751f\u6210\u9ad8\u98ce\u9669GUI\u4ea4\u4e92\u8f68\u8ff9\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5b66\u4e60\u62e6\u622a\u548c\u7ea0\u6b63\u4e0d\u5b89\u5168\u63a8\u7406\u94fe\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u4e2d\uff0c\u4f7f\u5176\u80fd\u591f\u901a\u8fc7\u56fe\u5f62\u7528\u6237\u754c\u9762\u81ea\u4e3b\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4ea4\u4e92\uff0c\u6267\u884c\u590d\u6742\u4efb\u52a1\u3002\u8fd9\u79cd\u81ea\u4e3b\u6027\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff1a\u6076\u610f\u6307\u4ee4\u6216\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u53ef\u80fd\u89e6\u53d1\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u5e76\u5bfc\u81f4\u6709\u5bb3\u7684\u7cfb\u7edf\u7ea7\u64cd\u4f5c\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u68c0\u6d4b\u7684\u9632\u5fa1\u65b9\u6cd5\u867d\u7136\u80fd\u9632\u6b62\u635f\u5bb3\uff0c\u4f46\u901a\u5e38\u4f1a\u8fc7\u65e9\u4e2d\u6b62\u4efb\u52a1\uff0c\u964d\u4f4e\u4e86\u4ee3\u7406\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faMirrorGuard\u9632\u5fa1\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u62df\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u6a21\u62df\u7ba1\u9053\uff0c\u5728\u7eaf\u6587\u672c\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u903c\u771f\u7684\u9ad8\u98ce\u9669GUI\u4ea4\u4e92\u8f68\u8ff9\uff0c\u6355\u83b7\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u6a21\u5f0f\u548c\u6f5c\u5728\u7cfb\u7edf\u5371\u9669\uff0c\u800c\u65e0\u9700\u6267\u884c\u771f\u5b9e\u64cd\u4f5c\u3002\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0cMirrorGuard\u5b66\u4e60\u5728CUAs\u4ea7\u751f\u548c\u6267\u884c\u4e0d\u5b89\u5168\u64cd\u4f5c\u4e4b\u524d\u62e6\u622a\u548c\u7ea0\u6b63\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u94fe\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0c\u8de8\u591a\u4e2a\u57fa\u51c6\u548cCUA\u67b6\u6784\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cMirrorGuard\u663e\u8457\u964d\u4f4e\u4e86\u5b89\u5168\u98ce\u9669\u3002\u4f8b\u5982\uff0c\u5728\u5b57\u8282\u8df3\u52a8\u7684UI-TARS\u7cfb\u7edf\u4e0a\uff0c\u5b83\u5c06\u4e0d\u5b89\u5168\u7387\u4ece66.5%\u964d\u4f4e\u523013.0%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8bef\u62d2\u7387\uff08FRR\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6700\u5148\u8fdb\u7684GuardAgent\u4ec5\u5c06\u4e0d\u5b89\u5168\u7387\u964d\u4f4e\u523053.9%\uff0c\u4e14\u8bef\u62d2\u7387\u9ad8\u51fa15.4%\u3002", "conclusion": "MirrorGuard\u8bc1\u660e\u4e86\u57fa\u4e8e\u6a21\u62df\u7684\u9632\u5fa1\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u4ee3\u7406\u57fa\u672c\u5b9e\u7528\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u5f3a\u5927\u7684\u771f\u5b9e\u4e16\u754c\u4fdd\u62a4\u3002\u8be5\u5de5\u4f5c\u8868\u660e\uff0c\u901a\u8fc7\u6a21\u62df\u751f\u6210\u7684\u9632\u5fa1\u673a\u5236\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u800c\u4e0d\u4f1a\u8fc7\u5ea6\u5f71\u54cd\u5176\u529f\u80fd\u6027\u3002"}}
{"id": "2601.14128", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14128", "abs": "https://arxiv.org/abs/2601.14128", "authors": ["Shoujie Li", "Changqing Guo", "Junhao Gong", "Chenxin Liang", "Wenhua Ding", "Wenbo Ding"], "title": "SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media", "comment": "Accepted by IEEE Transactions on Robotics", "summary": "Perception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw-actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event-based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac's 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system's practical performance.", "AI": {"tldr": "SandWorm\u673a\u5668\u4eba\u7ed3\u5408\u4eff\u751f\u87ba\u65cb\u9a71\u52a8\u548c\u8815\u52a8\u8fd0\u52a8\u589e\u5f3a\u5728\u9897\u7c92\u4ecb\u8d28\u4e2d\u7684\u8fd0\u52a8\u80fd\u529b\uff0cSWTac\u65b0\u578b\u4e8b\u4ef6\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u901a\u8fc7\u4e3b\u52a8\u632f\u52a8\u5f39\u6027\u4f53\u548c\u5f39\u7c27\u9694\u79bb\u673a\u5236\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89e6\u89c9\u6210\u50cf\uff0c\u7cfb\u7edf\u5728\u590d\u6742\u9897\u7c92\u4ecb\u8d28\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u9897\u7c92\u4ecb\u8d28\u4e2d\u7684\u611f\u77e5\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7c92\u5b50\u52a8\u6001\u96be\u4ee5\u9884\u6d4b\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u9897\u7c92\u73af\u5883\u4e2d\u6709\u6548\u611f\u77e5\u548c\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "1) SandWorm\uff1a\u4eff\u751f\u87ba\u65cb\u9a71\u52a8\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u8815\u52a8\u8fd0\u52a8\u589e\u5f3a\u8fd0\u52a8\u80fd\u529b\uff1b2) SWTac\uff1a\u65b0\u578b\u4e8b\u4ef6\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u91c7\u7528\u4e3b\u52a8\u632f\u52a8\u5f39\u6027\u4f53\uff0c\u901a\u8fc7\u5f39\u7c27\u9694\u79bb\u673a\u5236\u5c06\u4e8b\u4ef6\u76f8\u673a\u4e0e\u632f\u52a8\u673a\u68b0\u89e3\u8026\uff1b3) IMU\u5f15\u5bfc\u7684\u65f6\u95f4\u6ee4\u6ce2\u5668\u63d0\u9ad8\u6210\u50cf\u4e00\u81f4\u6027\uff1b4) \u7cfb\u7edf\u4f18\u5316\u632f\u52a8\u53c2\u6570\u3001\u4e8b\u4ef6\u76f8\u673a\u8bbe\u7f6e\u548c\u5f39\u6027\u4f53\u7279\u6027\uff1b5) \u57fa\u4e8e\u975e\u5bf9\u79f0\u8fb9\u7f18\u7279\u5f81\u7684U-Net\u63a5\u89e6\u8868\u9762\u4f30\u8ba1\u3002", "result": "SWTac\u5b9e\u73b00.2mm\u7eb9\u7406\u5206\u8fa8\u7387\u300198%\u77f3\u5757\u5206\u7c7b\u51c6\u786e\u7387\u30010.15N\u529b\u4f30\u8ba1\u8bef\u5dee\uff1bSandWorm\u5728\u6311\u6218\u6027\u5730\u5f62\u4e2d\u5b9e\u73b0\u591a\u6837\u5316\u8fd0\u52a8\uff08\u6700\u9ad812.5mm/s\uff09\uff0c\u5728\u590d\u6742\u9897\u7c92\u4ecb\u8d28\u4e2d\u6210\u529f\u6267\u884c\u7ba1\u9053\u758f\u6d5a\u548c\u5730\u4e0b\u52d8\u63a2\uff08\u6210\u529f\u738790%\uff09\uff1bIMU\u5f15\u5bfc\u6ee4\u6ce2\u5668\u5c06MSNR\u63d0\u9ad824%\u3002", "conclusion": "SandWorm\u548cSWTac\u7cfb\u7edf\u5728\u9897\u7c92\u4ecb\u8d28\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u611f\u77e5\u548c\u8fd0\u52a8\u80fd\u529b\uff0c\u73b0\u573a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u9897\u7c92\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12076", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12076", "abs": "https://arxiv.org/abs/2601.12076", "authors": ["H. Jiang", "Y. Sun", "Z. Dong", "T. Liu", "Y. Gu"], "title": "CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation", "comment": null, "summary": "Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5206\u5272\u57fa\u51c6RS-RVOS Bench\u548c\u57fa\u4e8eSAM\u7684\u8bb0\u5fc6\u8d28\u91cf\u63a7\u5236\u6846\u67b6MQC-SAM\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u573a\u666f\u4e2d\u76ee\u6807\u663e\u8457\u6027\u5f31\u3001\u89c6\u89c9\u4fe1\u606f\u622a\u65ad\u4ee5\u53ca\u8bb0\u5fc6\u504f\u5dee\u79ef\u7d2f\u7b49\u95ee\u9898\u3002", "motivation": "\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5206\u5272\u9762\u4e34\u76ee\u6807\u663e\u8457\u6027\u5f31\u3001\u52a8\u6001\u573a\u666f\u89c6\u89c9\u4fe1\u606f\u622a\u65ad\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e13\u7528\u57fa\u51c6\uff0c\u4e14\u73b0\u6709\u6a21\u578b\u5b58\u5728\u521d\u59cb\u8bb0\u5fc6\u6784\u5efa\u504f\u5dee\u548c\u566a\u58f0\u79ef\u7d2f\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u7684\u95ee\u9898\u3002", "method": "1) \u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5206\u5272\u57fa\u51c6RS-RVOS Bench\uff0c\u5305\u542b111\u4e2a\u89c6\u9891\u5e8f\u5217\u3001\u7ea625,000\u5e27\u548c213,000\u4e2a\u65f6\u5e8f\u6307\u4ee3\u6807\u6ce8\uff0c\u91c7\u7528\u4e25\u683c\u7684\u56e0\u679c\u611f\u77e5\u6807\u6ce8\u7b56\u7565\uff1b2) \u63d0\u51fa\u8bb0\u5fc6\u8d28\u91cf\u611f\u77e5\u7684\u5728\u7ebf\u6307\u4ee3\u5206\u5272\u6846\u67b6MQC-SAM\uff0c\u5305\u542b\u65f6\u5e8f\u8fd0\u52a8\u4e00\u81f4\u6027\u6a21\u5757\u8fdb\u884c\u521d\u59cb\u8bb0\u5fc6\u6821\u51c6\uff0c\u4ee5\u53ca\u89e3\u8026\u6ce8\u610f\u529b\u8bb0\u5fc6\u96c6\u6210\u673a\u5236\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u8bc4\u4f30\u548c\u9009\u62e9\u6027\u66f4\u65b0\u3002", "result": "\u5728RS-RVOS Bench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMQC-SAM\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u548c\u65b9\u6cd5\u7684\u53cc\u91cd\u8d21\u732e\u63a8\u8fdb\u4e86\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5206\u5272\u7814\u7a76\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2601.12330", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12330", "abs": "https://arxiv.org/abs/2601.12330", "authors": ["Zuha Fatima", "Muhammad Anser Sohaib", "Muhammad Talha", "Ayesha Kanwal", "Sidra Sultana", "Nazia Perwaiz"], "title": "IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning", "comment": null, "summary": "Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.", "AI": {"tldr": "IceWatch\u662f\u4e00\u4e2a\u7ed3\u5408\u7a7a\u95f4\u89c6\u89c9\u548c\u65f6\u5e8f\u7269\u7406\u52a8\u6001\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u51b0\u5ddd\u6e56\u6e83\u51b3\u6d2a\u6c34\u9884\u6d4b\uff0c\u63d0\u9ad8\u9884\u6d4b\u53ef\u9760\u6027\u548c\u5b9e\u65f6\u6027", "motivation": "\u4f20\u7edfGLOF\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6c34\u6587\u5efa\u6a21\u3001\u9608\u503c\u76d1\u6d4b\u548c\u4eba\u5de5\u536b\u661f\u56fe\u50cf\u5206\u6790\uff0c\u5b58\u5728\u66f4\u65b0\u6162\u3001\u4f9d\u8d56\u4eba\u5de5\u3001\u4e91\u5e72\u6270\u548c\u73b0\u573a\u6570\u636e\u7f3a\u5931\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u548c\u53ef\u9760\u7684\u9884\u6d4b\u7cfb\u7edf", "method": "\u63d0\u51faIceWatch\u6846\u67b6\uff0c\u5305\u542b\u89c6\u89c9\u7ec4\u4ef6RiskFlow\uff08\u57fa\u4e8eCNN\u7684Sentinel-2\u536b\u661f\u56fe\u50cf\u5206\u7c7b\u5668\uff09\u548c\u8868\u683c\u7ec4\u4ef6\uff08TerraFlow\u5efa\u6a21\u51b0\u5ddd\u901f\u5ea6\uff0cTempFlow\u9884\u6d4b\u8fd1\u5730\u8868\u6e29\u5ea6\uff09\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u5b9e\u73b0\u7269\u7406\u4fe1\u606f\u589e\u5f3a\u7684GLOF\u9884\u6d4b", "result": "\u7cfb\u7edf\u5177\u6709\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\u3001\u5feb\u901f\u6570\u636e\u5904\u7406\u80fd\u529b\u3001\u5bf9\u566a\u58f0\u548c\u7f3a\u5931\u4fe1\u606f\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u63d0\u9ad8GLOF\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "IceWatch\u4e3a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684GLOF\u9884\u8b66\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\uff0c\u5177\u6709\u6574\u5408\u591a\u79cd\u4f20\u611f\u5668\u8f93\u5165\u548c\u5168\u7403\u51b0\u5ddd\u76d1\u6d4b\u6d3b\u52a8\u7684\u6f5c\u529b"}}
{"id": "2601.12505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12505", "abs": "https://arxiv.org/abs/2601.12505", "authors": ["Ashish Raj Shekhar", "Shiven Agarwal", "Priyanuj Bordoloi", "Yash Shah", "Tejas Anvekar", "Vivek Gupta"], "title": "DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.", "AI": {"tldr": "DoPE\u662f\u4e00\u79cd\u6587\u6863\u5c42\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5728PDF/HTML\u8003\u8bd5\u6587\u6863\u4e2d\u5d4c\u5165\u8bed\u4e49\u8bf1\u9975\u6765\u5bf9\u6297\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9632\u6b62\u81ea\u52a8\u5316\u4f5c\u5f0a\u5e76\u68c0\u6d4bAI\u4f9d\u8d56\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5904\u7406\u8003\u8bd5\u6587\u6863\uff0c\u5a01\u80c1\u4f20\u7edf\u8bc4\u4f30\u65b9\u5f0f\u548c\u5b66\u672f\u8bda\u4fe1\u3002\u9700\u8981\u5f00\u53d1\u6a21\u578b\u65e0\u5173\u7684\u9632\u5fa1\u673a\u5236\u6765\u9632\u6b62\u81ea\u52a8\u5316\u4f5c\u5f0a\u548c\u68c0\u6d4bAI\u4f9d\u8d56\u3002", "method": "\u63d0\u51faDoPE\u6846\u67b6\uff0c\u5728\u6587\u6863\u521b\u4f5c\u65f6\u5d4c\u5165\u8bed\u4e49\u8bf1\u9975\uff0c\u5229\u7528MLLM\u6e32\u67d3-\u89e3\u6790\u5dee\u5f02\u3002\u5f00\u53d1FewSoRT-Q\u751f\u6210\u95ee\u9898\u7ea7\u8bed\u4e49\u8bf1\u9975\uff0cFewSoRT-D\u5c06\u5176\u5c01\u88c5\u5230\u6c34\u5370\u6587\u6863\u4e2d\u3002", "result": "\u5728Integrity-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf9OpenAI\u548cAnthropic\u7684\u9ed1\u76d2MLLM\uff0cDoPE\u5b9e\u73b091.4%\u68c0\u6d4b\u7387\uff088.7%\u8bef\u62a5\u7387\uff09\uff0c96.3%\u7684\u5c1d\u8bd5\u88ab\u963b\u6b62\u6216\u8bf1\u5bfc\u5931\u8d25\u3002", "conclusion": "DoPE\u63d0\u4f9b\u6709\u6548\u7684\u6587\u6863\u5c42\u9632\u5fa1\u673a\u5236\uff0c\u4fdd\u62a4\u5b66\u672f\u8bda\u4fe1\uff0c\u65e0\u9700\u4f9d\u8d56\u4f20\u7edf\u5355\u6b21\u5206\u7c7b\u5668\u3002\u5f00\u6e90\u57fa\u51c6\u3001\u5de5\u5177\u5305\u548c\u4ee3\u7801\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2601.12362", "categories": ["cs.LG", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2601.12362", "abs": "https://arxiv.org/abs/2601.12362", "authors": ["Natthapong Promsricha", "Chotirawee Chatpattanasiri", "Nuttavut Kerdgongsup", "Stavroula Balabani"], "title": "Machine Learning-Based Framework for Real Time Detection and Early Prediction of Control Valve Stiction in Industrial Control Systems", "comment": null, "summary": "Control valve stiction, a friction that prevents smooth valve movement, is a common fault in industrial process systems that causes instability, equipment wear, and higher maintenance costs. Many plants still operate with conventional valves that lack real time monitoring, making early predictions challenging. This study presents a machine learning (ML) framework for detecting and predicting stiction using only routinely collected process signals: the controller output (OP) from control systems and the process variable (PV), such as flow rate. Three deep learning models were developed and compared: a Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine (CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models, a data-driven labeling method based on slope ratio analysis was applied to a real oil and gas refinery dataset. The LSTM model achieved the highest accuracy and was able to predict stiction up to four hours in advance. To the best of the authors' knowledge, this is the first study to demonstrate ML based early prediction of control valve stiction from real industry data. The proposed framework can be integrated into existing control systems to support predictive maintenance, reduce downtime, and avoid unnecessary hardware replacement.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u63a7\u5236\u9600\u7c98\u6ede\u6545\u969c\u68c0\u6d4b\u4e0e\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u5e38\u89c4\u8fc7\u7a0b\u4fe1\u53f7\uff08OP\u548cPV\uff09\uff0cLSTM\u6a21\u578b\u5728\u771f\u5b9e\u70bc\u6cb9\u5382\u6570\u636e\u4e0a\u5b9e\u73b0\u6700\u9ad8\u7cbe\u5ea6\uff0c\u53ef\u63d0\u524d4\u5c0f\u65f6\u9884\u6d4b\u7c98\u6ede\u6545\u969c\u3002", "motivation": "\u63a7\u5236\u9600\u7c98\u6ede\u662f\u5de5\u4e1a\u8fc7\u7a0b\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u6545\u969c\uff0c\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u3001\u8bbe\u5907\u78e8\u635f\u548c\u7ef4\u62a4\u6210\u672c\u589e\u52a0\u3002\u8bb8\u591a\u5de5\u5382\u4ecd\u4f7f\u7528\u7f3a\u4e4f\u5b9e\u65f6\u76d1\u63a7\u7684\u4f20\u7edf\u9600\u95e8\uff0c\u4f7f\u5f97\u65e9\u671f\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aCNN\u3001CNN-SVM\u6df7\u5408\u6a21\u578b\u548cLSTM\u7f51\u7edc\u3002\u4f7f\u7528\u57fa\u4e8e\u659c\u7387\u6bd4\u5206\u6790\u7684\u6570\u636e\u9a71\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u70bc\u6cb9\u5382\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002", "result": "LSTM\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7cbe\u5ea6\uff0c\u80fd\u591f\u63d0\u524d4\u5c0f\u65f6\u9884\u6d4b\u63a7\u5236\u9600\u7c98\u6ede\u6545\u969c\u3002\u636e\u4f5c\u8005\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u5b9e\u73b0ML\u65e9\u671f\u9884\u6d4b\u63a7\u5236\u9600\u7c98\u6ede\u7684\u7814\u7a76\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u96c6\u6210\u5230\u73b0\u6709\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u652f\u6301\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u51cf\u5c11\u505c\u673a\u65f6\u95f4\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u786c\u4ef6\u66f4\u6362\u3002"}}
{"id": "2601.12401", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12401", "abs": "https://arxiv.org/abs/2601.12401", "authors": ["Jinmei Liu", "Haoru Li", "Zhenhong Sun", "Chaofeng Chen", "Yatao Bian", "Bo Wang", "Daoyi Dong", "Chunlin Chen", "Zhi Wang"], "title": "Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \\textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \\textbf{DRIFT} (\\textbf{D}ive\\textbf{R}sity-\\textbf{I}ncentivized Reinforcement \\textbf{F}ine-\\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \\textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \\textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \\textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\\%\\!\\sim\\! 43.46\\%$ increase in diversity at equivalent alignment levels and a $ 59.65\\% \\!\\sim\\! 65.86\\%$ increase in alignment at equivalent levels of diversity.", "AI": {"tldr": "DRIFT\u6846\u67b6\u901a\u8fc7\u91c7\u6837\u3001\u63d0\u793a\u548c\u4f18\u5316\u4e09\u4e2a\u89d2\u5ea6\u89e3\u51b3RL\u5fae\u8c03\u751f\u6210\u6a21\u578b\u65f6\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u5bf9\u9f50\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5fae\u8c03\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u65f6\u5b58\u5728\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5373\u4f18\u5316\u8fc7\u7a0b\u5bfc\u81f4\u7b56\u7565\u6536\u655b\u5230\u72c4\u62c9\u514b\u5206\u5e03\uff0c\u9650\u5236\u4e86\u751f\u6210\u591a\u6837\u6027\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u9700\u8981\u65e2\u5bf9\u9f50\u4efb\u52a1\u53c8\u4fdd\u6301\u591a\u6837\u6027\u7684\u751f\u6210\u7ed3\u679c\u3002", "method": "\u63d0\u51faDRIFT\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u89d2\u5ea6\u6fc0\u52b1\u591a\u6837\u6027\uff1a1) \u91c7\u6837\u5956\u52b1\u96c6\u4e2d\u5b50\u96c6\u8fc7\u6ee4\u5f02\u5e38\u503c\u9632\u6b62\u8fc7\u65e9\u5d29\u6e83\uff1b2) \u4f7f\u7528\u968f\u673a\u53d8\u4f53\u63d0\u793a\u6269\u5c55\u6761\u4ef6\u7a7a\u95f4\uff1b3) \u901a\u8fc7\u57fa\u4e8e\u52bf\u80fd\u7684\u5956\u52b1\u5851\u9020\u673a\u5236\u4f18\u5316\u7ec4\u5185\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDRIFT\u5728\u4efb\u52a1\u5bf9\u9f50\u548c\u751f\u6210\u591a\u6837\u6027\u65b9\u9762\u8fbe\u5230\u5e15\u7d2f\u6258\u6700\u4f18\uff0c\u5728\u540c\u7b49\u5bf9\u9f50\u6c34\u5e73\u4e0b\u591a\u6837\u6027\u63d0\u53479.08%~43.46%\uff0c\u5728\u540c\u7b49\u591a\u6837\u6027\u6c34\u5e73\u4e0b\u5bf9\u9f50\u5ea6\u63d0\u534759.65%~65.86%\u3002", "conclusion": "DRIFT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86RL\u5fae\u8c03\u751f\u6210\u6a21\u578b\u65f6\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u5bf9\u9f50\u4e0e\u751f\u6210\u591a\u6837\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u9700\u8981\u591a\u6837\u5316\u5019\u9009\u751f\u6210\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13565", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.13565", "abs": "https://arxiv.org/abs/2601.13565", "authors": ["Yu Qin", "Shimeng Fan", "Fan Yang", "Zixuan Xue", "Zijie Mai", "Wenrui Chen", "Kailun Yang", "Zhiyong Li"], "title": "Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation", "comment": "The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP", "summary": "Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.", "AI": {"tldr": "FiCoP\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5168\u5c40\u5339\u914d\u8f6c\u5411\u7a7a\u95f4\u7ea6\u675f\u7684patch\u7ea7\u5bf9\u5e94\u7684\u7ec6\u7c92\u5ea6\u5bf9\u5e94\u4f4d\u59ff\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7patch-to-patch\u5173\u8054\u77e9\u9635\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u6765\u7f29\u5c0f\u5339\u914d\u8303\u56f4\uff0c\u5728\u5f00\u653e\u8bcd\u6c476D\u7269\u4f53\u4f4d\u59ff\u4f30\u8ba1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c476D\u7269\u4f53\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u65e0\u7ea6\u675f\u7684\u5168\u5c40\u5339\u914d\u7b56\u7565\uff0c\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u5c06\u951a\u70b9\u7279\u5f81\u4e0e\u6574\u4e2a\u67e5\u8be2\u56fe\u50cf\u7a7a\u95f4\u5339\u914d\u4f1a\u5f15\u5165\u8fc7\u591a\u6b67\u4e49\uff0c\u76ee\u6807\u7279\u5f81\u5bb9\u6613\u4e0e\u80cc\u666f\u5e72\u6270\u7269\u6df7\u6dc6\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "1) \u7269\u4f53\u4e2d\u5fc3\u89e3\u8026\u9884\u5904\u7406\uff1a\u4ece\u73af\u5883\u566a\u58f0\u4e2d\u5206\u79bb\u8bed\u4e49\u76ee\u6807\uff1b2) \u8de8\u89c6\u89d2\u5168\u5c40\u611f\u77e5\u6a21\u5757\uff1a\u878d\u5408\u53cc\u89c6\u89d2\u7279\u5f81\uff0c\u901a\u8fc7\u663e\u5f0f\u4e0a\u4e0b\u6587\u63a8\u7406\u5efa\u7acb\u7ed3\u6784\u5171\u8bc6\uff1b3) Patch\u5173\u8054\u9884\u6d4b\u5668\uff1a\u751f\u6210\u7cbe\u786e\u7684\u5757\u7ea7\u5173\u8054\u56fe\uff0c\u4f5c\u4e3a\u7a7a\u95f4\u8fc7\u6ee4\u5668\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u6297\u566a\u58f0\u5339\u914d\u3002", "result": "\u5728REAL275\u548cToyota-Light\u6570\u636e\u96c6\u4e0a\uff0cFiCoP\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5206\u522b\u63d0\u9ad8\u4e868.0%\u548c6.1%\u7684\u5e73\u5747\u53ec\u56de\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u3001\u65e0\u7ea6\u675f\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u9c81\u68d2\u548c\u6cdb\u5316\u611f\u77e5\u7684\u80fd\u529b\u3002", "conclusion": "FiCoP\u901a\u8fc7\u4ece\u566a\u58f0\u654f\u611f\u7684\u5168\u5c40\u5339\u914d\u8f6c\u5411\u7a7a\u95f4\u7ea6\u675f\u7684patch\u7ea7\u5bf9\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c476D\u4f4d\u59ff\u4f30\u8ba1\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u611f\u77e5\u57fa\u7840\u3002"}}
{"id": "2601.12405", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12405", "abs": "https://arxiv.org/abs/2601.12405", "authors": ["Manasi Kanade", "Abhi Thakkar", "Gabriela Fernandes"], "title": "Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants", "comment": null, "summary": "Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.\n  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.\n  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.\n  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.\n  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u513f\u7ae5\u7259\u79d1\u98ce\u9669\u5206\u5c42\uff0c\u4f18\u5148\u8003\u8651\u53ef\u89e3\u91ca\u6027\u548c\u4f26\u7406\u90e8\u7f72\u800c\u975e\u6700\u5927\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u513f\u7ae5\u7259\u79d1\u75be\u75c5\u662f\u5168\u7403\u6700\u666e\u904d\u4e14\u4e0d\u516c\u5e73\u7684\u6162\u6027\u5065\u5eb7\u72b6\u51b5\u4e4b\u4e00\u3002\u867d\u7136\u6d41\u884c\u75c5\u5b66\u8bc1\u636e\u663e\u793a\u53e3\u8154\u5065\u5eb7\u7ed3\u679c\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u548c\u4eba\u53e3\u56e0\u7d20\u76f8\u5173\uff0c\u4f46\u5927\u591a\u6570\u7259\u79d1AI\u5e94\u7528\u4f9d\u8d56\u57fa\u4e8e\u56fe\u50cf\u7684\u8bca\u65ad\u548c\u9ed1\u76d2\u9884\u6d4b\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u513f\u7ae5\u7fa4\u4f53\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u4f26\u7406\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u4eba\u53e3\u6c34\u5e73\u7684\u513f\u7ae5\u6570\u636e\uff08\u5305\u62ec\u5e74\u9f84\u3001\u6536\u5165\u8d2b\u56f0\u6bd4\u3001\u79cd\u65cf/\u6c11\u65cf\u3001\u6027\u522b\u548c\u75c5\u53f2\uff09\u8bad\u7ec3\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u4f7f\u7528ROC\u5206\u6790\u548c\u6821\u51c6\u66f2\u7ebf\u8bc4\u4f30\u6027\u80fd\uff0c\u901a\u8fc7SHAP\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u4f9b\u5168\u5c40\u548c\u4e2a\u4f53\u5c42\u9762\u7684\u9884\u6d4b\u89e3\u91ca\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u9002\u5ea6\u7684\u533a\u5206\u80fd\u529b\uff08AUC=0.61\uff09\uff0c\u5177\u6709\u4fdd\u5b88\u7684\u6821\u51c6\u7279\u6027\uff0c\u5728\u9ad8\u6982\u7387\u6c34\u5e73\u4e0b\u4f4e\u4f30\u98ce\u9669\u3002SHAP\u5206\u6790\u663e\u793a\u5e74\u9f84\u548c\u6536\u5165\u8d2b\u56f0\u6bd4\u662f\u9884\u6d4b\u98ce\u9669\u7684\u6700\u5f3a\u8d21\u732e\u56e0\u7d20\uff0c\u5176\u6b21\u662f\u79cd\u65cf/\u6c11\u65cf\u548c\u6027\u522b\u3002", "conclusion": "\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u900f\u660e\u7684\u3001\u9884\u9632\u5bfc\u5411\u7684\u513f\u7ae5\u7259\u79d1\u98ce\u9669\u5206\u5c42\uff0c\u652f\u6301\u4eba\u7fa4\u7b5b\u67e5\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\uff0c\u800c\u975e\u7528\u4e8e\u8bca\u65ad\u51b3\u7b56\u3002"}}
{"id": "2601.12519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12519", "abs": "https://arxiv.org/abs/2601.12519", "authors": ["Abdullah Umut Hamzaogullari", "Arkadas Ozakin"], "title": "Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized Lagrangian Neural Networks", "comment": "21 pages", "summary": "Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from trajectory data, but their unusual optimization objective leads to significant training instabilities that limit their application to complex systems. We propose several improvements that address these fundamental challenges, namely, a Hessian regularization scheme that penalizes unphysical signatures in the Lagrangian's second derivatives with respect to velocities, preventing the network from learning unstable dynamics, activation functions that are better suited to the problem of learning Lagrangians, and a physics-aware coordinate scaling that improves stability. We systematically evaluate these techniques alongside previously proposed methods for improving stability. Our improved architecture successfully trains on systems of unprecedented complexity, including triple pendulums, and achieved 96.6\\% lower validation loss value and 90.68\\% better stability than baseline LNNs in double pendulum systems. With the improved framework, we show that our LNNs can learn Lagrangians representing geodesic motion in both non-relativistic and general relativistic settings. To deal with the relativistic setting, we extended our regularization to penalize violations of Lorentzian signatures, which allowed us to predict a geodesic Lagrangian under AdS\\textsubscript{4} spacetime metric directly from trajectory data, which to our knowledge has not been done in the literature before. This opens new possibilities for automated discovery of geometric structures in physics, including extraction of spacetime metric tensor components from geodesic trajectories. While our approach inherits some limitations of the original LNN framework, particularly the requirement for invertible Hessians, it significantly expands the practical applicability of LNNs for scientific discovery tasks.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc(LNNs)\u7684\u7a33\u5b9a\u6027\u65b9\u6cd5\uff0c\u5305\u62ecHessian\u6b63\u5219\u5316\u3001\u4e13\u7528\u6fc0\u6d3b\u51fd\u6570\u548c\u7269\u7406\u611f\u77e5\u5750\u6807\u7f29\u653e\uff0c\u6210\u529f\u8bad\u7ec3\u590d\u6742\u7cfb\u7edf\u5e76\u5e94\u7528\u4e8e\u76f8\u5bf9\u8bba\u573a\u666f\u3002", "motivation": "\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4ece\u8f68\u8ff9\u6570\u636e\u5b66\u4e60\u4efb\u610f\u62c9\u683c\u6717\u65e5\u91cf\uff0c\u4f46\u5176\u4e0d\u5bfb\u5e38\u7684\u4f18\u5316\u76ee\u6807\u5bfc\u81f4\u663e\u8457\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u6539\u8fdb\uff1a1) Hessian\u6b63\u5219\u5316\u65b9\u6848\uff0c\u60e9\u7f5a\u62c9\u683c\u6717\u65e5\u91cf\u5bf9\u901f\u5ea6\u4e8c\u9636\u5bfc\u6570\u4e2d\u7684\u975e\u7269\u7406\u7279\u5f81\uff1b2) \u66f4\u9002\u5408\u5b66\u4e60\u62c9\u683c\u6717\u65e5\u91cf\u7684\u6fc0\u6d3b\u51fd\u6570\uff1b3) \u7269\u7406\u611f\u77e5\u5750\u6807\u7f29\u653e\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002\u8fd8\u6269\u5c55\u6b63\u5219\u5316\u4ee5\u60e9\u7f5a\u6d1b\u4f26\u5179\u7279\u5f81\u8fdd\u53cd\uff0c\u5904\u7406\u76f8\u5bf9\u8bba\u573a\u666f\u3002", "result": "\u6539\u8fdb\u67b6\u6784\u6210\u529f\u8bad\u7ec3\u524d\u6240\u672a\u6709\u7684\u590d\u6742\u7cfb\u7edf\uff08\u5305\u62ec\u4e09\u6446\uff09\uff0c\u5728\u53cc\u6446\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u635f\u5931\u964d\u4f4e96.6%\uff0c\u7a33\u5b9a\u6027\u63d0\u9ad890.68%\u3002\u9996\u6b21\u4ece\u8f68\u8ff9\u6570\u636e\u9884\u6d4bAdS\u2084\u65f6\u7a7a\u5ea6\u91cf\u4e0b\u7684\u6d4b\u5730\u7ebf\u62c9\u683c\u6717\u65e5\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u6269\u5c55\u4e86LNNs\u5728\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u7269\u7406\u4e2d\u51e0\u4f55\u7ed3\u6784\u7684\u81ea\u52a8\u53d1\u73b0\uff08\u5305\u62ec\u4ece\u6d4b\u5730\u7ebf\u8f68\u8ff9\u63d0\u53d6\u65f6\u7a7a\u5ea6\u91cf\u5f20\u91cf\u5206\u91cf\uff09\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2601.12543", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12543", "abs": "https://arxiv.org/abs/2601.12543", "authors": ["Alireza Ghahtarani", "Martin Cousineau", "Amir-massoud Farahmand", "Jorge E. Mendoza"], "title": "Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem", "comment": "41 pages", "summary": "We study the online centralized charging scheduling problem (OCCSP). In this problem, a central authority must decide, in real time, when to charge dynamically arriving electric vehicles (EVs), subject to capacity limits, with the objective of balancing load across a finite planning horizon. To solve the problem, we first gamify it; that is, we model it as a game where charging blocks are placed within temporal and capacity constraints on a grid. We design heuristic policies, train learning agents with expert demonstrations, and improve them using Dataset Aggregation (DAgger). From a theoretical standpoint, we show that gamification reduces model complexity and yields tighter generalization bounds than vector-based formulations. Experiments across multiple EV arrival patterns confirm that gamified learning enhances load balancing. In particular, the image-to-movement model trained with DAgger consistently outperforms heuristic baselines, vector-based approaches, and supervised learning agents, while also demonstrating robustness in sensitivity analyses. These operational gains translate into tangible economic value. In a real-world case study for the Greater Montr\u00e9al Area (Qu\u00e9bec, Canada) using utility cost data, the proposed methods lower system costs by tens of millions of dollars per year over the prevailing practice and show clear potential to delay costly grid upgrades.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5728\u7ebf\u96c6\u4e2d\u5145\u7535\u8c03\u5ea6\u95ee\u9898(OCCSP)\uff0c\u901a\u8fc7\u6e38\u620f\u5316\u5efa\u6a21\uff0c\u4f7f\u7528DAgger\u8bad\u7ec3\u5b66\u4e60\u4ee3\u7406\uff0c\u5728\u8499\u7279\u5229\u5c14\u5b9e\u9645\u6848\u4f8b\u4e2d\u6bcf\u5e74\u53ef\u8282\u7701\u6570\u5343\u4e07\u7f8e\u5143\u6210\u672c", "motivation": "\u89e3\u51b3\u7535\u52a8\u6c7d\u8f66\u52a8\u6001\u5230\u8fbe\u65f6\u7684\u5b9e\u65f6\u5145\u7535\u8c03\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u5728\u5bb9\u91cf\u9650\u5236\u4e0b\u5e73\u8861\u8d1f\u8f7d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3", "method": "1) \u5c06\u95ee\u9898\u6e38\u620f\u5316\u5efa\u6a21\u4e3a\u5728\u65f6\u95f4\u548c\u5bb9\u91cf\u7ea6\u675f\u7f51\u683c\u4e0a\u653e\u7f6e\u5145\u7535\u5757\uff1b2) \u8bbe\u8ba1\u542f\u53d1\u5f0f\u7b56\u7565\uff1b3) \u4f7f\u7528\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u5b66\u4e60\u4ee3\u7406\uff1b4) \u91c7\u7528\u6570\u636e\u96c6\u805a\u5408(DAgger)\u8fdb\u884c\u6539\u8fdb", "result": "\u6e38\u620f\u5316\u65b9\u6cd5\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u83b7\u5f97\u4e86\u6bd4\u5411\u91cf\u5316\u65b9\u6cd5\u66f4\u7d27\u7684\u6cdb\u5316\u8fb9\u754c\u3002\u4f7f\u7528DAgger\u8bad\u7ec3\u7684image-to-movement\u6a21\u578b\u5728\u591a\u79cdEV\u5230\u8fbe\u6a21\u5f0f\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u542f\u53d1\u5f0f\u57fa\u7ebf\u3001\u5411\u91cf\u5316\u65b9\u6cd5\u548c\u76d1\u7763\u5b66\u4e60\u4ee3\u7406\uff0c\u5728\u8499\u7279\u5229\u5c14\u6848\u4f8b\u4e2d\u6bcf\u5e74\u53ef\u964d\u4f4e\u6570\u5343\u4e07\u7f8e\u5143\u7cfb\u7edf\u6210\u672c", "conclusion": "\u6e38\u620f\u5316\u5b66\u4e60\u80fd\u6709\u6548\u589e\u5f3a\u8d1f\u8f7d\u5e73\u8861\uff0c\u5177\u6709\u663e\u8457\u7684\u7ecf\u6d4e\u4ef7\u503c\uff0c\u80fd\u591f\u63a8\u8fdf\u6602\u8d35\u7684\u7535\u7f51\u5347\u7ea7\u9700\u6c42\uff0c\u4e3a\u89e3\u51b3OCCSP\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5"}}
{"id": "2601.12253", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12253", "abs": "https://arxiv.org/abs/2601.12253", "authors": ["Haoran Xu", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo"], "title": "Federated Joint Learning for Domain and Class Generalization", "comment": "ICASSP 2026", "summary": "Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \\textbf{Fed}erated Joint Learning for \\textbf{D}omain and \\textbf{C}lass \\textbf{G}eneralization, termed \\textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \\textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.", "AI": {"tldr": "FedDCG\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u7c7b\u522b\u548c\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u9886\u57df\u5206\u7ec4\u7b56\u7565\u548c\u53ef\u5b66\u4e60\u7f51\u7edc\u63d0\u5347\u672a\u89c1\u7c7b\u522b\u548c\u672a\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u7406\u672a\u89c1\u7c7b\u522b\u6216\u672a\u89c1\u9886\u57df\u95ee\u9898\uff0c\u7f3a\u4e4f\u540c\u65f6\u8003\u8651\u4e24\u8005\u7684\u8054\u5408\u6846\u67b6\u3002\u5728\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u7c7b\u522b\u548c\u9886\u57df\u6cdb\u5316\u6311\u6218\u3002", "method": "\u63d0\u51faFedDCG\u65b9\u6cd5\uff1a1\uff09\u9886\u57df\u5206\u7ec4\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u7ec4\u5185\u8bad\u7ec3\u7c7b\u522b\u6cdb\u5316\u7f51\u7edc\u4ee5\u907f\u514d\u51b3\u7b56\u8fb9\u754c\u6df7\u6dc6\uff1b2\uff09\u63a8\u7406\u65f6\u57fa\u4e8e\u9886\u57df\u76f8\u4f3c\u6027\u805a\u5408\u7c7b\u522b\u6cdb\u5316\u7ed3\u679c\uff1b3\uff09\u4f7f\u7528\u53ef\u5b66\u4e60\u7f51\u7edc\u589e\u5f3a\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\uff1b4\uff09\u89e3\u8026\u673a\u5236\u5206\u79bb\u901a\u7528\u77e5\u8bc6\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedDCG\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedDCG\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u540c\u65f6\u5904\u7406\u7c7b\u522b\u548c\u9886\u57df\u6cdb\u5316\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.12624", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12624", "abs": "https://arxiv.org/abs/2601.12624", "authors": ["Shiqi Wang", "Mahdi Khosravy", "Neeraj Gupta", "Olaf Witkowski"], "title": "Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach", "comment": null, "summary": "Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6d6e\u70b9\u7f16\u7801\u3001\u60e9\u7f5a\u9a71\u52a8\u7684\u5355\u76ee\u6807\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff0c\u5728\u964d\u4f4e\u53ef\u89c1\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387", "motivation": "\u901a\u7528\u5bf9\u6297\u6270\u52a8\u80fd\u591f\u7528\u5355\u4e00\u566a\u58f0\u6a21\u5f0f\u653b\u51fb\u591a\u4e2a\u8f93\u5165\uff0c\u8fdb\u5316\u7b97\u6cd5\u56e0\u5176\u80fd\u591f\u5904\u7406\u975e\u51f8\u3001\u65e0\u68af\u5ea6\u4f18\u5316\u95ee\u9898\u800c\u6210\u4e3a\u6709\u524d\u666f\u7684\u751f\u6210\u65b9\u6cd5", "method": "\u91c7\u7528\u6d6e\u70b9\u7f16\u7801\u7684\u57fa\u56e0\u8868\u793a\u4ee5\u9002\u5e94\u6df1\u5ea6\u5b66\u4e60\u89c4\u6a21\uff0c\u7ed3\u5408\u52a8\u6001\u8fdb\u5316\u7b97\u5b50\u4e0e\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u4f7f\u7528\u6a21\u5757\u5316PyTorch\u5b9e\u73b0\uff0c\u901a\u8fc7\u5468\u671f\u6027\u5207\u6362\u6279\u6b21\u9632\u6b62\u8fc7\u62df\u5408", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u8fdb\u5316\u65b9\u6cd5\u80fd\u751f\u6210\u8303\u6570\u66f4\u5c0f\u3001\u8bef\u5206\u7c7b\u6548\u679c\u66f4\u597d\u3001\u6536\u655b\u66f4\u5feb\u7684\u6270\u52a8", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u751f\u6210\u901a\u7528\u5bf9\u6297\u6270\u52a8\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784"}}
{"id": "2601.13735", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13735", "abs": "https://arxiv.org/abs/2601.13735", "authors": ["Hojin Kim", "Jaehyung Kim"], "title": "Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection", "comment": "15 pages, 4 figures", "summary": "Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u57fa\u4e8e\u6982\u7387\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\u65e0\u6cd5\u6709\u6548\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3b\u8981\u53cd\u6620\u8868\u9762\u6d41\u7545\u5ea6\u800c\u975e\u903b\u8f91\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5bf9\u6bd4\u56e0\u679c\u5ea6\u91cf\u65b9\u6cd5", "motivation": "\u6311\u6218\u5f53\u524d\u666e\u904d\u5047\u8bbe\u2014\u2014\u5373\u6982\u7387\u7f6e\u4fe1\u5ea6\u6307\u6807\u80fd\u53cd\u6620\u63a8\u7406\u8d28\u91cf\uff0c\u8d28\u7591\u8fd9\u4e9b\u6307\u6807\u662f\u5426\u771f\u6b63\u6355\u6349\u5230\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb", "method": "\u5f15\u5165\u4e09\u7c7b\u6b65\u95f4\u56e0\u679c\u6270\u52a8\uff0c\u7cfb\u7edf\u6027\u5730\u7834\u574f\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4f46\u4fdd\u6301\u5c40\u90e8\u6d41\u7545\u5ea6\uff1b\u63d0\u51fa\u5bf9\u6bd4\u56e0\u679c\u5ea6\u91cf\u65b9\u6cd5\uff0c\u663e\u5f0f\u9694\u79bb\u6b65\u95f4\u56e0\u679c\u4f9d\u8d56", "result": "\u5373\u4f7f\u4e25\u91cd\u5e72\u9884\uff08\u5982\u786c\u6ce8\u610f\u529b\u63a9\u7801\u963b\u6b62\u6a21\u578b\u5173\u6ce8\u5148\u524d\u63a8\u7406\u6b65\u9aa4\uff09\uff0c\u9009\u62e9\u51c6\u786e\u7387\u4ec5\u8f7b\u5fae\u4e0b\u964d\uff1b\u73b0\u6709\u6982\u7387\u6307\u6807\u5bf9\u903b\u8f91\u7ed3\u6784\u4e0d\u654f\u611f\uff0c\u4e3b\u8981\u6355\u6349\u8868\u9762\u6d41\u7545\u5ea6\u6216\u5206\u5e03\u5148\u9a8c", "conclusion": "\u73b0\u6709\u6982\u7387\u7f6e\u4fe1\u5ea6\u6307\u6807\u4e0d\u80fd\u6709\u6548\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\uff0c\u63d0\u51fa\u7684\u5bf9\u6bd4\u56e0\u679c\u5ea6\u91cf\u65b9\u6cd5\u5728\u8f93\u51fa\u9009\u62e9\u4e2d\u6bd4\u73b0\u6709\u6982\u7387\u65b9\u6cd5\u66f4\u5fe0\u5b9e"}}
{"id": "2601.13251", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13251", "abs": "https://arxiv.org/abs/2601.13251", "authors": ["Ebubekir Tosun", "Mehmet Emin Buldur", "\u00d6zay Ezerceli", "Mahmoud ElHussieni"], "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph", "comment": null, "summary": "Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.", "AI": {"tldr": "\u5f00\u53d1\u5927\u89c4\u6a21\u8bed\u4e49\u805a\u7c7b\u7cfb\u7edf\uff0c\u89e3\u51b3\u795e\u7ecf\u5d4c\u5165\u65e0\u6cd5\u533a\u5206\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u8def\u8bed\u4e49\u5173\u7cfb\u5224\u522b\u5668\u548c\u8f6f\u5230\u786c\u805a\u7c7b\u7b97\u6cd5\u751f\u6210290\u4e07\u4e2a\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u7c07\u3002", "motivation": "\u795e\u7ecf\u5d4c\u5165\u5b58\u5728\u4e00\u4e2a\u663e\u8457\u7f3a\u9677\uff1a\u65e0\u6cd5\u53ef\u9760\u5730\u533a\u5206\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\uff0c\u5bfc\u81f4\u63d0\u9ad8\u76f8\u4f3c\u5ea6\u9608\u503c\u4ecd\u65e0\u6cd5\u9632\u6b62\u53cd\u4e49\u8bcd\u88ab\u9519\u8bef\u5206\u7ec4\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5f62\u6001\u4e30\u5bcc\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5c24\u5176\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u8bed\u4e49\u5173\u7cfb\u6b67\u4e49\u7684\u7cfb\u7edf\u3002", "method": "1) \u6784\u5efa84.3\u4e07\u4e2a\u6982\u5ff5\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u540c\u4e49\u3001\u53cd\u4e49\u548c\u5171\u4e0b\u4f4d\u5173\u7cfb\uff0c\u4f7f\u7528Gemini 2.5-Flash LLM\u589e\u5f3a\u5e76\u7528\u4eba\u7f16\u8bcd\u5178\u9a8c\u8bc1\uff1b2) \u63d0\u51fa\u4e09\u8def\u8bed\u4e49\u5173\u7cfb\u5224\u522b\u5668\uff0c\u5b9e\u73b090%\u5b8fF1\u5206\u6570\uff1b3) \u5f00\u53d1\u65b0\u9896\u7684\u8f6f\u5230\u786c\u805a\u7c7b\u7b97\u6cd5\uff0c\u91c7\u7528\u62d3\u6251\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u6269\u5c55-\u526a\u679d\u8fc7\u7a0b\uff0c\u9632\u6b62\u8bed\u4e49\u6f02\u79fb\u5e76\u89e3\u51b3\u591a\u4e49\u6027\u3002", "result": "\u7cfb\u7edf\u5904\u74061500\u4e07\u4e2a\u8bcd\u6c47\u9879\uff0c\u8bc4\u4f305.2\u4ebf\u4e2a\u6f5c\u5728\u5173\u7cfb\uff0c\u6700\u7ec8\u751f\u6210290\u4e07\u4e2a\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u7c07\u3002\u5224\u522b\u5668\u8fbe\u523090%\u5b8fF1\u5206\u6570\uff0c\u805a\u7c7b\u7b97\u6cd5\u80fd\u9632\u6b62\u9519\u8bef\u4f20\u9012\u94fe\uff08\u5982hot->spicy->pain->depression\uff09\uff0c\u786e\u4fdd\u6bcf\u4e2a\u672f\u8bed\u88ab\u5206\u914d\u5230\u552f\u4e00\u8bed\u4e49\u8fde\u8d2f\u7684\u7c07\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u795e\u7ecf\u5d4c\u5165\u7684\u540c\u4e49\u8bcd-\u53cd\u4e49\u8bcd\u533a\u5206\u95ee\u9898\uff0c\u4e3a\u5f62\u6001\u4e30\u5bcc\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u641c\u7d22\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8d44\u6e90\uff0c\u586b\u8865\u4e86\u73b0\u6709\u540c\u4e49\u8bcd\u6570\u636e\u5e93\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.13317", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.13317", "abs": "https://arxiv.org/abs/2601.13317", "authors": ["Samantha Sudhoff", "Pranav Perumal", "Zhaoqing Wu", "Tunazzina Islam"], "title": "Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse", "comment": null, "summary": "Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u53d1\u73b0\u6846\u67b6\uff0c\u6bd4\u8f83Meta\u4ed8\u8d39\u5e7f\u544a\u548cBluesky\u516c\u5171\u5e16\u5b50\u4e2d\u7684\u6c14\u5019\u8bdd\u8bed\u5dee\u5f02\uff0c\u53d1\u73b0\u5e73\u53f0\u6fc0\u52b1\u673a\u5236\u5f71\u54cd\u53d9\u4e8b\u7ed3\u6784\u3001\u7acb\u573a\u5bf9\u9f50\u548c\u65f6\u95f4\u54cd\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5b64\u7acb\u5206\u6790\u4e0d\u540c\u5e73\u53f0\u7684\u6c14\u5019\u8bdd\u8bed\uff0c\u96be\u4ee5\u533a\u5206\u673a\u6784\u4fe1\u606f\u4e0e\u516c\u4f17\u8868\u8fbe\u3002\u4ed8\u8d39\u5e7f\u544a\u751f\u6001\u7cfb\u7edf\u6fc0\u52b1\u6709\u9488\u5bf9\u6027\u7684\u6218\u7565\u8bf4\u670d\uff0c\u800c\u516c\u5171\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e3b\u8981\u627f\u8f7d\u7528\u6237\u9a71\u52a8\u7684\u6709\u673a\u8ba8\u8bba\uff0c\u9700\u8981\u6bd4\u8f83\u5206\u6790\u8fd9\u4e24\u79cd\u7ed3\u6784\u4e0d\u540c\u7684\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u7aef\u5230\u7aef\u4e3b\u9898\u53d1\u73b0\u548c\u5206\u914d\u6846\u67b6\uff1a\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u805a\u7c7b\u6587\u672c\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b80\u6d01\u3001\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u6807\u7b7e\u3002\u57282024\u5e747\u6708\u81f32025\u5e749\u6708\u671f\u95f4\uff0c\u6bd4\u8f83Meta\u4ed8\u8d39\u5e7f\u544a\u548cBluesky\u516c\u5171\u5e16\u5b50\u4e2d\u7684\u6c14\u5019\u8bdd\u8bed\u3002\u901a\u8fc7\u4eba\u5de5\u5224\u65ad\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5668\u8bc4\u4f30\u4e3b\u9898\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u4e0b\u6e38\u7acb\u573a\u9884\u6d4b\u548c\u4e3b\u9898\u5f15\u5bfc\u68c0\u7d22\u4efb\u52a1\u9a8c\u8bc1\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "result": "\u5e73\u53f0\u5c42\u9762\u7684\u6fc0\u52b1\u673a\u5236\u53cd\u6620\u5728\u6c14\u5019\u53d9\u4e8b\u7684\u4e3b\u9898\u7ed3\u6784\u3001\u7acb\u573a\u5bf9\u9f50\u548c\u65f6\u95f4\u54cd\u5e94\u6027\u4e2d\u3002\u4ed8\u8d39\u6c14\u5019\u4fe1\u606f\u4e0e\u516c\u5171\u6c14\u5019\u8bdd\u8bed\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u4e3b\u9898\u6d41\u884c\u5ea6\u5728\u91cd\u5927\u653f\u6cbb\u4e8b\u4ef6\u5468\u56f4\u53d1\u751f\u53d8\u5316\u3002\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4e3b\u9898\u8d28\u91cf\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u4f20\u7edf\u4e3b\u9898\u5efa\u6a21\u57fa\u7ebf\u3002", "conclusion": "\u5e73\u53f0\u6fc0\u52b1\u673a\u5236\u663e\u8457\u5f71\u54cd\u6c14\u5019\u8bdd\u8bed\u7684\u7279\u5f81\u3002\u867d\u7136\u5b9e\u8bc1\u5206\u6790\u805a\u7126\u6c14\u5019\u4f20\u64ad\uff0c\u4f46\u63d0\u51fa\u7684\u6846\u67b6\u652f\u6301\u8de8\u5f02\u6784\u4f20\u64ad\u73af\u5883\u7684\u6bd4\u8f83\u53d9\u4e8b\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u533a\u5206\u673a\u6784\u4fe1\u606f\u4e0e\u516c\u4f17\u8868\u8fbe\u3002"}}
{"id": "2601.13368", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13368", "abs": "https://arxiv.org/abs/2601.13368", "authors": ["Zhenjiang Mao", "Anirudhh Venkat"], "title": "Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models", "comment": null, "summary": "As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6b65\u9aa4\u95f4\u6ce8\u610f\u529b\u5206\u6790\u548c\u9690\u85cf\u7f6e\u4fe1\u5ea6\u673a\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u6570\u5b66\u548c\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7b54\u6848\u4e0d\u786e\u5b9a\u6027\u7684\u51c6\u786e\u8bc4\u4f30\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7f6e\u4fe1\u5ea6\u7684\u65f6\u95f4\u5206\u5e03\uff0c\u5373\u4f7f\u65e9\u671f\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\u5f88\u4f4e\uff0c\u6574\u4f53\u7f6e\u4fe1\u5ea6\u4e5f\u53ef\u80fd\u88ab\u9ad8\u4f30\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u6b65\u9aa4\u95f4\u6ce8\u610f\u529b\u5206\u6790\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u5e76\u5f15\u5165\u9690\u85cf\u7f6e\u4fe1\u5ea6\u673a\u5236\u6765\u4fdd\u7559\u5386\u53f2\u7f6e\u4fe1\u4fe1\u606f\uff0c\u5c06\u9010\u6b65\u7f6e\u4fe1\u5ea6\u4e0e\u5386\u53f2\u4fe1\u606f\u7ed3\u5408\u5f97\u5230\u66f4\u51c6\u786e\u7684\u6574\u4f53\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728GAOKAO\u6570\u5b66\u57fa\u51c6\u548cCLadder\u56e0\u679c\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e3b\u6d41\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u8d1f\u5bf9\u6570\u4f3c\u7136\u548c\u671f\u671b\u6821\u51c6\u8bef\u5dee\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9884\u6d4b\u8d28\u91cf\u4e0e\u6821\u51c6\u7684\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u8003\u8651\u7f6e\u4fe1\u5ea6\u7684\u65f6\u95f4\u5206\u5e03\u548c\u6b65\u9aa4\u95f4\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6574\u4f53\u7f6e\u4fe1\u5ea6\u9ad8\u4f30\u95ee\u9898\u3002"}}
{"id": "2601.12672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12672", "abs": "https://arxiv.org/abs/2601.12672", "authors": ["Qimao Chen", "Fang Li", "Shaoqing Xu", "Zhiyi Lai", "Zixun Xie", "Yuechen Luo", "Shengyin Jiang", "Hanbing Li", "Long Chen", "Bing Wang", "Yi Zhang", "Zhi-Xin Yang"], "title": "VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness", "comment": "Accepted to AAAI 2026", "summary": "The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories. This direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.", "AI": {"tldr": "VILTA\uff1a\u4e00\u79cd\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u81ea\u52a8\u9a7e\u9a76\u95ed\u73af\u8bad\u7ec3\u4e2d\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u7f16\u8f91\u5468\u56f4\u8f66\u8f86\u7684\u672a\u6765\u8f68\u8ff9\u6765\u751f\u6210\u591a\u6837\u5316\u6311\u6218\u6027\u573a\u666f\uff0c\u89e3\u51b3\u957f\u5c3e\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u90e8\u7f72\u9762\u4e34\u957f\u5c3e\u95ee\u9898\uff0c\u5373\u7f55\u89c1\u4f46\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u4e25\u91cd\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u3001\u91cd\u91c7\u6837\u3001\u751f\u6210\u6a21\u578b\uff09\u751f\u6210\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u6709\u9650\uff0c\u800c\u4e24\u9636\u6bb5VLM\u65b9\u6cd5\u53d7\u4e0b\u6e38\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u9650\u5236\u3002", "method": "\u63d0\u51faVILTA\u6846\u67b6\uff0c\u5c06VLM\u96c6\u6210\u5230\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\u7684\u95ed\u73af\u8bad\u7ec3\u4e2d\u3002VLM\u7406\u89e3\u52a8\u6001\u9a7e\u9a76\u73af\u5883\uff0c\u901a\u8fc7\u76f4\u63a5\u3001\u7ec6\u7c92\u5ea6\u7f16\u8f91\u5468\u56f4\u4ee3\u7406\u7684\u672a\u6765\u8f68\u8ff9\u6765\u6218\u7565\u6027\u5730\u751f\u6210\u6311\u6218\u6027\u573a\u666f\uff0c\u5145\u5206\u53d1\u6325VLM\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5173\u952e\u957f\u5c3e\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8303\u56f4\u3002", "conclusion": "VILTA\u901a\u8fc7\u5c06VLM\u76f4\u63a5\u96c6\u6210\u5230\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u5229\u7528\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u521b\u5efa\u591a\u6837\u5316\u3001\u5408\u7406\u7684\u6311\u6218\u6027\u573a\u666f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u957f\u5c3e\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u6574\u4f53\u5b89\u5168\u6027\u3002"}}
{"id": "2601.13284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13284", "abs": "https://arxiv.org/abs/2601.13284", "authors": ["Duygu Nur Yaldiz", "Evangelia Spiliopoulou", "Zheng Qi", "Siddharth Varia", "Srikanth Doss", "Nikolaos Pappas"], "title": "Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.", "AI": {"tldr": "RLVR\u5fae\u8c03\u5bfc\u81f4LLM\u8fc7\u5ea6\u81ea\u4fe1\uff0cSFT\u6821\u51c6\u66f4\u597d\u4f46\u6027\u80fd\u63d0\u5347\u5c0f\uff1b\u63d0\u51fa\u6821\u51c6\u611f\u77e5RL\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4eECE\u5206\u6570\u8fbe9\u70b9", "motivation": "LLM\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u4ee5\u4fbf\u4e0b\u6e38\u7cfb\u7edf\u51b3\u5b9a\u4f55\u65f6\u4fe1\u4efb\u6a21\u578b\u6216\u4f7f\u7528\u5907\u7528\u673a\u5236\u3002\u7814\u7a76\u4e24\u79cd\u4e3b\u6d41\u5fae\u8c03\u8303\u5f0f\uff08SFT\u548cRLVR\uff09\u7684\u6821\u51c6\u7279\u6027\u5dee\u5f02\u3002", "method": "\u7cfb\u7edf\u7814\u7a76SFT\u548cRLVR\u7684\u6821\u51c6\u7279\u6027\uff1b\u8bca\u65adRLVR\u5931\u8d25\u539f\u56e0\uff08\u51b3\u7b56token\u4f5c\u4e3a\u63d0\u53d6\u6b65\u9aa4\u4e0d\u643a\u5e26\u7f6e\u4fe1\u4fe1\u606f\uff09\uff1b\u63d0\u51fa\u6821\u51c6\u611f\u77e5RL\u65b9\u6cd5\uff0c\u76f4\u63a5\u8c03\u6574\u51b3\u7b56token\u6982\u7387\u3002", "result": "RLVR\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u4f46\u4ea7\u751f\u6781\u5ea6\u8fc7\u5ea6\u81ea\u4fe1\u6a21\u578b\uff1bSFT\u6821\u51c6\u66f4\u597d\uff08\u5373\u4f7f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\uff09\u4f46\u6027\u80fd\u63d0\u5347\u8f83\u5c0f\uff1b\u63d0\u51fa\u7684\u65b9\u6cd5\u4fdd\u6301RLVR\u51c6\u786e\u5ea6\u6c34\u5e73\uff0c\u540c\u65f6\u51cf\u5c11ECE\u5206\u6570\u8fbe9\u70b9\u3002", "conclusion": "RLVR\u5fae\u8c03\u5bfc\u81f4\u6821\u51c6\u95ee\u9898\uff0c\u51b3\u7b56token\u673a\u5236\u662f\u6839\u672c\u539f\u56e0\uff1b\u63d0\u51fa\u7684\u6821\u51c6\u611f\u77e5RL\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4e3aLLM\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2601.13503", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13503", "abs": "https://arxiv.org/abs/2601.13503", "authors": ["Kyung Ho Lim", "Byung-Hoon Kim"], "title": "Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives", "comment": null, "summary": "Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.", "AI": {"tldr": "Anonpsy\uff1a\u4e00\u79cd\u5c06\u7cbe\u795e\u75c5\u5b66\u53d9\u4e8b\u8f6c\u5316\u4e3a\u8bed\u4e49\u56fe\uff0c\u901a\u8fc7\u56fe\u7ea6\u675f\u6270\u52a8\u4fee\u6539\u8bc6\u522b\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u4e34\u5e8a\u7ed3\u6784\uff0c\u518d\u751f\u6210\u6587\u672c\u7684\u53bb\u8bc6\u522b\u6846\u67b6", "motivation": "\u7cbe\u795e\u75c5\u5b66\u53d9\u4e8b\u4e0d\u4ec5\u5305\u542b\u660e\u786e\u7684\u60a3\u8005\u6807\u8bc6\u7b26\uff0c\u8fd8\u5305\u542b\u5d4c\u5165\u4e34\u5e8a\u7ed3\u6784\u4e2d\u7684\u72ec\u7279\u751f\u6d3b\u4e8b\u4ef6\u3002\u73b0\u6709\u7684\u53bb\u8bc6\u522b\u65b9\u6cd5\uff08\u5982PHI\u63a9\u7801\u548c\u57fa\u4e8eLLM\u7684\u5408\u6210\u91cd\u5199\uff09\u5728\u6587\u672c\u5c42\u9762\u64cd\u4f5c\uff0c\u5bf9\u4fdd\u7559\u6216\u6539\u53d8\u54ea\u4e9b\u8bed\u4e49\u5143\u7d20\u63a7\u5236\u6709\u9650\u3002", "method": "1) \u5c06\u6bcf\u4e2a\u53d9\u4e8b\u8f6c\u6362\u4e3a\u7f16\u7801\u4e34\u5e8a\u5b9e\u4f53\u3001\u65f6\u95f4\u951a\u70b9\u548c\u7c7b\u578b\u5316\u5173\u7cfb\u7684\u8bed\u4e49\u56fe\uff1b2) \u5e94\u7528\u56fe\u7ea6\u675f\u6270\u52a8\uff0c\u4fee\u6539\u8bc6\u522b\u4e0a\u4e0b\u6587\u540c\u65f6\u4fdd\u7559\u4e34\u5e8a\u5fc5\u9700\u7ed3\u6784\uff1b3) \u901a\u8fc7\u56fe\u6761\u4ef6LLM\u751f\u6210\u91cd\u65b0\u751f\u6210\u6587\u672c\u3002", "result": "\u572890\u4e2a\u4e34\u5e8a\u533b\u751f\u64b0\u5199\u7684\u7cbe\u795e\u75c5\u5b66\u6848\u4f8b\u53d9\u4e8b\u4e0a\u8bc4\u4f30\uff0cAnonpsy\u5728\u4fdd\u6301\u8bca\u65ad\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u5728\u4e13\u5bb6\u3001\u8bed\u4e49\u548cGPT-5\u8bc4\u4f30\u4e0b\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u4f4e\u91cd\u8bc6\u522b\u98ce\u9669\u3002\u4e0e\u5f3a\u5927\u7684\u4ec5LLM\u91cd\u5199\u57fa\u7ebf\u76f8\u6bd4\uff0cAnonpsy\u4ea7\u751f\u663e\u8457\u66f4\u4f4e\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u53ef\u8bc6\u522b\u6027\u3002", "conclusion": "\u663e\u5f0f\u7ed3\u6784\u8868\u793a\u4e0e\u7ea6\u675f\u751f\u6210\u76f8\u7ed3\u5408\uff0c\u4e3a\u7cbe\u795e\u75c5\u5b66\u53d9\u4e8b\u7684\u53bb\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2601.13422", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13422", "abs": "https://arxiv.org/abs/2601.13422", "authors": ["Dahai Yu", "Rongchao Xu", "Dingyi Zhuang", "Yuheng Bu", "Shenhao Wang", "Guang Wang"], "title": "TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction", "comment": null, "summary": "Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.", "AI": {"tldr": "TrustEnergy\uff1a\u4e00\u4e2a\u7528\u4e8e\u51c6\u786e\u53ef\u9760\u7528\u6237\u7ea7\u80fd\u6e90\u4f7f\u7528\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u65f6\u7a7a\u8868\u793a\u548c\u987a\u5e8f\u4fdd\u5f62\u5206\u4f4d\u6570\u56de\u5f52\u5b9e\u73b05.4%\u7684\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u548c5.7%\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u80fd\u6e90\u4f7f\u7528\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5927\u591a\u5ffd\u89c6\u5bb6\u5ead\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u6216\u65e0\u6cd5\u6269\u5c55\u5230\u4e2a\u4f53\u5316\u9884\u6d4b\uff1b2\uff09\u7531\u4e8e\u6781\u7aef\u5929\u6c14\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u80fd\u6e90\u4f7f\u7528\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u5de5\u4f5c\u672a\u5145\u5206\u63a2\u7d22\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u9700\u8981\u51c6\u786e\u53ef\u9760\u7684\u7528\u6237\u7ea7\u9884\u6d4b\u6765\u652f\u6301\u7535\u7f51\u7ba1\u7406\u3001\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u707e\u5bb3\u54cd\u5e94\u7b49\u5e94\u7528\u3002", "method": "TrustEnergy\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\u7ec4\u4ef6\uff1a1\uff09\u5206\u5c42\u65f6\u7a7a\u8868\u793a\u6a21\u5757\uff0c\u4f7f\u7528\u65b0\u578b\u8bb0\u5fc6\u589e\u5f3a\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u9ad8\u6548\u6355\u83b7\u5b8f\u89c2\u548c\u5fae\u89c2\u80fd\u6e90\u4f7f\u7528\u6a21\u5f0f\uff1b2\uff09\u987a\u5e8f\u4fdd\u5f62\u5206\u4f4d\u6570\u56de\u5f52\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u786e\u4fdd\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u6709\u6548\u9884\u6d4b\u533a\u95f4\uff0c\u65e0\u9700\u5bf9\u5e95\u5c42\u6570\u636e\u5206\u5e03\u505a\u5f3a\u5047\u8bbe\u3002", "result": "\u901a\u8fc7\u4e0e\u4f5b\u7f57\u91cc\u8fbe\u5dde\u7535\u529b\u4f9b\u5e94\u5546\u5408\u4f5c\u5b9e\u65bd\u548c\u8bc4\u4f30\uff0cTrustEnergy\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e865.4%\u7684\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u548c5.7%\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6539\u8fdb\u3002", "conclusion": "TrustEnergy\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u51c6\u786e\u53ef\u9760\u5730\u8fdb\u884c\u7528\u6237\u7ea7\u80fd\u6e90\u4f7f\u7528\u9884\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u76f8\u5173\u6027\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u7535\u7f51\u7ba1\u7406\u3001\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u707e\u5bb3\u54cd\u5e94\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u652f\u6301\u3002"}}
{"id": "2601.12765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12765", "abs": "https://arxiv.org/abs/2601.12765", "authors": ["Zhi Cai", "Yingjie Gao", "Yanan Zhang", "Xinzhu Ma", "Di Huang"], "title": "Towards Unbiased Source-Free Object Detection via Vision Foundation Models", "comment": null, "summary": "Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.", "AI": {"tldr": "\u63d0\u51faDSOD\u6846\u67b6\u89e3\u51b3\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6e90\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7VFM\u8f85\u52a9\u7279\u5f81\u6ce8\u5165\u548c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u6b63\u5219\u5316\uff0c\u5728\u591a\u4e2a\u8de8\u57df\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u6e90\u504f\u5dee\u95ee\u9898\uff0c\u5373\u9002\u5e94\u540e\u7684\u6a21\u578b\u4ecd\u7136\u504f\u5411\u6e90\u57df\u7279\u5f81\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u81ea\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef", "method": "\u63d0\u51faDSOD\u6846\u67b6\uff1a1) \u7edf\u4e00\u7279\u5f81\u6ce8\u5165\u6a21\u5757\uff0c\u901a\u8fc7\u7b80\u5355\u5c3a\u5ea6\u6269\u5c55\u548c\u57df\u611f\u77e5\u81ea\u9002\u5e94\u52a0\u6743\u5c06VFM\u7279\u5f81\u96c6\u6210\u5230CNN\u9aa8\u5e72\u7f51\u7edc\uff1b2) \u8bed\u4e49\u611f\u77e5\u7279\u5f81\u6b63\u5219\u5316\uff0c\u7ea6\u675f\u7279\u5f81\u5b66\u4e60\u9632\u6b62\u8fc7\u62df\u5408\u6e90\u57df\u7279\u5f81\uff1b3) \u9488\u5bf9\u8ba1\u7b97\u53d7\u9650\u573a\u666f\u63d0\u51faDSOD-distill\u53d8\u4f53\uff0c\u91c7\u7528\u53cc\u6559\u5e08\u84b8\u998f\u65b9\u6848", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff1a\u6b63\u5e38\u5230\u96fe\u5929\u5929\u6c14\u9002\u5e94\u8fbe\u523048.1% AP\uff0c\u8de8\u573a\u666f\u9002\u5e94\u8fbe\u523039.3% AP\uff0c\u5408\u6210\u5230\u771f\u5b9e\u9002\u5e94\u8fbe\u523061.4% AP", "conclusion": "DSOD\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6e90\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7VFM\u8f85\u52a9\u7684\u7279\u5f81\u589e\u5f3a\u548c\u6b63\u5219\u5316\u673a\u5236\u663e\u8457\u63d0\u5347\u8de8\u57df\u9002\u5e94\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u8ba1\u7b97\u53cb\u597d\u7684\u84b8\u998f\u53d8\u4f53"}}
{"id": "2601.13456", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13456", "abs": "https://arxiv.org/abs/2601.13456", "authors": ["Sahasra Kokkula", "Daniel David", "Aaditya Baruah"], "title": "Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay", "comment": "8 pages, 5 figures. Course project for Neural Networks & Deep Learning COMSW4776 course at Columbia University", "summary": "Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u5728\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u6807\u51c6FedAvg\u5728Fashion-MNIST\u7684\u5b63\u8282\u6027\u6f02\u79fb\u4e2d\u51c6\u786e\u7387\u4ece74%\u964d\u81f328%\u3002\u63d0\u51fa\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7ef4\u62a4\u5c11\u91cf\u5386\u53f2\u6837\u672c\u7f13\u51b2\u533a\uff0c\u65e0\u9700\u4fee\u6539\u670d\u52a1\u5668\u805a\u5408\uff0c\u4ec5\u7528\u6bcf\u7c7b50\u4e2a\u6837\u672c\u5373\u53ef\u5c06\u51c6\u786e\u7387\u6062\u590d\u81f378-82%\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\u7684\u6311\u6218\uff0c\u5373\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u3002\u6807\u51c6FedAvg\u65b9\u6cd5\u5728\u8fd9\u79cd\u573a\u666f\u4e0b\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7ef4\u62a4\u4e00\u4e2a\u5c0f\u578b\u5386\u53f2\u6837\u672c\u7f13\u51b2\u533a\uff0c\u5728\u672c\u5730\u8bad\u7ec3\u65f6\u5c06\u7f13\u51b2\u533a\u4e2d\u7684\u5386\u53f2\u6837\u672c\u4e0e\u5f53\u524d\u6570\u636e\u6df7\u5408\u4f7f\u7528\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u4fee\u6539\u670d\u52a1\u5668\u7aef\u7684\u805a\u5408\u7b97\u6cd5\uff0c\u4fdd\u6301\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6807\u51c6\u6846\u67b6\u3002", "result": "\u5728Fashion-MNIST\u7684\u5b63\u8282\u6027\u6f02\u79fb\u5b9e\u9a8c\u4e2d\uff0c\u6807\u51c6FedAvg\u51c6\u786e\u7387\u4ece74%\u964d\u81f328%\u3002\u4f7f\u7528\u6bcf\u7c7b50\u4e2a\u6837\u672c\u7684\u7f13\u51b2\u533a\u540e\uff0c\u51c6\u786e\u7387\u6062\u590d\u81f378-82%\uff0c\u6709\u6548\u9632\u6b62\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\u5b58\u5728\u660e\u663e\u7684\u5185\u5b58-\u51c6\u786e\u7387\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6297\u9057\u5fd8\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u5386\u53f2\u6837\u672c\u5373\u53ef\u663e\u8457\u7f13\u89e3\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u65e0\u9700\u6539\u53d8\u8054\u90a6\u5b66\u4e60\u7684\u57fa\u672c\u67b6\u6784\u3002"}}
{"id": "2601.13717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13717", "abs": "https://arxiv.org/abs/2601.13717", "authors": ["Zehan Li", "Yuxuan Wang", "Ali El Lahib", "Ying-Jieh Xia", "Xinyu Pi"], "title": "Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff", "comment": null, "summary": "Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably \"rewind\" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.", "AI": {"tldr": "\u6a21\u62df\u65e0\u77e5\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6291\u5236\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e0d\u80fd\u66ff\u4ee3\u771f\u5b9e\u65e0\u77e5\u72b6\u6001\u8fdb\u884c\u9884\u6d4b\u8bc4\u4f30", "motivation": "\u8bc4\u4f30LLM\u9884\u6d4b\u80fd\u529b\u9762\u4e34\u4e24\u96be\uff1a\u524d\u77bb\u6027\u8bc4\u4f30\u5ef6\u8fdf\u9ad8\uff0c\u56de\u987e\u6027\u9884\u6d4b\u9762\u4e34\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002\u6a21\u62df\u65e0\u77e5\u65b9\u6cd5\u88ab\u63d0\u51fa\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7477\u4e2a\u7ade\u8d5b\u7ea7\u95ee\u9898\u548c9\u4e2a\u6a21\u578b\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u6a21\u62df\u65e0\u77e5\u80fd\u5426\u8fd1\u4f3c\u771f\u5b9e\u65e0\u77e5\uff0c\u5206\u6790\u622a\u6b62\u6307\u4ee4\u6548\u679c\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u6291\u5236\u80fd\u529b\u3001\u63a8\u7406\u4f18\u5316\u6a21\u578b\u8868\u73b0\u3002", "result": "\u6a21\u62df\u65e0\u77e5\u7cfb\u7edf\u6027\u5931\u8d25\uff1a1) \u622a\u6b62\u6307\u4ee4\u9020\u621052%\u6027\u80fd\u5dee\u8ddd\uff1b2) \u601d\u7ef4\u94fe\u65e0\u6cd5\u6709\u6548\u6291\u5236\u5148\u9a8c\u77e5\u8bc6\uff1b3) \u63a8\u7406\u4f18\u5316\u6a21\u578b\u6a21\u62df\u65e0\u77e5\u4fdd\u771f\u5ea6\u66f4\u5dee\u3002\u63d0\u793a\u65e0\u6cd5\u53ef\u9760\"\u56de\u6eda\"\u6a21\u578b\u77e5\u8bc6\u3002", "conclusion": "\u57fa\u4e8e\u622a\u6b62\u524d\u4e8b\u4ef6\u7684\u56de\u987e\u6027\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u4e0d\u5e94\u4f7f\u7528\u6a21\u62df\u65e0\u77e5\u65b9\u6cd5\u8bc4\u4f30\u9884\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.13578", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13578", "abs": "https://arxiv.org/abs/2601.13578", "authors": ["Qian Feng", "JiaHang Tu", "Mintong Kang", "Hanbin Zhao", "Chao Zhang", "Hui Qian"], "title": "FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning", "comment": "This paper has been accepted by ICCV 2025. code: \\url{https://github.com/RAIAN08/FG-OrIU}", "summary": "Incremental unlearning (IU) is critical for pre-trained models to comply with sequential data deletion requests, yet existing methods primarily suppress parameters or confuse knowledge without explicit constraints on both feature and gradient level, resulting in \\textit{superficial forgetting} where residual information remains recoverable. This incomplete forgetting risks security breaches and disrupts retention balance, especially in IU scenarios. We propose FG-OrIU (\\textbf{F}eature-\\textbf{G}radient \\textbf{Or}thogonality for \\textbf{I}ncremental \\textbf{U}nlearning), the first framework unifying orthogonal constraints on both features and gradients level to achieve deep forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes feature spaces via Singular Value Decomposition (SVD), separating forgetting and remaining class features into distinct subspaces. It then enforces dual constraints: feature orthogonal projection on both forgetting and remaining classes, while gradient orthogonal projection prevents the reintroduction of forgotten knowledge and disruption to remaining classes during updates. Additionally, dynamic subspace adaptation merges newly forgetting subspaces and contracts remaining subspaces, ensuring a stable balance between removal and retention across sequential unlearning tasks. Extensive experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51faFG-OrIU\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u548c\u68af\u5ea6\u7684\u6b63\u4ea4\u7ea6\u675f\u5b9e\u73b0\u6df1\u5ea6\u9057\u5fd8\uff0c\u89e3\u51b3\u589e\u91cf\u9057\u5fd8\u4e2d\u7684\u8868\u9762\u9057\u5fd8\u95ee\u9898", "motivation": "\u73b0\u6709\u589e\u91cf\u9057\u5fd8\u65b9\u6cd5\u4e3b\u8981\u5728\u53c2\u6570\u5c42\u9762\u6291\u5236\u6216\u6df7\u6dc6\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u7279\u5f81\u548c\u68af\u5ea6\u5c42\u9762\u7684\u663e\u5f0f\u7ea6\u675f\uff0c\u5bfc\u81f4\"\u8868\u9762\u9057\u5fd8\"\u2014\u2014\u6b8b\u7559\u4fe1\u606f\u4ecd\u53ef\u6062\u590d\u3002\u8fd9\u79cd\u4e0d\u5b8c\u6574\u9057\u5fd8\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u7834\u574f\u4fdd\u7559\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u589e\u91cf\u9057\u5fd8\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faFG-OrIU\u6846\u67b6\uff1a1) \u4f7f\u7528SVD\u5206\u89e3\u7279\u5f81\u7a7a\u95f4\uff0c\u5c06\u9057\u5fd8\u7c7b\u548c\u4fdd\u7559\u7c7b\u7279\u5f81\u5206\u79bb\u5230\u4e0d\u540c\u5b50\u7a7a\u95f4\uff1b2) \u5b9e\u65bd\u53cc\u91cd\u6b63\u4ea4\u7ea6\u675f\uff1a\u7279\u5f81\u6b63\u4ea4\u6295\u5f71\u9632\u6b62\u7279\u5f81\u6df7\u5408\uff0c\u68af\u5ea6\u6b63\u4ea4\u6295\u5f71\u9632\u6b62\u66f4\u65b0\u65f6\u91cd\u65b0\u5f15\u5165\u9057\u5fd8\u77e5\u8bc6\uff1b3) \u52a8\u6001\u5b50\u7a7a\u95f4\u9002\u5e94\uff1a\u5408\u5e76\u65b0\u9057\u5fd8\u5b50\u7a7a\u95f4\u5e76\u6536\u7f29\u4fdd\u7559\u5b50\u7a7a\u95f4\uff0c\u5728\u8fde\u7eed\u9057\u5fd8\u4efb\u52a1\u4e2d\u4fdd\u6301\u7a33\u5b9a\u5e73\u8861\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u6df1\u5ea6\u9057\u5fd8\uff08\u9057\u5fd8\u6548\u679c\u4e0d\u53ef\u9006\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4fdd\u7559\u77e5\u8bc6\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "FG-OrIU\u662f\u9996\u4e2a\u5728\u7279\u5f81\u548c\u68af\u5ea6\u5c42\u9762\u7edf\u4e00\u6b63\u4ea4\u7ea6\u675f\u7684\u589e\u91cf\u9057\u5fd8\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8868\u9762\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u53ef\u9760\u7684\u6df1\u5ea6\u9057\u5fd8\uff0c\u5728\u8fde\u7eed\u9057\u5fd8\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u79fb\u9664\u4e0e\u4fdd\u7559\u7684\u5e73\u8861\u3002"}}
{"id": "2601.13836", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.13836", "abs": "https://arxiv.org/abs/2601.13836", "authors": ["Qian Chen", "Jinlan Fu", "Changsong Li", "See-Kiong Ng", "Xipeng Qiu"], "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs", "comment": "https://openmoss.github.io/FutureOmni", "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u97f3\u9891-\u89c6\u89c9\u672a\u6765\u9884\u6d4b\u80fd\u529b\u7684\u57fa\u51c6FutureOmni\uff0c\u5305\u542b919\u4e2a\u89c6\u9891\u548c1034\u4e2aQA\u5bf9\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86OFF\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6a21\u6001\u611f\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u80fd\u529b\u7684\u8bc4\u4f30\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u56de\u987e\u6027\u7406\u89e3\uff0c\u800c\u97f3\u9891-\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u672a\u6765\u9884\u6d4b\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1) \u901a\u8fc7LLM\u8f85\u52a9\u3001\u4eba\u673a\u534f\u540c\u7684\u6d41\u7a0b\u6784\u5efaFutureOmni\u57fa\u51c6\uff0c\u5305\u542b919\u4e2a\u89c6\u9891\u548c1034\u4e2a\u591a\u9879\u9009\u62e9\u9898\u5bf9\uff0c\u6db5\u76d68\u4e2a\u4e3b\u8981\u9886\u57df\uff1b2) \u63d0\u51faOmni-Modal Future Forecasting (OFF)\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e867K\u6837\u672c\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u4e8613\u4e2a\u591a\u6a21\u6001\u6a21\u578b\u548c7\u4e2a\u7eaf\u89c6\u9891\u6a21\u578b\uff0c\u53d1\u73b0\u5f53\u524d\u7cfb\u7edf\u5728\u97f3\u9891-\u89c6\u89c9\u672a\u6765\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u8bed\u97f3\u5bc6\u96c6\u573a\u666f\u4e2d\uff0c\u6700\u4f73\u51c6\u786e\u7387\u4ec5\u4e3a64.8%\uff08Gemini 3 Flash\uff09\u3002OFF\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u672a\u6765\u9884\u6d4b\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "FutureOmni\u586b\u8865\u4e86\u591a\u6a21\u6001\u672a\u6765\u9884\u6d4b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684OFF\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2601.13918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13918", "abs": "https://arxiv.org/abs/2601.13918", "authors": ["Yusheng Liao", "Chuan Xuan", "Yutong Cai", "Lina Yang", "Zhe Chen", "Yanfeng Wang", "Yu Wang"], "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization", "comment": "37 pages, 12 figures", "summary": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentEHR\u57fa\u51c6\u548cRetroSum\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u5728\u539f\u59cb\u9ad8\u566a\u58f0\u7535\u5b50\u75c5\u5386\u6570\u636e\u5e93\u4e2d\u6267\u884c\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u65f6\u7684\u4fe1\u606f\u4e22\u5931\u548c\u63a8\u7406\u8fde\u7eed\u6027\u65ad\u88c2\u95ee\u9898\uff0c\u901a\u8fc7\u56de\u987e\u6027\u603b\u7ed3\u548c\u6f14\u5316\u7ecf\u9a8c\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u4e3b\u8981\u4f9d\u8d56\u7cbe\u5fc3\u51c6\u5907\u7684\u8f93\u5165\u548c\u7b80\u5316\u7684\u68c0\u7d22\u4efb\u52a1\uff0c\u65e0\u6cd5\u5728\u539f\u59cb\u3001\u9ad8\u566a\u58f0\u7684\u7535\u5b50\u75c5\u5386\u6570\u636e\u5e93\u4e2d\u6267\u884c\u590d\u6742\u7684\u4e34\u5e8a\u51b3\u7b56\u4efb\u52a1\uff0c\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u63a8\u7406\u8fde\u7eed\u6027\u65ad\u88c2\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRetroSum\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u56de\u987e\u6027\u603b\u7ed3\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u8bc4\u4f30\u4ea4\u4e92\u5386\u53f2\u9632\u6b62\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e22\u5931\uff0c\u786e\u4fdd\u903b\u8f91\u8fde\u8d2f\u6027\uff1b2\uff09\u6f14\u5316\u7ecf\u9a8c\u7b56\u7565\uff0c\u901a\u8fc7\u4ece\u8bb0\u5fc6\u5e93\u4e2d\u68c0\u7d22\u7d2f\u79ef\u7ecf\u9a8c\u6765\u5f25\u5408\u9886\u57df\u5dee\u8ddd\u3002", "result": "RetroSum\u5728AgentEHR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe29.16%\uff0c\u540c\u65f6\u5c06\u603b\u4ea4\u4e92\u9519\u8bef\u663e\u8457\u964d\u4f4e\u9ad8\u8fbe92.3%\u3002", "conclusion": "RetroSum\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u56de\u987e\u6027\u603b\u7ed3\u548c\u6f14\u5316\u7ecf\u9a8c\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u539f\u59cb\u7535\u5b50\u75c5\u5386\u6570\u636e\u5e93\u4e2d\u6267\u884c\u590d\u6742\u4e34\u5e8a\u51b3\u7b56\u4efb\u52a1\u65f6\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u533b\u7597AI\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2601.13992", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13992", "abs": "https://arxiv.org/abs/2601.13992", "authors": ["Jin Cui", "Jiaqi Guo", "Jiepeng Zhou", "Ruixuan Yang", "Jiayi Lu", "Jiajun Xu", "Jiangcheng Song", "Boran Zhao", "Pengju Ren"], "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework", "comment": "11pages, 9figures", "summary": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.", "AI": {"tldr": "COMPACT\u662f\u4e00\u4e2a\u591a\u6559\u5e08CoT\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u6559\u5e08\u68af\u5ea6\u6765\u878d\u5408\u4e0d\u540c\u6559\u5e08\u7684\u76d1\u7763\uff0c\u907f\u514d\u5355\u4e00\u6559\u5e08\u7684\u504f\u89c1\u548c\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u5347\u5c0f\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709CoT\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6559\u5e08\u6a21\u578b\uff0c\u4f46\u5355\u4e2aLLM\u5b58\u5728\u80fd\u529b\u504f\u89c1\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6f5c\u529b\u3002\u867d\u7136\u4f7f\u7528\u591a\u6837\u5316\u6559\u5e08\u5f88\u5438\u5f15\u4eba\uff0c\u4f46\u6709\u6548\u878d\u5408\u5b83\u4eec\u7684\u76d1\u7763\u9762\u4e34\u6311\u6218\uff1a\u5e08\u751f\u4e0d\u517c\u5bb9\u53ef\u80fd\u653e\u5927\u5e7b\u89c9\uff0c\u88ab\u52a8\u76d1\u7763\u65e0\u6cd5\u786e\u4fdd\u771f\u6b63\u7684\u903b\u8f91\u5185\u5316\u3002", "method": "COMPACT\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u6559\u5e08\u68af\u5ea6\u6765\u878d\u5408\u4e0d\u540c\u6559\u5e08\u7684\u76d1\u7763\uff0c\u57fa\u4e8e\u4e09\u4e2a\u591a\u7ef4\u6307\u6807\u8bc4\u4f30\u5b66\u751f\u7684\u5b9e\u65f6\u517c\u5bb9\u6027\uff1a1) \u57fa\u4e8e\u56fe\u7684\u5171\u8bc6\u6027\uff0c\u901a\u8fc7\u8bc6\u522b\u4e3b\u6d41\u63a8\u7406\u8def\u5f84\u8fc7\u6ee4\u8bef\u5bfc\u6027\u7406\u7531\uff1b2) \u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u9002\u5e94\u6027\uff0c\u68c0\u6d4b\"\u987f\u609f\u65f6\u523b\"\u4ee5\u786e\u4fdd\u771f\u6b63\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\u800c\u975e\u7b80\u5355\u6a21\u4eff\uff1b3) \u57fa\u4e8e\u635f\u5931\u7684\u96be\u5ea6\uff0c\u8bc4\u4f30\u5b66\u751f\u5bf9\u6559\u5e08\u6307\u5bfc\u7684\u63a5\u53d7\u5ea6\uff0c\u9632\u6b62\u8d1f\u8fc1\u79fb\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u8868\u660e\uff0cCOMPACT\u80fd\u6709\u6548\u6574\u5408\u591a\u6837\u5316\u63a8\u7406\u80fd\u529b\u800c\u4e0d\u635f\u5bb3\u6a21\u578b\u7684\u539f\u59cb\u77e5\u8bc6\u7ed3\u6784\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u8f7b\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "COMPACT\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u591a\u6559\u5e08\u76d1\u7763\uff0c\u6210\u529f\u89e3\u51b3\u4e86CoT\u84b8\u998f\u4e2d\u7684\u5e08\u751f\u4e0d\u517c\u5bb9\u548c\u903b\u8f91\u5185\u5316\u95ee\u9898\uff0c\u4e3a\u5c0f\u6a21\u578b\u83b7\u5f97\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.14007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14007", "abs": "https://arxiv.org/abs/2601.14007", "authors": ["Junyu Zhang", "Yipeng Kang", "Jiong Guo", "Jiayu Zhan", "Junqi Wang"], "title": "BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models", "comment": "34 pagess, 16 figures, 6 tables, submitted to ACL 2026", "summary": "Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.", "AI": {"tldr": "LLMs\u786e\u5b9e\u62e5\u6709\u7ed3\u6784\u5316\u7684\u4ef7\u503c\u8868\u5f81\uff0c\u80fd\u591f\u8de8\u62bd\u8c61\u6982\u5ff5\u3001\u5177\u4f53\u4e8b\u4ef6\u548c\u51b3\u7b56\u63a8\u7406\u4e09\u4e2a\u5c42\u6b21\u8fdb\u884c\u8fc1\u79fb\uff0c\u4f46\u62bd\u8c61\u4ef7\u503c\u8868\u5f81\u76f8\u5bf9\u7a33\u5b9a\uff0c\u4e0d\u6613\u88ab\u5e72\u9884\u6539\u53d8\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u62bd\u8c61\u6982\u5ff5\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5728\u64cd\u4f5c\u7edf\u8ba1\u6a21\u5f0f\u3002\u4ee5\u4eba\u7c7b\u4ef7\u503c\u89c2\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u4e3a\u4ef7\u503c\u89c2\u5177\u6709\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u5bf9\u9f50\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u62bd\u8c61-\u5177\u8eab\u6846\u67b6\uff0c\u5c06\u6982\u5ff5\u7406\u89e3\u5206\u89e3\u4e3a\u4e09\u4e2a\u80fd\u529b\uff1a\u62bd\u8c61\u6982\u5ff5\u89e3\u91ca\uff08A-A\uff09\u3001\u62bd\u8c61\u6982\u5ff5\u5728\u5177\u4f53\u4e8b\u4ef6\u4e2d\u7684\u5177\u8eab\u5316\uff08A-C\uff09\u3001\u62bd\u8c61\u539f\u5219\u5728\u5177\u4f53\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff08C-C\uff09\u3002\u4f7f\u7528\u63a2\u6d4b\uff08\u68c0\u6d4b\u5185\u90e8\u6fc0\u6d3b\u4e2d\u7684\u4ef7\u503c\u75d5\u8ff9\uff09\u548c\u5f15\u5bfc\uff08\u4fee\u6539\u8868\u5f81\u4ee5\u6539\u53d8\u884c\u4e3a\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728\u516d\u4e2a\u5f00\u6e90LLM\u548c\u5341\u4e2a\u4ef7\u503c\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u63a2\u6d4b\u663e\u793a\uff1a\u4ec5\u57fa\u4e8e\u62bd\u8c61\u4ef7\u503c\u63cf\u8ff0\u8bad\u7ec3\u7684\u63a2\u6d4b\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u5728\u5177\u4f53\u4e8b\u4ef6\u53d9\u8ff0\u548c\u51b3\u7b56\u63a8\u7406\u4e2d\u68c0\u6d4b\u5230\u76f8\u540c\u7684\u4ef7\u503c\u89c2\uff0c\u8868\u73b0\u51fa\u8de8\u5c42\u6b21\u8fc1\u79fb\u80fd\u529b\u3002\u5f15\u5bfc\u5b9e\u9a8c\u63ed\u793a\u4e0d\u5bf9\u79f0\u6027\uff1a\u5e72\u9884\u4ef7\u503c\u8868\u5f81\u4f1a\u56e0\u679c\u6027\u5730\u6539\u53d8\u5177\u4f53\u5224\u65ad\u548c\u51b3\u7b56\uff08A-C\uff0cC-C\uff09\uff0c\u4f46\u4e0d\u4f1a\u6539\u53d8\u62bd\u8c61\u89e3\u91ca\uff08A-A\uff09\uff0c\u8868\u660e\u7f16\u7801\u7684\u62bd\u8c61\u4ef7\u503c\u89c2\u4f5c\u4e3a\u7a33\u5b9a\u951a\u70b9\u800c\u975e\u53ef\u5851\u6fc0\u6d3b\u3002", "conclusion": "LLMs\u7ef4\u6301\u7740\u7ed3\u6784\u5316\u7684\u4ef7\u503c\u8868\u5f81\uff0c\u80fd\u591f\u6865\u63a5\u62bd\u8c61\u4e0e\u884c\u52a8\uff0c\u8fd9\u4e3a\u6784\u5efa\u4ef7\u503c\u9a71\u52a8\u7684\u81ea\u4e3bAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u548c\u64cd\u4f5c\u6027\u7684\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u900f\u660e\u3001\u53ef\u6cdb\u5316\u7684\u5bf9\u9f50\u548c\u63a7\u5236\u3002"}}
{"id": "2601.14041", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14041", "abs": "https://arxiv.org/abs/2601.14041", "authors": ["Yunhe Wang", "Kai Han", "Huiling Zhen", "Yuchuan Tian", "Hanting Chen", "Yongbing Huang", "Yufei Cui", "Yingte Shu", "Shan Gao", "Ismail Elezi", "Roy Vaughan Miles", "Songcen Xu", "Feng Wen", "Chao Xu", "Sinan Zeng", "Dacheng Tao"], "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants", "comment": null, "summary": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLMs\uff09\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u5341\u5927\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u56db\u5927\u652f\u67f1\u7684\u8def\u7ebf\u56fe\u6765\u63a8\u52a8DLMs\u53d1\u5c55", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u91c7\u7528\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u5b58\u5728\u56e0\u679c\u74f6\u9888\u9650\u5236\u5168\u5c40\u7ed3\u6784\u9884\u89c1\u548c\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\u3002\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u6f5c\u529b\u672a\u5145\u5206\u53d1\u6325\uff0c\u9700\u8981\u7a81\u7834\u73b0\u6709AR\u9057\u7559\u6846\u67b6\u7684\u675f\u7f1a", "method": "\u8bc6\u522b\u4e86\u963b\u788dDLMs\u53d1\u5c55\u7684\u5341\u5927\u6839\u672c\u6311\u6218\uff0c\u5305\u62ec\u67b6\u6784\u60ef\u6027\u3001\u68af\u5ea6\u7a00\u758f\u6027\u3001\u7ebf\u6027\u63a8\u7406\u9650\u5236\u7b49\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u56db\u5927\u652f\u67f1\u7684\u6218\u7565\u8def\u7ebf\u56fe\uff1a\u57fa\u7840\u67b6\u6784\u3001\u7b97\u6cd5\u4f18\u5316\u3001\u8ba4\u77e5\u63a8\u7406\u548c\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd", "result": "\u63d0\u51fa\u4e86\u5411\u6269\u6563\u539f\u751f\u751f\u6001\u7cfb\u7edf\u8f6c\u578b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u5206\u8bcd\u3001\u4e3b\u52a8\u91cd\u63a9\u7801\u3001\u6f5c\u5728\u601d\u7ef4\u7b49\u5173\u952e\u6280\u672f\uff0c\u65e8\u5728\u8d85\u8d8a\u56e0\u679c\u89c6\u91ce\u7684\u7ea6\u675f", "conclusion": "\u5411\u6269\u6563\u539f\u751f\u751f\u6001\u7cfb\u7edf\u7684\u8f6c\u578b\u5bf9\u4e8e\u5f00\u53d1\u4e0b\u4e00\u4ee3AI\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u7ed3\u6784\u63a8\u7406\u3001\u52a8\u6001\u81ea\u6211\u4fee\u6b63\u548c\u65e0\u7f1d\u591a\u6a21\u6001\u96c6\u6210\uff0c\u63a8\u52a8DLMs\u8fbe\u5230\"GPT-4\u65f6\u523b\""}}
{"id": "2601.14022", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14022", "abs": "https://arxiv.org/abs/2601.14022", "authors": ["Rodrigo Pereira David", "Luciano Araujo Dourado Filho", "Daniel Marques da Silva", "Jo\u00e3o Alfredo Cal-Braz"], "title": "Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment", "comment": null, "summary": "Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5728\u76f8\u540c\u771f\u5b9e\u9a7e\u9a76\u6761\u4ef6\u4e0b\u516c\u5e73\u6bd4\u8f83\u5185\u71c3\u673a\u8f66\u548c\u7535\u52a8\u8f66\u7684CO2\u6392\u653e", "motivation": "\u9053\u8def\u8fd0\u8f93\u8131\u78b3\u9700\u8981\u4e00\u81f4\u900f\u660e\u7684\u65b9\u6cd5\u6765\u6bd4\u8f83\u4e0d\u540c\u8f66\u8f86\u6280\u672f\u7684CO2\u6392\u653e\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5728\u76f8\u540c\u771f\u5b9e\u9a7e\u9a76\u6761\u4ef6\u4e0b\u7684\u516c\u5e73\u6bd4\u8f83", "method": "\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u8bad\u7ec3ICEV\u548cEV\u6a21\u578b\uff0c\u5b66\u4e60\u4ece\u9a7e\u9a76\u53d8\u91cf\uff08\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u6e29\u5ea6\uff09\u5230\u5185\u90e8\u6267\u884c\u53d8\u91cf\uff08\u626d\u77e9\u3001\u6cb9\u95e8\uff09\u548c\u77ac\u65f6CO2\u5f53\u91cf\u6392\u653e\u7387\u7684\u6620\u5c04\uff0c\u6784\u5efa\u53cd\u4e8b\u5b9e\u573a\u666f\u8fdb\u884c\u5bf9\u6bd4", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u77ac\u65f6\u6392\u653e\u6307\u6807\u4e0b\u516c\u5e73\u8bc4\u4f30\u52a8\u529b\u7cfb\u7edf\u6280\u672f\uff0c\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u8f66\u8f86\u78b3\u6027\u80fd\u8bc4\u4f30", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e3a\u5728\u771f\u5b9e\u64cd\u4f5c\u6761\u4ef6\u4e0b\u8fdb\u884c\u53ef\u4fe1\u3001\u53ef\u91cd\u590d\u7684\u8f66\u8f86\u6280\u672f\u6bd4\u8f83\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u9053\u8def\u8fd0\u8f93\u8131\u78b3\u51b3\u7b56"}}
{"id": "2601.14121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14121", "abs": "https://arxiv.org/abs/2601.14121", "authors": ["Jonathan Tonglet", "Iryna Gurevych", "Tinne Tuytelaars", "Marie-Francine Moens"], "title": "NewsRECON: News article REtrieval for image CONtextualization", "comment": "Preprint under review. Code available at https://github.com/jtonglet/arxiv2025-newsrecon", "summary": "Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.", "AI": {"tldr": "NewsRECON\uff1a\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u8bc1\u636e\u7684\u65b0\u95fb\u56fe\u50cf\u65f6\u7a7a\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u94fe\u63a5\u5230\u76f8\u5173\u65b0\u95fb\u6587\u7ae0\u6765\u63a8\u65ad\u62cd\u6444\u65f6\u95f4\u548c\u5730\u70b9\u3002", "motivation": "\u65b0\u95fb\u56fe\u50cf\u7684\u65f6\u95f4\u548c\u5730\u70b9\u5b9a\u4f4d\u5bf9\u8bb0\u8005\u548c\u53d6\u8bc1\u4e13\u5bb6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u5de5\u5177\u7ecf\u5e38\u65e0\u6cd5\u8fd4\u56de\u7ed3\u679c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u9488\u5bf9\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u8bc1\u636e\u4e0d\u53ef\u7528\u7684\u6311\u6218\u6027\u573a\u666f\u3002", "method": "NewsRECON\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u53cc\u7f16\u7801\u5668\u7528\u4e8e\u68c0\u7d22\u4e8b\u4ef6\u76f8\u5173\u6587\u7ae0\uff1b2\uff09\u4e24\u4e2a\u4ea4\u53c9\u7f16\u7801\u5668\u5206\u522b\u6839\u636e\u4f4d\u7f6e\u4e00\u81f4\u6027\u548c\u4e8b\u4ef6\u4e00\u81f4\u6027\u5bf9\u6587\u7ae0\u8fdb\u884c\u91cd\u6392\u5e8f\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8d85\u8fc790,000\u7bc7\u6587\u7ae0\u7684\u8bed\u6599\u5e93\u3002", "result": "\u5728TARA\u548c5Pils-OOC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNewsRECON\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u7f3a\u4e4f\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u8bc1\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u65b0\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "NewsRECON\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u5931\u8d25\u65f6\u901a\u8fc7\u94fe\u63a5\u56fe\u50cf\u5230\u65b0\u95fb\u6587\u7ae0\u6765\u63a8\u65ad\u65f6\u7a7a\u4fe1\u606f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.14152", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14152", "abs": "https://arxiv.org/abs/2601.14152", "authors": ["Hyunjong Ok", "Jaeho Lee"], "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models", "comment": "preprint", "summary": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u591a\u9879\u9009\u62e9\u9898\u56de\u7b54\u4e2d\uff0c\u5c06\u4e0a\u4e0b\u6587\u653e\u5728\u95ee\u9898\u548c\u9009\u9879\u4e4b\u524d\uff08CQO\uff09\u6bd4\u53cd\u5411\u987a\u5e8f\uff08QOC\uff09\u6027\u80fd\u63d0\u5347\u8d85\u8fc714%\uff0c\u8fd9\u79cd\u5dee\u5f02\u6e90\u4e8e\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4QOC\u4e2d\u9009\u9879\u65e0\u6cd5\u5173\u6ce8\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u7ed3\u6784\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u654f\u611f\u6027\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u63a2\u7a76\u4e00\u4e2a\u663e\u8457\u73b0\u8c61\uff1a\u5728\u591a\u9879\u9009\u62e9\u9898\u56de\u7b54\u4e2d\uff0c\u4e0d\u540c\u7684\u63d0\u793a\u987a\u5e8f\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7684\u67b6\u6784\u5206\u6790\uff0c\u8bc6\u522b\u56e0\u679c\u6ce8\u610f\u529b\u4f5c\u4e3a\u6838\u5fc3\u673a\u5236\u3002\u5728QOC\u63d0\u793a\u4e2d\uff0c\u56e0\u679c\u63a9\u7801\u963b\u6b62\u9009\u9879\u6807\u8bb0\u5173\u6ce8\u4e0a\u4e0b\u6587\uff0c\u521b\u5efa\u4e86\u4fe1\u606f\u74f6\u9888\uff0c\u4f7f\u5f97\u4e0a\u4e0b\u6587\u5bf9\u9009\u9879\u4e0d\u53ef\u89c1\u3002", "result": "CQO\u987a\u5e8f\uff08\u4e0a\u4e0b\u6587-\u95ee\u9898-\u9009\u9879\uff09\u6bd4QOC\u987a\u5e8f\uff08\u95ee\u9898-\u9009\u9879-\u4e0a\u4e0b\u6587\uff09\u6027\u80fd\u63d0\u5347\u8d85\u8fc714\u4e2a\u767e\u5206\u70b9\uff0c\u8fd9\u79cd\u5dee\u5f02\u5728\u5e7f\u6cdb\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u662f\u5bfc\u81f4\u63d0\u793a\u7ed3\u6784\u654f\u611f\u6027\u7684\u6838\u5fc3\u539f\u56e0\uff0c\u7406\u89e3\u8fd9\u4e00\u673a\u5236\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u3002"}}
{"id": "2601.13299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13299", "abs": "https://arxiv.org/abs/2601.13299", "authors": ["Ethan Seefried", "Prahitha Movva", "Naga Harshita Marupaka", "Tilak Kasturi", "Tirthankar Ghosal"], "title": "Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams", "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Ai4 Science", "summary": "We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.", "AI": {"tldr": "Enginuity\uff1a\u9996\u4e2a\u5f00\u6e90\u3001\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u5de5\u7a0b\u56fe\u6570\u636e\u96c6\uff0c\u5305\u542b\u5168\u9762\u7684\u7ed3\u6784\u5316\u6807\u6ce8\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u56fe\u8868\u89e3\u6790", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u96be\u4ee5\u7406\u89e3\u548c\u5904\u7406\u5de5\u7a0b\u56fe\u4e2d\u7684\u89c6\u89c9-\u7ed3\u6784\u77e5\u8bc6\uff0c\u8fd9\u963b\u788d\u4e86AI\u5728\u79d1\u5b66\u53d1\u73b0\u5de5\u4f5c\u6d41\u4e2d\u7684\u5168\u9762\u53c2\u4e0e\u3002\u5de5\u7a0b\u56fe\u89e3\u6790\u3001\u6280\u672f\u56fe\u7eb8\u5206\u6790\u548c\u89c6\u89c9\u63a8\u7406\u5bf9\u4e8e\u5047\u8bbe\u751f\u6210\u3001\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u79d1\u5b66\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5f00\u653e\u3001\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684\u5de5\u7a0b\u56fe\u6570\u636e\u96c6\uff0c\u5305\u542b\u5c42\u6b21\u5316\u7ec4\u4ef6\u5173\u7cfb\u3001\u8fde\u63a5\u548c\u8bed\u4e49\u5143\u7d20\u7684\u5168\u9762\u7ed3\u6784\u5316\u6807\u6ce8\u3002\u8be5\u6570\u636e\u96c6\u652f\u6301\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5de5\u7a0b\u56fe\u89e3\u6790\u4efb\u52a1\u3002", "result": "\u63d0\u51fa\u7684Enginuity\u6570\u636e\u96c6\u5c06\u80fd\u591f\u652f\u6301\u7ed3\u6784\u5316\u56fe\u8868\u89e3\u6790\u3001\u8de8\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u548cAI\u8f85\u52a9\u5de5\u7a0b\u4eff\u771f\u7b49\u5173\u952e\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "Enginuity\u6570\u636e\u96c6\u5c06\u4e3a\u79d1\u5b66\u53d1\u73b0AI\u5e26\u6765\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u7406\u89e3\u548c\u64cd\u4f5c\u5de5\u7a0b\u56fe\u4e2d\u5d4c\u5165\u7684\u89c6\u89c9-\u7ed3\u6784\u77e5\u8bc6\uff0c\u6253\u7834\u5f53\u524d\u963b\u788dAI\u5168\u9762\u53c2\u4e0e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u57fa\u672c\u969c\u788d\u3002"}}
{"id": "2601.14230", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.14230", "abs": "https://arxiv.org/abs/2601.14230", "authors": ["Yiyang Wang", "Yiqiao Jin", "Alex Cabral", "Josiah Hester"], "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems", "comment": "15 pages, 9 figures", "summary": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.", "AI": {"tldr": "MASCOT\uff1a\u4e00\u4e2a\u9632\u6b62\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u89d2\u8272\u5d29\u6e83\u548c\u793e\u4f1a\u8c04\u5a9a\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u7b56\u7565\u534f\u8c03\u4e2a\u4f53\u4e0e\u96c6\u4f53\u884c\u4e3a", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u89d2\u8272\u5d29\u6e83\uff08\u667a\u80fd\u4f53\u9000\u5316\u4e3a\u901a\u7528\u52a9\u624b\u884c\u4e3a\uff09\u548c\u793e\u4f1a\u8c04\u5a9a\uff08\u4ea7\u751f\u5197\u4f59\u3001\u975e\u5efa\u8bbe\u6027\u5bf9\u8bdd\uff09\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u9677\u4ee5\u6784\u5efa\u771f\u6b63\u6709\u6548\u7684\u793e\u4ea4\u534f\u4f5c\u4f34\u4fa3", "method": "\u63d0\u51faMASCOT\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u7b56\u7565\uff1a1\uff09\u89d2\u8272\u611f\u77e5\u884c\u4e3a\u5bf9\u9f50\u2014\u2014\u57fa\u4e8eRLAIF\u7684\u7ba1\u9053\uff0c\u5fae\u8c03\u4e2a\u4f53\u667a\u80fd\u4f53\u4ee5\u786e\u4fdd\u4e25\u683c\u7684\u89d2\u8272\u4fdd\u771f\u5ea6\uff1b2\uff09\u534f\u4f5c\u5bf9\u8bdd\u4f18\u5316\u2014\u2014\u57fa\u4e8e\u7fa4\u4f53\u7ea7\u5956\u52b1\u7684\u5143\u7b56\u7565\uff0c\u786e\u4fdd\u591a\u6837\u5316\u548c\u5bcc\u6709\u6210\u6548\u7684\u5bf9\u8bdd", "result": "\u5728\u5fc3\u7406\u652f\u6301\u548c\u804c\u573a\u9886\u57df\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cMASCOT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u63d0\u5347\u9ad8\u8fbe+14.1\uff0c\u5728\u793e\u4f1a\u8d21\u732e\u65b9\u9762\u63d0\u5347\u9ad8\u8fbe+10.6", "conclusion": "MASCOT\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u793e\u4ea4\u667a\u80fd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8def\u7ebf\u56fe\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89d2\u8272\u5d29\u6e83\u548c\u793e\u4f1a\u8c04\u5a9a\u95ee\u9898"}}
{"id": "2601.13498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13498", "abs": "https://arxiv.org/abs/2601.13498", "authors": ["Nimrod Kruger", "Nicholas Owen Ralph", "Gregory Cohen", "Paul Hurley"], "title": "Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging", "comment": null, "summary": "Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.", "AI": {"tldr": "\u5c06\u4e8b\u4ef6\u6d41\u6620\u5c04\u5230\u5bf9\u6570\u5f3a\u5ea6\u548c\u5f3a\u5ea6\u5bfc\u6570\u4f30\u8ba1\uff0c\u5d4c\u5165\u52a8\u6001\u7ebf\u6027\u7cfb\u7edf\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u6570\u636e\u7684\u9006\u6ee4\u6ce2", "motivation": "\u4e8b\u4ef6\u89c6\u89c9\u4f20\u611f\u5668\uff08\u795e\u7ecf\u5f62\u6001\u76f8\u673a\uff09\u8f93\u51fa\u7a00\u758f\u3001\u5f02\u6b65\u7684\u4e8b\u4ef6\u6d41\uff0c\u4f46\u4f5c\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u96be\u4ee5\u4e0e\u5927\u591a\u6570\u8ba1\u7b97\u6210\u50cf\u548c\u5149\u5b66\u7cfb\u7edf\u8bbe\u8ba1\u6240\u4f9d\u8d56\u7684\u7ebf\u6027\u524d\u5411\u6a21\u578b\u96c6\u6210", "method": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u7684\u5904\u7406\u6d41\u7a0b\uff1a\u5c06\u4e8b\u4ef6\u6d41\u6620\u5c04\u5230\u50cf\u7d20\u7ea7\u5bf9\u6570\u5f3a\u5ea6\u548c\u5f3a\u5ea6\u5bfc\u6570\u4f30\u8ba1\uff0c\u5c06\u8fd9\u4e9b\u6d4b\u91cf\u5d4c\u5165\u5177\u6709\u65f6\u53d8\u70b9\u6269\u6563\u51fd\u6570\u7684\u52a8\u6001\u7ebf\u6027\u7cfb\u7edf\u6a21\u578b\uff0c\u4f7f\u7528\u9891\u57df\u7ef4\u7eb3\u53cd\u5377\u79ef\u8fdb\u884c\u9006\u6ee4\u6ce2", "result": "\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u8c03\u5236\u79bb\u7126\u4e0b\u5355\u70b9\u548c\u91cd\u53e0\u70b9\u6e90\u7684\u5904\u7406\u6548\u679c\uff0c\u5e76\u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\uff08\u53ef\u8c03\u7126\u671b\u8fdc\u955c\u89c2\u6d4b\u661f\u573a\uff09\u4e0a\u5c55\u793a\u4e86\u6e90\u5b9a\u4f4d\u548c\u53ef\u5206\u79bb\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e8b\u4ef6\u611f\u77e5\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u50cf\u4e4b\u95f4\u5efa\u7acb\u4e86\u5b9e\u7528\u6865\u6881\uff0c\u7279\u522b\u9002\u7528\u4e8e\u52a8\u6001\u5149\u5b66\u7cfb\u7edf"}}
{"id": "2601.13839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13839", "abs": "https://arxiv.org/abs/2601.13839", "authors": ["Aisha Al-Mohannadi", "Ayisha Firoz", "Yin Yang", "Muhammad Imran", "Ferda Ofli"], "title": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes", "comment": null, "summary": "Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.", "AI": {"tldr": "DisasterVQA\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u707e\u5bb3\u54cd\u5e94\u8bbe\u8ba1\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1,395\u5f20\u771f\u5b9e\u707e\u5bb3\u56fe\u50cf\u548c4,405\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u707e\u5bb3\u573a\u666f\u4e0b\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u5728\u707e\u5bb3\u671f\u95f4\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7684\u6001\u52bf\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u7684\u901a\u7528\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u5728\u707e\u5bb3\u54cd\u5e94\u8fd9\u79cd\u590d\u6742\u3001\u5b89\u5168\u5173\u952e\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u5c1a\u4e0d\u6e05\u695a\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u707e\u5bb3\u573a\u666f\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u548c\u6307\u5bfc\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u521b\u5efaDisasterVQA\u6570\u636e\u96c6\uff0c\u5305\u542b1,395\u5f20\u771f\u5b9e\u707e\u5bb3\u56fe\u50cf\uff08\u6d2a\u6c34\u3001\u91ce\u706b\u3001\u5730\u9707\u7b49\uff09\u548c4,405\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u95ee\u7b54\u5bf9\u3002\u95ee\u9898\u57fa\u4e8eFEMA ESF\u548cOCHA MIRA\u7b49\u4eba\u9053\u4e3b\u4e49\u6846\u67b6\u8bbe\u8ba1\uff0c\u6db5\u76d6\u4e8c\u5143\u9009\u62e9\u3001\u591a\u9879\u9009\u62e9\u548c\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u6d89\u53ca\u6001\u52bf\u611f\u77e5\u548c\u64cd\u4f5c\u51b3\u7b56\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u4e867\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u3001\u707e\u5bb3\u7c7b\u522b\u3001\u533a\u57df\u548c\u4eba\u9053\u4e3b\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002\u6a21\u578b\u5728\u4e8c\u5143\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u5b9a\u91cf\u63a8\u7406\u3001\u7269\u4f53\u8ba1\u6570\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u707e\u5bb3\u573a\u666f\u4e2d\u3002", "conclusion": "DisasterVQA\u4e3a\u707e\u5bb3\u54cd\u5e94\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u7684\u57fa\u51c6\uff0c\u80fd\u591f\u6307\u5bfc\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u64cd\u4f5c\u610f\u4e49\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u707e\u5bb3\u54cd\u5e94AI\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.14042", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14042", "abs": "https://arxiv.org/abs/2601.14042", "authors": ["Jiaze Li", "Haoran Xu", "Wanyi Wu", "Changwei Wang", "Shuaiguang Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Youyang Qu", "Longxiang Gao", "Xudong Yang", "Lumin Xing"], "title": "Federated Balanced Learning", "comment": null, "summary": "Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.", "AI": {"tldr": "\u63d0\u51faFBL\u8054\u90a6\u5e73\u8861\u5b66\u4e60\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u6837\u672c\u5e73\u8861\u89e3\u51b3\u975eIID\u6570\u636e\u4e0b\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u4f7f\u7528\u77e5\u8bc6\u586b\u5145\u548c\u77e5\u8bc6\u91c7\u6837\u5b9e\u73b0\u6837\u672c\u5e73\u8861", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u7684\u975eIID\u8bbe\u7f6e\u4e2d\uff0c\u5168\u5c40\u6a21\u578b\u4f1a\u51fa\u73b0\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u6700\u7ec8\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u635f\u5931\u51fd\u6570\u6216\u68af\u5ea6\u7ea0\u6b63\u5df2\u504f\u79bb\u7684\u5168\u5c40\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u5ba2\u6237\u7aef\u6837\u672c\u7684\u5f71\u54cd", "method": "\u63d0\u51faFBL\u65b9\u6cd5\uff0c\u5728\u5ba2\u6237\u7aef\u4fa7\u901a\u8fc7\u77e5\u8bc6\u586b\u5145\u548c\u77e5\u8bc6\u91c7\u6837\u5b9e\u73b0\u6837\u672c\u5e73\u8861\uff0c\u4f7f\u7528\u8fb9\u7f18\u4fa7\u751f\u6210\u6a21\u578b\uff1b\u8bbe\u8ba1\u77e5\u8bc6\u5bf9\u9f50\u7b56\u7565\u548c\u77e5\u8bc6\u4e22\u5f03\u7b56\u7565\uff1b\u6269\u5c55\u5230\u771f\u5b9e\u590d\u6742\u573a\u666f\uff0c\u5141\u8bb8\u4e0d\u540c\u5ba2\u6237\u7aef\u91c7\u7528\u4e0d\u540c\u65b9\u6cd5", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5ba2\u6237\u7aef\u6837\u672c\u5e73\u8861\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0cFBL\u65b9\u6cd5\u5728\u975eIID\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272"}}
