{"id": "2508.15852", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15852", "abs": "https://arxiv.org/abs/2508.15852", "authors": ["Bin Wen", "Tien-Ping Tan"], "title": "PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis", "comment": null, "summary": "We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep\nlearning framework designed for efficient and interpretable multimodal\nsentiment analysis. Our framework incorporates three primary innovations.\nFirstly, we propose a Progressive Intra-Layer Fusion paradigm, where a\nCross-Attention mechanism empowers the textual representation to dynamically\nquery and integrate non-linguistic features from audio and visual streams\nwithin the deep layers of a Transformer encoder. This enables a deeper,\ncontext-dependent fusion process. Secondly, the model incorporates an Adaptive\nGated Arbitration mechanism, which acts as a dynamic controller to balance the\noriginal linguistic information against the newly fused multimodal context,\nensuring stable and meaningful integration while preventing noise from\noverwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning\n(PEFT) strategy is employed, synergistically combining global adaptation via\nLoRA with local refinement through Post-Fusion Adapters. This significantly\nreduces trainable parameters, making the model lightweight and suitable for\nresource-limited scenarios. These innovations are integrated into a\nhierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,\nand interpretable multimodal sentiment analysis while maintaining exceptional\nparameter efficiency. Experimental results on MOSI dataset demonstrate that our\nproposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute\nError (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves\nthese results with only 3.09M trainable parameters, showcasing a superior\nbalance between performance and computational efficiency.", "AI": {"tldr": "PGF-Net\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u95e8\u63a7\u878d\u5408\u3001\u81ea\u9002\u5e94\u95e8\u63a7\u4ef2\u88c1\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\uff0c\u5728MOSI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u6df1\u5ea6\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7279\u5f81\u878d\u5408\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u53c2\u6570\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5e73\u8861\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u5e76\u9632\u6b62\u566a\u58f0\u5e72\u6270\u7684\u878d\u5408\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u6e10\u8fdb\u5f0f\u5c42\u5185\u878d\u5408\u8303\u5f0f\uff08Cross-Attention\u673a\u5236\uff09\u3001\u81ea\u9002\u5e94\u95e8\u63a7\u4ef2\u88c1\u673a\u5236\uff08\u52a8\u6001\u63a7\u5236\u5668\uff09\u548c\u6df7\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff08LoRA+Post-Fusion Adapters\uff09\uff0c\u96c6\u6210\u5230\u5206\u5c42\u7f16\u7801\u5668\u67b6\u6784\u4e2d\u3002", "result": "\u5728MOSI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97MAE 0.691\u548cF1-Score 86.9%\u7684SOTA\u6027\u80fd\uff0c\u4ec5\u4f7f\u75283.09M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5c55\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u5e73\u8861\u3002", "conclusion": "PGF-Net\u901a\u8fc7\u521b\u65b0\u7684\u878d\u5408\u673a\u5236\u548c\u53c2\u6570\u9ad8\u6548\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6df1\u5ea6\u3001\u52a8\u6001\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.15903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15903", "abs": "https://arxiv.org/abs/2508.15903", "authors": ["Kaining Li", "Shuwei He", "Zihan Xu"], "title": "VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos", "comment": null, "summary": "Human action recognition in long-term videos, characterized by complex\nbackgrounds and subtle action differences, poses significant challenges for\ntraditional deep learning models due to computational overhead, difficulty in\ncapturing long-range temporal dependencies, and limited semantic understanding.\nWhile Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nhave shown remarkable capabilities in multi-modal understanding and reasoning,\ntheir direct application to continuous video streams for fine-grained action\nrecognition remains an open problem. This paper introduces VT-LVLM-AR\n(Video-Temporal Large Vision-Language Model Adapter for Action Recognition), a\nnovel framework designed to bridge this gap. VT-LVLM-AR comprises a\nVideo-to-Event Mapper (VTEM) that efficiently transforms raw video into\ncompact, semantically rich, and temporally coherent \"visual event sequences\"\nthrough lightweight spatio-temporal feature extraction, adaptive temporal\npooling, and conceptual quantization with an event coherence bias. These visual\nevent sequences are then fed into an LVLM-based Action Reasoning module,\nspecifically a frozen LLaVA-1.5 model, adapted using parameter-efficient Prompt\nTuning (P-Tuning v2) for action classification. Comprehensive evaluations on\nthe NTU RGB+D and NTU RGB+D 120 datasets demonstrate that VT-LVLM-AR\nconsistently achieves state-of-the-art performance, surpassing existing methods\n(e.g., 94.1% accuracy on NTU RGB+D X-Sub). Ablation studies confirm the\ncritical contributions of VTEM's components and the efficacy of Prompt Tuning,\nwhile human evaluations underscore the interpretability of our visual event\nrepresentations. This work highlights the immense potential of leveraging LVLMs\nfor robust and interpretable video action understanding through effective\nvideo-to-language translation and efficient model adaptation.", "AI": {"tldr": "VT-LVLM-AR\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u5230\u4e8b\u4ef6\u6620\u5c04\u5668\u548cLVLM\u52a8\u4f5c\u63a8\u7406\u6a21\u5757\uff0c\u5728\u957f\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u957f\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u4e2d\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u65f6\u5e8f\u4f9d\u8d56\u548c\u8bed\u4e49\u7406\u89e3\u6709\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22LVLM\u5728\u8fde\u7eed\u89c6\u9891\u6d41\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u5e94\u7528", "method": "\u4f7f\u7528Video-to-Event Mapper\u5c06\u539f\u59cb\u89c6\u9891\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u89c6\u89c9\u4e8b\u4ef6\u5e8f\u5217\uff0c\u7136\u540e\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684Prompt Tuning\u9002\u914d\u51bb\u7ed3\u7684LLaVA-1.5\u6a21\u578b\u8fdb\u884c\u52a8\u4f5c\u5206\u7c7b", "result": "\u5728NTU RGB+D\u548cNTU RGB+D 120\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff08NTU RGB+D X-Sub\u51c6\u786e\u738794.1%\uff09\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u901a\u8fc7\u6709\u6548\u7684\u89c6\u9891\u5230\u8bed\u8a00\u8f6c\u6362\u548c\u9ad8\u6548\u6a21\u578b\u9002\u914d\uff0c\u5229\u7528LVLM\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u9891\u52a8\u4f5c\u7406\u89e3\u7684\u5de8\u5927\u6f5c\u529b"}}
{"id": "2508.16054", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16054", "abs": "https://arxiv.org/abs/2508.16054", "authors": ["Sonish Sivarajkumar", "Hang Zhang", "Yuelyu Ji", "Maneesh Bilalpur", "Xizhi Wu", "Chenyu Li", "Min Gu Kwak", "Shyam Visweswaran", "Yanshan Wang"], "title": "Generative Foundation Model for Structured and Unstructured Electronic Health Records", "comment": null, "summary": "Electronic health records (EHRs) are rich clinical data sources but complex\nrepositories of patient data, spanning structured elements (demographics,\nvitals, lab results, codes), unstructured clinical notes and other modalities\nof data. Harnessing this heterogeneity is critical for improving patient\noutcomes. Recent advances in large language models (LLMs) have enabled\nfoundation models that can learn from multiple data modalities and support\nclinical tasks. However, most current approaches simply serialize numeric EHR\ndata into text, which risks losing temporal and quantitative detail. We\nintroduce Generative Deep Patient (GDP), a multimodal foundation model that\nnatively encodes structured EHR time-series via a CNN-Transformer encoder and\nfuses it with unstructured EHRs through cross-modal attention into a\nLLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,\nwhere it learns to produce clinical narratives from raw patient timelines while\nalso performing masked feature prediction (MFP) and next time-step prediction\n(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for\nclinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day\nreadmission). In clinical prediction, GDP demonstrated superior performance on\nMIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and\n30-day readmission AUROC = 0.627. For narrative generation, GDP achieved\nROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,\nGDP-Instruct scored highest on faithfulness, fluency, and overall clinical\nutility, suggesting reduced hospital documentation workload without sacrificing\naccuracy. Our results demonstrate that a single multimodal foundation model can\nboth predict clinically actionable events and generate high-quality clinical\nnarratives. Furthermore, GDP's flexible architecture can be extended to\nadditional modalities.", "AI": {"tldr": "GDP\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7CNN-Transformer\u7f16\u7801\u5668\u5904\u7406\u7ed3\u6784\u5316EHR\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u4e0e\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u878d\u5408\uff0c\u5728\u4e34\u5e8a\u9884\u6d4b\u548c\u53d9\u4e8b\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHRs)\u5305\u542b\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u6570\u503c\u6570\u636e\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u4f1a\u4e22\u5931\u65f6\u95f4\u548c\u5b9a\u91cf\u7ec6\u8282\u4fe1\u606f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u539f\u751f\u5904\u7406\u591a\u6a21\u6001EHR\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faGenerative Deep Patient (GDP)\u6a21\u578b\uff1a1) \u4f7f\u7528CNN-Transformer\u7f16\u7801\u5668\u539f\u751f\u7f16\u7801\u7ed3\u6784\u5316EHR\u65f6\u95f4\u5e8f\u5217\uff1b2) \u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u4e0e\u65e0\u7ed3\u6784EHR\u878d\u5408\uff1b3) \u57fa\u4e8eLLaMA\u7684\u89e3\u7801\u5668\uff1b4) \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\uff08\u4e34\u5e8a\u53d9\u4e8b\u751f\u6210+\u63a9\u7801\u7279\u5f81\u9884\u6d4b+\u4e0b\u4e00\u65f6\u95f4\u6b65\u9884\u6d4b\uff09\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\uff1a\u5fc3\u8870\u9884\u6d4bAUROC=0.923\uff0c2\u578b\u7cd6\u5c3f\u75c5AUROC=0.817\uff0c30\u5929\u518d\u5165\u9662AUROC=0.627\uff1b\u53d9\u4e8b\u751f\u6210ROUGE-L=0.135\uff0cBERTScore-F1=0.545\uff1b\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u5728\u5fe0\u5b9e\u6027\u3001\u6d41\u7545\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u65b9\u9762\u5f97\u5206\u6700\u9ad8\u3002", "conclusion": "\u5355\u4e00\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u4e34\u5e8a\u53ef\u64cd\u4f5c\u4e8b\u4ef6\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u4e34\u5e8a\u53d9\u4e8b\uff0cGDP\u7684\u7075\u6d3b\u67b6\u6784\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u6a21\u6001\uff0c\u6709\u671b\u51cf\u5c11\u533b\u9662\u6587\u6863\u5de5\u4f5c\u91cf\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002"}}
{"id": "2508.16059", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16059", "abs": "https://arxiv.org/abs/2508.16059", "authors": ["Zhuomin Chen", "Dan Li", "Jiahui Zhou", "Shunyu Wu", "Haozheng Ye", "Jian Lou", "See-Kiong Ng"], "title": "Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting", "comment": "To be published in CIKM 2025", "summary": "Time series (TS) data are ubiquitous across various application areas,\nrendering time series forecasting (TSF) a fundamental task. With the astounding\nadvances in large language models (LLMs), a variety of methods have been\ndeveloped to adapt LLMs for time series forecasting. Despite unlocking the\npotential of LLMs in comprehending TS data, existing methods are inherently\nconstrained by their shallow integration of TS information, wherein LLMs\ntypically access TS representations at shallow layers, primarily at the input\nlayer. This causes the influence of TS representations to progressively fade in\ndeeper layers and eventually leads to ineffective adaptation between textual\nembeddings and TS representations. In this paper, we propose the Multi-layer\nSteerable Embedding Fusion (MSEF), a novel framework that enables LLMs to\ndirectly access time series patterns at all depths, thereby mitigating the\nprogressive loss of TS information in deeper layers. Specifically, MSEF\nleverages off-the-shelf time series foundation models to extract semantically\nrich embeddings, which are fused with intermediate text representations across\nLLM layers via layer-specific steering vectors. These steering vectors are\ndesigned to continuously optimize the alignment between time series and textual\nmodalities and facilitate a layer-specific adaptation mechanism that ensures\nefficient few-shot learning capabilities. Experimental results on seven\nbenchmarks demonstrate significant performance improvements by MSEF compared\nwith baselines, with an average reduction of 31.8% in terms of MSE. The code is\navailable at https://github.com/One1sAll/MSEF.", "AI": {"tldr": "\u63d0\u51fa\u4e86MSEF\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u53ef\u5f15\u5bfc\u5d4c\u5165\u878d\u5408\u6280\u672f\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u6240\u6709\u6df1\u5ea6\u76f4\u63a5\u8bbf\u95ee\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u5728\u6df1\u5c42\u9010\u6e10\u6d88\u5931\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u4e3b\u8981\u96c6\u6210\u5728\u8f93\u5165\u5c42\u6d45\u5c42\uff0c\u5bfc\u81f4\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u5728\u6df1\u5c42\u9010\u6e10\u6d88\u5931\uff0c\u9020\u6210\u6587\u672c\u5d4c\u5165\u548c\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e4b\u95f4\u7684\u65e0\u6548\u9002\u914d", "method": "\u5229\u7528\u73b0\u6210\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u4e30\u5bcc\u7684\u5d4c\u5165\uff0c\u901a\u8fc7\u5c42\u7279\u5b9a\u7684\u5f15\u5bfc\u5411\u91cf\u4e0eLLM\u4e2d\u95f4\u6587\u672c\u8868\u793a\u8fdb\u884c\u878d\u5408\uff0c\u6301\u7eed\u4f18\u5316\u65f6\u95f4\u5e8f\u5217\u548c\u6587\u672c\u6a21\u6001\u7684\u5bf9\u9f50", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747MSE\u964d\u4f4e31.8%\uff0c\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "MSEF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u5728\u6df1\u5c42\u6d88\u5931\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u4e3aLLM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.16574", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16574", "abs": "https://arxiv.org/abs/2508.16574", "authors": ["Yizhi Wang", "Degang Xu", "Yongfang Xie", "Shuzhong Tan", "Xianan Zhou", "Peng Chen"], "title": "Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems", "comment": null, "summary": "This paper presents a hierarchical decision-making framework for autonomous\nnavigation in four-wheel independent steering and driving (4WISD) systems. The\nproposed approach integrates deep reinforcement learning (DRL) for high-level\nnavigation with fuzzy logic for low-level control to ensure both task\nperformance and physical feasibility. The DRL agent generates global motion\ncommands, while the fuzzy logic controller enforces kinematic constraints to\nprevent mechanical strain and wheel slippage. Simulation experiments\ndemonstrate that the proposed framework outperforms traditional navigation\nmethods, offering enhanced training efficiency and stability and mitigating\nerratic behaviors compared to purely DRL-based solutions. Real-world\nvalidations further confirm the framework's ability to navigate safely and\neffectively in dynamic industrial settings. Overall, this work provides a\nscalable and reliable solution for deploying 4WISD mobile robots in complex,\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u9a71\u52a8\u7cfb\u7edf\u7684\u5206\u5c42\u51b3\u7b56\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u7cca\u903b\u8f91\u63a7\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u81ea\u4e3b\u5bfc\u822a", "motivation": "\u89e3\u51b34WISD\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u65e2\u8981\u4fdd\u8bc1\u4efb\u52a1\u6027\u80fd\u53c8\u8981\u6ee1\u8db3\u7269\u7406\u53ef\u884c\u6027\u7684\u6311\u6218\uff0c\u907f\u514d\u7eafDRL\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u7684\u673a\u68b0\u5e94\u53d8\u548c\u8f66\u8f6e\u6253\u6ed1\u95ee\u9898", "method": "\u5206\u5c42\u6846\u67b6\uff1a\u9ad8\u5c42\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u5168\u5c40\u8fd0\u52a8\u6307\u4ee4\uff0c\u4f4e\u5c42\u4f7f\u7528\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\u5f3a\u5236\u6267\u884c\u8fd0\u52a8\u5b66\u7ea6\u675f", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\uff0c\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u3001\u7a33\u5b9a\u6027\u66f4\u597d\uff0c\u51cf\u5c11\u4e86\u5f02\u5e38\u884c\u4e3a\uff1b\u771f\u5b9e\u73af\u5883\u9a8c\u8bc1\u8bc1\u5b9e\u4e86\u5728\u52a8\u6001\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6709\u6548\u5bfc\u822a\u80fd\u529b", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a4WISD\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.15989", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.15989", "abs": "https://arxiv.org/abs/2508.15989", "authors": ["Jiaqi Lin", "Malyaban Bal", "Abhronil Sengupta"], "title": "Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs", "comment": null, "summary": "Equilibrium Propagation (EP) is a biologically inspired local learning rule\nfirst proposed for convergent recurrent neural networks (CRNNs), in which\nsynaptic updates depend only on neuron states from two distinct phases. EP\nestimates gradients that closely align with those computed by Backpropagation\nThrough Time (BPTT) while significantly reducing computational demands,\npositioning it as a potential candidate for on-chip training in neuromorphic\narchitectures. However, prior studies on EP have been constrained to shallow\narchitectures, as deeper networks suffer from the vanishing gradient problem,\nleading to convergence difficulties in both energy minimization and gradient\ncomputation. To address the vanishing gradient problem in deep EP networks, we\npropose a novel EP framework that incorporates intermediate error signals to\nenhance information flow and convergence of neuron dynamics. This is the first\nwork to integrate knowledge distillation and local error signals into EP,\nenabling the training of significantly deeper architectures. Our proposed\napproach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100\ndatasets, showcasing its scalability on deep VGG architectures. These results\nrepresent a significant advancement in the scalability of EP, paving the way\nfor its application in real-world systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u89e3\u51b3\u5e73\u8861\u4f20\u64ad(EP)\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e2d\u95f4\u8bef\u5dee\u4fe1\u53f7\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u6df1\u5c42\u67b6\u6784\u7684\u8bad\u7ec3\uff0c\u5728CIFAR\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u5e73\u8861\u4f20\u64ad(EP)\u4f5c\u4e3a\u751f\u7269\u542f\u53d1\u7684\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff0c\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u9002\u5408\u795e\u7ecf\u5f62\u6001\u67b6\u6784\uff0c\u4f46\u5148\u524d\u7814\u7a76\u4ec5\u9650\u4e8e\u6d45\u5c42\u67b6\u6784\uff0c\u6df1\u5c42\u7f51\u7edc\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u80fd\u91cf\u6700\u5c0f\u5316\u548c\u68af\u5ea6\u8ba1\u7b97\u6536\u655b\u56f0\u96be", "method": "\u63d0\u51fa\u65b0\u9896\u7684EP\u6846\u67b6\uff0c\u6574\u5408\u4e2d\u95f4\u8bef\u5dee\u4fe1\u53f7\u6765\u589e\u5f3a\u4fe1\u606f\u6d41\u548c\u795e\u7ecf\u5143\u52a8\u529b\u5b66\u6536\u655b\uff0c\u9996\u6b21\u5c06\u77e5\u8bc6\u84b8\u998f\u548c\u5c40\u90e8\u8bef\u5dee\u4fe1\u53f7\u96c6\u6210\u5230EP\u4e2d", "result": "\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u6df1\u5ea6VGG\u67b6\u6784\u4e0a\u7684\u53ef\u6269\u5c55\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u663e\u8457\u63a8\u8fdb\u4e86EP\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2508.15801", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15801", "abs": "https://arxiv.org/abs/2508.15801", "authors": ["Seyedali Mohammadi", "Manas Paldhe", "Amit Chhabra"], "title": "LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions", "comment": "10 pages", "summary": "Phone call transcript labeling is prohibitively expensive (approximately 2\nUSD per minute) due to privacy regulations, consent requirements, and manual\nannotation costs requiring 3 hours of expert time per hour of audio. Existing\nextraction methods fail on conversational speech containing disfluencies,\ninterruptions, and speaker overlap. We introduce LingVarBench, a synthetic data\ngeneration pipeline that addresses these constraints through automated\nvalidation. First, we prompt an LLM to generate realistic structured field\nvalues across multiple use cases. Second, we recursively prompt the model to\ntransform these values into thousands of natural conversational utterances\ncontaining typical phone call characteristics. Third, we validate each\nsynthetic utterance by testing whether a separate LLM-based extractor can\nrecover the original structured information. We employ DSPy's SIMBA optimizer\nto automatically synthesize extraction prompts from validated synthetic\ntranscripts, eliminating manual prompt engineering. Our optimized prompts\nachieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent\nzero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for\ndates (vs. 72-77 percent) on real customer transcripts, demonstrating\nsubstantial gains over zero-shot prompting. The synthetic-to-real transfer\ndemonstrates that conversational patterns learned from generated data\ngeneralize effectively to authentic phone calls containing background noise and\ndomain-specific terminology. LingVarBench provides the first systematic\nbenchmark for structured extraction from synthetic conversational data,\ndemonstrating that automated prompt optimization overcomes cost and privacy\nbarriers preventing large-scale phone call analysis in commercial settings.", "AI": {"tldr": "LingVarBench\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u52a8\u9a8c\u8bc1\u751f\u6210\u5305\u542b\u7535\u8bdd\u5bf9\u8bdd\u7279\u5f81\u7684\u5408\u6210\u5bf9\u8bdd\u6570\u636e\uff0c\u5e76\u5229\u7528DSPy\u7684SIMBA\u4f18\u5316\u5668\u81ea\u52a8\u5408\u6210\u63d0\u53d6\u63d0\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u771f\u5b9e\u5ba2\u6237\u901a\u8bdd\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7535\u8bdd\u901a\u8bdd\u8bb0\u5f55\u6807\u6ce8\u6210\u672c\u6781\u9ad8\uff08\u7ea62\u7f8e\u5143/\u5206\u949f\uff09\uff0c\u4e14\u73b0\u6709\u63d0\u53d6\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u4e0d\u6d41\u5229\u3001\u6253\u65ad\u548c\u8bf4\u8bdd\u4eba\u91cd\u53e0\u7684\u5bf9\u8bdd\u8bed\u97f3\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u9690\u79c1\u6cd5\u89c4\u548c\u624b\u52a8\u6807\u6ce8\u6210\u672c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u7535\u8bdd\u901a\u8bdd\u5206\u6790\u3002", "method": "1. \u4f7f\u7528LLM\u751f\u6210\u591a\u4e2a\u7528\u4f8b\u7684\u73b0\u5b9e\u7ed3\u6784\u5316\u5b57\u6bb5\u503c\uff1b2. \u9012\u5f52\u63d0\u793a\u6a21\u578b\u5c06\u8fd9\u4e9b\u503c\u8f6c\u6362\u4e3a\u5305\u542b\u5178\u578b\u7535\u8bdd\u901a\u8bdd\u7279\u5f81\u7684\u81ea\u7136\u5bf9\u8bdd\u8bed\u53e5\uff1b3. \u901a\u8fc7\u6d4b\u8bd5\u5355\u72ec\u7684\u57fa\u4e8eLLM\u7684\u63d0\u53d6\u5668\u662f\u5426\u80fd\u6062\u590d\u539f\u59cb\u7ed3\u6784\u5316\u4fe1\u606f\u6765\u9a8c\u8bc1\u6bcf\u4e2a\u5408\u6210\u8bed\u53e5\uff1b4. \u4f7f\u7528DSPy\u7684SIMBA\u4f18\u5316\u5668\u4ece\u9a8c\u8bc1\u8fc7\u7684\u5408\u6210\u8bb0\u5f55\u4e2d\u81ea\u52a8\u5408\u6210\u63d0\u53d6\u63d0\u793a\u3002", "result": "\u4f18\u5316\u540e\u7684\u63d0\u793a\u5728\u771f\u5b9e\u5ba2\u6237\u8bb0\u5f55\u4e2d\u8fbe\u5230\uff1a\u6570\u5b57\u5b57\u6bb595%\u51c6\u786e\u7387\uff08\u96f6\u6837\u672c\u4e3a88-89%\uff09\u3001\u59d3\u540d90%\u51c6\u786e\u7387\uff08\u96f6\u6837\u672c\u4e3a47-79%\uff09\u3001\u65e5\u671f\u8d85\u8fc780%\u51c6\u786e\u7387\uff08\u96f6\u6837\u672c\u4e3a72-77%\uff09\u3002\u5408\u6210\u5230\u771f\u5b9e\u7684\u8fc1\u79fb\u8868\u660e\u4ece\u751f\u6210\u6570\u636e\u4e2d\u5b66\u5230\u7684\u5bf9\u8bdd\u6a21\u5f0f\u80fd\u6709\u6548\u6cdb\u5316\u5230\u5305\u542b\u80cc\u666f\u566a\u58f0\u548c\u9886\u57df\u7279\u5b9a\u672f\u8bed\u7684\u771f\u5b9e\u7535\u8bdd\u901a\u8bdd\u3002", "conclusion": "LingVarBench\u4e3a\u4ece\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u4e2d\u8fdb\u884c\u7ed3\u6784\u5316\u63d0\u53d6\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u8bc1\u660e\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u514b\u670d\u4e86\u5546\u4e1a\u73af\u5883\u4e2d\u5927\u89c4\u6a21\u7535\u8bdd\u901a\u8bdd\u5206\u6790\u7684\u6210\u672c\u548c\u9690\u79c1\u969c\u788d\u3002"}}
{"id": "2508.16157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16157", "abs": "https://arxiv.org/abs/2508.16157", "authors": ["Pi-Wei Chen", "Jerry Chun-Wei Lin", "Wei-Han Chen", "Jia Ji", "Zih-Ching Chen", "Feng-Hao Yeh", "Chao-Chun Chen"], "title": "Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection", "comment": null, "summary": "Pre-trained Vision-Language Models (VLMs) have recently shown promise in\ndetecting anomalies. However, previous approaches are fundamentally limited by\ntheir reliance on human-designed prompts and the lack of accessible anomaly\nsamples, leading to significant gaps in context-specific anomaly understanding.\nIn this paper, we propose \\textbf{A}daptive \\textbf{P}rompt \\textbf{T}uning\nwith semantic alignment for anomaly detection (APT), a groundbreaking prior\nknowledge-free, few-shot framework and overcomes the limitations of traditional\nprompt-based approaches. APT uses self-generated anomaly samples with noise\nperturbations to train learnable prompts that capture context-dependent\nanomalies in different scenarios. To prevent overfitting to synthetic noise, we\npropose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively\naligns the prompts with general anomaly semantics while incorporating diverse\nsynthetic anomaly. Our system not only advances pixel-wise anomaly detection,\nbut also achieves state-of-the-art performance on multiple benchmark datasets\nwithout requiring prior knowledge for prompt crafting, establishing a robust\nand versatile solution for real-world anomaly detection.", "AI": {"tldr": "APT\u662f\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u8c03\u4f18\u548c\u8bed\u4e49\u5bf9\u9f50\u6765\u514b\u670d\u4f20\u7edf\u57fa\u4e8e\u63d0\u793a\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u4e14\u7f3a\u4e4f\u5f02\u5e38\u6837\u672c\uff0c\u5bfc\u81f4\u5728\u7279\u5b9a\u4e0a\u4e0b\u6587\u5f02\u5e38\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "method": "\u4f7f\u7528\u566a\u58f0\u6270\u52a8\u751f\u6210\u7684\u81ea\u5f02\u5e38\u6837\u672c\u6765\u8bad\u7ec3\u53ef\u5b66\u4e60\u63d0\u793a\u8bcd\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u4f18\u5316\u5143\u63d0\u793a\u5f15\u5bfc\u65b9\u6848(SMGS)\u6765\u9632\u6b62\u5bf9\u5408\u6210\u566a\u58f0\u7684\u8fc7\u62df\u5408", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u63d0\u793a\u8bcd\u8bbe\u8ba1", "conclusion": "APT\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u63a8\u8fdb\u4e86\u50cf\u7d20\u7ea7\u5f02\u5e38\u68c0\u6d4b\uff0c\u8fd8\u5efa\u7acb\u4e86\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u7a33\u5065\u6846\u67b6"}}
{"id": "2508.16158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16158", "abs": "https://arxiv.org/abs/2508.16158", "authors": ["Haodong He", "Yancheng Bai", "Rui Lan", "Xu Duan", "Lei Sun", "Xiangxiang Chu", "Gui-Song Xia"], "title": "RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution", "comment": null, "summary": "The rich textual information of large vision-language models (VLMs) combined\nwith the powerful generative prior of pre-trained text-to-image (T2I) diffusion\nmodels has achieved impressive performance in single-image super-resolution\n(SISR). However, existing methods still face significant challenges in\ngenerating clear and accurate regional details, particularly in scenarios\ninvolving multiple objects. This challenge primarily stems from a lack of\nfine-grained regional descriptions and the models' insufficient ability to\ncapture complex prompts. To address these limitations, we propose a Regional\nAttention Guided Super-Resolution (RAGSR) method that explicitly extracts\nlocalized fine-grained information and effectively encodes it through a novel\nregional attention mechanism, enabling both enhanced detail and overall\nvisually coherent SR results. Specifically, RAGSR localizes object regions in\nan image and assigns fine-grained caption to each region, which are formatted\nas region-text pairs as textual priors for T2I models. A regional guided\nattention is then leveraged to ensure that each region-text pair is properly\nconsidered in the attention process while preventing unwanted interactions\nbetween unrelated region-text pairs. By leveraging this attention mechanism,\nour approach offers finer control over the integration of text and image\ninformation, thereby effectively overcoming limitations faced by traditional\nSISR techniques. Experimental results on benchmark datasets demonstrate that\nour approach exhibits superior performance in generating perceptually authentic\nvisual details while maintaining contextual consistency compared to existing\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAGSR\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u57df\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u591a\u5bf9\u8c61\u573a\u666f\u4e0b\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u533a\u57df\u7ec6\u8282\u751f\u6210\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7ec4\u5408\u6587\u672c\u5230\u56fe\u50cf\u6e17\u900f\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u5355\u56fe\u8d85\u5206\u8fa8\u7387\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u5bf9\u8c61\u573a\u666f\u4e0b\u751f\u6210\u6e05\u6670\u51c6\u786e\u7684\u533a\u57df\u7ec6\u8282\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u533a\u57df\u63cf\u8ff0\u548c\u6a21\u578b\u5bf9\u590d\u6742\u63d0\u793a\u7684\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u533a\u57df\u6ce8\u610f\u5bfc\u5411\u8d85\u5206\u8fa8\u7387(RAGSR)\u65b9\u6cd5\uff1a1)\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\u533a\u57df\u5e76\u4e3a\u6bcf\u4e2a\u533a\u57df\u5206\u914d\u7ec6\u7c92\u5ea6\u6807\u9898\uff0c\u5f62\u6210\u533a\u57df-\u6587\u672c\u5bf9\u4f5c\u4e3aT2I\u6a21\u578b\u7684\u6587\u672c\u5148\u9a8c\u77e5\u8bc6\uff1b2)\u91c7\u7528\u65b0\u7684\u533a\u57df\u5bfc\u5411\u6ce8\u610f\u673a\u5236\uff0c\u786e\u4fdd\u6bcf\u4e2a\u533a\u57df-\u6587\u672c\u5bf9\u5728\u6ce8\u610f\u8fc7\u7a0b\u4e2d\u5f97\u5230\u9002\u5f53\u8003\u8651\uff0c\u540c\u65f6\u9632\u6b62\u4e0d\u76f8\u5173\u533a\u57df\u95f4\u7684\u4e0d\u5e94\u6709\u4ea4\u4e92\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u611f\u77e5\u771f\u5b9e\u7684\u89c6\u89c9\u7ec6\u8282\u65f6\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RAGSR\u901a\u8fc7\u660e\u786e\u63d0\u53d6\u5c40\u90e8\u7ec6\u7c92\u5ea6\u4fe1\u606f\u5e76\u901a\u8fc7\u533a\u57df\u6ce8\u610f\u673a\u5236\u8fdb\u884c\u6709\u6548\u7f16\u7801\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u5347\u7ec6\u8282\u548c\u6574\u4f53\u89c6\u89c9\u534f\u8c03\u6027\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfSISR\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.16336", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16336", "abs": "https://arxiv.org/abs/2508.16336", "authors": ["Jin Li", "Kleanthis Malialis", "Stelios G. Vrachimis", "Marios M. Polycarpou"], "title": "Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks", "comment": "This paper is accepted by the 6th International Conference on Control\n  and Fault-Tolerant Systems (SysTol)", "summary": "Water Distribution Networks (WDNs), critical to public well-being and\neconomic stability, face challenges such as pipe blockages and background\nleakages, exacerbated by operational constraints such as data non-stationarity\nand limited labeled data. This paper proposes an unsupervised, online learning\nframework that aims to detect two types of faults in WDNs: pipe blockages,\nmodeled as collective anomalies, and background leakages, modeled as concept\ndrift. Our approach combines a Long Short-Term Memory Variational Autoencoder\n(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and\nadaptation under non-stationary conditions. Its lightweight, memory-efficient\ndesign enables real-time, edge-level monitoring. Experiments on two realistic\nWDNs show that the proposed approach consistently outperforms strong baselines\nin detecting anomalies and adapting to recurrent drift, demonstrating its\neffectiveness in unsupervised event detection for dynamic WDN environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM-VAE\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4f9b\u6c34\u7ba1\u7f51\u4e2d\u7684\u7ba1\u9053\u5835\u585e\uff08\u96c6\u4f53\u5f02\u5e38\uff09\u548c\u80cc\u666f\u6cc4\u6f0f\uff08\u6982\u5ff5\u6f02\u79fb\uff09\uff0c\u5728\u975e\u5e73\u7a33\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u3002", "motivation": "\u4f9b\u6c34\u7ba1\u7f51\u5bf9\u516c\u5171\u798f\u7949\u548c\u7ecf\u6d4e\u7a33\u5b9a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u7ba1\u9053\u5835\u585e\u548c\u80cc\u666f\u6cc4\u6f0f\u7b49\u6311\u6218\uff0c\u4e14\u5b58\u5728\u6570\u636e\u975e\u5e73\u7a33\u6027\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7b49\u64cd\u4f5c\u7ea6\u675f\u3002", "method": "\u7ed3\u5408\u957f\u77ed\u671f\u8bb0\u5fc6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08LSTM-VAE\uff09\u4e0e\u53cc\u91cd\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u3001\u5185\u5b58\u9ad8\u6548\u7684\u8bbe\u8ba1\u5b9e\u73b0\u5b9e\u65f6\u8fb9\u7f18\u7ea7\u76d1\u63a7\u3002", "result": "\u5728\u4e24\u4e2a\u73b0\u5b9e\u4f9b\u6c34\u7ba1\u7f51\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5f02\u5e38\u548c\u9002\u5e94\u5faa\u73af\u6f02\u79fb\u65b9\u9762 consistently \u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u4f9b\u6c34\u7ba1\u7f51\u73af\u5883\u4e2d\u8fdb\u884c\u65e0\u76d1\u7763\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u5e73\u7a33\u6761\u4ef6\u4e0b\u7684\u6545\u969c\u68c0\u6d4b\u3002"}}
{"id": "2508.16420", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16420", "abs": "https://arxiv.org/abs/2508.16420", "authors": ["Yue Pei", "Hongming Zhang", "Chao Gao", "Martin M\u00fcller", "Mengxiao Zhu", "Hao Sheng", "Haogang Zhu", "Liang Lin"], "title": "Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) has achieved significant advances in\ndomains such as robotic control, autonomous driving, and medical\ndecision-making. Most existing methods primarily focus on training policies\nthat maximize cumulative returns from a given dataset. However, many real-world\napplications require precise control over policy performance levels, rather\nthan simply pursuing the best possible return. Reinforcement learning via\nsupervised learning (RvS) frames offline RL as a sequence modeling task,\nenabling the extraction of diverse policies by conditioning on different\ndesired returns. Yet, existing RvS-based transformers, such as Decision\nTransformer (DT), struggle to reliably align the actual achieved returns with\nspecified target returns, especially when interpolating within underrepresented\nreturns or extrapolating beyond the dataset. To address this limitation, we\npropose Doctor, a novel approach that Double Checks the Transformer with target\nalignment for Offline RL. Doctor achieves superior target alignment both within\nand beyond the dataset, while enabling accurate and flexible control over\npolicy performance. Notably, on the dynamic treatment regime benchmark,\nEpiCare, our approach effectively modulates treatment policy aggressiveness,\nbalancing therapeutic returns against adverse event risk.", "AI": {"tldr": "\u63d0\u51fa\u4e86Doctor\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u68c0\u67e5\u673a\u5236\u6539\u8fdb\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u76ee\u6807\u56de\u62a5\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u7b56\u7565\u6027\u80fd\u63a7\u5236", "motivation": "\u73b0\u6709RvS\u65b9\u6cd5\u5982Decision Transformer\u5728\u76ee\u6807\u56de\u62a5\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u96c6\u5185\u63d2\u503c\u548c\u5916\u63a8\u65f6\u96be\u4ee5\u53ef\u9760\u5730\u5b9e\u73b0\u6307\u5b9a\u56de\u62a5", "method": "Doctor\u65b9\u6cd5\u91c7\u7528\u53cc\u91cd\u68c0\u67e5\u673a\u5236\uff0c\u901a\u8fc7\u76ee\u6807\u5bf9\u9f50\u6765\u76d1\u7763Transformer\uff0c\u786e\u4fdd\u5b9e\u9645\u56de\u62a5\u4e0e\u6307\u5b9a\u76ee\u6807\u56de\u62a5\u7cbe\u786e\u5339\u914d", "result": "\u5728\u52a8\u6001\u6cbb\u7597\u673a\u5236\u57fa\u51c6\u6d4b\u8bd5EpiCare\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u8c03\u8282\u6cbb\u7597\u7b56\u7565\u7684\u6fc0\u8fdb\u7a0b\u5ea6\uff0c\u5e73\u8861\u6cbb\u7597\u6548\u679c\u4e0e\u4e0d\u826f\u4e8b\u4ef6\u98ce\u9669", "conclusion": "Doctor\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u76ee\u6807\u56de\u62a5\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u6027\u80fd\u63a7\u5236\u80fd\u529b"}}
{"id": "2508.15853", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15853", "abs": "https://arxiv.org/abs/2508.15853", "authors": ["Xuwen Yang"], "title": "MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr", "comment": "12 pages, 5figures", "summary": "End-to-end ASR models, despite their success on benchmarks, often pro-duce\ncatastrophic semantic errors in noisy environments. We attribute this fragility\nto the prevailing 'direct mapping' objective, which solely penalizes final\noutput errors while leaving the model's internal computational pro-cess\nunconstrained. To address this, we introduce the Multi-Granularity Soft\nConsistency (MGSC) framework, a model-agnostic, plug-and-play module that\nenforces internal self-consistency by simultaneously regulariz-ing macro-level\nsentence semantics and micro-level token alignment. Cru-cially, our work is the\nfirst to uncover a powerful synergy between these two consistency\ngranularities: their joint optimization yields robustness gains that\nsignificantly surpass the sum of their individual contributions. On a public\ndataset, MGSC reduces the average Character Error Rate by a relative 8.7%\nacross diverse noise conditions, primarily by preventing se-vere\nmeaning-altering mistakes. Our work demonstrates that enforcing in-ternal\nconsistency is a crucial step towards building more robust and trust-worthy AI.", "AI": {"tldr": "\u63d0\u51faMGSC\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u6b63\u5219\u5316\u5b8f\u89c2\u53e5\u5b50\u8bed\u4e49\u548c\u5fae\u89c2\u8bcd\u5143\u5bf9\u9f50\u6765\u589e\u5f3aASR\u6a21\u578b\u7684\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u663e\u8457\u51cf\u5c11\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u9519\u8bef", "motivation": "\u7aef\u5230\u7aefASR\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u707e\u96be\u6027\u8bed\u4e49\u9519\u8bef\uff0c\u73b0\u6709\u76f4\u63a5\u6620\u5c04\u76ee\u6807\u53ea\u60e9\u7f5a\u6700\u7ec8\u8f93\u51fa\u9519\u8bef\uff0c\u7f3a\u4e4f\u5bf9\u5185\u90e8\u8ba1\u7b97\u8fc7\u7a0b\u7684\u7ea6\u675f", "method": "\u5f15\u5165\u591a\u7c92\u5ea6\u8f6f\u4e00\u81f4\u6027(MGSC)\u6846\u67b6\uff0c\u540c\u65f6\u6b63\u5219\u5316\u5b8f\u89c2\u53e5\u5b50\u8bed\u4e49\u548c\u5fae\u89c2\u8bcd\u5143\u5bf9\u9f50\uff0c\u53d1\u73b0\u4e24\u79cd\u7c92\u5ea6\u4e00\u81f4\u6027\u8054\u5408\u4f18\u5316\u7684\u534f\u540c\u6548\u5e94", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cMGSC\u5728\u4e0d\u540c\u566a\u58f0\u6761\u4ef6\u4e0b\u5e73\u5747\u5b57\u7b26\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e8.7%\uff0c\u4e3b\u8981\u9632\u6b62\u4e86\u4e25\u91cd\u7684\u8bed\u4e49\u6539\u53d8\u9519\u8bef", "conclusion": "\u5f3a\u5236\u5185\u90e8\u4e00\u81f4\u6027\u662f\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u53ef\u4fe1AI\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u591a\u7c92\u5ea6\u4e00\u81f4\u6027\u4f18\u5316\u5177\u6709\u5f3a\u5927\u7684\u534f\u540c\u6548\u5e94"}}
{"id": "2508.16270", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16270", "abs": "https://arxiv.org/abs/2508.16270", "authors": ["Vira Pyrih", "Adrian Rebmann", "Han van der Aa"], "title": "LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining", "comment": "Accepted at IEEE ICPM 2025, 8 pages, 2 figures", "summary": "Process mining is increasingly using textual information associated with\nevents to tackle tasks such as anomaly detection and process discovery. Such\nsemantics-aware process mining focuses on what behavior should be possible in a\nprocess (i.e., expectations), thus providing an important complement to\ntraditional, frequency-based techniques that focus on recorded behavior (i.e.,\nreality). Large Language Models (LLMs) provide a powerful means for tackling\nsemantics-aware tasks. However, the best performance is so far achieved through\ntask-specific fine-tuning, which is computationally intensive and results in\nmodels that can only handle one specific task. To overcome this lack of\ngeneralization, we use this paper to investigate the potential of\ninstruction-tuning for semantics-aware process mining. The idea of\ninstruction-tuning here is to expose an LLM to prompt-answer pairs for\ndifferent tasks, e.g., anomaly detection and next-activity prediction, making\nit more familiar with process mining, thus allowing it to also perform better\nat unseen tasks, such as process discovery. Our findings demonstrate a varied\nimpact of instruction-tuning: while performance considerably improved on\nprocess discovery and prediction tasks, it varies across models on anomaly\ndetection tasks, highlighting that the selection of tasks for\ninstruction-tuning is critical to achieving desired outcomes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6307\u4ee4\u5fae\u8c03\u5728\u8bed\u4e49\u611f\u77e5\u8fc7\u7a0b\u6316\u6398\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u5f71\u54cd\u5b58\u5728\u5dee\u5f02\uff1a\u5728\u8fc7\u7a0b\u53d1\u73b0\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02", "motivation": "\u4f20\u7edf\u8fc7\u7a0b\u6316\u6398\u4e3b\u8981\u57fa\u4e8e\u9891\u7387\u5206\u6790\uff0c\u800c\u8bed\u4e49\u611f\u77e5\u8fc7\u7a0b\u6316\u6398\u5173\u6ce8\u8fc7\u7a0b\u5e94\u6709\u7684\u884c\u4e3a\u671f\u671b\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u80fd\u5904\u7406\u8bed\u4e49\u611f\u77e5\u4efb\u52a1\uff0c\u4f46\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b", "method": "\u91c7\u7528\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\uff0c\u5c06LLM\u66b4\u9732\u4e8e\u4e0d\u540c\u8fc7\u7a0b\u6316\u6398\u4efb\u52a1\uff08\u5982\u5f02\u5e38\u68c0\u6d4b\u548c\u4e0b\u4e00\u6d3b\u52a8\u9884\u6d4b\uff09\u7684\u63d0\u793a-\u7b54\u6848\u5bf9\uff0c\u4f7f\u5176\u66f4\u719f\u6089\u8fc7\u7a0b\u6316\u6398\u9886\u57df\uff0c\u4ece\u800c\u63d0\u5347\u5728\u672a\u89c1\u4efb\u52a1\uff08\u5982\u8fc7\u7a0b\u53d1\u73b0\uff09\u4e0a\u7684\u6027\u80fd", "result": "\u6307\u4ee4\u5fae\u8c03\u5bf9\u8fc7\u7a0b\u53d1\u73b0\u548c\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6548\u679c\u56e0\u6a21\u578b\u9009\u62e9\u800c\u6709\u6240\u4e0d\u540c\uff0c\u8868\u660e\u6307\u4ee4\u5fae\u8c03\u4efb\u52a1\u7684\u9009\u62e9\u5bf9\u6700\u7ec8\u7ed3\u679c\u81f3\u5173\u91cd\u8981", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u662f\u63d0\u5347LLM\u5728\u8bed\u4e49\u611f\u77e5\u8fc7\u7a0b\u6316\u6398\u4e2d\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u9009\u62e9\u7528\u4e8e\u5fae\u8c03\u7684\u4efb\u52a1\u7ec4\u5408\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd"}}
