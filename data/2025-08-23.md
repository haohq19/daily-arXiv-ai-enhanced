<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VideoEraser: Concept Erasure in Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.15314)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Zhi Chen,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CV

TL;DR: VideoEraser是一个无需训练的即插即用框架，通过选择性提示嵌入调整和抗干扰噪声引导两阶段方法，有效防止文本到视频扩散模型生成有害内容，在四个任务上平均减少46%的不良内容生成。


<details>
  <summary>Details</summary>
Motivation: 文本到视频扩散模型的快速发展引发了隐私、版权和安全方面的担忧，这些模型可能被滥用来生成有害或误导性内容，因为它们通常在包含未经授权的个人身份、艺术创作和有害材料的数据集上训练。

Method: 提出了VideoEraser框架，采用两阶段方法：1）选择性提示嵌入调整（SPEA）和2）抗干扰噪声引导（ARNG）。这是一个即插即用的模块，可以无缝集成到代表性的文本到视频扩散模型中。

Result: 在四个任务（物体擦除、艺术风格擦除、名人擦除和露骨内容擦除）上进行了广泛评估，实验结果显示VideoEraser在效能、完整性、保真度、鲁棒性和泛化性方面始终优于先前的方法。

Conclusion: VideoEraser在抑制文本到视频生成过程中的不良内容方面达到了最先进的性能，与基线相比在四个任务上平均减少了46%的不良内容生成，为解决扩散模型的安全问题提供了有效解决方案。

Abstract: The rapid growth of text-to-video (T2V) diffusion models has raised concerns
about privacy, copyright, and safety due to their potential misuse in
generating harmful or misleading content. These models are often trained on
numerous datasets, including unauthorized personal identities, artistic
creations, and harmful materials, which can lead to uncontrolled production and
distribution of such content. To address this, we propose VideoEraser, a
training-free framework that prevents T2V diffusion models from generating
videos with undesirable concepts, even when explicitly prompted with those
concepts. Designed as a plug-and-play module, VideoEraser can seamlessly
integrate with representative T2V diffusion models via a two-stage process:
Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise
Guidance (ARNG). We conduct extensive evaluations across four tasks, including
object erasure, artistic style erasure, celebrity erasure, and explicit content
erasure. Experimental results show that VideoEraser consistently outperforms
prior methods regarding efficacy, integrity, fidelity, robustness, and
generalizability. Notably, VideoEraser achieves state-of-the-art performance in
suppressing undesirable content during T2V generation, reducing it by 46% on
average across four tasks compared to baselines.

</details>


### [2] [Spiking Variational Graph Representation Inference for Video Summarization](https://arxiv.org/abs/2508.15389)
*Wenrui Li,Wei Han,Liang-Jian Deng,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出SpiVG网络，结合脉冲神经网络和变分推理，解决视频摘要中的时序依赖和语义连贯性问题，在多个数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 短视频内容兴起需要高效的关键信息提取技术，但现有方法难以捕捉全局时序依赖、保持语义连贯性，且多通道特征融合易受噪声影响

Method: 基于脉冲神经网络的关键帧提取器+动态聚合图推理器+变分推理重建模块，使用ELBO优化捕获多通道特征分布的潜在结构

Result: 在SumMe、TVSum、VideoXum、QFVS等多个数据集上超越现有方法

Conclusion: SpiVG网络通过结合SNN的事件驱动计算和变分推理，有效提升了视频摘要的信息密度和语义连贯性，同时降低了计算复杂度

Abstract: With the rise of short video content, efficient video summarization
techniques for extracting key information have become crucial. However,
existing methods struggle to capture the global temporal dependencies and
maintain the semantic coherence of video content. Additionally, these methods
are also influenced by noise during multi-channel feature fusion. We propose a
Spiking Variational Graph (SpiVG) Network, which enhances information density
and reduces computational complexity. First, we design a keyframe extractor
based on Spiking Neural Networks (SNN), leveraging the event-driven computation
mechanism of SNNs to learn keyframe features autonomously. To enable
fine-grained and adaptable reasoning across video frames, we introduce a
Dynamic Aggregation Graph Reasoner, which decouples contextual object
consistency from semantic perspective coherence. We present a Variational
Inference Reconstruction Module to address uncertainty and noise arising during
multi-channel feature fusion. In this module, we employ Evidence Lower Bound
Optimization (ELBO) to capture the latent structure of multi-channel feature
distributions, using posterior distribution regularization to reduce
overfitting. Experimental results show that SpiVG surpasses existing methods
across multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes
and pre-trained models are available at https://github.com/liwrui/SpiVG.

</details>


### [3] [Aligning Moments in Time using Video Queries](https://arxiv.org/abs/2508.15439)
*Yogesh Kumar,Uday Agarwal,Manish Gupta,Anand Mishra*

Main category: cs.CV

TL;DR: MATR是一个基于Transformer的视频到视频时刻检索模型，通过双阶段序列对齐和自监督预训练技术，在ActivityNet-VRL和SportsMoments数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频到视频时刻检索任务面临语义帧级对齐和复杂依赖关系建模的挑战，需要开发能够捕获语义上下文和时序细节的精确定位方法。

Method: 提出MATR模型，使用基于Transformer的双阶段序列对齐来编码查询视频和目标视频之间的相关性，结合前景/背景分类和边界预测头，并采用自监督预训练技术进行任务特定初始化。

Result: 在ActivityNet-VRL数据集上R@1提升13.1%，mIoU提升8.1%；在新提出的SportsMoments数据集上R@1提升14.7%，mIoU提升14.4%。

Conclusion: MATR通过有效的序列对齐和自监督预训练，在视频到视频时刻检索任务上取得了显著的性能提升，证明了其在捕获语义匹配和时序细节方面的有效性。

Abstract: Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen
events or moments in a target video using a query video. This task poses
several challenges, such as the need for semantic frame-level alignment and
modeling complex dependencies between query and target videos. To tackle this
challenging problem, we introduce MATR (Moment Alignment TRansformer), a
transformer-based model designed to capture semantic context as well as the
temporal details necessary for precise moment localization. MATR conditions
target video representations on query video features using dual-stage sequence
alignment that encodes the required correlations and dependencies. These
representations are then used to guide foreground/background classification and
boundary prediction heads, enabling the model to accurately identify moments in
the target video that semantically match with the query video. Additionally, to
provide a strong task-specific initialization for MATR, we propose a
self-supervised pre-training technique that involves training the model to
localize random clips within videos. Extensive experiments demonstrate that
MATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU
on an absolute scale compared to state-of-the-art methods on the popular
ActivityNet-VRL dataset. Additionally, on our newly proposed dataset,
SportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an
absolute scale over strong baselines.

</details>


### [4] [When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding](https://arxiv.org/abs/2508.15641)
*Pengcheng Fang,Yuxia Chen,Rui Guo*

Main category: cs.CV

TL;DR: Grounded VideoDiT是一个视频大语言模型，通过扩散时间潜在编码器、对象接地表示和混合令牌方案，显著提升了视频时间感知和实体交互理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM在时间感知方面存在不足：时间戳编码不明确、帧级特征连续性差、语言视觉对齐漂移。需要提升细粒度时间推理和实体定位能力。

Method: 1. 扩散时间潜在(DTL)编码器增强边界敏感性和时间一致性；2. 对象接地表示将查询实体显式绑定到局部视觉证据；3. 混合令牌方案使用离散时间令牌进行显式时间戳建模

Result: 在Charades STA、NExT GQA和多个VideoQA基准测试中取得了最先进的性能，验证了其强大的接地能力

Conclusion: Grounded VideoDiT通过三项关键创新成功解决了视频LLM的时间感知局限性，为细粒度视频理解提供了有效解决方案

Abstract: Understanding videos requires more than answering open ended questions, it
demands the ability to pinpoint when events occur and how entities interact
across time. While recent Video LLMs have achieved remarkable progress in
holistic reasoning, they remain coarse in temporal perception: timestamps are
encoded only implicitly, frame level features are weak in capturing continuity,
and language vision alignment often drifts from the entities of interest. In
this paper, we present Grounded VideoDiT, a Video LLM designed to overcome
these limitations by introducing three key innovations. First, a Diffusion
Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains
temporal consistency. Second, object grounded representations explicitly bind
query entities to localized visual evidence, strengthening alignment. Third, a
mixed token scheme with discrete temporal tokens provides explicit timestamp
modeling, enabling fine grained temporal reasoning. Together, these designs
equip Grounded VideoDiT with robust grounding capabilities, as validated by
state of the art results on Charades STA, NExT GQA, and multiple VideoQA
benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving](https://arxiv.org/abs/2508.14926)
*Dianzhao Li,Ostap Okhrin*

Main category: cs.LG

TL;DR: 提出了一种结合伦理考量的分层安全强化学习框架，用于自动驾驶车辆的道德决策，在真实交通场景中优于基线方法


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要嵌入稳健的伦理推理来减少交通事故和提高运输效率，但其广泛采用依赖于将道德考量与标准驾驶目标相结合

Method: 分层安全强化学习框架：决策层使用包含碰撞概率和伤害严重性的复合伦理风险成本训练Safe RL智能体；执行层采用多项式路径规划结合PID和Stanley控制器；动态优先经验回放机制强化高风险事件学习

Result: 在包含多样化车辆、骑行者和行人的真实交通数据集上验证，在降低伦理风险和保持驾驶性能方面优于基线方法

Conclusion: 这是首个通过安全强化学习在真实场景中研究自动驾驶车辆道德决策的工作，展示了结合形式控制理论和数据驱动学习在复杂人车混合交通环境中推进伦理责任自主的潜力

Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and
improving transportation efficiency, yet their widespread adoption hinges on
embedding robust ethical reasoning into routine and emergency maneuvers. Here,
we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that
explicitly integrates moral considerations with standard driving objectives. At
the decision level, a Safe RL agent is trained using a composite ethical risk
cost, combining collision probability and harm severity, to generate high-level
motion targets. A dynamic Prioritized Experience Replay mechanism amplifies
learning from rare but critical, high-risk events. At the execution level,
polynomial path planning coupled with Proportional-Integral-Derivative (PID)
and Stanley controllers translates these targets into smooth, feasible
trajectories, ensuring both accuracy and comfort. We train and validate our
approach on rich, real-world traffic datasets encompassing diverse vehicles,
cyclists, and pedestrians, and demonstrate that it outperforms baseline methods
in reducing ethical risk and maintaining driving performance. To our knowledge,
this is the first study of ethical decision-making for autonomous vehicles via
Safe RL in real-world scenarios. Our results highlight the potential of
combining formal control theory and data-driven learning to advance ethically
accountable autonomy in complex, human-mixed traffic environments.

</details>


### [6] [Wormhole Dynamics in Deep Neural Networks](https://arxiv.org/abs/2508.15086)
*Yen-Lung Lai,Zhe Jin*

Main category: cs.LG

TL;DR: 本文研究了深度神经网络(DNN)的泛化行为，特别是'愚弄样本'现象，提出了基于最大似然估计的分析框架，发现了过参数化DNN中的特征空间坍塌现象，并提出了'虫洞'解决方案来绕过退化问题。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络在面对看似随机的输入时为何会产生高置信度分类的'愚弄样本'现象，探索DNN泛化行为的理论基础。

Method: 采用基于最大似然估计的分析框架，避免依赖传统的基于梯度的优化方法和显式标签，分析过参数化DNN的特征空间行为。

Result: 发现DNN在过参数化状态下会出现特征空间坍塌，虽然有助于泛化但增加层数会导致退化状态，模型将不同输入映射到相同输出。提出的'虫洞'解决方案能够绕过这种退化。

Conclusion: 研究为理解DNN泛化提供了新视角，揭示了短路学习的机制，并为无监督学习动态的未来研究指明了方向，有助于弥合理论与实践之间的差距。

Abstract: This work investigates the generalization behavior of deep neural networks
(DNNs), focusing on the phenomenon of "fooling examples," where DNNs
confidently classify inputs that appear random or unstructured to humans. To
explore this phenomenon, we introduce an analytical framework based on maximum
likelihood estimation, without adhering to conventional numerical approaches
that rely on gradient-based optimization and explicit labels. Our analysis
reveals that DNNs operating in an overparameterized regime exhibit a collapse
in the output feature space. While this collapse improves network
generalization, adding more layers eventually leads to a state of degeneracy,
where the model learns trivial solutions by mapping distinct inputs to the same
output, resulting in zero loss. Further investigation demonstrates that this
degeneracy can be bypassed using our newly derived "wormhole" solution. The
wormhole solution, when applied to arbitrary fooling examples, reconciles
meaningful labels with random ones and provides a novel perspective on shortcut
learning. These findings offer deeper insights into DNN generalization and
highlight directions for future research on learning dynamics in unsupervised
settings to bridge the gap between theory and practice.

</details>


### [7] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: 本文研究了一种输入驱动的单状态微分方程模型，该模型最初为实验验证的动态分子开关开发，具有类似大脑突触的切换特性。模型具有精确可解性、收敛性和衰减记忆等数学特性，能够稳定处理时序数据。


<details>
  <summary>Details</summary>
Motivation: 研究动态分子开关的计算特性，探索其作为神经形态计算单元的理论基础，旨在开发具有生物启发行为且数学性质稳定的计算模型。

Method: 采用线性状态、非线性输入的微分方程模型，通过数学分析证明模型的精确可解性、收敛性和衰减记忆特性。

Result: 模型展现出生物启发行为与稳定数学特性的共存，能够稳定处理时序输入，为动态分子开关在深度前馈和循环架构中的应用提供理论支持。

Conclusion: 该研究为动态分子开关作为神经形态计算单元提供了理论基础，并可能启发更多精确可解模型来模拟物理设备，实现脑启发行为和稳定计算。

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


### [8] [Classification errors distort findings in automated speech processing: examples and solutions from child-development research](https://arxiv.org/abs/2508.15637)
*Lucas Gautheron,Evan Kidd,Anton Malko,Marvin Lavechin,Alejandrina Cristia*

Main category: cs.LG

TL;DR: 这篇论文探讨了自动化分类器错误对儿童语言获得研究统计推断的影响，并提出了一种贝叶斯校准方法来应对这些偏差问题。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴录音设备的普及，科学家越来越多地使用自动化分析方法来研究儿童的语言获得。然而，当前研究少有关注分类错误对最终统计推断的下游影响。

Method: 采用贝叶斯方法来研究算法错误对关键科学问题的影响，包括兄弟姐妹对儿童语言经验的影响以及儿童语言输入与产出的关联性。研究涉及LENA系统和开源语音分类器。

Result: 发现分类错误会显著扭曲估计结果，比如自动注释会低估兄弟对成人语言输入的负面影响20-80%，可能导致统计显著性超出门槛。贝叶斯校准方法能够有效恢复无偏异的效应量估计，但不是万能的解决方案。

Conclusion: 分类器错误会对科学研究的统计推断产生重大影响，这种问题适用于任何包含事件检测和分类且错误率非零的分类器。贝叶斯校准方法为减少这些偏差提供了有效的工具，但研究人员仍需要认识到其局限性。

Abstract: With the advent of wearable recorders, scientists are increasingly turning to
automated methods of analysis of audio and video data in order to measure
children's experience, behavior, and outcomes, with a sizable literature
employing long-form audio-recordings to study language acquisition. While
numerous articles report on the accuracy and reliability of the most popular
automated classifiers, less has been written on the downstream effects of
classification errors on measurements and statistical inferences (e.g., the
estimate of correlations and effect sizes in regressions). This paper proposes
a Bayesian approach to study the effects of algorithmic errors on key
scientific questions, including the effect of siblings on children's language
experience and the association between children's production and their input.
In both the most commonly used \gls{lena}, and an open-source alternative (the
Voice Type Classifier from the ACLEW system), we find that classification
errors can significantly distort estimates. For instance, automated annotations
underestimated the negative effect of siblings on adult input by 20--80\%,
potentially placing it below statistical significance thresholds. We further
show that a Bayesian calibration approach for recovering unbiased estimates of
effect sizes can be effective and insightful, but does not provide a fool-proof
solution. Both the issue reported and our solution may apply to any classifier
involving event detection and classification with non-zero error rates.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis](https://arxiv.org/abs/2508.15189)
*Jiahao Xu,Changchang Yin,Odysseas Chatzipanagiotou,Diamantis Tsilimigras,Kevin Clear,Bingsheng Yao,Dakuo Wang,Timothy Pawlik,Ping Zhang*

Main category: cs.AI

TL;DR: 这篇论文提出了SurgWound数据集和WoundQwen三阶段学习框架，为手术伤口诊断创建了首个开源数据集和基准测试，解决了数据隐私和专家标注成本高的挑战。


<details>
  <summary>Details</summary>
Motivation: 手术部位感染(SSI)是常见且成本高的医疗相关感染，但深度学习在手术伤口筛查方面的进展遇到了数据隐私、专家标注成本高等挑战，缺乏公开数据集和开源工具。

Method: 创建SurgWound数据集(697张图片并标注8个临床属性)，设计手术伤口诊断基准测试(VQA和报告生成)，提出WoundQwen三阶段学习框架：第一阶段用5个MLLM预测伤口特征，第二阶段用两个MLLM进行诊断，第三阶段生成综合报告。

Result: 开发了首个包含多种手术伤口类型的开源数据集SurgWound，建立了手术伤口诊断基准测试，并设计了能够分析详细伤口特征和提供后续指导的三阶段框架。

Conclusion: 该研究为个性化伤口护理、及时干预和改善患者结果建立了基础，WoundQwen框架能够通过手术图像进行详细伤口特征分析并提供个性化的后续指导。

Abstract: Surgical site infection (SSI) is one of the most common and costly
healthcare-associated infections and and surgical wound care remains a
significant clinical challenge in preventing SSIs and improving patient
outcomes. While recent studies have explored the use of deep learning for
preliminary surgical wound screening, progress has been hindered by concerns
over data privacy and the high costs associated with expert annotation.
Currently, no publicly available dataset or benchmark encompasses various types
of surgical wounds, resulting in the absence of an open-source Surgical-Wound
screening tool. To address this gap: (1) we present SurgWound, the first
open-source dataset featuring a diverse array of surgical wound types. It
contains 697 surgical wound images annotated by 3 professional surgeons with
eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce
the first benchmark for surgical wound diagnosis, which includes visual
question answering (VQA) and report generation tasks to comprehensively
evaluate model performance. (3) Furthermore, we propose a three-stage learning
framework, WoundQwen, for surgical wound diagnosis. In the first stage, we
employ five independent MLLMs to accurately predict specific surgical wound
characteristics. In the second stage, these predictions serve as additional
knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess
infection risk and guide subsequent interventions. In the third stage, we train
a MLLM that integrates the diagnostic results from the previous two stages to
produce a comprehensive report. This three-stage framework can analyze detailed
surgical wound characteristics and provide subsequent instructions to patients
based on surgical images, paving the way for personalized wound care, timely
intervention, and improved patient outcomes.

</details>


### [10] [Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots](https://arxiv.org/abs/2508.15748)
*Emma Rath,Stuart Armstrong,Rebecca Gorman*

Main category: cs.AI

TL;DR: 使用语言模型构建实时评估框架，通过分析对话来检测AI与人类形成偏社交关系的危险信号，并在小规模测试中得到高准确度检测效果。


<details>
  <summary>Details</summary>
Motivation: 偏社交关系对人类健康造成严重危害，但防止此类关系形成面临挑战，因为相关线索逐渐在私密对话中出现，而且非所有情感交流都有害。

Method: 重新利用现有最优语言模型构建简单的响应评估框架，实时评估进行中对话的偏社交线索。使用30个合成对话的小规模数据集进行测试，包含偏社交、奉承和中性对话。

Result: 通过五阶段迭代测试，在宽松一致性规则下成功检测所有偏社交对话，且没有假阻性识别，检测多在对话初期阶段完成。

Conclusion: 评估机制为防止偏社交关系提供了可行解决方案，小规模测试结果显示了该方法的潜力。

Abstract: The development of parasocial relationships with AI agents has severe, and in
some cases, tragic effects for human well-being. Yet preventing such dynamics
is challenging: parasocial cues often emerge gradually in private
conversations, and not all forms of emotional engagement are inherently
harmful. We address this challenge by introducing a simple response evaluation
framework, created by repurposing a state-of-the-art language model, that
evaluates ongoing conversations for parasocial cues in real time. To test the
feasibility of this approach, we constructed a small synthetic dataset of
thirty dialogues spanning parasocial, sycophantic, and neutral conversations.
Iterative evaluation with five stage testing successfully identified all
parasocial conversations while avoiding false positives under a tolerant
unanimity rule, with detection typically occurring within the first few
exchanges. These findings provide preliminary evidence that evaluation agents
can provide a viable solution for the prevention of parasocial relations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [TComQA: Extracting Temporal Commonsense from Text](https://arxiv.org/abs/2508.15274)
*Lekshmi R Nair,Arun Sankar,Koninika Pal*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型自动挖掘时间常识的流水线方法，构建了TComQA数据集，并在时间问答任务上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 自然语言中事件的时间上下文往往没有明确表述，即使是先进的大型语言模型也难以生成需要时间常识推理的文本。自动挖掘时间常识有助于构建更鲁棒的语言模型。

Method: 提出时间常识提取流水线，利用LLMs从SAMSum和RealNews语料库中自动挖掘时间常识，构建TComQA数据集，并通过众包验证。

Result: TComQA数据集在提取时间常识方面达到超过80%的精确度，基于该数据集训练的模型在时间问答任务上优于在现有数据集上微调的LLM。

Conclusion: 该方法能够有效自动挖掘时间常识，构建高质量数据集，提升模型在时间推理任务上的性能。

Abstract: Understanding events necessitates grasping their temporal context, which is
often not explicitly stated in natural language. For example, it is not a
trivial task for a machine to infer that a museum tour may last for a few
hours, but can not take months. Recent studies indicate that even advanced
large language models (LLMs) struggle in generating text that require reasoning
with temporal commonsense due to its infrequent explicit mention in text.
Therefore, automatically mining temporal commonsense for events enables the
creation of robust language models. In this work, we investigate the capacity
of LLMs to extract temporal commonsense from text and evaluate multiple
experimental setups to assess their effectiveness. Here, we propose a temporal
commonsense extraction pipeline that leverages LLMs to automatically mine
temporal commonsense and use it to construct TComQA, a dataset derived from
SAMSum and RealNews corpora. TComQA has been validated through crowdsourcing
and achieves over 80\% precision in extracting temporal commonsense. The model
trained with TComQA also outperforms an LLM fine-tuned on existing dataset of
temporal question answering task.

</details>


### [12] [The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech](https://arxiv.org/abs/2508.15524)
*Naama Rivlin-Angert,Guy Mor-Lan*

Main category: cs.CL

TL;DR: 首个大规模政治去合法化话语(PDD)计算研究，构建希伯来语语料库并开发两阶段分类模型，发现PDD在社交媒体和选举期间显著增加


<details>
  <summary>Details</summary>
Motivation: 研究政治去合法化话语(PDD)对民主话语的影响，缺乏大规模计算分析工具来系统研究这种政治现象

Method: 构建包含10,410句希伯来语句子的语料库，手动标注1,812个PDD实例；开发两阶段分类管道，结合微调编码器模型和解码器LLM

Result: 最佳模型(DictaLM 2.0)在二元PDD检测中F1得分0.74，特征分类macro-F1得分0.67；发现PDD在30年间显著上升，社交媒体比议会辩论更普遍，右翼政客使用更多

Conclusion: 自动化PDD分析对于理解民主话语具有可行性和重要价值，能够揭示政治话语模式随时间和平��的变化

Abstract: We present the first large-scale computational study of political
delegitimization discourse (PDD), defined as symbolic attacks on the normative
validity of political entities. We curate and manually annotate a novel
Hebrew-language corpus of 10,410 sentences drawn from Knesset speeches
(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which
1,812 instances (17.4\%) exhibit PDD and 642 carry additional annotations for
intensity, incivility, target type, and affective framing. We introduce a
two-stage classification pipeline combining finetuned encoder models and
decoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary
PDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization
characteristics. Applying this classifier to longitudinal and cross-platform
data, we see a marked rise in PDD over three decades, higher prevalence on
social media versus parliamentary debate, greater use by male than female
politicians, and stronger tendencies among right-leaning actors - with
pronounced spikes during election campaigns and major political events. Our
findings demonstrate the feasibility and value of automated PDD analysis for
understanding democratic discourse.

</details>


### [13] [Stemming -- The Evolution and Current State with a Focus on Bangla](https://arxiv.org/abs/2508.15711)
*Abhijit Paul,Mashiat Amin Farin,Sharif Md. Abdullah,Ahmedul Kabir,Zarif Masud,Shebuti Rayana*

Main category: cs.CL

TL;DR: 对布拉语词干提取技术的综述性调查，指出当前研究缺口和评估方法不足，并提出开发方向建议。


<details>
  <summary>Details</summary>
Motivation: 布拉语作为世界第七大语言，因资源稀缺和标注数据集不足而数字化表现低下，而词干提取对于这种局需语言的语言分析至关重要。

Method: 进行了布拉语词干提取方法的综述性调查分析，重点关注形态变体处理效果，批判现有评估方法的不足之处。

Result: 发现布拉语词干提取领域存在显著研究空白，早期研究中断且可复现实现缺乏，评估指标不适用。

Conclusion: 建议加强布拉语词干提取器的开发研究，提出更适用的评估方法，以支持布拉语言分析和处理的发展。

Abstract: Bangla, the seventh most widely spoken language worldwide with 300 million
native speakers, faces digital under-representation due to limited resources
and lack of annotated datasets. Stemming, a critical preprocessing step in
language analysis, is essential for low-resource, highly-inflectional languages
like Bangla, because it can reduce the complexity of algorithms and models by
significantly reducing the number of words the algorithm needs to consider.
This paper conducts a comprehensive survey of stemming approaches, emphasizing
the importance of handling morphological variants effectively. While exploring
the landscape of Bangla stemming, it becomes evident that there is a
significant gap in the existing literature. The paper highlights the
discontinuity from previous research and the scarcity of accessible
implementations for replication. Furthermore, it critiques the evaluation
methodologies, stressing the need for more relevant metrics. In the context of
Bangla's rich morphology and diverse dialects, the paper acknowledges the
challenges it poses. To address these challenges, the paper suggests directions
for Bangla stemmer development. It concludes by advocating for robust Bangla
stemmers and continued research in the field to enhance language analysis and
processing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [14] [A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot](https://arxiv.org/abs/2508.14994)
*Murilo Vinicius da Silva,Matheus Hipolito Carvalho,Juliano Negri,Thiago Segreto,Gustavo J. G. Lahr,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出基于视觉姿态估计的直观四足机器人遥操作方法，通过外部摄像头检测操作者手腕位置，实时映射到机械臂控制，结合轨迹规划器确保安全操作


<details>
  <summary>Details</summary>
Motivation: 在危险和远程环境中，四足机器人需要更安全和高效的操作。传统遥操作方法（如操纵杆）不直观且需要专业知识，认知负荷高，缺乏集成的障碍物检测功能，增加了在受限或动态工作空间中的碰撞风险

Method: 使用基于视觉的姿态估计流水线，利用外部摄像头和机器学习模型检测操作者手腕位置，将这些手腕运动实时映射为机械臂命令。结合轨迹规划器检测和防止与障碍物及机械臂本身的碰撞

Result: 在真实机器人上验证了系统，展示了实时控制的鲁棒性能。该方法为工业应用提供了成本效益高的解决方案

Conclusion: 这种遥操作方法在安全性、精确性和易用性至关重要的工业应用中提供了可靠且直观的机器人控制，特别适用于高风险环境

Abstract: In hazardous and remote environments, robotic systems perform critical tasks
demanding improved safety and efficiency. Among these, quadruped robots with
manipulator arms offer mobility and versatility for complex operations.
However, teleoperating quadruped robots is challenging due to the lack of
integrated obstacle detection and intuitive control methods for the robotic
arm, increasing collision risks in confined or dynamically changing workspaces.
Teleoperation via joysticks or pads can be non-intuitive and demands a high
level of expertise due to its complexity, culminating in a high cognitive load
on the operator. To address this challenge, a teleoperation approach that
directly maps human arm movements to the robotic manipulator offers a simpler
and more accessible solution. This work proposes an intuitive remote control by
leveraging a vision-based pose estimation pipeline that utilizes an external
camera with a machine learning-based model to detect the operator's wrist
position. The system maps these wrist movements into robotic arm commands to
control the robot's arm in real-time. A trajectory planner ensures safe
teleoperation by detecting and preventing collisions with both obstacles and
the robotic arm itself. The system was validated on the real robot,
demonstrating robust performance in real-time control. This teleoperation
approach provides a cost-effective solution for industrial applications where
safety, precision, and ease of use are paramount, ensuring reliable and
intuitive robotic control in high-risk environments.

</details>


### [15] [Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation](https://arxiv.org/abs/2508.15663)
*Nikita Kachaev,Andrei Spiridonov,Andrey Gorodetsky,Kirill Muravyev,Nikita Oskolkov,Aditya Narendra,Vlad Shakhuro,Dmitry Makarov,Aleksandr I. Panov,Polina Fedotova,Alexey K. Kovalev*

Main category: cs.RO

TL;DR: Kitchen-R是一个新颖的厨房环境基准测试，统一评估任务规划和低级控制，填补了语言指令跟随与机器人控制之间的评估空白


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在显著差距：高级语言指令跟随基准假设完美低级执行，而低级控制基准依赖简单单步命令，缺乏对集成系统的综合评估

Method: 使用Isaac Sim模拟器构建数字孪生厨房环境，包含500+复杂语言指令，支持移动机械臂机器人。提供基于视觉语言模型的任务规划策略和基于扩散策略的低级控制策略基线方法

Result: 开发了包含轨迹收集系统的完整基准框架，支持三种评估模式：独立规划模块评估、独立控制策略评估以及关键的系统集成评估

Conclusion: Kitchen-R填补了具身AI研究的关键空白，为语言引导机器人代理提供了更全面和现实的基准测试框架

Abstract: Benchmarks are crucial for evaluating progress in robotics and embodied AI.
However, a significant gap exists between benchmarks designed for high-level
language instruction following, which often assume perfect low-level execution,
and those for low-level robot control, which rely on simple, one-step commands.
This disconnect prevents a comprehensive evaluation of integrated systems where
both task planning and physical execution are critical. To address this, we
propose Kitchen-R, a novel benchmark that unifies the evaluation of task
planning and low-level control within a simulated kitchen environment. Built as
a digital twin using the Isaac Sim simulator and featuring more than 500
complex language instructions, Kitchen-R supports a mobile manipulator robot.
We provide baseline methods for our benchmark, including a task-planning
strategy based on a vision-language model and a low-level control policy based
on diffusion policy. We also provide a trajectory collection system. Our
benchmark offers a flexible framework for three evaluation modes: independent
assessment of the planning module, independent assessment of the control
policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R
bridges a key gap in embodied AI research, enabling more holistic and realistic
benchmarking of language-guided robotic agents.

</details>
