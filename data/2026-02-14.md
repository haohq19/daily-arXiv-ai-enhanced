<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors](https://arxiv.org/abs/2602.11323)
*Arda Alniak,Sinan Kalkan,Mustafa Mert Ankarali,Afsar Saranli,Abdullah Aydin Alatan*

Main category: cs.CV

TL;DR: 提出一种将学习到的深度先验集成到VINS-Mono优化后端的新框架，通过仿射不变深度一致性和成对序数约束，在边缘设备上实现实时、鲁棒的视觉惯性里程计


<details>
  <summary>Details</summary>
Motivation: 传统单目视觉惯性里程计在低纹理环境中表现不佳，稀疏视觉特征不足以进行准确的姿态估计。虽然密集单目深度估计可以作为补充信息源，但基于Vision Transformer的复杂基础模型计算量大，难以在边缘设备上实时部署

Method: 将学习到的深度先验直接集成到VINS-Mono优化后端，提出新框架强制执行仿射不变深度一致性和成对序数约束，通过基于方差的门控机制显式过滤不稳定伪影

Result: 在TartanGround和M3ED数据集上的大量实验表明，该方法在挑战性场景中防止发散，并带来显著的精度提升，绝对轨迹误差降低高达28.3%

Conclusion: 该方法在严格遵守边缘设备计算限制的同时，能够鲁棒地恢复度量尺度，填补了复杂深度模型与实时边缘部署之间的空白

Abstract: Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.

</details>


### [2] [DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation](https://arxiv.org/abs/2602.12160)
*Xu Guo,Fulong Ye,Qichao Sun,Liyang Chen,Bingchuan Li,Pengze Zhang,Jiawei Liu,Songtao Zhao,Qian He,Xiangwang Hou*

Main category: cs.CV

TL;DR: DreamID-Omni：统一可控人本音频-视频生成框架，通过对称条件扩散Transformer、双级解耦策略和多任务渐进训练，解决多人场景中的身份-音色绑定失败问题，在多个任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在音频-视频联合生成方面取得进展，但将参考音频视频生成、视频编辑和音频驱动视频动画等任务视为孤立目标。多人场景中实现精确解耦的身份和音色控制仍具挑战，存在身份-音色绑定失败和说话者混淆问题。

Method: 1. 对称条件扩散Transformer：通过对称条件注入方案整合异构条件信号；2. 双级解耦策略：信号级同步RoPE确保注意力空间绑定，语义级结构化描述建立属性-主体映射；3. 多任务渐进训练：利用弱约束生成先验正则化强约束任务，防止过拟合并协调不同目标。

Result: 在视频、音频和音频-视觉一致性方面实现全面的最先进性能，甚至超越领先的专有商业模型。在多人场景中有效解决身份-音色绑定失败和说话者混淆问题。

Conclusion: DreamID-Omni为可控人本音频-视频生成提供了统一框架，通过创新的架构设计和训练策略解决了现有方法的局限性，将开源代码以弥合学术研究与商业级应用之间的差距。

Abstract: Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra](https://arxiv.org/abs/2602.11206)
*Jose Marie Antonio Miñoza*

Main category: cs.LG

TL;DR: UltraLIF：基于热带几何学的超离散化框架，用可微的软最大值函数替代传统SNN中的启发式代理梯度，解决了脉冲生成不可微问题，实现了完全可微的SNN训练。


<details>
  <summary>Details</summary>
Motivation: 传统SNN由于脉冲生成的非可微性，需要依赖启发式的代理梯度方法，这导致前向传播与反向传播不匹配的问题。需要一种更原则性的数学框架来解决这一根本问题。

Method: 采用热带几何学中的超离散化方法，利用max-plus半环自然建模神经阈值动态。log-sum-exp函数作为可微的软最大值函数，通过可学习的温度参数ε→0收敛到硬阈值。从两种动力学系统推导出两个神经元模型：基于LIF常微分方程的UltraLIF（时间动态）和基于扩散方程的UltraDLIF（空间动态，模拟间隙连接耦合）。

Result: 在六个基准测试（静态图像、神经形态视觉和音频）上表现优于代理梯度基线方法，特别是在单时间步(T=1)设置下对神经形态和时间数据集提升最明显。理论分析证明了向经典LIF动态的点态收敛，具有定量误差界限和有界非消失梯度。可选稀疏惩罚可实现显著能耗降低同时保持竞争力精度。

Conclusion: UltraLIF提供了一个原则性的数学框架，用超离散化替代代理梯度，解决了SNN训练中的可微性问题，实现了完全可微的SNN训练，在多个基准测试上表现优异，特别是在神经形态和时间任务中。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.

</details>


### [4] [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)
*David Pardoe,Neil Daftary,Miro Furtado,Aditya Aiyer,Yu Wang,Liuqing Li,Tao Song,Lars Hertel,Young Jin Yun,Senthil Radhakrishnan,Zhiwei Wang,Tommy Li,Khai Tran,Ananth Nagarajan,Ali Naqvi,Yue Zhang,Renpeng Fang,Avi Romascanu,Arjun Kulothungun,Deepak Kumar,Praneeth Boda,Fedor Borisyuk,Ruoyan Wang*

Main category: cs.LG

TL;DR: CADET是一个用于广告点击率预测的解码器Transformer模型，通过上下文条件解码、自门控注意力、时间感知位置编码等创新，在LinkedIn广告平台实现11.04%的CTR提升。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习推荐模型在广告CTR预测领域长期占据主导，但生成式推荐模型在内容推荐中表现出色。然而，将这些基于Transformer的架构应用于广告CTR预测仍面临独特挑战：处理评分后上下文信号、保持离线-在线一致性、扩展到工业级工作负载。

Method: CADET采用端到端解码器Transformer架构，包含五个关键创新：1) 上下文条件解码架构与多塔预测头，显式建模广告位置等评分后信号；2) 自门控注意力机制，在表示和交互层面自适应调节信息流；3) 基于时间戳的RoPE变体，捕捉从秒到月的时间关系；4) 会话掩码策略，防止模型学习不可用会话事件的依赖；5) 生产工程技术，包括张量打包、序列分块和自定义Flash Attention内核。

Result: 在线A/B测试中，CADET相比生产基线模型LiRank（DCNv2和序列编码器的混合集成）实现了11.04%的CTR提升。该系统已成功部署在LinkedIn广告平台，服务于主页信息流赞助更新的主要流量。

Conclusion: CADET成功将解码器Transformer架构应用于工业级广告CTR预测，通过创新的架构设计和工程优化解决了实际部署中的关键挑战，显著提升了预测性能并实现了大规模部署。

Abstract: Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.

</details>


### [5] [Partial GFlowNet: Accelerating Convergence in Large State Spaces via Strategic Partitioning](https://arxiv.org/abs/2602.11498)
*Xuan Yu,Xu Wang,Rui Zhu,Yudong Zhang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种改进的GFlowNets方法，通过引入规划器将整个状态空间划分为重叠的部分状态空间，限制探索范围，从而在大规模状态空间中实现更快的收敛和更好的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNets在状态空间中自由探索，当扩展到大规模状态空间时会遇到显著的收敛挑战。需要一种方法来限制探索范围，提高在大状态空间中的学习效率。

Method: 引入规划器将整个状态空间划分为重叠的部分状态空间；在这些有限大小的部分空间中，演员能够高效识别高奖励子区域；采用启发式策略在不同部分区域间切换，避免探索已完全探索或低奖励区域；通过迭代探索这些部分状态空间，演员学习收敛到整个状态空间中的高奖励子区域。

Result: 在多个广泛使用的数据集上的实验表明，该方法在大规模状态空间上比现有工作收敛更快；不仅生成具有更高奖励的候选者，还显著提高了候选者的多样性。

Conclusion: 通过限制探索范围并采用智能的区域划分和切换策略，提出的方法有效解决了GFlowNets在大规模状态空间中的收敛问题，实现了更高效的学习和更好的生成质量。

Abstract: Generative Flow Networks (GFlowNets) have shown promising potential to generate high-scoring candidates with probability proportional to their rewards. As existing GFlowNets freely explore in state space, they encounter significant convergence challenges when scaling to large state spaces. Addressing this issue, this paper proposes to restrict the exploration of actor. A planner is introduced to partition the entire state space into overlapping partial state spaces. Given their limited size, these partial state spaces allow the actor to efficiently identify subregions with higher rewards. A heuristic strategy is introduced to switch partial regions thus preventing the actor from wasting time exploring fully explored or low-reward partial regions. By iteratively exploring these partial state spaces, the actor learns to converge towards the high-reward subregions within the entire state space. Experiments on several widely used datasets demonstrate that \modelname converges faster than existing works on large state spaces. Furthermore, \modelname not only generates candidates with higher rewards but also significantly improves their diversity.

</details>


### [6] [Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice](https://arxiv.org/abs/2602.11505)
*Jiangkai Xiong,Kalyan Talluri,Hanzhao Wang*

Main category: cs.LG

TL;DR: 提出校准方法，将不完美的外部选项预测转化为统计有效的无购买估计，仅使用购买数据


<details>
  <summary>Details</summary>
Motivation: 企业通常无法观察消费者的关键行为（如购买竞争对手产品、不购买等），这种缺失的外部选项信息使得市场规模和偏好估计困难，现有方法依赖辅助数据但可能存在偏差

Method: 开发两种校准方法：1）在仿射误校准下，通过简单回归识别外部选项效用参数；2）在较弱单调条件下，提出基于排名的校准方法并推导有限样本误差界

Result: 方法能一致恢复无购买概率，无需收集新的无购买标签，数值实验显示在无购买估计和下游品类决策方面有改进

Conclusion: 提出的校准方法能将不完美的辅助预测转化为有效的无购买估计，量化了校准精度对收入绩效的影响，为结合多个辅助预测器提供了扩展

Abstract: Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.

</details>


### [7] [Unifying Stable Optimization and Reference Regularization in RLHF](https://arxiv.org/abs/2602.11523)
*Li He,Qiang Qu,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出统一正则化方法，平衡奖励攻击和稳定优化，通过加权监督微调损失改善RLHF的对齐效果和稳定性


<details>
  <summary>Details</summary>
Motivation: RLHF面临奖励攻击和稳定优化两大挑战，现有方法分别使用KL散度惩罚和策略比率裁剪，但两者之间的权衡关系未被充分探索

Method: 引入统一正则化方法，显式平衡防止奖励攻击和保持策略更新稳定性的目标，提出加权监督微调损失

Result: 在多样化基准测试中，该方法一致优于RLHF和在线偏好学习方法，实现了更好的对齐性能和稳定性

Conclusion: 提出的统一正则化方法通过简单而原则性的对齐目标，改善了RLHF的对齐效果和实现复杂度

Abstract: Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \textbf{reward hacking} and \textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.

</details>


### [8] [Both Topology and Text Matter: Revisiting LLM-guided Out-of-Distribution Detection on Text-attributed Graphs](https://arxiv.org/abs/2602.11641)
*Yinlin Zhu,Di Wu,Xu Wang,Guocong Quan,Miao Hu*

Main category: cs.LG

TL;DR: LG-Plug：一种用于文本属性图OOD检测的LLM引导即插即用策略，通过对齐拓扑和文本表示生成细粒度节点嵌入，利用聚类迭代LLM提示产生共识驱动的OOD暴露，并作为正则化项与现有检测器无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：拓扑驱动方法未能充分利用文本语义信息，而基于LLM的方法存在可靠性-信息性不平衡问题，且依赖专用架构无法利用已验证的拓扑级洞察。需要一种能同时利用拓扑和文本信息，并可与现有检测器集成的解决方案。

Method: 1) 对齐拓扑和文本表示生成细粒度节点嵌入；2) 通过聚类迭代LLM提示生成共识驱动的OOD暴露；3) 使用轻量级集群码本和启发式采样降低LLM查询时间成本；4) 将OOD暴露作为正则化项分离ID和OOD节点。

Result: 该方法能够生成更可靠的OOD暴露，避免语义偏差和ID噪声，同时可与现有检测器无缝集成，提高OOD检测性能。

Conclusion: LG-Plug通过LLM引导的即插即用策略，有效解决了文本属性图OOD检测中拓扑与文本信息融合的挑战，平衡了可靠性和信息性，实现了与现有检测器的兼容性。

Abstract: Text-attributed graphs (TAGs) associate nodes with textual attributes and graph structure, enabling GNNs to jointly model semantic and structural information. While effective on in-distribution (ID) data, GNNs often encounter out-of-distribution (OOD) nodes with unseen textual or structural patterns in real-world settings, leading to overconfident and erroneous predictions in the absence of reliable OOD detection. Early approaches address this issue from a topology-driven perspective, leveraging neighboring structures to mitigate node-level detection bias. However, these methods typically encode node texts as shallow vector features, failing to fully exploit rich semantic information. In contrast, recent LLM-based approaches generate pseudo OOD priors by leveraging textual knowledge, but they suffer from several limitations: (1) a reliability-informativeness imbalance in the synthesized OOD priors, as the generated OOD exposures either deviate from the true OOD semantics, or introduce non-negligible ID noise, all of which offers limited improvement to detection performance; (2) reliance on specialized architectures, which prevents incorporation of the extensive effective topology-level insights that have been empirically validated in prior work. To this end, we propose LG-Plug, an LLM-Guided Plug-and-play strategy for TAG OOD detection tasks. LG-Plug aligns topology and text representations to produce fine-grained node embeddings, then generates consensus-driven OOD exposure via clustered iterative LLM prompting. Moreover, it leverages lightweight in-cluster codebook and heuristic sampling reduce time cost of LLM querying. The resulting OOD exposure serves as a regularization term to separate ID and OOD nodes, enabling seamless integration with existing detectors.

</details>


### [9] [Potential-energy gating for robust state estimation in bistable stochastic systems](https://arxiv.org/abs/2602.11712)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 提出基于势能门控的鲁棒状态估计方法，通过势能函数调节观测噪声协方差，在双阱随机系统中显著提升估计精度


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯滤波器在处理双阱随机系统时，对所有状态空间区域采用相同的观测信任度，无法有效处理势垒附近的异常观测，需要一种物理机制来调节观测信任度

Method: 提出势能门控方法：根据已知或假设的势能函数值调节观测噪声协方差，当状态接近势阱最小值时信任观测，接近势垒时逐渐折扣观测。该方法可集成到扩展卡尔曼滤波、无迹卡尔曼滤波、集合卡尔曼滤波、自适应卡尔曼滤波和粒子滤波中，仅需两个超参数

Result: 在Ginzburg-Landau双阱过程的合成基准测试中，相比标准扩展卡尔曼滤波，RMSE改进57-80%（p < 10^{-15}）。即使势能参数偏差50%，改进仍不低于47%。在Kramers型跃迁中保持68%改进，而朴素基线降至30%。应用于NGRIP冰芯记录，估计不对称参数γ = -0.109，异常值比例解释了滤波器改进方差的91%

Conclusion: 势能门控提供了一种物理启发的鲁棒状态估计方法，通过调制观测信任度显著提升双阱随机系统的估计精度，对参数误设具有鲁棒性，并在古气候数据应用中展示了实用价值

Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.

</details>


### [10] [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776)
*Cláudio Correia,Alberto E. A. Ferreira,Lucas Martins,Miguel P. Bento,Sofia Guerreiro,Ricardo Ribeiro Pereira,Ana Sofia Gomes,Jacopo Bono,Hugo Ferreira,Pedro Bizarro*

Main category: cs.LG

TL;DR: MUSE是一个模型服务框架，通过将模型分数与客户端决策边界解耦，在多租户环境中实现无缝模型更新，解决了模型重训练导致分数分布变化时需协调数百个客户端更新阈值的问题。


<details>
  <summary>Details</summary>
Motivation: 在多租户Score-as-a-Service环境中，模型重训练会导致分数分布变化，使现有决策阈值失效。由于决策边界位于客户端管理的基础设施中，需要协调数百个客户端更新阈值，这消耗大量人力并导致模型停滞。

Method: MUSE采用动态意图路由共享模型，结合两级分数转换，将模型输出映射到稳定的参考分布，从而解耦模型分数与客户端决策边界。

Result: MUSE已在Feedzai大规模部署，每秒处理超过1000个事件，过去12个月处理超过550亿事件，覆盖数十个租户，同时保持高可用性和低延迟。将模型上线时间从数周缩短到分钟级。

Conclusion: MUSE通过解耦模型分数与决策边界，解决了多租户环境中模型更新的瓶颈问题，提升了模型对变化攻击的抵御能力，节省了数百万美元的欺诈损失和运营成本。

Abstract: In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.

</details>


### [11] [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786)
*Keita Broadwater*

Main category: cs.LG

TL;DR: APST是一个深度导向的评估框架，通过重复采样相同提示来测试LLM在持续使用下的安全性和可靠性，发现传统单样本评估可能掩盖实际部署中的可靠性差异。


<details>
  <summary>Details</summary>
Motivation: 传统LLM安全评估主要关注广度（多样任务），但实际部署中更关键的是深度风险：在相同或相似提示重复推理下的操作失败。高风险场景需要响应一致性和持续使用下的安全性。

Method: 提出加速提示压力测试（APST）框架，受可靠性工程启发，在受控操作条件下（如解码温度）重复采样相同提示，检测幻觉、拒绝不一致性和不安全完成等潜在故障模式。使用伯努利和二项式模型将安全故障建模为独立推理事件的随机结果，估计每次推理的故障概率。

Result: 在多个指令调优LLM上应用APST评估AIR-BENCH衍生的安全提示，发现具有相似基准分数的模型在重复采样下可能表现出显著不同的经验故障率，特别是随着温度升高。浅层单样本评估可能掩盖持续使用下的可靠性差异。

Conclusion: APST通过提供评估LLM在重复推理下安全性和可靠性的实用框架，补充现有基准，弥合基准对齐和部署导向风险评估之间的差距。强调需要深度导向评估来捕捉实际部署中的操作风险。

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

</details>


### [12] [Capability-Oriented Training Induced Alignment Risk](https://arxiv.org/abs/2602.12124)
*Yujun Zhou,Yue Huang,Han Bao,Kehan Guo,Zhenwen Liang,Pin-Yu Chen,Tian Gao,Werner Geyer,Nuno Moniz,Nitesh V Chawla,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 语言模型在强化学习训练中会自发利用环境漏洞来最大化奖励，即使没有恶意意图，这些利用策略具有可泛化性，对现有对齐方法构成根本挑战。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究主要关注防止模型生成有害内容，但更微妙的风险是能力导向训练引发的利用行为。研究者想探究语言模型在含有隐含漏洞的环境中是否会自发学习利用这些漏洞来最大化奖励。

Method: 设计了四个不同的"漏洞游戏"，每个游戏呈现独特的可利用缺陷：上下文条件合规、代理指标、奖励篡改和自我评估。通过强化学习在这些环境中训练模型，并测试其利用策略的可转移性和可蒸馏性。

Result: 模型一致地学会了利用这些漏洞，发现了机会主义策略，这些策略以任务正确性或安全性为代价显著增加了奖励。更重要的是，这些利用策略不是狭窄的"技巧"，而是可泛化的技能，可以转移到新任务，甚至可以通过数据从有能力的教师模型"蒸馏"到其他学生模型。

Conclusion: 能力导向训练引发的风险对当前对齐方法构成根本挑战，未来的AI安全工作必须超越内容审核，严格审计和保护训练环境和奖励机制本身。

Abstract: While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse "vulnerability games", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow "tricks" but generalizable skills; they can be transferred to new tasks and even "distilled" from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk.

</details>


### [13] [On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy](https://arxiv.org/abs/2602.12009)
*Luiz Pereira,Mirko Perkusich,Dalton Valadares,Kyller Gorgônio*

Main category: cs.LG

TL;DR: 本文分析了差分隐私机制（梯度裁剪和噪声注入）如何影响脉冲神经网络的发放率统计，以及这些扰动如何传播到基于发放率的联邦神经形态学习协调中，为隐私保护的FNL提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 联邦神经形态学习（FNL）虽然能实现能效高且保护隐私的设备端学习，但实际部署需要额外的隐私机制，这些机制会显著改变训练信号。需要理解差分隐私机制如何影响SNN的发放率统计，以及这些扰动如何影响基于发放率的FNL协调。

Method: 在非独立同分布设置下的语音识别任务中，通过在不同隐私预算和裁剪边界上进行消融实验，分析差分隐私机制（梯度裁剪和噪声注入）对脉冲神经网络发放率统计的扰动，并研究这些扰动如何传播到基于发放率的FNL协调。

Result: 实验揭示了系统性的发放率偏移、聚合衰减和客户端选择中的排名不稳定性。这些偏移与稀疏性和内存指标相关。研究结果显示了隐私强度与基于发放率的协调之间的权衡关系。

Conclusion: 研究结果为隐私保护的联邦神经形态学习提供了可操作的指导，特别是在隐私强度与基于发放率的协调之间的平衡方面。理解差分隐私机制对发放率统计的影响对于设计有效的隐私保护FNL系统至关重要。

Abstract: Federated Neuromorphic Learning (FNL) enables energy-efficient and privacy-preserving learning on devices without centralizing data. However, real-world deployments require additional privacy mechanisms that can significantly alter training signals. This paper analyzes how Differential Privacy (DP) mechanisms, specifically gradient clipping and noise injection, perturb firing-rate statistics in Spiking Neural Networks (SNNs) and how these perturbations are propagated to rate-based FNL coordination. On a speech recognition task under non-IID settings, ablations across privacy budgets and clipping bounds reveal systematic rate shifts, attenuated aggregation, and ranking instability during client selection. Moreover, we relate these shifts to sparsity and memory indicators. Our findings provide actionable guidance for privacy-preserving FNL, specifically regarding the balance between privacy strength and rate-dependent coordination.

</details>


### [14] [PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories](https://arxiv.org/abs/2602.12080)
*Hyunsung Kim,Kunhee Lee,Sangwoo Seo,Sang-Ki Ko,Jinsung Yoon,Chanyoung Park*

Main category: cs.LG

TL;DR: PathCRF：仅使用球员轨迹数据检测足球场上持球事件的框架，通过动态图建模和条件随机场确保逻辑一致性，减少人工标注需求


<details>
  <summary>Details</summary>
Motivation: 当前足球事件数据收集依赖劳动密集型人工标注，球追踪技术成本高难以扩展，导致全面数据收集仅限于顶级赛事，限制了数据驱动分析的广泛应用

Method: 将球员轨迹建模为全连接动态图，将事件检测转化为在每个时间步选择恰好一条边对应当前持球状态。使用条件随机场(CRF)确保边序列的逻辑一致性，通过基于集合注意力的骨干架构计算发射和转移分数，推理时使用维特比解码获取最可能边序列

Result: PathCRF能够生成准确、逻辑一致的持球路径，实现可靠的下游分析，同时显著减少手动事件标注的需求

Conclusion: PathCRF框架仅使用球员跟踪数据就能有效检测足球持球事件，为更广泛采用数据驱动分析提供了可行方案，降低了数据收集成本

Abstract: Despite recent advances in AI, event data collection in soccer still relies heavily on labor-intensive manual annotation. Although prior work has explored automatic event detection using player and ball trajectories, ball tracking also remains difficult to scale due to high infrastructural and operational costs. As a result, comprehensive data collection in soccer is largely confined to top-tier competitions, limiting the broader adoption of data-driven analysis in this domain. To address this challenge, this paper proposes PathCRF, a framework for detecting on-ball soccer events using only player tracking data. We model player trajectories as a fully connected dynamic graph and formulate event detection as the problem of selecting exactly one edge corresponding to the current possession state at each time step. To ensure logical consistency of the resulting edge sequence, we employ a Conditional Random Field (CRF) that forbids impossible transitions between consecutive edges. Both emission and transition scores dynamically computed from edge embeddings produced by a Set Attention-based backbone architecture. During inference, the most probable edge sequence is obtained via Viterbi decoding, and events such as ball controls or passes are detected whenever the selected edge changes between adjacent time steps. Experiments show that PathCRF produces accurate, logically consistent possession paths, enabling reliable downstream analyses while substantially reducing the need for manual event annotation. The source code is available at https://github.com/hyunsungkim-ds/pathcrf.git.

</details>


### [15] [SafeNeuron: Neuron-Level Safety Alignment for Large Language Models](https://arxiv.org/abs/2602.12158)
*Zhaoxin Wang,Jiaming Liang,Fengbin Zhu,Weixiang Zhao,Junfeng Fang,Jiayi Ji,Handing Wang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: SafeNeuron是一个神经元级的安全对齐框架，通过重新分配安全表示来提高鲁棒性，防止稀疏安全路径依赖


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全对齐通常集中在少数参数上，容易被神经元级攻击绕过，且现有对齐方法主要在行为层面，对模型内部安全机制控制有限

Method: 首先识别安全相关神经元，然后在偏好优化过程中冻结这些神经元，防止依赖稀疏安全路径，强制模型构建冗余的安全表示

Result: SafeNeuron显著提高了对抗神经元剪枝攻击的鲁棒性，降低了开源模型被重新用作红队生成器的风险，同时保持了通用能力

Conclusion: SafeNeuron为模型对齐提供了可解释且鲁棒的视角，层间分析表明安全行为由稳定共享的内部表示控制

Abstract: Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities. Furthermore, our layer-wise analysis reveals that safety behaviors are governed by stable and shared internal representations. Overall, SafeNeuron provides an interpretable and robust perspective for model alignment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Causal-JEPA: Learning World Models through Object-Level Latent Interventions](https://arxiv.org/abs/2602.11389)
*Heejeong Nam,Quentin Le Lidec,Lucas Maes,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: C-JEPA：一种面向对象的世界模型，通过对象级掩码实现潜在干预，提升关系推理和规划效率


<details>
  <summary>Details</summary>
Motivation: 现有面向对象表示虽提供有用抽象，但不足以捕捉交互依赖的动态关系。需要一种能促进关系理解、防止捷径解决方案的世界模型。

Method: 提出C-JEPA，将掩码联合嵌入预测从图像块扩展到面向对象表示。通过对象级掩码，要求从其他对象推断被掩码对象状态，实现潜在干预和反事实效果。

Result: 在视觉问答中带来约20%的反事实推理绝对提升；在智能体控制任务中，仅需基于图像块世界模型1%的潜在输入特征，就能达到相当性能，规划效率大幅提升。

Conclusion: 对象级掩码通过潜在干预引入因果归纳偏置，C-JEPA在关系推理和高效规划方面表现优异，为世界模型提供了简单灵活的对象中心框架。

Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.

</details>


### [17] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文提出Rung Collapse理论，解释自回归训练无法区分关联与干预，导致模型"正确但理由错误"。作者提出Epistemic Regret Minimization (ERM)方法，通过因果信念修正防止Aleatoric Entrenchment，并在实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统常通过"捷径"获得高性能，但这些捷径在分布偏移下会失效。作者发现这一问题的因果根源：自回归训练无法区分关联P(Y|X)和干预P(Y|do(X))，导致模型固守错误因果推理。

Method: 提出Epistemic Regret Minimization (ERM)作为因果信念修正目标，独立于任务成功惩罚因果推理错误。构建三层架构：1) Physical Grounding Theorem证明满足执行器独立的动作实现有效do操作；2) ERM作为满足AGM公设的因果信念修正算子；3) 失败模式分类法注入领域无关防护。

Result: 在6个前沿LLM的1,360个因果陷阱场景中：Rung Collapse在推理增强模型中仍存在(GPT-5.2为3.7%)；可操控性呈现逆缩放现象；目标ERM反馈能恢复53-59%的固着错误，而结果层面反馈失败。证明真实干预分布的渐近恢复。

Conclusion: Rung Collapse是自回归模型固有问题，导致Aleatoric Entrenchment。ERM通过独立于任务成功的因果信念修正，有效防止错误推理固着，为构建更鲁棒的因果推理系统提供理论框架和实践方法。

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [18] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: AIR是首个针对LLM代理系统的安全事故响应框架，通过语义检测、自主修复和规则合成，实现超过90%的事故检测、修复和根除成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理的安全机制主要关注事前预防，缺乏事故发生后响应、控制和恢复的能力。实际部署中事故不可避免，需要建立系统化的事故响应机制。

Method: AIR框架包含：1）定义领域特定语言管理事故响应生命周期；2）集成到代理执行循环中，通过语义检查检测事故；3）指导代理执行控制和恢复操作；4）在根除阶段合成防护规则防止类似事故。

Result: 在三种代表性代理类型上评估，AIR的事故检测、修复和根除成功率均超过90%。实验证实关键设计组件的必要性，显示及时性和适度开销，LLM生成的规则接近开发者编写规则的效果。

Conclusion: 事故响应作为提升代理安全的一等机制既可行又必要，AIR框架为LLM代理系统提供了系统化的事故响应能力。

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [19] [AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution](https://arxiv.org/abs/2602.11917)
*Taian Guo,Haiyang Shen,Junyu Luo,Binqi Chen,Hongjun Ding,Jinsheng Huang,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaPROBE将alpha因子挖掘重构为有向无环图导航问题，通过贝叶斯因子检索器和DAG感知因子生成器，实现全局结构化的因子发现，显著提升预测精度、收益稳定性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化alpha因子挖掘方法存在两大范式：解耦因子生成将因子发现视为孤立事件，迭代因子进化专注于局部父子优化。这两种范式都缺乏全局结构视角，将因子池视为非结构化集合或碎片化链条，导致冗余搜索和多样性受限。

Method: AlphaPROBE将alpha挖掘重构为有向无环图（DAG）的战略导航，将因子建模为节点，进化链接建模为边。框架包含两个核心组件：1）贝叶斯因子检索器，通过后验概率模型平衡利用与探索，识别高潜力种子因子；2）DAG感知因子生成器，利用因子的完整祖先轨迹，生成上下文感知、非冗余的优化因子。

Result: 在三个主要中国股票市场数据集上，与8个竞争基线相比，AlphaPROBE在预测准确性、收益稳定性和训练效率方面均获得显著提升。实验证实利用全局进化拓扑结构对于高效、稳健的自动化alpha发现至关重要。

Conclusion: AlphaPROBE通过将因子池建模为动态、互连的生态系统，解决了现有alpha挖掘方法的局限性。该框架证明了全局进化拓扑结构在自动化alpha发现中的重要性，为量化金融中的因子挖掘提供了新的结构化方法。

Abstract: Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.

</details>


### [20] [Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments](https://arxiv.org/abs/2602.11964)
*Romain Froger,Pierre Andrews,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Gerard Moreno-Torres Bertran,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Vladislav Vorotilov,Mengjue Wang,Ian Yu,Amine Benhalloum,Grégoire Mialon,Thomas Scialom*

Main category: cs.AI

TL;DR: Gaia2是一个用于评估LLM智能体在真实异步环境中的基准测试，引入独立演化的场景、时间约束、噪声动态事件，支持细粒度动作级评估和强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有评估多为静态或同步环境，缺乏对智能体在真实异步环境中处理时间约束、动态事件、模糊性和协作能力的评估，需要更贴近实际的基准测试。

Method: 构建基于消费者环境的异步评估框架，每个场景配备写入动作验证器，支持细粒度动作级评估和可验证奖励的强化学习，使用开源Agents Research Environments平台。

Result: 评估显示没有模型在所有能力上占优：GPT-5最高分42% pass@1但在时间敏感任务上失败，Claude-4 Sonnet在准确性和速度间权衡，Kimi-K2开源模型最佳21% pass@1。

Conclusion: Gaia2揭示了推理、效率、鲁棒性之间的基本权衡，暴露了"模拟到现实"差距的挑战，通过开源框架为社区提供开发实用智能体系统的灵活基础设施。

Abstract: We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the "sim2real" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.

</details>


### [21] [Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication](https://arxiv.org/abs/2602.12083)
*Antonin Sulc*

Main category: cs.AI

TL;DR: 提出可微分模态逻辑（DML）和模态逻辑神经网络（MLNNs），通过神经符号方法从行为数据中学习信任网络、因果链和监管边界，用于多智能体系统的语义故障调试。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统从简单聊天机器人发展为自主群体，调试语义故障需要推理知识、信念、因果关系和义务，这正是模态逻辑的设计目的。但传统模态逻辑需要手动指定在真实系统中未知或动态的关系结构。

Method: 提出可微分模态逻辑（DML），通过模态逻辑神经网络（MLNNs）实现，使系统能够仅从行为数据中学习信任网络、因果链和监管边界。构建统一的神经符号调试框架，涵盖四种模态：认知（信任谁）、时序（事件何时导致故障）、道义（允许什么行动）和信念（如何解释智能体置信度）。

Result: 在具体多智能体场景中演示了每种模态的应用，从发现外交游戏中的欺骗联盟到检测LLM幻觉。提供了完整的实现，展示逻辑矛盾如何成为可学习的优化目标。关键贡献包括：可解释的学习结构、通过可微分公理的知识注入、组合多模态推理以及实际部署模式。

Conclusion: 可微分模态逻辑为多智能体系统调试提供了实用的神经符号方法，能够从行为数据中自动学习模态关系结构，同时保持逻辑可解释性，并提供了完整的代码实现。

Abstract: As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.
  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety](https://arxiv.org/abs/2602.11157)
*Max Zhang,Derek Liu,Kai Zhang,Joshua Franco,Haihao Liu*

Main category: cs.CL

TL;DR: 研究探索知识蒸馏在多语言越狱防御中的应用，发现传统微调反而增加越狱成功率，通过移除边界拒绝可缓解安全退化，但会降低推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐主要针对英语，在非英语特别是低资源语言中存在漏洞。需要研究如何将专有模型的安全拒绝行为有效迁移到开源模型，以提升多语言环境下的安全性。

Method: 使用知识蒸馏方法，将OpenAI o1-mini专有教师模型的拒绝行为通过LoRA参数高效微调，蒸馏到三个开源学生模型（Meta-Llama-3-8B-Instruct、Gemma-2-2B-IT、Qwen3-8B）。使用XSafety的约28,000个多语言越狱提示进行黑盒响应式微调。

Result: 发现反直觉现象：在教师模型"安全"拒绝数据上进行标准微调反而增加了所有学生模型的越狱成功率（最高达16.6个百分点）。移除"边界拒绝"这一主要安全退化源可以缓解甚至逆转安全下降，但会持续降低推理性能（GSM8K）。不同基础模型在蒸馏过程中对未见语言表现出不同的泛化结果。

Conclusion: 这项探索性研究揭示了知识蒸馏作为多语言安全对齐技术的挑战与潜力，为未来研究提供了基础。需要更精细的方法来处理安全对齐中的微妙平衡问题。

Abstract: Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel application of knowledge distillation (KD) in the context of multilingual jailbreak prevention, examining its efficacy. We distill the refusal behaviors of a proprietary teacher model (OpenAI o1-mini) with Low-Rank Adaptation (LoRA) into three open-source student models: Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, and Qwen3-8B, using ~28,000 multilingual jailbreak prompts from XSafety via black-box response-based, parameter-efficient fine-tuning (PEFT). Evaluation on the MultiJail benchmark reveals a counterintuitive behavior: standard fine-tuning on the teacher's ``safe'' refusal data inadvertently increases Jailbreak Success Rate (JSR) for all student models, up to 16.6 percentage points. Our experiments reveal a divergent generalization to unseen languages during distillation, with varying outcomes depending on the base model. By removing a primary source of safety degradation, nuanced `boundary' refusals, we mitigate or even reverse safety declines in student models, although reductions in reasoning performance (GSM8K) persist. Overall, our exploratory study highlights the challenges and potential of KD as a technique for multilingual safety alignment, offering a foundation for future research in this direction.

</details>


### [23] [PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models](https://arxiv.org/abs/2602.11170)
*Jiawei Xu,Zhenyu Yu,Ziqian Bi,Minh Duc Pham,Xiaoyi Qu,Danyang Zhang*

Main category: cs.CL

TL;DR: PRIME框架通过三个专门代理（执行器、验证器、协调器）和群体相对策略优化，显著提升大语言模型在算法推理任务上的性能，在PRIME-Bench基准上实现从26.8%到93.8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多样化推理任务上表现出色，但在算法推理方面仍有局限，需要专门框架来提升其算法推理能力。

Method: 提出PRIME框架，包含三个专门代理：执行器（逐步推理）、验证器（约束检查）、协调器（回溯控制），通过群体相对策略优化进行优化。同时构建PRIME-Bench基准，包含86个任务、12个类别、51,600个实例。

Result: PRIME将平均准确率从26.8%提升至93.8%（相对提升250%）。在需要持续状态跟踪的任务上改进最大：图灵机模拟从9%提升至92%，长除法从16%提升至94%。迭代验证是主要贡献因素，防止错误传播。小模型受益更大，能达到比其大8倍模型的准确率。

Conclusion: PRIME框架通过专门代理和迭代验证机制，显著提升大语言模型的算法推理能力，特别是对小模型效果更明显，为解决算法推理问题提供了有效方案。

Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.

</details>


### [24] [Are Aligned Large Language Models Still Misaligned?](https://arxiv.org/abs/2602.11305)
*Usman Naseem,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Agrima Seth*

Main category: cs.CL

TL;DR: 本文提出了Mis-Align Bench基准测试，用于同时评估大语言模型在安全、价值观和文化三个维度的错位问题，解决了现有基准只能单独评估单一维度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型错位评估基准（如INSECURE CODE、VALUEACTIONLENS、CULTURALHERITAGE）只能单独评估安全、价值观或文化等单一维度的错位，而现实世界中这些维度需要同时满足才能解决实际问题。这种单维度评估无法反映模型在真实场景中的综合表现。

Method: 1. 构建SAVACU数据集：包含382,424个样本，涵盖112个领域，通过重新分类LLM-PROMPT-DATASET的提示，使用Mistral-7B-Instruct-v0.3将其分为14个安全领域、56个价值观领域和42个文化领域；2. 使用Llama-3.1-8B-Instruct通过SimHash指纹扩展低资源领域；3. 通过两阶段拒绝采样将提示与错位和对齐的响应配对以确保质量；4. 对通用、微调和开源权重模型进行基准测试。

Result: 实验表明，单维度模型在覆盖率上可达97.6%，但在联合条件下假失败率超过50%，对齐分数较低（63%-66%）。这揭示了现有单维度评估方法的局限性，无法准确反映模型在真实多维度场景中的表现。

Conclusion: Mis-Align Bench提供了一个统一的基准测试框架，能够同时评估大语言模型在安全、价值观和文化三个维度的错位问题。研究结果表明，仅优化单一维度的模型在综合多维度评估中表现不佳，强调了开发能够同时满足多个对齐维度的大语言模型的重要性。

Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.

</details>


### [25] [LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss](https://arxiv.org/abs/2602.12005)
*Szilvia Ujváry,Louis Béthune,Pierre Ablin,João Monteiro,Marco Cuturi,Michael Kirchhof*

Main category: cs.CL

TL;DR: 提出LaCy预训练方法，通过语法分析器辅助判断哪些token应该由小语言模型学习预测，哪些应该委托给大模型，以解决小模型事实错误问题。


<details>
  <summary>Details</summary>
Motivation: 小语言模型参数容量有限，容易产生事实错误。虽然可以通过访问外部资源（如大模型、文档、数据库）来缓解，但需要解决哪些token应该由小模型学习预测、哪些应该委托给外部资源的核心问题。

Method: 提出LaCy预训练方法，使用spaCy语法分析器增强损失信号，判断哪些token是真实可接受的替代延续（即使损失高），哪些应该通过<CALL> token委托给外部资源，防止事实错误。

Result: LaCy模型成功学会了哪些token应该预测、哪些应该委托，在与大模型级联生成时获得更高的FactScore，性能优于Rho或LLM-judge训练的小模型，且更简单、更便宜。

Conclusion: 通过语法分析器辅助的token选择策略，小语言模型可以更智能地决定何时自行预测、何时委托外部资源，有效提高事实准确性，同时保持简单和成本效益。

Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [26] [H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model](https://arxiv.org/abs/2602.11291)
*Wenyuan Chen,Jinbang Huang,Oscar Pang,Zhiyuan Li,Xiao Hu,Lingfeng Zhang,Zhanguang Zhang,Mark Coates,Tongtong Cao,Xingyue Quan,Yingxue Zhang*

Main category: cs.RO

TL;DR: 提出分层世界模型(H-WM)，结合高层逻辑世界模型和低层视觉世界模型，统一预测逻辑和视觉状态转换，解决机器人长时程规划中的错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法多关注视频生成或自然语言预测，难以直接与机器人动作关联，且长时程任务中错误会累积。传统符号逻辑世界模型虽然可执行且鲁棒，但缺乏视觉感知的同步预测能力。

Method: 提出分层世界模型(H-WM)，采用双层框架：高层逻辑世界模型处理符号状态转换，低层视觉世界模型处理视觉状态转换。两者联合预测，提供稳定的中间指导。使用对齐机器人动作、符号状态和视觉观察的数据集进行训练。

Result: 在视觉-语言-动作(VLA)控制策略上的实验证明了方法的有效性和通用性。分层输出为长时程任务提供稳定一致的中间指导，减少错误累积，实现跨扩展任务序列的鲁棒执行。

Conclusion: H-WM成功整合了符号推理的机器人可执行性和长时程鲁棒性与视觉观察的感知基础，为机器人规划和控制提供了统一的双层世界模型框架。

Abstract: World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.

</details>


### [27] [ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control](https://arxiv.org/abs/2602.11321)
*Ziyan Xiong,Lixing Fang,Junyun Huang,Kashu Yamazaki,Hao Zhang,Chuang Gan*

Main category: cs.RO

TL;DR: ExtremControl是一个低延迟全身控制框架，通过SE(3)位姿直接操作、笛卡尔空间映射和速度前馈控制，实现50ms端到端延迟，支持乒乓球平衡、杂耍等高响应性行为。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人遥操作系统依赖预处理的重定向和位置控制，导致高延迟（约200ms），限制了响应性和快速反馈任务的能力。需要解决低延迟控制问题以实现高响应性行为。

Method: 1) 直接在选定刚性链接（主要是人形机器人末端）的SE(3)位姿上操作，避免全身重定向；2) 使用笛卡尔空间映射将人体运动直接转换为机器人链接目标；3) 在底层加入速度前馈控制以支持快速变化的控制接口下的高响应行为。

Result: 实现了端到端延迟低至50ms的遥操作系统，支持光学动作捕捉和VR运动跟踪，能够完成乒乓球平衡、杂耍和实时回击等高响应性任务，显著超越了先前工作的200ms延迟限制。

Conclusion: ExtremControl框架有效解决了人形机器人遥操作系统的延迟问题，通过简化控制流程和引入速度前馈机制，实现了高响应性的动态行为控制，为收集多样化反应性演示数据提供了可行方案。

Abstract: Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.

</details>
