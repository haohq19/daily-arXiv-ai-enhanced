<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 20]
- [cs.LG](#cs.LG) [Total: 18]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: 本研究探讨了使用合成图像补充有限训练数据来提高麝牛检测性能的方法，在零样本和少样本设置下验证了合成图像的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统野生动物调查方法资源密集且受限于后勤挑战，而深度学习目标检测模型在小数据集上效果有限，特别是对于像麝牛这样稀疏分布的物种。

Method: 比较了仅使用真实图像的基线模型与5个零样本和5个少样本模型，这些模型在训练集中逐步加入更多合成图像。

Result: 零样本模型中，添加合成图像提高了检测性能，但超过基线模型训练数据集100%后出现收益递减。少样本模型中，真实图像与合成图像结合使用可获得更好的召回率和略高的整体准确率。

Conclusion: 合成图像在数据稀缺时能够训练准确的目标检测模型，为野生动物监测提供了重要视角，特别是对于稀有或难以接近的物种。

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [2] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: 首次使用事件相机进行图像去雾，通过事件引导的扩散模型将事件的高动态范围信息传输到RGB图像中，解决传统方法在雾霾场景下动态范围有限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB图像的去雾方法受限于有限的动态范围，容易丢失结构和光照细节。事件相机具有更高的HDR（120dB vs 60dB）和微秒级延迟，更适合雾霾场景。

Method: 提出事件引导的扩散模型，设计事件引导模块将稀疏的HDR事件特征（如边缘、角点）映射到扩散潜在空间，为生成过程提供精确的结构指导。

Result: 在两个基准测试和自建的重雾霾无人机数据集（AQI=341）上实现了最先进的去雾效果。

Conclusion: 事件相机结合扩散模型能够有效解决雾霾场景下的图像去雾问题，通过HDR信息传输显著提升去雾图像的质量和真实感。

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [3] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: 提出了LIHE框架解决弱监督广义指称表达理解任务，通过两阶段方法处理可变数量目标的表达，结合双曲和欧几里得几何解决语义表示崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督指称表达理解方法受限于一对一映射假设，无法处理现实场景中对应零个或多个目标的表达，需要更实用的广义指称表达理解范式。

Method: LIHE框架包含两个阶段：指称解耦阶段预测目标数量并分解复杂表达为子表达；指称定位阶段使用HEMix混合相似度模块，结合欧几里得几何的精确对齐能力和双曲几何的层次建模优势。

Result: 在gRefCOCO和Ref-ZOM数据集上建立了首个有效的弱监督WGREC基准，HEMix在标准REC基准上实现一致改进，IoU@0.5提升高达2.5%。

Conclusion: LIHE框架成功解决了弱监督广义指称表达理解任务，通过混合几何方法有效防止语义崩溃，为处理可变数量目标的指称表达提供了有效解决方案。

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [4] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: GCAgent是一个全局上下文感知代理框架，通过示意图和叙事情景记忆解决长视频理解中的长期依赖问题，在Video-MME基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中的token限制和长期时间依赖捕获困难的问题，现有方法难以捕捉全局上下文和复杂事件关系。

Method: 提出GCAgent框架，核心创新是示意图和叙事情景记忆，将事件及其因果和时间关系结构化建模为有组织的上下文，采用感知-行动-反思的多阶段循环，使用记忆管理器检索相关情景上下文。

Result: 在Video-MME Long split上相比强基线MLLM实现了23.5%的准确率提升，在7B规模MLLM中达到最先进性能，Long split准确率73.4%，整体平均71.9%。

Conclusion: 基于代理的推理范式和结构化记忆为认知启发的长视频理解提供了有效解决方案，验证了该框架的优越性。

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [5] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: PACE是首个针对SNN和事件视觉的数据集蒸馏框架，通过ST-DSM和PEQ-N模块将大型训练数据集压缩成紧凑的合成数据集，实现快速SNN训练，在多个数据集上显著减少训练时间和存储成本。


<details>
  <summary>Details</summary>
Motivation: 事件相机与SNN的生物学特性相契合，但SNN由于时间编码导致训练成本高昂，限制了实际部署。为降低SNN训练成本，需要开发高效的训练方法。

Method: PACE包含两个核心模块：ST-DSM使用残差膜电位来稠化基于尖峰的特征，并进行细粒度的时空匹配；PEQ-N提供即插即用的概率整数量化器，兼容标准事件帧流水线。

Result: 在DVS-Gesture、CIFAR10-DVS和N-MNIST数据集上，PACE优于现有的核心集选择和数据集蒸馏基线，特别是在动态事件流和低/中等IPC下表现突出。在N-MNIST上达到84.4%准确率，约为完整训练集性能的85%，同时减少训练时间50倍以上，存储成本6000倍。

Conclusion: PACE能够生成紧凑的替代数据集，实现分钟级的SNN训练和高效的边缘部署，为事件视觉和SNN的实际应用提供了可行的解决方案。

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [6] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM是首个面向SNN的半结构化N:M剪枝框架，通过M路基对数参数化和可微分top-k采样器，线性化计算复杂度，结合神经科学启发的资格蒸馏方法，在保持精度的同时实现硬件友好的稀疏模式。


<details>
  <summary>Details</summary>
Motivation: 解决SNN深度架构参数膨胀和计算成本高的问题，现有剪枝方法要么难以在通用硬件上加速（非结构化），要么缺乏灵活性且在高稀疏度下精度下降（结构化）。

Method: 提出半结构化N:M剪枝框架，使用M路基对数参数化和可微分top-k采样器线性化复杂度，结合资格蒸馏方法将时间累积信用转换为块级软目标。

Result: 在2:4稀疏度下，SpikeNM在主流数据集上保持甚至提升了精度，同时产生硬件友好的稀疏模式，与固有脉冲稀疏性互补。

Conclusion: SpikeNM成功平衡了SNN的稀疏性、精度和硬件友好性，为边缘部署提供了有效的解决方案。

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [7] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: 提出了一种轻量级C语言运行时系统，用于在边缘设备上高效执行脉冲神经网络推理，通过优化数据布局和利用稀疏性实现了显著的加速和内存减少。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)具有事件驱动特性和能效优势，但在边缘设备上的训练和部署仍面临挑战，需要轻量级运行时来充分发挥其潜力。

Method: 将SNNTorch训练的模型转换为紧凑的C表示，采用静态缓存友好的数据布局和预分配策略，并利用稀疏脉冲活动修剪不活跃的神经元和突触。

Result: 在N-MNIST和ST-MNIST数据集上实现了与Python基线功能相当的精度，在桌面CPU上获得约10倍加速，通过修剪获得额外增益，大幅减少内存占用，可在Arduino Portenta H7等微控制器上部署。

Conclusion: 当配备优化运行时和脉冲驱动模型压缩时，脉冲神经网络可以在传统嵌入式平台上高效执行。

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [8] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: TMKT是一个跨模态训练框架，通过时间步混合策略将RGB和DVS输入在多个时间步进行插值，结合两个轻量级模态感知目标，实现从RGB到事件相机的有效知识迁移。


<details>
  <summary>Details</summary>
Motivation: 事件相机和脉冲神经网络的结合具有能效优势，但事件数据稀缺且DVS输出稀疏，传统RGB到DVS的知识迁移方法因模态分布差异大而效果不佳。

Method: 提出时间步混合知识迁移框架，包含概率时间步混合策略和两个模态感知目标：模态感知指导和混合比感知，通过插值RGB和DVS输入并显式对齐时间特征与混合计划。

Result: 在多个基准测试和SNN骨干网络上的广泛实验表明，该方法实现了更平滑的知识迁移，缓解了模态不匹配问题，在脉冲图像分类任务中取得了优越性能。

Conclusion: TMKT框架通过时间步混合和模态感知目标，有效解决了RGB到事件相机的知识迁移问题，为跨模态视觉智能提供了有效解决方案。

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [9] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: 提出了DHMI，首个针对深度哈希的扩散模型反转攻击框架，能够在黑盒场景下从哈希码重构高质量原始图像，揭示了深度哈希系统的严重隐私风险。


<details>
  <summary>Details</summary>
Motivation: 深度哈希系统虽然提高了检索效率，但存在被忽视的严重隐私风险——攻击者可能从哈希码重构原始训练数据，导致生物特征伪造和隐私泄露。现有方法无法适应深度哈希的离散汉明空间特性。

Method: DHMI首先对辅助数据集聚类得到语义哈希中心作为代理锚点，然后提出代理引导的去噪优化方法，融合分类一致性和哈希邻近度作为攻击指标动态选择候选样本，通过代理模型簇指导候选样本细化。

Result: 在多个数据集上的实验表明，DHMI即使在最具挑战性的黑盒设置下也能成功重构高分辨率、高质量的图像，在黑盒场景下优于现有最先进的模型反转攻击方法。

Conclusion: DHMI证实了深度哈希系统存在严重的隐私风险，该框架能够有效从哈希码重构原始数据，为深度哈希系统的安全评估提供了重要参考。

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [10] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态外参标定框架，可同时估计事件相机、LiDAR和RGB相机之间的相对位姿，特别关注具有挑战性的事件相机标定。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等复杂视觉系统中，精确的多传感器对齐至关重要。现有方法通常依赖单独的双对标定，需要更高效的一站式联合标定方案。

Method: 设计并构建了新型3D标定靶标，包含平面特征、ChArUco图案和主动LED模式，分别针对LiDAR、RGB相机和事件相机的特性。实现一次性联合外参标定过程。

Result: 在自定义数据集上的广泛实验评估证实了方法的准确性和鲁棒性，该数据集使用先进的自动驾驶传感器设置记录。

Conclusion: 该方法为复杂视觉系统提供了一种准确、鲁棒的多传感器联合标定解决方案，特别适用于自动驾驶应用场景。

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [11] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: DT-R1是一个强化学习框架，通过训练大语言模型构建多模态视觉输入的数字化双胞胎表示，并基于这些高层表示进行统一视觉推理，在六个基准测试中超越任务特定模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法依赖任务特定的监督微调和架构设计，缺乏统一解决方案，限制了跨任务和跨模态的泛化能力。

Method: 使用GRPO强化学习训练大语言模型构建数字化双胞胎表示，并通过新颖的奖励函数验证结构完整性和输出准确性。

Result: 在涵盖两种模态和四种任务类型的六个视觉推理基准测试中，DT-R1持续优于最先进的任务特定模型。

Conclusion: DT-R1开创了通过数字化双胞胎表示的强化学习实现视觉推理的新方向。

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [12] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 提出了一个轻量级推理时框架，通过深度特征空间解耦偶然不确定性和认知不确定性，无需采样、集成或额外前向传播，实现计算效率提升和更紧密的预测区间。


<details>
  <summary>Details</summary>
Motivation: 传统估计器将所有不确定性模式压缩为单一置信度分数，无法可靠指导何时分配更多计算资源或调整推理策略。

Method: 使用正则化全局密度模型估计偶然不确定性，认知不确定性由三个互补组件构成：局部支持不足、流形谱塌陷和跨层特征不一致性。这些组件经验正交且无需额外计算。

Result: 在MOT17数据集上减少约60%计算量且精度损失可忽略，不确定性分解比总不确定性基线提高13.6个百分点的计算节省。

Conclusion: 该方法实现了实用的自调节视觉推理，通过正交不确定性分解显著提升计算效率。

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [13] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: 论文提出了视觉思维链(vCoT)方法，通过生成帧间过渡事件描述来提升多模态大语言模型的视频理解能力，发现视频微调模型已隐含掌握时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型从图像扩展到视频时，通常只是简单拼接帧token，缺乏对视频时序关系的有效建模。

Method: 提出视觉思维链(vCoT)，在连续帧之间生成过渡事件描述作为显式推理过程，系统比较图像模型与视频微调模型在有无过渡线索下的表现。

Result: vCoT显著提升图像模型在长视频问答中的性能，但对视频微调模型增益有限；视频模型能将时序推理能力迁移到静态场景，在关系视觉推理任务中优于图像模型。

Conclusion: 视频微调模型已隐含掌握帧间过渡推理能力，这种时序推理能力可迁移到静态视觉任务，为多模态模型设计提供了新思路。

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [14] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: EndoSight AI是一个深度学习系统，用于实时检测和分割胃肠道息肉，在GPU上达到35+ FPS的推理速度，检测mAP为88.3%，分割Dice系数达69%。


<details>
  <summary>Details</summary>
Motivation: 在内窥镜检查过程中精确实时检测胃肠道息肉对于结直肠癌的早期诊断和预防至关重要。

Method: 基于公开的Hyper-Kvasir数据集开发深度学习架构，采用临床相关性能指标和热感知训练程序确保模型鲁棒性。

Result: 系统实现88.3%的平均精度用于息肉检测，分割Dice系数达69%，GPU推理速度超过35帧/秒。

Conclusion: 该集成AI解决方案旨在无缝部署到内窥镜工作流程中，有望提高胃肠道医疗的诊断准确性和临床决策能力。

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [15] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 提出了REVISOR框架，通过跨模态反思机制增强多模态大语言模型在长视频理解中的推理能力，无需额外监督微调或外部模型。


<details>
  <summary>Details</summary>
Motivation: 纯文本反思机制在长视频理解中存在局限性：1) 仅反思文本信息不足以处理丰富的动态视觉输入；2) 缺乏跨模态交互能力，无法在反思中充分整合视觉信息。

Method: REVISOR框架支持文本和视觉模态的协作反思过程，设计了双归因解耦奖励机制(DADR)与GRPO训练策略结合，确保模型推理与所选视频证据的因果对齐。

Result: 在VideoMME、LongVideoBench、MLVU和LVBench四个基准测试中取得了显著成果，显著提升了MLLMs的长视频理解能力。

Conclusion: REVISOR通过跨模态反思机制有效解决了长视频理解中纯文本反思的局限性，为多模态推理提供了新的解决方案。

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [16] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: VEIL是一个针对文本到视频模型的越狱攻击框架，通过包含中性场景锚点、潜在听觉触发器和风格调制器的模块化提示设计，利用跨模态关联模式诱导模型生成违反安全策略的视频。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频模型越狱攻击通常通过添加明显不安全的对抗性扰动来实现，容易被检测和防御。本文旨在开发更隐蔽的攻击方法，利用看似良性的提示诱导模型生成语义上不安全的视频。

Method: 提出VEIL框架，采用模块化提示设计：中性场景锚点提供表面场景描述保持合理性；潜在听觉触发器利用音频-视觉共现先验；风格调制器通过电影指令增强触发效果。将攻击生成形式化为约束优化问题，使用引导搜索算法平衡隐蔽性和有效性。

Result: 在7个文本到视频模型上的实验表明，该攻击方法有效，在商业模型中平均攻击成功率提升了23%。

Conclusion: VEIL框架证明了通过精心设计的模块化提示，可以利用文本到视频模型的跨模态关联模式实现隐蔽的越狱攻击，揭示了现有安全防护的盲点。

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [17] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: RefineVAD是一个弱监督视频异常检测框架，通过模拟人类感知异常的双过程推理，联合利用时间动态和语义结构来检测不同类型的异常事件。


<details>
  <summary>Details</summary>
Motivation: 现有方法将异常事件视为单一类别，忽略了真实异常中多样的语义和时间特性。受人类感知异常方式的启发，需要同时解释时间运动模式和不同异常类型的语义结构。

Method: 提出RefineVAD框架，包含两个核心模块：MoTAR（运动感知时间注意力和重校准）估计运动显著性并通过移位注意力和Transformer动态调整时间焦点；CORE（类别导向细化）通过跨注意力将片段级特征与可学习类别原型对齐，注入软异常类别先验。

Result: 在WVAD基准测试上的广泛实验验证了RefineVAD的有效性，并强调了整合语义上下文以引导特征细化朝向异常相关模式的重要性。

Conclusion: 通过联合利用时间动态和语义结构，RefineVAD能够显式建模运动如何演化以及它类似于什么语义类别，为弱监督视频异常检测提供了更全面的解决方案。

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [18] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 提出了一种使用视频级监督检测监控视频中罕见多样异常的双主干框架，在UCF-Crime数据集上达到90.7% AUC


<details>
  <summary>Details</summary>
Motivation: 解决在仅有视频级监督的情况下检测监控视频中罕见且多样异常的挑战

Method: 双主干框架结合卷积和transformer表示，通过top-k池化整合特征

Result: 在UCF-Crime数据集上实现了90.7%的AUC性能

Conclusion: 该双主干框架能有效检测监控视频中的罕见异常，仅需视频级监督即可达到良好性能

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [19] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: TripleFDS是一个用于场景文本编辑的新框架，通过解耦文本样式、文本内容和背景三个属性，实现了更灵活和一致的文本编辑。该方法在主流基准测试中达到了最先进的图像保真度和文本准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的场景文本编辑方法在可编辑属性的解耦方面存在不足，通常只能处理文本内容编辑等单一方面，限制了可控性和视觉一致性。

Method: 提出TripleFDS框架和SCB Synthesis数据集，使用SCB Group作为基本训练单元，通过组间对比正则化和组内多特征正交性实现三特征解耦，在合成阶段进行特征重映射以防止重建中的"捷径"现象。

Result: 在125,000个SCB Groups上训练后，TripleFDS在主流STE基准测试中实现了44.54的SSIM和93.58%的ACC，达到了最先进的图像保真度和文本准确性。

Conclusion: TripleFDS不仅性能优越，还支持更灵活的编辑操作，如样式替换和背景转移，为场景文本编辑提供了更全面的解决方案。

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [20] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 提出VVS框架，通过部分验证跳过来加速视觉自回归模型生成，减少目标模型前向传递次数2.8倍，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型存在推理延迟问题，传统推测解码的'一步草稿、一步验证'范式无法直接减少前向传递次数，限制了加速潜力。

Method: 基于验证冗余和特征可重用性的观察，提出VVS框架，包含三个模块：动态截断的验证无关令牌选择器、令牌级特征缓存与重用、细粒度跳过步调度。

Result: 相比原始自回归解码，VVS将目标模型前向传递次数减少2.8倍，同时保持竞争力的生成质量。

Conclusion: VVS提供了优于传统推测解码框架的速度-质量权衡，展示了重塑推测解码范式的强大潜力。

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: 提出双向剪枝-再生策略，从满足硬件约束的极度压缩网络开始，选择性再生关键连接来恢复性能，解决高稀疏度下模型性能急剧下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当稀疏度超过特定阈值时，迭代和一次性剪枝方法都会导致模型性能急剧下降，限制了可实现的压缩比，无法满足某些硬件平台的严格尺寸约束。

Method: 双向剪枝-再生策略：从极度压缩的网络开始，选择性再生关键连接来恢复丢失的性能。

Result: 有效缓解了高稀疏度条件下常见的精度急剧下降问题。

Conclusion: 该方法能够克服传统剪枝方法在高稀疏度下的性能限制，使模型能够满足硬件平台的尺寸约束要求。

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [22] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: 提出了Learning with Preserving (LwP)框架，通过保持共享表示空间的几何结构来解决持续多任务学习中的灾难性遗忘问题，无需重放缓冲区。


<details>
  <summary>Details</summary>
Motivation: 关键领域的人工智能系统需要持续学习新任务而不遗忘先前能力，现有方法因学习碎片化的任务特定特征而失败。

Method: 引入动态加权距离保持(DWDP)损失，通过正则化潜在数据表示之间的成对距离来防止表示漂移，保持共享表示空间的几何结构。

Result: 在时间序列和图像基准测试中，LwP不仅缓解了灾难性遗忘，而且持续优于最先进的基线方法，是唯一超越强单任务学习基线的方法。

Conclusion: LwP框架通过保持表示空间的几何结构，有效支持现实世界动态环境中的持续多任务学习，具有对分布偏移的鲁棒性。

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [23] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: 比较GPT-2和LSTM两种深度学习模型在森林CO₂吸收量预测中的性能，发现LSTM整体表现更好，但GPT-2在极端事件中更优，揭示了模型架构、上下文长度和多模态输入对预测性能的共同影响。


<details>
  <summary>Details</summary>
Motivation: 解决森林CO₂吸收量时空动态监测的挑战，传统方法如涡度协方差塔空间覆盖有限，遥感方法大多依赖单一传感器和统计模型，难以捕捉复杂的GPP时间动态。

Method: 使用GPT-2（Transformer架构）和LSTM（循环神经网络）两种代表性深度学习模型，结合多变量输入进行GPP预测，分析不同时间上下文长度的影响。

Result: 两种模型达到相似精度，LSTM整体表现更好，但GPT-2在极端事件中表现更优。LSTM使用更短的输入窗口就能达到类似精度，辐射是最重要的预测因子。

Conclusion: 模型架构、上下文长度和多模态输入共同决定GPP预测性能，为开发监测陆地碳动态的深度学习框架提供指导。

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [24] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: SurvBench是一个开源预处理管道，将原始PhysioNet数据集转换为标准化的多模态生存分析张量，解决深度学习生存模型的可复现性问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据为深度学习生存分析提供了巨大机会，但由于预处理方法不一致，可复现性受到严重限制。

Method: 提供三个重症监护数据库的数据加载器，实现严格的数据质量控制、患者级分割、显式缺失值跟踪和标准化时间聚合，支持单风险和竞争风险场景。

Result: 输出与pycox库兼容，支持标准统计和深度学习模型，解决了阻碍深度学习生存模型公平比较的"预处理差距"。

Conclusion: 通过提供可复现的、配置驱动的预处理和全面文档，SurvBench使研究人员能够专注于方法创新而非数据工程。

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [25] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: 提出了一个多模态金融交易基础模型，整合结构属性和非结构化文本描述，在数据稀缺的开放银行场景中表现优异，是北美首个大规模跨金融机构研究。


<details>
  <summary>Details</summary>
Motivation: 解决传统特征工程和离散事件序列方法在金融交易分析中的局限性，特别是在数据稀缺的开放银行场景下，探索多模态表示在金融应用中的潜力。

Method: 采用掩码语言建模适应交易序列，整合结构化属性和非结构化文本描述，构建统一的多模态表示。

Result: 模型性能超越传统方法，在数据稀缺场景表现尤其突出，证明多模态表示能够跨地理区域和金融机构泛化。

Conclusion: 自监督模型在金融应用（如欺诈预防、信用风险和客户洞察）方面具有巨大潜力，多模态表示是推进金融技术发展的有效途径。

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [26] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: 提出了P³HF模型，通过个性引导表示学习、超图-Transformer架构和事件级领域解耦，在抑郁症检测任务上相比现有方法准确率和加权F1值提升约10%。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer或图神经网络的抑郁症检测方法在建模个体差异和跨模态时间依赖性方面存在挑战，需要更有效的个性化检测方法。

Method: 使用LLM进行个性引导表示学习，将离散个体特征转化为上下文描述；采用超图-Former架构建模高阶跨模态时间关系；通过事件级领域解耦和对比学习提高跨行为上下文的泛化能力。

Result: 在MPDD-Young数据集上，P³HF在二元和三元抑郁症分类任务上的准确率和加权F1值相比现有方法提升约10%。

Conclusion: 个性引导表示学习和高阶超图推理对于生成鲁棒的、个体感知的抑郁症相关表示都是必不可少的。

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [27] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出了AdaMeshNet框架，在基于网格的图神经网络中引入自适应重连过程，通过计算重连延迟分数来动态选择消息传递层进行重连，有效缓解过压缩问题并提高流体模拟精度。


<details>
  <summary>Details</summary>
Motivation: 传统网格重连方法在应用GNN前完成所有重连操作，假设远距离节点间瞬时相互作用且忽略粒子间距离信息，这在物理上不现实。需要一种能模拟物理相互作用逐步传播的方法。

Method: AdaMeshNet框架在消息传递过程中引入自适应重连，为网格图中的瓶颈节点计算基于最短路径距离和速度差的重连延迟分数，动态选择消息传递层进行重连。

Result: 在基于网格的流体模拟实验中，AdaMeshNet优于传统重连方法，能有效模拟物理相互作用的顺序特性，实现更准确的预测。

Conclusion: 自适应重连过程能更好地建模物理相互作用的逐步传播，解决传统方法在物理现实性和距离信息处理方面的局限性，提高网格GNN在流体动力学建模中的性能。

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [28] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: COLoKe是一个自适应更新非线性动力系统Koopman嵌入的新框架，结合深度特征学习和提升空间中的多步预测一致性，通过一致性评估机制选择性更新模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性动力系统建模中Koopman表示的自适应更新问题，防止过拟合并减少不必要的模型更新。

Method: 结合深度特征学习和多步预测一致性，在提升的线性空间中建模动力学，采用一致性评估机制动态校准阈值来触发选择性更新。

Result: 在基准动力系统上的实验表明，COLoKe能保持长期预测准确性，同时显著减少不必要的更新并避免过拟合。

Conclusion: COLoKe框架有效实现了Koopman嵌入的自适应在线学习，在保持预测精度的同时优化了更新效率。

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [29] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: 该论文从贝叶斯高斯视角研究神经网络特征的可解释性，通过希尔伯特空间和算子分解分析自编码器和混合密度网络，提出基于高斯算子迹和核范数的训练方法，以及使用内积和范数定义边界以增加样本多样性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络特征的可解释性，从概率角度理解优化过程，探索自编码器和混合密度网络的训练目标与概率边界的关系。

Method: 使用希尔伯特空间和算子分解方法，分析多输出网络产生的高斯混合模型；提出基于高斯算子迹训练自编码器，基于核范数训练MDNs；使用内积和范数定义边界以防止平凡解。

Result: 发现自编码器目标等价于最大化高斯算子迹；提出核范数作为MDNs的散度度量；设计了编码器-混合-解码器架构，解码器为多输出以产生多个中心。

Conclusion: 从贝叶斯高斯视角为神经网络训练提供了新的可解释性框架，通过算子迹和核范数方法改进了自编码器和混合密度网络的训练策略。

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [30] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: 提出基于FEDformer的混合框架，用于金融时间序列的异常检测和风险预测，相比传统方法在RMSE和F1分数上有显著提升


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度波动性，传统深度学习模型难以捕捉长期依赖和复杂周期模式，需要更有效的异常检测和风险预测方法

Method: 集成频率增强分解变换器(FEDformer)与基于残差的异常检测器和风险预测头，在时域和频域同时建模时间动态，分解信号为趋势和季节分量

Result: 在S&P 500、NASDAQ和布伦特原油数据集上实验显示，相比基准方法RMSE降低15.7%，异常检测F1分数提升11.5%

Conclusion: 该模型能有效捕捉金融波动性，为市场崩盘预测和风险管理提供可靠的早期预警系统

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [31] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的GCTAF架构，通过可学习的跨注意力全局令牌来增强长时间序列建模，用于解决太阳耀斑预测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑预测面临的主要挑战是类别不平衡问题——强烈耀斑事件相比轻微耀斑事件非常罕见，这阻碍了有效的学习。

Method: GCTAF架构引入可学习的跨注意力全局令牌，这些令牌通过跨注意力与输入序列交互并融合回时间表示中，能够识别对耀斑预测至关重要的全局显著非连续时间点。

Result: 在基准太阳耀斑数据集上的评估表明，GCTAF能够有效检测强烈耀斑并提高预测性能。

Conclusion: 改进基于Transformer的架构为太阳耀斑预测任务提供了高潜力的替代方案。

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [32] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的神经算子(PINO)方法，用于金属增材制造中实时预测未来15秒的z和y方向变形场，解决了传统数值模拟计算成本高和机器学习模型难以提取时空特征的问题。


<details>
  <summary>Details</summary>
Motivation: 数字孪生和智能制造系统的发展迫切需要实时变形场预测来控制金属增材制造缺陷，但传统数值模拟方法计算成本高、运行时间长，而传统机器学习模型难以提取长时域预测的时空特征且无法解耦热力场。

Method: 采用物理信息深度算子网络-循环神经网络(PIDeepONet-RNN)，使用分支网络处理温度历史，主干网络编码变形场，实现热力响应的解耦，并通过热传导方程作为软约束确保物理一致性。

Result: 模型在实验验证的有限元方法生成的数据集上训练测试，达到高精度、低误差累积和时间效率。z和y方向最大绝对误差分别为0.9733mm和0.2049mm，熔池区域误差较高但沉积区和关键区域梯度范数较低。

Conclusion: PINO代理模型在实时长时域物理场预测方面具有潜力，可用于缺陷控制，其基于物理定律的基函数为预测提供了稳健且可解释的基础。

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [33] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze是一种新颖的长推理路径压缩方法，能在保持性能的同时显著减少推理LLM的token使用量，通过自适应选择推理深度和分布对齐的语言精炼来实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有的推理LLM生成的长链式思维轨迹导致token使用量增加，带来更高的推理延迟和内存消耗。现有长转短方法往往牺牲准确性，需要在保持性能的同时降低token成本。

Method: 1) 自适应选择推理深度：根据问题复杂度选择自生成样本；2) 分布对齐的语言精炼：优化语言表达而不改变推理路径，保持逻辑完整性同时提高清晰度和简洁性。

Result: DeepSeek-R1-Distill-Qwen-7B使用该方法在MATH500基准上实现了50%的平均token减少，同时保持准确性。该方法仅使用模型自生成数据，无需人工标注的短答案数据集。

Conclusion: TokenSqueeze有效解决了推理LLM的效率-准确性权衡问题，为实际应用部署提供了高效且高保真的推理解决方案。

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [34] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: 提出了一种用于多模态食物学习的持续学习框架，结合双LoRA架构和质量增强伪重放，解决了现有大型多模态模型在食物分析中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的食物分析大型多模态模型在学习新任务时会出现灾难性遗忘，需要昂贵的从头重新训练。

Method: 采用双LoRA架构：专用LoRA学习任务特定知识并保持与先前任务子空间的正交性，协作LoRA通过伪重放整合跨任务共享知识。质量增强伪重放策略利用自一致性和语义相似性减少生成样本中的幻觉。

Result: 在综合Uni-Food数据集上的实验显示，该方法在减轻遗忘方面表现优异。

Conclusion: 这是首个针对复杂食物任务的有效持续学习方法。

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [35] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: 本文系统评估了六种参数空间合并技术，发现对于架构兼容的医疗大语言模型，简单的参数平均方法在计算效率和性能上优于复杂的剪枝方法，为分布式医疗AI部署提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: 解决分布式医疗中大语言模型面临的挑战：整合跨机构的专业知识同时保护隐私、降低计算开销、防止模型更新时的灾难性遗忘。

Method: 提出分层方法，结合选择性最优传输对齐注意力层和余弦相似度加权插值，评估六种参数合并技术（任务算术、线性平均、DARE-TIES、DELLA、Breadcrumbs和分层方法）。

Result: 架构兼容模型从简单平均方法中获益显著，任务算术在MedQA上达到45.80%准确率，优于复杂剪枝方法。

Conclusion: 对于架构兼容模型，简单平均提供了稳健且计算高效的知识整合基线，为可扩展医疗AI系统提供了实用路径。

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [36] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: 提出了MMWSTM-ADRAN+双流深度学习架构，结合天气状态转换模型和异常驱动注意力网络，用于极端气温事件的短期预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测极端气温事件在气候风险管理中仍是一个基本挑战，需要改进对罕见极端事件的预测能力。

Method: 使用双流架构：MMWSTM流结合双向LSTM和可学习马尔可夫状态转移矩阵捕捉天气状态变化；ADRAN流集成双向GRU、多头自注意力和异常放大层增强对低概率信号的敏感性。采用轻量级注意力融合门自适应确定各流贡献。

Result: 模型通过自定义的ExtremeWeatherLoss函数（对温度分布上下5%的误差加权）和时间序列数据增强套件（抖动、缩放、时间/幅度扭曲）有效优化训练。

Conclusion: 该架构通过耦合天气状态动态建模和异常信号增强机制，提升了极端气温事件的预测准确性。

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [37] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: Trine是一个非参数、基于核的框架，用于从时间序列数据中推断状态依赖的内在噪声，通过三阶段算法结合解析可解子问题和结构化核架构来捕获噪声驱动的突变波动和状态依赖的方差平滑变化。


<details>
  <summary>Details</summary>
Motivation: 在生物学和生态学等科学领域中，非线性相互作用和随机效应的不完全知识使得自下而上的建模方法往往无效，需要开发能够直接从数据中发现控制方程的方法。参数模型在没有强先验知识时表现不佳，特别是在估计内在噪声时，而纳入随机效应对于理解复杂系统（如基因调控网络和信号通路）的动态行为至关重要。

Method: Trine采用三阶段算法，结合解析可解子问题和结构化核架构。该框架能够捕获噪声驱动的突变波动和状态依赖的方差平滑变化，是一个非参数、基于核的方法。

Result: 在生物和生态系统上的验证表明，Trine能够在不依赖预定义参数假设的情况下揭示隐藏的动态。在多个基准问题上，Trine实现了与oracle相当的性能，这个oracle可以被视为能够直接跟踪细胞内分子浓度或反应事件随机波动的理想化观察者。

Conclusion: Trine框架为理解内在噪声如何影响复杂系统行为开辟了新途径，提供了一种有效的方法来从数据中发现控制方程并估计状态依赖的内在噪声。

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [38] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: 本文研究可解释AI在极端事件预测中的作用，以野火预测为例，使用SHAP方法揭示模型决策路径和潜在偏见，提升AI系统的可信度和实用性。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了极端事件的频率和严重性，需要准确、可解释且可操作的预测。虽然AI模型在预测方面表现出潜力，但其黑盒特性限制了在实际决策中的采用，影响了信任度和可操作性。

Method: 使用野火预测作为案例研究，评估多种AI模型，并采用SHAP方法来揭示关键特征、决策路径和模型行为中的潜在偏见。提供支持可视化以增强可解释性。

Result: 分析表明XAI不仅能澄清模型推理，还能支持领域专家和响应团队的关键决策。可视化增强了特征重要性和时空模式的可解释性。

Conclusion: 研究强调AI系统不仅需要准确，还需要可解释、可访问且值得信赖，这对于灾害准备、风险缓解和气候韧性规划至关重要。

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，能够实时检测和防止大语言模型的幻觉问题，在12个基准测试中达到89.3%的幻觉识别准确率，并将虚假声明减少近80%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在严重的幻觉问题，会自信地陈述听起来合理但实际错误的信息，这成为在准确性要求高的场景中使用这些模型的主要障碍。现有解决方案要么需要重新训练整个模型，要么增加显著计算成本，或者未能解决幻觉的根本原因。

Method: CausalGuard通过因果推理与符号逻辑相结合，在幻觉发生时进行检测和预防。系统理解导致错误陈述的因果链，并在过程中早期干预。通过两条互补路径工作：一条追踪模型知识与其生成内容之间的因果关系，另一条使用自动推理检查逻辑一致性。

Result: 在12个不同基准测试中，CausalGuard正确识别幻觉的时间达到89.3%，仅遗漏8.3%的实际幻觉。更重要的是，它将虚假声明减少了近80%，同时保持回答的自然性和帮助性。在需要多步逻辑推理的复杂推理任务上表现尤其出色。

Conclusion: CausalGuard通过展示其推理过程，在医疗诊断或金融分析等敏感领域表现良好，因为这些领域理解决策原因与决策本身同样重要。该方法为解决大语言模型的幻觉问题提供了一种有效且可解释的解决方案。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [40] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文提出了"反弹赢家通吃(RWTA)"基元作为可扩展神经形态控制架构的基本元素，该架构结合了离散计算的可靠性和连续调节的可调性。


<details>
  <summary>Details</summary>
Motivation: 开发一种统一的神经形态控制架构，能够同时处理连续节律生成和离散决策，结合离散计算的可靠性和连续调节的可调性。

Method: 使用RWTA基元构建从细胞级到系统级的架构，继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调节能力。

Result: 通过蛇形机器人神经系统设计展示了该架构的通用性、鲁棒性和模块化特性。

Conclusion: 提出的基于事件的框架在统一的物理建模语言中解决了连续节律生成和离散决策问题，为神经形态控制提供了可扩展的解决方案。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [41] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: 提出CFA-SMOTE方法，结合可解释AI的反事实生成和SMOTE过采样技术，解决气候变化数据中的类别不平衡问题，提高对气候异常事件的预测能力。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端天气事件增多，传统机器学习方法难以处理分布外数据。历史数据集缺乏足够的'气候异常事件'样本，造成类别不平衡问题。

Method: CFA-SMOTE方法将可解释AI的反事实生成与SMOTE过采样技术结合，生成代表气候异常事件的合成数据点来增强数据集。

Result: 在不同类别不平衡比例下，CFA-SMOTE方法相比基准反事实和类别不平衡方法表现出更好的预测性能。

Conclusion: CFA-SMOTE能有效解决气候变化预测中的类别不平衡问题，为处理气候异常事件提供了一种有前景的数据增强方法。

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [42] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过自学习范式将多模态大语言模型从被动模式识别转变为证据关联的诊断推理，无需大量标注数据或模型权重更新即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI工具虽然提高了筛查效率和标准化量化，但由于缺乏人类可读的推理过程，难以审计决策和防止错误，限制了临床应用。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格的解释，优化阶段精炼这些解释以提高准确性。该框架仅需少量标注数据，无需白盒访问或权重更新。

Result: 在乳腺癌和前列腺癌数据集上的评估显示，RECAP-PATH生成的推理与专家评估一致，诊断准确性相比基线方法有显著提升。

Conclusion: RECAP-PATH通过将视觉理解与推理相结合，提供了临床可信赖的AI，展示了实现证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [43] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个针对限价订单簿数据的通用基础模型，通过创新的标记化方案将多维消息作为单个标记处理，在预测中间价格变动和下一消息等任务中表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有的限价订单簿模型需要繁琐的数据表示，且缺乏原始任务之外的适应性，难以应对不规则事件时间、快速机制转换和高频交易者对可见订单流的反应等挑战。

Method: LOBERT基于BERT架构，采用新颖的标记化方案，将完整的多维消息作为单个标记处理，同时保留价格、数量和时间等连续特征的表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得领先性能，同时相比之前方法减少了所需的上下文长度。

Conclusion: LOBERT为限价订单簿数据提供了一个通用的基础模型，适合下游微调，在多个任务中表现出优越性能。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [44] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet：利用LLM量化非结构化事件报告，构建因果知识库，通过因果注意力机制将因果知识注入双流GNN-LSTM网络，显著提升非重复事件期间的交通预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时空图神经网络在处理重复性交通模式时表现优异，但在事故等非重复性事件期间可靠性急剧下降，因为GNN本质上是相关性模型，无法处理突发事件引入的新因果因素。

Method: 1）使用大语言模型量化非结构化事件报告；2）通过估计平均处理效应构建因果知识库；3）使用新颖的因果注意力机制将因果知识注入双流GNN-LSTM网络来调整和增强预测。

Result: 在真实世界数据集上的实验表明，Event-CausNet将预测误差（MAE）降低了高达35.87%，显著优于最先进的基线方法。

Conclusion: 该框架弥合了相关性模型与因果推理之间的差距，提供了更准确、可迁移的解决方案，同时提供关键的可解释性，为关键中断期间的实际交通管理提供了更可靠的基础。

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [45] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: 提出了Chain-of-Scheduling (CoS)框架，通过引导式调度过程激活大语言模型的事件调度能力，在事件社交网络中实现高效、有效且可解释的日程推荐。


<details>
  <summary>Details</summary>
Motivation: 事件社交网络中推荐日程是一个关键问题，现有方法在效率、效果和泛化性之间存在固有权衡，因为该问题具有NP难特性。

Method: 将日程任务分解为探索、验证和集成三个原子阶段，通过知识蒸馏使大语言模型能够自主生成CoS调度过程。

Result: 在三个真实世界数据集上实现了接近理论最优的效果，具有高效率且可解释，并在域外数据上表现出强大的零样本学习能力。

Conclusion: CoS框架成功激活了大语言模型的事件调度能力，解决了现有方法在效率、效果和泛化性之间的权衡问题。

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [46] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出人类共生健康智能（HSHI）框架，整合多模态传感器网络、边缘云协同计算和混合数据知识建模，实现从被动监测到主动协作的健康管理转变。


<details>
  <summary>Details</summary>
Motivation: 传统可穿戴设备依赖经验性材料设计和基础信号处理技术，存在局限性，需要克服这些限制以实现更精准的健康管理。

Method: 采用AI驱动的材料和微结构优化、多模态信号鲁棒解释、群体洞察与个性化适应的双机制，以及强化学习和数字孪生的闭环优化。

Result: HSHI框架能够动态适应个体间和个体内变异性，促进预防性、适应性的健康管理模式。

Conclusion: HSHI代表了医疗保健向预防、适应性和技术与健康管理和谐关系模式的重大转变。

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [47] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个评估LLM训练数据时间边界的基准测试，通过检测模型对近期事件的了解来识别其知识的时间限制。


<details>
  <summary>Details</summary>
Motivation: LLM在特定时间点前的文本数据上预训练，形成了严格的知识边界。当这一限制未知或被忽视时，模型可能在推理任务中无意间混合过时的时效性信息与通用知识，从而影响回答准确性。

Method: 引入LLMLagBench作为系统性方法，通过评估LLM对近期事件的知识来识别其训练数据的最早可能时间边界。该方法应用于大量LLM，包括明确声明和未声明训练截止时间的模型。

Result: 通过人工验证和与公开的LLM预训练信息比较，评估了该基准的可靠性。

Conclusion: LLMLagBench提供了一种系统性的方法来识别LLM的知识时间边界，有助于理解模型的知识局限性。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [48] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出通过中断机制来增强大语言模型的对齐性，该方法在用户输入中定期插入控制语句，以防止模型被越狱和恶意行为。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐研究主要关注通过训练和提示来提高模型对抗攻击的鲁棒性，但缺乏随着用户输入长度增加而有效增强对齐的方法。研究表明，LLM被越狱的概率会随着用户输入或对话长度的增加而上升。

Method: 提出中断机制，即在用户输入中每隔x个token插入控制语句，这种方法可以推广到思维链过程中以防止恶意计划。

Result: 论文提出了一种新的对齐增强方法，但未提供具体的实验结果数据。

Conclusion: 中断机制是增强大语言模型对齐性的一个潜在解决方案，能够随着用户输入长度的增加而有效防止模型被越狱。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [49] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: Agent-Event-Coder (AEC) 是一个多智能体框架，将零样本事件抽取视为类似软件工程的代码生成过程，通过专门的智能体协作解决LLM在事件抽取中的结构输出问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在零样本事件抽取中面临的挑战，包括不完整输出、错误分类触发词、缺失参数和模式违规等问题。

Method: 将事件抽取分解为检索、规划、编码和验证四个专门子任务，每个任务由专门的LLM智能体处理，事件模式表示为可执行的类定义，通过验证智能体进行确定性验证和精确反馈。

Result: 在五个不同领域和六个LLM上的实验表明，AEC始终优于先前的零样本基线方法，展示了将事件抽取视为代码生成的有效性。

Conclusion: 通过编程启发的方法和协作智能体工作流，AEC能够使LLM在零样本设置下产生精确、完整且模式一致的事件抽取结果。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [50] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 本文提出了一种基于方面情感分析(ABSA)的新方法，用于解码泰语财务年报中的模糊情感，并通过事件研究验证了其对股价的实际影响。


<details>
  <summary>Details</summary>
Motivation: 财务文件中的情感理解对洞察市场行为至关重要，但这些报告常使用模糊语言来呈现积极或中性前景，即使实际条件可能不利。

Method: 开发了标注模糊情感的特定指南，标注了100多份财务报告，并在该数据集上基准测试了多种文本分类模型。

Result: 在情感分类方面表现出色，事件研究表明市场反应受到报告中特定方面的选择性影响。

Conclusion: 研究结果强调了财务文本情感分析的复杂性，并凸显了解决模糊语言以准确评估市场情绪的重要性。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [51] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了O-Mem记忆框架，通过主动用户画像动态提取和更新用户特征，在长期交互中实现更好的个性化响应，在多个基准测试中超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在复杂环境中维持长期交互时面临上下文一致性和动态个性化挑战，传统基于语义分组的记忆系统会忽略语义无关但关键的用户信息并引入检索噪声。

Method: 基于主动用户画像的记忆框架，动态提取和更新用户特征与事件记录，支持人物属性和主题相关上下文的分层检索。

Result: 在LoCoMo基准上达到51.76%（比LangMem提升近3%），在PERSONAMEM上达到62.99%（比A-Mem提升3.5%），同时提升了token和交互响应时间效率。

Conclusion: 为开发高效且类人的个性化AI助手开辟了有前景的方向。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [52] [Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature](https://arxiv.org/abs/2511.11639)
*Jie Fan,Francesco Visentin,Barbara Mazzolai,Emanuela Del Dottore*

Main category: cs.RO

TL;DR: 提出了一种基于图像的3D几何建模方法，用于分析藤蔓触须在机械刺激下的形状变化，发现顶端区域响应性更高，为植物生物力学研究和仿生机器人设计提供基础。


<details>
  <summary>Details</summary>
Motivation: 研究藤蔓触须形状变化与机械刺激触发事件及接触位置之间的关系，目前仍具挑战性，需要建立这种关系的分析方法。

Method: 采用基于3D分段Clothoid模型的几何方法，重建藤蔓触须在机械摩擦后的形状配置，通过图像分析时间序列的形状变化。

Result: 重建方法具有高鲁棒性和可靠性（R2 > 0.99），相比深度学习方法具有数据需求少、计算成本低和可解释性强的优势，发现触须顶端区域响应性更高。

Conclusion: 该方法为植物生物力学研究提供了新视角，并为受攀援植物启发的智能机器人系统设计开发奠定了基础。

Abstract: Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.

</details>


### [53] [Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration](https://arxiv.org/abs/2511.12237)
*Alysson Ribeiro da Silva,Luiz Chaimowicz*

Main category: cs.RO

TL;DR: 提出了一种解决多机器人探索中通信约束和间歇连接问题的方法，包括MILP规划生成器和RTUS跟踪机制，在Gazebo仿真中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多机器人探索系统在通信受限环境下存在局限性，要么需要预先知道环境信息，要么无法生成最优计划或考虑实际轨迹，不适合现实部署。

Method: 提出了MRE-CCIC问题，使用混合整数线性规划生成会合计划，并通过RTUS机制让机器人跟踪计划，考虑未知环境条件。

Result: 在Gazebo大规模环境仿真中验证，方法能够及时跟踪计划并高效完成任务。

Conclusion: 该方法能够有效解决多机器人探索中的通信约束问题，提供了开源实现。

Abstract: Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.

</details>


### [54] [SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty](https://arxiv.org/abs/2511.12361)
*Leroy D'Souza,Akash Karthikeyan,Yash Vardhan Pant,Sebastian Fischmeister*

Main category: cs.RO

TL;DR: SAC-MoE是一种混合动力系统强化学习方法，通过混合专家模型和课程学习策略，在具有不可观测模式和切换事件的复杂系统中实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 解决混合动力系统中不可观测参数和模式切换带来的挑战，传统基于模型的方法无法处理这种不确定性，而标准无模型RL方法无法处理突然的模式切换。

Method: 将SAC框架中的行动者建模为混合专家模型，包含学习到的路由器自适应选择专家，并开发基于课程的训练算法优先收集挑战性数据。

Result: 在混合自主赛车和腿式运动任务中，SAC-MoE在零样本泛化到未见环境方面优于基线方法（最高6倍），课程策略在所有评估策略中持续提升性能。

Conclusion: SAC-MoE通过可解释的MoE路由器为不同潜在模式激活不同专家，有效解决了混合动力系统中的泛化挑战。

Abstract: Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.
  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.

</details>


### [55] [Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing](https://arxiv.org/abs/2511.13100)
*Xuecheng Chen,Jingao Xu,Wenhua Ding,Haoyang Wang,Xinyu Luo,Ruiyang Duan,Jialong Chen,Xueqian Wang,Yunhao Liu,Xinlei Chen*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机的无人机螺旋桨转速感知系统EventPro，通过精确测量螺旋桨转速来提升无人机感知性能，在真实无人机配送场景中实现了3ms的低延迟和0.23%的转速估计误差。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用的普及，从地面进行非接触式无人机感知变得至关重要。传统方法在感知性能上存在局限，需要更精确的无人机状态监测方案。

Method: 系统包含两个核心组件：Count Every Rotation通过减轻事件相机对环境噪声的超高敏感性，实现准确、实时的螺旋桨转速估计；Every Rotation Counts利用这些转速推断无人机的内外动态特性。

Result: 在真实无人机配送场景中的广泛评估显示：感知延迟仅为3ms，螺旋桨转速估计误差仅为0.23%，无人机飞行指令推断精度达96.5%，与其他感知模态结合时无人机跟踪精度提升超过22%。

Conclusion: EventPro通过专注于螺旋桨转速感知，显著提升了无人机感知性能，为无人机应用提供了高效可靠的感知解决方案。

Abstract: As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \sysname. \sysname features two components: \textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\%. Additionally, \sysname infers drone flight commands with 96.5\% precision and improves drone tracking accuracy by over 22\% when combined with other sensing modalities. \textit{ Demo: {\color{blue}https://eventpro25.github.io/EventPro/.} }

</details>
