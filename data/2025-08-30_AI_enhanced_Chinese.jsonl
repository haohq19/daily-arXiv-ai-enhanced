{"id": "2508.20221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20221", "abs": "https://arxiv.org/abs/2508.20221", "authors": ["Mert Cokelek", "Halit Ozsoy", "Nevrez Imamoglu", "Cagri Ozcinar", "Inci Ayhan", "Erkut Erdem", "Aykut Erdem"], "title": "Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos", "comment": "Accepted for publication in IEEE Transaction on Pattern Analysis and\n  Machine Intelligence (IEEE TPAMI)", "summary": "Omnidirectional videos (ODVs) are redefining viewer experiences in virtual\nreality (VR) by offering an unprecedented full field-of-view (FOV). This study\nextends the domain of saliency prediction to 360-degree environments,\naddressing the complexities of spherical distortion and the integration of\nspatial audio. Contextually, ODVs have transformed user experience by adding a\nspatial audio dimension that aligns sound direction with the viewer's\nperspective in spherical scenes. Motivated by the lack of comprehensive\ndatasets for 360-degree audio-visual saliency prediction, our study curates\nYT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying\naudio-visual conditions. Our goal is to explore how to utilize audio-visual\ncues to effectively predict visual saliency in 360-degree videos. Towards this\naim, we propose two novel saliency prediction models: SalViT360, a\nvision-transformer-based framework for ODVs equipped with spherical\ngeometry-aware spatio-temporal attention layers, and SalViT360-AV, which\nfurther incorporates transformer adapters conditioned on audio input. Our\nresults on a number of benchmark datasets, including our YT360-EyeTracking,\ndemonstrate that SalViT360 and SalViT360-AV significantly outperform existing\nmethods in predicting viewer attention in 360-degree scenes. Interpreting these\nresults, we suggest that integrating spatial audio cues in the model\narchitecture is crucial for accurate saliency prediction in omnidirectional\nvideos. Code and dataset will be available at\nhttps://cyberiada.github.io/SalViT360.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e360\u5ea6\u89c6\u9891\u663e\u8457\u6027\u9884\u6d4b\u7684\u65b0\u6570\u636e\u96c6YT360-EyeTracking\u548c\u4e24\u4e2a\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u6a21\u578bSalViT360\u548cSalViT360-AV\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u97f3\u9891\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u5168\u666f\u89c6\u9891\u4e2d\u7684\u6ce8\u610f\u529b\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5168\u9762\u7684360\u5ea6\u89c6\u542c\u663e\u8457\u6027\u9884\u6d4b\u6570\u636e\u96c6\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u7403\u9762\u7578\u53d8\u548c\u7a7a\u95f4\u97f3\u9891\u7684\u6574\u5408\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u89c6\u542c\u7ebf\u7d22\u6709\u6548\u9884\u6d4b360\u5ea6\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u663e\u8457\u6027\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b81\u4e2aODV\u7684YT360-EyeTracking\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86SalViT360\uff08\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u6846\u67b6\uff0c\u914d\u5907\u7403\u9762\u51e0\u4f55\u611f\u77e5\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\u5c42\uff09\u548cSalViT360-AV\uff08\u8fdb\u4e00\u6b65\u6574\u5408\u57fa\u4e8e\u97f3\u9891\u8f93\u5165\u7684Transformer\u9002\u914d\u5668\uff09\u4e24\u4e2a\u65b0\u9896\u7684\u663e\u8457\u6027\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ecYT360-EyeTracking\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSalViT360\u548cSalViT360-AV\u5728\u9884\u6d4b360\u5ea6\u573a\u666f\u4e2d\u7684\u89c2\u770b\u8005\u6ce8\u610f\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5728\u6a21\u578b\u67b6\u6784\u4e2d\u6574\u5408\u7a7a\u95f4\u97f3\u9891\u7ebf\u7d22\u5bf9\u4e8e\u51c6\u786e\u9884\u6d4b\u5168\u666f\u89c6\u9891\u4e2d\u7684\u663e\u8457\u6027\u81f3\u5173\u91cd\u8981\uff0c\u97f3\u9891\u4fe1\u606f\u7684\u52a0\u5165\u663e\u8457\u63d0\u5347\u4e86\u663e\u8457\u6027\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.20227", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20227", "abs": "https://arxiv.org/abs/2508.20227", "authors": ["Phu-Vinh Nguyen", "Tan-Hanh Pham", "Chris Ngo", "Truong Son Hy"], "title": "A Novel Framework for Automated Explain Vision Model Using Vision-Language Models", "comment": null, "summary": "The development of many vision models mainly focuses on improving their\nperformance using metrics such as accuracy, IoU, and mAP, with less attention\nto explainability due to the complexity of applying xAI methods to provide a\nmeaningful explanation of trained models. Although many existing xAI methods\naim to explain vision models sample-by-sample, methods explaining the general\nbehavior of vision models, which can only be captured after running on a large\ndataset, are still underexplored. Furthermore, understanding the behavior of\nvision models on general images can be very important to prevent biased\njudgments and help identify the model's trends and patterns. With the\napplication of Vision-Language Models, this paper proposes a pipeline to\nexplain vision models at both the sample and dataset levels. The proposed\npipeline can be used to discover failure cases and gain insights into vision\nmodels with minimal effort, thereby integrating vision model development with\nxAI analysis to advance image analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eVLM\u7684\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u6d41\u7a0b\uff0c\u80fd\u591f\u5728\u6837\u672c\u548c\u6570\u636e\u96c6\u5c42\u9762\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6a21\u578b\u884c\u4e3a\u89e3\u91ca\uff0c\u53d1\u73b0\u5931\u8d25\u6848\u4f8b\u5e76\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u6a21\u578b\u5f00\u53d1\u4e3b\u8981\u5173\u6ce8\u6027\u80fd\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u4e00\u822c\u6027\u884c\u4e3a\u7684\u89e3\u91ca\u65b9\u6cd5\u3002\u7406\u89e3\u6a21\u578b\u5728\u666e\u901a\u56fe\u50cf\u4e0a\u7684\u884c\u4e3a\u5bf9\u9632\u6b62\u504f\u89c1\u5224\u65ad\u548c\u8bc6\u522b\u6a21\u578b\u8d8b\u52bf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u6784\u5efa\u4e86\u4e00\u4e2a\u89e3\u91ca\u6d41\u7a0b\uff0c\u80fd\u591f\u540c\u65f6\u5728\u6837\u672c\u7ea7\u522b\u548c\u6570\u636e\u96c6\u7ea7\u522b\u5bf9\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u89e3\u91ca\u3002\u8be5\u6d41\u7a0b\u9700\u8981\u6700\u5c0f\u5316\u4eba\u5de5\u52a8\u4f5c\uff0c\u5c06\u89c6\u89c9\u6a21\u578b\u5f00\u53d1\u4e0exAI\u5206\u6790\u76f8\u7ed3\u5408\u3002", "result": "\u63d0\u51fa\u7684\u6d41\u7a0b\u53ef\u4ee5\u6709\u6548\u5730\u53d1\u73b0\u6a21\u578b\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u5e76\u63d0\u4f9b\u5bf9\u89c6\u89c9\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u4fc3\u8fdb\u56fe\u50cf\u5206\u6790\u7684\u8fdb\u6b65\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u89c6\u89c9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7VLM\u6280\u672f\u5b9e\u73b0\u4e86\u6837\u672c\u548c\u6570\u636e\u96c6\u5c42\u9762\u7684\u5168\u9762\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u53ef\u4fe1\u8d56\u6027\u548c\u53ef\u7406\u89e3\u6027\u3002"}}
{"id": "2508.20345", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.20345", "abs": "https://arxiv.org/abs/2508.20345", "authors": ["Xiao Li", "Yanfan Zhu", "Ruining Deng", "Wei-Qi Wei", "Yu Wang", "Shilin Zhao", "Yaohong Wang", "Haichun Yang", "Yuankai Huo"], "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models", "comment": null, "summary": "Recent advances in medical vision-language models (VLMs) open up remarkable\nopportunities for clinical applications such as automated report generation,\ncopilots for physicians, and uncertainty quantification. However, despite their\npromise, medical VLMs introduce serious security concerns, most notably risks\nof Protected Health Information (PHI) exposure, data leakage, and vulnerability\nto cyberthreats - which are especially critical in hospital environments. Even\nwhen adopted for research or non-clinical purposes, healthcare organizations\nmust exercise caution and implement safeguards. To address these challenges, we\npresent MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)\nenables physicians to manually select and use different models without\nprogramming expertise, (2) supports engineers in efficiently deploying medical\nVLMs in a plug-and-play fashion, with seamless integration of Hugging Face\nopen-source models, and (3) ensures privacy-preserving inference through\nDocker-orchestrated, operating system agnostic deployment. MedFoundationHub\nrequires only an offline local workstation equipped with a single NVIDIA A6000\nGPU, making it both secure and accessible within the typical resources of\nacademic research labs. To evaluate current capabilities, we engaged\nboard-certified pathologists to deploy and assess five state-of-the-art VLMs\n(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and\nLLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,\nyielding 1015 clinician-model scoring events. These assessments revealed\nrecurring limitations, including off-target answers, vague reasoning, and\ninconsistent pathology terminology.", "AI": {"tldr": "MedFoundationHub\u662f\u4e00\u4e2a\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578bGUI\u5de5\u5177\u5305\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u7597VLMs\u7684\u5b89\u5168\u9690\u79c1\u95ee\u9898\uff0c\u652f\u6301\u533b\u751f\u65e0\u7f16\u7a0b\u4f7f\u7528\u6a21\u578b\uff0c\u5de5\u7a0b\u5e08\u5feb\u901f\u90e8\u7f72\uff0c\u5e76\u901a\u8fc7Docker\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u63a8\u7406\u3002", "motivation": "\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u4e3a\u4e34\u5e8a\u5e94\u7528\u5e26\u6765\u5de8\u5927\u673a\u9047\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u53d7\u4fdd\u62a4\u5065\u5eb7\u4fe1\u606f\u6cc4\u9732\u3001\u6570\u636e\u6cc4\u6f0f\u548c\u7f51\u7edc\u5a01\u80c1\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u533b\u9662\u73af\u5883\u4e2d\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u5f00\u53d1MedFoundationHub\u56fe\u5f62\u7528\u6237\u754c\u9762\u5de5\u5177\u5305\uff0c\u652f\u6301\u533b\u751f\u624b\u52a8\u9009\u62e9\u4f7f\u7528\u4e0d\u540c\u6a21\u578b\u65e0\u9700\u7f16\u7a0b\uff0c\u5de5\u7a0b\u5e08\u4ee5\u5373\u63d2\u5373\u7528\u65b9\u5f0f\u9ad8\u6548\u90e8\u7f72\u533b\u7597VLMs\uff0c\u96c6\u6210Hugging Face\u5f00\u6e90\u6a21\u578b\uff0c\u901a\u8fc7Docker\u7f16\u6392\u5b9e\u73b0\u64cd\u4f5c\u7cfb\u7edf\u65e0\u5173\u7684\u9690\u79c1\u4fdd\u62a4\u63a8\u7406\u90e8\u7f72\u3002", "result": "\u4f7f\u7528\u5355\u4e2aNVIDIA A6000 GPU\u7684\u79bb\u7ebf\u672c\u5730\u5de5\u4f5c\u7ad9\u5373\u53ef\u8fd0\u884c\uff0c\u8bc4\u4f30\u4e865\u4e2a\u6700\u5148\u8fdbVLM\u6a21\u578b\uff08Google-MedGemma3-4B\u7b49\uff09\uff0c\u75c5\u7406\u5b66\u5bb6\u8bc4\u4f30\u4e861015\u4e2a\u4e34\u5e8a\u533b\u751f-\u6a21\u578b\u8bc4\u5206\u4e8b\u4ef6\uff0c\u53d1\u73b0\u5b58\u5728\u8131\u9776\u56de\u7b54\u3001\u6a21\u7cca\u63a8\u7406\u548c\u75c5\u7406\u672f\u8bed\u4e0d\u4e00\u81f4\u7b49\u5c40\u9650\u6027\u3002", "conclusion": "MedFoundationHub\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u3001\u6613\u7528\u7684\u533b\u7597VLM\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u5728\u533b\u7597\u672f\u8bed\u51c6\u786e\u6027\u548c\u63a8\u7406\u4e00\u81f4\u6027\u65b9\u9762\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2508.20392", "categories": ["cs.CV", "cs.AI", "I.4.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.20392", "abs": "https://arxiv.org/abs/2508.20392", "authors": ["Chengjun Zhang", "Yuhao Zhang", "Jie Yang", "Mohamad Sawan"], "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection", "comment": "12 pages, 8 figures", "summary": "Spiking Neural Networks (SNNs), inspired by the brain, are characterized by\nminimal power consumption and swift inference capabilities on neuromorphic\nhardware, and have been widely applied to various visual perception tasks.\nCurrent ANN-SNN conversion methods have achieved excellent results in\nclassification tasks with ultra-low time-steps, but their performance in visual\ndetection tasks remains suboptimal. In this paper, we propose a delay-spike\napproach to mitigate the issue of residual membrane potential caused by\nheterogeneous spiking patterns. Furthermore, we propose a novel\ntemporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This\nenables Integrate-and-fire (IF) neurons to dynamically adjust their\naccumulation and firing behaviors based on the temporal order of time-steps.\nOur method enables spikes to exhibit distinct temporal properties, rather than\nrelying solely on frequency-based representations. Moreover, the tdIF neuron\nmaintains energy consumption on par with traditional IF neuron. We demonstrate\nthat our method achieves more precise feature representation with lower\ntime-steps, enabling high performance and ultra-low latency in visual detection\ntasks. In this study, we conduct extensive evaluation of the tdIF method across\ntwo critical vision tasks: object detection and lane line detection. The\nresults demonstrate that the proposed method surpasses current ANN-SNN\nconversion approaches, achieving state-of-the-art performance with ultra-low\nlatency (within 5 time-steps).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5ef6\u8fdf\u8109\u51b2\u65b9\u6cd5\u548c\u65f6\u95f4\u4f9d\u8d56\u7684IF\u795e\u7ecf\u5143(tdIF)\uff0c\u89e3\u51b3\u4e86SNN\u5728\u89c6\u89c9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5728\u8d85\u4f4e\u65f6\u95f4\u6b65\u957f(5\u6b65\u5185)\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "motivation": "\u5f53\u524dANN-SNN\u8f6c\u6362\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u89c6\u89c9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6027\u80fd\u4ecd\u7136\u4e0d\u7406\u60f3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5f02\u8d28\u8109\u51b2\u6a21\u5f0f\u5bfc\u81f4\u7684\u6b8b\u4f59\u819c\u7535\u4f4d\u95ee\u9898", "method": "\u91c7\u7528\u5ef6\u8fdf\u8109\u51b2\u65b9\u6cd5\u7f13\u89e3\u5f02\u8d28\u8109\u51b2\u6a21\u5f0f\u95ee\u9898\uff0c\u5e76\u63d0\u51fatdIF\u795e\u7ecf\u5143\u67b6\u6784\uff0c\u4f7fIF\u795e\u7ecf\u5143\u80fd\u591f\u6839\u636e\u65f6\u95f4\u6b65\u957f\u7684\u65f6\u5e8f\u52a8\u6001\u8c03\u6574\u79ef\u7d2f\u548c\u53d1\u653e\u884c\u4e3a", "result": "\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u8f66\u9053\u7ebf\u68c0\u6d4b\u4e24\u4e2a\u5173\u952e\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u65b9\u6cd5\u8d85\u8d8a\u4e86\u5f53\u524dANN-SNN\u8f6c\u6362\u65b9\u6cd5\uff0c\u5728\u8d85\u4f4e\u5ef6\u8fdf(5\u65f6\u95f4\u6b65\u5185)\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u8109\u51b2\u80fd\u591f\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u65f6\u95f4\u7279\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u9891\u7387\u8868\u793a\uff0c\u5728\u8d85\u4f4e\u65f6\u95f4\u6b65\u957f\u4e0b\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u7279\u5f81\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edfIF\u795e\u7ecf\u5143\u76f8\u5f53\u7684\u80fd\u91cf\u6d88\u8017"}}
{"id": "2508.20478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20478", "abs": "https://arxiv.org/abs/2508.20478", "authors": ["Yuan Xie", "Tianshui Chen", "Zheng Ge", "Lionel Ni"], "title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding", "comment": "15 pages, 9 figures", "summary": "Long-form video understanding, characterized by long-range temporal\ndependencies and multiple events, remains a challenge. Existing methods often\nrely on static reasoning or external visual-language models (VLMs), which face\nissues like complexity and sub-optimal performance due to the lack of\nend-to-end training. In this paper, we propose Video-MTR, a reinforced\nmulti-turn reasoning framework designed to enable iterative key video segment\nselection and question comprehension. Unlike traditional video reasoning\npipeline, which generate predictions in a single turn, Video-MTR performs\nreasoning in multiple turns, selecting video segments progressively based on\nthe evolving understanding of previously processed segments and the current\nquestion. This iterative process allows for a more refined and contextually\naware analysis of the video. To ensure intermediate reasoning process, we\nintroduce a novel gated bi-level reward system, combining trajectory-level\nrewards based on answer correctness and turn-level rewards emphasizing\nframe-query relevance. This system optimizes both video segment selection and\nquestion comprehension, eliminating the need for external VLMs and allowing\nend-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,\nand EgoSchema demonstrate that Video-MTR outperforms existing methods in both\naccuracy and efficiency, advancing the state-of-the-art in long video\nunderstanding.", "AI": {"tldr": "Video-MTR\u662f\u4e00\u4e2a\u5f3a\u5316\u591a\u8f6e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u5173\u952e\u89c6\u9891\u7247\u6bb5\u548c\u95ee\u9898\u7406\u89e3\u6765\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u95ee\u9898\uff0c\u91c7\u7528\u95e8\u63a7\u53cc\u7ea7\u5956\u52b1\u7cfb\u7edf\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u5b58\u5728\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u548c\u591a\u4e2a\u4e8b\u4ef6\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u63a8\u7406\u6216\u5916\u90e8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b58\u5728\u590d\u6742\u5ea6\u9ad8\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u4e0d\u8db3\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u4f73\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u591a\u8f6e\u63a8\u7406\u6846\u67b6Video-MTR\uff0c\u8fed\u4ee3\u9009\u62e9\u89c6\u9891\u7247\u6bb5\u5e76\u57fa\u4e8e\u5148\u524d\u5904\u7406\u7247\u6bb5\u7684\u7406\u89e3\u8fdb\u884c\u6e10\u8fdb\u5f0f\u63a8\u7406\uff0c\u5f15\u5165\u95e8\u63a7\u53cc\u7ea7\u5956\u52b1\u7cfb\u7edf\uff08\u8f68\u8ff9\u7ea7\u5956\u52b1\u548c\u8f6e\u7ea7\u5956\u52b1\uff09\u4f18\u5316\u89c6\u9891\u7247\u6bb5\u9009\u62e9\u548c\u95ee\u9898\u7406\u89e3\u3002", "result": "\u5728VideoMME\u3001MLVU\u548cEgoSchema\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideo-MTR\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Video-MTR\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u63a8\u7406\u548c\u95e8\u63a7\u53cc\u7ea7\u5956\u52b1\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u6027\u80fd\u3002"}}
{"id": "2508.20828", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20828", "abs": "https://arxiv.org/abs/2508.20828", "authors": ["Jie Zhao", "Wanting Ning", "Yuxiao Fei", "Yubo Feng", "Lishuang Li"], "title": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction", "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP Findings)", "summary": "In Natural Language Processing(NLP), Event Temporal Relation Extraction\n(ETRE) is to recognize the temporal relations of two events. Prior studies have\nnoted the importance of language models for ETRE. However, the restricted\npre-trained knowledge of Small Language Models(SLMs) limits their capability to\nhandle minority class relations in imbalanced classification datasets. For\nLarge Language Models(LLMs), researchers adopt manually designed prompts or\ninstructions, which may introduce extra noise, leading to interference with the\nmodel's judgment of the long-distance dependencies between events. To address\nthese issues, we propose GDLLM, a Global Distance-aware modeling approach based\non LLMs. We first present a distance-aware graph structure utilizing Graph\nAttention Network(GAT) to assist the LLMs in capturing long-distance dependency\nfeatures. Additionally, we design a temporal feature learning paradigm based on\nsoft inference to augment the identification of relations with a short-distance\nproximity band, which supplements the probabilistic information generated by\nLLMs into the multi-head attention mechanism. Since the global feature can be\ncaptured effectively, our framework substantially enhances the performance of\nminority relation classes and improves the overall learning ability.\nExperiments on two publicly available datasets, TB-Dense and MATRES,\ndemonstrate that our approach achieves state-of-the-art (SOTA) performance.", "AI": {"tldr": "\u63d0\u51faGDLLM\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u5168\u5c40\u8ddd\u79bb\u611f\u77e5\u5efa\u6a21\uff0c\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u8f6f\u63a8\u7406\u8303\u5f0f\u89e3\u51b3\u4e8b\u4ef6\u65f6\u5e8f\u5173\u7cfb\u62bd\u53d6\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u5c11\u6570\u7c7b\u5173\u7cfb\u8bc6\u522b\u95ee\u9898", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b(SLMs)\u5728\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u7684\u5c11\u6570\u7c7b\u5173\u7cfb\u65f6\u53d7\u9650\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u4f7f\u7528\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u53ef\u80fd\u5f15\u5165\u566a\u58f0\uff0c\u5e72\u6270\u5bf9\u4e8b\u4ef6\u95f4\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u7684\u5224\u65ad", "method": "1) \u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(GAT)\u6784\u5efa\u8ddd\u79bb\u611f\u77e5\u56fe\u7ed3\u6784\u8f85\u52a9LLMs\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u7279\u5f81\uff1b2) \u8bbe\u8ba1\u57fa\u4e8e\u8f6f\u63a8\u7406\u7684\u65f6\u5e8f\u7279\u5f81\u5b66\u4e60\u8303\u5f0f\uff0c\u5c06LLMs\u751f\u6210\u7684\u6982\u7387\u4fe1\u606f\u8865\u5145\u5230\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d", "result": "\u5728TB-Dense\u548cMATRES\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6570\u5173\u7cfb\u7c7b\u7684\u8868\u73b0\u548c\u6574\u4f53\u5b66\u4e60\u80fd\u529b", "conclusion": "GDLLM\u6846\u67b6\u901a\u8fc7\u6709\u6548\u6355\u83b7\u5168\u5c40\u7279\u5f81\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5c11\u6570\u5173\u7cfb\u7c7b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u6574\u4f53\u5b66\u4e60\u80fd\u529b"}}
{"id": "2508.20594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20594", "abs": "https://arxiv.org/abs/2508.20594", "authors": ["Yuqi Han", "Songqian Zhang", "Weijian Su", "Ke Li", "Jiayu Yang", "Jinli Suo", "Qiang Zhang"], "title": "UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching", "comment": null, "summary": "The thermal camera excels at perceiving outdoor environments under low-light\nconditions, making it ideal for applications such as nighttime autonomous\ndriving and unmanned navigation. However, thermal cameras encounter challenges\nwhen capturing signage from objects made of similar materials, which can pose\nsafety risks for accurately understanding semantics in autonomous driving\nsystems. In contrast, the neuromorphic vision camera, also known as an event\ncamera, detects changes in light intensity asynchronously and has proven\neffective in high-speed, low-light traffic environments. Recognizing the\ncomplementary characteristics of these two modalities, this paper proposes\nUTA-Sign, an unsupervised thermal-event video augmentation for traffic signage\nin low-illumination environments, targeting elements such as license plates and\nroadblock indicators. To address the signage blind spots of thermal imaging and\nthe non-uniform sampling of event cameras, we developed a dual-boosting\nmechanism that fuses thermal frames and event signals for consistent signage\nrepresentation over time. The proposed method utilizes thermal frames to\nprovide accurate motion cues as temporal references for aligning the uneven\nevent signals. At the same time, event signals contribute subtle signage\ncontent to the raw thermal frames, enhancing the overall understanding of the\nenvironment. The proposed method is validated on datasets collected from\nreal-world scenarios, demonstrating superior quality in traffic signage\nsketching and improved detection accuracy at the perceptual level.", "AI": {"tldr": "UTA-Sign\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u70ed\u6210\u50cf-\u4e8b\u4ef6\u76f8\u673a\u89c6\u9891\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u7684\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\uff0c\u901a\u8fc7\u53cc\u589e\u5f3a\u673a\u5236\u878d\u5408\u70ed\u6210\u50cf\u5e27\u548c\u4e8b\u4ef6\u4fe1\u53f7\u6765\u63d0\u5347\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7cbe\u5ea6", "motivation": "\u70ed\u6210\u50cf\u76f8\u673a\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8bc6\u522b\u76f8\u4f3c\u6750\u6599\u5236\u6210\u7684\u6807\u5fd7\u65f6\u5b58\u5728\u76f2\u70b9\uff1b\u4e8b\u4ef6\u76f8\u673a\u80fd\u6709\u6548\u68c0\u6d4b\u5149\u5f3a\u53d8\u5316\u4f46\u91c7\u6837\u4e0d\u5747\u5300\u3002\u4e24\u79cd\u6a21\u6001\u5177\u6709\u4e92\u8865\u7279\u6027\uff0c\u9700\u8981\u878d\u5408\u6765\u89e3\u51b3\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u95ee\u9898", "method": "\u63d0\u51fa\u53cc\u589e\u5f3a\u673a\u5236\uff1a\u5229\u7528\u70ed\u6210\u50cf\u5e27\u63d0\u4f9b\u51c6\u786e\u8fd0\u52a8\u7ebf\u7d22\u4f5c\u4e3a\u65f6\u95f4\u53c2\u8003\u6765\u5bf9\u9f50\u4e0d\u5747\u5300\u7684\u4e8b\u4ef6\u4fe1\u53f7\uff0c\u540c\u65f6\u4e8b\u4ef6\u4fe1\u53f7\u4e3a\u539f\u59cb\u70ed\u6210\u50cf\u5e27\u63d0\u4f9b\u7ec6\u5fae\u7684\u6807\u5fd7\u5185\u5bb9\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u6807\u5fd7\u8868\u793a", "result": "\u5728\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5728\u4ea4\u901a\u6807\u5fd7\u63cf\u7ed8\u8d28\u91cf\u548c\u611f\u77e5\u5c42\u9762\u7684\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u70ed\u6210\u50cf\u7684\u6807\u5fd7\u76f2\u70b9\u548c\u4e8b\u4ef6\u76f8\u673a\u7684\u4e0d\u5747\u5300\u91c7\u6837\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u4ea4\u901a\u6807\u5fd7\u7684\u8bc6\u522b\u80fd\u529b"}}
{"id": "2508.20595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20595", "abs": "https://arxiv.org/abs/2508.20595", "authors": ["Mengxiao Huang", "Minglei Shu", "Shuwang Zhou", "Zhaoyang Liu"], "title": "Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations", "comment": "Accepted to IEEE IJCNN 2025", "summary": "Deepfake technology, driven by Generative Adversarial Networks (GANs), poses\nsignificant risks to privacy and societal security. Existing detection methods\nare predominantly passive, focusing on post-event analysis without preventing\nattacks. To address this, we propose an active defense method based on\nlow-frequency perceptual perturbations to disrupt face swapping manipulation,\nreducing the performance and naturalness of generated content. Unlike prior\napproaches that used low-frequency perturbations to impact classification\naccuracy,our method directly targets the generative process of deepfake\ntechniques. We combine frequency and spatial domain features to strengthen\ndefenses. By introducing artifacts through low-frequency perturbations while\npreserving high-frequency details, we ensure the output remains visually\nplausible. Additionally, we design a complete architecture featuring an\nencoder, a perturbation generator, and a decoder, leveraging discrete wavelet\ntransform (DWT) to extract low-frequency components and generate perturbations\nthat disrupt facial manipulation models. Experiments on CelebA-HQ and LFW\ndemonstrate significant reductions in face-swapping effectiveness, improved\ndefense success rates, and preservation of visual quality.", "AI": {"tldr": "\u57fa\u4e8e\u4f4e\u9891\u611f\u77e5\u5e72\u6270\u7684\u6df1\u5ea6\u5047\u5236\u9020\u4e3b\u52a8\u9632\u5fa1\u65b9\u6cd5\uff0c\u76f4\u63a5\u5e72\u6270\u9762\u90e8\u66ff\u6362\u751f\u6210\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u9632\u5fa1\u6548\u679c", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5047\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u662f\u88ab\u52a8\u7684\u4e8b\u540e\u5206\u6790\uff0c\u65e0\u6cd5\u9632\u6b62\u653b\u51fb\u53d1\u751f\uff0c\u9700\u8981\u4e3b\u52a8\u9632\u5fa1\u6280\u672f\u6765\u5e94\u5bf9\u5047\u5236\u9020\u6280\u672f\u5bf9\u9690\u79c1\u548c\u793e\u4f1a\u5b89\u5168\u7684\u5a01\u80c1", "method": "\u7ed3\u5408\u9891\u57df\u548c\u7a7a\u95f4\u57df\u7279\u5f81\uff0c\u4f7f\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362(DWT)\u63d0\u53d6\u4f4e\u9891\u7ec4\u4ef6\u5e76\u751f\u6210\u5e72\u6270\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u5e72\u6270\u751f\u6210\u5668-\u89e3\u7801\u5668\u5b8c\u6574\u67b6\u6784\uff0c\u5728\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\u7684\u540c\u65f6\u5f15\u5165\u4f4e\u9891\u4eba\u5de5\u4ea7\u7269", "result": "\u5728CelebA-HQ\u548cLFW\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u9762\u90e8\u66ff\u6362\u6548\u679c\uff0c\u63d0\u9ad8\u4e86\u9632\u5fa1\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u89c6\u89c9\u8d28\u91cf", "conclusion": "\u8be5\u4e3b\u52a8\u9632\u5fa1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e72\u6270\u6df1\u5ea6\u5047\u5236\u9020\u751f\u6210\u8fc7\u7a0b\uff0c\u4e3a\u5e94\u5bf9\u5047\u5236\u9020\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u524d\u7f6e\u6027\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.20697", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20697", "abs": "https://arxiv.org/abs/2508.20697", "authors": ["Weitao Feng", "Lixu Wang", "Tianyi Wei", "Jie Zhang", "Chongyang Gao", "Sinong Zhan", "Peizhuo Lv", "Wei Dong"], "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning", "comment": "Project Hompage: https://tokenbuncher.github.io/", "summary": "As large language models (LLMs) continue to grow in capability, so do the\nrisks of harmful misuse through fine-tuning. While most prior studies assume\nthat attackers rely on supervised fine-tuning (SFT) for such misuse, we\nsystematically demonstrate that reinforcement learning (RL) enables adversaries\nto more effectively break safety alignment and facilitate advanced harmful task\nassistance, under matched computational budgets. To counter this emerging\nthreat, we propose TokenBuncher, the first effective defense specifically\ntargeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation\non which RL relies: model response uncertainty. By constraining uncertainty,\nRL-based fine-tuning can no longer exploit distinct reward signals to drive the\nmodel toward harmful behaviors. We realize this defense through\nentropy-as-reward RL and a Token Noiser mechanism designed to prevent the\nescalation of expert-domain harmful capabilities. Extensive experiments across\nmultiple models and RL algorithms show that TokenBuncher robustly mitigates\nharmful RL fine-tuning while preserving benign task utility and finetunability.\nOur results highlight that RL-based harmful fine-tuning poses a greater\nsystemic risk than SFT, and that TokenBuncher provides an effective and general\ndefense.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u6bd4\u76d1\u7763\u5fae\u8c03\u66f4\u6709\u6548\u5730\u7834\u574fLLM\u5b89\u5168\u5bf9\u9f50\uff0c\u5e76\u63d0\u51faTokenBuncher\u9632\u5fa1\u65b9\u6cd5\u901a\u8fc7\u6291\u5236\u6a21\u578b\u54cd\u5e94\u4e0d\u786e\u5b9a\u6027\u6765\u5bf9\u6297RL\u6709\u5bb3\u5fae\u8c03", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u6709\u5bb3\u5fae\u8c03\u98ce\u9669\u589e\u52a0\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u76d1\u7763\u5fae\u8c03\uff0c\u4f46\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u80fd\u66f4\u6709\u6548\u5730\u7a81\u7834\u5b89\u5168\u9632\u62a4\uff0c\u9700\u8981\u4e13\u95e8\u9632\u5fa1\u673a\u5236", "method": "\u63d0\u51faTokenBuncher\u9632\u5fa1\u65b9\u6cd5\uff0c\u5305\u542b\u71b5\u4f5c\u4e3a\u5956\u52b1\u7684RL\u548cToken Noiser\u673a\u5236\uff0c\u901a\u8fc7\u7ea6\u675f\u6a21\u578b\u54cd\u5e94\u4e0d\u786e\u5b9a\u6027\u6765\u9632\u6b62RL\u5229\u7528\u5956\u52b1\u4fe1\u53f7\u9a71\u52a8\u6709\u5bb3\u884c\u4e3a", "result": "\u8de8\u591a\u4e2a\u6a21\u578b\u548cRL\u7b97\u6cd5\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTokenBuncher\u80fd\u6709\u6548\u7f13\u89e3\u6709\u5bb3RL\u5fae\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u4efb\u52a1\u6548\u7528\u548c\u5fae\u8c03\u80fd\u529b", "conclusion": "RL\u6709\u5bb3\u5fae\u8c03\u6bd4SFT\u6784\u6210\u66f4\u5927\u7cfb\u7edf\u6027\u98ce\u9669\uff0cTokenBuncher\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u901a\u7528\u7684\u9632\u5fa1\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.20818", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20818", "abs": "https://arxiv.org/abs/2508.20818", "authors": ["Anirudh Satheesh", "Keenan Powell", "Hua Wei"], "title": "cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending", "comment": "A shorter version has been accepted to the 2025 Conference on\n  Information and Knowledge Management", "summary": "Many multi-agent reinforcement learning (MARL) algorithms are trained in\nfixed simulation environments, making them brittle when deployed in real-world\nscenarios with more complex and uncertain conditions. Contextual MARL (cMARL)\naddresses this by parameterizing environments with context variables and\ntraining a context-agnostic policy that performs well across all environment\nconfigurations. Existing cMARL methods attempt to use curriculum learning to\nhelp train and evaluate context-agnostic policies, but they often rely on\nunreliable proxy signals, such as value estimates or generalized advantage\nestimates that are noisy and unstable in multi-agent settings due to\ninter-agent dynamics and partial observability. To address these issues, we\npropose Contextual Multi-Agent LLM-Guided Curriculum Learning with\nDiversity-Based Context Blending (cMALC-D), a framework that uses Large\nLanguage Models (LLMs) to generate semantically meaningful curricula and\nprovide a more robust evaluation signal. To prevent mode collapse and encourage\nexploration, we introduce a novel diversity-based context blending mechanism\nthat creates new training scenarios by combining features from prior contexts.\nExperiments in traffic signal control domains demonstrate that cMALC-D\nsignificantly improves both generalization and sample efficiency compared to\nexisting curriculum learning baselines. We provide code at\nhttps://github.com/DaRL-LibSignal/cMALC-D.", "AI": {"tldr": "\u63d0\u51fa\u4e86cMALC-D\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u5316\u8bfe\u7a0b\u5e76\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u4fe1\u53f7\uff0c\u901a\u8fc7\u591a\u6837\u6027\u4e0a\u4e0b\u6587\u6df7\u5408\u673a\u5236\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\uff0c\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u9886\u57df\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u4ee3\u7406\u4fe1\u53f7\uff08\u5982\u4ef7\u503c\u4f30\u8ba1\uff09\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u7531\u4e8e\u667a\u80fd\u4f53\u95f4\u52a8\u6001\u6027\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u800c\u53d8\u5f97\u5608\u6742\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u8bfe\u7a0b\u5b66\u4e60\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u5316\u8bfe\u7a0b\uff0c\u5f15\u5165\u57fa\u4e8e\u591a\u6837\u6027\u7684\u4e0a\u4e0b\u6587\u6df7\u5408\u673a\u5236\uff0c\u901a\u8fc7\u7ec4\u5408\u5148\u524d\u4e0a\u4e0b\u6587\u7279\u5f81\u521b\u5efa\u65b0\u7684\u8bad\u7ec3\u573a\u666f\uff0c\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\u5e76\u9f13\u52b1\u63a2\u7d22\u3002", "result": "\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0ccMALC-D\u76f8\u6bd4\u73b0\u6709\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "cMALC-D\u6846\u67b6\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\u548c\u591a\u6837\u6027\u4e0a\u4e0b\u6587\u6df7\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20765", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20765", "abs": "https://arxiv.org/abs/2508.20765", "authors": ["Gowreesh Mago", "Pascal Mettes", "Stevan Rudinac"], "title": "Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding", "comment": "Under Review for IJCV", "summary": "The automatic understanding of video content is advancing rapidly. Empowered\nby deeper neural networks and large datasets, machines are increasingly capable\nof understanding what is concretely visible in video frames, whether it be\nobjects, actions, events, or scenes. In comparison, humans retain a unique\nability to also look beyond concrete entities and recognize abstract concepts\nlike justice, freedom, and togetherness. Abstract concept recognition forms a\ncrucial open challenge in video understanding, where reasoning on multiple\nsemantic levels based on contextual information is key. In this paper, we argue\nthat the recent advances in foundation models make for an ideal setting to\naddress abstract concept understanding in videos. Automated understanding of\nhigh-level abstract concepts is imperative as it enables models to be more\naligned with human reasoning and values. In this survey, we study different\ntasks and datasets used to understand abstract concepts in video content. We\nobserve that, periodically and over a long period, researchers have attempted\nto solve these tasks, making the best use of the tools available at their\ndisposal. We advocate that drawing on decades of community experience will help\nus shed light on this important open grand challenge and avoid ``re-inventing\nthe wheel'' as we start revisiting it in the era of multi-modal foundation\nmodels.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u89c6\u9891\u4e2d\u62bd\u8c61\u6982\u5ff5\u7406\u89e3\u7684\u7efc\u8ff0\u8bba\u6587\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u57fa\u7840\u6a21\u578b\u6765\u8bc6\u522b\u548c\u7406\u89e3\u89c6\u9891\u4e2d\u7684\u62bd\u8c61\u6982\u5ff5\uff08\u5982\u6b63\u4e49\u3001\u81ea\u7531\u3001\u56e2\u7ed3\u7b49\uff09\uff0c\u8fd9\u662f\u5f53\u524d\u89c6\u9891\u5185\u5bb9\u7406\u89e3\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "motivation": "\u867d\u7136\u73b0\u4ee3AI\u7cfb\u7edf\u5728\u8bc6\u522b\u89c6\u9891\u4e2d\u5177\u4f53\u53ef\u89c1\u7684\u7269\u4f53\u3001\u52a8\u4f5c\u3001\u4e8b\u4ef6\u548c\u573a\u666f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7406\u89e3\u62bd\u8c61\u6982\u5ff5\u65b9\u9762\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u3002\u62bd\u8c61\u6982\u5ff5\u8bc6\u522b\u5bf9\u4e8e\u4f7f\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u63a8\u7406\u548c\u4ef7\u503c\u89c2\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u4e0d\u540c\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u56de\u987e\u4e86\u6570\u5341\u5e74\u6765\u793e\u533a\u5728\u62bd\u8c61\u6982\u5ff5\u7406\u89e3\u65b9\u9762\u7684\u7ecf\u9a8c\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7814\u7a76\u4eba\u5458\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u5728\u5c1d\u8bd5\u89e3\u51b3\u62bd\u8c61\u6982\u5ff5\u7406\u89e3\u4efb\u52a1\uff0c\u5e76\u5145\u5206\u5229\u7528\u5f53\u65f6\u53ef\u7528\u7684\u5de5\u5177\u3002\u8bba\u6587\u5f3a\u8c03\u4e86\u501f\u9274\u793e\u533a\u7ecf\u9a8c\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u907f\u514d\u5728\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\"\u91cd\u65b0\u53d1\u660e\u8f6e\u5b50\"\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u7684\u8fdb\u6b65\u4e3a\u89e3\u51b3\u89c6\u9891\u4e2d\u62bd\u8c61\u6982\u5ff5\u7406\u89e3\u8fd9\u4e00\u91cd\u8981\u5f00\u653e\u6311\u6218\u63d0\u4f9b\u4e86\u7406\u60f3\u6761\u4ef6\u3002\u901a\u8fc7\u6574\u5408\u6570\u5341\u5e74\u7684\u7814\u7a76\u7ecf\u9a8c\u548c\u73b0\u4ee3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u63a8\u8fdb\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.20813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20813", "abs": "https://arxiv.org/abs/2508.20813", "authors": ["Tao Luo", "Han Wu", "Tong Yang", "Dinggang Shen", "Zhiming Cui"], "title": "Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training", "comment": null, "summary": "Accurate dental caries detection from panoramic X-rays plays a pivotal role\nin preventing lesion progression. However, current detection methods often\nyield suboptimal accuracy due to subtle contrast variations and diverse lesion\nmorphology of dental caries. In this work, inspired by the clinical workflow\nwhere dentists systematically combine whole-image screening with detailed\ntooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training\nnetwork for accurate dental caries detection. Our DVCTNet starts with employing\nautomated tooth detection to establish two complementary views: a global view\nfrom panoramic X-ray images and a local view from cropped tooth images. We then\npretrain two vision foundation models separately on the two views. The\nglobal-view foundation model serves as the detection backbone, generating\nregion proposals and global features, while the local-view model extracts\ndetailed features from corresponding cropped tooth patches matched by the\nregion proposals. To effectively integrate information from both views, we\nintroduce a Gated Cross-View Attention (GCV-Atten) module that dynamically\nfuses dual-view features, enhancing the detection pipeline by integrating the\nfused features back into the detection model for final caries detection. To\nrigorously evaluate our DVCTNet, we test it on a public dataset and further\nvalidate its performance on a newly curated, high-precision dental caries\ndetection dataset, annotated using both intra-oral images and panoramic X-rays\nfor double verification. Experimental results demonstrate DVCTNet's superior\nperformance against existing state-of-the-art (SOTA) methods on both datasets,\nindicating the clinical applicability of our method. Our code and labeled\ndataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.", "AI": {"tldr": "DVCTNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cc\u89c6\u56fe\u534f\u540c\u8bad\u7ec3\u7684\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u666fX\u5c04\u7ebf\u56fe\u50cf\u7684\u5168\u5c40\u89c6\u56fe\u548c\u88c1\u526a\u7259\u9f7f\u56fe\u50cf\u7684\u5c40\u90e8\u89c6\u56fe\uff0c\u4f7f\u7528\u95e8\u63a7\u4ea4\u53c9\u89c6\u56fe\u6ce8\u610f\u529b\u6a21\u5757\u52a8\u6001\u878d\u5408\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7259\u79d1\u9f8b\u9f7f\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7259\u79d1\u9f8b\u9f7f\u68c0\u6d4b\u65b9\u6cd5\u7531\u4e8e\u5bf9\u6bd4\u5ea6\u53d8\u5316\u7ec6\u5fae\u548c\u75c5\u53d8\u5f62\u6001\u591a\u6837\uff0c\u51c6\u786e\u7387\u4e0d\u7406\u60f3\u3002\u53d7\u7259\u533b\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u542f\u53d1\uff0c\u9700\u8981\u7ed3\u5408\u6574\u4f53\u56fe\u50cf\u7b5b\u67e5\u548c\u8be6\u7ec6\u7259\u9f7f\u7ea7\u68c0\u67e5\u6765\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u7259\u9f7f\u68c0\u6d4b\u5efa\u7acb\u5168\u5c40\u548c\u5c40\u90e8\u4e24\u4e2a\u4e92\u8865\u89c6\u56fe\uff0c\u5206\u522b\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002\u5168\u5c40\u89c6\u56fe\u6a21\u578b\u4f5c\u4e3a\u68c0\u6d4b\u9aa8\u5e72\u751f\u6210\u533a\u57df\u5efa\u8bae\uff0c\u5c40\u90e8\u89c6\u56fe\u6a21\u578b\u63d0\u53d6\u8be6\u7ec6\u7279\u5f81\u3002\u901a\u8fc7\u95e8\u63a7\u4ea4\u53c9\u89c6\u56fe\u6ce8\u610f\u529b\u6a21\u5757\u52a8\u6001\u878d\u5408\u53cc\u89c6\u56fe\u7279\u5f81\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u65b0\u6784\u5efa\u7684\u9ad8\u7cbe\u5ea6\u6570\u636e\u96c6\u4e0a\uff0cDVCTNet\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u4e34\u5e8a\u9002\u7528\u6027\u3002", "conclusion": "DVCTNet\u901a\u8fc7\u53cc\u89c6\u56fe\u534f\u540c\u8bad\u7ec3\u548c\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7259\u79d1\u9f8b\u9f7f\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
