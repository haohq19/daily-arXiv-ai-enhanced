<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2511.20795)
*Souradeep Dutta,Keshav Bulia,Neena S Nair*

Main category: cs.CV

TL;DR: 本文对Facebook AI Research的KRISP模型进行了轻量级复现，显著减少了参数数量，揭示了原模型的设计缺陷和实际问题，并在资源受限条件下评估了知识增强VQA架构的可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 原KRISP模型虽然有效，但需要工业级训练规模、计算需求大且与大型骨干网络紧密耦合。本研究旨在重新审视KRISP，提供参数更少的轻量级版本，以揭示原模型未充分覆盖的设计缺陷和实际问题。

Method: 通过系统消融研究进行轻量级复现，包括在合成VQA数据上的概念验证和在DAQUAR数据集上的评估。模型采用低参数设置，并受外部知识图谱领域约束。

Result: 复现模型性能约为原模型的75%，但揭示了多个设计缺陷、现实陷阱和隐含问题。模型能够防止AI幻觉，仅在特定知识领域内生成输出。

Conclusion: 轻量级模型参数极少，可在智能手机和AR-VR等边缘设备上运行，进一步改善了离线视觉推理能力，为资源受限环境下的知识增强VQA架构提供了实用解决方案。

Abstract: Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.

</details>


### [2] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: Δ-NeRF是一个用于增量NeRF精化的模块化残差框架，通过残差控制器、不确定性感知门控机制和视图选择策略，实现在新视图增量输入时无需重新训练整个模型，同时避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF框架在引入新视图时需要完全重新训练，限制了在卫星地形分析等数据顺序到达场景中的应用。增量精化方法研究不足，简单方法在无法访问过去数据时会出现灾难性遗忘。

Method: 提出Δ-NeRF框架，包括：1）残差控制器向冻结的基础NeRF注入逐层修正；2）不确定性感知门控机制自适应结合基础和精化预测；3）视图选择策略减少训练数据；4）知识蒸馏将增强模型压缩为紧凑学生网络。

Result: 在卫星图像上实验表明，Δ-NeRF性能与联合训练相当，训练时间减少30-42%，PSNR比简单微调提升高达43.5%，在某些指标上甚至超过联合训练。

Conclusion: Δ-NeRF为增量NeRF精化提供了有效解决方案，在保持性能的同时显著减少训练时间和数据需求，适用于卫星地形分析等顺序数据场景。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [3] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 提出Null-Text Test-Time Alignment (Null-TTA)方法，通过优化无分类器引导中的无条件嵌入来对齐扩散模型，避免奖励黑客问题并保持跨奖励泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时对齐方法容易导致目标奖励函数的欠优化或过优化（奖励黑客），需要一种更稳健的对齐方法。

Method: 通过优化无分类器引导中的无条件嵌入而非潜在变量或噪声变量，在语义连贯的流形上进行对齐，防止利用非语义噪声模式。

Result: Null-TTA在目标测试时对齐方面达到最先进水平，同时保持强大的跨奖励泛化能力。

Conclusion: 语义空间优化是测试时对齐的有效且原则性的新范式。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


### [4] [Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI](https://arxiv.org/abs/2511.20983)
*Al Amin,Kamrul Hasan,Liang Hong,Sharif Ullah*

Main category: cs.CV

TL;DR: 提出了一种结合Vision Transformers和同态加密的隐私保护联邦学习框架，用于安全的多机构病理学分类，通过加密CLS令牌实现通信效率提升和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 医疗数据共享受隐私法规限制，传统联邦学习的梯度仍易受重建攻击，需要更安全的隐私保护方案。

Method: 使用Vision Transformers的CLS令牌作为紧凑特征表示，采用CKKS同态加密在传输前加密这些令牌，实现安全聚合。

Result: CLS令牌加密比梯度加密减少30倍通信量，防止模型反演攻击，在加密域实现90.02%的分类准确率。

Conclusion: 该方法在保持强隐私保证的同时，显著降低了通信开销，为医疗联邦学习提供了可行的隐私保护解决方案。

Abstract: Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.

</details>


### [5] [Scaling Foundation Models for Radar Scene Understanding](https://arxiv.org/abs/2511.21105)
*Pushkal Mishra,Kshitiz Bansal,Dinesh Bharadia*

Main category: cs.CV

TL;DR: RadarFM是一个雷达基础模型，通过结构化空间语言监督学习统一的场景级表示，解决了现有雷达方法碎片化、任务特定化的问题。


<details>
  <summary>Details</summary>
Motivation: 雷达传感器在恶劣天气、光照和远距离条件下提供可靠感知，但现有雷达方法碎片化且任务特定化，每个下游任务使用不同的架构和训练目标，阻碍了跨任务迁移。

Method: 提出结构化标题框架在原生雷达坐标中编码车辆分布，以及哈希感知对比学习目标量化连续场景相似性而非二元匹配，实现细粒度空间推理。利用CARLA模拟器生成大规模、良好标注的雷达数据集。

Result: 开发了RadarFM雷达基础模型，学习统一的场景级表示，并提出定位感知指标评估空间准确性。

Conclusion: RadarFM通过结构化空间语言监督和对比学习，为雷达感知提供了统一的基础模型框架，克服了现有方法的碎片化问题。

Abstract: Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.

</details>


### [6] [Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting](https://arxiv.org/abs/2511.21265)
*Juncheng Chen,Chao Xu,Yanjun Cao*

Main category: cs.CV

TL;DR: MatchGS是一个利用3D高斯溅射(3DGS)生成高质量图像匹配训练数据的框架，通过几何校正和2D-3D表示对齐，显著提升零样本图像匹配性能。


<details>
  <summary>Details</summary>
Motivation: 基于学习的图像匹配需要大规模、多样化且几何准确的训练数据。虽然3DGS能够生成逼真的新视角图像，但其几何不准确性和深度渲染偏差限制了其在对应关系标注中的应用。

Method: 1) 几何忠实的数据生成流程，通过精炼3DGS几何来产生高精度对应标签；2) 2D-3D表示对齐策略，将3DGS的显式3D知识注入2D匹配器中，引导学习视角不变的3D表示。

Result: 生成的对应关系将极线误差降低了40倍，在公共基准测试中，仅使用MatchGS数据训练的匹配器实现了高达17.7%的零样本性能提升。

Conclusion: 通过适当的几何精炼，3DGS可以成为可扩展、高保真且结构丰富的数据源，为新一代鲁棒零样本图像匹配器铺平道路。

Abstract: Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.

</details>


### [7] [Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure](https://arxiv.org/abs/2511.21337)
*Munish Rathee,Boris Bačić,Maryam Doborjeh*

Main category: cs.CV

TL;DR: SIFT-SNN框架：一种低延迟神经形态信号处理管道，用于实时检测交通基础设施中的结构异常，结合SIFT空间特征编码和SNN分类，在奥克兰海港大桥数据集上达到92.3%准确率，每帧推理时间9.5毫秒。


<details>
  <summary>Details</summary>
Motivation: 开发实时、低功耗的边缘部署系统，用于交通基础设施的结构安全监测，相比传统CNN方法提供更好的可解释性和空间特征保持。

Method: 集成尺度不变特征变换(SIFT)进行空间特征编码，使用延迟驱动的脉冲转换层和泄漏积分点火(LIF)脉冲神经网络(SNN)进行分类。

Result: 在奥克兰海港大桥数据集上达到92.3%分类准确率，每帧推理时间9.5毫秒，稀疏脉冲活动率为8.1%，支持实时低功耗边缘部署。

Conclusion: SIFT-SNN框架在保持空间特征基础的同时实现了高效实时处理，为可移动混凝土护栏等交通基础设施的结构安全监测提供了可推广的案例研究。

Abstract: This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.

</details>


### [8] [SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning](https://arxiv.org/abs/2511.21420)
*Futian Wang,Mengqi Wang,Xiao Wang,Haowen Wang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于SAM基础模型的遥感变化描述方法，通过提取区域级表示和注入感兴趣区域知识来改进变化描述性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化描述方法通常使用CNN/Transformer提取视觉表示或引入辅助任务，但存在区域感知能力弱和时间对齐有限的问题。

Method: 使用CNN/Transformer提取全局视觉特征，利用SAM基础模型划分语义和运动级变化区域，构建知识图谱提供感兴趣对象信息，通过交叉注意力融合异构信息，最后用Transformer解码器生成自然语言描述。

Result: 在多个广泛使用的基准数据集上实现了最先进的性能。

Conclusion: 该方法通过结合SAM基础模型和知识图谱，有效提升了遥感变化描述任务的性能。

Abstract: Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning

</details>


### [9] [E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework](https://arxiv.org/abs/2511.21422)
*Adeela Islam,Stefano Fiorini,Manuel Lecha,Theodore Tsesmelis,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: E-M3RF是一个等变多模态3D重组框架，通过结合几何和颜色特征来解决传统方法在几何特征不足或模糊时的重组问题，使用SE(3)流匹配预测碎片变换。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法主要依赖几何特征，在几何信息不足（如小碎片、侵蚀或对称碎片）时表现不佳，且缺乏防止重叠组装的物理约束。

Method: 使用旋转等变编码器提取几何特征，Transformer提取颜色特征，形成多模态表示，通过SE(3)流匹配预测碎片变换。

Result: 在RePAIR数据集上，相比竞争方法，旋转误差减少23.1%，平移误差减少13.2%，Chamfer距离减少18.4%。

Conclusion: E-M3RF通过融合多模态特征有效解决了3D重组中的几何模糊问题，在合成和真实文化遗产数据集上均表现优异。

Abstract: 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.

</details>


### [10] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang,Fan Zhang,Xiao Wang,Mengqi Wang,Dexing Huang,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种基于超图的时空事件流补全机制，通过超图连接不同时间和空间位置的事件标记，利用上下文信息传递来补全稀疏事件，并能灵活融合RGB标记实现多模态信息补全。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生的事件流在空间上稀疏但时间上密集，现有的事件表示学习方法使用事件帧、体素或张量作为输入，但难以解决空间稀疏性导致的欠采样问题。

Method: 使用超图引导的时空事件流补全机制，通过超图连接事件标记并利用上下文信息传递补全稀疏事件；可融合RGB标记实现多模态超图信息补全；通过自注意力聚合不同时间步的超图节点信息。

Result: 在单标签和多标签事件分类任务上的大量实验充分验证了所提框架的有效性。

Conclusion: 提出的超图引导事件流补全机制能够有效解决事件数据的空间稀疏性问题，实现多模态特征的有效学习和融合。

Abstract: Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding](https://arxiv.org/abs/2511.20696)
*Dan Li,Hye-Bin Shin,Yeon-Woo Choi*

Main category: cs.LG

TL;DR: 提出ProNECL框架，在无需历史EEG样本的情况下，通过原型引导实现跨被试的持续脑电解码学习，有效平衡知识保留和适应性。


<details>
  <summary>Details</summary>
Motivation: 由于脑电信号存在显著的个体差异，在持续EEG解码任务中，引入新被试时先前获得的知识容易被覆盖。现有方法依赖存储历史数据作为回放缓冲区，但存在隐私和内存限制问题。

Method: 构建类级别原型来总结每个被试的判别性表示，通过跨被试特征对齐和知识蒸馏，增量地将新特征空间与全局原型记忆对齐。

Result: 在BCI Competition IV 2a和2b数据集上验证，框架有效平衡知识保留和适应性，在跨被试持续EEG解码任务中实现优越性能。

Conclusion: ProNECL框架能够在无需访问历史EEG样本的情况下有效保留先验知识，为持续脑电解码提供了一种实用的解决方案。

Abstract: Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.

</details>


### [12] [ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training](https://arxiv.org/abs/2511.20718)
*Chenliang Li,Adel Elmahdy,Alex Boyd,Zhongruo Wang,Alfredo Garcia,Parminder Bhatia,Taha Kass-Hout,Cao Xiao,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了两种稳定多轮对话中PPO训练的技术：轮级重要性采样和裁剪偏差校正，解决了token级PPO在LLM训练中的不稳定性问题


<details>
  <summary>Details</summary>
Motivation: PPO在多轮对话和推理任务中训练LLM时性能不稳定且容易崩溃，主要源于token级重要性采样与多轮环境结构不匹配，以及离策略样本的优势估计不准确

Method: 引入轮级重要性采样（与多轮推理的自然结构对齐）和裁剪偏差校正（通过降低不可靠离策略样本的权重来归一化梯度），形成三种变体：Turn-PPO、S-PPO和ST-PPO

Result: 在多轮搜索任务（通用QA、多跳QA、医学多选题）上的实验表明，ST-PPO和S-PPO能防止大模型训练中的性能崩溃，保持较低的裁剪比率，并获得比标准token级PPO更高的任务性能

Conclusion: 轮级重要性采样与裁剪偏差校正相结合，为多轮LLM智能体训练提供了实用且可扩展的稳定性解决方案

Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.

</details>


### [13] [Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge](https://arxiv.org/abs/2511.20726)
*Yuhang Wang,Heye Huang,Zhenhua Xu,Kailai Sun,Baoshen Guo,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出了一种结合条件变分自编码器(CVAE)和大语言模型(LLM)的高保真场景生成框架，用于生成罕见长尾事件和复杂多智能体交互场景，以增强自动驾驶系统的安全验证。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在罕见长尾事件和复杂多智能体交互方面面临关键挑战，这些场景在真实数据中稀缺但对安全验证至关重要。

Method: 使用CVAE编码历史轨迹和地图信息学习潜在交通结构，生成物理一致的基础场景；利用LLM作为对抗推理引擎，将非结构化场景描述解析为领域特定损失函数，动态指导不同风险级别的场景生成。

Result: 在CARLA和SMARTS中的实验表明，该框架显著增加了高风险和长尾事件的覆盖范围，改善了模拟与现实交通分布的一致性，并暴露了比现有方法更具挑战性的交互场景。

Conclusion: 为自动驾驶安全验证建立了新途径，能够在罕见但重要的事件下对自主系统进行原则性的压力测试。

Abstract: Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.

</details>


### [14] [A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems](https://arxiv.org/abs/2511.21032)
*Yuxuan Zhu,Cong Fu,Yabo Ni,Anxiang Zeng,Yuan Fang*

Main category: cs.LG

TL;DR: 提出ELBOₜᴅꜱ概率框架解决推荐系统中的时间分布偏移问题，通过数据增强和因果图建模提升时间泛化能力，在Shopee产品搜索中成功部署


<details>
  <summary>Details</summary>
Motivation: 时间分布偏移会侵蚀推荐系统的长期准确性，现有方法如增量训练、不变性学习和自监督学习存在时间泛化不稳定、表征塌陷或数据利用效率低等问题

Method: 1) 通过真实生产数据统计分析识别关键偏移因子，设计有效的数据增强策略；2) 使用因果图建模时序推荐场景，基于因果结构推导自监督变分目标ELBOₜᴅꜱ

Result: 在理论和实证分析支持下，方法实现了优越的时间泛化能力，用户GMV提升2.33%，已在Shopee产品搜索中成功部署

Conclusion: ELBOₜᴅꜱ框架能无缝集成到工业级增量学习流程中，有效解决时间分布偏移问题，提升推荐系统的长期准确性

Abstract: Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.

</details>


### [15] [Efficient Diffusion Planning with Temporal Diffusion](https://arxiv.org/abs/2511.21054)
*Jiaming Guo,Rui Zhang,Zerun Li,Yunkai Gao,Shaohui Peng,Siming Lan,Xing Hu,Zidong Du,Xishan Zhang,Ling Li*

Main category: cs.LG

TL;DR: 提出了时序扩散规划器(TDP)，通过将去噪步骤分布在时间维度上，提高决策效率，相比每步重新生成规划的方法，决策频率提升11-24.8倍且性能相当或更好。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划方法每步重新生成规划导致计算开销大、决策频率低，且频繁切换规划影响性能。受人类制定详细短期规划和模糊长期规划的启发，希望改进规划效率。

Method: TDP首先生成随时间逐渐模糊的初始规划，后续时间步仅用少量去噪步骤更新前一步规划而非重新生成，并引入自动重新规划机制防止规划与现实偏差过大。

Result: 在D4RL基准测试中，相比每步重新规划的方法，TDP将决策频率提升11-24.8倍，同时达到相当或更好的性能。

Conclusion: TDP通过时序分布去噪步骤有效提高了扩散规划的决策效率，在保持性能的同时显著降低了计算开销。

Abstract: Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.

</details>


### [16] [Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning](https://arxiv.org/abs/2511.21075)
*Zhenchao Tang,Fang Wang,Haohuai He,Jiale Zhou,Tianxu Lv,Jun Zhu,Shouzhi Chen,Minghao Yang,Yu Wang,Jiayang Wu,Yidong Song,Jianhua Yao*

Main category: cs.LG

TL;DR: 提出了平衡微调（BFT）方法，通过双层加权机制解决生物医学领域LLM训练中的过拟合和奖励信号缺失问题，在医疗和生物任务中显著优于标准SFT。


<details>
  <summary>Details</summary>
Motivation: 生物医学推理涉及复杂机制且数据稀疏，标准SFT容易过拟合表面指令模式，而RL方法因需要实验验证奖励而不实用。

Method: BFT采用双层加权机制：1）令牌级别通过预测概率缩放损失来稳定梯度；2）样本级别使用"最小组置信度"自适应增强困难样本学习。

Result: 在医疗任务中，BFT能学习到SFT遗漏的知识；在生物任务中，BFT模型在生物过程推理上超越GeneAgent；其文本嵌入可直接用于下游任务。

Conclusion: BFT促进了LLM在生物医学研究中的广泛应用，无需外部奖励信号即可从稀疏数据中学习复杂推理。

Abstract: Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses "minimum group confidence" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.

</details>


### [17] [Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination](https://arxiv.org/abs/2511.21118)
*Pius Onobhayedo,Paul Osemudiame Oamen*

Main category: cs.LG

TL;DR: 该论文提出了一种解决联邦学习系统中关键挑战的框架，通过密码学收据、几何新颖性测量、并行对象所有权和时间锁定策略来解决聚合责任、激励机制、可扩展性和治理问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能正从集中式提供向分布式创造转变，但联邦学习的民主愿景因聚合器缺乏问责、经济机制缺失、协调串行化限制可扩展性以及治理允许追溯操纵等组合性差距而未能实现。

Method: 利用密码学收据证明聚合正确性，几何新颖性测量防止激励博弈，并行对象所有权实现线性可扩展性，时间锁定策略检查追溯操纵。

Result: 提出的方法解决了联邦学习系统中的关键组合性差距，为分布式AI创造提供了可行的技术解决方案。

Conclusion: 该工作通过密码学和分布式系统技术的创新组合，为实现真正民主化的联邦学习系统铺平了道路，使边缘设备能够安全、高效地参与模型改进。

Abstract: Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.

</details>


### [18] [Privacy in Federated Learning with Spiking Neural Networks](https://arxiv.org/abs/2511.21181)
*Dogukan Aksu,Jesus Martinez del Rincon,Ihsen Alouani*

Main category: cs.LG

TL;DR: 本文首次系统地研究了脉冲神经网络(SNN)中的梯度泄露问题，发现与传统人工神经网络(ANN)相比，SNN的梯度泄露攻击只能产生噪声大、时间不一致的重建结果，无法恢复有意义的空间或时间结构。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在嵌入式AI中具有低功耗优势，联邦学习是边缘设备的主流训练范式。然而梯度反转攻击对联邦学习构成严重隐私威胁，虽然这种漏洞在传统ANN中已被广泛研究，但在SNN中的影响仍未被探索。

Method: 将不同的梯度泄露攻击方法适配到脉冲域，通过实证研究比较SNN和ANN在面对梯度反转攻击时的表现差异。

Result: 实验结果显示与传统ANN形成鲜明对比：ANN梯度可靠地暴露输入内容，而SNN梯度只能产生噪声大、时间不一致的重建结果，无法恢复有意义的空间或时间结构。

Conclusion: 事件驱动动态和替代梯度训练的结合显著降低了梯度的信息含量，突显了神经形态计算固有的隐私保护潜力。

Abstract: Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.

</details>


### [19] [An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids](https://arxiv.org/abs/2511.21590)
*Muhammad Siddique,Sohaib Zafar*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的智能电网数字取证框架，集成传感器数据采集、认证通信、云存储和自动化取证分析，用于实时异常检测和入侵分析。


<details>
  <summary>Details</summary>
Motivation: 智能电网融合了传统电力基础设施和先进通信网络，这种集成带来了可能破坏电网稳定性和可靠性的安全漏洞，需要有效的数字取证方法来识别和缓解安全事件。

Method: 开发了一个全面的机器学习数字取证框架，结合传感器级数据采集、认证通信、可扩展云存储和自动化取证分析，使用随机森林、支持向量机、梯度提升树和深度神经网络等监督和无监督学习算法。

Result: 在实时智能电表数据流上的仿真和实验研究表明，该框架在准确性、可扩展性和对网络攻击的弹性方面表现优异，能够有效应对数据篡改、虚假数据注入和协调控制回路操纵等攻击。

Conclusion: 云服务是大数据驱动取证工作流程的最佳骨干，使能源公用事业能够实现快速态势感知和智能事件响应。

Abstract: Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.

</details>


### [20] [DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving](https://arxiv.org/abs/2511.21669)
*Fengze Yu,Leshu Li,Brad McDanel,Saiqian Zhang*

Main category: cs.LG

TL;DR: DSD是一个分布式推测解码框架，通过在多设备部署中协调草稿-目标执行来加速大语言模型推理，解决了现有推测解码技术局限于单节点执行的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理存在高解码延迟和在异构边缘-云环境中有限可扩展性的问题，现有的推测解码技术虽然能加速token生成，但仅限于单节点执行。

Method: 提出了DSD分布式推测解码框架，通过协调草稿-目标执行扩展到多设备部署；开发了DSD-Sim离散事件模拟器来模拟网络、批处理和调度动态；设计了自适应窗口控制策略来动态调整推测窗口大小。

Result: 在多样化工作负载上的实验表明，DSD相比现有推测解码基线实现了最高1.1倍的加速和9.7%的吞吐量提升。

Conclusion: DSD能够在边缘和云环境中实现敏捷且可扩展的大语言模型服务。

Abstract: Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术从NASA航天器模拟发展而来，现正推动医疗健康转型。它通过实时数据创建患者特异性虚拟模型，应用于心脏病、肿瘤学和药物开发等领域，但仍面临互操作性、数据隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 推动医疗从被动治疗向预测性、预防性和个性化医学转变，通过数字孪生技术提高诊断和治疗效果。

Method: 整合医学影像、生物传感器和计算模型，创建动态、数据驱动的患者特异性虚拟模型，并实现双向交互。

Result: 已在心脏病（预测心律失常治疗结果）、肿瘤学（跟踪肿瘤进展和优化放疗）和药理学（加速药物发现）等领域取得代表性应用成果。

Conclusion: 数字孪生技术有望彻底改变医疗模式，但需要解决互操作性、数据隐私等挑战，并通过可解释AI、联邦学习等技术推动未来发展。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [22] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 提出了ICPO方法，通过利用LLM生成不同响应的概率来反映其对推理过程的自我评估，结合可验证奖励来增强推理能力，解决了现有RLVR方法中的粗粒度奖励、奖励噪声和低效探索问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在粗粒度奖励、奖励噪声和低效探索等问题，导致训练不稳定和熵崩溃，需要一种更有效的方法来增强LLM的推理能力。

Method: ICPO方法通过计算每个响应的偏好优势分数，比较同一输入提示下多个响应的相对生成概率，并将该分数与可验证奖励结合来指导探索过程。

Result: 在四个通用领域基准和三个数学基准上的综合实验表明，ICPO相比GRPO能稳定提升推理能力。

Conclusion: ICPO通过偏好优势分数有效缓解了粗粒度奖励和奖励噪声问题，抑制了过度自信错误，增强了被低估高质量响应的相对优势，防止模型对特定策略过拟合，促进了更彻底的探索。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [23] [EWE: An Agentic Framework for Extreme Weather Analysis](https://arxiv.org/abs/2511.21444)
*Zhe Jiang,Jiong Wang,Xiaoyu Yue,Zijie Guo,Wenlong Zhang,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.AI

TL;DR: EWE是首个用于极端天气自动诊断推理的智能代理框架，通过知识引导规划、闭环推理和气象工具包，从原始气象数据自主生成多模态可视化分析，突破了传统专家驱动诊断的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件对全球社会构成日益严重的风险，但传统专家驱动的诊断范式存在分析瓶颈，阻碍科学进展。虽然AI在地球科学预测方面取得进展，但自动诊断推理这一同等重要的挑战尚未被充分探索。

Method: EWE框架通过知识引导规划模拟专家工作流程，采用闭环推理和专门的气象工具包，能够从原始气象数据自主生成和解释多模态可视化，实现全面的诊断分析。

Result: 开发了该新兴领域的首个基准测试，包含103个高影响事件的精选数据集和新的逐步评估指标。EWE展示了自动科学发现的潜力。

Conclusion: EWE代表了迈向自动科学发现的一步，具有民主化专业知识和智力资源的潜力，特别有助于易受极端天气影响的发展中国家。

Abstract: Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.

</details>
