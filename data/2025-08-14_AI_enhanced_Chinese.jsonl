{"id": "2508.09188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09188", "abs": "https://arxiv.org/abs/2508.09188", "authors": ["Seyed Muhammad Hossein Mousavi", "S. Younes Mirinezhad"], "title": "Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation", "comment": null, "summary": "Affective computing faces a major challenge: the lack of high-quality,\ndiverse depth facial datasets for recognizing subtle emotional expressions. We\npropose a framework for synthetic depth face generation using an optimized GAN\nwith Knowledge Distillation (EMA teacher models) to stabilize training, improve\nquality, and prevent mode collapse. We also apply Genetic Algorithms to evolve\nGAN latent vectors based on image statistics, boosting diversity and visual\nquality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in\nboth diversity and quality. For classification, we extract and concatenate LBP,\nHOG, Sobel edge, and intensity histogram features, achieving 94% and 96%\naccuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows\nconsistent improvement over state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316GAN\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u5408\u6210\u6df1\u5ea6\u4eba\u8138\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u63d0\u5347\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u9762\u4e34\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u6df1\u5ea6\u9762\u90e8\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8bc6\u522b\u7ec6\u5fae\u60c5\u611f\u8868\u8fbe\u65f6\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u7684GAN\u548c\u77e5\u8bc6\u84b8\u998f\uff08EMA\u6559\u5e08\u6a21\u578b\uff09\u7a33\u5b9a\u8bad\u7ec3\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6f5c\u5728\u5411\u91cf\uff0c\u63d0\u53d6\u591a\u7279\u5f81\u7528\u4e8e\u5206\u7c7b\u3002", "result": "\u5728\u591a\u6837\u6027\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8eGAN\u3001VAE\u3001GMM\u548cKDE\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe94%\u548c96%\uff0c\u8bc4\u4f30\u6307\u6807\uff08FID\u3001IS\u3001SSIM\u3001PSNR\uff09\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5408\u6210\u6df1\u5ea6\u4eba\u8138\u751f\u6210\u548c\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4e3a\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2508.09218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09218", "abs": "https://arxiv.org/abs/2508.09218", "authors": ["Zuoou Li", "Weitong Zhang", "Jingyuan Wang", "Shuyuan Zhang", "Wenjia Bai", "Bernhard Kainz", "Mengyun Qiao"], "title": "Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity", "comment": null, "summary": "Multimodal large language models (MLLMs) are widely used in vision-language\nreasoning tasks. However, their vulnerability to adversarial prompts remains a\nserious concern, as safety mechanisms often fail to prevent the generation of\nharmful outputs. Although recent jailbreak strategies report high success\nrates, many responses classified as \"successful\" are actually benign, vague, or\nunrelated to the intended malicious goal. This mismatch suggests that current\nevaluation standards may overestimate the effectiveness of such attacks. To\naddress this issue, we introduce a four-axis evaluation framework that\nconsiders input on-topicness, input out-of-distribution (OOD) intensity, output\nharmfulness, and output refusal rate. This framework identifies truly effective\njailbreaks. In a substantial empirical study, we reveal a structural trade-off:\nhighly on-topic prompts are frequently blocked by safety filters, whereas those\nthat are too OOD often evade detection but fail to produce harmful content.\nHowever, prompts that balance relevance and novelty are more likely to evade\nfilters and trigger dangerous output. Building on this insight, we develop a\nrecursive rewriting strategy called Balanced Structural Decomposition (BSD).\nThe approach restructures malicious prompts into semantically aligned\nsub-tasks, while introducing subtle OOD signals and visual cues that make the\ninputs harder to detect. BSD was tested across 13 commercial and open-source\nMLLMs, where it consistently led to higher attack success rates, more harmful\noutputs, and fewer refusals. Compared to previous methods, it improves success\nrates by $67\\%$ and harmfulness by $21\\%$, revealing a previously\nunderappreciated weakness in current multimodal safety systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u8f74\u8bc4\u4f30\u6846\u67b6\u548c\u9012\u5f52\u91cd\u5199\u7b56\u7565\uff08BSD\uff09\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u548c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5bf9\u6297\u6027\u63d0\u793a\u7684\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u53ef\u80fd\u9ad8\u4f30\u5bf9\u6297\u6027\u63d0\u793a\u7684\u653b\u51fb\u6548\u679c\uff0c\u4e14\u5b89\u5168\u673a\u5236\u65e0\u6cd5\u6709\u6548\u963b\u6b62\u6709\u5bb3\u8f93\u51fa\u3002", "method": "\u5f15\u5165\u56db\u8f74\u8bc4\u4f30\u6846\u67b6\uff08\u8f93\u5165\u76f8\u5173\u6027\u3001\u8f93\u5165OOD\u5f3a\u5ea6\u3001\u8f93\u51fa\u6709\u5bb3\u6027\u3001\u8f93\u51fa\u62d2\u7edd\u7387\uff09\uff0c\u5e76\u5f00\u53d1BSD\u7b56\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u5b50\u4efb\u52a1\u548cOOD\u4fe1\u53f7\u63d0\u5347\u653b\u51fb\u6548\u679c\u3002", "result": "BSD\u572813\u4e2aMLLMs\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u548c\u8f93\u51fa\u6709\u5bb3\u6027\uff0c\u6210\u529f\u7387\u63d0\u534767%\uff0c\u6709\u5bb3\u6027\u63d0\u534721%\u3002", "conclusion": "\u5f53\u524d\u591a\u6a21\u6001\u5b89\u5168\u7cfb\u7edf\u5b58\u5728\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u7684\u5f31\u70b9\uff0cBSD\u7b56\u7565\u63ed\u793a\u4e86\u5e73\u8861\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u7684\u5bf9\u6297\u6027\u63d0\u793a\u66f4\u6613\u7ed5\u8fc7\u68c0\u6d4b\u3002"}}
{"id": "2508.09239", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09239", "abs": "https://arxiv.org/abs/2508.09239", "authors": ["Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Jia-Chen Zhang", "Hong-Jian Zhan"], "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting", "comment": null, "summary": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced\nnovel view synthesis through explicit scene representation, enabling real-time\nphotorealistic rendering. However, existing approaches manifest two critical\nlimitations in complex scenarios: (1) Over-reconstruction occurs when\npersistent large Gaussians cannot meet adaptive splitting thresholds during\ndensity control. This is exacerbated by conflicting gradient directions that\nprevent effective splitting of these Gaussians; (2) Over-densification of\nGaussians occurs in regions with aligned gradient aggregation, leading to\nredundant component proliferation. This redundancy significantly increases\nmemory overhead due to unnecessary data retention. We present\nGradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware\nadaptive density control framework to address these challenges. Our key\ninnovations: the gradient coherence ratio (GCR), computed through normalized\ngradient vector norms, which explicitly discriminates Gaussians with concordant\nversus conflicting gradient directions; and a nonlinear dynamic weighting\nmechanism leverages the GCR to enable gradient-direction-aware density control.\nSpecifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting\noperations to enhance geometric details while suppressing redundant\nconcordant-direction Gaussians. Conversely, in cloning processes, GDAGS\npromotes concordant-direction Gaussian densification for structural completion\nwhile preventing conflicting-direction Gaussian overpopulation. Comprehensive\nevaluations across diverse real-world benchmarks demonstrate that GDAGS\nachieves superior rendering quality while effectively mitigating\nover-reconstruction, suppressing over-densification, and constructing compact\nscene representations with 50\\% reduced memory consumption through optimized\nGaussians utilization.", "AI": {"tldr": "GDAGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u96c6\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b58\u5728\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u96c6\u95ee\u9898\uff0c\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u589e\u52a0\u548c\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002", "method": "GDAGS\u901a\u8fc7\u68af\u5ea6\u4e00\u81f4\u6027\u6bd4\u7387\uff08GCR\uff09\u548c\u975e\u7ebf\u6027\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u5b9e\u73b0\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u7684\u5bc6\u5ea6\u63a7\u5236\u3002", "result": "GDAGS\u5728\u591a\u79cd\u771f\u5b9e\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e8650%\u7684\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "GDAGS\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u903c\u771f\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09158", "abs": "https://arxiv.org/abs/2508.09158", "authors": ["Siwen Jiao", "Kangan Qian", "Hao Ye", "Yang Zhong", "Ziang Luo", "Sicong Jiang", "Zilin Huang", "Yangyi Fang", "Jinyu Miao", "Zheng Fu", "Yunlong Wang", "Kun Jiang", "Diange Yang", "Rui Fan", "Baoyun Peng"], "title": "EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving", "comment": null, "summary": "Autonomous driving faces significant challenges in achieving human-like\niterative decision-making, which continuously generates, evaluates, and refines\ntrajectory proposals. Current generation-evaluation frameworks isolate\ntrajectory generation from quality assessment, preventing iterative refinement\nessential for planning, while reinforcement learning methods collapse\nmulti-dimensional preferences into scalar rewards, obscuring critical\ntrade-offs and yielding scalarization bias.To overcome these issues, we present\nEvaDrive, a novel multi-objective reinforcement learning framework that\nestablishes genuine closed-loop co-evolution between trajectory generation and\nevaluation via adversarial optimization. EvaDrive frames trajectory planning as\na multi-round adversarial game. In this game, a hierarchical generator\ncontinuously proposes candidate paths by combining autoregressive intent\nmodeling for temporal causality with diffusion-based refinement for spatial\nflexibility. These proposals are then rigorously assessed by a trainable\nmulti-objective critic that explicitly preserves diverse preference structures\nwithout collapsing them into a single scalarization bias.This adversarial\ninterplay, guided by a Pareto frontier selection mechanism, enables iterative\nmulti-round refinement, effectively escaping local optima while preserving\ntrajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks\ndemonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing\nDiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving\nScore on Bench2Drive. EvaDrive generates diverse driving styles via dynamic\nweighting without external preference data, introducing a closed-loop\nadversarial framework for human-like iterative decision-making, offering a\nnovel scalarization-free trajectory optimization approach.", "AI": {"tldr": "EvaDrive\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u4f18\u5316\u5b9e\u73b0\u8f68\u8ff9\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u95ed\u73af\u534f\u540c\u8fdb\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u6807\u91cf\u5316\u504f\u5dee\u548c\u8fed\u4ee3\u4f18\u5316\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8fed\u4ee3\u51b3\u7b56\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8f68\u8ff9\u751f\u6210\u4e0e\u8bc4\u4f30\u4e4b\u95f4\u5b58\u5728\u9694\u79bb\uff0c\u4e14\u5f3a\u5316\u5b66\u4e60\u5c06\u591a\u7ef4\u504f\u597d\u538b\u7f29\u4e3a\u6807\u91cf\u5956\u52b1\uff0c\u5bfc\u81f4\u5173\u952e\u6743\u8861\u88ab\u63a9\u76d6\u3002", "method": "EvaDrive\u91c7\u7528\u5206\u5c42\u751f\u6210\u5668\uff08\u7ed3\u5408\u81ea\u56de\u5f52\u610f\u56fe\u5efa\u6a21\u548c\u6269\u6563\u7ec6\u5316\uff09\u548c\u591a\u76ee\u6807\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u5bf9\u6297\u6e38\u620f\u5b9e\u73b0\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\uff0c\u5e76\u5229\u7528\u5e15\u7d2f\u6258\u524d\u6cbf\u9009\u62e9\u673a\u5236\u3002", "result": "\u5728NAVSIM\u548cBench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8fbe\u523094.9 PDMS\u548c64.96 Driving Score\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EvaDrive\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u751f\u6210\u591a\u6837\u9a7e\u9a76\u98ce\u683c\uff0c\u65e0\u9700\u5916\u90e8\u504f\u597d\u6570\u636e\uff0c\u4e3a\u6807\u91cf\u5316\u81ea\u7531\u7684\u8f68\u8ff9\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.09160", "categories": ["cs.LG", "cs.DB", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.09160", "abs": "https://arxiv.org/abs/2508.09160", "authors": ["Beyza Cinar", "Maria Maleshkova"], "title": "Presenting DiaData for Research on Type 1 Diabetes", "comment": "11 pages, 7 figures, 3 tables", "summary": "Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction\nof insulin-producing cells, resulting in insulin deficiency, as to why the\naffected individuals depend on external insulin injections. However, insulin\ncan decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a\nsevere event of low blood glucose levels ($\\le$70 mg/dL) with dangerous side\neffects of dizziness, coma, or death. Data analysis can significantly enhance\ndiabetes care by identifying personal patterns and trends leading to adverse\nevents. Especially, machine learning (ML) models can predict glucose levels and\nprovide early alarms. However, diabetes and hypoglycemia research is limited by\nthe unavailability of large datasets. Thus, this work systematically integrates\n15 datasets to provide a large database of 2510 subjects with glucose\nmeasurements recorded every 5 minutes. In total, 149 million measurements are\nincluded, of which 4% represent values in the hypoglycemic range. Moreover, two\nsub-databases are extracted. Sub-database I includes demographics, and\nsub-database II includes heart rate data. The integrated dataset provides an\nequal distribution of sex and different age levels. As a further contribution,\ndata quality is assessed, revealing that data imbalance and missing values\npresent a significant challenge. Moreover, a correlation study on glucose\nlevels and heart rate data is conducted, showing a relation between 15 and 55\nminutes before hypoglycemia.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6574\u5408\u4e8615\u4e2a\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2510\u540d\u53d7\u8bd5\u8005\u7684\u5927\u89c4\u6a21\u6570\u636e\u5e93\uff0c\u7528\u4e8e\u7814\u7a761\u578b\u7cd6\u5c3f\u75c5\u548c\u4f4e\u8840\u7cd6\u4e8b\u4ef6\u3002\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8840\u7cd6\u6c34\u5e73\uff0c\u5e76\u5206\u6790\u4e86\u8840\u7cd6\u4e0e\u5fc3\u7387\u6570\u636e\u7684\u5173\u7cfb\u3002", "motivation": "1\u578b\u7cd6\u5c3f\u75c5\uff08T1D\uff09\u60a3\u8005\u4f9d\u8d56\u5916\u90e8\u80f0\u5c9b\u7d20\u6ce8\u5c04\uff0c\u4f46\u80f0\u5c9b\u7d20\u53ef\u80fd\u5bfc\u81f4\u4f4e\u8840\u7cd6\uff0c\u5e26\u6765\u4e25\u91cd\u540e\u679c\u3002\u73b0\u6709\u7814\u7a76\u56e0\u6570\u636e\u4e0d\u8db3\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u6574\u5408\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u6539\u8fdb\u7cd6\u5c3f\u75c5\u62a4\u7406\u3002", "method": "\u7cfb\u7edf\u6574\u540815\u4e2a\u6570\u636e\u96c6\uff0c\u5f62\u6210\u5305\u542b2510\u540d\u53d7\u8bd5\u8005\u7684\u6570\u636e\u5e93\uff0c\u8bb0\u5f55\u6bcf5\u5206\u949f\u7684\u8840\u7cd6\u6d4b\u91cf\u503c\uff08\u51711.49\u4ebf\u6b21\uff09\u3002\u63d0\u53d6\u4e24\u4e2a\u5b50\u6570\u636e\u5e93\uff08\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u548c\u5fc3\u7387\u6570\u636e\uff09\uff0c\u5e76\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\u3002", "result": "\u6570\u636e\u5e93\u5305\u542b4%\u7684\u4f4e\u8840\u7cd6\u8303\u56f4\u6570\u636e\uff0c\u6027\u522b\u548c\u5e74\u9f84\u5206\u5e03\u5747\u8861\u3002\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u503c\u662f\u4e3b\u8981\u6311\u6218\u3002\u8840\u7cd6\u4e0e\u5fc3\u7387\u6570\u636e\u5728\u4f4e\u8840\u7cd6\u524d15\u81f355\u5206\u949f\u5b58\u5728\u76f8\u5173\u6027\u3002", "conclusion": "\u6574\u5408\u7684\u6570\u636e\u96c6\u4e3a\u7cd6\u5c3f\u75c5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\uff0c\u4f46\u9700\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002\u8840\u7cd6\u4e0e\u5fc3\u7387\u7684\u76f8\u5173\u6027\u4e3a\u4f4e\u8840\u7cd6\u9884\u8b66\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.09262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09262", "abs": "https://arxiv.org/abs/2508.09262", "authors": ["Dongwoo Kang", "Akhil Perincherry", "Zachary Coalson", "Aiden Gabriel", "Stefan Lee", "Sanghyun Hong"], "title": "Harnessing Input-Adaptive Inference for Efficient VLN", "comment": "Accepted to ICCV 2025 [Poster]", "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f93\u5165\u81ea\u9002\u5e94\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u7b97\u6cd5\u63d0\u5347\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u6a21\u578b\u7684\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u6709\u8f93\u5165\u81ea\u9002\u5e94\u673a\u5236\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u65f6\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u9009\u62e9\u6027\u5904\u7406\u5168\u666f\u89c6\u56fe\uff1b2. \u57fa\u4e8e\u91cd\u8981\u6027\u7684\u81ea\u9002\u5e94\u9608\u503c\u63d0\u524d\u9000\u51fa\uff1b3. \u7f13\u5b58\u673a\u5236\u907f\u514d\u91cd\u590d\u5904\u7406\u89c6\u56fe\u3002", "result": "\u5728\u4e03\u4e2aVLN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11\u8d85\u8fc72\u500d\uff0c\u6027\u80fd\u672a\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86VLN\u6a21\u578b\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2508.09603", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09603", "abs": "https://arxiv.org/abs/2508.09603", "authors": ["Skyler Hallinan", "Jaehun Jung", "Melanie Sclar", "Ximing Lu", "Abhilasha Ravichander", "Sahana Ramnath", "Yejin Choi", "Sai Praneeth Karimireddy", "Niloofar Mireshghallah", "Xiang Ren"], "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage", "comment": "CoLM 2025", "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aN-Gram Coverage Attack\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u76ee\u6807\u6a21\u578b\u7684\u6587\u672c\u8f93\u51fa\uff0c\u9002\u7528\u4e8e\u9ed1\u76d2\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u751f\u6210\u6587\u672c\u4e0e\u5019\u9009\u6210\u5458\u7684\u540e\u7f00\u76f8\u4f3c\u6027\u6765\u5224\u65ad\u6210\u5458\u8d44\u683c\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u9ed1\u76d2\u65b9\u6cd5\uff0c\u751a\u81f3\u5ab2\u7f8e\u767d\u76d2\u653b\u51fb\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u653b\u51fb\u6210\u529f\u7387\u968f\u8ba1\u7b97\u9884\u7b97\u589e\u52a0\u800c\u63d0\u5347\uff0c\u5e76\u53d1\u73b0GPT-4o\u7b49\u65b0\u6a21\u578b\u5bf9\u6210\u5458\u63a8\u7406\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u3002", "motivation": "\u5f53\u524d\u8bb8\u591a\u5148\u8fdb\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u6216\u6982\u7387\u5206\u5e03\uff0c\u9650\u5236\u4e86\u5176\u5728\u4ec5\u63d0\u4f9bAPI\u8bbf\u95ee\u7684\u6a21\u578b\uff08\u5982GPT-4\uff09\u4e0a\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u6587\u672c\u8f93\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "N-Gram Coverage Attack\u901a\u8fc7\u83b7\u53d6\u76ee\u6807\u6a21\u578b\u5728\u5019\u9009\u6210\u5458\u524d\u7f00\u6761\u4ef6\u4e0b\u7684\u591a\u4e2a\u751f\u6210\u6587\u672c\uff0c\u5229\u7528n-gram\u91cd\u53e0\u5ea6\u91cf\u8ba1\u7b97\u8fd9\u4e9b\u8f93\u51fa\u4e0e\u771f\u5b9e\u540e\u7f00\u7684\u76f8\u4f3c\u6027\uff0c\u9ad8\u76f8\u4f3c\u6027\u8868\u660e\u53ef\u80fd\u662f\u6210\u5458\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u9ed1\u76d2\u65b9\u6cd5\uff0c\u751a\u81f3\u5ab2\u7f8e\u767d\u76d2\u653b\u51fb\u3002\u653b\u51fb\u6210\u529f\u7387\u968f\u751f\u6210\u5e8f\u5217\u6570\u91cf\u589e\u52a0\u800c\u63d0\u5347\u3002\u5b9e\u9a8c\u8fd8\u53d1\u73b0GPT-4o\u7b49\u65b0\u6a21\u578b\u5bf9\u6210\u5458\u63a8\u7406\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\u3002", "conclusion": "N-Gram Coverage Attack\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684API\u6a21\u578b\u3002\u540c\u65f6\uff0c\u65b0\u6a21\u578b\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u51fa\u8fdb\u6b65\u3002"}}
{"id": "2508.09180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09180", "abs": "https://arxiv.org/abs/2508.09180", "authors": ["Huifa Li", "Jie Fu", "Xinlin Zhuang", "Haolin Yang", "Xinpeng Ling", "Tong Cheng", "Haochen xue", "Imran Razzak", "Zhili Chen"], "title": "scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering", "comment": null, "summary": "Accurate cell type annotation is a crucial step in analyzing single-cell RNA\nsequencing (scRNA-seq) data, which provides valuable insights into cellular\nheterogeneity. However, due to the high dimensionality and prevalence of zero\nelements in scRNA-seq data, traditional clustering methods face significant\nstatistical and computational challenges. While some advanced methods use graph\nneural networks to model cell-cell relationships, they often depend on static\ngraph structures that are sensitive to noise and fail to capture the\nlong-tailed distribution inherent in single-cell populations.To address these\nlimitations, we propose scAGC, a single-cell clustering method that learns\nadaptive cell graphs with contrastive guidance. Our approach optimizes feature\nrepresentations and cell graphs simultaneously in an end-to-end manner.\nSpecifically, we introduce a topology-adaptive graph autoencoder that leverages\na differentiable Gumbel-Softmax sampling strategy to dynamically refine the\ngraph structure during training. This adaptive mechanism mitigates the problem\nof a long-tailed degree distribution by promoting a more balanced neighborhood\nstructure. To model the discrete, over-dispersed, and zero-inflated nature of\nscRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for\nrobust feature reconstruction. Furthermore, a contrastive learning objective is\nincorporated to regularize the graph learning process and prevent abrupt\nchanges in the graph topology, ensuring stability and enhancing convergence.\nComprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC\nconsistently outperforms other state-of-the-art methods, yielding the best NMI\nand ARI scores on 9 and 7 datasets, respectively.Our code is available at\nAnonymous Github.", "AI": {"tldr": "scAGC\u662f\u4e00\u79cd\u5355\u7ec6\u80de\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6307\u5bfc\u81ea\u9002\u5e94\u7ec6\u80de\u56fe\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7a00\u758f\u6570\u636e\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u7684\u9ad8\u7ef4\u6027\u548c\u7a00\u758f\u6027\u4f7f\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u56fe\u7ed3\u6784\uff0c\u96be\u4ee5\u6355\u6349\u957f\u5c3e\u5206\u5e03\u3002", "method": "scAGC\u7ed3\u5408\u62d3\u6251\u81ea\u9002\u5e94\u56fe\u81ea\u7f16\u7801\u5668\u548cZINB\u635f\u5931\u51fd\u6570\uff0c\u52a8\u6001\u4f18\u5316\u56fe\u7ed3\u6784\u548c\u7279\u5f81\u8868\u793a\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "result": "\u57289\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cscAGC\u5728NMI\u548cARI\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "scAGC\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u548c\u5bf9\u6bd4\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u7ec6\u80de\u805a\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.09397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09397", "abs": "https://arxiv.org/abs/2508.09397", "authors": ["Zhengli Zhang", "Xinyu Luo", "Yuchen Sun", "Wenhua Ding", "Dongyu Huang", "Xinlei Chen"], "title": "Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety", "comment": null, "summary": "Drones operating in complex environments face a significant threat from thin\nobstacles, such as steel wires and kite strings at the submillimeter level,\nwhich are notoriously difficult for conventional sensors like RGB cameras,\nLiDAR, and depth cameras to detect. This paper introduces SkyShield, an\nevent-driven, end-to-end framework designed for the perception of submillimeter\nscale obstacles. Drawing upon the unique features that thin obstacles present\nin the event stream, our method employs a lightweight U-Net architecture and an\ninnovative Dice-Contour Regularization Loss to ensure precise detection.\nExperimental results demonstrate that our event-based approach achieves mean F1\nScore of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment\non edge and mobile platforms.", "AI": {"tldr": "SkyShield\u662f\u4e00\u4e2a\u4e8b\u4ef6\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e9a\u6beb\u7c73\u7ea7\u969c\u788d\u7269\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7U-Net\u67b6\u6784\u548c\u521b\u65b0\u7684Dice-Contour\u6b63\u5219\u5316\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u590d\u6742\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u9762\u4e34\u7684\u4e9a\u6beb\u7c73\u7ea7\u969c\u788d\u7269\uff08\u5982\u94a2\u4e1d\u3001\u98ce\u7b5d\u7ebf\uff09\u96be\u4ee5\u88ab\u4f20\u7edf\u4f20\u611f\u5668\u68c0\u6d4b\uff0c\u9700\u8981\u65b0\u7684\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4e8b\u4ef6\u6d41\u7684\u72ec\u7279\u7279\u5f81\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7U-Net\u67b6\u6784\u548cDice-Contour\u6b63\u5219\u5316\u635f\u5931\u8fdb\u884c\u7cbe\u786e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747F1\u5206\u6570\u4e3a0.7088\uff0c\u5ef6\u8fdf\u4ec5\u4e3a21.2\u6beb\u79d2\uff0c\u9002\u5408\u8fb9\u7f18\u548c\u79fb\u52a8\u5e73\u53f0\u90e8\u7f72\u3002", "conclusion": "SkyShield\u6846\u67b6\u5728\u4e9a\u6beb\u7c73\u7ea7\u969c\u788d\u7269\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09625", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09625", "abs": "https://arxiv.org/abs/2508.09625", "authors": ["Daoxin Zhong", "Jun Li", "Meng Yee Michael Chuah"], "title": "Plane Detection and Ranking via Model Information Optimization", "comment": "Accepted as contributed paper in the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Plane detection from depth images is a crucial subtask with broad robotic\napplications, often accomplished by iterative methods such as Random Sample\nConsensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic\nguarantees, the ambiguity of its inlier threshold criterion makes it\nsusceptible to false positive plane detections. This issue is particularly\nprevalent in complex real-world scenes, where the true number of planes is\nunknown and multiple planes coexist. In this paper, we aim to address this\nlimitation by proposing a generalised framework for plane detection based on\nmodel information optimization. Building on previous works, we treat the\nobserved depth readings as discrete random variables, with their probability\ndistributions constrained by the ground truth planes. Various models containing\ndifferent candidate plane constraints are then generated through repeated\nrandom sub-sampling to explain our observations. By incorporating the physics\nand noise model of the depth sensor, we can calculate the information for each\nmodel, and the model with the least information is accepted as the most likely\nground truth. This information optimization process serves as an objective\nmechanism for determining the true number of planes and preventing false\npositive detections. Additionally, the quality of each detected plane can be\nranked by summing the information reduction of inlier points for each plane. We\nvalidate these properties through experiments with synthetic data and find that\nour algorithm estimates plane parameters more accurately compared to the\ndefault Open3D RANSAC plane segmentation. Furthermore, we accelerate our\nalgorithm by partitioning the depth map using neural network segmentation,\nwhich enhances its ability to generate more realistic plane parameters in\nreal-world data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4fe1\u606f\u4f18\u5316\u7684\u5e73\u9762\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u6700\u5c0f\u5316\u9009\u62e9\u6700\u53ef\u80fd\u7684\u5730\u9762\u771f\u5b9e\u6a21\u578b\uff0c\u51cf\u5c11\u8bef\u68c0\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3RANSAC\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u56e0\u9608\u503c\u6a21\u7cca\u5bfc\u81f4\u7684\u8bef\u68c0\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5f53\u771f\u5b9e\u5e73\u9762\u6570\u91cf\u672a\u77e5\u65f6\u3002", "method": "\u5c06\u6df1\u5ea6\u6570\u636e\u89c6\u4e3a\u79bb\u6563\u968f\u673a\u53d8\u91cf\uff0c\u901a\u8fc7\u968f\u673a\u5b50\u91c7\u6837\u751f\u6210\u5019\u9009\u5e73\u9762\u6a21\u578b\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u7269\u7406\u548c\u566a\u58f0\u6a21\u578b\u8ba1\u7b97\u4fe1\u606f\u91cf\uff0c\u9009\u62e9\u4fe1\u606f\u6700\u5c11\u7684\u6a21\u578b\u4f5c\u4e3a\u771f\u5b9e\u5e73\u9762\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u7b97\u6cd5\u6bd4Open3D RANSAC\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u5e73\u9762\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5206\u5272\u52a0\u901f\u5904\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u8bef\u68c0\u5e76\u63d0\u9ad8\u5e73\u9762\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2508.09203", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09203", "abs": "https://arxiv.org/abs/2508.09203", "authors": ["Zhenhui Ou", "Dawei Li", "Zhen Tan", "Wenlin Li", "Huan Liu", "Siyuan Song"], "title": "Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research", "comment": "The paper was accepted on the CIKM 2025", "summary": "Construction safety research is a critical field in civil engineering, aiming\nto mitigate risks and prevent injuries through the analysis of site conditions\nand human factors. However, the limited volume and lack of diversity in\nexisting construction safety datasets pose significant challenges to conducting\nin-depth analyses. To address this research gap, this paper introduces the\nConstruction Safety Dataset (CSDataset), a well-organized comprehensive\nmulti-level dataset that encompasses incidents, inspections, and violations\nrecorded sourced from the Occupational Safety and Health Administration (OSHA).\nThis dataset uniquely integrates structured attributes with unstructured\nnarratives, facilitating a wide range of approaches driven by machine learning\nand large language models. We also conduct a preliminary approach benchmarking\nand various cross-level analyses using our dataset, offering insights to inform\nand enhance future efforts in construction safety. For example, we found that\ncomplaint-driven inspections were associated with a 17.3% reduction in the\nlikelihood of subsequent incidents. Our dataset and code are released at\nhttps://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Construction Safety Dataset (CSDataset)\uff0c\u4e00\u4e2a\u591a\u5c42\u6b21\u7684\u5efa\u7b51\u5b89\u5168\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u8bed\u8a00\u6a21\u578b\u5206\u6790\u3002\u521d\u6b65\u5206\u6790\u663e\u793a\u6295\u8bc9\u9a71\u52a8\u7684\u68c0\u67e5\u80fd\u51cf\u5c1117.3%\u7684\u4e8b\u6545\u6982\u7387\u3002", "motivation": "\u73b0\u6709\u5efa\u7b51\u5b89\u5168\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6df1\u5165\u5206\u6790\u3002", "method": "\u5f15\u5165CSDataset\uff0c\u6574\u5408OSHA\u7684\u4e8b\u6545\u3001\u68c0\u67e5\u548c\u8fdd\u89c4\u8bb0\u5f55\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u6295\u8bc9\u9a71\u52a8\u7684\u68c0\u67e5\u4e0e\u540e\u7eed\u4e8b\u6545\u6982\u7387\u964d\u4f4e17.3%\u76f8\u5173\u3002", "conclusion": "CSDataset\u4e3a\u5efa\u7b51\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765\u66f4\u6df1\u5165\u7684\u5206\u6790\u548c\u6539\u8fdb\u3002"}}
{"id": "2508.09223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09223", "abs": "https://arxiv.org/abs/2508.09223", "authors": ["Sameer Ambekar", "Daniel M. Lang", "Julia A. Schnabel"], "title": "Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation", "comment": null, "summary": "Test-time adaptation allows pretrained models to adjust to incoming data\nstreams, addressing distribution shifts between source and target domains.\nHowever, standard methods rely on single-dimensional linear classification\nlayers, which often fail to handle diverse and complex shifts. We propose\nHierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages\nmultiple layers of increasing size for dynamic test-time adaptation. By\ndecomposing the encoder's representation space into such hierarchically\norganized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to\nadapt to shifts of varying complexity. Our contributions are threefold: First,\nwe propose dynamic layer selection for automatic identification of the optimal\nlayer for adaptation to each test batch. Second, we propose a mechanism that\nmerges weights from the dynamic layer to other layers, ensuring all layers\nreceive target information. Third, we propose linear layer agreement that acts\nas a gating function, preventing erroneous fine-tuning by adaptation on noisy\nbatches. We rigorously evaluate the performance of Hi-Vec in challenging\nscenarios and on multiple target datasets, proving its strong capability to\nadvance state-of-the-art methods. Our results show that Hi-Vec improves\nrobustness, addresses uncertainty, and handles limited batch sizes and\nincreased outlier rates.", "AI": {"tldr": "Hi-Vec\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u81ea\u9002\u5e94\u7f51\u7edc\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5c42\u548c\u6743\u91cd\u5408\u5e76\u673a\u5236\uff0c\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u5206\u5e03\u53d8\u5316\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u5355\u7ef4\u7ebf\u6027\u5206\u7c7b\u5c42\u65e0\u6cd5\u5904\u7406\u590d\u6742\u5206\u5e03\u53d8\u5316\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7ec4\u7ec7\u5c42\u3001\u52a8\u6001\u5c42\u9009\u62e9\u3001\u6743\u91cd\u5408\u5e76\u673a\u5236\u548c\u7ebf\u6027\u5c42\u4e00\u81f4\u6027\u95e8\u63a7\u3002", "result": "\u5728\u591a\u79cd\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86Hi-Vec\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u548c\u5f02\u5e38\u503c\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "Hi-Vec\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5206\u5e03\u53d8\u5316\u573a\u666f\u3002"}}
{"id": "2508.09466", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.09466", "abs": "https://arxiv.org/abs/2508.09466", "authors": ["Tam Ngoc-Bang Nguyen", "Anh-Dzung Doan", "Zhipeng Cai", "Tat-Jun Chin"], "title": "Event-driven Robust Fitting on Neuromorphic Hardware", "comment": "11 pages, accepted in ICCV 2025 Workshop on Neuromorphic Vision\n  (NeVI)", "summary": "Robust fitting of geometric models is a fundamental task in many computer\nvision pipelines. Numerous innovations have been produced on the topic, from\nimproving the efficiency and accuracy of random sampling heuristics to\ngenerating novel theoretical insights that underpin new approaches with\nmathematical guarantees. However, one aspect of robust fitting that has\nreceived little attention is energy efficiency. This performance metric has\nbecome critical as high energy consumption is a growing concern for AI\nadoption. In this paper, we explore energy-efficient robust fitting via the\nneuromorphic computing paradigm. Specifically, we designed a novel spiking\nneural network for robust fitting on real neuromorphic hardware, the Intel\nLoihi 2. Enabling this are novel event-driven formulations of model estimation\nthat allow robust fitting to be implemented in the unique architecture of Loihi\n2, and algorithmic strategies to alleviate the current limited precision and\ninstruction set of the hardware. Results show that our neuromorphic robust\nfitting consumes only a fraction (15%) of the energy required to run the\nestablished robust fitting algorithm on a standard CPU to equivalent accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u8303\u5f0f\u7684\u9ad8\u6548\u80fd\u91cf\u9c81\u68d2\u62df\u5408\u65b9\u6cd5\uff0c\u4f7f\u7528\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728Intel Loihi 2\u786c\u4ef6\u4e0a\u5b9e\u73b0\uff0c\u80fd\u8017\u4ec5\u4e3a\u4f20\u7edfCPU\u65b9\u6cd5\u768415%\u3002", "motivation": "\u968f\u7740AI\u80fd\u8017\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u7814\u7a76\u80fd\u91cf\u9ad8\u6548\u7684\u9c81\u68d2\u62df\u5408\u65b9\u6cd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u5f00\u53d1\u4e86\u4e8b\u4ef6\u9a71\u52a8\u7684\u6a21\u578b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94Loihi 2\u786c\u4ef6\u7684\u72ec\u7279\u67b6\u6784\u3002", "result": "\u5728\u76f8\u540c\u7cbe\u5ea6\u4e0b\uff0c\u795e\u7ecf\u5f62\u6001\u9c81\u68d2\u62df\u5408\u7684\u80fd\u8017\u4ec5\u4e3a\u4f20\u7edfCPU\u65b9\u6cd5\u768415%\u3002", "conclusion": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e3a\u80fd\u91cf\u9ad8\u6548\u7684\u9c81\u68d2\u62df\u5408\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09878", "abs": "https://arxiv.org/abs/2508.09878", "authors": ["Archie Sage", "Jeroen Keppens", "Helen Yannakoudakis"], "title": "A Survey of Cognitive Distortion Detection and Classification in NLP", "comment": "Under review via ACL Rolling Review and committed to EMNLP 2025.\n  Camera-ready updates to follow", "summary": "As interest grows in the application of natural language processing (NLP)\ntechniques to mental health, a growing body of work explores the automatic\ndetection and classification of cognitive distortions (CDs). CDs are habitual\npatterns of negatively biased or flawed thinking that distort how people\nperceive events, judge themselves, and react to the world around them.\nIdentifying and addressing them is an important part of therapy. Despite its\nmomentum, the field remains fragmented, with inconsistencies in CD taxonomies,\ntask formulations, and evaluation practices. This survey reviews 38 studies\nspanning two decades, providing a structured overview of datasets, modelling\napproaches, and evaluation strategies. We provide a consolidated CD taxonomy\nreference, summarise common task setups, and highlight open challenges to\nsupport more coherent and reproducible research in this emerging area.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u8ba4\u77e5\u626d\u66f2\uff08CDs\uff09\u7684\u81ea\u52a8\u68c0\u6d4b\u4e0e\u5206\u7c7b\u7814\u7a76\u3002\u901a\u8fc7\u5206\u679038\u9879\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u3001\u5efa\u6a21\u65b9\u6cd5\u548c\u8bc4\u4f30\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u7684CD\u5206\u7c7b\u53c2\u8003\u3002", "motivation": "\u8ba4\u77e5\u626d\u66f2\u662f\u5fc3\u7406\u5065\u5eb7\u6cbb\u7597\u4e2d\u7684\u91cd\u8981\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u9886\u57df\u5b58\u5728\u5206\u7c7b\u3001\u4efb\u52a1\u5b9a\u4e49\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u4ee5\u63a8\u52a8\u66f4\u4e00\u81f4\u548c\u53ef\u91cd\u590d\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u8fc7\u53bb20\u5e74\u768438\u9879\u7814\u7a76\uff0c\u5206\u6790\u4e86\u6570\u636e\u96c6\u3001\u5efa\u6a21\u65b9\u6cd5\u548c\u8bc4\u4f30\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u7684CD\u5206\u7c7b\u53c2\u8003\u3002", "result": "\u603b\u7ed3\u4e86CD\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3aCD\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7efc\u8ff0\u548c\u5206\u7c7b\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u66f4\u4e00\u81f4\u548c\u53ef\u91cd\u590d\u53d1\u5c55\u3002"}}
{"id": "2508.09486", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.09486", "abs": "https://arxiv.org/abs/2508.09486", "authors": ["Yun Wang", "Long Zhang", "Jingren Liu", "Jiaqi Yan", "Zhanjie Zhang", "Jiahao Zheng", "Xun Yang", "Dapeng Wu", "Xiangyu Chen", "Xuelong Li"], "title": "Episodic Memory Representation for Long-form Video Understanding", "comment": "10 pages, 5 figures", "summary": "Video Large Language Models (Video-LLMs) excel at general video understanding\nbut struggle with long-form videos due to context window limits. Consequently,\nrecent approaches focus on keyframe retrieval, condensing lengthy videos into a\nsmall set of informative frames. Despite their practicality, these methods\nsimplify the problem to static text image matching, overlooking spatio temporal\nrelationships crucial for capturing scene transitions and contextual\ncontinuity, and may yield redundant keyframes with limited information,\ndiluting salient cues essential for accurate video question answering. To\naddress these limitations, we introduce Video-EM, a training free framework\ninspired by the principles of human episodic memory, designed to facilitate\nrobust and contextually grounded reasoning. Rather than treating keyframes as\nisolated visual entities, Video-EM explicitly models them as temporally ordered\nepisodic events, capturing both spatial relationships and temporal dynamics\nnecessary for accurately reconstructing the underlying narrative. Furthermore,\nthe framework leverages chain of thought (CoT) thinking with LLMs to\niteratively identify a minimal yet highly informative subset of episodic\nmemories, enabling efficient and accurate question answering by Video-LLMs.\nExtensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench\nbenchmarks confirm the superiority of Video-EM, which achieves highly\ncompetitive results with performance gains of 4-9 percent over respective\nbaselines while utilizing fewer frames.", "AI": {"tldr": "Video-EM\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86Video-LLMs\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u5e27\u5197\u4f59\u548c\u65f6\u7a7a\u5173\u7cfb\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u957f\u89c6\u9891\u7b80\u5316\u4e3a\u9759\u6001\u5173\u952e\u5e27\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u5173\u7cfb\u548c\u573a\u666f\u8fde\u7eed\u6027\uff0c\u5bfc\u81f4\u4fe1\u606f\u5197\u4f59\u548c\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "Video-EM\u5c06\u5173\u952e\u5e27\u5efa\u6a21\u4e3a\u65f6\u5e8f\u6027\u60c5\u666f\u4e8b\u4ef6\uff0c\u7ed3\u5408\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u5229\u7528LLMs\u7684\u94fe\u5f0f\u601d\u7ef4\u8fed\u4ee3\u7b5b\u9009\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideo-EM\u6027\u80fd\u63d0\u53474-9%\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u5e27\u6570\u3002", "conclusion": "Video-EM\u901a\u8fc7\u60c5\u666f\u8bb0\u5fc6\u548c\u94fe\u5f0f\u601d\u7ef4\u4f18\u5316\u5173\u952e\u5e27\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.09954", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.09954", "abs": "https://arxiv.org/abs/2508.09954", "authors": ["Johannes Sch\u00e4fer", "Roman Klinger"], "title": "Shaping Event Backstories to Estimate Potential Emotion Contexts", "comment": "May 2025 version", "summary": "Emotion analysis is an inherently ambiguous task. Previous work studied\nannotator properties to explain disagreement, but this overlooks the\npossibility that ambiguity may stem from missing information about the context\nof events. In this paper, we propose a novel approach that adds reasonable\ncontexts to event descriptions, which may better explain a particular\nsituation. Our goal is to understand whether these enriched contexts enable\nhuman annotators to annotate emotions more reliably. We disambiguate a target\nevent description by automatically generating multiple event chains conditioned\non differing emotions. By combining techniques from short story generation in\nvarious settings, we achieve coherent narratives that result in a specialized\ndataset for the first comprehensive and systematic examination of\ncontextualized emotion analysis. Through automatic and human evaluation, we\nfind that contextual narratives enhance the interpretation of specific emotions\nand support annotators in producing more consistent annotations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6dfb\u52a0\u5408\u7406\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a\u4e8b\u4ef6\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u60c5\u611f\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u60c5\u611f\u5206\u6790\u5177\u6709\u6a21\u7cca\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u4e0a\u4e0b\u6587\u7f3a\u5931\u53ef\u80fd\u662f\u6a21\u7cca\u6027\u7684\u6839\u6e90\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u4e0d\u540c\u60c5\u611f\u7684\u4e8b\u4ef6\u94fe\uff0c\u7ed3\u5408\u77ed\u6545\u4e8b\u751f\u6210\u6280\u672f\uff0c\u521b\u5efa\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u53d9\u8ff0\u3002", "result": "\u4e0a\u4e0b\u6587\u53d9\u8ff0\u589e\u5f3a\u4e86\u7279\u5b9a\u60c5\u611f\u7684\u89e3\u91ca\uff0c\u5e76\u63d0\u9ad8\u4e86\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u6709\u52a9\u4e8e\u63d0\u5347\u60c5\u611f\u6807\u6ce8\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.09447", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09447", "abs": "https://arxiv.org/abs/2508.09447", "authors": ["Siddharth Srikanth", "John Krumm", "Jonathan Qin"], "title": "NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)", "comment": "Extended version of short paper in 32nd ACM SIGSPATIAL International\n  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL\n  2024)", "summary": "Road traffic congestion is a persistent problem. Focusing resources on the\ncauses of congestion is a potentially efficient strategy for reducing\nslowdowns. We present NEXICA, an algorithm to discover which parts of the\nhighway system tend to cause slowdowns on other parts of the highway. We use\ntime series of road speeds as inputs to our causal discovery algorithm. Finding\nother algorithms inadequate, we develop a new approach that is novel in three\nways. First, it concentrates on just the presence or absence of events in the\ntime series, where an event indicates the temporal beginning of a traffic\nslowdown. Second, we develop a probabilistic model using maximum likelihood\nestimation to compute the probabilities of spontaneous and caused slowdowns\nbetween two locations on the highway. Third, we train a binary classifier to\nidentify pairs of cause/effect locations trained on pairs of road locations\nwhere we are reasonably certain a priori of their causal connections, both\npositive and negative. We test our approach on six months of road speed data\nfrom 195 different highway speed sensors in the Los Angeles area, showing that\nour approach is superior to state-of-the-art baselines in both accuracy and\ncomputation speed.", "AI": {"tldr": "NEXICA\u7b97\u6cd5\u901a\u8fc7\u5206\u6790\u9ad8\u901f\u516c\u8def\u901f\u5ea6\u6570\u636e\uff0c\u8bc6\u522b\u5bfc\u81f4\u4ea4\u901a\u62e5\u5835\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4ea4\u901a\u62e5\u5835\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u7126\u62e5\u5835\u539f\u56e0\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "method": "\u5f00\u53d1\u65b0\u7b97\u6cd5\uff0c\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u4e8b\u4ef6\u3001\u6982\u7387\u6a21\u578b\u548c\u4e8c\u5143\u5206\u7c7b\u5668\u8bc6\u522b\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728\u6d1b\u6749\u77f6\u5730\u533a195\u4e2a\u4f20\u611f\u5668\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "NEXICA\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u8bc6\u522b\u62e5\u5835\u539f\u56e0\u3002"}}
{"id": "2508.09489", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09489", "abs": "https://arxiv.org/abs/2508.09489", "authors": ["Hao Yu", "Xin Yang", "Boyang Fan", "Xuemei Cao", "Hanlin Gu", "Lixin Fan", "Qiang Yang"], "title": "Large-Small Model Collaborative Framework for Federated Continual Learning", "comment": null, "summary": "Continual learning (CL) for Foundation Models (FMs) is an essential yet\nunderexplored challenge, especially in Federated Continual Learning (FCL),\nwhere each client learns from a private, evolving task stream under strict data\nand communication constraints. Despite their powerful generalization abilities,\nFMs often exhibit suboptimal performance on local downstream tasks, as they are\nunable to utilize private local data. Furthermore, enabling FMs to learn new\ntasks without forgetting prior knowledge is inherently a challenging problem,\nprimarily due to their immense parameter count and high model complexity. In\ncontrast, small models can be trained locally under resource-constrained\nconditions and benefit from more mature CL techniques. To bridge the gap\nbetween small models and FMs, we propose the first collaborative framework in\nFCL, where lightweight local models act as a dynamic bridge, continually\nadapting to new tasks while enhancing the utility of the large model. Two novel\ncomponents are also included: Small Model Continual Fine-tuning is for\npreventing small models from temporal forgetting; One-by-One Distillation\nperforms personalized fusion of heterogeneous local knowledge on the server.\nExperimental results demonstrate its superior performance, even when clients\nutilize heterogeneous small models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8054\u90a6\u6301\u7eed\u5b66\u4e60\uff08FCL\uff09\u4e2d\u7ed3\u5408\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5728\u672c\u5730\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\u548c\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u672c\u5730\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u96be\u4ee5\u907f\u514d\u9057\u5fd8\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u66f4\u6613\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\u52a8\u6001\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5e76\u7ed3\u5408\u5c0f\u578b\u6a21\u578b\u6301\u7eed\u5fae\u8c03\u548c\u4e00\u5bf9\u4e00\u84b8\u998f\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u5f02\u6784\u672c\u5730\u6a21\u578b\u4e0b\u4ecd\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2508.09585", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09585", "abs": "https://arxiv.org/abs/2508.09585", "authors": ["Stefan Haag", "Bharanidhar Duraisamy", "Felix Govaers", "Wolfgang Koch", "Martin Fritzsche", "Juergen Dickmann"], "title": "Offline Auto Labeling: BAAS", "comment": null, "summary": "This paper introduces BAAS, a new Extended Object Tracking (EOT) and\nfusion-based label annotation framework for radar detections in autonomous\ndriving. Our framework utilizes Bayesian-based tracking, smoothing and\neventually fusion methods to provide veritable and precise object trajectories\nalong with shape estimation to provide annotation labels on the detection level\nunder various supervision levels. Simultaneously, the framework provides\nevaluation of tracking performance and label annotation. If manually labeled\ndata is available, each processing module can be analyzed independently or\ncombined with other modules to enable closed-loop continuous improvements. The\nframework performance is evaluated in a challenging urban real-world scenario\nin terms of tracking performance and the label annotation errors. We\ndemonstrate the functionality of the proposed approach for varying dynamic\nobjects and class types", "AI": {"tldr": "BAAS\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u8ddf\u8e2a\u548c\u878d\u5408\u7684\u96f7\u8fbe\u68c0\u6d4b\u6807\u6ce8\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6269\u5c55\u76ee\u6807\u8ddf\u8e2a\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u8f68\u8ff9\u548c\u5f62\u72b6\u4f30\u8ba1\uff0c\u5e76\u652f\u6301\u591a\u7ea7\u76d1\u7763\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96f7\u8fbe\u68c0\u6d4b\u7684\u7cbe\u786e\u6807\u6ce8\u548c\u8ddf\u8e2a\u95ee\u9898\uff0c\u652f\u6301\u95ed\u73af\u6301\u7eed\u6539\u8fdb\u3002", "method": "\u5229\u7528\u8d1d\u53f6\u65af\u8ddf\u8e2a\u3001\u5e73\u6ed1\u548c\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u7ea7\u76d1\u7763\u6807\u6ce8\uff0c\u72ec\u7acb\u6216\u8054\u5408\u5206\u6790\u5404\u6a21\u5757\u3002", "result": "\u5728\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8ddf\u8e2a\u6027\u80fd\u548c\u6807\u6ce8\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u52a8\u6001\u76ee\u6807\u548c\u7c7b\u522b\u3002", "conclusion": "BAAS\u6846\u67b6\u5728\u96f7\u8fbe\u68c0\u6d4b\u6807\u6ce8\u548c\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u95ed\u73af\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2508.09527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09527", "abs": "https://arxiv.org/abs/2508.09527", "authors": ["Fang Wang", "Ernesto Damiani"], "title": "Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring", "comment": "32 pages", "summary": "Predictive Business Process Monitoring (PBPM) aims to forecast future events\nin ongoing cases based on historical event logs. While Graph Neural Networks\n(GNNs) are well suited to capture structural dependencies in process data,\nexisting GNN-based PBPM models remain underdeveloped. Most rely either on short\nprefix subgraphs or global architectures that overlook temporal relevance and\ntransition semantics. We propose a unified, interpretable GNN framework that\nadvances the state of the art along three key axes. First, we compare\nprefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention\nNetworks(GATs) to quantify the performance gap between localized and global\nmodeling. Second, we introduce a novel time decay attention mechanism that\nconstructs dynamic, prediction-centered windows, emphasizing temporally\nrelevant history and suppressing noise. Third, we embed transition type\nsemantics into edge features to enable fine grained reasoning over structurally\nambiguous traces. Our architecture includes multilevel interpretability\nmodules, offering diverse visualizations of attention behavior. Evaluated on\nfive benchmarks, the proposed models achieve competitive Top-k accuracy and DL\nscores without per-dataset tuning. By addressing architectural, temporal, and\nsemantic gaps, this work presents a robust, generalizable, and explainable\nsolution for next event prediction in PBPM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u53ef\u89e3\u91ca\u7684GNN\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\u4e2d\u7684\u672a\u6765\u4e8b\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GNN\u6a21\u578b\u5728\u65f6\u95f4\u76f8\u5173\u6027\u548c\u8f6c\u79fb\u8bed\u4e49\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709GNN\u6a21\u578b\u5728\u9884\u6d4b\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u65f6\u95f4\u76f8\u5173\u6027\u548c\u8f6c\u79fb\u8bed\u4e49\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u524d\u7f00GCN\u548c\u5168\u5c40GAT\uff0c\u5f15\u5165\u65f6\u95f4\u8870\u51cf\u6ce8\u610f\u529b\u673a\u5236\u548c\u8f6c\u79fb\u7c7b\u578b\u8bed\u4e49\u5d4c\u5165\uff0c\u6784\u5efa\u52a8\u6001\u9884\u6d4b\u7a97\u53e3\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u9488\u5bf9\u6570\u636e\u96c6\u8c03\u6574\u5373\u53ef\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aPBPM\u4e2d\u7684\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u901a\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09544", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09544", "abs": "https://arxiv.org/abs/2508.09544", "authors": ["Sasan Tavakkol", "Lin Chen", "Max Springer", "Abigail Schantz", "Bla\u017e Bratani\u010d", "Vincent Cohen-Addad", "MohammadHossein Bateni"], "title": "SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification", "comment": null, "summary": "Scarcity of labeled data, especially for rare events, hinders training\neffective machine learning models. This paper proposes SYNAPSE-G (Synthetic\nAugmentation for Positive Sampling via Expansion on Graphs), a novel pipeline\nleveraging Large Language Models (LLMs) to generate synthetic training data for\nrare event classification, addressing the cold-start problem. This synthetic\ndata serve as seeds for semi-supervised label propagation on a similarity graph\nconstructed between the seeds and a large unlabeled dataset. This identifies\ncandidate positive examples, subsequently labeled by an oracle (human or LLM).\nThe expanded dataset then trains/fine-tunes a classifier. We theoretically\nanalyze how the quality (validity and diversity) of the synthetic data impacts\nthe precision and recall of our method. Experiments on the imbalanced SST2 and\nMHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,\noutperforming baselines including nearest neighbor search.", "AI": {"tldr": "SYNAPSE-G\u5229\u7528LLMs\u751f\u6210\u5408\u6210\u6570\u636e\u89e3\u51b3\u7f55\u89c1\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u6807\u7b7e\u4f20\u64ad\u6269\u5c55\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u7f55\u89c1\u4e8b\u4ef6\uff0c\u963b\u788d\u4e86\u6709\u6548\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u63d0\u51faSYNAPSE-G\uff0c\u5229\u7528LLMs\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u6807\u7b7e\u4f20\u64ad\u6269\u5c55\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u4eba\u5de5\u6216LLM\u6807\u6ce8\u3002", "result": "\u5728SST2\u548cMHS\u6570\u636e\u96c6\u4e0a\uff0cSYNAPSE-G\u5728\u53d1\u73b0\u6b63\u6807\u7b7e\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SYNAPSE-G\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u6807\u7b7e\u4f20\u64ad\u6709\u6548\u89e3\u51b3\u4e86\u7f55\u89c1\u4e8b\u4ef6\u5206\u7c7b\u95ee\u9898\u3002"}}
{"id": "2508.09650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09650", "abs": "https://arxiv.org/abs/2508.09650", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazely", "Sunil Aryal"], "title": "TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos", "comment": "8 pages, 6 figures,", "summary": "Robust ball tracking under occlusion remains a key challenge in sports video\nanalysis, affecting tasks like event detection and officiating. We present\nTOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,\nvisibility-weighted loss, and occlusion augmentation to improve performance\nunder partial and full occlusions. Developed in collaboration with Paralympics\nAustralia, TOTNet is designed for real-world sports analytics. We introduce\nTTA, a new occlusion-rich table tennis dataset collected from\nprofessional-level Paralympic matches, comprising 9,159 samples with 1,996\nocclusion cases. Evaluated on four datasets across tennis, badminton, and table\ntennis, TOTNet significantly outperforms prior state-of-the-art methods,\nreducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded\nframes from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for\noffline sports analytics in fast-paced scenarios. Code and data\naccess:\\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.", "AI": {"tldr": "TOTNet\u662f\u4e00\u79cd\u7528\u4e8e\u4f53\u80b2\u89c6\u9891\u5206\u6790\u4e2d\u906e\u6321\u60c5\u51b5\u4e0b\u7403\u4f53\u8ddf\u8e2a\u7684\u65f6\u5e8f\u906e\u6321\u8ddf\u8e2a\u7f51\u7edc\uff0c\u901a\u8fc73D\u5377\u79ef\u3001\u53ef\u89c1\u6027\u52a0\u6743\u635f\u5931\u548c\u906e\u6321\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f53\u80b2\u89c6\u9891\u5206\u6790\u4e2d\uff0c\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u7403\u4f53\u8ddf\u8e2a\u662f\u5173\u952e\u6311\u6218\uff0c\u5f71\u54cd\u4e8b\u4ef6\u68c0\u6d4b\u548c\u88c1\u5224\u51b3\u7b56\u3002", "method": "TOTNet\u91c7\u75283D\u5377\u79ef\u3001\u53ef\u89c1\u6027\u52a0\u6743\u635f\u5931\u548c\u906e\u6321\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u906e\u6321\u4e30\u5bcc\u7684\u4e52\u4e53\u7403\u6570\u636e\u96c6TTA\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cTOTNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cRMSE\u4ece37.30\u964d\u81f37.19\uff0c\u5168\u906e\u6321\u5e27\u51c6\u786e\u7387\u4ece0.63\u63d0\u5347\u81f30.80\u3002", "conclusion": "TOTNet\u5728\u5feb\u901f\u573a\u666f\u4e0b\u7684\u79bb\u7ebf\u4f53\u80b2\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.09693", "categories": ["cs.LG", "math.FA", "math.OC", "stat.ML", "47H09, 47H10, 90C25, 65K10, 68T07", "F.1.1; G.1.6; I.2.6; G.1.2"], "pdf": "https://arxiv.org/pdf/2508.09693", "abs": "https://arxiv.org/abs/2508.09693", "authors": ["Faruk Alpay", "Bugra Kilictas", "Hamdi Alakkad"], "title": "Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture", "comment": "16 pages, 2 figures, 2 tables", "summary": "We develop an operator-theoretic framework for temporal anchoring in\nembedding spaces, modeled as drift maps interleaved with event-indexed blocks\nculminating in affine projections. We provide complete proofs for a\nvariable-block contraction lemma (products of Lipschitz factors), a\ndrift--projection convergence theorem with explicit uniform-gap envelopes, and\nontological convergence under nested affine anchors with a robustness variant.\nWe formalize an internal Manuscript Computer (MC) whose computations are\ndefined purely by these operators and prove a rigorous finite-run equivalence\ntheorem (with perturbation bounds). For attention layers, we give a\nself-contained proof that softmax is $1/2$-Lipschitz in $\\ell_2$ and derive\nsufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All\nfloats are placed exactly where written; the manuscript uses only in-paper\npseudocode and appendix figures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b97\u5b50\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u65f6\u95f4\u951a\u5b9a\uff0c\u901a\u8fc7\u6f02\u79fb\u6620\u5c04\u548c\u4e8b\u4ef6\u7d22\u5f15\u5757\u7684\u4ea4\u66ff\u5b9e\u73b0\uff0c\u6700\u7ec8\u901a\u8fc7\u4eff\u5c04\u6295\u5f71\u5b8c\u6210\u3002", "motivation": "\u4e3a\u65f6\u95f4\u951a\u5b9a\u95ee\u9898\u63d0\u4f9b\u4e00\u4e2a\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u8ba1\u7b97\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u6f02\u79fb\u6620\u5c04\u4e0e\u4e8b\u4ef6\u7d22\u5f15\u5757\u4ea4\u66ff\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4eff\u5c04\u6295\u5f71\uff0c\u5e76\u8bc1\u660e\u4e86\u76f8\u5173\u7684\u6536\u655b\u5b9a\u7406\u548c\u9c81\u68d2\u6027\u53d8\u4f53\u3002", "result": "\u8bc1\u660e\u4e86\u8f6f\u6ce8\u610f\u529b\u5c42\u7684Lipschitz\u6027\u8d28\uff0c\u5e76\u63a8\u5bfc\u4e86\u5c42\u6536\u7f29\u7684\u5145\u5206\u6761\u4ef6\uff08\u6b63\u4ea4/\u975e\u6b63\u4ea4\u5934\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7406\u8bba\u548c\u8ba1\u7b97\u4e0a\u5747\u5177\u6709\u4e25\u683c\u6027\uff0c\u9002\u7528\u4e8e\u6ce8\u610f\u529b\u5c42\u7b49\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2508.09747", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09747", "abs": "https://arxiv.org/abs/2508.09747", "authors": ["Nazira Dunbayeva", "Yulong Li", "Yutong Xie", "Imran Razzak"], "title": "A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers", "comment": null, "summary": "Predicting an individual's aging trajectory is a central challenge in\npreventative medicine and bioinformatics. While machine learning models can\npredict chronological age from biomarkers, they often fail to capture the\ndynamic, longitudinal nature of the aging process. In this work, we developed\nand validated a machine learning pipeline to predict age using a longitudinal\ncohort with data from two distinct time periods (2019-2020 and 2021-2022). We\ndemonstrate that a model using only static, cross-sectional biomarkers has\nlimited predictive power when generalizing to future time points. However, by\nengineering novel features that explicitly capture the rate of change (slope)\nof key biomarkers over time, we significantly improved model performance. Our\nfinal LightGBM model, trained on the initial wave of data, successfully\npredicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for\nmales, $R^2 = 0.498$ for females), significantly outperforming both traditional\nlinear models and other tree-based ensembles. SHAP analysis of our successful\nmodel revealed that the engineered slope features were among the most important\npredictors, highlighting that an individual's health trajectory, not just their\nstatic health snapshot, is a key determinant of biological age. Our framework\npaves the way for clinical tools that dynamically track patient health\ntrajectories, enabling early intervention and personalized prevention\nstrategies for age-related diseases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u901a\u8fc7\u7eb5\u5411\u751f\u7269\u6807\u5fd7\u7269\u6570\u636e\u9884\u6d4b\u5e74\u9f84\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u9884\u6d4b\u4e2a\u4f53\u8870\u8001\u8f68\u8ff9\u662f\u9884\u9632\u533b\u5b66\u548c\u751f\u7269\u4fe1\u606f\u5b66\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8870\u8001\u7684\u52a8\u6001\u7eb5\u5411\u7279\u6027\u3002", "method": "\u5229\u75282019-2022\u5e74\u7684\u7eb5\u5411\u6570\u636e\uff0c\u8bbe\u8ba1\u6355\u6349\u751f\u7269\u6807\u5fd7\u7269\u53d8\u5316\u7387\u7684\u65b0\u7279\u5f81\uff0c\u6784\u5efaLightGBM\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u672a\u6765\u65f6\u95f4\u70b9\u5e74\u9f84\u65f6\u8868\u73b0\u4f18\u5f02\uff08\u7537\u6027R\u00b2=0.515\uff0c\u5973\u6027R\u00b2=0.498\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u5065\u5eb7\u8f68\u8ff9\u6bd4\u9759\u6001\u5065\u5eb7\u5feb\u7167\u66f4\u80fd\u9884\u6d4b\u751f\u7269\u5e74\u9f84\uff0c\u4e3a\u4e34\u5e8a\u5de5\u5177\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.09810", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.09810", "abs": "https://arxiv.org/abs/2508.09810", "authors": ["Qi Gan", "Stephan Cl\u00e9men\u00e7on", "Moun\u00eem A. El-Yacoubi", "Sao Mai Nguyen", "Eric Fenaux", "Ons Jelassi"], "title": "Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques", "comment": "15 pages, 6 figures", "summary": "Biomechanical features have become important indicators for evaluating\nathletes' techniques. Traditionally, experts propose significant features and\nevaluate them using physics equations. However, the complexity of the human\nbody and its movements makes it challenging to explicitly analyze the\nrelationships between some features and athletes' final performance. With\nadvancements in modern machine learning and statistics, data analytics methods\nhave gained increasing importance in sports analytics. In this study, we\nleverage machine learning models to analyze expert-proposed biomechanical\nfeatures from the finals of long jump competitions in the World Championships.\nThe objectives of the analysis include identifying the most important features\ncontributing to top-performing jumps and exploring the combined effects of\nthese key features. Using quantile regression, we model the relationship\nbetween the biomechanical feature set and the target variable (effective\ndistance), with a particular focus on elite-level jumps. To interpret the\nmodel, we apply SHapley Additive exPlanations (SHAP) alongside Partial\nDependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The\nfindings reveal that, beyond the well-documented velocity-related features,\nspecific technical aspects also play a pivotal role. For male athletes, the\nangle of the knee of the supporting leg before take-off is identified as a key\nfactor for achieving top 10% performance in our dataset, with angles greater\nthan 169{\\deg}contributing significantly to jump performance. In contrast, for\nfemale athletes, the landing pose and approach step technique emerge as the\nmost critical features influencing top 10% performances, alongside velocity.\nThis study establishes a framework for analyzing the impact of various features\non athletic performance, with a particular emphasis on top-performing events.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790\u4e13\u5bb6\u63d0\u51fa\u7684\u751f\u7269\u529b\u5b66\u7279\u5f81\uff0c\u8bc6\u522b\u5f71\u54cd\u8df3\u8fdc\u6bd4\u8d5b\u9876\u7ea7\u8868\u73b0\u7684\u5173\u952e\u7279\u5f81\uff0c\u5e76\u63a2\u7d22\u5176\u7ec4\u5408\u6548\u5e94\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u660e\u786e\u5206\u6790\u751f\u7269\u529b\u5b66\u7279\u5f81\u4e0e\u8fd0\u52a8\u5458\u8868\u73b0\u7684\u5173\u7cfb\uff0c\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u548c\u7edf\u8ba1\u65b9\u6cd5\u4e3a\u8fd0\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u5206\u4f4d\u6570\u56de\u5f52\u5efa\u6a21\u751f\u7269\u529b\u5b66\u7279\u5f81\u4e0e\u6709\u6548\u8ddd\u79bb\u7684\u5173\u7cfb\uff0c\u7ed3\u5408SHAP\u3001PDP\u548cICE\u56fe\u89e3\u91ca\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u9664\u901f\u5ea6\u76f8\u5173\u7279\u5f81\u5916\uff0c\u7279\u5b9a\u6280\u672f\u7279\u5f81\uff08\u5982\u7537\u6027\u8fd0\u52a8\u5458\u652f\u6491\u817f\u819d\u76d6\u89d2\u5ea6\u3001\u5973\u6027\u8fd0\u52a8\u5458\u7740\u9646\u59ff\u52bf\u548c\u52a9\u8dd1\u6280\u672f\uff09\u5bf9\u9876\u7ea7\u8868\u73b0\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5206\u6790\u8fd0\u52a8\u8868\u73b0\u4e2d\u5404\u79cd\u7279\u5f81\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5c24\u5176\u5173\u6ce8\u9876\u7ea7\u8868\u73b0\u4e8b\u4ef6\u3002"}}
{"id": "2508.09912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09912", "abs": "https://arxiv.org/abs/2508.09912", "authors": ["Chaoran Feng", "Zhenyu Tang", "Wangbo Yu", "Yatian Pang", "Yian Zhao", "Jianbin Zhao", "Li Yuan", "Yonghong Tian"], "title": "E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras", "comment": "16 pages, 10 figures, 5 Tables, accepted by ACMMM 2025", "summary": "Novel view synthesis and 4D reconstruction techniques predominantly rely on\nRGB cameras, thereby inheriting inherent limitations such as the dependence on\nadequate lighting, susceptibility to motion blur, and a limited dynamic range.\nEvent cameras, offering advantages of low power, high temporal resolution and\nhigh dynamic range, have brought a new perspective to addressing the scene\nreconstruction challenges in high-speed motion and low-light scenes. To this\nend, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting\napproach, for novel view synthesis from multi-view event streams with\nfast-moving cameras. Specifically, we introduce an event-based initialization\nscheme to ensure stable training and propose event-adaptive slicing splatting\nfor time-aware reconstruction. Additionally, we employ intensity importance\npruning to eliminate floating artifacts and enhance 3D consistency, while\nincorporating an adaptive contrast threshold for more precise optimization. We\ndesign a synthetic multi-view camera setup with six moving event cameras\nsurrounding the object in a 360-degree configuration and provide a benchmark\nmulti-view event stream dataset that captures challenging motion scenarios. Our\napproach outperforms both event-only and event-RGB fusion baselines and paves\nthe way for the exploration of multi-view event-based reconstruction as a novel\napproach for rapid scene capture.", "AI": {"tldr": "E-4DGS\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRGB\u76f8\u673a\u5728\u5149\u7167\u4e0d\u8db3\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u52a8\u6001\u8303\u56f4\u6709\u9650\u7b49\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRGB\u76f8\u673a\u5728\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u5149\u7167\u4f9d\u8d56\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u52a8\u6001\u8303\u56f4\u9650\u5236\u7b49\u95ee\u9898\uff0c\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u4f4e\u529f\u8017\u3001\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u5bbd\u52a8\u6001\u8303\u56f4\uff0c\u4e3a\u9ad8\u901f\u8fd0\u52a8\u548c\u4f4e\u5149\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff08E-4DGS\uff09\uff0c\u5305\u62ec\u4e8b\u4ef6\u521d\u59cb\u5316\u65b9\u6848\u3001\u4e8b\u4ef6\u81ea\u9002\u5e94\u5207\u7247\u6cfc\u6e85\u3001\u5f3a\u5ea6\u91cd\u8981\u6027\u526a\u679d\u548c\u81ea\u9002\u5e94\u5bf9\u6bd4\u5ea6\u9608\u503c\u4f18\u5316\u3002", "result": "E-4DGS\u5728\u5408\u6210\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7eaf\u4e8b\u4ef6\u548c\u4e8b\u4ef6-RGB\u878d\u5408\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "E-4DGS\u4e3a\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u5feb\u901f\u573a\u666f\u6355\u6349\u7684\u63a2\u7d22\u3002"}}
