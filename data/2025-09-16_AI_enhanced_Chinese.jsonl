{"id": "2509.10692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10692", "abs": "https://arxiv.org/abs/2509.10692", "authors": ["Giuseppe Silano", "Amr Afifi", "Martin Saska", "Antonio Franchi"], "title": "STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle", "comment": "39 pages, 13 figures", "summary": "This paper presents a novel approach to motion planning and risk analysis for\nenhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).\nThe proposed method uses Signal Temporal Logic (STL) to encode key mission\nobjectives, such as safety, timing, and human preferences, with a strong focus\non ergonomics and comfort. An optimization framework generates dynamically\nfeasible trajectories while considering the MRAV's physical constraints. Given\nthe nonlinear and non-convex nature of the problem, smooth approximations and\ngradient-based techniques assist in handling the problem's computational\ncomplexity. Additionally, an uncertainty-aware risk analysis is incorporated to\nassess potential deviations from the mission specifications, providing insights\ninto the likelihood of mission success under uncertain conditions. Further, an\nevent-triggered replanning strategy is implemented to respond to unforeseen\nevents and external disturbances. The approach is validated through MATLAB and\nGazebo simulations, using an object handover task in a mock-up environment\ninspired by power line maintenance scenarios. The results highlight the\nmethod's effectiveness in achieving safe, efficient, and resilient human-robot\ncollaboration.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u4fe1\u53f7\u65f6\u6001\u903b\u8f91\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u751f\u6210\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u98ce\u9669\u5206\u6790\u548c\u4e8b\u4ef6\u89e6\u53d1\u91cd\u65b0\u89c4\u5212\u7b56\u7565\uff0c\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u5f39\u6027\u3002", "motivation": "\u63d0\u9ad8\u591a\u65cb\u7ffc\u822a\u7a7a\u5668\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u5f39\u6027\uff0c\u7279\u522b\u662f\u5728\u8fd0\u7ef4\u62a4\u7b49\u590d\u6742\u573a\u666f\u4e2d\u4fdd\u969c\u4eba\u673a\u4ea4\u4e92\u7684\u5b89\u5168\u4e0e\u8212\u9002\u6027\u3002", "method": "\u4f7f\u7528\u4fe1\u53f7\u65f6\u6001\u903b\u8f91(STL)\u7f16\u7801\u5b89\u5168\u3001\u65f6\u95f4\u548c\u4eba\u7c7b\u504f\u597d\u7b49\u4efb\u52a1\u76ee\u6807\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u751f\u6210\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\uff0c\u91c7\u7528\u5145\u5206\u5e73\u6ed1\u8fd1\u4f3c\u548c\u68af\u5ea6\u57fa\u7840\u6280\u672f\u5904\u7406\u975e\u7ebf\u6027\u975e\u51f8\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u98ce\u9669\u5206\u6790\u548c\u4e8b\u4ef6\u89e6\u53d1\u91cd\u65b0\u89c4\u5212\u7b56\u7565\u3002", "result": "\u901a\u8fc7MATLAB\u548cGazebo\u6a21\u62df\u9a8c\u8bc1\uff0c\u5728\u4eff\u771f\u8fd0\u7ef4\u7ef4\u62a4\u573a\u666f\u4e2d\u8fdb\u884c\u7269\u4f53\u4ea4\u63a5\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u548c\u5177\u6709\u5f39\u6027\u7684\u4eba\u673a\u534f\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u98ce\u9669\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u786e\u4fdd\u4efb\u52a1\u7684\u5b89\u5168\u5b8c\u6210\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u8212\u9002\u6027\u3002"}}
{"id": "2509.10707", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10707", "abs": "https://arxiv.org/abs/2509.10707", "authors": ["Sajjad Abdoli", "Rudi Cilibrasi", "Rima Al-Shikh"], "title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions", "comment": null, "summary": "As AI systems increasingly evaluate other AI outputs, understanding their\nassessment behavior becomes crucial for preventing cascading biases. This study\nanalyzes vision-language descriptions generated by NVIDIA's Describe Anything\nModel and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to\nuncover distinct \"evaluation personalities\" the underlying assessment\nstrategies and biases each model demonstrates. GPT-4o-mini exhibits systematic\nconsistency with minimal variance, GPT-4o excels at error detection, while\nGPT-5 shows extreme conservatism with high variability. Controlled experiments\nusing Gemini 2.5 Pro as an independent question generator validate that these\npersonalities are inherent model properties rather than artifacts. Cross-family\nanalysis through semantic similarity of generated questions reveals significant\ndivergence: GPT models cluster together with high similarity while Gemini\nexhibits markedly different evaluation strategies. All GPT models demonstrate a\nconsistent 2:1 bias favoring negative assessment over positive confirmation,\nthough this pattern appears family-specific rather than universal across AI\narchitectures. These findings suggest that evaluation competence does not scale\nwith general capability and that robust AI assessment requires diverse\narchitectural perspectives.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.10501", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10501", "abs": "https://arxiv.org/abs/2509.10501", "authors": ["Wentao Gao", "Jiuyong Li", "Lin Liu", "Thuc Duy Le", "Xiongren Chen", "Xiaojing Du", "Jixue Liu", "Yanchang Zhao", "Yun Chen"], "title": "From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction", "comment": "ECAI 2025 Accepted", "summary": "Zero-inflated data pose significant challenges in precipitation forecasting\ndue to the predominance of zeros with sparse non-zero events. To address this,\nwe propose the Zero Inflation Diffusion Framework (ZIDF), which integrates\nGaussian perturbation for smoothing zero-inflated distributions,\nTransformer-based prediction for capturing temporal patterns, and\ndiffusion-based denoising to restore the original data structure. In our\nexperiments, we use observational precipitation data collected from South\nAustralia along with synthetically generated zero-inflated data. Results show\nthat ZIDF demonstrates significant performance improvements over multiple\nstate-of-the-art precipitation forecasting models, achieving up to 56.7\\%\nreduction in MSE and 21.1\\% reduction in MAE relative to the baseline\nNon-stationary Transformer. These findings highlight ZIDF's ability to robustly\nhandle sparse time series data and suggest its potential generalizability to\nother domains where zero inflation is a key challenge.", "AI": {"tldr": "\u63d0\u51fa\u4e86Zero Inflation Diffusion Framework (ZIDF)\u6765\u5904\u7406\u964d\u6c34\u9884\u6d4b\u4e2d\u7684\u96f6\u81a8\u80c0\u6570\u636e\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u65af\u6270\u52a8\u3001Transformer\u9884\u6d4b\u548c\u6269\u6563\u53bb\u566a\u6280\u672f\uff0c\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u964d\u6c34\u9884\u6d4b\u4e2d\u96f6\u81a8\u80c0\u6570\u636e\uff08\u5927\u91cf\u96f6\u503c\u548c\u7a00\u758f\u975e\u96f6\u4e8b\u4ef6\uff09\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u7279\u6b8a\u7684\u6570\u636e\u5206\u5e03\u3002", "method": "ZIDF\u6846\u67b6\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u9ad8\u65af\u6270\u52a8\u7528\u4e8e\u5e73\u6ed1\u96f6\u81a8\u80c0\u5206\u5e03\u3001\u57fa\u4e8eTransformer\u7684\u9884\u6d4b\u7528\u4e8e\u6355\u6349\u65f6\u95f4\u6a21\u5f0f\u3001\u57fa\u4e8e\u6269\u6563\u7684\u53bb\u566a\u7528\u4e8e\u6062\u590d\u539f\u59cb\u6570\u636e\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u5357\u6fb3\u5927\u5229\u4e9a\u7684\u89c2\u6d4b\u964d\u6c34\u6570\u636e\u548c\u5408\u6210\u96f6\u81a8\u80c0\u6570\u636e\uff0c\u7ed3\u679c\u663e\u793aZIDF\u76f8\u6bd4\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u964d\u6c34\u9884\u6d4b\u6a21\u578b\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u5bf9\u4e8e\u57fa\u51c6Non-stationary Transformer\uff0cMSE\u964d\u4f4e56.7%\uff0cMAE\u964d\u4f4e21.1%\u3002", "conclusion": "ZIDF\u80fd\u591f\u9c81\u68d2\u5730\u5904\u7406\u7a00\u758f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u663e\u793a\u51fa\u5728\u5176\u4ed6\u96f6\u81a8\u80c0\u95ee\u9898\u9886\u57df\u7684\u6f5c\u5728\u901a\u7528\u6027\u3002"}}
{"id": "2509.10833", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10833", "abs": "https://arxiv.org/abs/2509.10833", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "title": "Towards Automated Error Discovery: A Study in Conversational AI", "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86SEEED\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u68c0\u6d4b\u5bf9\u8bddAI\u4e2d\u7684\u9519\u8bef\uff0c\u901a\u8fc7\u6539\u8fdbSoft Nearest Neighbor Loss\u548c\u6807\u7b7e\u6837\u672c\u6392\u5e8f\u6765\u63d0\u5347\u672a\u77e5\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aGPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u867d\u7136\u6d41\u7545\u8fde\u8d2f\uff0c\u4f46\u4ecd\u4f1a\u4ea7\u751f\u96be\u4ee5\u9884\u9632\u7684\u9519\u8bef\u884c\u4e3a\u3002\u73b0\u6709LLM\u96be\u4ee5\u68c0\u6d4b\u6307\u4ee4\u4e2d\u672a\u660e\u786e\u6307\u5b9a\u7684\u9519\u8bef\uff0c\u7279\u522b\u662f\u5f53\u751f\u6210\u6a21\u578b\u66f4\u65b0\u6216\u7528\u6237\u884c\u4e3a\u53d8\u5316\u65f6\u3002", "method": "\u63d0\u51faAutomated Error Discovery\u6846\u67b6\u548cSEEED\u5b9e\u73b0\u65b9\u6cd5\uff1a\u6539\u8fdbSoft Nearest Neighbor Loss\uff08\u589e\u5f3a\u8d1f\u6837\u672c\u8ddd\u79bb\u6743\u91cd\uff09\uff0c\u5f15\u5165Label-Based Sample Ranking\u9009\u62e9\u9ad8\u5bf9\u6bd4\u5ea6\u6837\u672c\u4ee5\u6539\u5584\u8868\u793a\u5b66\u4e60\u3002", "result": "SEEED\u5728\u591a\u4e2a\u9519\u8bef\u6807\u6ce8\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aGPT-4o\u548cPhi-4\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u672a\u77e5\u9519\u8bef\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe8\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u672a\u77e5\u610f\u56fe\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SEEED\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u8bddAI\u4e2d\u672a\u77e5\u9519\u8bef\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u597d\u7684\u9519\u8bef\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.10922", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.10922", "abs": "https://arxiv.org/abs/2509.10922", "authors": ["Tsuyoshi Iwata", "Guillaume Comte", "Melissa Flores", "Ryoma Kondo", "Ryohei Hisano"], "title": "Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction", "comment": "Author accepted manuscript. This paper has been accepted for\n  presentation at the ISWC 2025 Posters & Demos Track. License details will be\n  updated once the official proceedings are published", "summary": "The growing importance of environmental, social, and governance data in\nregulatory and investment contexts has increased the need for accurate,\ninterpretable, and internationally aligned representations of non-financial\nrisks, particularly those reported in unstructured news sources. However,\naligning such controversy-related data with principle-based normative\nframeworks, such as the United Nations Global Compact or Sustainable\nDevelopment Goals, presents significant challenges. These frameworks are\ntypically expressed in abstract language, lack standardized taxonomies, and\ndiffer from the proprietary classification systems used by commercial data\nproviders. In this paper, we present a semi-automatic method for constructing\nstructured knowledge representations of environmental, social, and governance\nevents reported in the news. Our approach uses lightweight ontology design,\nformal pattern modeling, and large language models to convert normative\nprinciples into reusable templates expressed in the Resource Description\nFramework. These templates are used to extract relevant information from news\ncontent and populate a structured knowledge graph that links reported incidents\nto specific framework principles. The result is a scalable and transparent\nframework for identifying and interpreting non-compliance with international\nsustainability guidelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u672c\u4f53\u8bbe\u8ba1\u3001\u5f62\u5f0f\u5316\u6a21\u5f0f\u5efa\u6a21\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06ESG\u65b0\u95fb\u4e8b\u4ef6\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u89e3\u51b3\u975e\u8d22\u52a1\u98ce\u9669\u6570\u636e\u4e0e\u89c4\u8303\u6027\u6846\u67b6\u5bf9\u9f50\u7684\u6311\u6218\u3002", "motivation": "\u73af\u5883\u3001\u793e\u4f1a\u548c\u6cbb\u7406\u6570\u636e\u5728\u76d1\u7ba1\u548c\u6295\u8d44\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5c06\u65b0\u95fb\u4e2d\u7684\u4e89\u8bae\u76f8\u5173\u6570\u636e\u4e0e\u8054\u5408\u56fd\u5168\u7403\u5951\u7ea6\u7b49\u539f\u5219\u6027\u89c4\u8303\u6846\u67b6\u5bf9\u9f50\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6846\u67b6\u8bed\u8a00\u62bd\u8c61\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u5206\u7c7b\u6cd5\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u672c\u4f53\u8bbe\u8ba1\u3001\u5f62\u5f0f\u5316\u6a21\u5f0f\u5efa\u6a21\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u89c4\u8303\u6027\u539f\u5219\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u7684RDF\u6a21\u677f\uff0c\u4ece\u65b0\u95fb\u5185\u5bb9\u4e2d\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\u5e76\u6784\u5efa\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u89e3\u91ca\u4e0e\u56fd\u9645\u53ef\u6301\u7eed\u53d1\u5c55\u6307\u5357\u7684\u4e0d\u5408\u89c4\u60c5\u51b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3ESG\u4e8b\u4ef6\u6570\u636e\u4e0e\u89c4\u8303\u6027\u6846\u67b6\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u53ef\u6301\u7eed\u53d1\u5c55\u76d1\u7ba1\u548c\u6295\u8d44\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.10528", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10528", "abs": "https://arxiv.org/abs/2509.10528", "authors": ["Amirhossein Ghaffari", "Huong Nguyen", "Lauri Lov\u00e9n", "Ekaterina Gilman"], "title": "STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph Neural Network Predictions", "comment": "Accepted manuscript (CC BY 4.0). To appear in ACM CIKM 2025, Seoul,\n  Nov 10-14, 2025. DOI: 10.1145/3746252.3761645. The Version of Record will be\n  uploaded when available", "summary": "Urban spatio-temporal data present unique challenges for predictive analytics\ndue to their dynamic and complex nature. We introduce STM-Graph, an open-source\nPython framework that transforms raw spatio-temporal urban event data into\ngraph representations suitable for Graph Neural Network (GNN) training and\nprediction. STM-Graph integrates diverse spatial mapping methods, urban\nfeatures from OpenStreetMap, multiple GNN models, comprehensive visualization\ntools, and a graphical user interface (GUI) suitable for professional and\nnon-professional users. This modular and extensible framework facilitates rapid\nexperimentation and benchmarking. It allows integration of new mapping methods\nand custom models, making it a valuable resource for researchers and\npractitioners in urban computing. The source code of the framework and GUI are\navailable at: https://github.com/Ahghaffari/stm_graph and\nhttps://github.com/tuminguyen/stm_graph_gui.", "AI": {"tldr": "STM-Graph\u662f\u4e00\u4e2a\u5f00\u6e90Python\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u57ce\u5e02\u65f6\u7a7a\u4e8b\u4ef6\u6570\u636e\u8f6c\u6362\u4e3a\u9002\u5408GNN\u8bad\u7ec3\u7684\u56fe\u8868\u793a\uff0c\u5305\u542b\u7a7a\u95f4\u6620\u5c04\u65b9\u6cd5\u3001OpenStreetMap\u7279\u5f81\u3001\u591a\u79cdGNN\u6a21\u578b\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002", "motivation": "\u57ce\u5e02\u65f6\u7a7a\u6570\u636e\u5177\u6709\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6846\u67b6\u6765\u652f\u6301\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86STM-Graph\u6846\u67b6\uff0c\u6574\u5408\u4e86\u591a\u79cd\u7a7a\u95f4\u6620\u5c04\u65b9\u6cd5\u3001OpenStreetMap\u57ce\u5e02\u7279\u5f81\u3001\u591a\u4e2aGNN\u6a21\u578b\u3001\u53ef\u89c6\u5316\u5de5\u5177\u548c\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u91c7\u7528\u6a21\u5757\u5316\u53ef\u6269\u5c55\u8bbe\u8ba1\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5141\u8bb8\u96c6\u6210\u65b0\u7684\u6620\u5c04\u65b9\u6cd5\u548c\u81ea\u5b9a\u4e49\u6a21\u578b\uff0c\u4e3a\u57ce\u5e02\u8ba1\u7b97\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "conclusion": "STM-Graph\u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u3001\u6613\u4e8e\u4f7f\u7528\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u57ce\u5e02\u65f6\u7a7a\u6570\u636e\uff0c\u4fc3\u8fdb\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u57ce\u5e02\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002"}}
{"id": "2509.11922", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11922", "abs": "https://arxiv.org/abs/2509.11922", "authors": ["Xilei Dai", "Ruotian Chen", "Songze Guan", "Wen-Tai Li", "Chau Yuen"], "title": "BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning", "comment": null, "summary": "Reinforcement learning (RL) has proven effective for AI-based building energy\nmanagement. However, there is a lack of flexible framework to implement RL\nacross various control problems in building energy management. To address this\ngap, we propose BuildingGym, an open-source tool designed as a\nresearch-friendly and flexible framework for training RL control strategies for\ncommon challenges in building energy management. BuildingGym integrates\nEnergyPlus as its core simulator, making it suitable for both system-level and\nroom-level control. Additionally, BuildingGym is able to accept external\nsignals as control inputs instead of taking the building as a stand-alone\nentity. This feature makes BuildingGym applicable for more flexible\nenvironments, e.g. smart grid and EVs community. The tool provides several\nbuilt-in RL algorithms for control strategy training, simplifying the process\nfor building managers to obtain optimal control strategies. Users can achieve\nthis by following a few straightforward steps to configure BuildingGym for\noptimization control for common problems in the building energy management\nfield. Moreover, AI specialists can easily implement and test state-of-the-art\ncontrol algorithms within the platform. BuildingGym bridges the gap between\nbuilding managers and AI specialists by allowing for the easy configuration and\nreplacement of RL algorithms, simulators, and control environments or problems.\nWith BuildingGym, we efficiently set up training tasks for cooling load\nmanagement, targeting both constant and dynamic cooling load management. The\nbuilt-in algorithms demonstrated strong performance across both tasks,\nhighlighting the effectiveness of BuildingGym in optimizing cooling strategies.", "AI": {"tldr": "BuildingGym\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u96c6\u6210EnergyPlus\u6a21\u62df\u5668\uff0c\u652f\u6301\u7cfb\u7edf\u7ea7\u548c\u623f\u95f4\u7ea7\u63a7\u5236\uff0c\u5e76\u80fd\u63a5\u53d7\u5916\u90e8\u4fe1\u53f7\u8f93\u5165\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7075\u6d3b\u7684\u6846\u67b6\u6765\u5728\u5404\u79cd\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u63a7\u5236\u95ee\u9898\u4e2d\u5b9e\u65bd\u5f3a\u5316\u5b66\u4e60\uff0c\u9700\u8981\u4e00\u79cd\u7814\u7a76\u53cb\u597d\u4e14\u901a\u7528\u7684\u5de5\u5177\u3002", "method": "\u5f00\u53d1BuildingGym\u5f00\u6e90\u5de5\u5177\uff0c\u96c6\u6210EnergyPlus\u4f5c\u4e3a\u6838\u5fc3\u6a21\u62df\u5668\uff0c\u63d0\u4f9b\u5185\u7f6eRL\u7b97\u6cd5\uff0c\u652f\u6301\u5916\u90e8\u4fe1\u53f7\u8f93\u5165\uff0c\u5141\u8bb8\u8f7b\u677e\u914d\u7f6e\u548c\u66ff\u6362\u7b97\u6cd5\u3001\u6a21\u62df\u5668\u548c\u63a7\u5236\u73af\u5883\u3002", "result": "\u5728\u51b7\u5374\u8d1f\u8377\u7ba1\u7406\u4efb\u52a1\u4e2d\uff0c\u5185\u7f6e\u7b97\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86BuildingGym\u5728\u4f18\u5316\u51b7\u5374\u7b56\u7565\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "BuildingGym\u586b\u8865\u4e86\u5efa\u7b51\u7ba1\u7406\u4eba\u5458\u4e0eAI\u4e13\u5bb6\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684RL\u63a7\u5236\u7b56\u7565\u8bad\u7ec3\u5e73\u53f0\u3002"}}
{"id": "2509.11688", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.11688", "abs": "https://arxiv.org/abs/2509.11688", "authors": ["Mostafa Eslami", "Maryam Babazadeh"], "title": "Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems", "comment": null, "summary": "The control of robotic systems in complex, shared collaborative workspaces\npresents significant challenges in achieving robust performance and safety when\nlearning from experienced or simulated data is employed in the pipeline. A\nprimary bottleneck is the reliance on coordinate-dependent models, which leads\nto profound data inefficiency by failing to generalize physical interactions\nacross different frames of reference. This forces learning algorithms to\nrediscover fundamental physical principles in every new orientation,\nartificially inflating the complexity of the learning task. This paper\nintroduces a novel framework that synergizes a coordinate-free, unreduced\nmultibody dynamics and kinematics model based on tensor mechanics with a\nData-Assisted Control (DAC) architecture. A non-recursive, closed-form\nNewton-Euler model in an augmented matrix form is derived that is optimized for\ntensor-based control design. This structure enables a principled decomposition\nof the system into a structurally certain, physically grounded part and an\nuncertain, empirical, and interaction-focused part, mediated by a virtual port\nvariable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,\ncontrol, and learning is proposed. The coordinate-free control laws for the\nstructurally certain part provide a stable and abstract command interface,\nproven via Lyapunov analysis. Eventually, the model and closed-loop system are\nvalidated through simulations. This work provides a naturally ideal input for\ndata-efficient, frame-invariant learning algorithms, such as equivariant\nlearning, designed to learn the uncertain interaction. The synergy directly\naddresses the data-inefficiency problem, increases explainability and\ninterpretability, and paves the way for more robust and generalizable robotic\ncontrol in interactive environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u529b\u5b66\u7684\u5750\u6807\u65e0\u5173\u591a\u4f53\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u8f85\u52a9\u63a7\u5236\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u534f\u4f5c\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5750\u6807\u4f9d\u8d56\u6a21\u578b\u65e0\u6cd5\u5728\u4e0d\u540c\u53c2\u8003\u7cfb\u95f4\u6cdb\u5316\u7269\u7406\u4ea4\u4e92\uff0c\u5bfc\u81f4\u5b66\u4e60\u7b97\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u65b9\u5411\u91cd\u65b0\u53d1\u73b0\u7269\u7406\u539f\u7406\uff0c\u9020\u6210\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u548c\u590d\u6742\u6027\u589e\u52a0\u3002", "method": "\u5f00\u53d1\u4e86\u975e\u9012\u5f52\u95ed\u5f0f\u725b\u987f-\u6b27\u62c9\u6a21\u578b\u7684\u589e\u5f3a\u77e9\u9635\u5f62\u5f0f\uff0c\u91c7\u7528\u5f20\u91cf\u4e0d\u53d8\u6027\u5efa\u6a21\u548c\u63a7\u5236\u7ba1\u9053\uff0c\u5c06\u7cfb\u7edf\u5206\u89e3\u4e3a\u7ed3\u6784\u786e\u5b9a\u90e8\u5206\u548c\u4e0d\u786e\u5b9a\u4ea4\u4e92\u90e8\u5206\uff0c\u901a\u8fc7\u865a\u62df\u7aef\u53e3\u53d8\u91cf\u8fde\u63a5\u3002", "result": "\u901a\u8fc7\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u8bc1\u660e\u4e86\u7a33\u5b9a\u6027\u548c\u62bd\u8c61\u547d\u4ee4\u63a5\u53e3\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6a21\u578b\u548c\u95ed\u73af\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u5e27\u4e0d\u53d8\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u60f3\u8f93\u5165\uff0c\u76f4\u63a5\u89e3\u51b3\u4e86\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4ea4\u4e92\u73af\u5883\u4e2d\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u63a7\u5236\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.11940", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11940", "abs": "https://arxiv.org/abs/2509.11940", "authors": ["Marcel van Gerven"], "title": "Neuromorphic Intelligence", "comment": "18 pages, 3 figures", "summary": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which depend on massive computational and\nenergy resources, neuromorphic systems exploit brain-inspired principles of\ncomputation to achieve orders of magnitude greater energy efficiency. By\ndrawing on insights from artificial intelligence, neuroscience, physics,\nchemistry, and materials science, neuromorphic computing promises to deliver\nintelligent systems that are sustainable, transparent, and widely accessible. A\ncentral challenge, however, is to identify a unifying theoretical framework\ncapable of bridging these diverse disciplines. We argue that dynamical systems\ntheory provides such a foundation. Rooted in differential calculus, it offers a\nprincipled language for modeling inference, learning, and control in both\nnatural and artificial substrates. Within this framework, noise can be\nharnessed as a resource for learning, while differential genetic programming\nenables the discovery of dynamical systems that implement adaptive behaviors.\nEmbracing this perspective paves the way toward emergent neuromorphic\nintelligence, where intelligent behavior arises from the dynamics of physical\nsubstrates, advancing both the science and sustainability of AI.", "AI": {"tldr": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u901a\u8fc7\u52a8\u529b\u5b66\u7cfb\u7edf\u7406\u8bba\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\uff0c\u5229\u7528\u8111\u542f\u53d1\u539f\u7406\u5b9e\u73b0\u9ad8\u80fd\u6548\u667a\u80fd\u7cfb\u7edf\uff0c\u5c06\u566a\u58f0\u4f5c\u4e3a\u5b66\u4e60\u8d44\u6e90\uff0c\u63a8\u52a8\u53ef\u6301\u7eedAI\u53d1\u5c55", "motivation": "\u5bfb\u6c42\u590d\u5236\u4eba\u8111\u9ad8\u6548\u80fd\u3001\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u89e3\u51b3\u4f20\u7edf\u6570\u5b57\u65b9\u6cd5\u8ba1\u7b97\u548c\u80fd\u6e90\u6d88\u8017\u5de8\u5927\u7684\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u8de8\u5b66\u79d1\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6", "method": "\u91c7\u7528\u52a8\u529b\u5b66\u7cfb\u7edf\u7406\u8bba\u4f5c\u4e3a\u7406\u8bba\u57fa\u7840\uff0c\u5229\u7528\u5fae\u5206\u8ba1\u7b97\u5efa\u6a21\u63a8\u7406\u3001\u5b66\u4e60\u548c\u63a7\u5236\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5fae\u5206\u9057\u4f20\u7f16\u7a0b\u53d1\u73b0\u5b9e\u73b0\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u52a8\u529b\u5b66\u7cfb\u7edf", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a8\u529b\u5b66\u7cfb\u7edf\u7406\u8bba\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u566a\u58f0\u8f6c\u5316\u4e3a\u5b66\u4e60\u8d44\u6e90\uff0c\u5728\u7269\u7406\u57fa\u5e95\u4e2d\u5b9e\u73b0\u667a\u80fd\u884c\u4e3a\u7684\u6d8c\u73b0", "conclusion": "\u52a8\u529b\u5b66\u7cfb\u7edf\u7406\u8bba\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8de8\u5b66\u79d1\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u53ef\u6301\u7eedAI\u7684\u53d1\u5c55\uff0c\u4f7f\u667a\u80fd\u884c\u4e3a\u80fd\u591f\u4ece\u7269\u7406\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u4e2d\u81ea\u7136\u6d8c\u73b0"}}
{"id": "2509.11740", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.11740", "abs": "https://arxiv.org/abs/2509.11740", "authors": ["Davide Peron", "Victor Nan Fernandez-Ayala", "Lukas Segelmark"], "title": "From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting", "comment": "Submitted for publication at IEEE ICRA 2026", "summary": "Autonomous stocking in retail environments, particularly supermarkets,\npresents challenges due to dynamic human interactions, constrained spaces, and\ndiverse product geometries. This paper introduces an efficient end-to-end\nrobotic system for autonomous shelf stocking and fronting, integrating\ncommercially available hardware with a scalable algorithmic architecture. A\nmajor contribution of this work is the system integration of off-the-shelf\nhardware and ROS2-based perception, planning, and control into a single\ndeployable platform for retail environments. Our solution leverages Behavior\nTrees (BTs) for task planning, fine-tuned vision models for object detection,\nand a two-step Model Predictive Control (MPC) framework for precise shelf\nnavigation using ArUco markers. Laboratory experiments replicating realistic\nsupermarket conditions demonstrate reliable performance, achieving over 98%\nsuccess in pick-and-place operations across a total of more than 700 stocking\nevents. However, our comparative benchmarks indicate that the performance and\ncost-effectiveness of current autonomous systems remain inferior to that of\nhuman workers, which we use to highlight key improvement areas and quantify the\nprogress still required before widespread commercial deployment can\nrealistically be achieved.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5546\u4e1a\u786c\u4ef6\u7684\u7aef\u5230\u7aef\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u8d85\u5e02\u81ea\u4e3b\u8d27\u67b6\u8865\u8d27\u548c\u524d\u7f6e\u64cd\u4f5c\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u4ecd\u4e0d\u5982\u4eba\u5de5\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u8d85\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8865\u8d27\u6311\u6218\uff0c\u5305\u62ec\u52a8\u6001\u4eba\u7c7b\u4ea4\u4e92\u3001\u7a7a\u95f4\u7ea6\u675f\u548c\u591a\u6837\u5316\u5546\u54c1\u5f62\u72b6\u7b49\u95ee\u9898\u3002", "method": "\u96c6\u6210\u5546\u4e1a\u786c\u4ef6\u548cROS2\u57fa\u7840\u7684\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u91c7\u7528\u884c\u4e3a\u6811\u505a\u4efb\u52a1\u89c4\u5212\uff0c\u7cbe\u8c03\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0c\u4ee5\u53ca\u4e24\u6b65\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u5b9e\u73b0\u7cbe\u786e\u8d27\u67b6\u5bfc\u822a\u3002", "result": "\u5728\u6a21\u62df\u8d85\u5e02\u73af\u5883\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u5728700\u591a\u6b21\u8865\u8d27\u64cd\u4f5c\u4e2d\u8fbe\u5230\u4e86\u8d85\u8fc798%\u7684\u6210\u529f\u7387\uff0c\u4f46\u7efc\u5408\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u4ecd\u4f4e\u4e8e\u4eba\u5de5\u6c34\u5e73\u3002", "conclusion": "\u867d\u7136\u7cfb\u7edf\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5f53\u524d\u81ea\u4e3b\u8865\u8d27\u7cfb\u7edf\u5728\u6027\u80fd\u548c\u6210\u672c\u65b9\u9762\u4ecd\u4e0d\u53ca\u4eba\u7c7b\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u624d\u80fd\u5b9e\u73b0\u5546\u4e1a\u5316\u90e8\u7f72\u3002"}}
{"id": "2509.11943", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11943", "abs": "https://arxiv.org/abs/2509.11943", "authors": ["Antonin Sulc", "Thorsten Hellert"], "title": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics", "comment": "10 pages, 1 figure, Scaling Environments for Agents (SEA) Workshop at\n  NeuralIPS", "summary": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4f7f\u7528Kripke\u6a21\u578b\u5f62\u5f0f\u5316\u8868\u793a\u667a\u80fd\u4f53\u4fe1\u5ff5\u72b6\u6001\uff0c\u7ed3\u5408\u6a21\u6001\u903b\u8f91\u8fdb\u884c\u53ef\u80fd\u6027\u4e0e\u5fc5\u7136\u6027\u63a8\u7406\uff0c\u5728\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\u6210\u529f\u8bca\u65ad\u590d\u6742\u7ea7\u8054\u6545\u969c", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u7684\u6269\u5c55\uff0c\u4f46\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u63a8\u7406\u7ed3\u6784\u3001\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7684\u6269\u5c55\u9700\u6c42\uff0c\u8fd9\u662fAI\u7814\u7a76\u4e2d\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7ef4\u5ea6", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u4e2a\u4f53\u667a\u80fd\u4f53\u4fe1\u5ff5\u72b6\u6001\u8868\u793a\u4e3aKripke\u6a21\u578b\uff0c\u4f7f\u7528\u6a21\u6001\u903b\u8f91\u5f62\u5f0f\u8bed\u8a00\u8fdb\u884c\u53ef\u80fd\u6027\u4e0e\u5fc5\u7136\u6027\u63a8\u7406\uff0c\u5229\u7528\u4e0d\u53ef\u53d8\u7684\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u4f5c\u4e3a\u903b\u8f91\u7ea6\u675f\u6765\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u7684\u5047\u8bbe\u751f\u6210", "result": "\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u7c92\u5b50\u52a0\u901f\u5668\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u8bca\u65ad\u4e86\u590d\u6742\u7684\u7ea7\u8054\u6545\u969c\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u4e49\u76f4\u89c9\u4e0e\u6a21\u6001\u903b\u8f91\u7684\u4e25\u683c\u53ef\u9a8c\u8bc1\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u9760\u548c\u53ef\u9a8c\u8bc1\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u53ef\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u903b\u8f91\u7ea6\u675f\u6709\u6548\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u5f97\u51fa\u7269\u7406\u6216\u903b\u8f91\u4e0a\u4e0d\u53ef\u884c\u7684\u7ed3\u8bba"}}
{"id": "2509.11232", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11232", "abs": "https://arxiv.org/abs/2509.11232", "authors": ["Seongwan Park", "Jieun Woo", "Siheon Yang"], "title": "MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction", "comment": "ICTC 2025", "summary": "This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with\nan LSTM sequence model for sleep quality and stress prediction at the day level\nfrom multimodal lifelog data. Continuous sensor streams are first partitioned\ninto N-hour blocks and rendered as multi-channel images, while sparse discrete\nevents are encoded with a dedicated 1D-CNN. A Convolutional Block Attention\nModule fuses the two modalities into refined block embeddings, which an LSTM\nthen aggregates to capture long-range temporal dependencies. To further boost\nrobustness, we introduce UALRE, an uncertainty-aware ensemble that overrides\nlowconfidence majority votes with high-confidence individual predictions.\nExperiments on the 2025 ETRI Lifelog Challenge dataset show that Our base\nMISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to\n0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm\n(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the\nbenefit of a 4-hour block granularity, and (iii) the efficacy of\nmodality-specific discrete encoding.", "AI": {"tldr": "MIS-LSTM\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408CNN\u7f16\u7801\u5668\u548cLSTM\u5e8f\u5217\u6a21\u578b\uff0c\u4ece\u591a\u6a21\u6001\u751f\u6d3b\u65e5\u5fd7\u6570\u636e\u9884\u6d4b\u7761\u7720\u8d28\u91cf\u548c\u538b\u529b\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u8fde\u7eed\u4f20\u611f\u5668\u548c\u79bb\u6563\u4e8b\u4ef6\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u96c6\u6210\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5728ETRI\u6570\u636e\u96c6\u4e0a\u53d6\u5f970.647\u7684Macro-F1\u5206\u6570\u3002", "motivation": "\u9700\u8981\u4ece\u591a\u6a21\u6001\u751f\u6d3b\u65e5\u5fd7\u6570\u636e\u4e2d\u51c6\u786e\u9884\u6d4b\u7761\u7720\u8d28\u91cf\u548c\u538b\u529b\u6c34\u5e73\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u4f20\u611f\u5668\u6d41\u548c\u7a00\u758f\u79bb\u6563\u4e8b\u4ef6\u7684\u878d\u5408\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u5c06\u8fde\u7eed\u4f20\u611f\u5668\u6d41\u5206\u5272\u4e3aN\u5c0f\u65f6\u5757\u5e76\u6e32\u67d3\u4e3a\u591a\u901a\u9053\u56fe\u50cf\uff0c\u7a00\u758f\u79bb\u6563\u4e8b\u4ef6\u75281D-CNN\u7f16\u7801\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u4e24\u79cd\u6a21\u6001\uff0cLSTM\u6355\u83b7\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\uff0c\u5e76\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u96c6\u6210(UALRE)\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u57fa\u7840MIS-LSTM\u8fbe\u5230Macro-F1 0.615\uff0c\u52a0\u5165UALRE\u96c6\u6210\u540e\u63d0\u5347\u81f30.647\uff0c\u4f18\u4e8eLSTM\u30011D-CNN\u548cCNN\u57fa\u7ebf\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u591a\u901a\u9053\u6210\u50cf\u30014\u5c0f\u65f6\u5757\u7c92\u5ea6\u548c\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u7684\u6709\u6548\u6027\u3002", "conclusion": "MIS-LSTM\u6846\u67b6\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u751f\u6d3b\u65e5\u5fd7\u6570\u636e\uff0cUALRE\u96c6\u6210\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u7761\u7720\u8d28\u91cf\u548c\u538b\u529b\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12194", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12194", "abs": "https://arxiv.org/abs/2509.12194", "authors": ["Thomas A. Buckley", "Riccardo Conci", "Peter G. Brodeur", "Jason Gusdorf", "Sourik Beltr\u00e1n", "Bita Behrouzi", "Byron Crowe", "Jacob Dockterman", "Muzzammil Muhammad", "Sarah Ohnigian", "Andrew Sanchez", "James A. Diao", "Aashna P. Shah", "Daniel Restrepo", "Eric S. Rosenberg", "Andrew S. Lea", "Marinka Zitnik", "Scott H. Podolsky", "Zahir Kanjee", "Raja-Elie E. Abdulnour", "Jacob M. Koshy", "Adam Rodman", "Arjun K. Manrai"], "title": "Advancing Medical Artificial Intelligence Using a Century of Cases", "comment": null, "summary": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.", "AI": {"tldr": "LLM\u5728\u590d\u6742\u6587\u672c\u8bca\u65ad\u65b9\u9762\u8d85\u8d8a\u533b\u751f\u8868\u73b0\uff0c\u80fd\u6709\u6548\u6a21\u62df\u4e13\u5bb6\u533b\u5b66\u6f14\u793a\uff0c\u4f46\u5728\u56fe\u50cf\u89e3\u8bfb\u548c\u6587\u732e\u68c0\u7d22\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u6d4b\u8bd5AI\u5728\u533b\u5b66\u8bca\u65ad\u63a8\u7406\u548c\u6f14\u793a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8d85\u8d8a\u4ee5\u5f80\u4ec5\u5173\u6ce8\u6700\u7ec8\u8bca\u65ad\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u4f7f\u75287102\u4e2a\u4e34\u5e8a\u75c5\u7406\u4f1a\u8bae\u6848\u4f8b\u548c1021\u4e2a\u56fe\u50cf\u6311\u6218\u521b\u5efaCPC-Bench\u57fa\u51c6\uff0c\u8bc4\u4f30\u9886\u5148LLM\u5e76\u5f00\u53d1Dr. CaBot AI\u8ba8\u8bba\u7cfb\u7edf\u3002", "result": "o3\u6a21\u578b\u572860%\u6848\u4f8b\u4e2d\u6392\u540d\u7b2c\u4e00\u8bca\u65ad\uff0c84%\u8fdb\u5165\u524d\u5341\uff0c\u4f18\u4e8e20\u540d\u533b\u751f\u57fa\u7ebf\uff1b\u56fe\u50cf\u4efb\u52a1\u51c6\u786e\u738767%\uff1bCaBot\u751f\u6210\u5185\u5bb974%\u88ab\u533b\u751f\u8bef\u8ba4\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u6240\u5199\u3002", "conclusion": "LLM\u5728\u6587\u672c\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u6a21\u62df\u4e13\u5bb6\u6f14\u793a\uff0c\u4f46\u56fe\u50cf\u548c\u6587\u732e\u68c0\u7d22\u4ecd\u9700\u6539\u8fdb\u3002CPC-Bench\u548cCaBot\u6709\u52a9\u4e8e\u6301\u7eed\u8ffd\u8e2a\u533b\u5b66AI\u8fdb\u5c55\u3002"}}
{"id": "2509.11687", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11687", "abs": "https://arxiv.org/abs/2509.11687", "authors": ["Di Jin", "Jun Yang", "Xiaobao Wang", "Junwei Zhang", "Shuqi Li", "Dongxiao He"], "title": "A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection", "comment": null, "summary": "As the Internet and social media evolve rapidly, distinguishing credible news\nfrom a vast amount of complex information poses a significant challenge. Due to\nthe suddenness and instability of news events, the authenticity labels of news\ncan potentially shift as events develop, making it crucial for fake news\ndetection to obtain the latest event updates. Existing methods employ\nretrieval-augmented generation to fill knowledge gaps, but they suffer from\nissues such as insufficient credibility of retrieved content and interference\nfrom noisy information. We propose a dynamic knowledge update-driven model for\nfake news detection (DYNAMO), which leverages knowledge graphs to achieve\ncontinuous updating of new knowledge and integrates with large language models\nto fulfill dual functions: news authenticity detection and verification of new\nknowledge correctness, solving the two key problems of ensuring the\nauthenticity of new knowledge and deeply mining news semantics. Specifically,\nwe first construct a news-domain-specific knowledge graph. Then, we use Monte\nCarlo Tree Search to decompose complex news and verify them step by step.\nFinally, we extract and update new knowledge from verified real news texts and\nreasoning paths. Experimental results demonstrate that DYNAMO achieves the best\nperformance on two real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86DYNAMO\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u52a8\u6001\u66f4\u65b0\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u89e3\u51b3\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u77e5\u8bc6\u771f\u5b9e\u6027\u9a8c\u8bc1\u548c\u8bed\u4e49\u6df1\u5ea6\u6316\u6398\u4e24\u4e2a\u5173\u952e\u95ee\u9898", "motivation": "\u4e92\u8054\u7f51\u548c\u793e\u4ea4\u5a92\u4f53\u5feb\u901f\u53d1\u5c55\uff0c\u6d77\u91cf\u590d\u6742\u4fe1\u606f\u4e2d\u533a\u5206\u53ef\u4fe1\u65b0\u95fb\u9762\u4e34\u6311\u6218\u3002\u65b0\u95fb\u4e8b\u4ef6\u7684\u7a81\u53d1\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4\u771f\u5b9e\u6027\u6807\u7b7e\u53ef\u80fd\u53d8\u5316\uff0c\u9700\u8981\u83b7\u53d6\u6700\u65b0\u4e8b\u4ef6\u66f4\u65b0", "method": "1. \u6784\u5efa\u65b0\u95fb\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31 2. \u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5206\u89e3\u590d\u6742\u65b0\u95fb\u5e76\u9010\u6b65\u9a8c\u8bc1 3. \u4ece\u5df2\u9a8c\u8bc1\u7684\u771f\u5b9e\u65b0\u95fb\u6587\u672c\u548c\u63a8\u7406\u8def\u5f84\u4e2d\u63d0\u53d6\u66f4\u65b0\u65b0\u77e5\u8bc6", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd", "conclusion": "DYNAMO\u6a21\u578b\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u77e5\u8bc6\u771f\u5b9e\u6027\u548c\u8bed\u4e49\u6316\u6398\u95ee\u9898"}}
{"id": "2509.11015", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.11015", "abs": "https://arxiv.org/abs/2509.11015", "authors": ["Rohan Tan Bhowmik", "Youn Soo Jung", "Juan Aguilera", "Mary Prunicki", "Kari Nadeau"], "title": "California Wildfire Inventory (CAWFI): An Extensive Dataset for Predictive Techniques based on Artificial Intelligence", "comment": null, "summary": "Due to climate change and the disruption of ecosystems worldwide, wildfires\nare increasingly impacting environment, infrastructure, and human lives\nglobally. Additionally, an exacerbating climate crisis means that these losses\nwould continue to grow if preventative measures are not implemented. Though\nrecent advancements in artificial intelligence enable wildfire management\ntechniques, most deployed solutions focus on detecting wildfires after\nignition. The development of predictive techniques with high accuracy requires\nextensive datasets to train machine learning models. This paper presents the\nCalifornia Wildfire Inventory (CAWFI), a wildfire database of over 37 million\ndata points for building and training wildfire prediction solutions, thereby\npotentially preventing megafires and flash fires by addressing them before they\nspark. The dataset compiles daily historical California wildfire data from 2012\nto 2018 and indicator data from 2012 to 2022. The indicator data consists of\nleading indicators (meteorological data correlating to wildfire-prone\nconditions), trailing indicators (environmental data correlating to prior and\nearly wildfire activity), and geological indicators (vegetation and elevation\ndata dictating wildfire risk and spread patterns). CAWFI has already\ndemonstrated success when used to train a spatio-temporal artificial\nintelligence model, predicting 85.7% of future wildfires larger than 300,000\nacres when trained on 2012-2017 indicator data. This dataset is intended to\nenable wildfire prediction research and solutions as well as set a precedent\nfor future wildfire databases in other regions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u52a0\u5dde\u91ce\u706b\u6570\u636e\u5e93(CAWFI)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b3700\u591a\u4e07\u6570\u636e\u70b9\u7684\u91ce\u706b\u9884\u6d4b\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e862012-2018\u5e74\u7684\u5386\u53f2\u91ce\u706b\u6570\u636e\u548c2012-2022\u5e74\u7684\u6307\u6807\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u91ce\u706b\u9884\u6d4b\u3002", "motivation": "\u7531\u4e8e\u6c14\u5019\u53d8\u5316\u548c\u751f\u6001\u7cfb\u7edf\u7834\u574f\uff0c\u5168\u7403\u91ce\u706b\u5bf9\u73af\u5883\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u4eba\u7c7b\u751f\u547d\u7684\u5f71\u54cd\u65e5\u76ca\u4e25\u91cd\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u706b\u707e\u53d1\u751f\u540e\u7684\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u9884\u6d4b\u6280\u672f\u6765\u9884\u9632\u706b\u707e\u53d1\u751f\u3002", "method": "\u6784\u5efa\u52a0\u5dde\u91ce\u706b\u6570\u636e\u5e93(CAWFI)\uff0c\u5305\u542b\u5386\u53f2\u91ce\u706b\u6570\u636e\u548c\u4e09\u7c7b\u6307\u6807\u6570\u636e\uff1a\u9886\u5148\u6307\u6807(\u6c14\u8c61\u6570\u636e)\u3001\u6ede\u540e\u6307\u6807(\u73af\u5883\u6570\u636e)\u548c\u5730\u8d28\u6307\u6807(\u690d\u88ab\u548c\u6d77\u62d4\u6570\u636e)\u3002", "result": "\u4f7f\u75282012-2017\u5e74\u6570\u636e\u8bad\u7ec3\u7684\u65f6\u7a7aAI\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e8685.7%\u7684\u672a\u6765\u8d85\u8fc730\u4e07\u82f1\u4ea9\u7684\u5927\u578b\u91ce\u706b\u3002", "conclusion": "CAWFI\u6570\u636e\u96c6\u4e3a\u91ce\u706b\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u4e3a\u5176\u4ed6\u5730\u533a\u5efa\u7acb\u7c7b\u4f3c\u6570\u636e\u5e93\u6811\u7acb\u4e86\u5148\u4f8b\uff0c\u6709\u52a9\u4e8e\u9884\u9632\u7279\u5927\u91ce\u706b\u548c\u7a81\u53d1\u706b\u707e\u3002"}}
{"id": "2509.11802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11802", "abs": "https://arxiv.org/abs/2509.11802", "authors": ["Dvora Goncharok", "Arbel Shifman", "Alexander Apartsin", "Yehudit Aperstein"], "title": "When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries", "comment": "5 pages, 2 figures", "summary": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u6807\u6ce8\u7684\u836f\u7269\u76f8\u5173\u5728\u7ebf\u8bba\u575b\u95ee\u9898\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u53ef\u80fd\u9884\u793a\u4e25\u91cd\u5065\u5eb7\u98ce\u9669\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u7ebf\u533b\u7597\u8bba\u575b\u5305\u542b\u5927\u91cf\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u60a3\u8005\u7528\u836f\u95ee\u9898\u4fe1\u606f\uff0c\u5176\u4e2d\u4e00\u4e9b\u95ee\u9898\u53ef\u80fd\u9884\u793a\u7740\u836f\u7269\u8bef\u7528\u6216\u5065\u5eb7\u5371\u673a\uff0c\u9700\u8981\u53ca\u65f6\u68c0\u6d4b\u8fd9\u4e9b\u5173\u952e\u95ee\u9898\u4ee5\u8fdb\u884c\u5e72\u9884\u5e76\u63d0\u9ad8\u60a3\u8005\u5b89\u5168\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u4e34\u5e8a\u98ce\u9669\u7b49\u7ea7\u7684\u836f\u7269\u76f8\u5173\u95ee\u9898\u6570\u636e\u96c6\uff0c\u4f7f\u7528TF-IDF\u6587\u672c\u8868\u793a\u7684\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff086\u79cd\uff09\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u5206\u7c7b\u65b9\u6cd5\uff083\u79cd\uff09\u8fdb\u884c\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u4ee3\u65b9\u6cd5\u5728\u652f\u6301\u6570\u5b57\u5065\u5eb7\u7a7a\u95f4\u5b9e\u65f6\u5206\u8bca\u548c\u8b66\u62a5\u7cfb\u7edf\u65b9\u9762\u90fd\u5177\u6709\u6f5c\u529b\uff0c\u76f8\u5173\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u4f7f\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u60a3\u8005\u751f\u6210\u6570\u636e\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5173\u952e\u5065\u5eb7\u4e8b\u4ef6\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u7684\u4ea4\u53c9\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u548c\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6570\u5b57\u5065\u5eb7\u5e72\u9884\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.11804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.11804", "abs": "https://arxiv.org/abs/2509.11804", "authors": ["Yulong Chen", "Michael Sejr Schlichtkrull", "Zhenyun Deng", "David Corney", "Nasim Asl", "Joshua Salisbury", "Andrew Dudfield", "Andreas Vlachos"], "title": "PledgeTracker: A System for Monitoring the Fulfilment of Pledges", "comment": "EMNLP 2025 demo", "summary": "Political pledges reflect candidates' policy commitments, but tracking their\nfulfilment requires reasoning over incremental evidence distributed across\nmultiple, dynamically updated sources. Existing methods simplify this task into\na document classification task, overlooking its dynamic, temporal and\nmulti-document nature. To address this issue, we introduce\n\\textsc{PledgeTracker}, a system that reformulates pledge verification into\nstructured event timeline construction. PledgeTracker consists of three core\ncomponents: (1) a multi-step evidence retrieval module; (2) a timeline\nconstruction module and; (3) a fulfilment filtering module, allowing the\ncapture of the evolving nature of pledge fulfilment and producing interpretable\nand structured timelines. We evaluate PledgeTracker in collaboration with\nprofessional fact-checkers in real-world workflows, demonstrating its\neffectiveness in retrieving relevant evidence and reducing human verification\neffort.", "AI": {"tldr": "PledgeTracker\u662f\u4e00\u4e2a\u5c06\u653f\u6cbb\u627f\u8bfa\u9a8c\u8bc1\u91cd\u65b0\u6784\u5efa\u4e3a\u7ed3\u6784\u5316\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u6784\u5efa\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u8bc1\u636e\u68c0\u7d22\u3001\u65f6\u95f4\u7ebf\u6784\u5efa\u548c\u5c65\u884c\u8fc7\u6ee4\u6a21\u5757\u6765\u8ddf\u8e2a\u627f\u8bfa\u7684\u5c65\u884c\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5c06\u653f\u6cbb\u627f\u8bfa\u9a8c\u8bc1\u7b80\u5316\u4e3a\u6587\u6863\u5206\u7c7b\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5176\u52a8\u6001\u6027\u3001\u65f6\u5e8f\u6027\u548c\u591a\u6587\u6863\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u589e\u91cf\u8bc1\u636e\u548c\u52a8\u6001\u66f4\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u591a\u6b65\u9aa4\u8bc1\u636e\u68c0\u7d22\u6a21\u5757\uff1b(2)\u65f6\u95f4\u7ebf\u6784\u5efa\u6a21\u5757\uff1b(3)\u5c65\u884c\u8fc7\u6ee4\u6a21\u5757\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u6765\u6355\u6349\u627f\u8bfa\u5c65\u884c\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002", "result": "\u4e0e\u4e13\u4e1a\u4e8b\u5b9e\u6838\u67e5\u4eba\u5458\u5728\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5408\u4f5c\u8bc4\u4f30\uff0c\u8bc1\u660e\u7cfb\u7edf\u5728\u68c0\u7d22\u76f8\u5173\u8bc1\u636e\u548c\u51cf\u5c11\u4eba\u5de5\u9a8c\u8bc1\u5de5\u4f5c\u91cf\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "PledgeTracker\u901a\u8fc7\u7ed3\u6784\u5316\u65f6\u95f4\u7ebf\u6784\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u653f\u6cbb\u627f\u8bfa\u9a8c\u8bc1\u7684\u52a8\u6001\u6027\u548c\u591a\u6587\u6863\u7279\u6027\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9a8c\u8bc1\u7ed3\u679c\u3002"}}
{"id": "2509.11345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11345", "abs": "https://arxiv.org/abs/2509.11345", "authors": ["Azher Ahmed Efat", "Farzana Islam", "Annajiat Alim Rasel", "Munima Haque"], "title": "BiLSTM-VHP: BiLSTM-Powered Network for Viral Host Prediction", "comment": null, "summary": "Recorded history shows the long coexistence of humans and animals, suggesting\nit began much earlier. Despite some beneficial interdependence, many animals\ncarry viral diseases that can spread to humans. These diseases are known as\nzoonotic diseases. Recent outbreaks of SARS-CoV-2, Monkeypox and swine flu\nviruses have shown how these viruses can disrupt human life and cause death.\nFast and accurate predictions of the host from which the virus spreads can help\nprevent these diseases from spreading. This work presents BiLSTM-VHP, a\nlightweight bidirectional long short-term memory (LSTM)-based architecture that\ncan predict the host from the nucleotide sequence of orthohantavirus, rabies\nlyssavirus, and rotavirus A with high accuracy. The proposed model works with\nnucleotide sequences of 400 bases in length and achieved a prediction accuracy\nof 89.62% for orthohantavirus, 96.58% for rotavirus A, and 77.22% for rabies\nlyssavirus outperforming previous studies. Moreover, performance of the model\nis assessed using the confusion matrix, F-1 score, precision, recall,\nmicroaverage AUC. In addition, we introduce three curated datasets of\northohantavirus, rotavirus A, and rabies lyssavirus containing 8,575, 95,197,\nand 22,052 nucleotide sequences divided into 9, 12, and 29 host classes,\nrespectively. The codes and dataset are available at\nhttps://doi.org/10.17605/OSF.IO/ANFKR", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411LSTM\u7684\u8f7b\u91cf\u7ea7\u6a21\u578bBiLSTM-VHP\uff0c\u80fd\u591f\u6839\u636e\u6838\u9178\u5e8f\u5217\u51c6\u786e\u9884\u6d4b\u75c5\u6bd2\u7684\u5bbf\u4e3b\u6765\u6e90\uff0c\u4ee5\u9632\u6b62\u4eba\u517d\u5171\u60a3\u75c5\u7684\u4f20\u64ad\u3002", "motivation": "\u4eba\u517d\u5171\u60a3\u75c5\u5982SARS-CoV-2\u3001\u7334\u75d8\u548c\u732a\u6d41\u611f\u7b49\u5bf9\u4eba\u7c7b\u751f\u547d\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5feb\u901f\u51c6\u786e\u9884\u6d4b\u75c5\u6bd2\u5bbf\u4e3b\u6765\u6e90\u53ef\u4ee5\u6709\u6548\u9632\u6b62\u75c5\u6bd2\u4f20\u64ad\u3002", "method": "\u4f7f\u7528\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc(BiLSTM)\u6784\u5efa\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5904\u7406400\u4e2a\u57fa\u56e0\u957f\u5ea6\u7684\u6838\u9178\u5e8f\u5217\uff0c\u5bf9\u6c49\u5764\u75c5\u6bd2\u3001\u65cb\u75c5\u6bd2A\u548c\u72fc\u75c3\u75c5\u6bd2\u8fdb\u884c\u5bbf\u4e3b\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u5728\u4e09\u79cd\u75c5\u6bd2\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523089.62%\u300196.58%\u548c77.22%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4ee5\u5f80\u7814\u7a76\u3002\u4f7f\u7528\u6df7\u6dc6\u77e9\u9635\u3001F1\u5206\u6570\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u7b49\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "conclusion": "BiLSTM-VHP\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u9884\u6d4b\u75c5\u6bd2\u5bbf\u4e3b\u6765\u6e90\uff0c\u4e3a\u4eba\u517d\u5171\u60a3\u75c5\u9632\u63a7\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u540c\u65f6\u516c\u5f00\u4e86\u4e09\u4e2a\u7ecf\u8fc7\u7cbe\u5fc3\u6574\u7406\u7684\u75c5\u6bd2\u5e8f\u5217\u6570\u636e\u96c6\u3002"}}
{"id": "2509.11449", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11449", "abs": "https://arxiv.org/abs/2509.11449", "authors": ["Shriyank Somvanshi", "Pavan Hebli", "Gaurab Chhetri", "Subasish Das"], "title": "Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models", "comment": "This is the author's preprint version of a paper accepted for\n  presentation at the 24th International Conference on Machine Learning and\n  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final\n  published version will appear in the official IEEE proceedings. Conference\n  site: https://www.icmla-conference.org/icmla25/", "summary": "This study presents a deep tabular learning framework for predicting crash\nseverity in electric vehicle (EV) collisions using real-world crash data from\nTexas (2017-2023). After filtering for electric-only vehicles, 23,301\nEV-involved crash records were analyzed. Feature importance techniques using\nXGBoost and Random Forest identified intersection relation, first harmful\nevent, person age, crash speed limit, and day of week as the top predictors,\nalong with advanced safety features like automatic emergency braking. To\naddress class imbalance, Synthetic Minority Over-sampling Technique and Edited\nNearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-art\ndeep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked for\nseverity prediction. While TabPFN demonstrated strong generalization,\nMambaAttention achieved superior performance in classifying severe injury cases\ndue to its attention-based feature reweighting. The findings highlight the\npotential of deep tabular architectures for improving crash severity prediction\nand enabling data-driven safety interventions in EV crash contexts.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u8868\u683c\u5b66\u4e60\u6846\u67b6\u9884\u6d4b\u7535\u52a8\u8f66\u78b0\u649e\u4e25\u91cd\u7a0b\u5ea6\uff0c\u901a\u8fc7SMOTEENN\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u6a21\u578b\u6bd4\u8f83\u663e\u793aMambaAttention\u5728\u4e25\u91cd\u4f24\u5bb3\u5206\u7c7b\u4e0a\u8868\u73b0\u6700\u4f18", "motivation": "\u4f53\u73b0\u6df1\u5ea6\u8868\u683c\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u8f66\u78b0\u649e\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u5b89\u5168\u5e72\u9884\u7b56\u7565", "method": "\u4f7f\u7528\u5f97\u5143\u5dde2017-2023\u5e74\u7535\u52a8\u8f66\u78b0\u649e\u6570\u636e\uff0c\u901a\u8fc7XGBoost\u548c\u968f\u673a\u68ee\u6797\u8bc6\u522b\u5173\u952e\u7279\u5f81\uff0c\u91c7\u7528SMOTEENN\u91cd\u91c7\u6837\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u6bd4\u8f83TabPFN\u3001MambaNet\u548cMambaAttention\u4e09\u79cd\u6df1\u5ea6\u8868\u683c\u6a21\u578b\u7684\u8868\u73b0", "result": "\u4ea4\u53c9\u53e3\u5173\u7cfb\u3001\u9996\u6b21\u6709\u5bb3\u4e8b\u4ef6\u3001\u5e74\u9f84\u3001\u78b0\u649e\u901f\u5ea6\u9650\u5236\u548c\u661f\u671f\u51e0\u662f\u6700\u91cd\u8981\u9884\u6d4b\u56e0\u7d20\uff0cMambaAttention\u6a21\u578b\u5728\u4e25\u91cd\u4f24\u5bb3\u5206\u7c7b\u4e0a\u8868\u73b0\u6700\u4f18\uff0cTabPFN\u663e\u793a\u826f\u597d\u7684\u666e\u904d\u5316\u80fd\u529b", "conclusion": "\u6df1\u5ea6\u8868\u683c\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u7535\u52a8\u8f66\u78b0\u649e\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5b89\u5168\u5e72\u9884\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6491"}}
{"id": "2509.11520", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11520", "abs": "https://arxiv.org/abs/2509.11520", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "Know What You Don't Know: Selective Prediction for Early Exit DNNs", "comment": "To appear in the the Fifth International Conference on AI ML Systems", "summary": "Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the\nbottlenecks in deploying them in critical applications like sensitive tasks.\nEarly Exit (EE) DNNs overcome the latency issues by allowing samples to exit\nfrom intermediary layers if they attain `high' confidence scores on the\npredicted class. However, the DNNs are known to exhibit overconfidence, which\ncan lead to many samples exiting early and render EE strategies untrustworthy.\nWe use Selective Prediction (SP) to overcome this issue by checking the\n`hardness' of the samples rather than just relying on the confidence score\nalone. We propose SPEED, a novel approach that uses Deferral Classifiers (DCs)\nat each layer to check the hardness of samples before performing EEs.\nSpecifically, the DCs identify if a sample is hard to predict at an\nintermediary layer, leading to hallucination, and defer it to an expert. Early\ndetection of hard samples for inference prevents the wastage of computational\nresources and improves trust by deferring the hard samples to the expert. We\ndemonstrate that EE aided with SP improves both accuracy and latency. Our\nmethod minimizes the risk of wrong prediction by $50\\%$ with a speedup of\n$2.05\\times$ as compared to the final layer. The anonymized source code is\navailable at https://github.com/Div290/SPEED", "AI": {"tldr": "SPEED\u662f\u4e00\u79cd\u7ed3\u5408\u9009\u62e9\u6027\u9884\u6d4b\u548c\u65e9\u671f\u9000\u51fa\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ef6\u8fdf\u5206\u7c7b\u5668\u5728\u4e2d\u95f4\u5c42\u68c0\u6d4b\u56f0\u96be\u6837\u672c\uff0c\u63d0\u9ad8DNN\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u6027\u80fd", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u7279\u522b\u662f\u65e9\u671f\u9000\u51fa\u7b56\u7565\u56e0\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u800c\u5bfc\u81f4\u7684\u4e0d\u53ef\u9760\u9884\u6d4b\u95ee\u9898", "method": "\u5728\u6bcf\u4e00\u5c42\u4f7f\u7528\u5ef6\u8fdf\u5206\u7c7b\u5668(DCs)\u6765\u68c0\u67e5\u6837\u672c\u7684\u96be\u5ea6\uff0c\u8bc6\u522b\u96be\u4ee5\u9884\u6d4b\u7684\u6837\u672c\u5e76\u5c06\u5176\u63a8\u8fdf\u5230\u4e13\u5bb6\u5c42\u5904\u7406\uff0c\u907f\u514d\u65e9\u671f\u9000\u51fa\u5bfc\u81f4\u7684\u9519\u8bef\u9884\u6d4b", "result": "\u65b9\u6cd5\u5c06\u9519\u8bef\u9884\u6d4b\u98ce\u9669\u964d\u4f4e\u4e8650%\uff0c\u76f8\u6bd4\u6700\u7ec8\u5c42\u5b9e\u73b0\u4e862.05\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u6027\u80fd", "conclusion": "\u9009\u62e9\u6027\u9884\u6d4b\u4e0e\u65e9\u671f\u9000\u51fa\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u4fe1\u5ea6\u548c\u63a8\u7406\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5173\u952e\u5e94\u7528\u573a\u666f"}}
{"id": "2509.12188", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12188", "abs": "https://arxiv.org/abs/2509.12188", "authors": ["Antonin Sulc"], "title": "Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences", "comment": "10 pages, 3 figures, Symmetry and Geometry in Neural Representations\n  Workshop at NeuralIPS (Neurreps) 2025", "summary": "The study of neural representations, both in biological and artificial\nsystems, is increasingly revealing the importance of geometric and topological\nstructures. Inspired by this, we introduce Event2Vec, a novel framework for\nlearning representations of discrete event sequences. Our model leverages a\nsimple, additive recurrent structure to learn composable, interpretable\nembeddings. We provide a theoretical analysis demonstrating that, under\nspecific training objectives, our model's learned representations in a\nEuclidean space converge to an ideal additive structure. This ensures that the\nrepresentation of a sequence is the vector sum of its constituent events, a\nproperty we term the linear additive hypothesis. To address the limitations of\nEuclidean geometry for hierarchical data, we also introduce a variant of our\nmodel in hyperbolic space, which is naturally suited to embedding tree-like\nstructures with low distortion. We present experiments to validate our\nhypothesis and demonstrate the benefits of each geometry, highlighting the\nimproved performance of the hyperbolic model on hierarchical event sequences.", "AI": {"tldr": "Event2Vec\u662f\u4e00\u4e2a\u5b66\u4e60\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u8868\u793a\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u52a0\u6cd5\u5faa\u73af\u7ed3\u6784\u5b66\u4e60\u53ef\u7ec4\u5408\u3001\u53ef\u89e3\u91ca\u7684\u5d4c\u5165\u3002\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u6536\u655b\u5230\u7406\u60f3\u52a0\u6cd5\u7ed3\u6784\uff0c\u5e8f\u5217\u8868\u793a\u662f\u5176\u7ec4\u6210\u4e8b\u4ef6\u7684\u5411\u91cf\u548c\u3002\u8fd8\u63d0\u51fa\u4e86\u53cc\u66f2\u7a7a\u95f4\u53d8\u4f53\uff0c\u66f4\u9002\u5408\u5206\u5c42\u6570\u636e\u3002", "motivation": "\u53d7\u795e\u7ecf\u8868\u793a\u4e2d\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\u91cd\u8981\u6027\u7684\u542f\u53d1\uff0c\u9700\u8981\u4e3a\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u5f00\u53d1\u80fd\u591f\u6355\u6349\u5176\u5185\u5728\u7ed3\u6784\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u52a0\u6cd5\u5faa\u73af\u7ed3\u6784\u5b66\u4e60\u53ef\u7ec4\u5408\u5d4c\u5165\uff0c\u5728\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5206\u522b\u5b9e\u73b0\uff0c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5728\u7279\u5b9a\u8bad\u7ec3\u76ee\u6807\u4e0b\u6536\u655b\u5230\u7ebf\u6027\u52a0\u6cd5\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ebf\u6027\u52a0\u6cd5\u5047\u8bbe\uff0c\u53cc\u66f2\u7a7a\u95f4\u6a21\u578b\u5728\u5206\u5c42\u4e8b\u4ef6\u5e8f\u5217\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u51e0\u4f55\u7a7a\u95f4\u7684\u4f18\u52bf\u3002", "conclusion": "Event2Vec\u6846\u67b6\u6210\u529f\u5b66\u4e60\u4e86\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u7684\u6709\u6548\u8868\u793a\uff0c\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u9002\u5408\u4e00\u822c\u5e8f\u5217\uff0c\u53cc\u66f2\u7a7a\u95f4\u66f4\u9002\u5408\u5206\u5c42\u7ed3\u6784\uff0c\u51e0\u4f55\u9009\u62e9\u5bf9\u8868\u793a\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.11628", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.11628", "abs": "https://arxiv.org/abs/2509.11628", "authors": ["Jiacheng Liu", "Chang Zou", "Yuanhuiyi Lyu", "Fei Ren", "Shaobo Wang", "Kaixin Li", "Linfeng Zhang"], "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching", "comment": "15 pages, 9 figures, ACM Multimedia 2025", "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}", "AI": {"tldr": "SpeCa\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u6d4b\u91c7\u6837\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4e2d\u95f4\u7279\u5f81\u548c\u9a8c\u8bc1\u673a\u5236\u5b9e\u73b06-7\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "motivation": "\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u5b58\u5728\u4e25\u683c\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u524d\u5411\u4f20\u9012\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42", "method": "\u5f15\u5165\u63a8\u6d4b\u91c7\u6837\u9884\u6d4b\u540e\u7eed\u65f6\u95f4\u6b65\u7279\u5f81\uff0c\u91c7\u7528\u53c2\u6570\u65e0\u5173\u9a8c\u8bc1\u673a\u5236\u8bc4\u4f30\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u5e76\u5b9e\u73b0\u6837\u672c\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d", "result": "\u5728FLUX\u4e0a\u5b9e\u73b06.34\u500d\u52a0\u901f\uff08\u8d28\u91cf\u4e0b\u964d5.5%\uff09\uff0cDiT\u4e0a7.3\u500d\u52a0\u901f\u4fdd\u6301\u4fdd\u771f\u5ea6\uff0cHunyuanVideo\u4e0a6.1\u500d\u52a0\u901f\u83b7\u5f9779.84% VBench\u5206\u6570", "conclusion": "SpeCa\u4e3a\u6269\u6563\u6a21\u578b\u63a8\u7406\u5efa\u7acb\u4e86\u9ad8\u6548\u65b0\u8303\u5f0f\uff0c\u5728\u6fc0\u8fdb\u52a0\u901f\u6bd4\u4e0b\u4ecd\u80fd\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u9a8c\u8bc1\u673a\u5236\u5f00\u9500\u6781\u5c0f\uff081.67%-3.5%\uff09"}}
{"id": "2509.12046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12046", "abs": "https://arxiv.org/abs/2509.12046", "authors": ["Zirui Zheng", "Takashi Isobe", "Tong Shen", "Xu Jia", "Jianbin Zhao", "Xiaomin Li", "Mengmeng Ge", "Baolu Li", "Qinghe Wang", "Dong Li", "Dong Zhou", "Yunzhi Zhuge", "Huchuan Lu", "Emad Barsoum"], "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking", "comment": "10 pages, 3 figures", "summary": "While autoregressive (AR) models have demonstrated remarkable success in\nimage generation, extending them to layout-conditioned generation remains\nchallenging due to the sparse nature of layout conditions and the risk of\nfeature entanglement. We present Structured Masking for AR-based\nLayout-to-Image (SMARLI), a novel framework for layoutto-image generation that\neffectively integrates spatial layout constraints into AR-based image\ngeneration. To equip AR model with layout control, a specially designed\nstructured masking strategy is applied to attention computation to govern the\ninteraction among the global prompt, layout, and image tokens. This design\nprevents mis-association between different regions and their descriptions while\nenabling sufficient injection of layout constraints into the generation\nprocess. To further enhance generation quality and layout accuracy, we\nincorporate Group Relative Policy Optimization (GRPO) based post-training\nscheme with specially designed layout reward functions for next-set-based AR\nmodels. Experimental results demonstrate that SMARLI is able to seamlessly\nintegrate layout tokens with text and image tokens without compromising\ngeneration quality. It achieves superior layoutaware control while maintaining\nthe structural simplicity and generation efficiency of AR models.", "AI": {"tldr": "SMARLI\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a9\u7801\u7b56\u7565\u548cGRPO\u540e\u8bad\u7ec3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e03\u5c40\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5e03\u5c40\u63a7\u5236\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6269\u5c55\u5230\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5e03\u5c40\u6761\u4ef6\u7684\u7a00\u758f\u6027\u548c\u7279\u5f81\u7ea0\u7f20\u98ce\u9669\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6574\u5408\u7a7a\u95f4\u5e03\u5c40\u7ea6\u675f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u63a9\u7801\u7b56\u7565\u6765\u63a7\u5236\u5168\u5c40\u63d0\u793a\u3001\u5e03\u5c40\u548c\u56fe\u50cftoken\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u9632\u6b62\u4e0d\u540c\u533a\u57df\u4e0e\u5176\u63cf\u8ff0\u4e4b\u95f4\u7684\u9519\u8bef\u5173\u8054\u3002\u540c\u65f6\u91c7\u7528\u57fa\u4e8eGroup Relative Policy Optimization\u7684\u540e\u8bad\u7ec3\u65b9\u6848\uff0c\u914d\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u5e03\u5c40\u5956\u52b1\u51fd\u6570\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u5e03\u5c40\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSMARLI\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u5e03\u5c40token\u4e0e\u6587\u672c\u548c\u56fe\u50cftoken\uff0c\u5728\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5353\u8d8a\u7684\u5e03\u5c40\u611f\u77e5\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u56de\u5f52\u6a21\u578b\u7684\u7ed3\u6784\u7b80\u5355\u6027\u548c\u751f\u6210\u6548\u7387\u3002", "conclusion": "SMARLI\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5e03\u5c40\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5e03\u5c40\u63a7\u5236\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7b80\u6d01\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5e03\u5c40\u63a7\u5236\u751f\u6210\u3002"}}
{"id": "2509.11789", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11789", "abs": "https://arxiv.org/abs/2509.11789", "authors": ["Timilehin B. Aderinola", "Luca Palmerini", "Ilaria D'Ascanio", "Lorenzo Chiari", "Jochen Klenk", "Clemens Becker", "Brian Caulfield", "Georgiana Ifrim"], "title": "Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall Detection in Real-World Streaming Scenarios", "comment": null, "summary": "Real-time fall detection is crucial for enabling timely interventions and\nmitigating the severe health consequences of falls, particularly in older\nadults. However, existing methods often rely on simulated data or assumptions\nsuch as prior knowledge of fall events, limiting their real-world\napplicability. Practical deployment also requires efficient computation and\nrobust evaluation metrics tailored to continuous monitoring. This paper\npresents a real-time fall detection framework for continuous monitoring without\nprior knowledge of fall events. Using over 60 hours of inertial measurement\nunit (IMU) data from the FARSEEING real-world falls dataset, we employ recent\nefficient classifiers to compute fall probabilities in streaming mode. To\nenhance robustness, we introduce a cost-sensitive learning strategy that tunes\nthe decision threshold using a cost function reflecting the higher risk of\nmissed falls compared to false alarms. Unlike many methods that achieve high\nrecall only at the cost of precision, our framework achieved Recall of 1.00,\nPrecision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls\nwhile keeping false alarms low, with average inference time below 5 ms per\nsample. These results demonstrate that cost-sensitive threshold tuning enhances\nthe robustness of accelerometer-based fall detection. They also highlight the\npotential of our computationally efficient framework for deployment in\nreal-time wearable sensor systems for continuous monitoring.", "AI": {"tldr": "\u57fa\u4e8eIMU\u6570\u636e\u7684\u5b9e\u65f6\u8ecc\u5019\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6210\u672c\u654f\u611f\u5b66\u4e60\u7b56\u7565\u8c03\u6574\u51b3\u7b56\u9608\u503c\uff0c\u5728FARSEEING\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e861.00\u7684\u53ec\u56de\u7387\u548c0.84\u7684\u7cbe\u786e\u5ea6\uff0c\u6bcf\u6b21\u68c0\u6d4b\u5e73\u5747\u8017\u65f65\u6beb\u79d2\u4ee5\u5185\u3002", "motivation": "\u73b0\u6709\u8ecc\u5019\u68c0\u6d4b\u65b9\u6cd5\u591a\u4f9d\u8d56\u4eff\u771f\u6570\u636e\u6216\u5047\u8bbe\u77e5\u9053\u8ecc\u5019\u4e8b\u4ef6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u7528\u6027\uff0c\u9700\u8981\u9ad8\u6548\u8ba1\u7b97\u548c\u7a33\u5065\u8bc4\u4f30\u6307\u6807\u6765\u652f\u6301\u8fde\u7eed\u76d1\u6d4b\u3002", "method": "\u4f7f\u7528FARSEEING\u5b9e\u9645\u8ecc\u5019\u6570\u636e\u96c6\u768460\u5c0f\u65f6IMU\u6570\u636e\uff0c\u91c7\u7528\u9ad8\u6548\u5206\u7c7b\u5668\u8fdb\u884c\u6d41\u5f0f\u8ecc\u5019\u6982\u7387\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u6210\u672c\u654f\u611f\u5b66\u4e60\u7b56\u7565\u8c03\u6574\u51b3\u7b56\u9608\u503c\uff0c\u4ee5\u53cd\u6620\u6f0f\u68c0\u8ecc\u5019\u6bd4\u5047\u62a5\u8b66\u7684\u9ad8\u98ce\u9669\u3002", "result": "\u5728FARSEEING\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u53ec\u56de\u73871.00\u3001\u7cbe\u786e\u5ea60.84\u3001F1\u52060.91\uff0c\u68c0\u6d4b\u6240\u6709\u8ecc\u5019\u4e8b\u4ef6\u7684\u540c\u65f6\u4fdd\u6301\u4f4e\u5047\u62a5\u8b66\u7387\uff0c\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u5c0f\u4e8e5\u6beb\u79d2\u3002", "conclusion": "\u6210\u672c\u654f\u611f\u9608\u503c\u8c03\u6574\u63d0\u9ad8\u4e86\u52a0\u901f\u8ba1\u57fa\u7840\u7684\u8ecc\u5019\u68c0\u6d4b\u7cfb\u7edf\u7684\u7a33\u5065\u6027\uff0c\u8be5\u9ad8\u6548\u8ba1\u7b97\u6846\u67b6\u6709\u6f5c\u529b\u90e8\u7f72\u4e8e\u5b9e\u65f6\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u8fdb\u884c\u8fde\u7eed\u76d1\u6d4b\u3002"}}
{"id": "2509.12068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12068", "abs": "https://arxiv.org/abs/2509.12068", "authors": ["Farahdiba Zarin", "Nicolas Padoy", "J\u00e9r\u00e9my Dana", "Vinkle Srivastav"], "title": "End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data", "comment": null, "summary": "The fine-grained surface reconstruction of different organs from 3D medical\nimaging can provide advanced diagnostic support and improved surgical planning.\nHowever, the representation of the organs is often limited by the resolution,\nwith a detailed higher resolution requiring more memory and computing\nfootprint. Implicit representations of objects have been proposed to alleviate\nthis problem in general computer vision by providing compact and differentiable\nfunctions to represent the 3D object shapes. However, architectural and\ndata-related differences prevent the direct application of these methods to\nmedical images. This work introduces ImplMORe, an end-to-end deep learning\nmethod using implicit surface representations for multi-organ reconstruction\nfrom 3D medical images. ImplMORe incorporates local features using a 3D CNN\nencoder and performs multi-scale interpolation to learn the features in the\ncontinuous domain using occupancy functions. We apply our method for single and\nmultiple organ reconstructions using the totalsegmentator dataset. By\nleveraging the continuous nature of occupancy functions, our approach\noutperforms the discrete explicit representation based surface reconstruction\napproaches, providing fine-grained surface details of the organ at a resolution\nhigher than the given input image. The source code will be made publicly\navailable at: https://github.com/CAMMA-public/ImplMORe", "AI": {"tldr": "ImplMORe\u662f\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u8868\u9762\u8868\u793a\u7684\u6df1\u5ea6\u5b66\u4e60\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece3D\u533b\u5b66\u56fe\u50cf\u8fdb\u884c\u591a\u5668\u5b98\u91cd\u5efa\uff0c\u901a\u8fc7\u8fde\u7eed\u57df\u7279\u5f81\u5b66\u4e60\u548c\u591a\u5c3a\u5ea6\u63d2\u503c\u5b9e\u73b0\u6bd4\u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\u66f4\u9ad8\u7684\u7cbe\u7ec6\u8868\u9762\u91cd\u5efa\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u5668\u5b98\u7684\u7cbe\u7ec6\u8868\u9762\u91cd\u5efa\u5bf9\u8bca\u65ad\u548c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u5206\u8fa8\u7387\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u867d\u7136\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u63d0\u51fa\u4e86\u9690\u5f0f\u8868\u793a\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u67b6\u6784\u548c\u6570\u636e\u5dee\u5f02\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u3002", "method": "\u4f7f\u75283D CNN\u7f16\u7801\u5668\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u63d2\u503c\u5728\u8fde\u7eed\u57df\u4e2d\u5b66\u4e60\u7279\u5f81\uff0c\u91c7\u7528\u5360\u636e\u51fd\u6570\u8868\u793a\u5668\u5b98\u8868\u9762\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u91cd\u5efa\u3002", "result": "\u5728totalsegmentator\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u5668\u5b98\u548c\u591a\u5668\u5b98\u91cd\u5efa\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u4e8e\u79bb\u6563\u663e\u5f0f\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u6bd4\u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\u66f4\u9ad8\u7684\u7cbe\u7ec6\u8868\u9762\u7ec6\u8282\u3002", "conclusion": "ImplMORe\u901a\u8fc7\u9690\u5f0f\u8868\u9762\u8868\u793a\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5668\u5b98\u91cd\u5efa\u4e2d\u7684\u5206\u8fa8\u7387\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u7cbe\u7ec6\u8868\u9762\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2509.12145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12145", "abs": "https://arxiv.org/abs/2509.12145", "authors": ["Hyolim Kang", "Yunsu Park", "Youngbeom Yoo", "Yeeun Choi", "Seon Joo Kim"], "title": "Open-ended Hierarchical Streaming Video Understanding with Vision Language Models", "comment": "17 pages", "summary": "We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u5c42\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u7ed3\u5408\u5728\u7ebf\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u548c\u81ea\u7531\u5f62\u5f0f\u63cf\u8ff0\u751f\u6210\uff0c\u5f00\u53d1\u4e86OpenHOUSE\u7cfb\u7edf\u6765\u63d0\u5347\u6d41\u5f0f\u52a8\u4f5c\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5206\u5c42\u548c\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u6807\u6ce8\uff0c\u9700\u8981\u5c06\u539f\u5b50\u52a8\u4f5c\u7ec4\u5408\u6210\u9ad8\u7ea7\u4e8b\u4ef6\uff0c\u5e76\u6269\u5c55\u6d41\u5f0f\u52a8\u4f5c\u611f\u77e5\u8d85\u8d8a\u52a8\u4f5c\u5206\u7c7b\u7684\u8303\u7574\u3002", "method": "\u5229\u7528LLMs\u5c06\u539f\u5b50\u52a8\u4f5c\u5206\u7ec4\u4e3a\u9ad8\u7ea7\u4e8b\u4ef6\u6765\u4e30\u5bcc\u6570\u636e\u96c6\uff0c\u63d0\u51faOpenHOUSE\u7cfb\u7edf\uff0c\u5305\u542b\u4e13\u95e8\u7684\u6d41\u5f0f\u6a21\u5757\u6765\u51c6\u786e\u68c0\u6d4b\u76f8\u90bb\u52a8\u4f5c\u8fb9\u754c\u3002", "result": "OpenHOUSE\u7cfb\u7edf\u5728\u68c0\u6d4b\u76f8\u90bb\u52a8\u4f5c\u8fb9\u754c\u65b9\u9762\u6027\u80fd\u63a5\u8fd1\u7ffb\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u76f4\u63a5\u6269\u5c55\u3002", "conclusion": "\u6d41\u5f0f\u52a8\u4f5c\u611f\u77e5\u7684\u672a\u6765\u5728\u4e8e\u96c6\u6210\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0cOpenHOUSE\u662f\u5411\u8fd9\u4e2a\u65b9\u5411\u8fc8\u51fa\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2509.12147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12147", "abs": "https://arxiv.org/abs/2509.12147", "authors": ["Maria Conchita Agana Navarro", "Geng Li", "Theo Wolf", "Mar\u00eda P\u00e9rez-Ortiz"], "title": "Do machine learning climate models work in changing climate dynamics?", "comment": "8 pages, 2 figures", "summary": "Climate change is accelerating the frequency and severity of unprecedented\nevents, deviating from established patterns. Predicting these\nout-of-distribution (OOD) events is critical for assessing risks and guiding\nclimate adaptation. While machine learning (ML) models have shown promise in\nproviding precise, high-speed climate predictions, their ability to generalize\nunder distribution shifts remains a significant limitation that has been\nunderexplored in climate contexts. This research systematically evaluates\nstate-of-the-art ML-based climate models in diverse OOD scenarios by adapting\nestablished OOD evaluation methodologies to climate data. Experiments on\nlarge-scale datasets reveal notable performance variability across scenarios,\nshedding light on the strengths and limitations of current models. These\nfindings underscore the importance of robust evaluation frameworks and provide\nactionable insights to guide the reliable application of ML for climate risk\nforecasting.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6c14\u5019\u53d8\u5316\u9884\u6d4b\u4e2d\u7684\u5206\u5e03\u5916\u901a\u7528\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u6ce2\u52a8", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u901f\u5f15\u53d1\u65e0\u524d\u4f8b\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u9884\u6d4b\u8fd9\u4e9b\u5206\u5e03\u5916\u4e8b\u4ef6\u5bf9\u98ce\u9669\u8bc4\u4f30\u548c\u9002\u5e94\u63aa\u65bd\u81f3\u5173\u91cd\u8981", "method": "\u901a\u8fc7\u9002\u914d\u5206\u5e03\u5916\u8bc4\u4f30\u65b9\u6cd5\u5230\u6c14\u5019\u6570\u636e\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u5404\u79cd\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8868\u73b0", "result": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0d\u540c\u573a\u666f\u4e0b\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4f18\u7f3a\u70b9", "conclusion": "\u5efa\u7acb\u5065\u58ee\u7684\u8bc4\u4f30\u6846\u67b6\u5bf9\u4e8e\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u89c1\u89e3"}}
